<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ASOC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="asoc---950">ASOC - 950</h2>
<ul>
<li><details>
<summary>
(2023). Convolutional architecture search based on particle swarm
algorithm for functional brain network classification. <em>ASOC</em>,
<em>149</em>, 111049. (<a
href="https://doi.org/10.1016/j.asoc.2023.111049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The functional brain network (FBN) classification based on convolutional neural networks (CNN) is of great significance for discovery and diagnosis of brain diseases, and has attracted increasing attention. However, all the CNN architectures of current studies mainly depend on hand-crafted, which are labor-intensive and unreliable. To solve it, we propose a neural architecture search (NAS) method based on particle swarm optimization , to automatically design the CNN architecture for FBN classification. Specifically, this method includes three phases, namely the individual expression phase, the individual evaluation phase, and the individual update phase. In the first phase, we treat the neural architecture as the individual in particle swarm . The individual vector consists of six elements, and the value of each element represents the number of a special convolution operation . The six special convolution operations can effectively extract brain network multilevel topological features . In the second phase, we propose a novel surrogate-assisted predictor to evaluate the fitness of the individuals more efficiently. In the last phase, we apply the predicted fitness to acquire the historical optimum of each individual and the global optimum of the population, and use them to update all individuals in the particle swarm . The second and third phases are repeatedly performed until the end condition is met. Experiments on benchmark datasets demonstrate that the CNN architecture searched by our method achieves better classification performance than state-of-the-art hand-crafted CNN architectures.},
  archive      = {J_ASOC},
  author       = {Junzhong Ji and Xingyu Wang},
  doi          = {10.1016/j.asoc.2023.111049},
  journal      = {Applied Soft Computing},
  pages        = {111049},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convolutional architecture search based on particle swarm algorithm for functional brain network classification},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Influence maximization in social networks using discretized
harris’ hawks optimization algorithm. <em>ASOC</em>, <em>149</em>,
111037. (<a href="https://doi.org/10.1016/j.asoc.2023.111037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence Maximization (IM) is the task of determining k k optimal influential nodes in a social network to maximize the influence spread using a propagation model . IM is a prominent problem for viral marketing and helps significantly in social media advertising. Previous Greedy and Reverse Influence Sampling-based IM approaches are ineffective in real-world social networks due to their significant computational cost and execution time. Further, even heuristic approaches applied to IM generally yield minimal performance gain relative to the decreased time complexity. This presents a challenge in developing cost-effective algorithms with low execution time that can handle diverse social networks. In this paper, we propose the discretization of the nature-inspired Harris’ Hawks Optimization meta-heuristic algorithm using community structures for optimal selection of seed nodes for influence spread. In addition to Harris’ Hawks’ intelligence, we employ a neighbor scout strategy algorithm to avoid blindness and enhance the searching ability of the hawks. Further, we use a candidate nodes-based random population initialization approach, and these candidate nodes aid in accelerating the convergence process for the entire populace, reducing the total computational cost. We evaluate the efficacy of our proposed DHHO approach on eight social networks using the Independent Cascade model for information diffusion . We observe that DHHO is comparable or better than competing meta-heuristic approaches for Influence Maximization across five metrics, and performs noticeably better than competing heuristic approaches.},
  archive      = {J_ASOC},
  author       = {Inder Khatri and Arjun Choudhry and Aryaman Rao and Aryan Tyagi and Dinesh Kumar Vishwakarma and Mukesh Prasad},
  doi          = {10.1016/j.asoc.2023.111037},
  journal      = {Applied Soft Computing},
  pages        = {111037},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Influence maximization in social networks using discretized harris’ hawks optimization algorithm},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Semi-disentangled non-negative matrix factorization for
rating prediction. <em>ASOC</em>, <em>149</em>, 111034. (<a
href="https://doi.org/10.1016/j.asoc.2023.111034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rating predictions have been extensively used in evaluation websites for recommendation tasks in recent past. Due to the single data category and the simple way of feature interaction, traditional rating prediction methods do not portray the actual scoring properly, and lack of interpretation at the same time, which problem is particularly acute in latent factor model. Thanks to the excellent interpretability and generalizability of disentangled representations, in this paper, we propose a Semi-Disentangled Non-negative Matrix Factorization (SDNMF) by splitting user- and item-latent embedding into two blocks with an improved regularizer: the whiten block and the blacked block. Specifically, the whiten block uses external prior knowledge instead of latent factors to improve the internal interpretability of SDNMF, whereas the blacked block consists of those latent factors different from the prior knowledge to promote more feature interaction and preserve algorithm performance, and the regularizer is defined by Kullback–Leibler divergence to characterize the relationship between whiten and black blocks. By disentangling user–item relations to obtain disentangled representation, we want to be able to on this basis, the non-negative matrix factorization for a certain degree of interpretation. Extensive experiments conducted on four real-world data sets (Amazon, Yelp, Yelp10, Dianping) indicate that the proposed SDNMF not only is the state-of-the-art rating prediction method compared with other existing methods, but also preserves the ability to be interpreted.},
  archive      = {J_ASOC},
  author       = {Xiaoxia Zhang and Xianjun Zhou and Lu Chen and Yanjun Liu},
  doi          = {10.1016/j.asoc.2023.111034},
  journal      = {Applied Soft Computing},
  pages        = {111034},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-disentangled non-negative matrix factorization for rating prediction},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable self-organizing map assisted interactive
multi-criteria decision-making following pareto-race. <em>ASOC</em>,
<em>149</em>, 111032. (<a
href="https://doi.org/10.1016/j.asoc.2023.111032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem-solving task of the multi-criteria decision-making (MCDM) approach involves decision makers’ (DMs’) interaction by incorporating their preferences to arrive at one or more preferred near Pareto-optimal solution(s). The Pareto-Race is an interactive MCDM approach that relies on finding preferred solutions as the DM navigates on the Pareto surface by selecting the preferred reference direction and uniform/varying speed using the concept of Achievement Scalarization Function (ASF) or its augmented version (AASF). Selecting a reference direction or speed is possible, algorithmically, but the effectiveness of the Pareto-Race concept relies on suitable visualization methods that present the series of past solutions and convey the current objective trade-off information to assist the DM’s in decision-making task. Traditional multi-dimensional Pareto-optimal front visualization methods like parallel coordinate plots, radial visualization, and heat maps are deemed insufficient for this purpose. Therefore, the paper proposes integrating the concept of the Pareto-Race MCDM approach with a recently developed interpretable self-organizing map (iSOM) based visualization method. The iSOM maps high-dimensional data into lower-dimensional space, facilitating visual convenience for the DM. The proposed method requires a finite-size representation of non-dominated solutions near the complete Pareto Front to generate iSOM plots of objectives. We also propose visualizing the metrics such as closeness to constraint boundaries, trade-off value, and robustness using iSOM to check the quality of the preferred solutions, which is rarely considered in existing MCDM approaches. iSOM plots of objective functions, closeness to constraint, trade-off, and robustness metrics assist the DM in effectively choosing new reference direction and step size in each iteration to arrive at the most preferred solution. The proposed iSOM-enabled Pareto-Race approach is demonstrated on benchmark analytical and real-world engineering examples with 3 to 10 objectives.},
  archive      = {J_ASOC},
  author       = {Deepanshu Yadav and Palaniappan Ramu and Kalyanmoy Deb},
  doi          = {10.1016/j.asoc.2023.111032},
  journal      = {Applied Soft Computing},
  pages        = {111032},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interpretable self-organizing map assisted interactive multi-criteria decision-making following pareto-race},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty aware neural network from similarity and
sensitivity. <em>ASOC</em>, <em>149</em>, 111027. (<a
href="https://doi.org/10.1016/j.asoc.2023.111027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent uncertainty quantification approaches lack transparency. Algorithms often perform poorly in an input domain and the reason for poor performance remains unknown. Therefore, we present a neural network training method that considers similar samples with sensitivity awareness in this paper. In the proposed NN training method for UQ, first, we train a shallow NN for the point prediction. Then, we train another NN to predict absolute differences between targets and predictions. In the next step, we select each sample in the training set one by one and compute both prediction and error sensitivities. Then we select similar samples with sensitivity consideration and save indexes of similar samples. The ranges of an input parameter become narrower when the output is highly sensitive to that parameter. After that, we construct initial uncertainty bounds (UB) by considering the distribution of sensitivity-aware similar samples. Prediction intervals (PIs) from initial uncertainty bounds are larger and cover more samples than required. Therefore, we train bound correction NN. As following all the steps for finding UB for each sample requires a lot of computation and memory access, we train a UB computation NN. The UB computation NN takes an input sample and provides an uncertainty bound. The UB computation NN is the final product of the proposed approach. We have achieved superior performance in most situations through the proposed transparent method. Scripts of the proposed method are available in the following GitHub repository: https://github.com/dipuk0506/UQ},
  archive      = {J_ASOC},
  author       = {H.M. Dipu Kabir and Subrota Kumar Mondal and Sadia Khanam and Abbas Khosravi and Shafin Rahman and Mohammad Reza Chalak Qazani and Roohallah Alizadehsani and Houshyar Asadi and Shady Mohamed and Saeid Nahavandi and U. Rajendra Acharya},
  doi          = {10.1016/j.asoc.2023.111027},
  journal      = {Applied Soft Computing},
  pages        = {111027},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty aware neural network from similarity and sensitivity},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated differential evolution algorithm for
reconfigurable manufacturing systems. <em>ASOC</em>, <em>149</em>,
111025. (<a href="https://doi.org/10.1016/j.asoc.2023.111025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product platforms consider an effective strategy to empower mass customization (MC) in a reconfigurable manufacturing system (RMS) to have the capacity to reconfigure the hardware and manage resources at all organisational and functional levels. This enables quick modification of production functions in reaction to unforeseen changes in the market or regulatory requirements. Although several platform formation and assembly line techniques have emerged to satisfy the increased demand for MC and enable the manufacture of products with high variety, they also increase the system’s complexity and make it less cost-efficient as variety increases. This study aims to minimize the complexity of a production system by reducing the number of separate components in different production processes. An efficient approach that integrates the Median-Joining Phylogenetic Networks (MJPN) classification tool and differential evolution (DE) in one unified framework is developed to do that. The proposed framework uses the concept of the package to group separate components that are often employed jointly to produce particular products and allows the use of the ‘all-unit discount policy’, which applies discounts to the purchasing costs of a specific quantity of materials/components. Finally, two case studies are applied to validate the proposed MJPN-DE in different dimensions. The experimental results show that applying the proposed package model saves 57.5\% of the purchasing costs of components and 41.35\% average setup costs compared to MJPN- and non-platform-based models and increases the postponement effectiveness of the obtained layout by 41.51\% and 74.11\% compared with two other layout strategies . In algorithmic comparisons, the results of comparing MJPN-DE with standard DE and genetic algorithm (GA) show an enhanced performance of MJPN-DE where the total production cost is reduced in the small case study by 63.34\% and 38.52\%, and in the large case study by 16.90\% and 11.51\%, respectively.},
  archive      = {J_ASOC},
  author       = {Ismail M. Ali and Sumana Biswas and Hasan H. Turan and Ripon K. Chakrabortty and Sondoss Elsawah and Michael J. Ryan},
  doi          = {10.1016/j.asoc.2023.111025},
  journal      = {Applied Soft Computing},
  pages        = {111025},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated differential evolution algorithm for reconfigurable manufacturing systems},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Circular economy oriented future building information
processing: PSO for CNN approach. <em>ASOC</em>, <em>149</em>, 111013.
(<a href="https://doi.org/10.1016/j.asoc.2023.111013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the acceleration of urbanization, resource depletion is becoming increasingly serious. The innovative strategy of resource utilization is of great significance to the development of new green building. Therefore, this paper combines artificial intelligence technology and deeply excavates the basic information of the old building images collected by the survey. It determines the specific future building construction information from the direction of sustainable development. Based on the traditional convolutional neural network U-Net model, this paper improves its shortcomings, proposes an improved IU-Net model and optimizes the full convolutional neural network structure based on Particle Swarm Optimization (PSO). This paper analyzes and designs the future building information system platform oriented to circular economy, and it compares the improved IU-Net model with other models through experiments to obtain the accuracy and other indicators. The experimental results indicate that the system using the IU-Net model of full convolutional neural network based on PSO has higher prediction accuracy. It has better functionality for the analysis and decision-making of future building.},
  archive      = {J_ASOC},
  author       = {Xiao Chen and Zhi Li and J. Dinesh Peter and Adam Slowik},
  doi          = {10.1016/j.asoc.2023.111013},
  journal      = {Applied Soft Computing},
  pages        = {111013},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Circular economy oriented future building information processing: PSO for CNN approach},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault detection with confidence level evaluation for
perception module of autonomous vehicles based on long short term memory
and gaussian mixture model. <em>ASOC</em>, <em>149</em>, 111010. (<a
href="https://doi.org/10.1016/j.asoc.2023.111010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliability of perception is crucial for developing higher levels of autonomy to enhance the safety and performance of fully autonomous vehicles. As a result, fault detection in the perception module is an essential function for autonomous vehicles. While many approaches rely on hardware redundancy , necessitating additional sensors, achieving analytical redundancy in perception is a pivotal factor in reducing the cost and complexity of the autonomous system . This paper utilizes a motion predictor for surrounding vehicles and a Gaussian Mixture Model (GMM) to evaluate the confidence level of the perception module without requiring additional sensor installations. The motion predictor is designed based on a long short-term memory-based recurrent neural network . The error between the motion prediction and detection results is leveraged to estimate the confidence level of the perception module. In essence, the motion prediction results are treated as redundant sensor measurements. The error distribution is modeled using a GMM, and the cumulative probability density of the GMM is employed to assess the confidence level. The proposed algorithm&#39;s effectiveness is evaluated through a driving data-based simulation study with fault injection . This study shows an enhanced sensitivity to injected faults and a reduced time for fault detection. Furthermore, the proposed algorithm provides a quantitative estimate of the performance degradation level of the perception module. This estimation can serve as an indicator of uncertainty that the motion planner should account for.},
  archive      = {J_ASOC},
  author       = {Yonghwan Jeong},
  doi          = {10.1016/j.asoc.2023.111010},
  journal      = {Applied Soft Computing},
  pages        = {111010},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault detection with confidence level evaluation for perception module of autonomous vehicles based on long short term memory and gaussian mixture model},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auction-based deep learning-driven smart agricultural supply
chain mechanism. <em>ASOC</em>, <em>149</em>, 111009. (<a
href="https://doi.org/10.1016/j.asoc.2023.111009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agricultural product prices are subject to significant fluctuations due to variations in sales cycles, impacting people&#39;s quality of life and farmers&#39; income while potentially giving rise to social problems. To address this issue in the smart agriculture supply chain, advancements in technologies like big data and artificial intelligence have made it feasible to predict agricultural product demand and price trends. Among the research methods, deep learning , being highly reliant on data and possessing multi-layered implicit information, has emerged as the most popular research method, exhibiting better data representation and prediction capabilities. In this study, we propose an end-to-end agricultural product price prediction model that combines complementary ensemble empirical mode decomposition (CEEMD) with long short-term memory (LSTM). The prediction generated by this model will provide guidance throughout various supply chain links, including production, harvesting, storage, and logistics. Moreover, the study aims to develop auction mechanisms based on the prediction results to achieve an optimal allocation of agricultural products. To address issues of low computational efficiency and limited benefit distribution in the auction process, this study introduces a deep learning-based iterative bilateral auction algorithm (DL-IDA), which improves upon existing methodologies by leveraging the power of deep learning . Empirical analysis is conducted using the daily price of Fuji apples at the Beijing Xinfadi market. The results demonstrate that the CEEMD-LSTM model proposed in this study exhibits superior performance in predicting agricultural product prices. Furthermore, experimental findings validate the effectiveness and superiority of DL-IDA. Compared to existing iterative bilateral auction algorithms, DL-IDA offers better performance in terms of running time, social welfare, buyer benefits, seller benefits, and broker benefits. Consequently, it can contribute to a transparent pricing mechanism for agricultural products.},
  archive      = {J_ASOC},
  author       = {Yu Feng and Dong Mei and Hua Zhao},
  doi          = {10.1016/j.asoc.2023.111009},
  journal      = {Applied Soft Computing},
  pages        = {111009},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Auction-based deep learning-driven smart agricultural supply chain mechanism},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feedforward neural network-based augmented salp swarm
optimizer for accurate software development cost forecasting.
<em>ASOC</em>, <em>149</em>, 111008. (<a
href="https://doi.org/10.1016/j.asoc.2023.111008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research proposes the use of feed-forward backpropagation neural networks (FFNN) to develop an accurate cost forecasting model in light of the challenges associated with forecasting software development costs (FSDC). The salp swarm algorithm (SSA) is first augmented and then employed to optimize the parameters of the developed FFNN predictor. A search enhancement mechanism and an elitism technique have been developed and incorporated into the SSA optimization process as two fresh and effective techniques for this goal. The search enhancement mechanism is employed to keep up a high rate of global exploration while also driving convergence towards the optimal area. Whereas elitism is used throughout the research phase to prevent stagnation in the local optima. Nineteen benchmark test functions and twelve benchmark software development cost forecasting data sets are utilized to assess the performance of the recommended enhancement techniques and developed algorithms. The results obtained from experiments show the superiority of the proposed techniques. In addition, the developed technique has been compared with many state-of-the-art methods, which demonstrates its durability. This is in addition to the statistical validation carried out on the results obtained, which also supports the robustness of the proposed technique.},
  archive      = {J_ASOC},
  author       = {Mohammed Azmi Al-Betar and Sofian Kassaymeh and Sharif Naser Makhadmeh and Salam Fraihat and Salwani Abdullah},
  doi          = {10.1016/j.asoc.2023.111008},
  journal      = {Applied Soft Computing},
  pages        = {111008},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feedforward neural network-based augmented salp swarm optimizer for accurate software development cost forecasting},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid electricity load prediction system based on
weighted fuzzy time series and multi-objective differential evolution.
<em>ASOC</em>, <em>149</em>, 111007. (<a
href="https://doi.org/10.1016/j.asoc.2023.111007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in science and technology, the demand for electricity is increasing dramatically. Consequently, reliable short-term power load prediction is critical to ensure the safety, efficiency, and cost-effectiveness of electricity grids. However, owing to the volatility and randomness of power load time series, traditional forecasting models have struggled to meet the current power system requirements for load prediction precision and stability. In light of this, this study proposes a new hybrid short-term electricity load prediction system that combines the variational modal decomposition (VMD) technique, multi-objective differential evolutionary algorithm (MODE), and weighted fuzzy time series (WFTS) prediction model to increase load prediction precision and stability. Four Belgian datasets (spring, summer, autumn, and winter) were used for short-term power load prediction experiments and were compared with prevalent prediction methods. The experimental results indicate that the designed system obtains MAPE S p r i n g = 0 . 6670\% MAPESpring=0.6670\% , MAPE S u m m e r = 0 . 5835\% MAPESummer=0.5835\% , MAPE A u t u m n = 0 . 7049\% MAPEAutumn=0.7049\% , MAPE W i n t e r = 0 . 7123\% MAPEWinter=0.7123\% and STD S p r i n g = 60 . 2575 STDSpring=60.2575 , STD S p r i n g = 62 . 6573 STDSpring=62.6573 , STD S p r i n g = 72 . 7393 STDSpring=72.7393 , STD S p r i n g = 71 . 5549 STDSpring=71.5549 , which are obviously superior to the comparison models in terms of prediction accuracy and stability.},
  archive      = {J_ASOC},
  author       = {Zhining Cao and Jianzhou Wang and Li Yin and Danxiang Wei and Yiyao Xiao},
  doi          = {10.1016/j.asoc.2023.111007},
  journal      = {Applied Soft Computing},
  pages        = {111007},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid electricity load prediction system based on weighted fuzzy time series and multi-objective differential evolution},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-objective decomposition evolutionary algorithm with
objective modification-based dominance and external archive.
<em>ASOC</em>, <em>149</em>, 111006. (<a
href="https://doi.org/10.1016/j.asoc.2023.111006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, the multi-objective optimization problem (MOP) is typically challenging in two aspects. On the one hand, its Pareto front has imbalanced search difficulties; on the other hand, its search space contains many dominance resistant solutions (DRSs). Decomposing a complicated MOP into several simple MOPs for collaborative optimization (M2M) has been acknowledged to be efficient in coping with the imbalanced search difficulty. Nevertheless, the convergence efficiency of the M2M-based multi-objective evolutionary algorithm (MOEA) is rarely investigated, especially on the MOP with DRSs. This paper reveals two convergence challenges faced by M2M-based MOEAs. Subsequently, a variant called MOEA/D-OMDEA is proposed to achieve better convergence efficiency without sacrificing advantages in diversity preservation. MOEA/D-OMDEA integrates a new relaxed dominance criterion, namely the OM-dominance criterion, into its environmental selection to alleviate the negative influence of inferior solutions (e.g., DRSs) as well as to better balance convergence and diversity. MOEA/D-OMDEA is compared with ten state-of-the-art MOEAs on two sets of MOP benchmarks with different characteristics and a real-world problem. Experimental results indicate that MOEA/D-OMDEA can significantly outperform the other ten competitors on these problems. In addition, this paper provides a thorough analysis of the effectiveness of each new algorithmic component and the sensitivity of each newly-introduced parameter.},
  archive      = {J_ASOC},
  author       = {Zhenkun Wang and Qingyan Li and Genghui Li and Qingfu Zhang},
  doi          = {10.1016/j.asoc.2023.111006},
  journal      = {Applied Soft Computing},
  pages        = {111006},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective decomposition evolutionary algorithm with objective modification-based dominance and external archive},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Digital twin model construction of robot and multi-object
under stacking environment for grasping planning. <em>ASOC</em>,
<em>149</em>, 111005. (<a
href="https://doi.org/10.1016/j.asoc.2023.111005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object grasping is the basic function for robots to complete various tasks. However, it is still a hot and difficult problem to grasp object in multi-object stacking environment. A digital twin model construction method for robot grasping under multi-object stacking environment is proposed based on digital twin technology to realize multi-object grasping planning and virtual-real interaction. The robot digital twin model is constructed based on the requirements of the twin model, which includes the robot information model and information perception method. Then, the logical relationship between the virtual models of multi-object stacked objects is established based on the interactive relationship of stacked objects. The twin model of robot grasping under the multi-object stacking scene is built to realize the monitoring and visualization of the key elements of the physical entity of the robot based on ROS and Unity3D, and the interaction between the virtual model and the physical entity is realized to monitor the progress of robot grasping of multi-target objects. What’s more, the interaction relationship between objects is detected at the same time with an accuracy of 80\%. The experimental results verify the validity and reliability of the proposed method for establishing the digital twin model of robot grasping and the logical relation of the virtual model of objects.},
  archive      = {J_ASOC},
  author       = {Juntong Yun and Gongfa Li and Du Jiang and Manman Xu and Feng Xiang and Li Huang and Guozhang Jiang and Xin Liu and Yuanmin Xie and Bo Tao and Zifan Fang},
  doi          = {10.1016/j.asoc.2023.111005},
  journal      = {Applied Soft Computing},
  pages        = {111005},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Digital twin model construction of robot and multi-object under stacking environment for grasping planning},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Volleyball premier league algorithm with ACO and ALNS for
simultaneous pickup–delivery location routing problem. <em>ASOC</em>,
<em>149</em>, 111004. (<a
href="https://doi.org/10.1016/j.asoc.2023.111004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reverse logistics problem, which involves customer returns and incurs high additional logistics costs, has become a topic of increasing interest. This problem is modeled as the location routing problem with simultaneous pickup and delivery under time constraints (LRPSPDTW), which is a more complex version of the location routing problem (LRP). Previous studies have shown that existing algorithms are unable to provide satisfactory solution accuracy for this problem. In order to address this, a novel algorithm called the hybrid volleyball premier league (HVPL) algorithm is proposed. Drawing inspiration from the competition mechanism of the original volleyball premier league (VPL) algorithm, the HVPL algorithm incorporates adaptive large neighborhood search (ALNS) and improved ant colony optimization (ACO) as sub-algorithms, allowing them to work together synergistically and strike a balance between exploration and exploitation. The learning and elimination phases of the VPL algorithm have been redesigned to enhance convergence speed. Furthermore, the Bellman’s algorithm has been enhanced to determine depot locations and construct vehicle paths for the solution. Prior to solving the LRPSPDTW problem, benchmark testing was conducted using 35 latest benchmark functions . Experimental results and Friedman mean rank test demonstrated that HVPL exhibits superior exploration and exploitation capabilities. HVPL was subsequently tested on 66 instances based on the modified Solomon’s benchmark and the NEO research group benchmarks, and the results showcased its outperformance over other methods in the majority of cases.},
  archive      = {J_ASOC},
  author       = {Shuo Sun and Liang Ma and Yong Liu and Chunjian Shang},
  doi          = {10.1016/j.asoc.2023.111004},
  journal      = {Applied Soft Computing},
  pages        = {111004},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Volleyball premier league algorithm with ACO and ALNS for simultaneous pickup–delivery location routing problem},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A heterogeneous fuzzy collaborative intelligence approach:
Air quality monitor selection study. <em>ASOC</em>, <em>149</em>,
111000. (<a href="https://doi.org/10.1016/j.asoc.2023.111000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart backpacks with novel functions have an enormous potential market. Therefore, the ability to embed smart functions (e.g., air quality detection) into a backpack design is critical. This study aimed to select an air quality monitor suitable for the design of a smart backpack. Accordingly, a heterogeneous fuzzy collaborative intelligence approach is proposed. In the proposed methodology, experts apply heterogeneous methods to derive the fuzzy priorities of the criteria and evaluate the suitability of an air quality monitor. The evaluation results by all experts are aggregated using fuzzy-weighted intersection (FWI). After defuzzification , the top-performing air quality monitor is chosen. Based on the experimental results, the most critical features of an air quality monitor are price and size. Furthermore, not all air quality monitors are suitable for a smart backpack design. Among the compared air quality monitors, OO-KFR-PMA was the optimal choice. The contribution of this research is twofold. First, experts are enabled to apply heterogeneous fuzzy multi-criteria decision-making methods. Second, the unequal authority levels of experts are considered by aggregating their evaluation results using FWI.},
  archive      = {J_ASOC},
  author       = {Tin-Chih Toly Chen and Yu-Cheng Lin and Yu-Cheng Wang},
  doi          = {10.1016/j.asoc.2023.111000},
  journal      = {Applied Soft Computing},
  pages        = {111000},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A heterogeneous fuzzy collaborative intelligence approach: Air quality monitor selection study},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent target search strategy optimization:
Hierarchical reinforcement learning with multi-criteria negative
feedback. <em>ASOC</em>, <em>149</em>, 110999. (<a
href="https://doi.org/10.1016/j.asoc.2023.110999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, using unmanned platforms to perform target search tasks has received extensive attention and research. The complexity of the search scenario and the unpredictability of the target movement pose significant challenges for unmanned platforms to perform the search task. Developing efficient search strategies is crucial for their success. In this study, we model the problem as a Partially Observable Markov Decision Process (POMDP) and propose a Hierarchical Deep Q Network with Multi-criteria Negative Feedback method named MNF-HDQN to solve the problem efficiently. The MNF-HDQN incorporates point and area evaluations instead of the original sparsity calculation to enhance the cooperation competence of unmanned platforms in searching for a target in various search tasks. Additionally, the integration of convex polygon theory into reward shaping and the design of a new two-stage search strategy encouragement function further improve the performance of the proposed method. We conduct detailed experiments to verify the superiority of the MNF-HDQN. And experimental results show that our method provides a high successful search rate in a shorter timestep compared with state-of-the-art baselines. This advantage is more evident when the search scenario is more complex or the target movement is more unpredictable.},
  archive      = {J_ASOC},
  author       = {Xin Cao and He Luo and Jianwei Tai and Ruhao Jiang and Guoqiang Wang},
  doi          = {10.1016/j.asoc.2023.110999},
  journal      = {Applied Soft Computing},
  pages        = {110999},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-agent target search strategy optimization: Hierarchical reinforcement learning with multi-criteria negative feedback},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization of time based fuzzy multi-objective reliability
redundancy allocation problem for xj−out−of−m system using tuning and
neighborhood based fuzzy MOPSO algorithm. <em>ASOC</em>, <em>149</em>,
110998. (<a href="https://doi.org/10.1016/j.asoc.2023.110998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a time based fuzzy multi objective reliability redundancy allocation problem (FMORRAP) is proposed for the x j − o u t − o f − m xj−out−of−m series–parallel system. The main objective is to maximize the system reliability with minimization of system cost and system repairing cost by optimizing the number of redundant components at each stage. The objectives are achieved by satisfying entropy constraints with limited redundant components at each stage and same for the whole system. Here the uncertainty of reliability, cost and repairing cost of each component is maintained by using triangular fuzzy number (TFN). The decreasing factor of component reliability and cost follows the change in the length of radius for the inverse logarithmic spiral with respect to time. Similarly the increasing factor of component repairing cost follows the same for the logarithmic spiral. Tuning and Neighborhood based Fuzzy Multi-Objective Particle Swarm Optimization (TNF-MOPSO) algorithm is proposed to solve fuzzy multi-objective optimization problems (MOOP). The proposed problem and proposed algorithm are illustrated by using a bench mark problem. The proposed algorithm produced better membership of objective functions for most of the time values and high satisfaction level of reliability for all values of the time parameter. It also outperforms the standard approaches MOPSO and NF-MOPSO.},
  archive      = {J_ASOC},
  author       = {Satyajit De and Payel Rakshit and Anil Bikash Chowdhury},
  doi          = {10.1016/j.asoc.2023.110998},
  journal      = {Applied Soft Computing},
  pages        = {110998},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of time based fuzzy multi-objective reliability redundancy allocation problem for xj−out−of−m system using tuning and neighborhood based fuzzy MOPSO algorithm},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards white box modeling of compressive strength of
sustainable ternary cement concrete using explainable artificial
intelligence (XAI). <em>ASOC</em>, <em>149</em>, 110997. (<a
href="https://doi.org/10.1016/j.asoc.2023.110997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the production of sustainable ternary cement concrete (TCC) involves a large range of constituents which can affect the compressive strength (CS) of TCC in different ways, the evaluation of CS in a unified manner is necessary. Unlike the black box machine learning (ML) models, the predictions of CS of TCC using local explanations to global understanding with explainable artificial intelligence (XAI) models have been investigated in a systematic approach by means of SHapley Additive exPlanations (SHAP). For this, two conventional ML and three ensemble ML models were used. Hyper-parameter tuning was also carried out. The ensemble ML models had a higher accuracy than the conventional ML models . Taylor diagram, model evaluation parameters and the SHAP values were used to interpret machine learning model predictivity. From insights obtained through local explanations, it can be concluded that predictions made by black box models should be used carefully.},
  archive      = {J_ASOC},
  author       = {Syed Muhammad Ibrahim and Saad Shamim Ansari and Syed Danish Hasan},
  doi          = {10.1016/j.asoc.2023.110997},
  journal      = {Applied Soft Computing},
  pages        = {110997},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards white box modeling of compressive strength of sustainable ternary cement concrete using explainable artificial intelligence (XAI)},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stealthy dynamic backdoor attack against neural networks for
image classification. <em>ASOC</em>, <em>149</em>, 110993. (<a
href="https://doi.org/10.1016/j.asoc.2023.110993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, which are external entity model providers that can inject backdoors into the network for illegal purposes. Current attack methods rely on manipulated images with embedded triggers to infiltrate carrier networks, but they suffer from detectable distortions and limited integration into neural networks . To delve further into the potential vulnerabilities of these models, this study introduces an innovative backdoor attack strategy, leveraging deep learning steganography through a Generative Adversarial Network (GAN). Our approach utilizes steganography to create manipulated images, capitalizing on the unique sensitivity of neural networks to minute perturbations. The network is then trained de novo with these manipulated images, creating a backdoor-infused model. Fundamentally, our method harnesses the pronounced sensitivity of DNNs to nuanced alterations. Experimental outcomes substantiate that our backdoor can be effectively integrated into the models, yielding high attack success rates. Notably, it also adeptly circumvents both contemporary state-of-the-art defense mechanisms and human inspection. Our source code is publicly accessible at https://github.com/DLAIResearch/NNSDB .},
  archive      = {J_ASOC},
  author       = {Liang Dong and Jiawei Qiu and Zhongwang Fu and Leiyang Chen and Xiaohui Cui and Zhidong Shen},
  doi          = {10.1016/j.asoc.2023.110993},
  journal      = {Applied Soft Computing},
  pages        = {110993},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stealthy dynamic backdoor attack against neural networks for image classification},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing convolutional neural network by utilizing
nematode connectome: A brain-inspired method. <em>ASOC</em>,
<em>149</em>, 110992. (<a
href="https://doi.org/10.1016/j.asoc.2023.110992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have achieved impressive results in areas such as computer vision tasks. Recently, more complex architectures have been designed that add additional operations and connections to the standard architecture to enable deeper network training. A large number of trainable parameters are often required to acquire accurate results. As the number of parameters increases, the network structure becomes more intricate and complex, resulting in low model accuracy and wasted computational resources. Motivated by the promising performance of brain-inspired neural computation principles, a new network structure can be constructed using a composition of the biological connectome. The nematode Caenorhabditis elegans (C. elegans), as the only organism whose connectome has been fully mapped, is suitable to be selected for the construction of neural network structures . In this paper, the neuron connectome of C. elegans is encapsulated as a connectome block and used as the connection pattern of the convolutional layer in the convolutional network . Therefore, this modified convolutional neural network structure is called the nematode connectome neural network (NCNN), and the detailed implementation is presented. Moreover, comparative experiments on the CIFAR-100, CIFAR-10, MNIST, and ImageNet datasets are conducted to fully demonstrate the effectiveness and feasibility of the proposed NCNN model on image classification . In addition, the success of the NCNN also provides new ideas for future network structure design. The source code is publicly available at https://github.com/LongJin-lab/Nematode-Connectome-Neural-Network .},
  archive      = {J_ASOC},
  author       = {Dan Su and Liangming Chen and Xiaohao Du and Mei Liu and Long Jin},
  doi          = {10.1016/j.asoc.2023.110992},
  journal      = {Applied Soft Computing},
  pages        = {110992},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constructing convolutional neural network by utilizing nematode connectome: A brain-inspired method},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-directional learning particle swarm optimization for
large-scale optimization. <em>ASOC</em>, <em>149</em>, 110990. (<a
href="https://doi.org/10.1016/j.asoc.2023.110990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale optimization problems (LSOPs) are an essential research topic in the evolutionary computation (EC) community with two challenges: slow convergence in the huge search space and the trap in massive locally optimal solutions. To tackle these two challenges, this paper proposes a bi-directional learning particle swarm optimization (BLPSO) with two learning strategies, called diversity learning strategy (DLS) and convergence learning strategy (CLS). In DLS, we first propose a diversity evaluation mechanism based on locally sensitive hashing (LSH) to measure the diversity of individuals. Then the density individuals will learn from other dispersed individuals with good diversity to enhance the diversity of the population. In CLS, the inferior individuals will learn from other superior individuals with excellent evolution information to help the population accelerate convergence speed. These two learning strategies act in different roles and complement each other. With these two learning strategies, BLPSO achieves a balance between sufficient diversity and fast convergence in solving LSOPs. Two large-scale test suites, IEEE CEC2010 and IEEE CEC2013, are used to test the performance between BLPSO and other state-of-the-art algorithms. The experimental results show that BLPSO outperforms other algorithms on both test suites, including the winner algorithms of the IEEE CEC2010 and IEEE CEC2012 competitions. Finally, BLPSO is applied to a large-scale portfolio optimization problem to illustrate its application capability.},
  archive      = {J_ASOC},
  author       = {Shuai Liu and Zi-Jia Wang and Yuan-Gen Wang and Sam Kwong and Jun Zhang},
  doi          = {10.1016/j.asoc.2023.110990},
  journal      = {Applied Soft Computing},
  pages        = {110990},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-directional learning particle swarm optimization for large-scale optimization},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Corrigendum to “a decision modeling approach for smart
e-tourism data management applications based on spherical fuzzy rough
environment” [applied soft computing 143(2023) 110297]. <em>ASOC</em>,
<em>149</em>, 110989. (<a
href="https://doi.org/10.1016/j.asoc.2023.110989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {R.T. Mohammed and A.H. Alamoodi and O.S. Albahri and A.A. Zaidan and H.A. AlSattar and Uwe Aickelin and A.S. Albahri and B.B. Zaidan and Amelia Ritahani Ismail and R.Q. Malik},
  doi          = {10.1016/j.asoc.2023.110989},
  journal      = {Applied Soft Computing},
  pages        = {110989},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “A decision modeling approach for smart e-tourism data management applications based on spherical fuzzy rough environment” [Applied soft computing 143(2023) 110297]},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effectiveness of neural networks and transfer learning to
forecast photovoltaic power production. <em>ASOC</em>, <em>149</em>,
110988. (<a href="https://doi.org/10.1016/j.asoc.2023.110988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Neural Networks (ANNs) can successfully be integrated into smart models for energy prediction, but require large datasets for training. This investigation presents an innovative methodology for photovoltaic power generation forecasting with ANNs, when only a limited amount of real data is available, and has been tested and validated on a real-life photovoltaic installation. Feature selection identifies which meteorological features most impact photovoltaic power generation. A simulator, which accurately models a real photovoltaic installation, is used to create an artificial, but accurate and realistic, dataset of power generation large enough to effectively train and test different ANNs. These are then exploited on a portion of real, but limited, dataset of power generated by the real photovoltaic installation on which the simulator is modeled. Finally, different transfer learning techniques are used to tune the ANN models with the remaining portion of the real, but limited, dataset of photovoltaic power generation.},
  archive      = {J_ASOC},
  author       = {Andrea Bellagarda and Donato Grassi and Alessandro Aliberti and Lorenzo Bottaccioli and Alberto Macii and Edoardo Patti},
  doi          = {10.1016/j.asoc.2023.110988},
  journal      = {Applied Soft Computing},
  pages        = {110988},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effectiveness of neural networks and transfer learning to forecast photovoltaic power production},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor shape search for efficient compression of tensorized
data and neural networks. <em>ASOC</em>, <em>149</em>, 110987. (<a
href="https://doi.org/10.1016/j.asoc.2023.110987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressing big data and model parameters via tensor decomposition such as the tensor train (TT) format has gained great success in recent years. The application of tensor compression methods requires the data be high dimensional. However, not all the real-world data primarily are high-dimensional, and sometimes reshaping is necessary before the application of tensor compression methods. Meantime, reordering and reshaping data may affect the efficiency of the compression. This work utilizes tensor reshaping to improve the efficiency of tensor compression using the TT format. An optimization model is proposed that maximizes the space-saving of tensor compression with respect to the shape of a given tensor while the compression error is bounded. The study is narrowed down to the TT decomposition and the TT-SVD algorithm is linked with a genetic algorithm (GA) to find an optimal tensor shape. The proposed method is applied to compress RGB images and a neural network to exemplify its capability. The results of the proposed tensor shape search using the GA are also compared with a purely random search. The results demonstrate that the proposed tensor shape search method significantly improves the space-saving and compression ratio of the data compression and enhances the efficiency of tensorized neural networks using the TT decomposition.},
  archive      = {J_ASOC},
  author       = {Ryan Solgi and Zichang He and William Jiahua Liang and Zheng Zhang and Hugo A. Loaiciga},
  doi          = {10.1016/j.asoc.2023.110987},
  journal      = {Applied Soft Computing},
  pages        = {110987},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tensor shape search for efficient compression of tensorized data and neural networks},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imbalanced node classification with graph neural networks: A
unified approach leveraging homophily and label information.
<em>ASOC</em>, <em>149</em>, 110985. (<a
href="https://doi.org/10.1016/j.asoc.2023.110985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The homophily assumption in graph theory posits that nodes with similar characteristics have a higher tendency to form connections. This principle has rendered Graph Neural Networks (GNNs) as vital tools for graph representation learning . However, many real-world graphs may exhibit a phenomenon often termed as neighbor class imbalance, which is characterized by frequent connections between dissimilar nodes, a scenario reflecting low homophily. Classical GNNs tend to overlook this issue, leading to a significant decline in performance. Prior research has attempted to address this challenge by employing high-order neighborhoods and filtering out dissimilar neighbors, yet they have paid little attention to homophily degree estimation and label utilization. In this work, we initially explore the performance of classical GNNs on a synthetic graph with varying homophily degrees, designated as SynG-N. Following this, we introduce a novel method, HLA-GNN, which integrates homophily degree estimation and label utilization to enhance classical GNNs. The degrees of homophily between node pairs are estimated using a limited set of ground-truth labels, which can be integrated into classic GNNs to guide the message aggregation process. Drawing on the label propagation algorithm , we combine the partially observed class labels to enhance the original feature space. Here, the observed class labels are randomly masked as a feature augmentation and training signal. Our experimental results on eight datasets with varying degrees of homophily underscore the effectiveness of our method. HLA-GNN achieves a 12.69\% ∼ ∼ 34.19\% improvement on low-homophily graphs, while maintaining competitive results in homophilous settings.},
  archive      = {J_ASOC},
  author       = {Dingyang Lv and Zhengjia Xu and Jinghui Zhang and Yuchen Wang and Fang Dong},
  doi          = {10.1016/j.asoc.2023.110985},
  journal      = {Applied Soft Computing},
  pages        = {110985},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Imbalanced node classification with graph neural networks: A unified approach leveraging homophily and label information},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Financial transaction fraud detector based on imbalance
learning and graph neural network. <em>ASOC</em>, <em>149</em>, 110984.
(<a href="https://doi.org/10.1016/j.asoc.2023.110984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection is a vital and challenging task in the financial domain. Traditional machine learning approaches have limitations in dealing with the high imbalance and complexity of transaction data and the stealthy behavior of fraudulent entities. To address these issues, we propose HHLN-GNN, a graph neural network-based fraud detection model that leverages subgraph generators and neighborhood samplers to handle category imbalance. Furthermore, the model employs a self-attentive module and differentiates between homogeneous and heterogeneous connections to reduce the artificiality of fraudulent nodes and exploit the hidden information of transaction data more fully. We conduct experiments on three real-world public benchmark datasets, YelpChi, Amazon and Elliptic, and demonstrate that our model significantly outperforms existing benchmark methods on various performance metrics, such as F1-macro, AUC and GMean, by 10\%, 12.5\%, and 17.3\% on YelpChi, 0.7\%, 2.5\% and 4\% on Amazon, respectively.},
  archive      = {J_ASOC},
  author       = {Guoxiang Tong and Jieyu Shen},
  doi          = {10.1016/j.asoc.2023.110984},
  journal      = {Applied Soft Computing},
  pages        = {110984},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Financial transaction fraud detector based on imbalance learning and graph neural network},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimization framework with improved auction-based
initialization for highly constrained on-orbit servicing mission
planning. <em>ASOC</em>, <em>149</em>, 110983. (<a
href="https://doi.org/10.1016/j.asoc.2023.110983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The on-orbit servicing (OOS) mission planning problem for multiple geosynchronous earth orbit (GSO) satellites is a critical issue due to its huge economic value. However, given the complexity of the space scenario and the subjective nature of the formulation process, the problem is highly constrained in actual use, and the performance of conventional optimization algorithms decreases due to fuel or mission deadline constraints. With the goal of enhancing solution quality and speeding up the solution process, this paper proposes a novel optimization framework. The problem is decomposed into three sub-problems and then solved using the proposed three-level algorithm to lessen its complexity. In addition, a new initialization method is embedded to improve global search capabilities. Notably, considering the high computation efficiency of the auction algorithm (AA), an improved version is designed to generate primary solutions, which introduces the grouping and reallocation mechanism. Then, several variations of the primary solutions are used as the initial variables in the optimization. Numerical simulations demonstrate that the proposed methodology has higher computational efficiency and convergence performance than conventional strategies.},
  archive      = {J_ASOC},
  author       = {Hang Xu and Bin Song and Yanning Guo and Lujiang Liu and Xinglong Li and Guangfu Ma},
  doi          = {10.1016/j.asoc.2023.110983},
  journal      = {Applied Soft Computing},
  pages        = {110983},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimization framework with improved auction-based initialization for highly constrained on-orbit servicing mission planning},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Interpretable hierarchical error correction GRU model for
effective observation selection. <em>ASOC</em>, <em>149</em>, 110982.
(<a href="https://doi.org/10.1016/j.asoc.2023.110982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretability of extended target effective observation selection model in artificial intelligence (AI) symbolizes a severe challenge: both accuracy effective observation selection while ensuring real-time performance with an extended target multiple observation model and physical interpretability are vital. To meet the challenge, this paper proposes a novel hierarchical Gated Recurrent Unit (GRU) interpretable model. The contribution of this study lies in filling the research gap in the field of effective observation processing for extended targets and intuitively explaining the spatiotemporal visualization of the model. Compared with other interpretable models, the proposed model has improved data clustering and feature extraction capabilities, noise robustness, and computational resource consumption in the face of complex extended target multivariate physical factors. More importantly, interpretable studies are presented on the spatiotemporal characteristics of the multiple observation model of the extended target. The experimental results in real-world scenarios demonstrate that the proposed model is better than benchmark models in terms of prediction performance. The visualization and interpretation of the integrated predictive model reflects the reasonability of the proposed ensemble model.},
  archive      = {J_ASOC},
  author       = {Chao Zhang and Defu Jiang and Yiyue Gao and Kanghui Jiang},
  doi          = {10.1016/j.asoc.2023.110982},
  journal      = {Applied Soft Computing},
  pages        = {110982},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interpretable hierarchical error correction GRU model for effective observation selection},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint DR-DME grading classification using optimal feature
selection-based deep graph correlation network. <em>ASOC</em>,
<em>149</em>, 110981. (<a
href="https://doi.org/10.1016/j.asoc.2023.110981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic macular edema (DME) and diabetic retinopathy (DR) are the leading causes of human blindness, and accurate grading of individual DME, individual DR, and the joint DR-DME is very important for the diagnosis of human eye diseases. However, the conventional methods failed to separate the features such as disease-specific, disease-dependent, and joint DR-DME, which resulted in poor grading accuracy. In addition, optimal feature selection is also vital in DR-DME grading classification for improving the performance of joint DR-DME grading classification. Therefore, this work focuses on the implementation of an advanced deep graph correlation learning model based on a joint DR-DME network (JDD-Net) for disease detection and classification from color fundus images. Initially, convolutional block attention module (CBAM) and joint disease attention (JDA) modules are combined to extract the DR-specific, DME-specific, and joint DR-DME disease-dependent features. Here, interdependent DR and DME features are separated by a CBAM-based channel-spatial split attention mechanism . In addition, an iterative random forest network (IRF-Net) is used to select the optimal features by adopting fast machine learning properties. Finally, a deep graph correlation network (DGCN) is used to classify the different diseases using a pre-trained model. The simulations conducted on the Indian Diabetic Retinopathy Image Dataset (IDRiD) disclose that the proposed JDD-Net results in improved individual DR, individual DME, and joint DR-DME performance as compared to state-of-the-art approaches with DR, DME, and joint DR-DME accuracy of 99.53\%, 99.1\%, and 99.01\%, respectively.},
  archive      = {J_ASOC},
  author       = {Purna Chandra Reddy V and Kiran Kumar Gurrala},
  doi          = {10.1016/j.asoc.2023.110981},
  journal      = {Applied Soft Computing},
  pages        = {110981},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint DR-DME grading classification using optimal feature selection-based deep graph correlation network},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on multi-granularity sequential three-way decisions
based on the fuzzy t-equivalence relation. <em>ASOC</em>, <em>149</em>,
110980. (<a href="https://doi.org/10.1016/j.asoc.2023.110980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy decision-theoretic rough set (FDTRS) is a data processing method based on the fuzzy theory designed to handle decision problems with uncertainty and fuzziness . It simplifies the complexity of decision-making and provides decision strategies by integrating multiple sources of information. This approach makes the decision process more scientific and rational. The mainstream FDTRS model breaks the over restrictive limits of classical decision-theoretic rough sets, computes the fuzzy similarity relation between two objects utilizing the Gaussian kernel function, which is another major breakthrough and innovation in the field of decision-theoretic rough sets. However, this model suffers from the problems of insufficient decision information, single perspective and high decision-making cost when dealing with dynamic, complex and uncertain problems. Emerging in recent years, sequential three-way decisions not only provides a flexible mechanism to implement the idea of progressive computation, but also deals with dynamic, complex and uncertain problems. To this end, this paper proposes a sequential three-way decision model based on the fuzzy T-equivalence relation (FTE-based sequential three-way decision model) from the perspective of sequential three-way decisions, and gives relevant definitions and examples to effectively solve the problem of insufficient information. Then in order to solve the single-view problem, we extend the sequential three-way decisions to the case of multi-granularity, and propose FTE-based multi-granularity sequential three-way decisions in four aggregation strategies with the corresponding properties and algorithms. In addition, the rationality of models are tentatively analyzed through specific examples. Finally, six sets of UCI standard datasets are used to validate the proposed models, which demonstrate their good performance. These models contribute to the field of fuzzy decision-theoretic rough sets and sequential three-way decisions.},
  archive      = {J_ASOC},
  author       = {Jin Qian and Xing Han and Ying Yu and Caihui Liu and Jiamao Yu},
  doi          = {10.1016/j.asoc.2023.110980},
  journal      = {Applied Soft Computing},
  pages        = {110980},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Research on multi-granularity sequential three-way decisions based on the fuzzy T-equivalence relation},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised domain adaptation methods for photovoltaic
power forecasting. <em>ASOC</em>, <em>149</em>, 110979. (<a
href="https://doi.org/10.1016/j.asoc.2023.110979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate forecasting of photovoltaic (PV) power generation is of great significance in renewable energy systems , as it enables optimal energy management and grid stability. Despite the importance of this issue, substantial limitations still exist in the majority of existing research initiatives, which employ shallow machine learning algorithms . Recently, some studies have proposed employing convolutional and long short-term memory neural networks (LSTMs) in conjunction with transfer learning techniques; however, these approaches require that the production of PV systems is known during training. To overcome these limitations, we present the first study in the task of PV power forecasting utilizing unsupervised domain adaptation methods. Specifically, we employ two unsupervised methods , namely Domain Adversarial Neural Network and Margin Disparity Discrepancy. Both approaches use a source and a target domain during training, where the target labels of the target domain are unknown during training. We use production and weather data from seven PV systems with nominal capacities ranging from 23.52 kW to 271.53 kW, located in different areas. The findings demonstrate that our proposed architectures improve root mean squared error (RMSE), normalized RMSE, and R 2 R2 scores over the smart persistence model across all the PV systems used for testing. Furthermore, our approaches improve the performance of the smart persistence model, with a forecast skill index reaching up to 45.35\%. Our extensive experiments demonstrate that our introduced approaches offer valuable advantages over state-of-the-art ones, as the target variable of the target domain is unknown during training. We also demonstrate the robustness of our approaches by conducting a series of ablation experiments.},
  archive      = {J_ASOC},
  author       = {Loukas Ilias and Elissaios Sarmas and Vangelis Marinakis and Dimitris Askounis and Haris Doukas},
  doi          = {10.1016/j.asoc.2023.110979},
  journal      = {Applied Soft Computing},
  pages        = {110979},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised domain adaptation methods for photovoltaic power forecasting},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sequential three-way classification model based on risk
preference and decision correction. <em>ASOC</em>, <em>149</em>, 110978.
(<a href="https://doi.org/10.1016/j.asoc.2023.110978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sequential three-way decision (S3WD) model, which merges three-way decisions and granular computing , is increasingly crucial in classification. The risk attitude to the decision process and result costs affects the decisive actions in the S3WD model. Furthermore, decision conflict arises when there is a discrepancy between coarse-grained and fine-grained definite decision-making for the same object, which can potentially impact decision accuracy. However, current studies show incomplete risk preference research and a lack of decision correction strategies to address decision conflict. To address the limitation, four sequential three-way classifiers (S3WCs) are proposed. First, three prominent distance functions are employed to compute similarity classes for condition probability estimation . Second, optimistic, pessimistic, and weighted compromise sequential three-way classifiers are established to reflect the risk preference for the two types of costs. Third, four precision differences in the S3WCs are defined from local and global perspectives. An S3WC with decision correction is presented to improve precision by judging precision differences in adjacent granularity levels and the entire granular structure. Finally, a series of experiments are conducted to thoroughly analyze the characteristics and applications of these S3WCs. The superior classification performance of the proposed models on diverse datasets is demonstrated.},
  archive      = {J_ASOC},
  author       = {Pei Liang and Wanying Cao and Junhua Hu},
  doi          = {10.1016/j.asoc.2023.110978},
  journal      = {Applied Soft Computing},
  pages        = {110978},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sequential three-way classification model based on risk preference and decision correction},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective optimal power flow of thermal-wind-solar
power system using an adaptive geometry estimation based multi-objective
differential evolution. <em>ASOC</em>, <em>149</em>, 110977. (<a
href="https://doi.org/10.1016/j.asoc.2023.110977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable energy is a key component of sustainable development. The current grid can be supplied by fossil fuel generators and renewable energy sources (RESs)-based generators, such as solar photovoltaic (PV) and wind power generators . In an electrical network, power generation from several sources must be optimally coordinated to ensure efficient and economical operation. However, the intermittent and uncertain nature of RESs complicate the operation of power systems . In this study, an adaptive geometry estimation-based multi-objective differential evolution (AGE-MODE) method is proposed for multi-objective optimal power flow in a hybrid power system of thermal, wind, and solar energy sources (MOOPF-TWS). In the proposed approach, wind and solar PV power outputs are predicted based on Weibull and lognormal probability distribution functions , respectively. Therefore, the generation costs for solar and wind power can be divided into direct costs, penalty costs for underestimation, and reserve costs for overestimation. Furthermore, the emissions, voltage deviation , and real power loss are considered in particular cases. AGE-MODE is applied to modified IEEE 30-bus and 57-bus systems, where different case studies are simulated with combinations of two-, three-, and four-objective optimizations in MOOPF-TWS problems. Comparisons between AGE-MODE and other recently developed multi-objective methods demonstrate its effectiveness in resolving MOOPF-TWS problems, particularly for cases with more than two objectives.},
  archive      = {J_ASOC},
  author       = {Truong Hoang Bao Huy and Hien Thanh Doan and Dieu Ngoc Vo and Kyu-haeng Lee and Daehee Kim},
  doi          = {10.1016/j.asoc.2023.110977},
  journal      = {Applied Soft Computing},
  pages        = {110977},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimal power flow of thermal-wind-solar power system using an adaptive geometry estimation based multi-objective differential evolution},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semantic web-based risk assessment framework for
collaborative planning to enhance overall supply chain effectiveness for
semiconductor industry. <em>ASOC</em>, <em>149</em>, 110976. (<a
href="https://doi.org/10.1016/j.asoc.2023.110976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiconductor supply chain uncertainty is increasingly complicated due to shortening product life cycles, demand fluctuation and a series of collaborative decisions, yet little research focuses on risk assessment from operational planning level to supply chain planning level. Limitations of existing approaches can be traced in part to the lack of a framework within which the decisions involved in different levels can be integrated and aligned in light of supply chain dynamics. This study aims to develop a semantic web based risk assessment framework to integrate collaborative planning and enable the interpretability among real world and simulation modelling . This study starts by proposing a performance elevation metric from interpreting planning level for overall supply chain effectiveness. This study further constructs an ontology for planning and control decisions to support building the risk assessment simulation model to ensure information system interoperability and link domain knowledge for semiconductor supply chain network resilience. For validation, a simulation model is constructed for backend production from a leading semiconductor company in Germany . The contribution of the proposed semantic web-based framework provides the interoperability for supporting supply chain management and reduces the gaps between simulation modelling and physical setting to enhance overall supply chain effectiveness.},
  archive      = {J_ASOC},
  author       = {Hsuan-An Kuo and Chen-Fu Chien and Hans Ehm and Thomas Ponsignon},
  doi          = {10.1016/j.asoc.2023.110976},
  journal      = {Applied Soft Computing},
  pages        = {110976},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semantic web-based risk assessment framework for collaborative planning to enhance overall supply chain effectiveness for semiconductor industry},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble reinforcement learning: A survey. <em>ASOC</em>,
<em>149</em>, 110975. (<a
href="https://doi.org/10.1016/j.asoc.2023.110975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) has emerged as a highly effective technique for addressing various scientific and applied problems. Despite its success, certain complex tasks remain challenging to be addressed solely with a single model and algorithm. In response, ensemble reinforcement learning (ERL), a promising approach that combines the benefits of both RL and ensemble learning (EL), has gained widespread popularity. ERL leverages multiple models or training algorithms to comprehensively explore the problem space and possesses strong generalization capabilities. In this study, we present a comprehensive survey on ERL to provide readers with an overview of recent advances and challenges in the field. Firstly, we provide an introduction to the background and motivation for ERL. Secondly, we conduct a detailed analysis of strategies such as model selection and combination that have been successfully implemented in ERL. Subsequently, we explore the application of ERL, summarize the datasets, and analyze the algorithms employed. Finally, we outline several open questions and discuss future research directions of ERL. By offering guidance for future scientific research and engineering applications , this survey significantly contributes to the advancement of ERL.},
  archive      = {J_ASOC},
  author       = {Yanjie Song and Ponnuthurai Nagaratnam Suganthan and Witold Pedrycz and Junwei Ou and Yongming He and Yingwu Chen and Yutong Wu},
  doi          = {10.1016/j.asoc.2023.110975},
  journal      = {Applied Soft Computing},
  pages        = {110975},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble reinforcement learning: A survey},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Renyi entropy analysis of a deep convolutional
representation for texture recognition. <em>ASOC</em>, <em>149</em>,
110974. (<a href="https://doi.org/10.1016/j.asoc.2023.110974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent success of convolutional neural networks in computer vision in general, texture images still pose an important challenge to those models, especially when dealing with textures “in the wild”. In this context, deep learning models can benefit from the introduction of features long known to be useful for texture modeling. And that is the case of entropy, a measure of texture regularity that has played an important role in classical computer vision . Based on this observation, here we propose an alternative analysis over deep convolutional neural features based on entropy for texture representation and, particularly, texture classification . More precisely, we couple a module to the convolutional backbone that locally computes the Renyi entropy of the latent representation at multiple levels. The rationale for using Renyi entropy is essentially two-fold: (1) It connects the concept of entropy with multifractal theory, another well explored measure especially in domains of application where we seek some physical interpretation of the descriptors, e.g., in medicine, biology, and others; (2) It has an extra degree of freedom ( α α parameter) that can be fine-tuned. The main contribution of this study is the development of a strategy that can improve the performance of convolutional neural networks in texture recognition tasks, adding low computational cost. The effectiveness of our method is verified in texture classification of benchmark datasets, as well as in a practical task of plant species identification. Our method achieves a competitive accuracy of 84.5\% in the classification of KTH-TIPS-2b and 80.9\% in FMD, which are two challenging benchmark databases. Besides, an accuracy of 91.6\% is obtained in the plant identification application, to our knowledge outperforming all the results previously published on this task. In both scenarios, the proposed descriptors outperform several approaches from the state-of-the-art, confirming the method potential as a rich and robust solution for texture analysis in general. The results also suggest that information-theoretical measures like entropy can be a reliable source of information to compose a precise and robust latent representation of texture images.},
  archive      = {J_ASOC},
  author       = {Joao B. Florindo},
  doi          = {10.1016/j.asoc.2023.110974},
  journal      = {Applied Soft Computing},
  pages        = {110974},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Renyi entropy analysis of a deep convolutional representation for texture recognition},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum annealing algorithm for fault section location in
distribution networks. <em>ASOC</em>, <em>149</em>, 110973. (<a
href="https://doi.org/10.1016/j.asoc.2023.110973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the last link of the power system , the distribution network is responsible for ensuring stable power consumption and improving power quality. Therefore, a more reliable and fast fault section location(FSL) method is essential for the stable operation and optimization of distribution networks. In this context, this paper adopts an effective method to apply the quantum annealing algorithm(QA) based on the quantum tunneling mechanism to the distribution network fault section location problem. A quantum Hamiltonian function consisting of potential and kinetic energy terms is constructed based on the theoretical knowledge of QA. Among them, FSL objective function is mapped to the potential energy term , and the transverse magnetic field is introduced to construct the kinetic energy term, which can realize the quantum tunneling effect and approximate or even reach the global optimal solution . Based on the quantum Hamiltonian function construction, this paper modifies some parameters in the QA framework to propose an improved quantum annealing algorithm(IQA) to improve the accuracy. In the two test systems of IEEE 33-node distribution network and IEEE 33-node distribution network with distributed generation sources(DGs), QA and IQA are compared and analyzed with other intelligent algorithms using the average number of iterations and localization accuracy as indicators. We find that QA is more likely to obtain the global optimal solution compared with the simulated annealing algorithm(SA). IQA can search for faulty sections with 100\% accuracy and the least number of average iterations in both single power distribution networks and distribution networks containing DGs. Under the scenarios of fault signal distortion and increasing fault sections, IQA shows superb competitive advantages by exhibiting good fault tolerance performance, global optimal search capability and stability.},
  archive      = {J_ASOC},
  author       = {Zhongqin Bi and Xiaoting Yang and Baonan Wang and Weina Zhang and Zhen Dong and Dan Zhang},
  doi          = {10.1016/j.asoc.2023.110973},
  journal      = {Applied Soft Computing},
  pages        = {110973},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum annealing algorithm for fault section location in distribution networks},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain feature learning and data augmentation for
few-shot proxy development in oil industry. <em>ASOC</em>, <em>149</em>,
110972. (<a href="https://doi.org/10.1016/j.asoc.2023.110972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reservoir engineering, numerical simulators are crucial for analyzing risks and uncertainties. The decision-making plan is complex due to numerous uncertain variables, which lead to high dimensional spaces, such as porosity and permeability 3-dimensional inputs, and a substantial computational footprint. The scientific literature has directed efforts toward proxy model construction that operates as a faster estimation function. Yet, many samples are needed to develop a reliable solution, increasing the proxy construction time. Our investigation introduces a novel Few-Shot Proxy method, which opens the possibility of dealing with high-dimensional inputs, decreasing simulator dependence without incurring a high computational cost. Our methodology utilized two initial sample sets comprising only 30 and 40 examples each. The data augmentation method ensures the requisite sample diversity essential for robust training. The study leverages the simulator’s reservoir uncertainties collection to supplement the variety and representativeness of the training instances. The Few-Shot Proxy method builds upon synthetic data modeling in a Cross-Domain Feature scheme. New training data are assembled based on a meticulous selection of features, underpinned by clustering and metric learning. For time series construction, we exploited the notion of fuzzy logic and prototypical networks to cover the sparse distribution. The proxy method generates the cumulative fluid production curve from geostatistical realizations and provides the risk curve considering an uncertainty distribution. The presented strategy decreased the simulator’s processing time by 87\%, evaluating the training, validation, testing, and risk analysis steps. In the risk analysis , the symmetric mean absolute percentage error was less than 2\% in all cases.},
  archive      = {J_ASOC},
  author       = {Gabriel Cirac and Jeanfranco Farfan and Guilherme Daniel Avansi and Denis José Schiozer and Anderson Rocha},
  doi          = {10.1016/j.asoc.2023.110972},
  journal      = {Applied Soft Computing},
  pages        = {110972},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-domain feature learning and data augmentation for few-shot proxy development in oil industry},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Software systems supporting remote education – fuzzy
assessment using a multi-criteria group decision-making method.
<em>ASOC</em>, <em>149</em>, 110971. (<a
href="https://doi.org/10.1016/j.asoc.2023.110971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the COVID-19 pandemic, the demand for systems supporting remote education increased significantly, and after the pandemic this form of education remained in place to supplement the stationary education. Among the tools that comprehensively support remote learning , there are two most popular systems, i.e. Google Workspace for Education and Microsoft Office 365, available in several different variants, differing in the functionalities offered. The aim of the article is to analyse the functionality, as well as to compare and evaluate this software. Since the assessment of e-learning systems is based on the subjective judgements of experts, and the assessment criteria are qualitative in nature, a new MCGDM (Multi-Criteria Group Decision Making) method called NEAT F-PROMETHEE GDSS (New Easy Approach To Fuzzy PROMETHEE – Group Decision Support System) was developed for the purposes of the study. This method captures the uncertain and imprecise qualitative assessments expressed by many experts and aggregates them into an overall quantitative assessment . As a result of the assessment, it was found that all variants of Google Workspace for Education and Microsoft Office 365 meet the basic requirements for tools for conducting remote learning. In addition, the use of the NEAT F-PROMETHEE GDSS method made it possible to create a ranking of individual software variants. Based on the group evaluation, it was determined that the best system in terms of the relationship between the offered capabilities and the cost of use is Google Workspace for Education Plus, with other variants of Google Workspace for Education taking subsequent positions in the ranking. In turn, Microsoft Office 365 solutions received a worse group rating, largely due to the higher cost of use and slightly less ability to control student work.},
  archive      = {J_ASOC},
  author       = {Paweł Ziemba and Mateusz Piwowarski and Kesra Nermend},
  doi          = {10.1016/j.asoc.2023.110971},
  journal      = {Applied Soft Computing},
  pages        = {110971},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Software systems supporting remote education – fuzzy assessment using a multi-criteria group decision-making method},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of an adaptive fuzzy-neural inference system-based
control approach for robotic manipulators. <em>ASOC</em>, <em>149</em>,
110970. (<a href="https://doi.org/10.1016/j.asoc.2023.110970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive fuzzy-neural inference system (ANFIS)-based control approach for a six degrees of freedom (6-DoF) robotic manipulator . Its main objective is to guarantee the error convergence of the controlled system in the presence of uncertainties and unknown disturbances. The suggested controller is a parallel combination of an ANFIS network with a proportional-integral-derivative (PID) controller. The ANFIS system is used as an estimator to approximate a part of the system and then applied as the feedback linearization in the suggested control structure. The convergence of system errors to zero was proven using Barbalat’s lemma. The suggested control law combines the simplicity and ease of implementation of PID control with the estimation properties of ANFIS networks. The suggested approach was evaluated using a simulation study and further validated experimentally using the 6-DoF IRB-120 robotic manipulator (IRB-120-RM). The obtained results confirmed its superior performance and suitability for practical implementation to industrial actuators.},
  archive      = {J_ASOC},
  author       = {Mojtaba Hadi Barhaghtalab and Mohammadreza Askari Sepestanaki and Saleh Mobayen and Abolfazl Jalilvand and Afef Fekih and Vahid Meigoli},
  doi          = {10.1016/j.asoc.2023.110970},
  journal      = {Applied Soft Computing},
  pages        = {110970},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of an adaptive fuzzy-neural inference system-based control approach for robotic manipulators},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A metaverse text recognition model based on character-level
contrastive learning. <em>ASOC</em>, <em>149</em>, 110969. (<a
href="https://doi.org/10.1016/j.asoc.2023.110969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient text recognition can enhance the accuracy and time efficiency of human–computer interaction and information exchange in noise Metaverse scenarios. For example, it can improve the safety of digital twin-based intelligent transportation and the intelligence of Non-Player Characters. Robust features play a vital role in the performance of scene text recognition models in noise Metaverse situations. To extract robust features, and improve the accuracy and time efficiency of scene text recognition, we propose a Chara cter level text recognition model in Meta verse applications, called MetaChara. It contains two main components: a lightweight text feature extraction module (LightFeature), and a robust character recognition module (RobChara). LightFeature leverages the advantage of global feature aggregation in the primitive representation learning network to handle irregular text images.RobChara incorporates the capability of contrastive learning from the momentum contrast method, improving the robustness of feature extraction in MetaChara. It structures a feature queue for organized storage. By optimizing the similarity of intra-character features and maximizing inter-character differences, it makes the model better adapted to scene text recognition tasks in Metaverse. Experiment results demonstrate that MetaChara is light with 29.14 million parameters and time efficient with an average recognition speed of 1.73 s. It also achieves excellent performance in terms of FLoating-point Operations (FLOPs), registering only 59.60 billion times for each operation. MetaChara achieves an average accuracy of 0.969 for character recognition. We present a case study where MetaChara quickly and accurately recognizes scene texts within the context of autonomous driving in the Metaverse. This demonstrates how MetaChara enhances safety and improves time efficiency for intelligent transportation systems .},
  archive      = {J_ASOC},
  author       = {Le Sun and Huiyun Li and Ghulam Muhammad},
  doi          = {10.1016/j.asoc.2023.110969},
  journal      = {Applied Soft Computing},
  pages        = {110969},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A metaverse text recognition model based on character-level contrastive learning},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive fuzzy multi-neighborhood feature selection with
hybrid sampling and its application for class-imbalanced data.
<em>ASOC</em>, <em>149</em>, 110968. (<a
href="https://doi.org/10.1016/j.asoc.2023.110968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For imbalanced data , classification efficiency degrades significantly due to the missing information for the positive class, and existing sampling schemes do not consider the distributions of samples. Additionally, the global parameters of fuzzy neighborhoods are set manually. These defects affect the effectiveness of classifier. To address these problems, we offer an adaptive fuzzy multi-neighborhood feature selection methodology with intercluster distance-based hybrid sampling for class-imbalanced data. First, the number of clusters can be defined in terms of the number of samples in the negative or positive class. The initial centers of the clusters are determined according to the number of clusters, and the dissimilarity and similarity measures are calculated by using the intercluster distances between samples. Then, the cluster center, fuzzy membership matrix, and intercluster distance are studied, and then the optimization objective function is designed. The hybrid sampling scheme can be used to combine the generated positive class samples and negative class samples and obtain a class-balanced system. Second, according to the sample distribution, the standard deviation and a set of adaptive fuzzy multi-neighborhood radii are designed. A fuzzy multi-neighborhood similarity relation is defined by introducing a Gaussian kernel model to obtain a fuzzy multi-neighborhood granule, and an improved fuzzy multi-neighborhood rough set model is provided. Uncertain measures of fuzzy neighborhood systems are evaluated by the positive region and dependency. Third, by integrating fuzzy dependence with fuzzy complementary condition entropy, fuzzy multi-neighborhood complementary mutual information is provided on two viewpoints of algebra and information. Finally, a heuristic feature subset selection methodology for imbalanced classification with hybrid sampling using fuzzy c-means clustering is studied to obtain this excellent set of features. Experiments on 26 imbalanced datasets show the effectiveness of our designed algorithm .},
  archive      = {J_ASOC},
  author       = {Lin Sun and Mengmeng Li and Weiping Ding and Jiucheng Xu},
  doi          = {10.1016/j.asoc.2023.110968},
  journal      = {Applied Soft Computing},
  pages        = {110968},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive fuzzy multi-neighborhood feature selection with hybrid sampling and its application for class-imbalanced data},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A grammar-based multi-objective neuroevolutionary algorithm
to generate fully convolutional networks with novel topologies.
<em>ASOC</em>, <em>149</em>, 110967. (<a
href="https://doi.org/10.1016/j.asoc.2023.110967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of complex and deep neural networks is often performed by identifying and combining building blocks and progressively selecting the most promising combination. Neuroevolution automates this process by employing evolutionary algorithms to guide the search. Within this field, grammar-based evolutionary algorithms have been demonstrated to be powerful tools to describe and thus encode complex neural architectures effectively. Following this trend, the present work proposes a novel grammar-based multi-objective neuroevolutionary for generating Fully Convolutional Networks . The proposed method, named Multi-Objective gRammatical Evolution for FUlly convolutional Networks (MOREFUN), includes a new efficient way to encode skip connections, facilitating the description of complex search spaces and the injection of domain knowledge in the search procedure, generation of fully convolutional networks, upsampling of lower-resolution inputs in multi-input layers, usage of multi-objective fitness, and inclusion of data augmentation and optimiser settings to the grammar. Our best networks outperformed previous grammar evolution algorithms, achieving 90.5\% accuracy on CIFAR-10 without using transfer learning , ensembles, or test-time data augmentation. Our best models had 13.39±5.25 trainable parameters and the evolutionary process required 90 min per generation.},
  archive      = {J_ASOC},
  author       = {Thiago Z. Miranda and Diorge B. Sardinha and Ferrante Neri and Márcio P. Basgalupp and Ricardo Cerri},
  doi          = {10.1016/j.asoc.2023.110967},
  journal      = {Applied Soft Computing},
  pages        = {110967},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A grammar-based multi-objective neuroevolutionary algorithm to generate fully convolutional networks with novel topologies},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-mutation mechanism-driven snake optimizer for
scheduling multiple budget constrained workflows in the cloud.
<em>ASOC</em>, <em>149</em>, 110966. (<a
href="https://doi.org/10.1016/j.asoc.2023.110966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling workflows in cloud computing is to find the most appropriate mapping from a series of dependent tasks to a set of available virtual resources so as to minimize or maximize some user concerned objectives, which is very significant to a sustainable and high-efficient cloud data center . However, it still faces challenges since its NP-hardness and the diversified requirements of both the cloud service consumers and cloud service providers need to be satisfied at the same time, especially in addressing multiple applications requested simultaneously. This work proposes a dual-mutation mechanism-driven snake optimizer for scheduling multiple workflows in the cloud for minimizing the makespan of each workflow under user pre-defined budget constraints. Firstly, a continuous optimization algorithm , namely Snake Optimizer (SO) is adopted into discrete optimization , i.e., workflow scheduling. Secondly, a task execution order aware fitness function is designed to reduce the gaps or waiting time between parent and child tasks within a workflow and thus reduce the total execution time of the workflow. Besides, we analyze the existing snake optimizer and adjust the parameters corresponding to different evolutionary stages to adapt to our considered problem. Finally, a dual-mutation mechanism is developed by introducing a non-improvement iteration number for each snake and applying a standard bit mutation operation to prematurely converging snakes and the individuals randomly selected from the remaining snakes so that the population diversity can be enhanced and more potential solutions can be explored. Experimental results on a set of real-world scientific workflows show that our proposed algorithm is of great superiority in constraint satisfiability, meaning that compared with its peers, it is always the first to find feasible solutions.},
  archive      = {J_ASOC},
  author       = {Huifang Li and Guanghao Xu and Boyuan Chen and Shuangxi Huang and Yuanqing Xia and Senchun Chai},
  doi          = {10.1016/j.asoc.2023.110966},
  journal      = {Applied Soft Computing},
  pages        = {110966},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-mutation mechanism-driven snake optimizer for scheduling multiple budget constrained workflows in the cloud},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An three-in-one on-demand ride-hailing prediction model
based on multi-agent reinforcement learning. <em>ASOC</em>,
<em>149</em>, 110965. (<a
href="https://doi.org/10.1016/j.asoc.2023.110965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ride-hailing behaviors of customers are often impacted by various factors including time, geographic distance between locations and weather conditions, causing imbalance between the supply and demand of on-demand ride-hailing dispatch. An effective on-demand ride-hailing dispatching management approach can dispatch idle vehicles and utilize traffic resources more reasonably, increase drivers’ income, and improve customers’ satisfaction and experience. In order to overcome the disadvantages in on-demand ride-hailing systems, we propose a three-in-one multi-agent reinforcement learning based online algorithm for ride-hailing demand prediction, called ERPM, which can achieve intelligent prediction in an effective and efficient fashion. ERPM tackles the problem that the training phase of traditional reinforcement learning models is difficult to converge due to the high dimensions of input and output data after partitioning the areas that provide platform services into grids, and uses the Actor–Critic strategy to perform on-demand ride-hailing dispatching actions, which are evaluated and optimized to intelligently predict the demand of ride-hailing in the grid areas. In addition, ERPM achieves intelligent parameter update by applying newly designed loss function, learning rate and optimization algorithm , and design an accurate on-demand ride-hailing prediction algorithm on the basis of maximizing the GMV (gross merchandise volume), i.e., the revenues of all on-demand ride-hailing orders served. Compared with traditional machine learning models, the proposed ERPM model is proved to be capable of capturing more complex features of supply and demand from the high dimensions to obtain higher prediction accuracy. Empirical studies are performed on the real Didi Chuxing data, by evaluating the results from extensive experiments, it can be observed that ERPM achieves the highest accuracy of demand prediction on daily GMV and a higher order response rate than the commonly-used famous methods, i.e., for GMV, ERPM outperforms DQN by 9.7\% and the Naive model by 14.8\%, for the order response rate of ERPM is 1.4\% and 4.1\% higher than that of DQN and Naive, MAPE (Mean Absolute Percentage Error) of DQN and Naive is 1.61 and 5.94 times higher than that of ERPM, and R 2 R2 of ERPM is improved by 8.70\% and 64.8\% when compared to that of DQN and Naive, respectively.},
  archive      = {J_ASOC},
  author       = {Shaojie Qiao and Nan Han and Jiangtao Huang and Yuzhong Peng and Hongguo Cai and Xiao Qin and Zhengyi Lei},
  doi          = {10.1016/j.asoc.2023.110965},
  journal      = {Applied Soft Computing},
  pages        = {110965},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An three-in-one on-demand ride-hailing prediction model based on multi-agent reinforcement learning},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Green and reliable medical device supply chain network
design under deep dynamic uncertainty: A novel approach in the context
of COVID-19 outbreak. <em>ASOC</em>, <em>149</em>, 110964. (<a
href="https://doi.org/10.1016/j.asoc.2023.110964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditions governing industrial activities during and after global shocks with societal and economic transformations such as the COVID-19 pandemic have led to the loss of effectiveness of conventional approaches to dealing with uncertainties. The occurrence of sharp fluctuations in the essential parameters has left decision-makers in an unpredictable situation. Therefore, proactive efforts should be made to develop current approaches for adapting to new conditions. This paper establishes a strategic, tactical, and operational decision-making framework under the COVID-19 outbreak by developing a new uncertainty type called deep dynamic uncertainty. In the first step, a Mixed-Integer Linear Programming (MILP) model is proposed for the green and reliable closed-loop supply chain network design. The proposed model allows the decision-maker (DM) to manage and control co 2 co2 emissions and e-waste generation. In the second step, a new three-step algorithm called Augmented Adjustable Column-Wise Robust Optimization (AACWRO) is first proposed. Then, by combining the proposed column-wise uncertainty with multi-stage stochastic programming (MSSP) approach, deep dynamic uncertainty is theorized for modeling the demand uncertainty under pandemic conditions. The model&#39;s performance under deep dynamic uncertainty has been carefully investigated based on the real ventilator and infusion pump supply chain network in Iran. The model under deep dynamic uncertainty, while maintaining tractability and adjustability , provides flexibility in entering data into the problem and significantly increases the coverage of modeling uncertainties. The results clearly demonstrate the efficiency of the proposed approach. The model under deep dynamic uncertainty at all levels of conservatism has on average 42.96\% lower cost and 32\% higher stability than the MSSP model.},
  archive      = {J_ASOC},
  author       = {Amin Reza Kalantari Khalil Abad and Farnaz Barzinpour and Mir Saman Pishvaee},
  doi          = {10.1016/j.asoc.2023.110964},
  journal      = {Applied Soft Computing},
  pages        = {110964},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Green and reliable medical device supply chain network design under deep dynamic uncertainty: A novel approach in the context of COVID-19 outbreak},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theta-mechanism based cluster search algorithm for global
constrained optimization. <em>ASOC</em>, <em>149</em>, 110963. (<a
href="https://doi.org/10.1016/j.asoc.2023.110963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study concerns constructing an evolutionary search system to solve the global constrained optimization problems. Firstly, we proposed a hybrid constraint-handling method, called theta-mechanism, which blends two types of constraint-handling functions and alternates use of them in the searching process to balance two competing objectives: seeking as much as possible feasible regions and quickly converging to the optimum point in the found feasible regions. Secondly, to enable the search system to cooperate well with the theta-mechanism, we designed the cluster search algorithm (CSA) and developed the search reachability analysis (SRA) method. Based on SRA, we evaluated the characteristics of several typical search operators in order to assemble them into different operator combinations in CSA to maximize its performance, which enables CSA with theta-mechanism to accomplish the two inconsistent search objectives effectively. We tested the proposed method on 18 benchmark functions from IEEE CEC2010 and 32 real-world constrained optimization problems collected in IEEE CEC2020. Our results show the CSA with theta-mechanism is more competitive than the existing state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Hao Chen and Fengzhu Jia and Xiaoying Pan and Zhi Wei},
  doi          = {10.1016/j.asoc.2023.110963},
  journal      = {Applied Soft Computing},
  pages        = {110963},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Theta-mechanism based cluster search algorithm for global constrained optimization},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast calculation for approximations in dominance-based rough
set approach using dual information granule. <em>ASOC</em>,
<em>149</em>, 110962. (<a
href="https://doi.org/10.1016/j.asoc.2023.110962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dominance-based Rough Set Approach (DRSA) is an extension of RST , which utilizes the dominance relation in attributes. However, traditional DRSA-based methods do not exploit the properties and logical structure in depth, which causes a high computational cost in lower and upper approximations . Besides, these methods are object-based, leading to repeated calculations from identical instances, and they involve numerous redundant computations to approximate different decisions. Therefore, we propose a novel approach, called DIGAC (Dual Information Granule-based Approximation Calculation), to improve DRSA by replacing the object-based calculation with the granule-based calculation. It effectively reduces the time complexity by constructing a granule-based ordered decision system. Additionally, this novel approach uses three types of Dual Information Granules (DIGs) to avoid repeated calculations from identical samples. In the process of lower and upper approximation, we leverage their transitivity to put forward a Distributed Storage-based Lower Approximation Calculation (DSLAC) strategy and a Query-based Upper Approximation Calculation (QUAC) strategy to eliminate redundant computations. Importantly, we theoretically prove that the DIG-based approach extensively reduces the time complexity of the approximations and obtains the same approximations as the original counterpart. Our approach is investigated on 23 datasets, and the experimental results show that it outperforms existing algorithms in terms of efficiency and stability, especially for large-scale and high-dimensional datasets, where the average decrease in execution time is up to 99\%.},
  archive      = {J_ASOC},
  author       = {Jie Zhao and Daiyang Wu and JiaXin Wu and Eric W.K. See-To and Faliang Huang},
  doi          = {10.1016/j.asoc.2023.110962},
  journal      = {Applied Soft Computing},
  pages        = {110962},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fast calculation for approximations in dominance-based rough set approach using dual information granule},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An explainable decision model based on extended
belief-rule-based systems to predict admission to the intensive care
unit during COVID-19 breakout. <em>ASOC</em>, <em>149</em>, 110961. (<a
href="https://doi.org/10.1016/j.asoc.2023.110961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the shortage of intensive care units (ICUs) due to the coronavirus disease (COVID-19), a prediction model is essential in ensuring the availability of ICU beds. However, several challenges, such as the importance of distinguishing indicators, efficiency of ICU admission records, and the explainability and effectiveness of the prediction model, hinder the effective prediction of ICU admissions. To mitigate these challenges, an explainable decision model that uses the extended belief rule-based system is introduced to predict ICU admission. First, an indicator extraction model is proposed to measure the importance of the various indicators and obtain representative indicators. Second, a Charnes, Cooper, and Rhodes (CCR) model is constructed to measure the efficiencies of the belief rules to achieve the compact structure of an extended belief rule base. Third, a new extended belief rule-based model, optimized by parameter optimization and domain division-based rule reduction, is developed to predict ICU admission. These procedures enable the explainable decision model to adapt to big data situation, offer explanations and realize high efficiency. Finally, a case study of ICU admission during a COVID-19 outbreak is conducted to demonstrate the implementation and effectiveness of the proposed model in a comparative analysis.},
  archive      = {J_ASOC},
  author       = {Jing Zheng and Long-Hao Yang and Ying-Ming Wang and Jian-Qing Gao and Kai Zhang},
  doi          = {10.1016/j.asoc.2023.110961},
  journal      = {Applied Soft Computing},
  pages        = {110961},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explainable decision model based on extended belief-rule-based systems to predict admission to the intensive care unit during COVID-19 breakout},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A three-way decision method based on cumulative prospect
theory for the hierarchical diagnosis and treatment system of chronic
diseases. <em>ASOC</em>, <em>149</em>, 110960. (<a
href="https://doi.org/10.1016/j.asoc.2023.110960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase in the number of patients with chronic diseases, the contradiction between medical resources and the demand for medical services is increasingly stressed. The hierarchical medical system becomes an important measure to efficiently allocate medical resources. Given that the two-way referral of patients is a core issue of a hierarchical medical system, this paper takes the upward referral as an example and constructs a three-way decision model for the hierarchical diagnosis and treatment system of chronic diseases, which takes teleconsultation as a compromise. Firstly, considering the credibility of decision-making information, a new representation of the loss function is proposed by combining the subjective evaluations of doctors with the personal preferences of patients. Secondly, the credibility adjustment rules are designed, subject to the constraints of the loss function. Furthermore, taking into account the doctors’ psychological behaviors such as risk attitudes when facing uncertain decisions, a three-way decision model based on the cumulative prospect theory was constructed. The main highlight and contribution of this paper is to put forward a new form of the loss function and the adjustment method of the loss function. A medical decision support system is designed based on the proposed method. Finally, a real case of upward referral of patients with coronary heart disease in First Hospital of Qinhuangdao is given to illustrate the validity of the proposed method, and comparisons are conducted to show the superiority of the method. This study provides methodical support for the hierarchical diagnosis and treatment system in the medical field, and it is conducive to promoting the equalization of basic medical services, which has potential theoretical and practical significance.},
  archive      = {J_ASOC},
  author       = {Meng Zhao and Yajun Wang and Xinyu Meng and Huchang Liao},
  doi          = {10.1016/j.asoc.2023.110960},
  journal      = {Applied Soft Computing},
  pages        = {110960},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way decision method based on cumulative prospect theory for the hierarchical diagnosis and treatment system of chronic diseases},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning-based comprehensive learning grey
wolf optimizer for feature selection. <em>ASOC</em>, <em>149</em>,
110959. (<a href="https://doi.org/10.1016/j.asoc.2023.110959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Turning heavy and high-dimensional raw data into knowledge for decision makers is a complex process. Feature selection (FS) can do this task well by removing irrelevant and/or redundant parts from the raw dataset, aiming at reducing dimensionality and improving accuracy. In this work, a reinforcement learning-based comprehensive learning grey wolf optimizer (RLCGWO) is designed to solve the FS problem, which is modeled as a combinatorial optimization problem . First, a comprehensive learning operator is proposed, containing the static and dynamic learning strategies. These two strategies provide GWO with the exploration capability in different ways. Second, a novel RL-based policy regulation technique is developed, which is based on the Q-learning framework. Individuals are considered as agents that obtain the state of the environment based on the state encoding technique. Meanwhile, agents select the most appropriate actions from the well-designed action set based on the information provided by the Q-table. Furthermore, agents update the stand-alone Q-table with rewards to provide themselves with more timely and accurate feedback. Third, a chaotic-based learning strategy is devised for leaders to improve the quality of the optimal solution. The comparison results of the proposed RLCGWO with six successful GWO variants and three typical algorithms on the benchmarks initially demonstrate its advantages in convergence speed and accuracy. The proposed RLCGWO is finally applied to the challenging FS problem. The comparison results with six popular algorithms on 15 UCI datasets and 3 real world high-dimensional datasets underscore its high adaptability and versatility. Taken together, the proposed RLCGWO is a promising technique for FS.},
  archive      = {J_ASOC},
  author       = {Zhengpeng Hu and Xiaobing Yu},
  doi          = {10.1016/j.asoc.2023.110959},
  journal      = {Applied Soft Computing},
  pages        = {110959},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reinforcement learning-based comprehensive learning grey wolf optimizer for feature selection},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ANFIS learning using expectation maximization based gaussian
mixture model and multilayer perceptron learning. <em>ASOC</em>,
<em>149</em>, 110958. (<a
href="https://doi.org/10.1016/j.asoc.2023.110958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Adaptive Neuro-Fuzzy Inference System (ANFIS) is a hybrid learning algorithm that combines the learning ability of neural networks with fuzzy inference systems. While ANFIS has been successfully applied to several real-world problems, effective parameter optimization remains a challenge. This paper presents a novel approach for optimizing parameters of ANFIS in supervised settings. The proposed approach utilizes a combination of probabilistic mixture models and perceptron-based learning to parameterize ANFIS. To learn ANFIS membership functions, it generates a mixture of the finite probability distributions for each input feature. Then the consequent parameters are learned by transforming them into weights of a multi-layer perceptron instance. The effectiveness of the proposed method is evaluated on classification problems of varying complexity, ranging from binary-class to multi-class with variable dimensions. The results show that the proposed algorithm improves ANFIS performance in terms of accuracy rate and speed (train-time) analysis. The effectiveness and efficiency of the proposed approach have been further confirmed using a 5 × 2 cross-validation paired significance t-test. In particular, for binary-class problems, our model outperformed standard methods by up to 10\% accuracy improvement. Similarly, for multi-class problems, our model achieved an average increase of 8\% in accuracy while reaching up to 14\% improvement. On average, the proposed model showed a reduction of about 75\% in training time compared to the other models. Overall, the proposed approach offers competitive computational performance and acceptable efficacy for parameter optimization of ANFIS.},
  archive      = {J_ASOC},
  author       = {Sadaf Jabeen and Mubasher Baig and Mian Muhammad Awais},
  doi          = {10.1016/j.asoc.2023.110958},
  journal      = {Applied Soft Computing},
  pages        = {110958},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ANFIS learning using expectation maximization based gaussian mixture model and multilayer perceptron learning},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven distributionally robust support vector machine
method for multiple criteria sorting problem with uncertainty.
<em>ASOC</em>, <em>149</em>, 110957. (<a
href="https://doi.org/10.1016/j.asoc.2023.110957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiple criteria sorting decision is a critical area of research in decision science. However, accurately capturing the decision maker’s cognition for each criterion using precise values poses significant challenges. To address this issue, our study investigates an uncertain multiple criteria sorting problem with indeterminate values for each criterion, which closely aligns with real-life decision-making processes. We adopt stochastic programming (SP) and distributionally robust optimization (DRO) to deal with the associated uncertainties. We formulate the preference learning process as a binary classification problem, where a piecewise-linear function is employed to measure the preference. Specifically, we develop a distributionally robust support vector machine model that incorporates the Wasserstein metric. By leveraging duality theory, the resulting DRO-based model can be transformed into a tractable convex problem . Then, we introduce an incremental projected subgradient algorithm to solve the tractable model, which can effectively utilize the separable structure. Finally, the efficiency of the proposed method is demonstrated through numerical experiments. The comparative evolution of the marginal value functions exhibits a similar tendency, highlighting the advantage and robustness of our approach.},
  archive      = {J_ASOC},
  author       = {Zhongming Wu and Ye Song and Ying Ji and Shaojian Qu and Zaiwu Gong},
  doi          = {10.1016/j.asoc.2023.110957},
  journal      = {Applied Soft Computing},
  pages        = {110957},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven distributionally robust support vector machine method for multiple criteria sorting problem with uncertainty},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A rankability-based fuzzy decision making procedure for oil
supplier selection. <em>ASOC</em>, <em>149</em>, 110956. (<a
href="https://doi.org/10.1016/j.asoc.2023.110956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-criteria decision-making (MCDM) explicitly assesses several conflicting criteria for our daily lives in selecting products, vehicles, techniques, etc. Weighting on criteria is a critical step in MCDM as the invalid weight of criteria will lead to a wrong decision. The proposed method addresses some drawbacks of the entropy-based weighting method commonly used in MCDM. The proposed new weighting method considers multiple evaluation factors, including the performance of the decision-maker, the edge weight basis of a digraph , and dominance relationships in the data. By incorporating these factors, the proposed method overcomes the limitations of the entropy-based method and reduces the total computation required. We conducted experiments using sustainable transportation data and comprehensively analyzed the results. We also propose a fuzzy MCDM model incorporating the proposed weighting method and Dempster–Shafer theory. Our model aims to handle uncertainty and imprecision in decision-making. Finally, the correctness and effectiveness of the proposed model were tested on real-life applications. The results of these tests demonstrated that the proposed method provides a practical and effective approach to decision-making in various domains. Overall, the work introduces a new weighting method based on rankability in MCDM, addresses the limitations of the entropy-based method, and presents a fuzzy MCDM model for handling uncertainty. The experimental results suggest that the proposed approach is promising and offers valuable insights for decision-makers.},
  archive      = {J_ASOC},
  author       = {Václav Snášel and Irina Perfilieva and Meenu Singh and Millie Pant and Zahra Alijani and Lingping Kong},
  doi          = {10.1016/j.asoc.2023.110956},
  journal      = {Applied Soft Computing},
  pages        = {110956},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A rankability-based fuzzy decision making procedure for oil supplier selection},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving audit opinion prediction accuracy using
metaheuristics-tuned XGBoost algorithm with interpretable results
through SHAP value analysis. <em>ASOC</em>, <em>149</em>, 110955. (<a
href="https://doi.org/10.1016/j.asoc.2023.110955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to create a machine learning model that can predict opinions in external audits and surpass the benchmark set in a prior study from the literature. This tool could reduce audit risk, which is a crucial task in external audits . Previous studies have shown that it is possible to create models that can predict the audit opinion a company will receive. In these studies, authors used statistics and machine learning models, and both non-financial (e.g. audit lag) and financial data (e.g. financial ratios , or absolute value items available from financial statements) to make predictions. In this study, the performance of the XGBoost model optimized by metaheuristics algorithms is examined and evaluated. This study compares the performance of six different metaheuristic algorithms used to tune the XGBoost model in two separate scenarios. The first scenario represents a realistic client portfolio, where a majority of the clients are known, while the second scenario simulates a new clients-only portfolio, a more difficult scenario where prior information such as audit lag is not available. The study uses a dataset of 12,690 observations of Serbian companies and their audit opinions from 2016 to 2019. The findings indicate an improvement over the benchmark due to a more optimized hyperparameter tuning process and the use of the iterative sine-cosine algorithm for the XGBoost model.},
  archive      = {J_ASOC},
  author       = {Mihailo Todorovic and Nemanja Stanisic and Miodrag Zivkovic and Nebojsa Bacanin and Vladimir Simic and Erfan Babaee Tirkolaee},
  doi          = {10.1016/j.asoc.2023.110955},
  journal      = {Applied Soft Computing},
  pages        = {110955},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving audit opinion prediction accuracy using metaheuristics-tuned XGBoost algorithm with interpretable results through SHAP value analysis},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep local-temporal architecture with attention for
lightweight human activity recognition. <em>ASOC</em>, <em>149</em>,
110954. (<a href="https://doi.org/10.1016/j.asoc.2023.110954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) is an essential area of pervasive computing deployed in numerous fields. In order to seamlessly capture human activities, various inertial sensors embedded in wearable devices have been used to generate enormous amounts of signals, which are multidimensional time series of state changes. Therefore, the signals must be divided into windows for feature extraction. Deep learning (DL) methods have recently been used to automatically extract local and temporal features from signals obtained using wearable sensors. Likewise, multiple input deep learning architectures have been proposed to improve the quality of learned features in wearable sensor HAR. However, these architectures are often designed to extract local and temporal features on a single pipeline, which affects feature representation quality. Also, such models are always parameter-heavy due to the number of weights involved in the architecture. Since resources (CPU, battery , and memory) of end devices are limited, it is crucial to propose lightweight deep architectures for easy deployment of activity recognition models on end devices. To contribute, this paper presents a new deep parallel architecture named DLT , based on pipeline concatenation. Each pipeline consists of two sub-pipelines, where the first sub-pipeline learns local features in the current window using 1D-CNN, and the second sub-pipeline learns temporal features using Bi-LSTM and LSTMs before concatenating the feature maps and integrating channel attention. By doing this, the proposed DLT model fully harnessed the capabilities of CNN and RNN equally in capturing more discriminative features from wearable sensor signals while increasing responsiveness to essential features. Also, the size of the model is reduced by adding a lightweight module to the top of the architecture, thereby ensuring the proposed DLT architecture is lightweight. Experiments on two publicly available datasets showed that the proposed architecture achieved an accuracy of 98.52\% on PAMAP2 and 97.90\% on WISDM datasets, outperforming existing models with few model parameters.},
  archive      = {J_ASOC},
  author       = {Ayokunle Olalekan Ige and Mohd Halim Mohd Noor},
  doi          = {10.1016/j.asoc.2023.110954},
  journal      = {Applied Soft Computing},
  pages        = {110954},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep local-temporal architecture with attention for lightweight human activity recognition},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal wind power generation system by honey badger
algorithm with differential evolution strategies. <em>ASOC</em>,
<em>149</em>, 110953. (<a
href="https://doi.org/10.1016/j.asoc.2023.110953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to build an optimal hybrid wind power system consisting of a permanent magnet direct-drive wind power generation unit, a hybrid energy storage system (HESS), a power electronic converter , and loads. Moreover, a reasonable control method is designed for each part, and a honey badger algorithm (HBA) with differential evolution strategies is employed to realize the coordinated control of each unit and improve the system stability. The real anti-interference capability of wind power system can be improved by the proposed HBA with differential evolution strategies (IHBA). Firstly, the wind hybrid system model is constructed, in which the wind power generation unit adopts the variable step climbing method to achieve the maximum power point tracking control; the HESS designs the power distribution method based on the bus voltage ; the converter supplying energy to the AC load introduces the virtual synchronous generator (VSG) control strategy. Secondly, due to the existence of energy exchange in each unit of the system, VSG has more parameters and its control performance is influenced by the system itself, this study proposes HBA with differential evolution strategies for system parameter optimization and constructs an IHBA-VSG model with the objective of minimizing the bus voltage fluctuation value. The IHBA improves original honey badger algorithm by three mechanisms which are time control function, Gaussian variation factor and differential evolution strategy. Finally, the parameters obtained from the HBA with differential evolution strategies optimization search are substituted into the wind hybrid system, and the simulation system is built by MATLAB/Simulink. The simulation results show that when the environment of the system changes or the load on the AC side changes abruptly, the proposed control method can reduce the bus voltage fluctuation by 2.9\%− 18.22\% compared with the honey badger algorithm Optimized VSG (HBA-VSG), and can reduce the bus voltage fluctuation by 5.09\%− 17.98\% compared with the traditional droop control without considering the system interaction. This study can effectively improve the stability of wind power generation system and promote the development of new energy industry.},
  archive      = {J_ASOC},
  author       = {Wanxing Sheng and Rui Li and Hui Hui and Kuo-Ping Lin and Kung-Ming Lan and Qiying Ren and Lingling Li},
  doi          = {10.1016/j.asoc.2023.110953},
  journal      = {Applied Soft Computing},
  pages        = {110953},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal wind power generation system by honey badger algorithm with differential evolution strategies},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An oversampling method based on differential evolution and
natural neighbors. <em>ASOC</em>, <em>149</em>, 110952. (<a
href="https://doi.org/10.1016/j.asoc.2023.110952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification problem of imbalanced data is a research focus in machine learning . An effective method for solving the class-imbalance problem is to generate synthetic samples for minority class data. Popular methods for synthesize samples include SMOTE, variants of SMOTE, and oversampling methods applying metaheuristic algorithms . Recently, an oversampling method DEBOHID based on differential evolutionary algorithm has been proposed. It uses nearest neighbors of the minority class to synthesize samples for balancing the data set. However, samples synthesized by nearest neighbors are easily interfered by noisy samples. Therefore, we proposed an oversampling method based on natural neighbors, called NaN-DEBOHID. In NaN-DEBOHID, minority samples are divided into dense samples and outliers according to the number of natural neighbors. Samples of different types are processed by different methods. The main advantages of NaN-DEBOHID are concluded as follows: (a) it finds samples that better represent the minority class distribution by applying natural neighbors; (b) it creates synthetic samples that showed better consistency with the surrounding samples by the DEBOHID method; (c) it removes noise samples of the minority class to enhance the classification performance. For experiments, Support Vector Machine (SVM) and k-Nearest Neighbor (kNN) are used as classifiers. The results indicated that NaN-DEBOHID performed competitively in terms of Accuracy, F F -measure and AUC.},
  archive      = {J_ASOC},
  author       = {Xialin Wang and Yanying Li and Jiaoni Zhang and Baoshuang Zhang and Huanhuan Gong},
  doi          = {10.1016/j.asoc.2023.110952},
  journal      = {Applied Soft Computing},
  pages        = {110952},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An oversampling method based on differential evolution and natural neighbors},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated picture fuzzy z-AHP &amp; TOPSIS methodology:
Application to solar panel selection. <em>ASOC</em>, <em>149</em>,
110951. (<a href="https://doi.org/10.1016/j.asoc.2023.110951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AHP is the most popular multi-criteria decision-making (MCDM) method to prioritize the evaluation criteria and to rank the considered alternatives. TOPSIS method is another often used MCDM method based on negative and positive ideal solutions for the selection of the best alternative. Although the picture fuzzy extensions of the TOPSIS method are used to consider experts’ hesitancy and refusal degrees in their judgments, there is a need to add reliability degrees to these judgments to provide better solutions and a reliable decision environment for real-life decision problems. This study presents a decision support tool that integrates the picture fuzzy Z-AHP (PF Z-AHP) method based on pairwise comparisons and a novel PF Z-TOPSIS method which includes reliability information for ranking the alternatives. The proposed PF Z-AHP&amp;TOPSIS methodology is applied for solar energy panel selection problem to analyze the practicality and superiority of the methodology. The sensitivity analysis demonstrates the robustness of the proposed methodology, and the comparison analysis presents evidence that the reliability functions offered by Z-numbers have the potential to affect outcomes.},
  archive      = {J_ASOC},
  author       = {Nurdan Tüysüz and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2023.110951},
  journal      = {Applied Soft Computing},
  pages        = {110951},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated picture fuzzy Z-AHP &amp; TOPSIS methodology: Application to solar panel selection},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting local label correlation from sample perspective
for multi-label classification via three-way decision theory.
<em>ASOC</em>, <em>149</em>, 110950. (<a
href="https://doi.org/10.1016/j.asoc.2023.110950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label classification, the expansion of output dimension seriously interferes learning performance, and even fails to build a joint prediction model. In order to restrain the proliferation of multi-label classifier’s hypothesis space, the current works focus on the application of global positive label correlation. However, the “black or white” mechanism ignore other possible forms of label correlation, such as negative or neutral correlation. By introducing the doctrine of the mean, three-way decision (3WD) theory provides a solution for in-depth research on local label correlation, and aims to handle the uncertainty of multi-label learning tasks. In this paper, a novel learning algorithm for multi-label joint classification, namely ML-3WD, is proposed by considering the 3WD label correlation from the perspective of samples. According to the weights of different features on any label, the comprehensive loss of each sample to three action strategies can be measured. Obviously, the 3WD rules for any label variable in multi-label output space is obtained. By aggregating the cutting thresholds between different labels, the division principles of 3WD label correlation are further established. Given any multi-label sample, the local fuzzy membership to co-occurrence or mutual state for label pair is examined based on kernelized fuzzy rough sets . The 3WD local label relevance of each sample is confirmed, that is, positive, negative or neutral. The global application strategy for multi-label classification is utilized to avoid over-fitting induced by local mining strategy. Based on the integral mean of the distribution of 3WD local label relevance in multi-label sample space, two different versions of empirical label relevance are constructed. By constraining the relative position between sub-separation hyperplanes , the 3WD label correlation distribution-based model for multi-label joint classification is designed. The experiment results on fifteen real world multi-label datasets reflect that our algorithm achieves good classification ability and versatility. The impact of core parameters on learning performance is also dissected.},
  archive      = {J_ASOC},
  author       = {Xiaoya Che and Degang Chen and Jiang Deng and Jusheng Mi},
  doi          = {10.1016/j.asoc.2023.110950},
  journal      = {Applied Soft Computing},
  pages        = {110950},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exploiting local label correlation from sample perspective for multi-label classification via three-way decision theory},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neutrosophic hyperbolic programming strategy for uncertain
multi-objective transportation problem. <em>ASOC</em>, <em>149</em>,
110949. (<a href="https://doi.org/10.1016/j.asoc.2023.110949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper suggests a neutrosophic hyperbolic programming approach based on the hyperbolic membership function to get an efficient solution for an uncertain multi-objective transportation problem where the uncertain normal distribution presents uncertainty. This technique provides flexibility to the decision-maker to select various confidence levels according to his/her own choice and analyze the satisfaction percentage of the obtained optimal compromise solution along with its truth, indeterminacy , and falsity values. In this approach, the parameters are changed from uncertain to crisp by using the inverse measure theorem, and the neutrosophic non-linear hyperbolic membership functions are used to convert the whole model into a non-linear programming problem in a neutrosophic environment. The advantages of our recommended method in real-world applications are effectively illustrated by a numerical illustration. This illustration also makes it easier to compare the results attained using our suggested strategy to those obtained using the methodologies put forward by Uddin et al. (2021) and Miah et al. (2022). Also, MATLAB software (version R2019a,64bit(win64)) is used to develop the code for the solution of this non-linear uncertain multi-objective transportation problem .},
  archive      = {J_ASOC},
  author       = {Anesh Kumar and Pitam Singh and Yadvendra Kacher},
  doi          = {10.1016/j.asoc.2023.110949},
  journal      = {Applied Soft Computing},
  pages        = {110949},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neutrosophic hyperbolic programming strategy for uncertain multi-objective transportation problem},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligently adjusted carbon price forecasting approach
based on breakpoints segmentation, feature selection and adaptive
machine learning. <em>ASOC</em>, <em>149</em>, 110948. (<a
href="https://doi.org/10.1016/j.asoc.2023.110948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate carbon price prediction is conducive to the stable operation and development of carbon financial markets. Affected by major policies and economies, the laws of carbon price may show huge changes, generating various breakpoints that hinder the prediction work. Therefore, a hybrid model based on adaptive segmentation and feature clustering is developed to forecast carbon price, which responds to the growing requirement of the high precision prediction. It solves the issue that the accuracy and stability of model are limited by the breakpoint via the adaptive processing and the fully learning for data. Meanwhile, the proposed adaptive structure improves the robustness and adaptability of prediction model, achieving accurate prediction for carbon emission trading markets with different features. The eight carbon emission trading markets in China are used to evaluate prediction performance. The obtained results indicated that the proposed model was effective and robust, with the average mean absolute error and root mean square error of only 0.2272 and 0.3321, respectively. According to the comparative analysis, the segmentation based on breakpoint and adaptive prediction based on feature clustering improve the model forecasting accuracy by 69.87\% and 45.51\%, respectively. Hence, the proposed model can meet the requirements of carbon financial markets and provide a benchmark for the carbon emission reduction work.},
  archive      = {J_ASOC},
  author       = {Shunyu Zhao and Yelin Wang and Gen Deng and Ping Yang and Zhi Chen and Youjie Li},
  doi          = {10.1016/j.asoc.2023.110948},
  journal      = {Applied Soft Computing},
  pages        = {110948},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligently adjusted carbon price forecasting approach based on breakpoints segmentation, feature selection and adaptive machine learning},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aquila-particle swarm based cooperative search optimizer
with superpixel techniques for epithelial layer segmentation.
<em>ASOC</em>, <em>149</em>, 110947. (<a
href="https://doi.org/10.1016/j.asoc.2023.110947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of epithelial layers from oral histopathology images plays a crucial role for early detection of oral cancer disease. As a result, more accurate segmentation of this layer is of the utmost importance for a Computer Aided Diagnosis (CAD) system. Therefore, this paper presents a superpixel image-based clustering technique using an improved Nature-Inspired Optimization Algorithm (NIOA), called the Cooperative Search (CS) algorithm. Here, superpixel image is utilized to construct a faster and noise-resistant oral histopathology image clustering method . The designed CS, on the other hand, is based on two popular NIOAs, the Aquila Optimizer (AO) and the Particle Swarm Optimizer (PSO), has better exploration-exploitation abilities. This CS has been used in the image clustering field to circumvent the problem of local optima trapping. On the other hand, a comparative investigation of three popular superpixel strategies with the proposed CS optimizer has been conducted to determine the optimal superpixel strategy for epithelial layer segmentation from clean as well as noisy oral histopathology images. Finally, the optimal superpixel strategy with CS has been tested with other state-of-the-art image segmentation techniques in the epithelial layer segmentation domain. The results obtained by the proposed segmentation technique are 98.82\%, 96.70\%, 97.60\%, and 95.25\%, in terms of average Accuracy, MCC, Dice, and Jaccard, respectively, which all are better than the cutting-edge segmentation methods . The proposed segmentation model is also evaluated over the leaf segmentation model and achieved better results visually and numerically. The optimization efficiency of the CS has also been measured over CEC2019 mathematical benchmark test functions and produced competitive results compared to other tested NIOAs.},
  archive      = {J_ASOC},
  author       = {Buddhadev Sasmal and Arunita Das and Krishna Gopal Dhal and Swarnajit Ray},
  doi          = {10.1016/j.asoc.2023.110947},
  journal      = {Applied Soft Computing},
  pages        = {110947},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Aquila-particle swarm based cooperative search optimizer with superpixel techniques for epithelial layer segmentation},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-phase decision-making approach for supplier
selection and order allocation with corporate social responsibility.
<em>ASOC</em>, <em>149</em>, 110946. (<a
href="https://doi.org/10.1016/j.asoc.2023.110946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite attention of the developed countries towards social responsibility, this issue is still not well-established in developing countries. The short-term management period in these countries, numerous management changes in organizations, and incomplete, vague, and uncertain information make the organization’s senior management pays attention only to economic issues. Accordingly, this study outlines a multi-phase methodology to address the challenges of integrating Corporate Social Responsibility (CSR) in Supplier Selection and Order Allocation Problem (SSOAP) in developing countries. A multi-objective model is developed based on traditional criteria, which takes into account the concerns of senior managers. Suppliers are then evaluated based on their CSR practices using linguistic terms and fuzzy numbers to account for the vagueness and uncertainty in these practices. The study also applies the Best Worst Method (BWM) to calculate the weight that is integrated into different stages of the methodology. Interval-valued CSR scores for each supplier are employed to estimate the interval-valued CSR score for each supply chain belonging to solutions obtained from the mathematical model. Additionally, a method based on the TOPSIS structure is developed to rank solutions. The proposed method maintains an interval-based structure throughout its steps and ranks alternatives according to the interval relative closeness index using the order relation between two intervals. The study is validated with a numerical example and a real case problem. The results are discussed with respect to the feedback provided from the case managers and a comprehensive sensitivity analysis is performed to investigate the behavior of the model under different circumstances. The model demonstrates that incorporating CSR practices alongside traditional criteria can alleviate senior managers&#39; economic worries. Furthermore, the community stands to gain from such practices. Our methodology also highlights the impact of CSR on supplier purchases and verifies incorporating CSR practices alongside traditional criteria can enhance sourcing efficacy.},
  archive      = {J_ASOC},
  author       = {Hesam Shidpour and Mohsen Shidpour and Erfan Babaee Tirkolaee},
  doi          = {10.1016/j.asoc.2023.110946},
  journal      = {Applied Soft Computing},
  pages        = {110946},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-phase decision-making approach for supplier selection and order allocation with corporate social responsibility},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DResInceptionNasNet method for offline grounding detection
of distribution networks. <em>ASOC</em>, <em>149</em>, 110945. (<a
href="https://doi.org/10.1016/j.asoc.2023.110945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Almost all existing distribution network grounding tests are online. The existing offline distribution network grounding detection methods are all manual handling methods, which are time-consuming, labor-intensive and cause damage to the environment. To save labor cost, reduce environmental damage, and improve detection efficiency and accuracy, this study designs an offline grounding detection device for distribution networks and proposes a DResInceptionNasNet method for grounding detection. The DResInceptionNasNet network, which is trained offline, can predict the location of distribution network grounding wire presence offline and accurately. In this study, the DResInceptionNasNet network is experimented in IEEE 9 and IEEE 39 systems. The obtained experimental results verify that the DResInceptionNasNet network has an average accuracy of at least 2.93\% higher than other 17 network models such as ResNet, NasNet, and Inception V3. In addition, the developed distribution network offline detection device and the proposed DResInceptionNasNet network can effectively and accurately detect the grounding location of distribution networks.},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Jiahao Huang},
  doi          = {10.1016/j.asoc.2023.110945},
  journal      = {Applied Soft Computing},
  pages        = {110945},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DResInceptionNasNet method for offline grounding detection of distribution networks},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive consensus model in large-scale group decision
making with noncooperative and compromising behaviors. <em>ASOC</em>,
<em>149</em>, 110944. (<a
href="https://doi.org/10.1016/j.asoc.2023.110944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, different feedback mechanisms have been reported in many consensus models to improve decision levels. However, the improvement of decision level often leads to the reduction of decision efficiency, which has been rarely considered in existing consensus models. This paper proposes an adaptive consensus model consisting of automatic strategy and interactive strategy, which are implemented in different consensus stages to balance decision efficiency and decision level. In addition, the behavior diversity of decision makers (DMs) is often unavoidable, such as noncooperative behavior, which brings greater complexity to the consensus reaching. Meanwhile, cooperative behavior is usually accompanied by compromise behavior. Considering that the compromise behavior of DMs will change subgroup structure, dynamic cluster analysis is performed in the consensus reaching process. On the premise of dynamic clustering, the traditional weight penalty mechanism will fail to manage the noncooperative behaviors of subgroups. To this end, this paper proposes a new penalty mechanism. The proposed adaptive consensus model is applied to the selection of cities for establishing the freight hub. Finally, some numerical simulations and comparative analyses are presented to verify the effectiveness of the proposed model.},
  archive      = {J_ASOC},
  author       = {Cui Shang and Runtong Zhang and Xiaomin Zhu},
  doi          = {10.1016/j.asoc.2023.110944},
  journal      = {Applied Soft Computing},
  pages        = {110944},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive consensus model in large-scale group decision making with noncooperative and compromising behaviors},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated recommender system for multi-day tourist
itinerary. <em>ASOC</em>, <em>149</em>, 110942. (<a
href="https://doi.org/10.1016/j.asoc.2023.110942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning a personalized itinerary that satisfies the preferences and limitations of a tourist can be a complex task. The challenge is further compounded by the lack of information about tourists&#39; tastes, which makes it difficult to ensure their satisfaction, and the impracticality of extracting this information through extensive questioning. To overcome these challenges, this paper presents a novel integrated recommender system for predicting tourists&#39; interests and planning multi-day itineraries. The system utilizes a hybrid prediction method based on interests of similar tourists to attractions, attraction categorization, and the similarity between touristic attractions, to predict the tourist&#39;s tastes. The optimization and planning problem is formulated as a multi-day categorized orienteering problem with count-dependent profits, with the goal of finding multiple paths between points of interest to maximize the cumulative profit. As the problem is NP-hard, a hybrid meta-heuristic algorithm combining Genetic Algorithm and Variable Neighborhood Descent is proposed to solve it. The performance of the hybrid prediction method and the hybrid meta-heuristic algorithm is evaluated on the MovieLens and Tisligirides datasets. Finally, the proposed integrated recommender system is thoroughly evaluated on a real-world dataset from Tehran, demonstrating its effectiveness in recommending personalized multi-day itineraries.},
  archive      = {J_ASOC},
  author       = {Faezeh Ghobadi and Ali Divsalar and Hossein Jandaghi and Reza Barzegar Nozari},
  doi          = {10.1016/j.asoc.2023.110942},
  journal      = {Applied Soft Computing},
  pages        = {110942},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated recommender system for multi-day tourist itinerary},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective effort-aware defect prediction approach
based on NSGA-II. <em>ASOC</em>, <em>149</em>, 110941. (<a
href="https://doi.org/10.1016/j.asoc.2023.110941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effort-Aware Defect Prediction (EADP) technique sorts software modules by the defect density and aims to find more bugs when testing a certain number of Lines of Code (LOC). The existing EADP methods ignore the number of required inspected modules and thus resulting in more testing cost. Therefore, we propose a multi-objective effort-aware defect prediction approach based on NSGA-II named MOOAC for EADP, which aims to maximize the Proportion of the found Bugs (PofB@20\%) and minimize the Proportion of Module Inspected (PMI@20\%) when inspecting the top 20\% LOC. MOOAC firstly trains a random forest classification model . Then, it builds a logistic regression model , and utilizes the NSGA-II algorithm to generate the coefficient vector of the model by maximizing the PofB@20\% value and minimizing the PMI@20\% value simultaneously. In the model prediction phase , MOOAC firstly employs the built random forest classifier to decide whether modules are defective. Next, the predicted defective modules are first inspected based on the ratio between the predicted defect probability by the logistic regression model and LOC, which can make testers to find more bugs and test as fewer LOC as possible. The clean modules are then inspected to reduce the Initial False Alarms (IFA), if there is still the testing budget left. The results show that MOOAC exhibits the best overall performance on the PofB@20\% and PMI@20\%. In other words, MOOAC enables testers to identify more bugs per 1\% module.},
  archive      = {J_ASOC},
  author       = {Xiao Yu and Liming Liu and Lin Zhu and Jacky Wai Keung and Zijian Wang and Fuyang Li},
  doi          = {10.1016/j.asoc.2023.110941},
  journal      = {Applied Soft Computing},
  pages        = {110941},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective effort-aware defect prediction approach based on NSGA-II},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ARIMA-LSTM model for predicting volatile agricultural
price series with random forest technique. <em>ASOC</em>, <em>149</em>,
110939. (<a href="https://doi.org/10.1016/j.asoc.2023.110939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning mechanism is establishing itself as a promising area for modelling and forecasting complex time series over conventional statistical models. In this article, focus has been made on presenting a machine learning algorithm with special attention to deep learning model in form of a potential alternative to statistical models such as Autoregressive Integrated Moving Average (ARIMA) and ARIMA-Generalised Autoregressive Conditional Heteroscedasticity (GARCH) models. Further, an improved hybrid ARIMA-Long Short-Term Memory (LSTM) model based on the random forest lag selection criterion has been introduced. ARIMA model has been used to estimate the mean effect and the GARCH model is employed with the residuals obtained from the ARIMA model to estimate the volatile behaviour of the series. ARIMA-GARCH models act as superior statistical models over ARIMA models based on the lowest AIC and BIC values. LSTM model is employed on all normalised training data series. After which we built a comparison scenario independently between ARIMA, ARIMA-GARCH, LSTM and ARIMA-LSTM models on forecasting accuracy in terms of the lowest RMSE, MAPE and MASE values. The proposed random forest-based ARIMA-LSTM model proved its superiority over the conventional statistical models with an improvement to the tune of 8–25\% for RMSE, 2–28\% for MAPE and 2–29\% for MASE. The proposed hybrid model has been successfully applied to volatile monthly price indices of pulses namely gram, moong and urad. This piece of work will enrich the literature on machine learning and further intrigue researchers to apply it to various other volatile data sets.},
  archive      = {J_ASOC},
  author       = {Soumik Ray and Achal Lama and Pradeep Mishra and Tufleuddin Biswas and Soumitra Sankar Das and Bishal Gurung},
  doi          = {10.1016/j.asoc.2023.110939},
  journal      = {Applied Soft Computing},
  pages        = {110939},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ARIMA-LSTM model for predicting volatile agricultural price series with random forest technique},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bridging machine learning and weighted residual methods for
delay differential equations of fractional order. <em>ASOC</em>,
<em>149</em>, 110936. (<a
href="https://doi.org/10.1016/j.asoc.2023.110936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, an attempt has been made to build a bridge between a machine learning model called Least-Squares Support Vector Regression (LS-SVR) and Weighted Residual Methods (WRMs) to solve delay differential equations in fractional derivative order. Our machine learning algorithm , which is influenced by the collocation method, uses Legendre polynomials as a kernel function to approximate the solution of linear and nonlinear delay differential equations, including the Lane–Emden pantograph equation and some fractional cases. In the proposed algorithm, the estimated solution of delay differential equations is approximated as a linear combination of M degrees of Legendre polynomials and a set of weights that are learned during the fitting process. The roots of the Legendre functions are utilized as training data to develop the algorithm. The effectiveness of our method is demonstrated through the use of numerical graphs and tables. A comparative analysis of the numerical results has shown that the proposed method yields better accuracy and convergence compared to other numerical techniques .},
  archive      = {J_ASOC},
  author       = {Tayebeh Taheri and Alireza Afzal Aghaei and Kourosh Parand},
  doi          = {10.1016/j.asoc.2023.110936},
  journal      = {Applied Soft Computing},
  pages        = {110936},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bridging machine learning and weighted residual methods for delay differential equations of fractional order},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive multi-objective competitive swarm optimization
algorithm based on kinematic analysis for municipal solid waste
incineration. <em>ASOC</em>, <em>149</em>, 110925. (<a
href="https://doi.org/10.1016/j.asoc.2023.110925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimization for the municipal solid waste incineration process is considered as a valuable technique to improve energy recovery and reduce pollutant emission. However, the complex mechanism analysis and multimodal problem of the municipal solid waste incineration process set challenges for both the modeling and optimization studies. To overcome this problem, an adaptive multi-objective optimization for the municipal solid waste incineration process is proposed in this paper. First, a bi-objective model of the municipal solid waste incineration , the basis for optimization, is established based on mass balance and energy balance, which takes furnace temperature and flue gas oxygen content as decision variables to mathematically deduce the generated heat and exhaust gases. Second, an adaptive multi-objective competitive swarm optimization algorithm is proposed for the optimization of the municipal solid waste incineration process. Two-step competition and multi-strategy learning are designed to provide a clear division of labor for particles and a novel idea for detecting evolutionary environment is proposed based on kinematic analysis of particles. Finally, relevant experiments are conducted on benchmark instances and the municipal solid waste incineration optimization model. The proposed algorithm shows promising convergence, diversity, fastness by comparing with several representative and state-of-the-art algorithms. The proposed algorithm achieves the optimization effects with 4.36\% improvement of the available heat for power generation and 4.13\% reduction of the exhaust gas assessment in the optimization of the municipal solid waste incineration process.},
  archive      = {J_ASOC},
  author       = {Weimin Huang and Haixu Ding and Junfei Qiao},
  doi          = {10.1016/j.asoc.2023.110925},
  journal      = {Applied Soft Computing},
  pages        = {110925},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive multi-objective competitive swarm optimization algorithm based on kinematic analysis for municipal solid waste incineration},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physics-informed digital twin for wind turbine main bearing
fatigue: Quantifying uncertainty in grease degradation. <em>ASOC</em>,
<em>149</em>, 110921. (<a
href="https://doi.org/10.1016/j.asoc.2023.110921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of prognostics and health management for industrial equipment, digital twins stand out as essential tools. Wind park operators can harness the potential of digital twins to monitor component health, enabling proactive measures to optimize energy production while reducing maintenance costs. This study introduces a novel hybrid digital twin application tailored for monitoring wind turbine main bearing fatigue. It combines physics-based and data-driven kernels to address variable grease quality and biased observations. Our study covers model initialization, cross-validation, fleet management, and the influence of sampled turbines on prediction. Our approach offers two key advantages: (a) It reduces the need for extensive datasets, typical in other machine learning methods, by incorporating physics-based knowledge into the network architecture , and (b) it quantifies output uncertainties using tailored network layers and loss functions. Our findings conclude that even under compound uncertainty scenario, our hybrid model can estimate fleet unreliability only off by 4.7 weeks. However, it is essential to note that computational costs are tied to data processing, similar to other recurrent neural networks . A limitation is that the physics-based kernels must align with common machine learning linear algebra practices.},
  archive      = {J_ASOC},
  author       = {Yigit A. Yucesan and Felipe A.C. Viana},
  doi          = {10.1016/j.asoc.2023.110921},
  journal      = {Applied Soft Computing},
  pages        = {110921},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Physics-informed digital twin for wind turbine main bearing fatigue: Quantifying uncertainty in grease degradation},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of hesitant fuzzy VIKOR methods for supplier
selection. <em>ASOC</em>, <em>149</em>, 110920. (<a
href="https://doi.org/10.1016/j.asoc.2023.110920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supplier selection is a complex decision-making process which is influenced by uncertainty and DM hesitation. To deal with this problem, various studies have proposed the use of methods based on Fuzzy Set Theory and its extensions. There are several varieties of Hesitant Fuzzy Linguistic VIKOR (HFLVIKOR) methods in the literature which can use two or more linguistic terms in each evaluation and can generate solutions based on the group’s utility or individual regret. Despite these benefits, we have not found comparative studies involving HFLVIKOR methods to supplier selection. Given this gap, this study applies and compares Extended Hesitant Fuzzy Linguistic VIKOR (EHFLVIKOR) methods and Hesitant Fuzzy Linguistic Term Set VIKOR (PDHFLVIKOR) methods with Possibility Distributions within the supplier selection context. This application was realized using a service company. We also performed 8 scenario simulations to analyze the effect of including other criteria and alternatives. Despite the identified limitations of each method, we conclude that both are appropriate for dealing with group decision making under uncertainty and require little effort in their data collection. The PDHFLVIKOR results were more consistent in maintaining the ranking and the compromise solution in the real application as well as simulated scenarios. Unlike PDHFLVIKOR, EHFLVIKOR loses information when there is a consensus. On the other hand, because it makes it possible to create subgroups of DMs, EHFLVIKOR appears to be more appropriate for companies in which there is more than one DM involved in supplier selection for each functional area. This study’s results will help managers and academics in selecting the most appropriate method for supplier selection.},
  archive      = {J_ASOC},
  author       = {Mery Ellen Brandt de Oliveira and Francisco Rodrigues Lima-Junior and Nadya Regina Galo},
  doi          = {10.1016/j.asoc.2023.110920},
  journal      = {Applied Soft Computing},
  pages        = {110920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparison of hesitant fuzzy VIKOR methods for supplier selection},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale SE-residual network with transformer encoder for
myocardial infarction classification. <em>ASOC</em>, <em>149</em>,
110919. (<a href="https://doi.org/10.1016/j.asoc.2023.110919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an increase in the number of deaths caused by heart disease. The design of computer-aided systems for the diagnosis of heart diseases, especially myocardial infarction (MI), has become a hot topic. The diagnosis of MI is a classification task . Electrocardiogram (ECG) is one of the most important tools for diagnosing heart disease. ECG signals are classified into corresponding MI categories based on their extracted features. Thus, the diagnosis of MI depends on the feature extraction of the ECG signal. Because signal processing methods, such as wavelet transform , may not effectively extract hidden features, researchers nowadays turn to using convolutional neural networks to solve the problem of hidden feature extraction. However, this approach ignores the role of temporal information. we proposed a new network called Multi-scale SE-Residual Network with Transformer encoder (MRTNet) to extract hidden features and temporal information features better. The data processed are localized to heartbeat units based on the way cardiologists diagnose myocardial infarction. Inspired by multi-scale learning, the sampling module was designed to acquire heartbeat units at different scales. We provided a processing model for multi-scale data, called the feature extraction module. The residual network with SE block and Customized Pooling Component (CPC) was used to extract global and local features of the heartbeat units. A Transformer encoder was used to extract the common features of the previous module’s output. The common features at different scales were fused to provide the basis for the classification task . The experiment was conducted on the public PTB-XL dataset. MRTNet achieved a higher accuracy and F1 score than other models by approximately 2\%. Furthermore, MRTNet demonstrated a superior AUC of approximately 0.77. It is a superior classifier when compared to other models. The results demonstrate the effectiveness of the designed multi-scale architecture. And it provides a way into exploring further potential information from ECG signals.},
  archive      = {J_ASOC},
  author       = {Qingyu Yao and Luming Zhang and Wenguang Zheng and Yuxi Zhou and Yingyuan Xiao},
  doi          = {10.1016/j.asoc.2023.110919},
  journal      = {Applied Soft Computing},
  pages        = {110919},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale SE-residual network with transformer encoder for myocardial infarction classification},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BERT models for brazilian portuguese: Pretraining,
evaluation and tokenization analysis. <em>ASOC</em>, <em>149</em>,
110901. (<a href="https://doi.org/10.1016/j.asoc.2023.110901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in language representation using neural networks have made it viable to transfer the learned internal states of large pretrained language models (LMs) to downstream natural language processing (NLP) tasks. This transfer learning approach improves the overall performance on many tasks and is highly beneficial when labeled data is scarce, making pretrained LMs valuable resources specially for languages with few annotated training examples. In this work, we train BERT (Bidirectional Encoder Representations from Transformers) models for Brazilian Portuguese, which we nickname BERTimbau. We evaluate our models on three downstream NLP tasks: sentence textual similarity, recognizing textual entailment, and named entity recognition . Our models improve the state-of-the-art in all of these tasks, outperforming Multilingual BERT and confirming the effectiveness of large pretrained LMs for Portuguese. We release our models to the community hoping to provide strong baselines for future NLP research: https://github.com/neuralmind-ai/portuguese-bert .},
  archive      = {J_ASOC},
  author       = {F.C. Souza and R.F. Nogueira and R.A. Lotufo},
  doi          = {10.1016/j.asoc.2023.110901},
  journal      = {Applied Soft Computing},
  pages        = {110901},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BERT models for brazilian portuguese: Pretraining, evaluation and tokenization analysis},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diagnosis of alzheimer’s disease via intuitionistic fuzzy
least squares twin SVM. <em>ASOC</em>, <em>149</em>, 110899. (<a
href="https://doi.org/10.1016/j.asoc.2023.110899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative disorders like Alzheimer’s disease (AD) are irreversible and show atrophies in the area of the cerebral cortex of brain. AD leads to loss of memory and other cognitive impairments. The AD subjects are evaluated based on magnetic resonance imaging scans. The data may have the problem of class imbalance, noise and outliers which is a great challenge for classification. Support vector machines and twin support vector machine-based classifiers may not effectively deal with these problems as both these models assume that all the samples are equally important for the separating hyperplane . To overcome these issues, we propose intuitionistic fuzzy least square twin support vector machine for class imbalance problems (IFLSTSVM) and class specific-IFLSTSVM (CS-IFLSTSVM). To minimize the effects of class imbalance, the samples are appropriately weighted to minimize their effect on the optimal hyperplane . Moreover, we use intuitionistic fuzzy scores to overcome the issues of noise and outliers. Intuitionistic fuzzy score values generate appropriate weights by considering both the distance of the samples from the class centroid as well as the heterogeneity of the samples. The proposed models IFLSTSVM and CS-IFLSTSVM are efficient as they need to solve a system of linear equations . In Alzheimer’s disease diagnosis, the proposed IFLSTSVM and CS-IFLSTSVM models showed better performance in MCI_vs_AD and CN_vs_MCI cases, respectively. Moreover, the proposed models showed better performance in the diagnosis of breast cancer classification. The statistical analysis carried out over KEEL and UCI data leads to the superiority of the proposed models. The source code of the proposed model is available at https://github.com/mtanveer1/Diagnosis-of-Alzheimer-s-disease-via-Intuitionistic-fuzzy-least-squares-twin-SVM .},
  archive      = {J_ASOC},
  author       = {M.A. Ganaie and Anuradha Kumari and Anouck Girard and Josephine Kasa-Vubu and M. Tanveer and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.asoc.2023.110899},
  journal      = {Applied Soft Computing},
  pages        = {110899},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diagnosis of alzheimer’s disease via intuitionistic fuzzy least squares twin SVM},
  volume       = {149},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flood susceptibility mapping using AutoML and a deep
learning framework with evolutionary algorithms for hyperparameter
optimization. <em>ASOC</em>, <em>148</em>, 110846. (<a
href="https://doi.org/10.1016/j.asoc.2023.110846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flooding is one of the most common natural hazards that have extremely detrimental consequences. Understanding which areas are vulnerable to flooding is crucial to addressing these effects. In this work, we use machine learning models and Automated machine learning (AutoML) systems for flood susceptibility mapping in Kerala, India. In particular, we used a three-dimensional convolutional neural network (CNN) architecture for this purpose. The CNN model was assisted with hyperparameter optimization techniques that combine Bayesian optimization with evolutionary algorithms like differential evolution and covariance matrix adaptation evolutionary strategies. The performances of all models are compared in terms of cross-entropy loss, accuracy, precision, recall, area under the curve (AUC) and kappa score. The CNN model shows better performance than the AutoML models. Evolutionary algorithm-assisted hyperparameter optimization methods improved the efficiency of the CNN model by 4 and 9 percent in terms of accuracy and by 0.0265 and 0.0497 with reference to the AUC score.},
  archive      = {J_ASOC},
  author       = {Amala Mary Vincent and Parthasarathy K.S.S. and P. Jidesh},
  doi          = {10.1016/j.asoc.2023.110846},
  journal      = {Applied Soft Computing},
  pages        = {110846},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Flood susceptibility mapping using AutoML and a deep learning framework with evolutionary algorithms for hyperparameter optimization},
  volume       = {148},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three-way fusion measures and three-level feature selections
based on neighborhood decision systems. <em>ASOC</em>, <em>148</em>,
110842. (<a href="https://doi.org/10.1016/j.asoc.2023.110842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty measures exhibit algebraic and informational perspectives, and the two-view measure integration facilitates feature selections in classification learning . According to neighborhood decision systems (NDSs), two basic algorithms of feature selections (called JE-FS and DE-FS) already exist by using joint and decisional entropies, respectively, but they have advancement space for informationally fusing algebraic measures. In this paper on NDSs, three-way fusion measures are systematically constructed by combining three-way algebraic and informational measures, and thus three-level feature selections are hierarchically investigated by using corresponding monotonic and nonmonotonic measures and strategies. At first, the accuracy, granularity , and composite granularity-accuracy constitute three-way algebraic measures, while the joint , conditional, and decisional entropies (JE, CE , DE) formulate three-way informational measures. Then, three-way algebraic and informational measures are combined via normalization and multiplication, so three-way fusion measures based on JE , CE , DE are established. These new measures acquire granulation monotonicity and nonmonotonicity. Furthermore by relevant measures and monotonicity/nonmonotonicity, three-level feature selections (with null, single, and double fusion levels) related to JE , CE, DE are proposed, and corresponding heuristic algorithms are designed by monotonic and nonmonotonic principles. 4 × 3 = 12 4×3=12 selection algorithms comprehensively emerge, and they extend and improve current JE-FS and DE-FS. Finally by data experiments, related uncertainty measures and granulation properties are validated, and all 12 selection algorithms are compared in classification learning . As a result, new algorithms outperform JE-FS and DE-FS for classification performance, and the algorithmic improvements accord with the fusion-hierarchical deepening and entropy-systematic development of uncertainty measures.},
  archive      = {J_ASOC},
  author       = {Hongyuan Gou and Xianyong Zhang and Jilin Yang and Zhiying Lv},
  doi          = {10.1016/j.asoc.2023.110842},
  journal      = {Applied Soft Computing},
  pages        = {110842},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way fusion measures and three-level feature selections based on neighborhood decision systems},
  volume       = {148},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long-term water quality model in aquaculture based on
dilated convolution feature fusion and interactive learning.
<em>ASOC</em>, <em>148</em>, 110801. (<a
href="https://doi.org/10.1016/j.asoc.2023.110801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of water quality parameters is of great significance for maintaining water stability and controlling marine environments. However, due to the temporal nature of water quality parameters and the influence of surrounding changeable environments, water quality is characterized by complex nonlinearity and instability. In current prediction research, there are many training parameters and poor generalization ability , and its prediction accuracy cannot be guaranteed. Thus, this paper proposes a network model called Feature Fusion Interactive learning network (FFINet) based on dilated convolution feature fusion and interactive learning. FFINet is composed of the SCFINet module and TCINet module in parallel, and the two modules accept the same original sequence. The SCFINet module captures the local features of the sequence through feature fusion and proposes interactive learning to extract the trend, periodicity and irregularity information of the subsequence. The TCINet module uses expansion convolution to capture the long-term dependencies of sequences and uses interactive learning to compensate for information loss during separation, can process sequences simultaneously in parallel mode, and can retain irregular information of sequences. The model adds a dual residual connection to weaken the problem of model gradient vanishing, and each layer outputs sequences of the same length to be added as input to the lower layer. Dissolved oxygen concentration experiments are conducted at multiple marine ranches, and the results of the proposed model are compared with those of other deep learning models, showing that FFINet has better prediction performance than existing approaches. This paper provides important theoretical guidance for sustainable aquaculture development.},
  archive      = {J_ASOC},
  author       = {Dashe Li and Weijie Zhao},
  doi          = {10.1016/j.asoc.2023.110801},
  journal      = {Applied Soft Computing},
  pages        = {110801},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Long-term water quality model in aquaculture based on dilated convolution feature fusion and interactive learning},
  volume       = {148},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variation-learning high-resolution network for capsulorhexis
recognition of cataract surgery. <em>ASOC</em>, <em>147</em>, 110841.
(<a href="https://doi.org/10.1016/j.asoc.2023.110841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cataract surgery, capsulorhexis is the first, one of the most critical steps. However, recognizing the margin of rhexis during capsulorhexis is challenging and has rarely been studied due to its intraoperative invisibility, variable shape, and lack of a proprietary labeled dataset. Capsulorhexis margin recognition is essential for robot-assisted surgery, surgical assessment, and intraoperative reference. To solve the problem, this paper proposes a creative dynamic margin recognition structure called THRC, which includes a novel segmentation network, THRNet, and a compensation block. First, the dilated HRNet is designed to expand the receptive field while maintaining high-resolution representation for better detail preservation. Second, cascaded transformer blocks are proposed. The channel transformer block (CTB) is developed to include fixed and dynamic weights while generating global context dependency for nonrigid tissue variation pattern learning. The layer transformer block (LTB) with redistributed transformer variables is then developed to fuse multi-resolution features effectively for better multi-scale semantic segmentation. Moreover, the compensation block compensates for inter-frame movement by key-point registration and recovers a more realistic margin by a smoothing process. Finally, to evaluate the proposed structure, we establish the capsulorhexis dataset CapsSeg. The THRNet achieves 78.24\% mean intersection over union (mIoU) and 86.59\% mean Dice (mDice), making it equivalent in performance to state-of-the-art networks, but with a lower computational cost. The feasibility of the compensation block is also visualized.},
  archive      = {J_ASOC},
  author       = {Gui-Bin Bian and Wen-Qian Yue and Zhen Li and Li Zhang and Shuai Zhang and Wei-Peng Liu and Shuo Li and Elias Paulino Medeiros and Wan-Qing Wu and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2023.110841},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110841},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Variation-learning high-resolution network for capsulorhexis recognition of cataract surgery},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brake light detection of vehicles using differential
evolution based neural architecture search. <em>ASOC</em>, <em>147</em>,
110839. (<a href="https://doi.org/10.1016/j.asoc.2023.110839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of brake light status in vehicles is crucial for anticipating speed changes and preventing rear-end collisions for autonomous driving systems. Existing literature presents two types of methods for brake light detection: hand-crafted feature-based methods and deep learning-based methods. However, hand-crafted methods often struggle to capture brake light characteristics accurately in real-life conditions. In contrast, deep learning-based systems can adapt to diverse brake light variations across different vehicle types and environments. Nevertheless, manually designing Deep Neural Network (DNN) models requires expertise and is prone to errors. To address this limitation, we propose a novel approach that leverages Neural Architecture Search (NAS) to automatically generate optimal DNN architectures for object detection tasks, specifically for brake light detection. In contrast to the existing NAS approaches that focus on classification models, our technique explores NAS for object detection tasks. We employ a modified Differential Evolution algorithm, incorporating evaluation correction-based selection for mutation and species protection-based selection to identify the optimal DNN backbone architecture with optimal training parameters. The proposed approach achieved mean accuracy of 89.73\%, and 88.90\% on four-wheeler datasets CaltechGraz and UC Merced Vehicle Rear Signal datasets, respectively, and it has achieved 97.97\% on the proposed two-wheeler NITW-MBS dataset. The proposed approach’s generalization capability and practical applicability are ascertained through cross-dataset evaluation and experiments on real-world traffic video.},
  archive      = {J_ASOC},
  author       = {Medipelly Rampavan and Earnest Paul Ijjina},
  doi          = {10.1016/j.asoc.2023.110839},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110839},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Brake light detection of vehicles using differential evolution based neural architecture search},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gaussian random walk salp swarm algorithm for optimal
dynamic charging of electric vehicles. <em>ASOC</em>, <em>147</em>,
110838. (<a href="https://doi.org/10.1016/j.asoc.2023.110838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salp swarm algorithm (SSA) is one of the recently developed meta-heuristic optimization algorithms. Since SSA outperforms other swarm-based algorithms, it has recently been employed in various applications, including feature selection, neural network training and renewable energy systems . In this paper, an improved salp swarm algorithm based on a Gaussian random walk is proposed, which enhances the algorithm’s performance particularly for multidimensional constrained global optimization problems . The integration of a Gaussian random walk into the algorithm balances between its exploration and exploitation capabilities. Furthermore, the proposed algorithm introduces a new re-dispersion strategy in the case of stagnation at local optimum points, which considerably enhances exploration. The performance of the proposed algorithm is evaluated using a set of twenty-three benchmark test functions and is compared to the performance of prevalent metaheuristic algorithms . Statistical analysis is performed using Wilcoxon signed-rank test, and the results reveal considerable improvement over the competing algorithms. Then, 21 real-world optimization problems are used to further evaluate the efficacy of the proposed algorithm. The winners of the CEC2020 Competition on Real-World Single Objective Constrained Optimization , SASS, sCMAgES, EnMODE, and COLSHADE algorithms, are used as four comparable algorithms in the real-world optimization problems. The convergence curves and simulations provide very competitive performance compared to the comparative algorithms. The proposed algorithm is used to address one of the most challenging real-world constrained problems in power system applications, namely, determining the optimal charging schedule for electric vehicles at charging stations. The results reveal that the proposed algorithm outperforms other existing algorithms in terms of increasing the charging revenues and achieving maximum power grid stability.},
  archive      = {J_ASOC},
  author       = {Mohamed Ahmed and Sara H. Kamel and Nabil H. Abbasy and Yasmine Abouelseoud},
  doi          = {10.1016/j.asoc.2023.110838},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110838},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gaussian random walk salp swarm algorithm for optimal dynamic charging of electric vehicles},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiobjective sparrow search feature selection with sparrow
ranking and preference information and its applications for
high-dimensional data. <em>ASOC</em>, <em>147</em>, 110837. (<a
href="https://doi.org/10.1016/j.asoc.2023.110837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the dimensionality of high-dimensional data and enhance its classification accuracy, feature selection can be regarded as a multiobjective optimization problem that can be addressed by evolutionary computation algorithms with promising results. However, balancing the convergence and diversity of nondominated solutions remains challenging. To address these issues, this paper proposes a multiobjective sparrow search feature selection approach with sparrow ranking and preference information and its applications for high-dimensional data. First, during the population updating process, the updating formula of observers is combined with the mutualism phase of the symbiotic organisms search algorithm to design a location updating formula of observers in the sparrow search algorithm, balancing the local development ability and global search ability of sparrow search algorithm and increasing the possibility of moving closer to the optimal solution. Second, based on the dominant and nondominated sparrow individuals of each sparrow, the dominant and nondominated distances are defined, and the sparrow ranking is proposed. The feature ranking is then designed by using the definition of sparrow ranking. Following the definition of sparrow ranking and feature ranking, the selection strategy and location updating formula of the producers in the sparrow search algorithm are proposed. Finally, a preference information-based mutation algorithm is designed to augment the diversity of the nondominated solutions and more effectively guide the whole sparrow population to a better solution. The experimental results on 14 high-dimensional datasets show that the proposed algorithm outperforms different single-objective and multiobjective optimization feature selection algorithms in terms of classification efficiency and diversity of solutions.},
  archive      = {J_ASOC},
  author       = {Lin Sun and Shanshan Si and Weiping Ding and Xinya Wang and Jiucheng Xu},
  doi          = {10.1016/j.asoc.2023.110837},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective sparrow search feature selection with sparrow ranking and preference information and its applications for high-dimensional data},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Remaining useful life prediction method combining the life
variation laws of aero-turbofan engine and auto-expandable cascaded LSTM
model. <em>ASOC</em>, <em>147</em>, 110836. (<a
href="https://doi.org/10.1016/j.asoc.2023.110836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time status monitoring and health management of aero-turbofan engines (ATEs) can effectively reduce the risk of engine failure and ensure aircraft flight safety. Accurate prediction of the remaining useful life (RUL) of ATE is a vital tool for effectively monitoring the engine operating condition, in which long-short term memory (LSTM) networks are often applied to RUL prediction. However, because of the complex mechanical structure and operation mode of the aero-engine, the prediction accuracy of the LSTM network is not enough to meet the actual demand. This paper proposes an auto-expandable cascaded long-short term memory (ACLSTM) prediction model that incorporates the lifetime variation laws of ATE, which is mainly applied for the degradation assessment of ATEs and the accurate prediction of RUL. The ACLSTM model adopts the network structure of multiple LSTM modules connected step by step to continuously set the prediction error of the previous module as the training outputs of the latter module. This data processing method transforms the prediction process of the original data into that of the output error, effectively reducing the prediction error and improving the prediction effect. In addition, to further improve the prediction accuracy, this paper comprehensively proposes several empirical formulas for further correction of the prediction effect obtained by the ACLSTM model. In the experimental part, the prediction effectiveness of the proposed method is tested based on four subsets of the C-MAPSS dataset published by the National Aeronautics and Space Administration. The experimental results on the four datasets show that the root mean square error (RMSE) of the ACLSTM prediction model decreases by 95.44\% on average compared to the traditional LSTM network. In addition, the RMSE of the model decreases by 96.48\% on average after incorporating the empirical formula. The proposed method has the lowest RMSE compared to other methods with the highest prediction accuracy. The experiments thoroughly verify that the ACLSTM model and the proposed empirical formula are feasible and effective for improving the prediction accuracy of the RUL of ATE.},
  archive      = {J_ASOC},
  author       = {Likun Hu and Xujie He and Linfei Yin},
  doi          = {10.1016/j.asoc.2023.110836},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Remaining useful life prediction method combining the life variation laws of aero-turbofan engine and auto-expandable cascaded LSTM model},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stock price forecasting using PSO hypertuned neural nets and
ensembling. <em>ASOC</em>, <em>147</em>, 110835. (<a
href="https://doi.org/10.1016/j.asoc.2023.110835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stock market is a platform that allows individuals and organizations to buy stocks of publicly listed companies. It is imperative for investors and traders to utilize the platform to buy and sell stocks efficiently, but they must also determine when to do it in order to maximize profits. As trading involves holding stocks for shorter periods, projecting the future direction of a stock’s price becomes essential. In recent years, deep neural network-based trading strategies have been researched and implemented to identify when a stock’s price will increase or decrease. The main issues in implementing such solutions are that they need to deal with the noisy nature of the stock market and the problem of overfitting. The objective of the paper is to put forth an approach that utilizes deep learning techniques to predict price movements in the Nifty 50 index. The paper will explore the use of Recurrent Neural Networks (RNNs) for the given task. The paper will also look into applying metaheuristic algorithms to further improve the results of the prediction models. In this approach, RNNs, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), are utilized to predict the movement of the index. The models are trained on a unique and efficient feature set that takes into consideration the stock price of large market capitalization companies present in the National Stock Exchange (NSE). Our findings show that ensembled architectures produce better results than individual models. An LSTM and GRU ensembled architecture produced an accuracy of 56.66\% and a precision of 0.4734. Particle swarm optimization (PSO) was put forth as a method to hypertune the models to improve their performance. The LSTM, hypertuned with PSO, produced an accuracy of 57.64\% and a precision of 0.2882. To further enhance the model’s stock price prediction performance, the LSTM and GRU ensembled architecture was ensembled with the PSO hypertuned LSTM architecture to produce a model that gives the highest accuracy of 57.72\%. The proposed ensemble approach outperforms the other cutting edge techniques used to forecast how the stock price of the NSE will move. Additionally, the ensemble method increased precision from 0.2882 to 0.5485, demonstrating that ensembling and the PSO algorithm combine to produce models with superior performance. Based on the results, combining PSO hyper parameter optimized models with ensembling provides a good approach towards price movement predictions and also shows the potential of using this approach in other Artificial Intelligence (AI) fields to improve the performance of deep learning models.},
  archive      = {J_ASOC},
  author       = {Akshat Chauhan and Shivaprakash S.J. and Sabireen H. and Abdul Quadir Md. and Neelanarayanan Venkataraman},
  doi          = {10.1016/j.asoc.2023.110835},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110835},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stock price forecasting using PSO hypertuned neural nets and ensembling},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-layer double deep q network for active distribution
network equivalent modeling with internal identification for EV loads.
<em>ASOC</em>, <em>147</em>, 110834. (<a
href="https://doi.org/10.1016/j.asoc.2023.110834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With more and more distributed renewable energy generations and diversified electric vehicle (EV) loads connected to active distribution networks (ADNs), the power interaction between the ADNs and backbone network becomes more complicated. Furthermore, it is difficult to exactly simulate the power flows in the ADN due to large amounts of branches without power measurement units . Consequently, this paper proposes an equivalent modeling for the ADN, consisting of a motor and a synthetic constant impedance (Z), constant current (I), constant power (P) load (ZIP) with an internal identification for EV loads. The equivalent modeling utilizes a multi-layer double deep Q network (MLDDQN) to track the dynamics of the original ADN using few power measurements in the boundary with high precision. In the MLDDQN, the types of the ZIP load and motor are selected from the load pool to obtain the dynamic equivalent modeling of the ADN in the first layer, and the weight of the model is determined to obtain the active power ( P P ) and reactive power ( Q Q ) ( P Q PQ component) of motor and ZIP load in the second layer through the powerful feature extraction ability of deep learning and the optimization decision-making ability of reinforcement learning . Another layer is adopted to optimize the parameters of the equivalent EV to independently trace the characteristics of the original EV loads. The simulation results show that the proposed MLDDQN improves the sample selection and behavior value function evaluation in deep reinforcement learning through prior experience playback, huber loss function strategy and dueling network. Additionally, the precision and applicability of the equivalent modeling is tested with comparisons with traditional DDQN and DQN.},
  archive      = {J_ASOC},
  author       = {J.H. Zheng and W.H. Wang and Zhigang Li and Q.H. Wu},
  doi          = {10.1016/j.asoc.2023.110834},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110834},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-layer double deep q network for active distribution network equivalent modeling with internal identification for EV loads},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal reactive power dispatch problem using exchange
market based butterfly optimization algorithm. <em>ASOC</em>,
<em>147</em>, 110833. (<a
href="https://doi.org/10.1016/j.asoc.2023.110833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Butterfly Optimization Algorithm (BOA) is a swarm-based optimization technique which takes its inspiration from the butterflies’ foraging activity. BOA has gained extensive popularity among the research community and is now utilized to tackle a variety of optimization challenges. However, literatures suggest that its exploration and exploitation are not properly balanced. To overcome this problem, BOA is fused with the crossover of Exchange Market Algorithm (EMA) and with non-uniform mutation. In this paper, enhanced BOA (EBOA) is used for solving the Optimal Reactive Power Dispatch (ORPD) problems. ORPD is traditionally a power system optimization tool that regulates control variables such as Generator bus Voltage , tap settings of tap-changing transformers, and VAR output of compensating devices in order to reduce real power loss, improve voltage deviation , and increase voltage stability. The suggested technique is evaluated using the IEEE 30 bus standard test system, IEEE 118 bus standard test system and Indian 62 bus system. To determine the efficacy of the algorithm, the findings from BOA and EBOA are compared to those produced by other researchers and published in the literature. From the results it can be observed that the proposed algorithm is able to reduce the active power loss by 22\%, voltage deviation by 91.6\% and L-Index by 42.02\% from their corresponding initial value for IEEE 30 bus system. Similar results can be seen from other test system also. The proposed algorithm is also tested in solving benchmark problems. The simulated results confirm the efficiency and robustness of the algorithm for solving ORPD problems.},
  archive      = {J_ASOC},
  author       = {Bimal Kumar Dora and Abhishek Rajan and Sourav Mallick and Sudip Halder},
  doi          = {10.1016/j.asoc.2023.110833},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110833},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal reactive power dispatch problem using exchange market based butterfly optimization algorithm},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive large neighborhood search algorithm for the
unmanned aerial vehicle routing problem with recharging. <em>ASOC</em>,
<em>147</em>, 110831. (<a
href="https://doi.org/10.1016/j.asoc.2023.110831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The applications of Unmanned aerial vehicles (UAVs) in both civilian and military fields are drawing increasing attention recently. This paper investigates a new routing problem of small UAVs for information collection with time windows, where UAVs can be recharged at platforms (e.g. ground vehicles or stations) distributed in a given area. Different from the previous works on UAV routing, the UAVs are allowed to partially recharge their batteries according to the requirement in the following route. A mixed integer nonlinear programming model is developed to formulate the problem, where the overall time for completing all targets’ observation and the number of UAVs are minimized. An improved adaptive large neighborhood search (ALNS) algorithm with simulated annealing strategies is designed, and a recharging platform insertion heuristic is developed to determine the recharging strategy and construct feasible solutions . To verify the efficiency of the proposed algorithms, a set of new benchmark instances are designed based on the well-known Solomon data set and solved. The computational results are compared with those obtained by the ant colony optimization and variable neighborhood search , which shows that ALNS performs significantly better and stable. Furthermore, experimental analysis indicates that important advantages can be obtained through introducing the recharging strategy for small UAVs.},
  archive      = {J_ASOC},
  author       = {Jianmai Shi and Huiting Mao and Zhongbao Zhou and Long Zheng},
  doi          = {10.1016/j.asoc.2023.110831},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110831},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive large neighborhood search algorithm for the unmanned aerial vehicle routing problem with recharging},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Possibilistic chance-constrained data envelopment analysis
framework for failure modes and effects analysis. <em>ASOC</em>,
<em>147</em>, 110830. (<a
href="https://doi.org/10.1016/j.asoc.2023.110830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure modes and effects analysis (FMEA) is an intensively used risk analysis technique for prioritizing failure modes of products, systems, and services. Traditionally, FMEA assumes deterministic and precise values for risk factors. In engineering scenarios, the observed values of decision-makers on risk factors are, however, potentially stochastic and imprecise in nature due to the lack of adequate knowledge and experience. On the other hand, the structural correlation between risks (or failure modes) and induced factors (e.g., the loss cost and recovery time of products, systems, and services) is rarely reported in FMEA. To this end, we develop a possibilistic chance-constrained data envelopment analysis (DEA) framework to overcome the above deficiencies. In this framework, three differentiated categories of DEA approaches (namely, series, parallel, and network structure DEA) are devised to characterize such structural correlation. More specially, the Me measure is introduced to handle imprecise parameters associated with risk factors. Simultaneously, decision-makers are endowed to express flexible attitudes on these imprecise parameters. Moreover, in order to reveal the mechanism of risk priority of failure modes, the proposed framework enables decision-makers to offer optimistic and pessimistic attitudes on risk input and propagation, respectively. A case study of a production system, along with some comparisons and discussions, is given to demonstrate the effectiveness and merits of the proposed framework. As indicated from this case study, no matter which DEA approach is conducted, the risk priority of failure modes is stable in a stochastic environment.},
  archive      = {J_ASOC},
  author       = {Jing Zhou and Yu Liu},
  doi          = {10.1016/j.asoc.2023.110830},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110830},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Possibilistic chance-constrained data envelopment analysis framework for failure modes and effects analysis},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An embedded feature selection approach for depression
classification using short text sequences. <em>ASOC</em>, <em>147</em>,
110828. (<a href="https://doi.org/10.1016/j.asoc.2023.110828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression has become a serious mental health issue worldwide, particularly due to the rise of the Global Pandemic. Identifying depression of an individual from short texts shared in social media is a challenging task. The present work aims to select the optimal feature subset for classifying short texts for depression detection. By performing feature selection, it is possible to eliminate redundant and noisy features in high-dimensional datasets with small sample sizes. This can prevent the ”curse of dimensionality” and enhance the effectiveness of classification algorithms. However, current feature selection methods often focus on optimizing classification or clustering performance, while neglecting the stability of the selected features. This can lead to unstable results and make it challenging to identify meaningful and interpretable features. This paper introduces a novel embedded feature selection approach named Statistical Relevance Class Frequency based on Whale Optimization Algorithm (SRCF-WOA) for selecting feature subsets from short texts in social media. The proposed methodology extracts both the unigram features and composite features to capture the semantic and structural information. χ 2 . r c f χ2.rcf (Chi-squared relevance class frequency) filter approach is applied to rank the extracted features to signify the importance of the features. WOA is adapted to retrieve the optimal subset of features with low-dimensional space using its high exploration and high exploitation capability. In the evaluation process, four benchmark short text datasets and two classifiers are used. The comparison shows that the proposed embedded feature selection method outperforms other algorithms in terms of accuracy and F β Fβ scores( β = 0 . 5 , 1 , β=0.5,1, and 2). The sensitivity analysis is carried out to check the robustness and stability of the proposed method. The findings indicate that the SRCF-WOA surpasses other methods on the majority of datasets, achieving the maximum classification accuracy while utilizing the minimal features. The statistical importance of these findings is further supported by the Analysis of Variance (ANOVA) F-test. Moreover, the proposed method strikes the optimal balance between classification accuracy and feature stability.},
  archive      = {J_ASOC},
  author       = {Kavi Priya S. and Pon Karthika K.},
  doi          = {10.1016/j.asoc.2023.110828},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110828},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An embedded feature selection approach for depression classification using short text sequences},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient optimization state-based coyote optimization
algorithm and its applications. <em>ASOC</em>, <em>147</em>, 110827. (<a
href="https://doi.org/10.1016/j.asoc.2023.110827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coyote Optimization Algorithm (COA) has demonstrated efficient performance by utilizing the multiple pack (subpopulation) mechanism. However, the fixed number of packs and a relatively singular evolutionary strategy limit its comprehensive optimization performance . Thus, this paper proposes a COA variant, referred to as the Optimization State-based Coyote Optimization Algorithm (OSCOA). In the OSCOA algorithm, a Population Optimization State Estimation Mechanism is employed for estimating the current population optimization state. Then, the estimation result is used to guide the algorithm in setting the number of packs appropriately as well as selecting appropriate evolutionary strategies to refine search directions , thereby avoiding blind exploration. Additionally, the estimation result assists each pack in selecting suitable parents to generate pups, further improving the global search efficiency of the algorithm. To validate the effectiveness of the proposed algorithm, the OSCOA algorithm is subjected to comprehensive testing and analysis along with seven efficient optimizers on 71 benchmark functions derived from the CEC2014, CEC2017, and CEC2022 benchmark suites. The results of these extensive experiments indicate the competitive performance of OSCOA. Furthermore, to further assess the capability of the OSCOA algorithm in addressing real-world problems, two practical applications is considered: wireless sensor network deployment and image segmentation . The outcomes of these applications further confirm the efficacy and stability of the OSCOA algorithm in tackling real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Qingke Zhang and Xianglong Bu and Zhi-Hui Zhan and Junqing Li and Huaxiang Zhang},
  doi          = {10.1016/j.asoc.2023.110827},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110827},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient optimization state-based coyote optimization algorithm and its applications},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A selection hyper-heuristic algorithm with q-learning
mechanism. <em>ASOC</em>, <em>147</em>, 110815. (<a
href="https://doi.org/10.1016/j.asoc.2023.110815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of an algorithm in the real world of the application domain is a challenging problem as no specific algorithm exists capable of solving all issues to a satisfactory requirement. Selecting a suitable algorithm presents major challenges such as solving problems requiring expert knowledge or trial-and-error algorithms, which have hindered advancements in this field. In this work, we introduce a novel method that uniquely addresses these challenges by integrating hyper-heuristic and Q-learning mechanism techniques. A selection hyper-heuristic algorithm with Q-learning (QLSHH) is proposed to select appropriate low-level heuristic (LLH) for the computation stages of the optimization process. The Q-learning mechanism guided by the feedback of the solution state was designed according to the environment. Four low-level heuristics (LLHs) were proposed according to the optimization mechanism for continuous optimization problems. The QLSHH learns the successful experience in the optimization process through Q-learning to select the appropriate LLH at each decision point . The results tested on the CEC 2017 and CEC 2020 benchmark suite show that the QLSHH outperforms the other nine comparison algorithms on 50\% of the functions and the experimental results of algorithm complexity show that the proposed algorithm is the fastest compared with other algorithms.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Yuebao Liu and Ningning Zhu and Tianpeng Xu and Jonrinaldi},
  doi          = {10.1016/j.asoc.2023.110815},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110815},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A selection hyper-heuristic algorithm with Q-learning mechanism},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-stage deep residual collaboration learning framework
for complex spatial–temporal traffic data imputation. <em>ASOC</em>,
<em>147</em>, 110814. (<a
href="https://doi.org/10.1016/j.asoc.2023.110814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing accurate and efficient traffic data repair has become an essential task before proceeding with other applications of intelligent transportation systems . However, existing repair models often impose assumptions on the underlying data generation process or fail to sufficiently capture the complex spatial–temporal dependencies between several nodes at different intervals. To address these issues, we propose a novel multi-stage deep residual collaboration learning framework (named Multi-DRCF) to tackle the complex task of repairing missing traffic data. In particular, a spatiotemporal fusion network (i.e., ST-imputator) incorporating the graph-based convolutions with recurrent structures is introduced for modeling the spatial topology and dynamic temporal coherences of the traffic graph network in turn. Moreover, to further achieve both higher accuracy and efficiency, we exploit a stackable bi-directional residual optimization (i.e., Bi-RO) structure to enhance the role of ST-imputator in the utilization of the spatiotemporal properties of the residuals. Indeed, this combination of ST-imputator and Bi-RO structure can serve as an iterable unit for Multi-DRCF, and the whole computation process is highly modular and flexible in operation. We evaluate Multi-DRCF using two real-world large-scale traffic speed datasets, i.e., Seattle-Loop and METR-LA. Experimental results demonstrate that Multi-DRCF can effectively extract the spatial and temporal properties from traffic data, resulting in an excellent repair performance with an acceptable computational cost. Compared with the state-of-the-art baseline models , Multi-DRCF performs more competitively for three types of missing patterns, and also provides steadier repair results with missing rates ranging from 10\% to 90\%. Finally, the ablation experiments and visualization analysis also present well insights for better understanding the superiority of the Multi-DRCF for complex spatial–temporal traffic data imputation.},
  archive      = {J_ASOC},
  author       = {Jinlong Li and Ruonan Li and Lunhui Xu},
  doi          = {10.1016/j.asoc.2023.110814},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110814},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-stage deep residual collaboration learning framework for complex spatial–temporal traffic data imputation},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A quasi oppositional smell agent optimization and its levy
flight variant: A PV/wind/battery system optimization application.
<em>ASOC</em>, <em>147</em>, 110813. (<a
href="https://doi.org/10.1016/j.asoc.2023.110813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, two novel algorithms are developed: the quasi-oppositional smell agent optimization (QOBL-SAO) and its levy flight variation (LFQOBL-SAO), and their performance is compared to that of the conventional smell agent optimization (SAO). Two investigations were carried out. First, the capabilities of the novel algorithms were tested in solving ten benchmarked functions and five CEC2020 real-world optimization problems . Second, they are applied to optimize the hybrid photovoltaic (PV)/wind/battery, PV/battery, and wind/battery power system for a healthcare center in a Nigerian village. Load demand, PV and wind profiles of the aforementioned location were used to develop the hybrid system. All simulations were carried out in MATLAB software. The results show that the novel algorithms can solve benchmarked functions and the CEC2020 real-world constrained optimization competition. In particular, the performance of the QOBL and the LF-QOBL are as good as the top-performing functions like the IUDE, ϵ ϵ MAgES and the iLSHAD ɛ ɛ ɛ algorithms. However, in terms of convergence time, lowest cost of energy (LCE), and total annualized cost (TAC), the novel algorithms outperformed the SAO for the PV/wind/battery optimization application. The hybrid PV/wind/battery system is the most cost-effective when using LFQOBL-SAO and QOBL-SAO, with a TAC value of $15100. Furthermore, the results demonstrate that the LFQOBL-SAO method is accurate and outperforms the QOBL-SAO and SAO algorithms .},
  archive      = {J_ASOC},
  author       = {Abdullahi Abubakar Mas’ud and Ahmed T. Salawudeen and Abubakar Ahmad Umar and Ali Saleh Aziz and Yusuf A. Shaaban and Firdaus Muhammad-Sukki and Umar Musa},
  doi          = {10.1016/j.asoc.2023.110813},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110813},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A quasi oppositional smell agent optimization and its levy flight variant: A PV/Wind/battery system optimization application},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Picture fuzzy VIKOR-TOPSIS approach based on knowledge and
accuracy measures for suitable adsorbent decision making. <em>ASOC</em>,
<em>147</em>, 110807. (<a
href="https://doi.org/10.1016/j.asoc.2023.110807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to a fuzzy and intuitionistic fuzzy (IF) set, a picture fuzzy (PF) set may be taken as an effective tool to tackle decision-making problems. Fuzzy and intuitionistic fuzzy set theory can be extended to PF-set theory. As per our knowledge, there is no PF-knowledge measure has been proposed by any researcher so far. A novel picture fuzzy knowledge measure is proposed in the present manuscript. Its mathematical properties are investigated along with its validation. Its performance is evaluated using four distinct examples. Besides this, the newly presented PF-knowledge measure is used to derive a new PF-Accuracy measure. The derived PF-accuracy measure is checked for its validation and its properties are explained. The applications of the derived PF-accuracy measure are given in pattern recognition. Furthermore, a VIKOR-TOPSIS approach based on the proposed PF-knowledge and accuracy measure is designed. Finally, the suggested approach is utilized to identify an appropriate adsorbent for extracting hexavalent chromium from wastewater.},
  archive      = {J_ASOC},
  author       = {Amandeep Singh and Satish Kumar},
  doi          = {10.1016/j.asoc.2023.110807},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110807},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Picture fuzzy VIKOR-TOPSIS approach based on knowledge and accuracy measures for suitable adsorbent decision making},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter identification of fuel cell using repairable grey
wolf optimization algorithm. <em>ASOC</em>, <em>147</em>, 110791. (<a
href="https://doi.org/10.1016/j.asoc.2023.110791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of population-based meta-heuristic algorithms can be divided into two phases: exploitation and exploration. However, a major drawback of these methods is their inability to guarantee thorough searching of the entire search space. This is because the population (wolves) is randomly distributed and may not cover all areas in searching for the best solution. To address this issue, this paper introduces a simple technique called Repairable Grey Wolf Optimization (RGWO), which can be easily integrated into any population-based algorithm to enhance the exploration phase. The RGWO is implemented within the Grey Wolf Optimization (GWO) method and involves two main phases: elimination and search space management. In the elimination phase, underperforming wolves are gradually replaced with new ones, and new design parameters are introduced to improve the control of exploitation and exploration. In the search space management phase, instead of initially spreading the wolves across the entire search space, they are concentrated in a small portion and gradually expand from there. To evaluate the effectiveness of this approach, an extensive comparative study was conducted, comparing it with various classic and state-of-the-art meta-heuristic optimization algorithms. The results demonstrate several advantages over existing techniques, including fast convergence and high robustness.},
  archive      = {J_ASOC},
  author       = {S. Mohammadreza Ebrahimi and Sajjad Hasanzadeh and Sahand Khatibi},
  doi          = {10.1016/j.asoc.2023.110791},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110791},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter identification of fuel cell using repairable grey wolf optimization algorithm},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refining the rule base of fuzzy classifier to support the
evaluation of fetal condition. <em>ASOC</em>, <em>147</em>, 110790. (<a
href="https://doi.org/10.1016/j.asoc.2023.110790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a method to simplify a rule base of zero order Takagi–Sugeno–Kang fuzzy classifier, involving the determination of the ɛ ɛ ɛ -similar rules based on fuzzy clustering with ɛ ɛ ɛ -hyperballs. The rule simplification process is based on the concept of ɛ ɛ ɛ -insensitivity areas underlying the partitioning process of rule centers (centers of membership functions in the rule premises), which directly corresponds to the idea of rule ɛ ɛ ɛ -similarity. Clustering parameters leading to the best performance of the modified rule base, including the degree of rule ɛ ɛ ɛ -similarity, are determined by means of the evolution strategy. Since our main objective was to maintain the high performance of the resulting classifier, two rule-based simplification procedures, both called rule base refinement, are proposed. The work focuses mainly on the practical application to support the diagnosis of fetal condition based on the analysis of CardioTocoGraphic (CTG) signals. The publicly available collection of CTG recordings (CTU-UHB) was used in order to verify the quality of the introduced solutions. The classification performance was assessed with respect to the reference evaluation of fetal state determined on the basis of a retrospective analysis using the newborn outcome defined with different thresholds of the blood pH from the umbilical artery. The experiments confirmed the high generalization ability of the refined fuzzy classifier, in particular its high efficiency in supporting the qualitative assessment of fetal condition based on the analysis of parameters quantitatively describing fetal signals.},
  archive      = {J_ASOC},
  author       = {Robert Czabanski and Michal Jezewski and Jacek Leski and Krzysztof Horoba and Janusz Wrobel and Radek Martinek and Katerina Barnova},
  doi          = {10.1016/j.asoc.2023.110790},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110790},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Refining the rule base of fuzzy classifier to support the evaluation of fetal condition},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous cartesian genetic programming based
representation for multi-objective neural architecture search.
<em>ASOC</em>, <em>147</em>, 110788. (<a
href="https://doi.org/10.1016/j.asoc.2023.110788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel neural architecture search (NAS) approach for the challenge of designing convolutional neural networks (CNNs) that achieve a good tradeoff between complexity and accuracy. We rely on Cartesian genetic programming (CGP) and integrated real-based and block-chained CNN representation, for optimization using multi-objective evolutionary algorithms (MOEAs) in the continuous domain. We introduce two variants, CGP-NASV1 and CGP-NASV2, which differ in the granularity of their respective search spaces. To evaluate the proposed algorithms, we utilized the non-dominated sorting genetic algorithm II (NSGA-II) on the CIFAR-10, CIFAR-100,and SVHN datasets. Additionally, we extended the empirical analysis while maintaining the same solution representation to assess other searching techniques such as differential evolution (DE), the multi-objective evolutionary algorithm based on decomposition (MOEA/D), and the S metric selection evolutionary multi-objective algorithm (SMS-EMOA). The experimental results demonstrate that our approach exhibits competitive classification performance and model complexity compared to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Cosijopii Garcia-Garcia and Alicia Morales-Reyes and Hugo Jair Escalante},
  doi          = {10.1016/j.asoc.2023.110788},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110788},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Continuous cartesian genetic programming based representation for multi-objective neural architecture search},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate-assisted operator-repeated evolutionary algorithm
for computationally expensive multi-objective problems. <em>ASOC</em>,
<em>147</em>, 110785. (<a
href="https://doi.org/10.1016/j.asoc.2023.110785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted strategies work differently in surrogate-assisted multi-objective evolutionary algorithms. How to select suitable surrogate-assisted strategies to balance global and local search and diversity of Pareto optimal solutions is usually difficult for efficient optimization of computationally expensive multi-objective problems, which only allow limited time costs in the optimization process. Therefore, to further improve the optimization efficiency for expensive multi-objective problems, a surrogate-assisted operator-repeated multi-objective evolutionary algorithm is proposed by reasonably integrating several surrogate-assisted strategies in this study. Specifically, the proposed algorithm is based on a operator-repeated offspring creation strategy, which can produce many diverse candidate offspring individuals and thus make the proposed algorithm search globally and efficiently. In addition, a novel infill criterion termed as reference-vector-guided surrogate-assisted penalty-based boundary intersection is proposed and combined with a common Kriging-based expected improvement matrix infill criterion for complementary prescreening over the candidate offspring individuals. These two infill criteria can make a good balance between global search and diversity of Pareto optimal solutions . In addition, to fasten convergence speed, an improved surrogate-based multi-objective local search method with minimum distance-angle sampling is also proposed and embedded in the proposed algorithm. Several benchmark problems with dimensions varying from 8 to 50 and a practical two-objective airfoil design optimization problem with 14 design variables are tested to validate efficiency of the proposed algorithm. The experimental results demonstrate that the proposed algorithm significantly outperforms some state-of-the-art surrogate-assisted multi-objective optimization algorithms on most of problems.},
  archive      = {J_ASOC},
  author       = {Xiwen Cai and Tao Zou and Liang Gao},
  doi          = {10.1016/j.asoc.2023.110785},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110785},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-assisted operator-repeated evolutionary algorithm for computationally expensive multi-objective problems},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiobjective evolutionary pruning of deep neural networks
with transfer learning for improving their performance and robustness.
<em>ASOC</em>, <em>147</em>, 110757. (<a
href="https://doi.org/10.1016/j.asoc.2023.110757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary Computation algorithms have been used to solve optimization problems in relation with architectural, hyper-parameter or training configuration, forging the field known today as Neural Architecture Search . These algorithms have been combined with other techniques such as the pruning of Neural Networks , which reduces the complexity of the network, and the Transfer Learning , which lets the import of knowledge from another problem related to the one at hand. The usage of several criteria to evaluate the quality of the evolutionary proposals is also a common case, in which the performance and complexity of the network are the most used criteria. This work proposes MO-EvoPruneDeepTL, a multi-objective evolutionary pruning algorithm. MO-EvoPruneDeepTL uses Transfer Learning to adapt the last layers of Deep Neural Networks , by replacing them with sparse layers evolved by a genetic algorithm , which guides the evolution based in the performance, complexity and robustness of the network, being the robustness a great quality indicator for the evolved models. We carry out different experiments with several datasets to assess the benefits of our proposal. Results show that our proposal achieves promising results in all the objectives, and direct relation are presented among them. The experiments also show that the most influential neurons help us explain which parts of the input images are the most relevant for the prediction of the pruned neural network. Lastly, by virtue of the diversity within the Pareto front of pruning patterns produced by the proposal, it is shown that an ensemble of differently pruned models improves the overall performance and robustness of the trained networks.},
  archive      = {J_ASOC},
  author       = {Javier Poyatos and Daniel Molina and Aitor Martínez-Seras and Javier Del Ser and Francisco Herrera},
  doi          = {10.1016/j.asoc.2023.110757},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110757},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective evolutionary pruning of deep neural networks with transfer learning for improving their performance and robustness},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-level parallel decomposition-based artificial bee
colony method for dynamic multi-objective optimization problems.
<em>ASOC</em>, <em>147</em>, 110741. (<a
href="https://doi.org/10.1016/j.asoc.2023.110741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world multiple-objective optimization problems have objectives that change over time. These multiple-objective optimization problems are called dynamic multiple-objective optimization problems (DMOPs) and have received an increased attention. To track the changing Pareto front in DMOPs, the Pareto front at a certain moment needs to be obtained as efficiently as possible, which is challenging for most of existing methods. To this end, we propose a two-level parallel decomposition-based artificial bee colony method for solving DMOPs. To sufficiently accelerate the process of obtaining the Pareto front, a two-level parallel structure is designed in our method. In the first-level parallel structure, the dynamic multi-objective optimization problem at a certain moment is decomposed into a set of single-objective optimization problems that could be solved in parallel. In the second-level parallel structure, a parallel artificial bee colony algorithm is applied to solve each decomposed single-objective optimization problem. Specially, the parallel bee colony algorithm in our method is improved to support the exchange of information among neighbor problems, which is widely accepted to be effective in improving the efficiency of obtaining optimal solutions. To support the implementation of our improved parallel artificial bee colony algorithm, a two-level shared memory structure is designed. Our proposed method is compared with 4 widely used methods on CEC’ 2018 multi-objective optimization benchmarks and two constrained dynamic multi-objective optimization problems. The experimental results show that our method outperforms other compared methods in efficiency while maintaining good scalability and convergence.},
  archive      = {J_ASOC},
  author       = {Yuyang Bai and Changsheng Zhang and Weitong Bai},
  doi          = {10.1016/j.asoc.2023.110741},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {110741},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-level parallel decomposition-based artificial bee colony method for dynamic multi-objective optimization problems},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy clustering method with approximate orthogonal
regularization. <em>ASOC</em>, <em>147</em>, 110829. (<a
href="https://doi.org/10.1016/j.asoc.2023.110829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely used unsupervised learning method, clustering model plays an indispensable role in exploring data structures . Spectral analysis is a useful technique for clustering problems . The spectral analysis could yield good results when used in conjunction with fuzzy clustering models. However, the existing methods are limited by factors such as the definition of the similarity graph and predetermined number of clusters. In this paper, we propose a new fuzzy clustering model with the approximate orthogonal regularization . To obtain the relatively stable results, we reconstruct the Laplacian matrix with geometrically-nearest-neighbor similarity measurement. This new Laplacian matrix can represent the properties of datasets. For this constrained optimization problem , by applying the indicator function to simplify it as an unconstrained optimization problem . Then we propose an algorithm based on Alternating Direction Method of Multipliers (ADMM) and theoretically prove its convergence. We make some empirical experiments on both synthetic and real datasets. The experimental results demonstrate the model’s effectiveness and showcase the desirable properties of the resulting solutions.},
  archive      = {J_ASOC},
  author       = {Jiaojiao Yang and Andong Qiu and Zhouwang Yang},
  doi          = {10.1016/j.asoc.2023.110829},
  journal      = {Applied Soft Computing},
  pages        = {110829},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy clustering method with approximate orthogonal regularization},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effectiveness of deep learning vs. Traditional methods
for lung disease diagnosis using chest x-ray images: A systematic
review. <em>ASOC</em>, <em>147</em>, 110817. (<a
href="https://doi.org/10.1016/j.asoc.2023.110817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has proven to be a successful technique especially in medical image analysis. This paper aims to highlight the importance of deep learning architectures in lung disease diagnosis using CXR images. Related articles were identified through searches of electronic resources, including IEEE, Springer, Elsevier, PubMed, Nature and, Hindawi digital library. The inclusion of articles was based on high-performance artificial intelligence models, developed for the classification of possible findings in CXR images published from 2018 to 2023. After the quality assessment of papers, 129 articles were included according to PRISMA guidelines. Papers were studied by types of lung disease, data source, algorithm type, and outcome metrics. Three main categories of computer-aided lung disease detection were covered: traditional machine learning , deep learning-based methods, and combination of aforementioned methods for all lung diseases. The results showed that various pre-trained networks including ResNet , VGG, and DenseNet, are the most frequently used CNN architectures and would result in a notable increase in sensitivity and accuracy. Recent research suggests that utilizing a combination of deep networks with a robust machine learning classifier can outperform deep learning approaches that rely solely on fully connected neural networks as their classifier. Finally, the limitations of the existing literature and potential future research opportunities in possible findings in CXR images using deep learning architectures are discussed in this systematic review.},
  archive      = {J_ASOC},
  author       = {Samira Sajed and Amir Sanati and Jorge Esparteiro Garcia and Habib Rostami and Ahmad Keshavarz and Andreia Teixeira},
  doi          = {10.1016/j.asoc.2023.110817},
  journal      = {Applied Soft Computing},
  pages        = {110817},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The effectiveness of deep learning vs. traditional methods for lung disease diagnosis using chest X-ray images: A systematic review},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exponential stability of uncertain functional differential
equations. <em>ASOC</em>, <em>147</em>, 110816. (<a
href="https://doi.org/10.1016/j.asoc.2023.110816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a class of non-linear uncertain functional differential equations driven by canonical Process that is the counterpart of Wiener process in the framework of uncertain theory. By using a novel approach, some new and exhibit criteria for the exponential stability in mean square of the considered equations is obtained. Lastly, some examples are investigated to illustrate the theory.},
  archive      = {J_ASOC},
  author       = {Zhi Li and Xueqi Wen and Liping Xu},
  doi          = {10.1016/j.asoc.2023.110816},
  journal      = {Applied Soft Computing},
  pages        = {110816},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exponential stability of uncertain functional differential equations},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on computational intelligence methods for modeling
of light weight composite materials. <em>ASOC</em>, <em>147</em>,
110812. (<a href="https://doi.org/10.1016/j.asoc.2023.110812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light weight composite materials (LWCM) have gained tremendous attention, thanks to their low cost, eco-friendly nature, biodegradability, life-cycle superiority, noble mechanical properties for polymer composites and excellent electrical properties for carbon composites . LWCM are widely used in various engineering fields including aerospace, automobile, civil and architecture. The diversified applications of LWCM demand an extensive investigation of their mechanical and structural behavior under elevated conditions. Therefore, modeling of LWCM for mechanical and structural properties is a challenging task. Nowadays, computational intelligence (CI) methods are ones of the immensely growing topics in materials data science including materials imaging, feature identification, process and quality control, prediction, uncertainty quantification and design optimization. CI has not only been emerged as a revolutionary way to dig out material-related information from various data sources to serve next generation composite materials with unprecedented properties but also as an active surrogate model for conventional experimental methods. CI techniques are able to increase the quality of LWCM by optimizing the characteristics of individual constituent, their orientation, volume fractions, laminas and thickness using an adequate model. The process of designing, controlling and optimizing LWCM with distinguished properties has been redefined by various CI methods. In this paper, a high-level overview of CI methods has been introduced from the beginning of raw material selection to the final output performance of LWCM, discusses the strength, challenges and limitations of CI methods for LWCM, and proposes solutions and future directions.},
  archive      = {J_ASOC},
  author       = {Nesrine Amor and Muhammad Tayyab Noman and Michal Petru and Neethu Sebastian and Deepak Balram},
  doi          = {10.1016/j.asoc.2023.110812},
  journal      = {Applied Soft Computing},
  pages        = {110812},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A review on computational intelligence methods for modeling of light weight composite materials},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective robust evolutionary optimization approach
applied to the multivariate helical milling process of super duplex
steel. <em>ASOC</em>, <em>147</em>, 110811. (<a
href="https://doi.org/10.1016/j.asoc.2023.110811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an approach for multivariate modeling and multi-objective robust evolutionary optimization for processes presenting as inputs control and noise variables and as outputs a set of correlated responses regarding process performance and product quality. Factor analysis is applied to obtain a transformed output set that is uncorrelated and retains most part of the information of the original outputs. Models that are robust regarding noise variables relating this transformed output set with the control variables are obtained through robust parameter design. Response models in the function of process and noise variables are obtained through the least squares method . Mean and variance models as a function of process variables are obtained through error propagation. Robust mean square error models are obtained to model the bias and variance of each latent variable. Finally, multi-objective evolutionary optimization is performed. Three evolutionary algorithms were applied and compared through hypervolume. The approach is applied in the helical milling of super duplex stainless steel UNS S32760. Tool overhang length, hole depth, and lubri-cooling flow rate were set as noise factors. Cutting forces were monitored, while roughness and roundness were measured to evaluate process performance and product quality. The three factors, obtained to represent the six outputs, guaranteed dimensionality reduction and minimized the redundancy in output space. The mean square error models obtained to these three factors as a function of control variables minimize both bias and variance regarding noise factors. The AGE-MOEA multi-objective evolutionary algorithm obtained the best performance considering the hypervolume. Some solutions with high trade-off were selected through pseudo-weights to aid the decision-making.},
  archive      = {J_ASOC},
  author       = {José Veríssimo Ribeiro de Toledo and Thaís Fernanda Pires and Robson Bruno Dutra Pereira and João Roberto Ferreira},
  doi          = {10.1016/j.asoc.2023.110811},
  journal      = {Applied Soft Computing},
  pages        = {110811},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective robust evolutionary optimization approach applied to the multivariate helical milling process of super duplex steel},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized bayesian convolutional neural networks for
invasive breast cancer diagnosis system. <em>ASOC</em>, <em>147</em>,
110810. (<a href="https://doi.org/10.1016/j.asoc.2023.110810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has depicted unprecedented results in the diagnosis of invasive ductal carcinoma (IDC). However, most DL methods are not able to infer the uncertainty in their predictions, and are often too confident which may cause undesirable consequences. The ability of DL models to express their uncertainty in their predictions is particularly important in diagnosing diseases. Therefore, this paper proposes an uncertainty-aware approach called optimized Bayesian convolutional neural network (OBCNN) for the automated detection of IDC from histopathology images and is capable of estimating and inferring the uncertainty in its predictions. The proposed OBCNN employs an approximate Bayesian version of a pre-trained convolutional neural network (CNN) model named ResNet101V2 that is fine-tuned to the histopathology images of IDC. The Bayesian approximation is performed by adding Monte Carlo dropout (MC-dropout) to ResNet101V2 architecture. Due to the effect of MC-dropout on the obtained posterior predictive distribution (PPD), the value of MC-dropout should be chosen carefully because if the MC-dropout value is too large, the predictive distribution (PD) will be very diverse. Whereas if the MC-dropout value is very small, the PD will be very similar. Therefore, an optimization algorithm known as the slime mould algorithm is utilized to set the optimal value of the MC-dropout. During inference, the proposed OBCNN runs for T T forward passes to configure the PPD. The mean, standard deviation (SD) and entropy are then calculated over the obtained PD to estimate the predictive uncertainty (PU) of the OBCNN. The proposed OBCNN achieved an accuracy of 93.83\%, a precision of 96.14\%, a G-mean of 93.04\%, False Negative Rate of 4.72\%, and False Positive Rate of 9.96\%. The proposed OBCNN has also been compared with recent related work, the comparison’s results demonstrated that OBCNN performed better in the diagnosis of IDC.},
  archive      = {J_ASOC},
  author       = {Dalia Ezzat and Aboul Ella Hassanien},
  doi          = {10.1016/j.asoc.2023.110810},
  journal      = {Applied Soft Computing},
  pages        = {110810},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized bayesian convolutional neural networks for invasive breast cancer diagnosis system},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bicriteria single-machine scheduling with multiple job
classes and customer orders. <em>ASOC</em>, <em>147</em>, 110809. (<a
href="https://doi.org/10.1016/j.asoc.2023.110809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer order scheduling problems focus on completing all jobs of the same order consecutively to reduce the holding costs. Multiple job-class scheduling problems involving sequence independent family (class) setup times require completing all jobs of the same class consecutively to reduce the setup times. While these problems have been extensively studied in the literature, the scheduling problems jointly incorporating job classes and customer orders are relatively unexplored in the existing literature. Therefore, in this paper, we contribute to the existing literature by (1) addressing a single-machine scheduling problem involving multiple job classes with setup times and customer orders to minimize the bicriteria objective that is a linear combination of the makespan of all jobs and the sum of the holding costs of all orders; (2) proposing a mixed integer linear programming formulation; (3) developing a branch and bound algorithm that is equipped with a lower bound and two dominance properties to find an optimal solution needed to evaluate the effectiveness of the proposed heuristic algorithms ; and (4) proposing six two-phase polynomially bounded heuristic algorithms and six variants of the water-wave optimization algorithms for producing approximate solutions The effectiveness and efficiency of the proposed algorithms are empirically evaluated through extensive computational experiments.},
  archive      = {J_ASOC},
  author       = {Jatinder N.D. Gupta and Chin-Chia Wu and Win-Chin Lin and Xin-Gong Zhang and Danyu Bai and Bertrand M.T. Lin and Chia-Cheng Liao},
  doi          = {10.1016/j.asoc.2023.110809},
  journal      = {Applied Soft Computing},
  pages        = {110809},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bicriteria single-machine scheduling with multiple job classes and customer orders},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of SVR models built with AOA and chaos mapping
for predicting tunnel crown displacement induced by blasting excavation.
<em>ASOC</em>, <em>147</em>, 110808. (<a
href="https://doi.org/10.1016/j.asoc.2023.110808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study utilizes the support vector regression (SVR) model to predict the tunnel crown displacement (TCD) induced by the blasting excavation. 95 blasting operations considering the tunnel shape parameters, the blasting parameters, the explosive parameters, and the rock mass properties are used to train and test models. To enhance the model prediction performance, a novel combination of the arithmetic optimization algorithm (AOA) with the chaos mapping (CM) called CMAOA, is proposed to select the optimal hyperparameters of the SVR model with advantages of the computing speed and the optimization ability. Sixteen benchmark functions are adopted to verify the authenticity of the prediction performance improvement of the CMAOA, and six statistical indices are used to evaluate the performance of the proposed hybrid models for predicting the TCD. The evaluation results show that the SVR optimized by the AOA with Circle map (CirAOA-SVR) model has the best prediction accuracy among all models. The results of parameter analysis indicate that the degree of bonding of bedding surface is the most important input parameter for predicting the TCD, but the blasting and explosive parameters with fine importance scores cannot be ignored either.},
  archive      = {J_ASOC},
  author       = {Chuanqi Li and Xiancheng Mei},
  doi          = {10.1016/j.asoc.2023.110808},
  journal      = {Applied Soft Computing},
  pages        = {110808},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of SVR models built with AOA and chaos mapping for predicting tunnel crown displacement induced by blasting excavation},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary conditional GANs for supervised data
augmentation: The case of assessing berry number per cluster in
grapevine. <em>ASOC</em>, <em>147</em>, 110805. (<a
href="https://doi.org/10.1016/j.asoc.2023.110805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In viticulture, training a deep learning model to assess the number of berries in a cluster image requires labeling by manually counting the berries on each grape cluster. This repeating, demanding task directly opposes the need for large amounts of data of deep learning methods. The objective of this work is the development of evolutionary conditional generative adversarial networks (GANs) for supervised data augmentation in the task of assessing berry number per cluster in grapevine. Ninety-seven grape cluster images were collected and labeled by manually counting the berries, making up the original dataset, and it was used for the training of a conditional GAN using evolutionary schemes involving a population of generators competing within a common and dynamic environment: the discriminator . After generative networks training, the best performing generator was used for supervised data augmentation: generation of labeled images conditioned to a berry number, making up the augmented dataset. Two models were trained with one dataset each, original and augmented, both having approximately the same number of samples, around 400. The original dataset was enriched with traditional image transforms, while the augmented dataset had incorporated images from the trained GAN generator. The two models were trained using the same convolutional regression network architecture , and then tested on an external image dataset, with more than 1300 images not used in any training, to compare the performance of GAN data augmentation. Results showed that the model augmented with GANs yielded lower error values, with a validation error of 43 berries, than those from the original model, with a validation error of 65 berries. In the test dataset , the augmented model obtained an error of 39 berries (R 2 = = 0.75), surpassing again the original model, that obtained an error of 57 berries (R 2 = = 0.65). These results evidence that evolutionary conditional GANs generate synthetic labeled images that lead to higher performance deep learning regression models for assessing berry number from cluster images.},
  archive      = {J_ASOC},
  author       = {Salvador Gutiérrez and Javier Tardaguila},
  doi          = {10.1016/j.asoc.2023.110805},
  journal      = {Applied Soft Computing},
  pages        = {110805},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary conditional GANs for supervised data augmentation: The case of assessing berry number per cluster in grapevine},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EDCNNS: Federated learning enabled evolutionary deep
convolutional neural network for alzheimer disease detection.
<em>ASOC</em>, <em>147</em>, 110804. (<a
href="https://doi.org/10.1016/j.asoc.2023.110804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s is a dangerous disease prevalent in human societies, and unfortunately, its incidence is increasing daily. The number of patients is on the rise, while the availability of physical doctors has become limited and their schedules are packed. Consequently, the adoption of digital healthcare systems for Alzheimer’s disease (AD) has become more common, aiming to alleviate the burden on both AD patients and doctors. AD digital healthcare is a highly complex domain that incorporates various technologies, including fog computing , cloud computing , and deep learning algorithms. However, the implementation of these fog, cloud, and deep learning technologies has encountered challenges related to high computational time during AD detection processes. To address these challenges, this paper focuses on the convex optimization problem, which aims to optimize computation time and accuracy constraints in digital healthcare applications for AD. Convex optimization necessitates the use of an evolutionary algorithm that can enhance different AD constraints in distinct phases. The paper introduces a novel scheme called Evolutionary Deep Convolutional Neural Network Scheme (EDCNNS), designed to minimize computation time and achieve the highest prediction accuracy criteria for AD. EDCNNS comprises several phases, including feature extraction, selection, execution, and scheduling on the fog cloud nodes. The simulation results demonstrate that EDCNNS optimized security by 38\%, reduced the deadline failure ratio by 29\%, and improved selection accuracy by 50\% across different Alzheimer’s classes compared to existing studies.},
  archive      = {J_ASOC},
  author       = {Abdullah Lakhan and Tor-Morten Grønli and Ghulam Muhammad and Prayag Tiwari},
  doi          = {10.1016/j.asoc.2023.110804},
  journal      = {Applied Soft Computing},
  pages        = {110804},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EDCNNS: Federated learning enabled evolutionary deep convolutional neural network for alzheimer disease detection},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perceptual maps to aggregate assessments from different
rating profiles: A hesitant fuzzy linguistic approach. <em>ASOC</em>,
<em>147</em>, 110803. (<a
href="https://doi.org/10.1016/j.asoc.2023.110803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In decision making environments under uncertainty, assessments are frequently expressed in linguistic terms . When people express their opinions using linguistic terms , the meanings ascribed to these terms may not always align. This phenomenon is captured by the concept of a linguistic perceptual map, which draws from the established lattice of hesitant fuzzy linguistic term sets. Each individual or group of people (referred to as a ’profile’) possesses their own distinct perceptual map. By projecting and aggregating the opinions of these individuals or groups onto a common perceptual map, an average opinion and a level of consensus are derived. This paper extensively studies the mathematical properties of the projection function. We prove that it is a monomorphism between lattices, preserving crucial order relations. Additionally, we progress beyond existing research by introducing an interpretation function. This function facilitates the translation of the aggregated result (referred to as the ’centroid’) from the common perceptual map to each individual’s perceptual map. The properties of the interpretation function are also subject to analysis, demonstrating its role in preserving previous order relations, despite not being a morphism. To illustrate the practicality of these concepts, we propose a methodology that we apply to a real-world case study involving data in the form of ratings from the Amazon books platform. The results obtained highlight that utilizing distinct perceptual maps for each user profile statistically enhances the degree of consensus compared to scenarios where perceptual maps are not differentiated.},
  archive      = {J_ASOC},
  author       = {Walaa Abuasaker and Jennifer Nguyen and Francisco J. Ruiz and Mónica Sánchez and Núria Agell},
  doi          = {10.1016/j.asoc.2023.110803},
  journal      = {Applied Soft Computing},
  pages        = {110803},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Perceptual maps to aggregate assessments from different rating profiles: A hesitant fuzzy linguistic approach},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algorithmic trading using combinational rule vector and deep
reinforcement learning. <em>ASOC</em>, <em>147</em>, 110802. (<a
href="https://doi.org/10.1016/j.asoc.2023.110802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic trading rules are widely used in financial markets as technical analysis tools for security trading. However, traditional trading rules are not sufficient to make a trading decision. In this paper, we propose a new algorithmic trading method called CR-DQN, which incorporates deep Q-learning with two popular trading rules: moving average (MA) and trading range break-out (TRB). The input of deep Q-learning is combinational rule vectors, whose component is a linear combination of 140 rules produced by MA and TRB with different parameters. Due to non-stationary characteristics, we devise a reward driven combination weight updating scheme to generate combinational rule vectors, which can capture intrinsic features of financial data. Since the sparse reward exists in CR-DQN, we design a piecewise reward function which shows great potential in the experiments. Taking combinational rule vectors as input, the LSTM based Deep Q-learning network is used to learn an optimal algorithmic trading strategy. We apply our model to both Chinese and non-Chinese stock markets, and CR-DQN exhibits the best performance on a variety of evaluation criteria compared to many other approaches, demonstrating the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Zhen Huang and Ning Li and Wenliang Mei and Wenyong Gong},
  doi          = {10.1016/j.asoc.2023.110802},
  journal      = {Applied Soft Computing},
  pages        = {110802},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Algorithmic trading using combinational rule vector and deep reinforcement learning},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy topology on RNA shapes. <em>ASOC</em>, <em>147</em>,
110800. (<a href="https://doi.org/10.1016/j.asoc.2023.110800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The understanding of how genotypes relate to phenotypes is crucial for unravelling the mechanisms of evolution. A fundamental aspect of this relationship lies in the genotype–phenotype mapping, which provides insights into the changes observed in phenotypes corresponding to alterations in genotypes. From a mathematical perspective, the concept of continuity proves invaluable in comprehending such transformations. However, the presence of uncertainty necessitates the utilization of the fuzzy concept as an ideal tool to discuss these changes effectively. Specifically, when considering RNA sequences and their mutations, the genotype space exhibits a topological structure in a fuzzy setting. As a result, the phenotype space, comprised of RNA shapes, can also be endowed with a fuzzy topological structure . This paper explores the fuzzy accessibility of RNA shapes, which enables fuzzy continuous genotype–phenotype mapping. By employing fuzzy techniques, we aim to enhance our understanding of the intricate relationship between genotypes–phenotypes mapping.},
  archive      = {J_ASOC},
  author       = {Minakshi Biswas Hathiwala and Devangi Sojitra and Chandra Kanta Phukan and Gautam Hathiwala and Jignesh Pravin Chauhan and Sagar R. Khirsariya},
  doi          = {10.1016/j.asoc.2023.110800},
  journal      = {Applied Soft Computing},
  pages        = {110800},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy topology on RNA shapes},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A granular sigmoid extreme learning machine and its
application in a weather forecast. <em>ASOC</em>, <em>147</em>, 110799.
(<a href="https://doi.org/10.1016/j.asoc.2023.110799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme Learning Machine (ELM) is a class of machine learning systems or methods based on feedforward neural networks, which are suitable for supervised learning problems. The ELM is regarded as a special kind of FNN in the research, which is characterized by the fact that the weights of hidden layer nodes are randomly or artificially given. As a result, the ELM feature mapping is random. The generic approximation theorem states that any nonlinear piecewise continuous function may be used as the feature map. By introducing the theory of granular computing , this paper proposes a new granulation method of random sigmoid function . The Granular Sigmoid Extreme Learning Machine (GSELM) algorithm was proposed by combining the random sigmoid function with the extreme learning machine algorithm. After granulated data sets are processed by GSELM, the granulated multi-feature data will form granular vectors. The granular vector is processed in parallel, which makes the granular sigmoid extreme learning machine parallel computing . The GSELM is utilized to address the fundamental real-time issue of weather forecasting. The GSELM algorithm can predict whether the next day will be sunny or rainy more accurately and quickly, which is crucial to the field of meteorology. Several UCI data sets are used to test the feasibility of the GSELM algorithm. The verified GSELM algorithm is applied to the Australian weather forecast data set. According to the experimental findings, the GSELM algorithm can predict whether the next day will be sunny or rainy more accurately and quickly.},
  archive      = {J_ASOC},
  author       = {Hailiang Jiang and Yumin Chen and Hongbo Jiang and Yue Ni and Huijun Su},
  doi          = {10.1016/j.asoc.2023.110799},
  journal      = {Applied Soft Computing},
  pages        = {110799},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A granular sigmoid extreme learning machine and its application in a weather forecast},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metaverse and microorganism digital twins: A deep transfer
learning approach. <em>ASOC</em>, <em>147</em>, 110798. (<a
href="https://doi.org/10.1016/j.asoc.2023.110798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preparing the infrastructure for analyzing, recognizing, and characterizing microorganisms in the Metaverse can transform the fields of biology, medicine, and drug discovery. Accordingly, the realization of digital twins of microorganisms, such as viruses, fungi, algae, bacteria, protozoa, archaea, and multicellular animal parasites, can streamline the applicability of the Metaverse and similar emerging technologies like Cyber-Physical Healthcare Systems (CPHS). This is why a new approach to the digital twinning of bacteria has been presented in this research. This method of digital twinning can revolutionize the research and study of bacteria because it allows us to separate useful and harmful bacterial species , increasing the efficiency of treatment noticeably. This innovative method can easily be used by clinics, medical centers, or even by private users physically and virtually and can be adapted to every centralized or decentralized Metaverse Platform. To determine the proper treatment, biologists have always tried to identify the correct bacterial species that prompted a bacterial infection. They use various indicators, such as the bacterial cell’s shape and the size of the colony formed by the bacteria, to classify different types of bacteria with different biochemistries and shapes. However, it is challenging because of the extensive similarities between some species. For instance, such similarities exist between Staphylococcus aureus and Staphylococcus saprophyticus, which has caused numerous false diagnostic reports by operators. Wrong species reports bring about treatment failure and increase antibiotic resistance issues. Therefore, the digital twins of bacteria cover all of their identifiable characteristics and overcome many limitations to study them. In this approach, DTL techniques, including MobileNetV2, EfficientNetV2-S, and ResNet-50, have been employed to build digital twins of bacteria species. A hybrid dataset was used for training and evaluation. Among the models, EfficientNetV2-S exhibited the best performance, with a validation accuracy of 99.58\% and a test accuracy of 99.33\%. The results showed the ability of deep learning models to make bacteria digital twins based on image processing from different labs, thus assisting experts in speeding up the process and reducing diagnostic errors. In addition, in the mispredicted cases, the correct species was among the first three choices of the model. Therefore, not only can experts use DTL approaches to speed up the digital twin realization of microorganisms in the Metaverse, but they can also use these methods to reduce diagnostic errors.},
  archive      = {J_ASOC},
  author       = {Mohammad (Behdad) Jamshidi and Saleh Sargolzaei and Salimeh Foorginezhad and Omid Moztarzadeh},
  doi          = {10.1016/j.asoc.2023.110798},
  journal      = {Applied Soft Computing},
  pages        = {110798},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaverse and microorganism digital twins: A deep transfer learning approach},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A temporal convolutional recurrent autoencoder based
framework for compressing time series data. <em>ASOC</em>, <em>147</em>,
110797. (<a href="https://doi.org/10.1016/j.asoc.2023.110797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sharply growing volume of time series data due to recent sensing technology advancement poses emerging challenges to the data transfer speed and storage as well as corresponding energy consumption. To tackle the overwhelming volume of time series data in transmission and storage, compressing time series, which encodes time series into smaller size representations while enables authentic restoration of compressed ones with minimizing the reconstruction error, has attracted significant attention. Numerous methods have been developed and recent deep learning ones with minimal assumptions on data characteristics, such as recurrent autoencoders , have shown themselves to be competitive. Yet, capturing long-term dependencies in time series compression is a significant challenge calling further development. To make a response, this paper proposes a temporal convolutional recurrent autoencoder framework for more effective time series compression. First, two autoencoder modules, the temporal convolutional network encoder with a recurrent neural network decoder (TCN-RNN) and the temporal convolutional network encoder with an attention assisted recurrent neural network decoder (TCN-ARNN), are developed. The TCN-RNN employs only the recurrent neural network decoder to reconstruct the time series in reverse order. In contrast, the TCN-ARNN uses two recurrent neural networks to reconstruct the time series in both forward and reverse order in parallel. In addition, a timestep-wise attention network is developed to incorporate the forward and reverse reconstructions into the ultimate reconstruction with adaptive weights. Finally, a model selection procedure is developed to adaptively select between the TCN-RNN and TCN-ARNN based on their reconstruction performance on the validation dataset . Computational experiments on five datasets show that the proposed temporal convolutional recurrent autoencoder outperforms state-of-the-art benchmarking models in terms of lower reconstruction errors with the same compression ratio, achieving an improvement of up to 45.14\% in the average of mean squared errors . Results indicate a promising potential of the proposed temporal convolutional recurrent autoencoder on the time series compression for various applications involving long time series data.},
  archive      = {J_ASOC},
  author       = {Zhong Zheng and Zijun Zhang},
  doi          = {10.1016/j.asoc.2023.110797},
  journal      = {Applied Soft Computing},
  pages        = {110797},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A temporal convolutional recurrent autoencoder based framework for compressing time series data},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective firefly algorithm with adaptive region
division. <em>ASOC</em>, <em>147</em>, 110796. (<a
href="https://doi.org/10.1016/j.asoc.2023.110796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of single optimization strategy and poor comprehensive performance of MOFA , multi-objective firefly algorithm with adaptive region division is proposed in this paper. By leveraging the convergence index, our algorithm intelligently divides the dominant and non-dominant solution groups into three sub-regions, namely balance, exploration, and development areas, each with a distinct learning strategy that complements the strengths of fireflies. Specifically, fireflies in the balance area learn from global optimal particles with diversity to achieve a balanced exploration and development ability. Fireflies in the exploration area jointly learn from globally optimal particles with convergence and diversity, increasing the algorithm’s likelihood of discovering Pareto optimal solutions . Lastly, fireflies in the development area rapidly converge under the guidance of the globally optimal particle of convergence, thus improving the algorithm’s development ability. To further enhance the comprehensive optimization performance , we introduce a novel fusion index as an external archive update strategy that preserves solutions with superior convergence and diversity. Our experiments on 20 benchmark functions and a multi-objective optimization power flow example demonstrate that our algorithm outperforms other multi-objective optimization algorithms, highlighting its superior optimization performance .},
  archive      = {J_ASOC},
  author       = {Jia Zhao and Dandan Chen and Renbin Xiao and Juan Chen and Jeng-Shyang Pan and ZhiHua Cui and Hui Wang},
  doi          = {10.1016/j.asoc.2023.110796},
  journal      = {Applied Soft Computing},
  pages        = {110796},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective firefly algorithm with adaptive region division},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust coordination of overcurrent relays for several
simultaneous faults in wind power generation systems. <em>ASOC</em>,
<em>147</em>, 110795. (<a
href="https://doi.org/10.1016/j.asoc.2023.110795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a new robust coordination procedure for the coordination of overcurrent relays that takes into account the specific requirements and conditions that arise in wind power generation plants. It is discussed here that, in those systems, coordination schemes that guarantee that protection coordination is maintained in the face of the simultaneous loss of several lines in the network can be of great importance, although this characteristic is not considered relevant in the usual transmission and distribution systems. Several new features have been incorporated in the proposed procedure, for achieving the computational efficiency that is necessary for performing the proposed task. A new mathematical model is presented, considering the characteristic curves of relays as design variables and including new constraints/penalties related to the maximum desirable relay actuation time. The proposed tool integrates a Differential Evolution algorithm with Linear Programming and a new Discrete Feasibility Search operator to minimize the sum of relay actuation times in scenarios for multiple fault conditions and to maximize robustness of the solution under scenarios with changes in network topology . The proposed robust coordination procedure with those features constitutes the first coordination algorithm designed for dealing with the reconfigured topology after the simultaneous loss of multiple lines in a network. A set of Pareto-optimal solutions is delivered, allowing the designer to perform a trade-off analysis between smaller actuation times and smaller disconnected areas after the occurrence of a fault. In the specific case of wind power generation plants, those robust solutions allow a more efficient use of resources allocated to network maintenance. A case study of the application of the proposed methodology to the protection of a real wind farm is presented, showing that the proposed procedure allows a reduction of 44.9\% of the non-delivered energy in the case of the simultaneous occurrence of two faults, and of 51.9\% of the non-delivered energy in the case of three simultaneous faults.},
  archive      = {J_ASOC},
  author       = {Mariana M. Guimarães and Ricardo H.C. Takahashi and Mateus H. Costa and Celso H.U. Moia and Eduardo G. Carrano},
  doi          = {10.1016/j.asoc.2023.110795},
  journal      = {Applied Soft Computing},
  pages        = {110795},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust coordination of overcurrent relays for several simultaneous faults in wind power generation systems},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A surrogate-assisted highly cooperative coevolutionary
algorithm for hyperparameter optimization in deep convolutional neural
networks. <em>ASOC</em>, <em>147</em>, 110794. (<a
href="https://doi.org/10.1016/j.asoc.2023.110794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperparameter optimization in convolutional neural networks (CNNs) plays a vital role in ensuring the effectiveness of the models. However, with the depth of the existing CNN expanding, this task becomes very challenging due to the high-dimensional and computationally expensive characteristics. Given these difficulties, this study proposes a surrogate-assisted highly cooperative hyperparameter optimization (SHCHO) algorithm for large-scale chain-styled CNNs. SHCHO tackles the original hyperparameter optimization problem by cooperatively optimizing its subproblems . Specifically, it first decomposes the whole CNN into several overlapping sub-CNNs, which conforms to the inherent overlapping interaction structure among hyperparameters and significantly reduces the search space. The resulting hyperparameter optimization subproblems on these sub-CNNs are then coevolved collaboratively and competitively, facilitating a proper hyperparameter configuration for the whole CNN. Besides, SHCHO also employs the computationally efficient surrogate technique to assist in each subproblem optimization. As a result, the expensive computational cost can be greatly reduced. Extensive experimental results on three widely-used image classification datasets indicate that SHCHO shows competitive performance when compared with several state-of-the-art algorithms in terms of both hyperparameter superiority and computational cost.},
  archive      = {J_ASOC},
  author       = {An Chen and Zhigang Ren and Muyi Wang and Hui Chen and Haoxi Leng and Shuai Liu},
  doi          = {10.1016/j.asoc.2023.110794},
  journal      = {Applied Soft Computing},
  pages        = {110794},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate-assisted highly cooperative coevolutionary algorithm for hyperparameter optimization in deep convolutional neural networks},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy based bipolar intuitionistic fuzzy digraph
decision-making system in selecting COVID-19 vaccines. <em>ASOC</em>,
<em>147</em>, 110793. (<a
href="https://doi.org/10.1016/j.asoc.2023.110793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The energy of a graph, defined as the absolute sum of the eigenvalues (EIGs) of its adjacency matrix , has been applied to numerous research fields, including decision-making techniques. It can be defined for a fuzzy graph when the graph has uncertain vertices and edges. A bipolar intuitionistic fuzzy graph (BIFG) is an extended version of fuzzy graph that helps to deal with uncertainty in an effective manner. Therefore, this study unites the above-said concepts to examine the energy concepts with additional information and utilizes them in the decision-making problem to explore their applicability. Initially, the notions, relations and bounds of the energy, Laplacian energy (LE) and signless Laplacian energy (SLE) are defined. Further, a novel energy-based multi-criteria decision-making (MCDM) technique is designed by utilizing the energy of BIFG to obtain the objective weights for the decision experts. The selection problem of five COVID-19 vaccines with respect to eight important criteria is taken to elucidate the proposed decision-making technique, where Moderna has resulted as the finest vaccine among other vaccines. Moreover, a comparative is done with the correlation and distance measures to show the superiority of the proposed method. Finally, a sensitivity analysis is done to depict the reliability of the proposed technique.},
  archive      = {J_ASOC},
  author       = {Deva Nithyanandham and Felix Augustin and David Raj Micheal and Nagarajan Deivanayagam Pillai},
  doi          = {10.1016/j.asoc.2023.110793},
  journal      = {Applied Soft Computing},
  pages        = {110793},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy based bipolar intuitionistic fuzzy digraph decision-making system in selecting COVID-19 vaccines},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unveiling hierarchical relationships for social image
representation learning. <em>ASOC</em>, <em>147</em>, 110792. (<a
href="https://doi.org/10.1016/j.asoc.2023.110792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the rapid propagation of images across social media platforms , the representation learning of social images has emerged as a prime research pursuit. A salient characteristic of social images lies in their multimodal properties, encompassing the visual content and textual descriptions present within individual images and the social relationships among different images. Notably, the content information within individual images and the structural information among different images exhibit a hierarchical nature. However, existing endeavors generally employ a flat framework, leading to suboptimal utilization of hierarchical relationships. Furthermore, the heterogeneity inherent in data content and structural information presents additional challenges in social image representation. In light of these challenges, we propose a novel Hierarchical Heterogeneous Graph Neural Network model for Social Image Representation learning, dubbed HHGSI. Our motivation lies in exploring and exploiting the hierarchical relationship between diverse modalities through designing the hierarchical heterogeneous network framework. HHGSI consists of an Intra-node Multimodal Graph Encoder and an Inter-node Heterogeneous Graph Neural Network to simultaneously capture fine-grained correlation within the image and heterogeneous relationships among the images. Moreover, a task-independent optimization objective is designed to make the model suitable for numerous network-oriented and multimodal tasks. Our proposal is extensively evaluated over four real-world datasets, and experimental results demonstrate the superiority of our proposal. Our code is publicly available in https://github.com/multimodal-code/HHGSI .},
  archive      = {J_ASOC},
  author       = {Linfeng Han and Xiaoming Zhang and Litian Zhang and Ming Lu and Feiran Huang and Yun Liu},
  doi          = {10.1016/j.asoc.2023.110792},
  journal      = {Applied Soft Computing},
  pages        = {110792},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unveiling hierarchical relationships for social image representation learning},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Failure mode and effects analysis method based on fermatean
fuzzy weighted muirhead mean operator. <em>ASOC</em>, <em>147</em>,
110789. (<a href="https://doi.org/10.1016/j.asoc.2023.110789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure mode and effects analysis (FMEA) is an important work in product reliability design. It can identify the weak links and key items in design through fault analysis, and provide basic information for evaluating and improving the reliability of system design. In view of the shortcomings of the existing FMEA method, such as the weak ability of experts to describe and process fuzzy information and the lack of consideration of the weight and correlation between risk factors. In this paper, by fusing Fermatean fuzzy sets (FFS) and Muirhead Mean (MM) operators, we propose Fermatean fuzzy weighted Muirhead mean (FFWMM) operator and develop a new FMEA method. Firstly, two new risk factors are proposed to overcome the shortcoming of insufficient consideration of the FMEA method. By introducing FFS, the ability of experts to express fuzzy information is enhanced. Using FFWMM operator to aggregate evaluation information can deal with the weight and correlation between influencing factors well. Finally, an example is given to illustrate the superiority of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yuan Zhong and Guofa Li and Chuanhai Chen and Yan Liu},
  doi          = {10.1016/j.asoc.2023.110789},
  journal      = {Applied Soft Computing},
  pages        = {110789},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Failure mode and effects analysis method based on fermatean fuzzy weighted muirhead mean operator},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ABT-SVDD: A method for uncertainty handling in domain
adaptation using belief function theory. <em>ASOC</em>, <em>147</em>,
110787. (<a href="https://doi.org/10.1016/j.asoc.2023.110787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation involves adapting a model trained on one domain to work effectively on another, which can have different statistical properties, such as distributions, correlations, and relationships between features. These heterogeneities can lead to uncertainty, impacting the model’s performance. Despite many studies that have been done on domain adaptation, most have ignored the adverse impact of uncertain and noisy data on adaptation and classification. To address this issue, the proposed method, Adaptive Belief-based Twin Support Vector Data Description (ABT-SVDD), extends the one-class support vector data description (SVDD) to an adaptive twin classifier and integrates it with a belief-based sample weighting approach. Also, it utilizes a combination of Hermite polynomial and Gaussian kernels to enhance the computational power of the linear objective function of the SVDD classifier while improving the generalization capability. The effectiveness of ABT-SVDD has been compared to the state-of-the-art methods on several tasks taken from two benchmark datasets. The experimental results demonstrate that ABT-SVDD significantly improves classification accuracy on various tasks with varying amounts of labeled data in the target domain. Specifically, in normal situations, ABT-SVDD outperforms competing methods by 6.33\% to 9.08\%, while in noisy situations, it achieves a more significant improvement of 9.87\% compared to the competing methods. Besides, the Wilcoxon statistical test demonstrates the superiority of ABT-SVDD over state-of-the-art ones in terms of classification accuracy and computational time.},
  archive      = {J_ASOC},
  author       = {Mona Moradi and Javad Hamidzadeh and Reza Monsefi},
  doi          = {10.1016/j.asoc.2023.110787},
  journal      = {Applied Soft Computing},
  pages        = {110787},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ABT-SVDD: A method for uncertainty handling in domain adaptation using belief function theory},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Matrix representations of the inverse problem in the graph
model for conflict resolution with fuzzy preference. <em>ASOC</em>,
<em>147</em>, 110786. (<a
href="https://doi.org/10.1016/j.asoc.2023.110786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inverse analysis of the graph model for conflict resolution (GMCR) determines all the preference relationships required by each decision-maker (DM) according to a given stability definition to ensure that the given state is stable. In the paper, a matrix representation approach for the inverse graph model with fuzzy preference relations is proposed to obtain all the preference relations for each DM making the stability result based on the fuzzy stability definition, including fuzzy Nash, fuzzy general metarationality, fuzzy symmetric metarationality, and fuzzy sequential stability definition. For the two-DM graph model, the fuzzy preference relation over states, unilateral movements, and fuzzy unilateral improvements are refreshed by using the matrix forms. Furthermore, for the multiple DMs model, the joint movements and joint improvement for a collection including at least two DM are represented by using matrix representation . Finally, the matrix relationship for specifying the required preference relation in the framework of inverse graph model with fuzzy preferences is derived. To illustrate the usefulness of the matrix representation of inverse graph model with fuzzy preference relations , this paper demonstrates it using a real-life conflict of the Elmira groundwater contamination conflict.},
  archive      = {J_ASOC},
  author       = {Dayong Wang and Jing Huang and Yejun Xu},
  doi          = {10.1016/j.asoc.2023.110786},
  journal      = {Applied Soft Computing},
  pages        = {110786},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Matrix representations of the inverse problem in the graph model for conflict resolution with fuzzy preference},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative adversarial network to alleviate information
insufficiency in intelligent fault diagnosis by generating continuations
of signals. <em>ASOC</em>, <em>147</em>, 110784. (<a
href="https://doi.org/10.1016/j.asoc.2023.110784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces Con-GAN, an innovative improvement of GAN-based data augmentation designed to address data insufficiency in fault diagnosis methodologies. Distinctly different from traditional GAN-based data augmentation , Con-GAN aims to generate realistic continuations of existing signals and subsequently integrates these continuations with the original signals. This ’real-fake-mixed’ strategy fully leverages the existing signals and results in high-quality new signals for data augmentation, making our approach both safer and more effective. With rigorous validation across multiple datasets, which include both artificially induced and test-caused real faults, Con-GAN’s consistent effectiveness has been substantiated. Compared to other GANs, Con-GAN presents distinct advantages, yielding new signals with better validity and variety. In summary, Con-GAN introduces a novel and feasible paradigm in GAN-based data augmentation, demonstrating practical value for industrial. In the future, we plan to evaluate Con-GAN’s performance under conditions with high noise levels, exploring its robustness and adaptability across a wider range of scenarios.},
  archive      = {J_ASOC},
  author       = {Zhenglin Dai and Liping Zhao and Ke Wang and Yanlin Zhou},
  doi          = {10.1016/j.asoc.2023.110784},
  journal      = {Applied Soft Computing},
  pages        = {110784},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generative adversarial network to alleviate information insufficiency in intelligent fault diagnosis by generating continuations of signals},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Location and path planning for urban emergency rescue by a
hybrid clustering and ant colony algorithm approach. <em>ASOC</em>,
<em>147</em>, 110783. (<a
href="https://doi.org/10.1016/j.asoc.2023.110783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rescue station setup and rescue path planning are two important tasks in urban emergency rescue. The former task ensures rescue response capability and the latter task provides effective rescue solutions. When emergencies occur in cities, evacuees are distributed along the urban road network . Rescue resources refer to rescue vehicles whose available number and capacity are both limited. With the constraints of rescue resources and the number of rescues, this paper aims to simultaneously optimize the tasks of rescue station setup and rescue path planning . In the addressed scenario, the priority of each evacuee is quantified as a weight value that is used as the main optimization objective . To solve the problem, a comprehensive urban emergency rescue planning approach is proposed. The proposed approach consists of components of road network processing, road network weight calculation, rescue station setup and rescue path planning. For the setup of rescue stations, this paper employs a clustering method to provide a set of high-quality candidate rescue stations for subsequent path planning based on the locations of evacuees and the road network structure. For rescue path planning, an improved ant colony optimization algorithm is developed. The proposed method is called the planning algorithm with clustering and improved ant colony optimization (PA-C-IACO). The proposed PA-C-IACO redefines the degree of heuristic and pheromone concentration increments for transfer between intersections in the ant colony algorithm and incorporates a reward mechanism during the pheromone update process. Experimental results on six different size datasets show that PA-C-IACO outperforms state-of-the-art algorithms and shows good robustness and feasibility.},
  archive      = {J_ASOC},
  author       = {Bing Yang and Lunwen Wu and Jian Xiong and Yuxin Zhang and Lidong Chen},
  doi          = {10.1016/j.asoc.2023.110783},
  journal      = {Applied Soft Computing},
  pages        = {110783},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Location and path planning for urban emergency rescue by a hybrid clustering and ant colony algorithm approach},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing deep transfer networks with fruit fly
optimization for accurate diagnosis of diabetic retinopathy.
<em>ASOC</em>, <em>147</em>, 110782. (<a
href="https://doi.org/10.1016/j.asoc.2023.110782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is crucial to develop a smart analytics system capable of accurately diagnosing diabetic retinopathy. This research uses a new deep transfer network framework to diagnose Diabetic Retinopathy (DR). The core of this framework is to employ a new Fruit Fly Optimization Algorithm (MALBFOA) enhanced by the Levy Flight (LF), Gaussian Transboundary Correction (GTC), Multi-subgroups, and Subgroups Annihilation (SA) mechanisms to optimize two fully connected layers parameters in one transfer deep learning model and establish a MALBFOA-based Deep Learning (MALBFOA-DL) for diagnosing diabetic retinopathy with a large set of color fundus photography obtained under a variety of imaging conditions as input. To verify the proposed method’s effectiveness, we quantitatively compare the proposed MALBFOA with the original FOA, FOA-based variants, and other traditional meta-heuristic algorithms in a comprehensive set of 49 benchmark functions (shifted and swirled). The experimental results validate that MALBFOA holds a faster convergence rate and better solutions in almost all benchmark functions , especially in solving asymmetric complicated optimization problems . The proposed MALBFOA-DL model can also grade the degree of diabetic retinopathy with more accurate recall rates than the benchmark model and assist doctors in diagnosing diabetic retinopathy.},
  archive      = {J_ASOC},
  author       = {Maofa Wang and Qizhou Gong and Huiling Chen and Guangda Gao},
  doi          = {10.1016/j.asoc.2023.110782},
  journal      = {Applied Soft Computing},
  pages        = {110782},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing deep transfer networks with fruit fly optimization for accurate diagnosis of diabetic retinopathy},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stacked ensemble forecast for photovoltaic power plants
combining deterministic and stochastic methods. <em>ASOC</em>,
<em>147</em>, 110781. (<a
href="https://doi.org/10.1016/j.asoc.2023.110781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature engineering (or creation of new features) that strengthens the importance of fundamental variables (e.g., cloud cover, partial shading, solar irradiance) is essential in gaining an accurate PV forecast for different use cases. On-grid and off-grid PV systems for residential prosumers, and larger scale PV systems are investigated from the PV forecast point of view. Therefore, in this paper, we propose a robust and scalable method designed for PV systems with various sizes and connectivity that provides accurate predictions. The originality of our research consists in feature engineering and combining the deterministic and stochastic models into a meta-learning Stacked Ensemble Forecast (SEF) method that is properly adjusted considering the connectivity of the PV system. The predictions of several standout Machine Learning (ML) algorithms are stacked and scaled to create an accurate forecast model. The proposed methodology is tested on three case studies that cover different types of PV systems: small (residential) on-grid, off-grid, and industrial PV power plants. In all cases, the SEF model provides accurate predictions with an R 2 between 0.91 and 0.99 and MAPE between 0.1 and 0.29, followed closely by the stochastic models that have an average R 2 of 0.85 and MAPE of 0.3. The deterministic model provides less accurate predictions with an average R 2 of 0.8 and MAPE of 0.37. Furthermore, RMSE and MAE improved with more than 16\% using the SEF model compared to the stochastic models and with more than 45\% compared to the deterministic model.},
  archive      = {J_ASOC},
  author       = {Simona-Vasilica Oprea and Adela Bâra},
  doi          = {10.1016/j.asoc.2023.110781},
  journal      = {Applied Soft Computing},
  pages        = {110781},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A stacked ensemble forecast for photovoltaic power plants combining deterministic and stochastic methods},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multitasking evolutionary algorithm based on adaptive seed
transfer for combinatorial problem. <em>ASOC</em>, <em>147</em>, 110780.
(<a href="https://doi.org/10.1016/j.asoc.2023.110780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary computing (EC) is widely used in dealing with combinatorial optimization problems (COP). Traditional EC methods can only solve a single task in a single run, while real-life scenarios often need to solve multiple COPs simultaneously. In recent years, evolutionary multitasking optimization (EMTO) has become an emerging topic in the EC community. And many methods have been designed to deal with multiple COPs concurrently through exchanging knowledge. However, many-task optimization, cross-domain knowledge transfer, and negative transfer are still significant challenges in this field. A new evolutionary multitasking algorithm based on adaptive seed transfer (MTEA-AST) is developed for multitasking COPs in this work. First, a dimension unification strategy is proposed to unify the dimensions of different tasks. And then, an adaptive task selection strategy is designed to capture the similarity between the target task and other online optimization tasks. The calculated similarity is exploited to select suitable source tasks for the target one and determine the transfer strength. Next, a task transfer strategy is established to select seeds from source tasks and correct unsuitable knowledge in seeds to suppress negative transfer. Finally, the experimental results indicate that MTEA-AST can adaptively transfer knowledge in both same-domain and cross-domain many-task environments. And the proposed method shows competitive performance compared to other state-of-the-art EMTOs in experiments consisting of four COPs.},
  archive      = {J_ASOC},
  author       = {Haoyuan Lv and Ruochen Liu},
  doi          = {10.1016/j.asoc.2023.110780},
  journal      = {Applied Soft Computing},
  pages        = {110780},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multitasking evolutionary algorithm based on adaptive seed transfer for combinatorial problem},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving the RNA inverse folding problem through target
structure decomposition and multiobjective evolutionary computation.
<em>ASOC</em>, <em>147</em>, 110779. (<a
href="https://doi.org/10.1016/j.asoc.2023.110779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The RNA inverse folding problem involves discovering a nucleotide sequence that folds into a desired target structure. Although numerous computational methods have been proposed over the years to tackle the problem, none have successfully solved the complete Eterna100 set. The Eterna100 set is widely recognized as a benchmark in this field. Therefore, there is still ample room for improvement in this area. This paper aims to address this challenge by introducing eM2dRNAs, an enhanced version of our previous approach called m2dRNAs, which is a multiobjective metaheuristic to design RNA sequences. By introducing eM2dRNAs, we aim to make significant advancements in RNA inverse folding. Our approach starts with the recursive decomposition of the target structure, simplifying the problem to be solved. We conducted a comparative study of our method against several published methods using the Eterna100 benchmark. The results showed that our proposal performs significantly better than the other methods across almost all metrics and categories considered, thus achieving our objective of improving the ability to solve the RNA inverse folding problem.},
  archive      = {J_ASOC},
  author       = {Álvaro Rubio-Largo and Nuria Lozano-García and José M. Granado-Criado and Miguel A. Vega-Rodríguez},
  doi          = {10.1016/j.asoc.2023.110779},
  journal      = {Applied Soft Computing},
  pages        = {110779},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving the RNA inverse folding problem through target structure decomposition and multiobjective evolutionary computation},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MISPSO-attack: An efficient adversarial watermarking attack
based on multiple initial solution particle swarm optimization.
<em>ASOC</em>, <em>147</em>, 110777. (<a
href="https://doi.org/10.1016/j.asoc.2023.110777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vulnerability of deep learning models to adversarial attacks is a growing concern, as the emergence of adversarial samples exposes almost all models to the risk of such attacks. This paper proposes a new method for adversarial attacks through watermarking. Our goal is to leverage the properties of adversarial samples to prevent people’s images from being maliciously collected and compared, thereby avoiding the leakage of private information. Our method, which improves on the multi-swarm particle swarm optimization (MPSO) algorithm, outperforms existing similar methods on two popular computer vision datasets. We conducted attack experiments on the widely used Imagenet dataset and achieve the highest attack success rate of 89.50\%. The experimental results demonstrate the superiority of our method over existing similar methods. We simulate the attacks on the online social environment using two face photographs datasets and face recognition models. Our method reaches the best deception performance compared to similar methods, with the highest success rate of 97.03\%, demonstrating our approach’s ability to protect individuals’ privacy. Furthermore, we investigate the natural causes of adversarial samples and demonstrate their inevitability, providing valuable insights for developing more robust deep models. The source code of the proposed method is available online at: https://github.com/grandwang/main_attack .},
  archive      = {J_ASOC},
  author       = {Xianyu Zuo and Xiangyu Wang and Wenbo Zhang and Yadi Wang},
  doi          = {10.1016/j.asoc.2023.110777},
  journal      = {Applied Soft Computing},
  pages        = {110777},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MISPSO-attack: An efficient adversarial watermarking attack based on multiple initial solution particle swarm optimization},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel hybrid coyote–particle swarm optimization algorithm
for three-dimensional constrained trajectory planning of unmanned aerial
vehicle. <em>ASOC</em>, <em>147</em>, 110776. (<a
href="https://doi.org/10.1016/j.asoc.2023.110776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) have been considered the future of transportation systems. However, mostly the reported optimizers struggle to estimate the flyable trajectories within acceptable accuracy and time bound under various constraints, particularly in a complex 3D environment. Therefore, this work proposes a novel hybrid optimizer (HCPSOA) by combining the Particle Swarm Optimization (PSO) and Coyote Optimization Algorithm (COA). Further, the chaotic logistic map and dynamic weight adjustments have been incorporated to enhance the exploration–exploitation capabilities. After validating the promising performance of HCPSOA against popular metaheuristics (COA, PSO, Improved COA, Glowworm Swarm Optimization, and Hybrid Fireworks PSO) for several benchmark functions , the proposed HCPSOA has been employed to estimate the flyable path by formulating a novel cost function and smoothened by cubic B-spline curve. The simulated results reveal that the HCPSOA provide a non-colliding path with up to 10.00\% lesser average cost for the considered real world scenario (map 3), which confirms its supremacy for the estimation of the safe and flyable relative to other compared algorithms.},
  archive      = {J_ASOC},
  author       = {Himanshu Gupta and Om Prakash Verma},
  doi          = {10.1016/j.asoc.2023.110776},
  journal      = {Applied Soft Computing},
  pages        = {110776},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid Coyote–Particle swarm optimization algorithm for three-dimensional constrained trajectory planning of unmanned aerial vehicle},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization time-cost-quality-work continuity in
construction management using mutation–crossover slime mold algorithm.
<em>ASOC</em>, <em>147</em>, 110775. (<a
href="https://doi.org/10.1016/j.asoc.2023.110775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents the mutation–crossover slime mold algorithm (MCSMA) to balance the time, cost, quality, and work continuity in a particular construction project. The slime mold algorithm is combined with the mutation–crossover method to modify the operating mechanism and increase the ability of finding optimal solutions in the exploration and exploitation space during the optimization process. The optimal exchange problem considers all the logical aspects of occurring activities and the obvious reasoning applied to choose a compromise solution in the project implementation. The MCSMA is compared with five well-known algorithms (i.e., OMOSOS, MOABC, MODE, MOPSO, and NSGA-II) to verify the effectiveness and performance of the proposed model. According to the analysis results, the MCSMA generates a diversification measure for both case studies with 0.501 and 0.485 C-Metric, 0.785 and 0.923 Spread, and 0.843 and 0.806 Hyper-volume. The algorithmic model represents development, improvement, and diversification in the problem of achieving model convergence and wide distribution and gives a better uniformity of optimal solutions compared to the other algorithms.},
  archive      = {J_ASOC},
  author       = {Pham Vu Hong Son and Luu Ngoc Quynh Khoi},
  doi          = {10.1016/j.asoc.2023.110775},
  journal      = {Applied Soft Computing},
  pages        = {110775},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization time-cost-quality-work continuity in construction management using mutation–crossover slime mold algorithm},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A meta-heuristic-based algorithm for designing
multi-objective multi-echelon supply chain network. <em>ASOC</em>,
<em>147</em>, 110774. (<a
href="https://doi.org/10.1016/j.asoc.2023.110774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, practitioners and researchers have concentrated on the use of meta-heuristics in the design of multi-objective supply chain networks. The number of publications in this field reflects this interest. The purpose of this paper is to propose a novel metaheuristic approach for designing supply chain network problems in the case of multi-objective supply chains. The proposed algorithm hybridizes three meta-heuristic approaches; simulated annealing, tabu search , and variable neighborhood algorithms. The proposed algorithm is also combined with a linear programming approach. The purpose of hybridization is to aggregate the approaches in order to benefit from the advantages of these solution algorithms . The features of these algorithms are investigated in this paper for the first time. The most useful characteristics of each algorithm are then utilized for developing the new algorithm. The performance of the proposed algorithm is compared with an exact algorithm, simulated annealing, and tabu search algorithms . The results indicated that the proposed algorithm is comparable to the exact in the case of small and medium-sized supply chain problems. The findings revealed that the proposed algorithm takes on an average of 6.3 min to solve small and medium problems, while the exact algorithm takes 412 min. Moreover, the proposed algorithm outperforms other algorithms with a mean ideal distance (MID) of 13, 606, 871, a percent of domination (POD) of 0.84, and a computational time of 15.64 min compared to the tabu search algorithm’s MID of 15, 574, 523, a POD of 0.55, and a computational time of 21.46 min. The simulated annealing algorithm , on the other hand, achieved an average MID of 18, 145, 931, a POD of 0.40, and a computational time of 30 min. Furthermore, the findings showed that the proposed algorithm is capable of solving complex and large supply chain problems in a reasonable time when exact approaches fail.},
  archive      = {J_ASOC},
  author       = {Awsan Mohammed and Maged S. Al-shaibani and Salih O. Duffuaa},
  doi          = {10.1016/j.asoc.2023.110774},
  journal      = {Applied Soft Computing},
  pages        = {110774},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A meta-heuristic-based algorithm for designing multi-objective multi-echelon supply chain network},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Q-learning-based unmanned aerial vehicle path planning with
dynamic obstacle avoidance. <em>ASOC</em>, <em>147</em>, 110773. (<a
href="https://doi.org/10.1016/j.asoc.2023.110773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unmanned aerial vehicles (UAVs) have shown promising results for autonomous sensing. UAVs have been deployed for multiple applications that include surveillance, mapping, tracking, and search operations. Finding an efficient path between a source and a goal is a critical issue that has been the focus of recent exploration. Many path-planning algorithms are utilized to find an efficient path for a UAV to navigate from a source to a goal with obstacle avoidance. Despite the extensive literature and numerous research proposals for path planning , dynamic obstacle avoidance has not been addressed with machine learning . When the obstacles are dynamic, i.e., they can change their position over time, and the constraints of the path planning algorithm become more challenging. This in turn adds a layer of complexity to the path planning algorithm. To address this challenge, a Q-learning algorithm is proposed in this work to facilitate efficient path planning for UAVs with both static and dynamic obstacle avoidance. We introduced the Shortest Distance Prioritization policy in the learning process which marginally reduces the distance that the UAV has to travel to reach the goal. Further, the proposed Q-learning algorithm adopts a grid-graph-based method to solve the path-planning problem. It learns to maximize the reward based on the agent’s behavior in the environment. Through results, the performance comparison between the proposed approach and state-of-the-art path planning approaches such as A-star, Dijkstra, and Sarsa algorithms are evaluated in terms of learning time and path length. We show through results that the proposed approach results in improved performance when compared to state-of-the-art approaches. Further, the effect of an increased number of obstacles are evaluated on the performance of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Amala Sonny and Sreenivasa Reddy Yeduri and Linga Reddy Cenkeramaddi},
  doi          = {10.1016/j.asoc.2023.110773},
  journal      = {Applied Soft Computing},
  pages        = {110773},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Q-learning-based unmanned aerial vehicle path planning with dynamic obstacle avoidance},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subpopulation preference adjective non-dominated sorting
genetic algorithm for multi-objective capacity expansion for matured
fabs. <em>ASOC</em>, <em>147</em>, 110772. (<a
href="https://doi.org/10.1016/j.asoc.2023.110772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capacity expansion strategy consists of fulfilling the demand with capacity portfolio via alternative configurations with long lead time for procurement and installation. Sequential dependent decisions including demand planning and cost structure of capital expenditure shall be taken into consideration for achieving enterprise profitability. Though a number of studies have been done to address related issues, limitations of existing approaches can be traced in part to the lack of a systematic framework in which multiple objectives of related total resource management problems can be considered and integrated. Little research has been done to address the present problem for capacity expansion for matured fabs from the perspective of total resource management. To fill the gaps, this study applies the concept of total resource management to integrate operational strategies and the overall usage of resources. This study develops a capacity expansion model with multiple objectives including minimizing resource costs, maximizing overall return, maximizing revenue, and minimizing capacity risk. Since the formulation of the multi-objectives can be non-linear and the size of the problem is increasing, it is difficult to solve the problem in reasonable time for practical use. A subpopulation preference adjective non-dominated sorting genetic algorithm (SPANS-GA) is developed to solve the decision problem. An empirical study in a leading semiconductor company in Taiwan is conducted for validation. The results have shown practical viability of the developed solution for multi-objective capacity planning for matured wafer fabs for total resource management.},
  archive      = {J_ASOC},
  author       = {Hsuan-An Kuo and Chia-Ching Peng and Chen-Fu Chien},
  doi          = {10.1016/j.asoc.2023.110772},
  journal      = {Applied Soft Computing},
  pages        = {110772},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Subpopulation preference adjective non-dominated sorting genetic algorithm for multi-objective capacity expansion for matured fabs},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing the initial weights of a PID neural network
controller for voltage stabilization of microgrids using a PEO-GA
algorithm. <em>ASOC</em>, <em>147</em>, 110771. (<a
href="https://doi.org/10.1016/j.asoc.2023.110771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive PID neural network (PIDNN) controller for direct and quadrature voltage control of three-phase inverters for islanded microgrids . A hybrid metaheuristic optimization algorithm is proposed for the initial weight selection of the proposed PIDNN controller using a discrete-time simulation model incorporating the nonlinearity of the insulated gate bipolar transistor (IGBT) or metal oxide semiconductor field effect transistor (MOSFET) switches of the inverter. The proposed hybrid optimization (HO) approach is a combination of population extremal optimization (PEO) and genetic algorithm (GA). It turned out that the proposed HO-PIDNN control scheme excelled the PEO, GA, and particle swarm optimization-based PIDNN controllers in terms of the objective function value, which is composed of the integral time absolute tracking error of the output voltage and a chattering penalty factor. Also, the control system outperformed the model reference adaptive PID control technique from the literature.},
  archive      = {J_ASOC},
  author       = {Md. Mahmudul Hasan and M.S. Rana and Fariya Tabassum and H.R. Pota and Md. Hassanul Karim Roni},
  doi          = {10.1016/j.asoc.2023.110771},
  journal      = {Applied Soft Computing},
  pages        = {110771},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing the initial weights of a PID neural network controller for voltage stabilization of microgrids using a PEO-GA algorithm},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virus control optimizer based on the prevention and control
mechanism of COVID-19 for engineering optimization problems.
<em>ASOC</em>, <em>147</em>, 110770. (<a
href="https://doi.org/10.1016/j.asoc.2023.110770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms have been widely used in many application fields and have achieved some good results, but there is still a strong demand for these algorithms in different engineering application fields. In this paper, a novel metaheuristics algorithm named Virus Control Optimizer (VCO) is proposed to solve engineering optimization problems . The algorithm is inspired by the prevention and control mechanism of COVID-19, which designed an effective solution space partition strategy and a multi-subpopulation collaborative search framework. This framework will maintain the whole population’s diversity while making an accurate search on the local optimal region . Compared to other existing algorithms, VCO has a unique search mechanism and can achieve better convergence efficiency, especially in engineering optimization problems . The search capabilities of VCO are assessed by testing on 12 classical benchmark functions , 29 unconstrained benchmark functions obtained from CEC 2017, 4 constrained engineering optimization problems, 5 real-world constrained problems from CEC 2020 and 3 simulation test of multi-UAVs path planning problems . In addition, VCO is compared with other typical and state-of-the-art heuristic algorithms , our results reveal that VCO can obtain better solutions than the comparison algorithms.},
  archive      = {J_ASOC},
  author       = {Chang He and Weiqing Huang and Haibin Ouyang and Steven Li and Jianhua Xiang},
  doi          = {10.1016/j.asoc.2023.110770},
  journal      = {Applied Soft Computing},
  pages        = {110770},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Virus control optimizer based on the prevention and control mechanism of COVID-19 for engineering optimization problems},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization of neural network parameters in improvement of
particulate matter concentration prediction of open-pit mining.
<em>ASOC</em>, <em>147</em>, 110769. (<a
href="https://doi.org/10.1016/j.asoc.2023.110769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of particulate matter (PM) concentration around open-pit mining is crucial for its control. To achieve this, machine learning (ML) techniques have been attempted in PM estimation without reaching their full potential. In the current study, an artificial neural network (ANN) is employed for the prediction of PM 2.5 2.5 concentration (PM μm ), PM 10 concentration (PM μm ), and total suspended particulate (TSP). The influence of training algorithms and epochs on the modeling accuracy of ANN is investigated. Moreover, the convergence of ANN modeling is studied using Monte Carlo simulations . Compared with the quasi-Newton method (QNM) and conjugate gradient backpropagation (CGB), the one-step secant algorithm (OSSM) is the most suitable training algorithm for PM concentration prediction. Using the 0.1\% bound as a convergence criterion, 500 Monte Carlo simulations are enough to get the converged results. The ANN-OSSM achieves high accuracy in PM concentration prediction. The R values on the testing set of PM 2.5 2.5 , PM 10 , and TSP are 0.887, 0.943, 0.886, respectively. Compared with the results in the literature, accuracy improvement is achieved using ANN-OSSM, with R increases from 0.86 to 0.92 (TSP dataset), from 0.91 to 0.94 (PM 2.5 2.5 dataset), and from 0.84 to 0.89 (PM 10 dataset). The present ML model is higher in accuracy and more straightforward in methodology compared with the hybridized model in the literature, which will promote its industrial application in the near future.},
  archive      = {J_ASOC},
  author       = {Xiang Lu and Wei Zhou and Hai Bang Ly and Chongchong Qi and Thuy-Anh Nguyen and May Huu Nguyen and Jiandong Huang and Binh Thai Pham},
  doi          = {10.1016/j.asoc.2023.110769},
  journal      = {Applied Soft Computing},
  pages        = {110769},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of neural network parameters in improvement of particulate matter concentration prediction of open-pit mining},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint dual-stream interaction and multi-scale feature
extraction network for multi-spectral pedestrian detection.
<em>ASOC</em>, <em>147</em>, 110768. (<a
href="https://doi.org/10.1016/j.asoc.2023.110768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral images can provide more information, so pedestrian detection based on multispectral images has received wide attention. Existing multispectral networks mainly focus on the misalignment of image pairs and the difference between modalities. However, these network structures lack effective information interaction between two feature streams and fail to consider the scale characteristics of pedestrian objects. To deal with this issue, we propose a high-performance network structure, which is called dual-stream interaction and multi-scale feature extraction network (DSI-MSE), and contains a dual-stream feature interaction (DSI) block, a multi-scale feature extraction (MSE) block and a detection (DET) block. The DSI block extracts the features through the dual-stream interaction of RGB images and thermal images , which fuses the intra-modal information and the inter-modal information. The MSE block is designed by multiple parallel branches for matching multiple scales of pedestrian, which enhances the expressiveness of features and refines richer feature expressions at different scales. Experimental results on KAIST and CVC-14 datasets demonstrate that the proposed DSI-MSE can obtain the state-of-the-art results on multi-spectral pedestrian detection tasks.},
  archive      = {J_ASOC},
  author       = {Wenjun Hu and Chenglong Fu and Runlong Cao and Ying Zang and Xiao-Jun Wu and Shigen Shen and Xiao-Zhi Gao},
  doi          = {10.1016/j.asoc.2023.110768},
  journal      = {Applied Soft Computing},
  pages        = {110768},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint dual-stream interaction and multi-scale feature extraction network for multi-spectral pedestrian detection},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuroevolution with box mutation: An adaptive and modular
framework for evolving deep neural networks. <em>ASOC</em>,
<em>147</em>, 110767. (<a
href="https://doi.org/10.1016/j.asoc.2023.110767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of self-evolving neural networks has driven the emerging field of Evolutionary Deep Learning , which combines the strengths of Deep Learning and Evolutionary Computation. This work presents a novel method for evolving deep neural networks by adapting the principles of Geometric Semantic Genetic Programming , a subfield of Genetic Programming, and Semantic Learning Machine . Our approach integrates evolution seamlessly through natural selection with the optimization power of backpropagation in deep learning, enabling the incremental growth of neural networks’ neurons across generations. By evolving neural networks that achieve nearly 89\% accuracy on the CIFAR-10 dataset with relatively few parameters, our method demonstrates remarkable efficiency, evolving in GPU minutes compared to the field standard of GPU days.},
  archive      = {J_ASOC},
  author       = {Frederico J.J.B. Santos and Ivo Gonçalves and Mauro Castelli},
  doi          = {10.1016/j.asoc.2023.110767},
  journal      = {Applied Soft Computing},
  pages        = {110767},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neuroevolution with box mutation: An adaptive and modular framework for evolving deep neural networks},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Speech emotion recognition based on meta-transfer learning
with domain adaption. <em>ASOC</em>, <em>147</em>, 110766. (<a
href="https://doi.org/10.1016/j.asoc.2023.110766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning often requires large amounts of labeled data to train the model, which is not always readily available in the field of speech emotion recognition (SER). Related research work on SER in few shot conditions has reported problem with overfifitting and domain transfer of training. In this study, a few-shot learning method based on meta-transfer learning with domain adaption (MTLDA) is proposed for SER. It not only effectively reduces the over-fitting phenomenon of deep neural networks (DNN) trained with a small number of samples, but also solves the forgetting problem in meta-learning and the target domain adaptability problem in transfer learning . Experiments on three databases (i.e., CASIA is used for pre-training, Emo-DB and SAVEE are used for few-shot learning) are performed for few-shot learning of SER, from which the WAR is 65.12\% and UAR is 64.50\% on Emo-DB, and the WAR is 58.84\% and UAR is 53.26\% on SAVEE.},
  archive      = {J_ASOC},
  author       = {Zhen-Tao Liu and Bao-Han Wu and Meng-Ting Han and Wei-Hua Cao and Min Wu},
  doi          = {10.1016/j.asoc.2023.110766},
  journal      = {Applied Soft Computing},
  pages        = {110766},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Speech emotion recognition based on meta-transfer learning with domain adaption},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A competitive intelligence acquisition framework for mining
user perception from user generated content. <em>ASOC</em>,
<em>147</em>, 110764. (<a
href="https://doi.org/10.1016/j.asoc.2023.110764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitive intelligence is an important basis for companies in the process of developing competitive strategies. However, competitive intelligence acquisition methods such as surveys and expert ratings are not able to achieve rapid responses to user perceptions. In addition, the huge amount of unstructured user-generated content makes it more difficult to analyze user perceptions. In this paper, we propose a competitive intelligence mining framework for acquiring user perceptions from user-generated content. The framework covers multiple aspects of competitive intelligence acquisition, mining, analysis, and decision-making. A unified text processing model (WS-TCM) can automatically filter the content irrelevant to competitive intelligence and quickly extract fine-grained user perceptions of competitive attributes from the huge amount of user-generated content. In addition, the quantile-based intelligence mapping method (QB-IM) determines the competitive landscape of an enterprise based on fine-grained user perceptions and provides help for managers’ strategic decisions. In the case study, the method proposed in this paper has significantly improved compared with the baseline model in the stages of competitive intelligence identification, competitive attribute identification, and user perception identification. Especially, the accuracy improvement in identifying user dissatisfaction perception is more obvious. Meanwhile, our model shows strong robustness. In the competitive analysis stage, the results of the framework are consistent with those obtained by market analysts through surveys. This study provides a new approach to competitive intelligence research and fine-grained user perception mining, which improves the efficiency of competitive intelligence acquisition for companies while also improving the reliability, accuracy, and usefulness of competitive intelligence.},
  archive      = {J_ASOC},
  author       = {Jie Lin and Xiaoyan Jiang and Qing Li and Chao Wang},
  doi          = {10.1016/j.asoc.2023.110764},
  journal      = {Applied Soft Computing},
  pages        = {110764},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A competitive intelligence acquisition framework for mining user perception from user generated content},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual attention-based multi-step ahead prediction enhancement
for monitoring systems in industrial processes. <em>ASOC</em>,
<em>147</em>, 110763. (<a
href="https://doi.org/10.1016/j.asoc.2023.110763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial processes, the ability to predict future steps is essential as it offers long-term insights, benefiting strategic decision-making. However, traditional sequence-to-sequence models designed to predict dynamic behaviors suffer from accumulating errors during recurrent predictions which use previous outputs as inputs for the next time step. In this article, we propose a dual attention-based encoder–decoder framework, specifically designed to enhance multi-step ahead predictions in industrial processes. The dual attention model strategically minimizes the error accumulation of output sequence by leveraging a temporal attention mechanism , which focuses on relevant time-steps in the input sequence, and a supervised attention mechanism that assigns different weights to output sequence errors during training. The supervised attention method, in particular, provides a significant improvement by focusing on minimizing the error of earlier steps during backpropagation using predefined attention weights, resulting in enhanced overall multistep prediction performance. Experiments on real-world industrial datasets demonstrate that our approach outperforms baseline models , specifically simple sequence-to-sequence and single attention-based sequence-to-sequence models. In fact, our dual attention framework consistently surpasses single attention models, currently regarded as state-of-the-art, at all prediction stages. The suggested approach has potential applications in the field of process monitoring and model predictive control .},
  archive      = {J_ASOC},
  author       = {Nahyeon An and Seokyoung Hong and Yurim Kim and Hyungtae Cho and Jongkoo Lim and Il Moon and Junghwan Kim},
  doi          = {10.1016/j.asoc.2023.110763},
  journal      = {Applied Soft Computing},
  pages        = {110763},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual attention-based multi-step ahead prediction enhancement for monitoring systems in industrial processes},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructing shadowed set based on game analysis of
uncertainty and decision cost. <em>ASOC</em>, <em>147</em>, 110762. (<a
href="https://doi.org/10.1016/j.asoc.2023.110762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a three-way approximation scheme to deal with fuzzy set, shadowed set approximately divides fuzzy objects with uncertainty into elevated, reduced, and shadow areas by a pair of decision thresholds α , β α, β , thus effectively reducing the uncertainty of information. However, the current construction of shadowed set is all based on a single principle, lacking the comprehensive consideration of multiple principles, and these construction principles focus on the rationality of semantic interpretation of model construction process and threshold determination, and lack of consideration of the validity of partition results. To resolve these issues, this paper proposes a new shadowed set model through the game analysis between uncertainty and decision cost (UC-GTSS). First, Combining shadowed set construction with game theory, UC-GTSS is proposed based on game competition mechanism. Through the game analysis of uncertainty and decision cost, the optimal α , β α, β is found, and a game payoff fusion algorithm based on Topsis is proposed. Second, the definition of UC-GTSS model expression, game player, strategy, payoff function , equilibrium analysis, optimization α , β α, β determination and the influence of model parameters are analyzed and discussed. Third, the UC-GTSS model is extended and discussed based on the validity of the partition results, such as the game between approximate classification accuracy and coverage, the game between the Gini coefficient of the partition areas. Finally, through the analysis of algorithms, instances and data experiments, the construction process , validity, and rationality of UC-GTSS and its extended model are demonstrated.},
  archive      = {J_ASOC},
  author       = {Man Gao and Qinghua Zhang and Fan Zhao and Chengying Wu and Guoyin Wang and Deyou Xia},
  doi          = {10.1016/j.asoc.2023.110762},
  journal      = {Applied Soft Computing},
  pages        = {110762},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Constructing shadowed set based on game analysis of uncertainty and decision cost},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Study on deep reinforcement learning-based multi-objective
path planning algorithm for inter-well connected-channels.
<em>ASOC</em>, <em>147</em>, 110761. (<a
href="https://doi.org/10.1016/j.asoc.2023.110761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defining inter-well connectivity is very important for the water injection development of carbonate fractured-vuggy reservoirs. However, most conventional methods based on logging or seismic data are subjective. In this research, multi-attribute seismic data were used to characterize the 3D search space, and an improved reward function was designed based on the law of fluid flow and appropriate heuristics. A deep reinforcement learning model based on an improved reward function was designed to search for inter-well connected channels. Selecting the shortest channel length and the largest flow volume were the optimization objectives . We propose the multi-objective evolutionary optimization algorithm with decomposition and differential evolution (MOEA/D-DE) based on an improved mutation operator to automatically obtain the optimal inter-well connected channels. The searched channels can readily show the spatial distribution of multi-scale fractures and caves. The experimental results of the Tahe oilfield show that the improved algorithm effectively enhanced the local convergence performance of the multi-objective algorithm and that the automatic search paths were consistent with the characteristics of seismic static data and tracer test results. Our model can better reflect the spatial distribution of a fracture cavity at different scales and offers important guidance for on-site development adjustment work in the water injection development stage.},
  archive      = {J_ASOC},
  author       = {Ruiqi Wang and Dongmei Zhang and Zhijiang Kang and Rucheng Zhou and Gang Hui},
  doi          = {10.1016/j.asoc.2023.110761},
  journal      = {Applied Soft Computing},
  pages        = {110761},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Study on deep reinforcement learning-based multi-objective path planning algorithm for inter-well connected-channels},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-epsilon adversarial attack against a neural network
online image stream classifier. <em>ASOC</em>, <em>147</em>, 110760. (<a
href="https://doi.org/10.1016/j.asoc.2023.110760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adversary intercepts a stream of images between a sender and a receiver neural network classifier. To minimize its footprint, the adversary only attacks a limited number of images within the stream. The adversary is interested in maximizing the number of successfully conducted attacks among all performed attacks. Upon the arrival of each image and before the arrival of the following image, the adversary must irrevocably decide whether it wants to attack the current image or not. The target model is a fixed deep neural network that may use any form of regularization . The adversary has query access to the target model, which can feed images and obtain the loss, which may contain regularization and classification loss terms. Since this paper’s proposed method needs classification loss term alone, it also suggests a novel method in which the adversary estimates the regularization loss term and eliminates it. All images are partitioned into three groups based on their after-attack classification loss and treated according to their group. Moreover, this paper provides some promising test results on various datasets.},
  archive      = {J_ASOC},
  author       = {Hossein Mohasel Arjomandi and Mohammad Khalooei and Maryam Amirmazlaghani},
  doi          = {10.1016/j.asoc.2023.110760},
  journal      = {Applied Soft Computing},
  pages        = {110760},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Low-epsilon adversarial attack against a neural network online image stream classifier},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A holistic failure modes and effects analysis for university
plastic injection laboratory under bayesian network. <em>ASOC</em>,
<em>147</em>, 110759. (<a
href="https://doi.org/10.1016/j.asoc.2023.110759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {University Research &amp; Development (R&amp;D) laboratories are facilities used for research and manufacturing activities in engineering and basic science fields. Various filling materials are used in plastic R&amp;D laboratories to improve the mechanical properties of thermoplastics or to recycle waste materials. The use of filling materials causes changes in plastic production parameters. This study dealt with the risks arising from extrusion, injection and shredder in a plastic injection laboratory, which led to the inability to manufacture the appropriate product under a holistic methodology. The proposed holistic methodology merges the concepts of failure mode and effects analysis (FMEA), fuzzy best-worst method (FBWM), and a rule-based three-hierarchical Bayesian network (RB-THBN). First, 15 root risks have been identified under a three-stage hierarchy with the support of laboratory decision makers . Then, questionnaires were created using the FMEA concept. The importance values (degree of belief-DoB) of three different FMEA parameters (severity, occurrence and detection) by experts for each risk have been calculated with FBWM. And finally, the RB-THBN has been created, and the final risk values have been computed using the linear utility function for the network. The system has been analyzed via a Bayesian modeling software. A validity test and a control measures planning has also been executed. According to the results, the highest root risk has been determined as “E2-The prepared mixture is not homogeneous” with a crisp risk score of 0.639559. In addition, “Failure due to extrusion” has been determined as the highest risk category with a score of 0.564059. It was discussed in detail that these risks arise from homogenization and how the used mixture should be homogenized. A comparative study among traditional FMEA, FMEA extended under FBWM and the proposed approach is performed to observe the differences in final ranking of root risks and to highlight the arguments that strengthen the advantages of the proposed method over the other two.},
  archive      = {J_ASOC},
  author       = {Melih Yucesan and Muhammet Gul and Dragan Pamučar},
  doi          = {10.1016/j.asoc.2023.110759},
  journal      = {Applied Soft Computing},
  pages        = {110759},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A holistic failure modes and effects analysis for university plastic injection laboratory under bayesian network},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A population-based approach for multi-agent interpretable
reinforcement learning. <em>ASOC</em>, <em>147</em>, 110758. (<a
href="https://doi.org/10.1016/j.asoc.2023.110758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Agent Reinforcement Learning (MARL) made significant progress in the last decade, mainly thanks to the major developments in the field of Deep Neural Networks (DNNs). However, DNNs suffer from a fundamental issue: their lack of interpretability. While this is true for most applications of DNNs, this is exacerbated in their applications in MARL. In fact, the mutual interactions between agents and environment, as well as across agents, make it particularly difficult to understand learned strategies in these settings. One possible way to achieve explainability in MARL is through the use of interpretable models, such as decision trees, that allow for a direct inspection and understanding of their inner workings. In this work, we make a step forward in this direction, proposing a population-based algorithm that combines evolutionary principles with RL for training interpretable models in multi-agent systems. We evaluate the proposed approach in a highly dynamic task where two teams of agents compete with each other. We test different variants of the proposed method in different settings, namely with/without coevolution and with/without initialization from a handcrafted policy. We find that, in most settings, our method is able to find fairly effective policies. Moreover, we show that the learned policies are easy to inspect and, possibly, interpreted based on domain knowledge.},
  archive      = {J_ASOC},
  author       = {Marco Crespi and Andrea Ferigo and Leonardo Lucio Custode and Giovanni Iacca},
  doi          = {10.1016/j.asoc.2023.110758},
  journal      = {Applied Soft Computing},
  pages        = {110758},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A population-based approach for multi-agent interpretable reinforcement learning},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and consensus content validity of the questionnaire
for b-learning education: A 2-tuple fuzzy linguistic delphi based
decision support tool. <em>ASOC</em>, <em>147</em>, 110755. (<a
href="https://doi.org/10.1016/j.asoc.2023.110755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic Delphi and Fuzzy Delphi methods are used to test content validity of data collection tools such as questionnaires. Fuzzy Delphi takes the opinion issued by judges from a linguistic perspective reducing ambiguity in opinions by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic Delphi method to deal with scenarios in which judges show different expertise degrees by using fuzzy multigranular semantics of the linguistic terms and to obtain intermediate and final results expressed by 2-tuple linguistic values . The key idea of our proposal is to validate the full questionnaire by means of the evaluation of its parts, defining the validity of each item as a Decision Making problem. Taking the opinion of experts, we measure the degree of consensus, the degree of consistency, and the linguistic score of each item, in order to detect those items that affect, positively or negatively, the quality of the instrument. Considering the real need to evaluate a b-learning educational experience with a consensual questionnaire, we present a Decision Making model for questionnaire validation that solves it. Additionally, we contribute to this consensus reaching problem by developing an online tool under GPL v3 license. The software visualizes the collective valuations for each iteration and assists to determine which parts of the questionnaire should be modified to reach a consensual solution.},
  archive      = {J_ASOC},
  author       = {Rosana Montes and Cristina Zuheros and Jeovani Morales and Noe Zermeño and Jeronimo Duran and Francisco Herrera},
  doi          = {10.1016/j.asoc.2023.110755},
  journal      = {Applied Soft Computing},
  pages        = {110755},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design and consensus content validity of the questionnaire for b-learning education: A 2-tuple fuzzy linguistic delphi based decision support tool},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-based POI grouping with transformer for next point
of interest recommendation. <em>ASOC</em>, <em>147</em>, 110754. (<a
href="https://doi.org/10.1016/j.asoc.2023.110754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence of location-based services, Point of Interest (POI) recommendation has become an active research topic. While Graph Neural Networks (GNNs) have been widely used in POI recommendation models, they suffer from computational efficiency limitations when the graph structure is large. In this paper, we propose a new next POI recommendation model, which is backboned by a lightweight, feature-based POI grouping (FPG) method and a Transformer network. A unique feature of the proposed model is it uses the FPG method, which divides POIs into multiple groups based on their geographical and popularity features and analyze the similarity among the users’ preferences on the groups. By using the FPG method rather than graph-based structures, the proposed model largely reduces the computational cost in making next POI recommendation. The POI embeddings generated by the FPG method are then fed into a Transformer to generate the recommendation result. We test the proposed model on three real-world datasets and conduct comprehensive comparison studies to validate the performance of the model. The experiment results show that the proposed model has superior computational efficiency while preserving sufficient next POI recommendation accuracy. Key findings and critical implications from the experiment result and the mechanistic design of the model are also discussed in detail.},
  archive      = {J_ASOC},
  author       = {Yuhang He and Wei Zhou and Fengji Luo and Min Gao and Junhao Wen},
  doi          = {10.1016/j.asoc.2023.110754},
  journal      = {Applied Soft Computing},
  pages        = {110754},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature-based POI grouping with transformer for next point of interest recommendation},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A decision support mechanism in the determination of organic
waste collection and recycling center location: A sample application for
turkiye. <em>ASOC</em>, <em>147</em>, 110752. (<a
href="https://doi.org/10.1016/j.asoc.2023.110752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The location problem of organic waste collection and recycling plants is a significant concern for policy makers in developing countries. Factors such as rapid population growth, unplanned urbanization, and limited resources pose challenges to the efficient management of organic waste. Inadequate waste disposal practices not only endanger the environment and human health but also hinder the development of sustainable and livable cities. Therefore, it is crucial to address the specific challenges associated with locating organic waste collection and recycling facilities to ensure effective waste management and promote environmental sustainability. This study aims to identify the most appropriate location for an organic waste collection and recycling facility for compost production. This study involves identifying the most influential aspects of organic waste collection and recycling center location selection through a literature review and expert opinion. The Analytical Hierarchy Process (AHP) methodology with fuzzy interval type-2 (T2F) sets is employed to weight the criteria. The COmbinative Distance-based Assessment (CODAS) approach is extended in a T2F environment to better reflect the uncertainty and evaluate the alternative locations. The study concludes by revealing the optimal location for the food waste collection center for Trabzon city in Turkiye. To verify the validity and reliability of the proposed integrated methodology, a comparative analysis was also performed using various decision-making methods.},
  archive      = {J_ASOC},
  author       = {Ertugrul Ayyildiz and Melike Erdogan},
  doi          = {10.1016/j.asoc.2023.110752},
  journal      = {Applied Soft Computing},
  pages        = {110752},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decision support mechanism in the determination of organic waste collection and recycling center location: A sample application for turkiye},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimized equation based on the gene expression
programming method for estimating tunnel construction costs considering
a variety of variables and indexes. <em>ASOC</em>, <em>147</em>, 110749.
(<a href="https://doi.org/10.1016/j.asoc.2023.110749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate cost estimation in tunneling is key to the project’s success. Such information is critical for the early conceptual and design phases when key choices must be made. Numerous variables influence the cost of tunnel construction, and limited information is available at the early stages of design when the possible use of tunnels is being studied. Therefore, there is a limited number of models at engineers’ disposal to develop a proper cost estimate for tunneling projects. This study aimed to offer a model for estimating the construction cost of drilling and blasting tunnels in the preliminary stage of a project. For this purpose, an optimized gene expression programming (GEP) method was used based on the study of 900 data points obtained from ten drilling and blasting tunnels, which were randomly split into the training (800 data points) and testing (100 data points) datasets. With the experience of previously constructed tunnels, eleven parameters were considered the most effective for the tunnel’s construction cost. The best fit on predictions generated an equation for the optimized GEP model. Finally, by comparing the equation’s outputs with the actual costs and its behavior with practice, its potential ability to estimate the construction cost of drilling and blasting tunnels was approved. Moreover, the Graphical User Interface (GUI) of the GEP model was created as a practical tool for estimating the construction cost of tunnels. This model can reduce tunnel uncertainties and give ML development in tunnel planning.},
  archive      = {J_ASOC},
  author       = {Arsalan Mahmoodzadeh and Hamid Reza Nejati},
  doi          = {10.1016/j.asoc.2023.110749},
  journal      = {Applied Soft Computing},
  pages        = {110749},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized equation based on the gene expression programming method for estimating tunnel construction costs considering a variety of variables and indexes},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TLTD: Transfer learning for tabular data. <em>ASOC</em>,
<em>147</em>, 110748. (<a
href="https://doi.org/10.1016/j.asoc.2023.110748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have become effective for various machine learning tasks. DNNs are known to achieve high accuracy with unstructured data in which each data sample (e.g., image) consists of many raw features (e.g., pixels) of the same type. The effectiveness of this approach diminishes for structured (tabular) data. In most cases, decision tree-based models such as Random Forest (RF) or Gradient Boosting Decision Trees (GBDT) outperform DNNs. In addition, DNNs tend to perform poorly when the number of samples in the dataset is small. This paper introduces Transfer Learning for Tabular Data (TLTD) which utilizes a novel learning architecture designed to extract new features from structured datasets. Using the DNN’s learning capabilities on images, we convert the tabular data into images, then use the distillation technique to achieve better learning. We evaluated our approach with 25 structured datasets, and compared the outcomes to those of RF, eXtreme Gradient Boosting (XGBoost), Tabnet, KNN, and TabPFN. The results demonstrate the usefulness of the TLTD approach.},
  archive      = {J_ASOC},
  author       = {Maxim Bragilovski and Zahi Kapri and Lior Rokach and Shelly Levy-Tzedek},
  doi          = {10.1016/j.asoc.2023.110748},
  journal      = {Applied Soft Computing},
  pages        = {110748},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TLTD: Transfer learning for tabular data},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A currency trading system based on simplified models using
fuzzy multi-criteria hierarchical optimization. <em>ASOC</em>,
<em>147</em>, 110747. (<a
href="https://doi.org/10.1016/j.asoc.2023.110747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is based on some assumptions validated using real data and the most used Forex trading platform Meta Trader 4. First, we assume that any reasonable and relatively simple models, reflecting some trading hypotheses, can be profitable for a certain period. Having a sufficient set of such models optimized for selected currency pairs, we can use the model that provides the greatest profit in the current trading period. The second proposal is to use a fuzzy multiple-criteria approach at the training optimization stage using historical data in order to overcome or reduce the negative effect of overfitting. Here for the first time, the problem of fuzzy multiple-criteria optimization of trading was formulated and solved based on the output parameters of the developed simple single-criteria crisp models. This provides significantly greater profit than that obtained using single-criteria crisp models. The third proposal is to use the hierarchical structure of fuzzy local criteria to solve the multiple-criteria problem. It is shown that this additionally provides a significant increase in profit. The profitability and riskless of the developed trading models are studied based on real quotations of currency pairs USDJPY, EURUSD and AUDUSD using H4 timeframe.},
  archive      = {J_ASOC},
  author       = {Pavel Sevastjanov and Krzysztof Kaczmarek and Leszek Rutkowski},
  doi          = {10.1016/j.asoc.2023.110747},
  journal      = {Applied Soft Computing},
  pages        = {110747},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A currency trading system based on simplified models using fuzzy multi-criteria hierarchical optimization},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep residual error and bag-of-tricks learning for
gravitational wave surrogate modeling. <em>ASOC</em>, <em>147</em>,
110746. (<a href="https://doi.org/10.1016/j.asoc.2023.110746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have been employed in gravitational-wave astronomy to accelerate the construction of surrogate waveforms for the inspiral of spin-aligned black hole binaries, among other applications. We face the challenge of modeling the residual error of an artificial neural network that models the coefficients of the surrogate waveform expansion (especially those of the phase of the waveform) which we demonstrate has sufficient structure to be learnable by a second network. Adding this second network, we were able to reduce the maximum mismatch for waveforms in a validation set by 13.4 times. We also explored several other ideas for improving the accuracy of the surrogate model , such as the exploitation of similarities between waveforms, the augmentation of the training set, the dissection of the input space, using dedicated networks per output coefficient and output augmentation. In several cases, small improvements can be observed, but the most significant improvement still comes from the addition of a second network that models the residual error . Since the residual error for more general surrogate waveform models (when e.g., eccentricity is included) may also have a specific structure, one can expect our method to be applicable to cases where the gain in accuracy could lead to significant gains in computational time.},
  archive      = {J_ASOC},
  author       = {Styliani-Christina Fragkouli and Paraskevi Nousi and Nikolaos Passalis and Panagiotis Iosif and Nikolaos Stergioulas and Anastasios Tefas},
  doi          = {10.1016/j.asoc.2023.110746},
  journal      = {Applied Soft Computing},
  pages        = {110746},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep residual error and bag-of-tricks learning for gravitational wave surrogate modeling},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentiment analysis of online reviews for electric vehicles
using the SMAA-2 method and interval type-2 fuzzy sets. <em>ASOC</em>,
<em>147</em>, 110745. (<a
href="https://doi.org/10.1016/j.asoc.2023.110745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous electric vehicles (EVs) consumers express their opinions about EVs on automotive websites. These online reviews can be mined and analyzed to understand consumer sentiment and preferences. Although the existing sentiment analysis (SA) has made insight into sentiment polarity of EVs, the SA results on different EV attributes do not consider the accuracy of sentiment classification and the difficulty of accurately obtaining attribute weights in the process of all attribute information fusion. To address these issues, we provide an integrated framework for the EV SA problem using the stochastic multi-criteria acceptability analysis (SMAA) method with interval type-2 fuzzy sets (IT2FSs). We first use a logistic regression model to classify the sentiment polarity for each online comment and construct IT2FS by sentiment classification accuracy to represent the SA results for different EV attributes. Based on this, we considered all feasible weights in the weight space and utilize the SMAA-IT2FS method to aggregate SA results and achieve EV ranking. In addition, our approach provide decision makers with information to assist in decision-making such as the use of central weight vectors to identify the strengths and weaknesses of EVs and the analysis of potential competitors through dominance relationships. Finally, real data extracted from AutoHome websites are used as experimental data to illustrate the implementation of the proposed method and to demonstrate the effectiveness of our approach.},
  archive      = {J_ASOC},
  author       = {Bengang Gong and Rui Liu and Xiaoqi Zhang and Ching-Ter Chang and Zhi Liu},
  doi          = {10.1016/j.asoc.2023.110745},
  journal      = {Applied Soft Computing},
  pages        = {110745},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment analysis of online reviews for electric vehicles using the SMAA-2 method and interval type-2 fuzzy sets},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking surrogate-based optimisation algorithms on
expensive black-box functions. <em>ASOC</em>, <em>147</em>, 110744. (<a
href="https://doi.org/10.1016/j.asoc.2023.110744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate algorithms such as Bayesian optimisation are especially designed for black-box optimisation problems with expensive objectives, such as hyperparameter tuning or simulation-based optimisation. In the literature, these algorithms are usually evaluated with synthetic benchmarks which are well established but have no expensive objective, and only on one or two real-life applications which vary wildly between papers. There is a clear lack of standardisation when it comes to benchmarking surrogate algorithms on real-life, expensive, black-box objective functions. This makes it very difficult to draw conclusions on the effect of algorithmic contributions and to give substantial advice on which method to use when. A new benchmark library, EXPObench, provides first steps towards such a standardisation. The library is used to provide an extensive comparison of six different surrogate algorithms on four expensive optimisation problems from different real-life applications. This has led to new insights regarding the relative importance of exploration, the evaluation time of the objective, and the used model. We also provide rules of thumb for which surrogate algorithm to use in which situation. A further contribution is that we make the algorithms and benchmark problem instances publicly available, contributing to more uniform analysis of surrogate algorithms. Most importantly, we include the results of the six algorithms on all evaluated problem instances. This unique new dataset lowers the bar for researching new methods as the number of expensive evaluations required for comparison and for the creation of new surrogate models is significantly reduced.},
  archive      = {J_ASOC},
  author       = {Laurens Bliek and Arthur Guijt and Rickard Karlsson and Sicco Verwer and Mathijs de Weerdt},
  doi          = {10.1016/j.asoc.2023.110744},
  journal      = {Applied Soft Computing},
  pages        = {110744},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Benchmarking surrogate-based optimisation algorithms on expensive black-box functions},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-averse two-stage stochastic programming-based
closed-loop supply chain network design under uncertain demand.
<em>ASOC</em>, <em>147</em>, 110743. (<a
href="https://doi.org/10.1016/j.asoc.2023.110743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a growing awareness and acceptance of the concept of green environmental protection among the public, primarily due to the worsening environmental issues and increased emphasis by governments on corporate social responsibility regulations. As a result, the management of closed-loop supply chains has gained significant attention from both businesses and researchers. Most stochastic programming models addressing closed-loop supply chain (CLSC) under uncertainty are typically designed to be risk-neutral. In this paper, a risk-averse two-stage stochastic programming (RATSSP) model for the closed-loop supply chain network design (CLSCND) problem considering facility capacity level decisions under uncertain demand. It is aiming to trade off the expected total profit and the profit risk, based on a given degree of responsive risk aversion. Due to the NP-hard nature of the CLSCND problem, this paper applies simplified swarm optimization (SSO), genetic algorithm (GA), and particle swarm optimization (PSO) algorithms to solve the problem. To improve the efficiency and quality of the obtained solutions, we propose a parallel computation meta-heuristic algorithm, parallel simplified swarm optimization (PSSO), along with parallel versions of PSO and GA. Computational results show the effectiveness of our proposed model, with PSSO demonstrating the best robustness and solution quality among the considered algorithms.},
  archive      = {J_ASOC},
  author       = {Zhenyao Liu and Li-Man Hu and Wei-Chang Yeh},
  doi          = {10.1016/j.asoc.2023.110743},
  journal      = {Applied Soft Computing},
  pages        = {110743},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Risk-averse two-stage stochastic programming-based closed-loop supply chain network design under uncertain demand},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BAND: BAgging noise detectors with application to
semiconductor wafer denoising. <em>ASOC</em>, <em>147</em>, 110742. (<a
href="https://doi.org/10.1016/j.asoc.2023.110742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In semiconductor manufacturing processes, spatial defect patterns on semiconductor wafers can provide useful information to quality engineers regarding the root causes of abnormalities. As early detection of process problems increases the wafer yield, the automatic recognition of defect patterns is crucial. It its intrinsic assumption that a decluttering of noise defects helps in accurately classifying defect patterns. However, most existing noise detection methods require the pre-determination of parameters that significantly affect the detection performance. In this paper, we propose a parameter-free detection framework called BAND, which refers to BAgging Noise Detectors, to solve this problem. By adopting the framework of bootstrap aggregating (bagging), which combines the detection results from multiple outlier detection algorithms, the proposed framework ensures the diversity of randomly chosen parameter values while maintaining an accurate detection of noise defects. Based on the estimated distributions of outlierness scores, a group of outliers is discriminated from the other group of inliers via spectral clustering . This procedure enables the proposed framework to be parameter-free. Moreover, numerical experiments based on synthetic and real-world datasets are presented to demonstrate the effectiveness of the proposed framework.},
  archive      = {J_ASOC},
  author       = {Taeheung Kim and Jong-Seok Lee},
  doi          = {10.1016/j.asoc.2023.110742},
  journal      = {Applied Soft Computing},
  pages        = {110742},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BAND: BAgging noise detectors with application to semiconductor wafer denoising},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Short-term urban rail transit passenger flow forecasting
based on fusion model methods using univariate time series.
<em>ASOC</em>, <em>147</em>, 110740. (<a
href="https://doi.org/10.1016/j.asoc.2023.110740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global urbanization has made the urban rail transit system an essential service for a growing population. To help urban rail transit stations design optimal operational plans, previous studies have devoted extensive efforts to passenger flow forecasting, especially short-term predictions. By considering the complex pattern of passenger flow, previous research investigated the feasibility of machine learning (ML) methods on different data features and found the limited application of a single ML method. Based on the dynamic historical passenger flow data at an urban rail station, this study proposes an ML-fusion strategy to enhance prediction accuracy, including data aggregation, time series forecasting model selection, and fusion model strategy. First, this study aggregates the data into working days, weekends, and hourly time series for single model development. Based on the predictive performance of single model development, this study selects XGBoost , AdaBoost , and LightGBM from the widely used ML-method pool. To overcome prediction errors caused by the discrepancy between characteristics of passenger flow and single prediction models, the proposed ML-fusion model combines single forecasting models with dynamically predicted passenger flow to enhance the accuracy and efficiency of the prediction. Based on the experimental results, the mean absolute error is 1.54, and the regression coefficient is 0.99, which is in close agreement with unity, which validates that the proposed ML-fusion method has displayed superiority over all other single models tested both in accuracy and stability.},
  archive      = {J_ASOC},
  author       = {Dung David Chuwang and Weiya Chen and Ming Zhong},
  doi          = {10.1016/j.asoc.2023.110740},
  journal      = {Applied Soft Computing},
  pages        = {110740},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term urban rail transit passenger flow forecasting based on fusion model methods using univariate time series},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-EvoGAN: Bi-level evolutionary approach for generative
adversarial networks. <em>ASOC</em>, <em>147</em>, 110738. (<a
href="https://doi.org/10.1016/j.asoc.2023.110738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) are an adversarial approach to generative modeling using deep learning methods , which have become one of the most relevant topics in the last years. Despite producing interesting outputs, recent studies have demonstrated that GANs remain difficult to control and suffer from training problems, such as instability and mode collapse, as well as several aspects of their models are designed by hand. Neuroevolution is a recent evolution strategy that uses Evolutionary Algorithms (EAs) in order to provide an automatic design of deep neural networks architectures. Many recent efforts have been dedicated to contributing on GANs by developing different single-level EAs approaches, however they could not solve their common issues completely which are still open problems. In this paper, we propose a new design of evolutionary algorithms in the context of GANs, consisting of dividing the evolution search space into two complementary levels, modeling a Bi-level Evolutionary Approach called Bi-EvoGAN searching to surpass their adversarial optimization difficulties and also to demonstrate the performance of considering two levels of optimization in comparison to a single optimization level. To the best of our knowledge, this work presents the first application of the bi-level optimization in the context of generative adversarial networks. The upper level uses an Evolutionary Algorithm-based Topology Optimization method named “EA-TO” to evolve a population of GAN networks as a set of individuals in order to find the best network topology . The lower level applies an Evolutionary Algorithm-based Decomposition for Hyperparameter Optimization termed “EA/D-HO” in order to select the best hyperparameters of each GAN individual. The performance of Bi-EvoGAN is explained by the double optimization of the GAN topology and their hyperparameters through the new bilevel optimization design allowing to evolve a GAN population in its higher fitness which will improve the quality of the search method and consequently to accelerate the convergence. Experimental studies are carried out based on four well-known benchmark datasets, MNIST, Fashion-MNIST, CelebA and LSUN-Bedroom allowing to show the efficiency of our approach in comparison to the other state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Houssem Eddine Nouri and Abdennaceur Ghandri and Olfa Belkahla Driss and Khaled Ghedira},
  doi          = {10.1016/j.asoc.2023.110738},
  journal      = {Applied Soft Computing},
  pages        = {110738},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-EvoGAN: Bi-level evolutionary approach for generative adversarial networks},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Backpropagation-free 4D continuous ant-based neural topology
search. <em>ASOC</em>, <em>147</em>, 110737. (<a
href="https://doi.org/10.1016/j.asoc.2023.110737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous Ant-based Topology Search (CANTS) is a previously introduced novel nature-inspired neural architecture search (NAS) algorithm that is based on ant colony optimization (ACO). CANTS utilizes a continuous search space to indirectly-encode a neural architecture search space. Synthetic ant agents explore CANTS’ continuous search space based on the density and distribution of pheromones, strongly inspired by how ants move in the real world. This continuous search space allows CANTS to automate the design of artificial neural networks (ANNs) of any size, removing a key limitation inherent to many current NAS algorithms that must operate within structures of a size that is predetermined by the user. This work expands CANTS by adding a fourth dimension to its search space representing potential neural synaptic weights . Adding this extra dimension allows CANTS agents to optimize both the architecture as well as the weights of an ANN without applying backpropagation (BP), which leads to a significant reduction in the time consumed in the optimization process: at least an average of 96\% less time consumption with very competitive optimization performance , if not better. The experiments of this study – using real-world data – demonstrate that the BP-Free CANTS algorithm exhibits highly competitive performance compared to both CANTS and ANTS while requiring significantly less operation time.},
  archive      = {J_ASOC},
  author       = {AbdElRahman ElSaid and Karl Ricanek and Zimeng Lyu and Alexander Ororbia and Travis Desell},
  doi          = {10.1016/j.asoc.2023.110737},
  journal      = {Applied Soft Computing},
  pages        = {110737},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Backpropagation-free 4D continuous ant-based neural topology search},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kriging-assisted indicator-based evolutionary algorithm for
expensive multi-objective optimization. <em>ASOC</em>, <em>147</em>,
110736. (<a href="https://doi.org/10.1016/j.asoc.2023.110736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selection mechanisms based on performance indicators are popular for solving multi-objective optimization problems (MOPs), which can provide a comprehensive evaluation of the convergence and diversity of candidate solutions. However, these mechanisms recently encounter difficulties when tackling computationally expensive MOPs attributed to the restricted quantity of real function evaluations. To tackle this problem, this paper proposes a Kriging-assisted indicator-based evolutionary algorithm . The primary concept is to utilize the Kriging model to approximate computationally expensive objective functions, thereby reducing the computational cost, and to employ the R2 indicator to assess the quality of candidate solutions. In addition, a dual selection mechanism based on the lower confidence bound is proposed as the model management strategy to balance exploration and exploitation. The experimental results demonstrate that the proposed algorithm is competitive when compared to five representative algorithms.},
  archive      = {J_ASOC},
  author       = {Fei Li and Yujie Yang and Zhengkun Shang and Siyuan Li and Haibin Ouyang},
  doi          = {10.1016/j.asoc.2023.110736},
  journal      = {Applied Soft Computing},
  pages        = {110736},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Kriging-assisted indicator-based evolutionary algorithm for expensive multi-objective optimization},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing innovation capabilities of manufacturing companies
by combination of unsupervised and supervised machine learning
approaches. <em>ASOC</em>, <em>147</em>, 110735. (<a
href="https://doi.org/10.1016/j.asoc.2023.110735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study suggests the use of unsupervised and supervised machine learning algorithms to categorize companies according to their innovation capabilities. Companies are categorized into three groups: good, satisfactory, and unsatisfactory, in order to create a thorough and reliable assessment procedure. In this study, unsupervised and supervised machine learning methods are used to solve an innovation capability evaluation problem. Data is provided via a survey which is performed in manufacturing industry in Turkiye Firstly, dimensions of innovation capability were determined Principal Component Analysis (PCA). Then data labels were determined by k-means clustering algorithm which is an unsupervised learning technique. A model is first trained using data provided via questionnaire survey, and it is then tested using fresh, unused data. The model is trained using classification algorithms including KNN, GaussianNB, RandomForest, Gradient Boosting, AdaBoost , DesisionTree, XGBOOST and LightGBMC, MLPC, and SVMC and its performance is evaluated against test data. Each classification techniques are evaluated using the performance metrics. With the highest accuracy rate of 93\% and lowest MAE , MSE and RMSE values, The LightGBMC and SVMC methods were found the most efficient supervised learning method for innovation capability evaluation.},
  archive      = {J_ASOC},
  author       = {Gulsen Akman and Bahadir Yorur and Ali Ihsan Boyaci and Ming-Chuan Chiu},
  doi          = {10.1016/j.asoc.2023.110735},
  journal      = {Applied Soft Computing},
  pages        = {110735},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessing innovation capabilities of manufacturing companies by combination of unsupervised and supervised machine learning approaches},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ensemble learning approach for resampling forgery
detection using markov process. <em>ASOC</em>, <em>147</em>, 110734. (<a
href="https://doi.org/10.1016/j.asoc.2023.110734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resampling is an extremely well-known technique performed for Image forgery detection . It includes the changes in the content of a picture in terms of rotation, stretching/zooming, and shrinking, to frame a forged picture that is a localized forgery in comparison to the original picture. With the wrong intention, resampling forgery has been increased day by day, and its negative impact has been increased in criminology, law enforcement, forensics, research etc. Accordingly, the interest in the algorithm of image resampling forgery detection is significantly developed in image forensics . In this paper, a novel image resampling forgery detection technique has been proposed. In the proposed technique, two types of Markov feature with spatial and Discrete Cosine Transform domains have been extracted to recognize the resampling operation. The spatial domain gives the information for the distribution of the pixels and DCT gives the edge information. Further, these Markov features are consolidated. Due to high dimensionality hard thresholding technique is used for reducing the dimensionality. Then, these Markov features are applied to the set of models of different classifiers. With the utilization of classifiers, weighted majority voting values have been calculated during the ensemble classification. Unlike the other techniques, these weighted voting boundaries have been consequently balanced during the training process until the best accuracy has been obtained. However, it is very difficult to get best accuracy so for getting best accuracy this research needs to do lots of iterations and trained the dataset. For the comparative study very few research has been found for this resampling forgery technique with different interpolation techniques and classifier. Still, comparison has been done with some latest research work. The comparative analysis shows that the proposed ensemble learning-based algorithm provides the best outcomes with the accuracy of 99.12\% for bicubic, 98.89\% for bilinear, and 98.23\% for lanczos3 kernel with considerably less complexity and high speed in comparison to prior techniques which are using single support vector machine for classification. Moreover, the proposed algorithm also detects a very low probability of error of 0.44\% and detects the type of interpolation kernel, size of the forgery, and the type of resampling, whether it is up sampling and down sampling, using Graphical User Interface which has not been detected previously with multiple forgery detection.},
  archive      = {J_ASOC},
  author       = {Rachna Mehta and Karan Kumar and Adi Alhudhaif and Fayadh Alenezi and Kemal Polat},
  doi          = {10.1016/j.asoc.2023.110734},
  journal      = {Applied Soft Computing},
  pages        = {110734},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble learning approach for resampling forgery detection using markov process},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-strategy surrogate-assisted competitive swarm
optimizer for expensive optimization problems. <em>ASOC</em>,
<em>147</em>, 110733. (<a
href="https://doi.org/10.1016/j.asoc.2023.110733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary computation is a powerful tool for solving nonconvex optimization problems . Generally, evolutionary algorithms take numerous fitness evaluations to obtain the potential optimal solutions. This poses a critical challenge for applying them to real-world complex engineering optimization problems. Recently, surrogate-assisted evolutionary algorithms (SAEAs) have attracted an increasing amount of research. In this paper, a surrogate-assisted competitive swarm optimizer (SACSO) is proposed to exploit the potential of evolutionary algorithms to handle expensive optimization problems. In SACSO, global search, local search and opposition-based search are implemented as three different criteria to select the appropriate particle for realistic fitness evaluation. In order to trade off global exploitation and local exploration, a dynamic adaptation strategy is also proposed in this paper. Search approaches are dynamically adjusted to select a particle or perform variations at different stages of the algorithm. The combination of generalized surrogate model (GSM) with global search and elite surrogate model (ESM) with local and opposition-based search, effectively enhances the optimal performance of SACSO. The proposed SACSO is comprehensively compared with the state-of-the-art SAEAs and well-known EAs on seven benchmark functions . Additionally, SACSO is applied to the speed reducer design optimization problem. Experimental simulation results suggest SACSO is a prospective tool for dealing with expensive optimization problems.},
  archive      = {J_ASOC},
  author       = {Jeng-Shyang Pan and Qingwei Liang and Shu-Chuan Chu and Kuo-Kun Tseng and Junzo Watada},
  doi          = {10.1016/j.asoc.2023.110733},
  journal      = {Applied Soft Computing},
  pages        = {110733},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-strategy surrogate-assisted competitive swarm optimizer for expensive optimization problems},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A convolutional neural network based classification for
fuzzy datasets using 2-d transformation. <em>ASOC</em>, <em>147</em>,
110732. (<a href="https://doi.org/10.1016/j.asoc.2023.110732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researches on deep learning methods have been actively conducted for the past 10 years, and various deep learning techniques have been proposed by many researchers. In addition, prediction methods using deep learning are widely used in various fields. In particular, convolution neural network (CNN) is most commonly applied to analyze visual images, but it can be also applied to many other data. On the other hand, fuzzy theory has been applied to deep learning techniques in traffic problem, agriculture, and airline customer service. In the case of data containing ambiguous information, data analysis can be performed using soft methods. In particular, the fuzzy theory is widely used to deal with such data. So, when the data includes vague information a fuzzy number can be applied to input/output data. In this paper, seven models using CNN have been proposed to analyze fuzzy input containing ambiguous or linguistic information. Our proposed models use five activation functions . For the data analysis, three datasets including Iris data, US Health Insurance data, Wine quality data are used to compare the seven proposed Fuzzy CNN models .},
  archive      = {J_ASOC},
  author       = {Jon-Lark Kim and Byung-Sun Won and Jin Hee Yoon},
  doi          = {10.1016/j.asoc.2023.110732},
  journal      = {Applied Soft Computing},
  pages        = {110732},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A convolutional neural network based classification for fuzzy datasets using 2-D transformation},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MS-LSTM: Exploring spatiotemporal multiscale representations
in video prediction domain. <em>ASOC</em>, <em>147</em>, 110731. (<a
href="https://doi.org/10.1016/j.asoc.2023.110731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The drastic variation of motion in spatial and temporal dimensions makes the video prediction task extremely challenging. Existing RNN models obtain higher performance by deepening or widening the model. They obtain the multi-scale features of the video only by stacking layers, which is inefficient and brings unbearable training costs (such as memory, FLOPs, and training time). Different from them, this paper proposes a spatiotemporal multi-scale model called MS-LSTM wholly from a multi-scale perspective. On the basis of stacked layers, MS-LSTM incorporates two additional efficient multi-scale designs to fully capture spatiotemporal context information. Concretely, we employ LSTMs with mirrored pyramid structures to construct spatial multi-scale representations and LSTMs with different convolution kernels to construct temporal multi-scale representations. We theoretically analyze the training cost and performance of MS-LSTM and its components. Detailed comparison experiments with twelve baseline models on four video datasets show that MS-LSTM has better performance but lower training costs.},
  archive      = {J_ASOC},
  author       = {Zhifeng Ma and Hao Zhang and Jie Liu},
  doi          = {10.1016/j.asoc.2023.110731},
  journal      = {Applied Soft Computing},
  pages        = {110731},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MS-LSTM: Exploring spatiotemporal multiscale representations in video prediction domain},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image forgery detection by transforming local descriptors
into deep-derived features. <em>ASOC</em>, <em>147</em>, 110730. (<a
href="https://doi.org/10.1016/j.asoc.2023.110730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forgery is the intentional alteration of digital images, either manually using image editors or through deep fake techniques, for the purpose of disseminating fake information. We propose a forgery detection approach that efficiently detects copy-move and splicing attacks of varying scales in digital images. Our goal is to identify the homogeneous region(s) inconsistent with the rest of the image. This region property has been typically employed in object detection and classification, while we exploit this property to detect forgery in images. Thus, we generate the deep-derived features from the existing hand-crafted features in forgery detection as input to the VGG16, a deep learning method, trained for object classification. We use a binary class SVM trained on the obtained deep-derived features to determine whether an image is real or fake. We perform extensive experiments on three publicly available image manipulation datasets, DVMM, Casia and Korus to validate the effectiveness of the proposed methodology. The results show a better accuracy compared to the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Muhammad Aqib Anwar and Syed Fahad Tahir and Labiba Gillani Fahad and Kashif Kifayat},
  doi          = {10.1016/j.asoc.2023.110730},
  journal      = {Applied Soft Computing},
  pages        = {110730},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image forgery detection by transforming local descriptors into deep-derived features},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-surrogate assisted PSO with adaptive speciation for
expensive multimodal multi-objective optimization. <em>ASOC</em>,
<em>147</em>, 110724. (<a
href="https://doi.org/10.1016/j.asoc.2023.110724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expensive multimodal multi-objective optimization problems (MMOPs) commonly arise in real-world situations, in which multiple Pareto solutions correspond to the same objective values. In order to provide satisfactory solutions for decision makers , plenty of evolutionary algorithms (EAs) have been used to solve MMOPs. However, the traditional EAs is limited in practice due to the time-consuming problem caused by a large number of fitness evaluations. To tackle this issue, a multi-surrogate assisted particle swarm optimization with adaptive speciation is proposed in this paper. In the proposed algorithm, a multi-surrogate model using adaptive scheduling strategy is introduced to reveal the many-to-one mapping relationship between decision space and objective space. To trade-off between convergence and diversity in both decision and objective space, a speciation method with adaptive niche radius is designed to perform species self-organization and evolution. Moreover, a surrogate assisted evolution with leader updating strategy is performed to guide the evolution of species. In order to verify the effectiveness of the proposed method, the benchmark problem (from IEEE Congress on evolutionary computation 2019) is analyzed experimentally. The experimental results show that the proposed method is superior to the state-of-the-art multimodal multi-objective evolutionary algorithm in term of maintaining the diversity of solutions in the decision and objective space.},
  archive      = {J_ASOC},
  author       = {Zhiming Lv and Dangdang Niu and Shuqin Li and Hongguang Sun},
  doi          = {10.1016/j.asoc.2023.110724},
  journal      = {Applied Soft Computing},
  pages        = {110724},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-surrogate assisted PSO with adaptive speciation for expensive multimodal multi-objective optimization},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Air quality particulate-pollution prediction applying GAN
network and the neural turing machine. <em>ASOC</em>, <em>147</em>,
110723. (<a href="https://doi.org/10.1016/j.asoc.2023.110723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban areas in many countries face significant concerns over the presence of aerosol particles and their effects on human health. These particles, which range in size from 1 nanometer to 100 micrometers, can easily penetrate organic matter and transport toxic gas compounds and mineral substances such as carbon monoxide, ozone, nitrogen dioxide, and sulfur dioxide. High concentrations of airborne particles pose serious challenges to health, the economy, the environment, and society, and it is crucial to investigate ways to improve the quality of life for people. The main objective of this study is to develop a framework that accurately predicts aerosol and air quality index (AQI) values in advance by estimating missing data and predicting future data. To achieve this, we propose the DAerosol.GAN.NTM framework, which combines a neural Turing machine with a generative adversarial network (GAN) to address the limitations of previous studies. Our framework outperforms previous methods, including DAerosol.NTM (without GAN) and four baseline studies using Multilayer Perceptron (MLP), Deep Neural Networks (DNN), Long-short Term Memory (LSTM), and Deep LSTM (DLSTM) models, in terms of accuracy, precision, root mean square error (RMSE), mean absolute percentage error (MAPE), and the prediction of aerosol pollution surge hours in advance according to the Time Interval Before and After the Aerosol Event (TIBAAE) criterion.},
  archive      = {J_ASOC},
  author       = {Zahra-Sadat Asaei-Moamam and Faramraz Safi-Esfahani and Seyedali Mirjalili and Reza Mohammadpour and Mohamad-Hosein Nadimi-Shahraki},
  doi          = {10.1016/j.asoc.2023.110723},
  journal      = {Applied Soft Computing},
  pages        = {110723},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Air quality particulate-pollution prediction applying GAN network and the neural turing machine},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascaded-ANFIS to simulate nonlinear rainfall–runoff
relationship. <em>ASOC</em>, <em>147</em>, 110722. (<a
href="https://doi.org/10.1016/j.asoc.2023.110722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hydrologic models require atmospheric, dynamic and static models to simulate river flow from catchments. Thus the accuracy of hydrologic modelling highly depends on the data quality. Therefore, simulation is always challenging in data-scarcity environments. In addition, physical flow measurements are infeasible in the Spatiotemporal domain, and soft computing techniques are helpful in river flow simulation in data-scarcity environments. In this research paper, an efficient and accurate Cascaded-ANFIS-based model for rainfall–runoff was proposed and evaluated using five case studies in three countries: Japan , Vietnam, and Sri Lanka. The investigation focused on predicting streamflow by the influence of past data, with each river’s dataset examined to determine the best configuration of past rainfalls affecting streamflow volume. The proposed algorithm was compared against six state-of-the-art regression algorithms. The results showed that it outperformed the other algorithms in every case study except the Kalu River dataset, with zero bias calculated. The developed R-R model can be considered a generic model for streamflow prediction in data-scarcity environments, with excellent acceptability of simulated river flows against measured river flows observed across different geographic and climatic regions.},
  archive      = {J_ASOC},
  author       = {Namal Rathnayake and Upaka Rathnayake and Imiya Chathuranika and Tuan Linh Dang and Yukinobu Hoshino},
  doi          = {10.1016/j.asoc.2023.110722},
  journal      = {Applied Soft Computing},
  pages        = {110722},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cascaded-ANFIS to simulate nonlinear rainfall–runoff relationship},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary learning of selection hyper-heuristics for text
classification. <em>ASOC</em>, <em>147</em>, 110721. (<a
href="https://doi.org/10.1016/j.asoc.2023.110721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an evolutionary model in the scope of automated machine learning . This model is in charge of learning hyper-heuristics that represent selection rules of the form if-then, such that given a dataset for a text classification problem, the hyper-heuristics select the best classification method to use with it, based on the data distribution of the dataset. The evolutionary model starts by building a set of hyper-heuristics using a series of meta-features extracted from a training group of datasets that represent their data distribution. Hyper-heuristics are then evolved using adapted crossover and mutation operators . During the evolution, each hyper-heuristic is evaluated on its performance to classify each dataset in the training group. When the evolutionary process is done, the best hyper-heuristic is selected and evaluated for its generality with an independent test group of datasets. The results show that the best learned hyper-heuristic obtains an average classification performance close to the general optimum, and has a similar performance to the two most popular state-of-the-art automated machine learning systems, but with less computational cost. The approach used by the present model is relevant for automated machine learning in three aspects, the generality of the hyper-heuristics so they could be applied to groups of datasets; the interpretability of the representations that facilitate the understanding of the method selection by non-expert users; and the reduction of computational time and resources to reach a decision. Furthermore, the model extends the applicability of evolutionary computation methods, with their problem-independent properties and their ability to explore search spaces, to tackle new complex problems, such as the decision of the best classifier for a text classification dataset.},
  archive      = {J_ASOC},
  author       = {Jonathán de Jesús Estrella Ramírez and Juan Carlos Gomez},
  doi          = {10.1016/j.asoc.2023.110721},
  journal      = {Applied Soft Computing},
  pages        = {110721},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary learning of selection hyper-heuristics for text classification},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capacity-aware fair POI recommendation combining transformer
neural networks and resource allocation policy. <em>ASOC</em>,
<em>147</em>, 110720. (<a
href="https://doi.org/10.1016/j.asoc.2023.110720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point of Interest (POI) recommendations have primarily focused on maximising user satisfaction, while neglecting the needs of POIs and their operators. One such need is recommendation exposure, which can lead to envy among the POIs. Some POIs may be under-recommended, while others may be over-recommended, resulting in dissatisfaction for both staff and users due to long queues or overcrowding. Existing work has not addressed the trade-off between satisfying user preferences and being fair to POIs, which typically aim to operate at capacity. Therefore, we introduce the POI fair allocation problem to model this issue, taking into account both user satisfaction and POI exposure fairness. To address this problem, we propose a fair POI allocation technique that balances user satisfaction and POI capacity-based exposure simultaneously. Our proposed model utilises existing (transformer neural networks and attention LSTM model) personalised POI recommendation models that capture users’ spatio-temporal influences and interests in POI visits. We then propose POI capacity-based allocation using the over-demand cut policy and under-demand add policy, which ensures POI exposure ratio and envy-freeness up to certain thresholds. We evaluate the performance of our proposed model on five datasets containing real-life POI visits. Experimental evaluations show that our proposed model outperforms baselines in terms of user and POI-based evaluation metrics . To ensure reproducibility, we have publicly shared our source code at Codeocean .},
  archive      = {J_ASOC},
  author       = {Sajal Halder and Kwan Hui Lim and Jeffrey Chan and Xiuzhen Zhang},
  doi          = {10.1016/j.asoc.2023.110720},
  journal      = {Applied Soft Computing},
  pages        = {110720},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Capacity-aware fair POI recommendation combining transformer neural networks and resource allocation policy},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PLAHS: A partial labelling autonomous hyper-heuristic system
for industry 4.0 with application on classification of cold stamping
process. <em>ASOC</em>, <em>147</em>, 110718. (<a
href="https://doi.org/10.1016/j.asoc.2023.110718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-life industry it is difficult to have fully-labelled datasets due to lack of time, resources or knowledge. In this sense, this paper proposes the design and development of a Partial Labelling Autonomous Hyper-heuristic System PLAHS, a solution that autonomously labels partially labelled databases and evaluates the yielded labelling solution by means of a novel Trustworthiness Metric (TM). The proposal combines a hyper-heuristic inspired approach with a Semi Supervised Learning Clustering (SSLC) methodology that optimizes the parameters of different clustering algorithms , based on a novel semi-supervised metric named Partially Supervised Optimization Metric (PSOM). The proposal has been tested with promising and excellent results on both a real use case for labelling work orders in a cold stamping press, and 13 databases from the UCI (multivariate data) and UCR (time series data) repositories.},
  archive      = {J_ASOC},
  author       = {Adriana Navajas-Guerrero and Eva Portillo and Diana Manjarres},
  doi          = {10.1016/j.asoc.2023.110718},
  journal      = {Applied Soft Computing},
  pages        = {110718},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PLAHS: A partial labelling autonomous hyper-heuristic system for industry 4.0 with application on classification of cold stamping process},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-attribute decision-making fusion model for stock
trading with customizable investor personality traits in a picture fuzzy
environment. <em>ASOC</em>, <em>147</em>, 110715. (<a
href="https://doi.org/10.1016/j.asoc.2023.110715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a fuzzy logic-based machine learning (ML) algorithm is introduced. This proposed ML algorithm accepts picture fuzzy sets (PFS) as the fuzzified input and incorporates genetic algorithm (GA) during the training process. The proposed ML algorithm is then incorporated into two well-known decision-making methods, namely the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) and Evaluation Based on Distance from Average Solution (EDAS). These two decision-making methods and the proposed ML algorithm are then applied to solve a multi-attribute decision-making (MADM) problem related to the evaluation and ranking of public listed companies based on their stock performance, in accordance with investors’ personalities. The actual daily closing stock price of five public listed companies from the big market capitalization (Big Cap) category traded in the Kuala Lumpur Stock Exchange (KLSE) for a period of 10 years is used as the datasets for this study. Monte Carlo simulation is used to verify the accuracy of the results. In addition, a comprehensive comparative study of some recent PFS-based decision-making methods in the existing literature and the proposed methods is conducted, and all the typical instances of the investors’ personalities are observed. The results obtained through this comparative study corroborates the results obtained via the proposed methods, and this proves the effectiveness of the proposed methods. The differences in the results obtained via the different methods are analyzed and discussed, and this again proves that the results obtained via the proposed methods are effective and consistent with the judgments of human experts.},
  archive      = {J_ASOC},
  author       = {Shio Gai Quek and Ganeshsree Selvachandran and Angie Yih Tsyr Wong and Feng Shin Wong and Weiping Ding and Ajith Abraham},
  doi          = {10.1016/j.asoc.2023.110715},
  journal      = {Applied Soft Computing},
  pages        = {110715},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-attribute decision-making fusion model for stock trading with customizable investor personality traits in a picture fuzzy environment},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Problem feature based meta-heuristics with q-learning for
solving urban traffic light scheduling problems. <em>ASOC</em>,
<em>147</em>, 110714. (<a
href="https://doi.org/10.1016/j.asoc.2023.110714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An urban traffic light scheduling problem (UTLSP) is studied by using problem feature based meta-heuristics with Q-learning. The goal is to minimize the network-wise total delay time within a time window by finding a high-quality schedule of traffic lights. First, a dynamic flow model is used to describe the UTLSP in a scheduling framework. Second, four improved meta-heuristics combining Q-learning are proposed, including harmony search (HS), water cycle algorithm (WCA), Jaya, and artificial bee colony (ABC) algorithms. Five problem feature based local search operators are constructed. During the iterative process, Q-learning is employed to select the local search operators with strong competitiveness. Two ensemble strategies are proposed to combine meta-heuristics and Q-learning. Finally, experiments are conducted based on real traffic data. The performance of the improved meta-heuristics with Q-learning is verified by solving eighteen cases with different scales. Numerical results and comparisons show that the proposed algorithms have statistical improvements over their peers. The proposed feature-based ABC with Q-learning has the strongest competitiveness among all compared ones.},
  archive      = {J_ASOC},
  author       = {Liang Wang and Kaizhou Gao and Zhongjie Lin and Wuze Huang and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2023.110714},
  journal      = {Applied Soft Computing},
  pages        = {110714},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Problem feature based meta-heuristics with Q-learning for solving urban traffic light scheduling problems},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SupRB in the context of rule-based machine learning methods:
A comparative study. <em>ASOC</em>, <em>147</em>, 110706. (<a
href="https://doi.org/10.1016/j.asoc.2023.110706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehending why an artificial intelligence–based agent makes certain predictions is often discussed as one of the central issues that needs to be solved in the near future to make such agents ubiquitously deployable. However, many common black-box machine learning models are hard to analyse. Recently, we proposed a new machine learning algorithm to construct concise sets of rules. This algorithm, SupRB, creates compact, interpretable and transparent models. One key feature that differentiates it from many similar previously proposed systems is that it optimizes the model selection tasks concerning rule discovery and rule set composition separately. Therefore, users can tailor a model’s structure more easily towards fulfilling use-case specific explainability requirements. Keeping the model selection tasks separate during optimization allows for more clearly defined goals when compared to state-of-the-art systems and – importantly – facilitates independent rule fitnesses, making the training process easier to explain and the models easier to analyse post-hoc. In this paper, we extend previous benchmarking of SupRB, where it was compared with XCSF, a prominent rule set learning system, to also include Decision Trees (DTs) and Random Forests (RFs) in our analysis. We find that – as expected – SupRB performs comparable to XCSF, while allowing easier control of model structure and showing a substantially smaller sensitivity to random seeds and data splits, and it performs slightly worse than RFs, and better than DTs. Importantly, the overall model size was substantially smaller with SupRB, making it the easiest to interpret system while not falling significantly short in terms of prediction errors.},
  archive      = {J_ASOC},
  author       = {Michael Heider and Helena Stegherr and Roman Sraj and David Pätzel and Jonathan Wurth and Jörg Hähner},
  doi          = {10.1016/j.asoc.2023.110706},
  journal      = {Applied Soft Computing},
  pages        = {110706},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SupRB in the context of rule-based machine learning methods: A comparative study},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective full connection neural network updating using a
quantized full FORCE algorithm. <em>ASOC</em>, <em>147</em>, 110703. (<a
href="https://doi.org/10.1016/j.asoc.2023.110703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new training algorithm that can update the situation of layers’ network, and therefore, connections, neurons, and firing rate of neurons based on FORCE (first-order reduced and controlled error) training algorithm . The Quantized Full FORCE algorithm (QFF) also updates the number of neurons and connections between different layers in the network per iteration in a way that the whole firing rate of each layer is updated via selecting the best neurons and combining strong features. The update method is sequential, so that with each instance passing through the network, the network structure is updated with the Full FORCE algorithm. The algorithm updates the structure of networks with a multiple/single middle layer of the supervised version of feed forward networks such as Multilayer perceptron (MLP), changing them into partially-connected networks. A combination of principal component analysis PCA and Linear Discriminant Analysis (LDA) algorithms has been used to cluster the network input features. The paper focuses on the deep supervised MLP network with backpropagation (BP) and various datasets and its comparison with other MLP based stat of art methods and hybrid evolutionary algorithms . We achieved 98.15 percent accuracy for facial expression 98.6 and 97.7 percent for Wisconsin breast Cancer and Iris Flower in respectively. The training algorithm employed in the study enjoys a lower computational complexity while yielding faster and more accurate convergence, starting with a very low level of errors of 0.009 in comparison with the full connection network and it solves the challenge of getting stuck in local minima and poor convergence of Gradient Decent with BP.},
  archive      = {J_ASOC},
  author       = {Mehdi Heidarian and Gholamreza Karimi},
  doi          = {10.1016/j.asoc.2023.110703},
  journal      = {Applied Soft Computing},
  pages        = {110703},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective full connection neural network updating using a quantized full FORCE algorithm},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiobjective dynamic rebalancing evolutionary algorithm
for free-floating bike sharing. <em>ASOC</em>, <em>147</em>, 110696. (<a
href="https://doi.org/10.1016/j.asoc.2023.110696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The big challenge of free-floating bike sharing system is to rebalance the supply and demand in different places. Most studies are static rebalancing that moves bikes at night. Dynamic rebalancing considers the scheduling of transport vehicles during the day. It is more effective and difficult to solve. In this paper, a multiobjective dynamic rebalancing problem is solved. The two optimization objectives are considered: minimizing the total distance of rebalancing vehicle and minimizing the total demand unmet. A mathematic model is built to formulate the problem. A new algorithm SFMEA-AP (Sequence Flow Multiobjective Evolutionary Algorithm with Area Protected) that uses the sequence flow encoding is proposed. An novel area protected strategy that includes a non-dominated sorting with similarity comparison is proposed. It simultaneously considers the fairness and competitiveness in solution space. Six neighborhood search operators are designed. They are used as variable neighborhood search strategy. Computational experiments show that SFMEA-AP can obtain better solution quality than SFMEA-VNS, QMOEA, NSGA-II, ILS and LSMOVRPTW. The IGD values of non-dominated solutions of SFMEA-AP are better than those of SFMEA-VNS, QMOEA, NSGA-II, ILS and LSMOVRPTW. In addition, the non-dominated solution set of SFMEA-AP have good diversity.},
  archive      = {J_ASOC},
  author       = {Sheng Su and Haijie Yu and Dongwen Xiong and Xiaohua Dong},
  doi          = {10.1016/j.asoc.2023.110696},
  journal      = {Applied Soft Computing},
  pages        = {110696},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multiobjective dynamic rebalancing evolutionary algorithm for free-floating bike sharing},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Archimedean t-norm and t-conorm based intuitionistic fuzzy
WASPAS method to evaluate health-care waste disposal alternatives with
unknown weight information. <em>ASOC</em>, <em>146</em>, 110751. (<a
href="https://doi.org/10.1016/j.asoc.2023.110751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health-care waste (HCW) contains numerous hazardous, infectious, and radioactive contents that may cause serious health issues for society and drastic environmental problems. Therefore, it is crucial to dispose of HCW safely and appropriately. Four commonly used HCW disposal methods are incineration , steam sterilization , microwave, and landfill disposal. To select an appropriate HCW disposal method, there are several conflicting criteria. Furthermore, when information is lacking due to uncertainty and criteria weighing conditions are also unknown, the decision to choose an appropriate HCW disposal method becomes more challenging. So, the objective of this paper is to introduce a novel integrated multi-criteria group decision-making (MCGDM) approach to evaluate HCW disposal methods in which incomplete information is quantified using intuitionistic fuzzy sets (IFSs), unknown criteria weights are evaluated using the maximizing deviation method, and the evaluation of HCW disposal methods is done by applying the Archimedean t t -norm and t t -conorm-based interactive intuitionistic fuzzy weighted aggregated sum product assessment (WASPAS) method. Herein, the Archimedean t t -norm and t t -conorm cover a wide range of t t -norms and t t -conorms, including the Algebraic, Einstein, Hamacher, and Frank, etc., by adopting prescribed combinations of additive generators. The proposed MCGDM approach has been applied to evaluate the above-listed four HCW disposal methods for a case study conducted by Mishra et al. (2020) in the context of India. A sensitivity analysis is performed to examine the impact of changes in the parameters involved in the proposed MCGDM approach. A comparative study is also performed with some well-known multi-criteria decision making(MCDM) and MCGDM methods to demonstrate the consistency and stability of the proposed approach. Findings suggest that the newly developed MCGDM approach is more general, flexible, and provides realistic results to evaluate HCW disposal methods under uncertainty and unknown criteria weight information.},
  archive      = {J_ASOC},
  author       = {Komal},
  doi          = {10.1016/j.asoc.2023.110751},
  journal      = {Applied Soft Computing},
  pages        = {110751},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Archimedean t-norm and t-conorm based intuitionistic fuzzy WASPAS method to evaluate health-care waste disposal alternatives with unknown weight information},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential evolution with exponential crossover can be
also competitive on numerical optimization. <em>ASOC</em>, <em>146</em>,
110750. (<a href="https://doi.org/10.1016/j.asoc.2023.110750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential Evolution (DE) is a powerful population-based evolutionary algorithm for global optimization, and it is widely used in many scientific and engineering applications . There are two commonly used crossover schemes in the literature: one is binomial crossover and the other is exponential crossover. The majority of DE researchers believe that DE variants employing binomial crossover usually obtain superior performance than the ones employing exponential crossover on numerical optimization and DE variants with exponential crossover are good at tackling optimization problems with linkages among neighboring variables. On the contrary, here in this paper, a new perspective is proposed that DE variant with exponential crossover can obtain competitive performance with the ones employing binomial crossover on numerical optimization regardless of whether there are linkages among the variables or not after discovering the proper crossover rate C r Cr and its corresponding parameter control, and the main contributions of the paper can be summarized as follows: (1) The first powerful DE variant with exponential crossover, namely the DE-EXP algorithm, which is superior to the recent winner DE variants, e.g. LSHADE, iLSHADE and jSO, in Congress on Evolutionary Computation (CEC) competitions, is developed for numerical optimization; (2) A novel parameter control of crossover rate C r Cr is developed for exponential crossover and the value of C r Cr can be automatically generated not only in the initialization stage but also during the evolution. (3) A black-box model illustrating the fitness-value-dependency weakness of the recent winner DE variants in CEC competitions is given and a novel fitness-value-independent adaptation scheme for scale factor F F is proposed in the DE-EXP algorithm to overcome the fitness-value-dependency weakness. A larger test suite containing 88 benchmarks is used for the validation of our DE-EXP algorithm, and experiment results show its superiority in comparison with several state-of-the-art DE variants.},
  archive      = {J_ASOC},
  author       = {Zhenyu Meng and Yuxin Chen},
  doi          = {10.1016/j.asoc.2023.110750},
  journal      = {Applied Soft Computing},
  pages        = {110750},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution with exponential crossover can be also competitive on numerical optimization},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A class-level matching unsupervised transfer learning
network for rolling bearing fault diagnosis under various working
conditions. <em>ASOC</em>, <em>146</em>, 110739. (<a
href="https://doi.org/10.1016/j.asoc.2023.110739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective method, deep transfer learning is used to solve the problem of unsupervised fault diagnosis of rolling bearings . In the process of obtaining domain invariant features, the feature matching at the domain-level does not consider the distribution of each category in matching the global distribution of source domain features and target domain features. To solve this problem, a class-level matching transfer learning network is proposed. In this method, source domain data and target domain data from different categories are matched first. Meanwhile, domain-level matching and class-level matching are combined based on the maximum classifier discrepancy structure. Then the transfer training process is divided into three stages: domain-level matching, class-level matching, and target domain pseudo-label direct guidance network training. The proposed method utilizes the target domain pseudo-label output by the network to extract the features of the target data by self-directing. Compared with the improved maximum classifier discrepancy on the Paderborn University dataset, the proposed method improves the fault diagnosis average accuracy by 12.95\% on the six transfer tasks. It significantly improves the diagnostic accuracy of unsupervised bearing fault diagnosis under various working conditions.},
  archive      = {J_ASOC},
  author       = {Chunran Huo and Quansheng Jiang and Yehu Shen and Xiaoshan Lin and Qixin Zhu and Qingkui Zhang},
  doi          = {10.1016/j.asoc.2023.110739},
  journal      = {Applied Soft Computing},
  pages        = {110739},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A class-level matching unsupervised transfer learning network for rolling bearing fault diagnosis under various working conditions},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From black box to clear box: A hypothesis testing framework
for scalar regression problems using deep artificial neural networks.
<em>ASOC</em>, <em>146</em>, 110729. (<a
href="https://doi.org/10.1016/j.asoc.2023.110729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive predictive performance exhibited by deep learning across various domains, its application in research models within the social and behavioral sciences has been limited. Deep learning lacks human-accessible interpretability and does not provide statistical inferences. To address these limitations, this article presents a novel model-agnostic hypothesis testing framework tailored to scalar regression problems using deep artificial neural networks . The new framework not only determines each input variable’s direction of influence and statistical significance, but computes effect size measures akin to Cohen’s f 2 in traditional ordinary least squares (OLS) regression models. Effect sizes are an important complement to null hypothesis significance testing by providing a practical significance measure that is independent of sample size considerations. To showcase the usefulness of the new framework, its application is demonstrated on both an artificial data set and a social survey using a Python sandbox implementation.},
  archive      = {J_ASOC},
  author       = {Wolfgang Messner},
  doi          = {10.1016/j.asoc.2023.110729},
  journal      = {Applied Soft Computing},
  pages        = {110729},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From black box to clear box: A hypothesis testing framework for scalar regression problems using deep artificial neural networks},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of a cyber physical production system framework
for 3D printing analytics. <em>ASOC</em>, <em>146</em>, 110719. (<a
href="https://doi.org/10.1016/j.asoc.2023.110719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D printing technology is considered one of the emerging areas to deal with global sustainability challenges and to facilitate the Industry 4.0 adoption. However, 3D printing technology is still immature due to several limitations and negative perceptions about its quality and performance. The goal of this paper is to propose a cyber physical production system (CPPS) framework for a 3D printer to (i) monitor the process, parameters, and carbon footprint, (ii) predict the nozzle’s remaining useful life (RUL), and (iii) prescribe optimum 3D printing parameters for minimizing carbon footprint and printing time, simultaneously at the targeted surface quality. Experiments were designed based on Taguchi L-27 orthogonal array to investigate the relationship between printing parameters and performance characteristics. The usefulness of the proposed framework has been demonstrated for a 3D printer to predict the remaining useful life of the printer nozzle (prognostic model), and to find an optimal combination of printing parameters for the simultaneous optimization of sustainability and productivity at the targeted surface quality (prescriptive model). Layer height was found to have a statically significant impact on the specific carbon footprint followed by scale and bed temperature. Layer height is the only statically significant contributor to the surface roughness of 3D printed parts. The scale and layer height followed by infill have significant effect on the printing time. The significance of the present work lies in enhancing the performance of a conventional 3D printer using low-cost smart sensors , devices, and open-source software. The usefulness of the proposed CPPS framework is demonstrated as a decision support tool for a 3D printer real-time monitoring, visualization, and control. The proposed CPPS framework and its application for prognostic and prescriptive analytics is generic in nature, and is transferable and applicable to other FDM 3D printers, irrespective of brand and size.},
  archive      = {J_ASOC},
  author       = {Kuldip Singh Sangwan and Rishi Kumar and Christoph Herrmann and Dev Kartik Sharma and Rushil Patel},
  doi          = {10.1016/j.asoc.2023.110719},
  journal      = {Applied Soft Computing},
  pages        = {110719},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of a cyber physical production system framework for 3D printing analytics},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A thin-provisioned and functionalized memetic algorithm for
the single row facility layout problem. <em>ASOC</em>, <em>146</em>,
110716. (<a href="https://doi.org/10.1016/j.asoc.2023.110716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of single row facility layout problem (SRFLP) is to assemble a full permutation of facilities along a straight line so that the weighted sum of pairwise distances is minimized. This category of layout positioning problem was proven to be NP-hard. In this work, a thin-provisioned and functionalized memetic algorithm (TPFMA) for solving this problem is developed. The three main contributions of TPFMA are as follows: (i) a fixed population of four individuals equipped with different roles helps to remove redundancy and maintain the balance between exploitation and exploration; (ii) a pairwise precedences-based crossover contributes to the rapid identification of common good genes; and (iii) a specific probabilistic model assists in repairing infeasible solutions and restarting the population in a low-cost, efficient way. In particular, evaluations over 43 small instances show that this method successfully achieves the optimum solution but employs considerably less computing time than its competitors over quite a few examples. Moreover, TPFMA improves 21 previous best results out of the 40 most challenging large instances when running for the same amount of computing time. The results on other SRFLP instances with up to 1000 facilities suggest that TPFMA matches the best known values for the 40 benchmark instances out of the total 50 instances.},
  archive      = {J_ASOC},
  author       = {Zheng Wang and Chen Xu},
  doi          = {10.1016/j.asoc.2023.110716},
  journal      = {Applied Soft Computing},
  pages        = {110716},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A thin-provisioned and functionalized memetic algorithm for the single row facility layout problem},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate-assisted analysis of the parameter configuration
landscape for meta-heuristic optimisation. <em>ASOC</em>, <em>146</em>,
110705. (<a href="https://doi.org/10.1016/j.asoc.2023.110705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-heuristics can provide high-quality solutions to challenging problems in a reasonable amount of time, but are highly sensitive to the values assigned to their control parameters. The parameter configuration landscape (PCL) offers insight into the characteristics associated with optimisation of the parameter configuration of a meta-heuristic, but is poorly understood for most meta-heuristics. Further exacerbating this issue, determining the characteristics of the PCL is an extremely computationally expensive process. This study proposes the usage of artificial neural networks (ANNs) as surrogate models to greatly reduce the computational burden associated with characterising the PCL. Notably, this study represents the first usage of surrogate models in PCL research. Furthermore, this study presents a characterisation of the PCLs for both particle swarm optimization (PSO) and differential evolution (DE) employing ANN surrogate models, using five well-established fitness landscape analysis (FLA) metrics, and finds that the common assumption of correlation between the fitness and distance of control parameter settings is not strictly met. Overall, the training and usage of the surrogate models leads to a 99.86\% reduction in the number of algorithm executions required to attain the PCL samples used in the characterisation.},
  archive      = {J_ASOC},
  author       = {Kyle Robert Harrison},
  doi          = {10.1016/j.asoc.2023.110705},
  journal      = {Applied Soft Computing},
  pages        = {110705},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-assisted analysis of the parameter configuration landscape for meta-heuristic optimisation},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-stage feature selection approach using hybrid
quasi-opposition self-adaptive coati optimization algorithm for breast
cancer classification. <em>ASOC</em>, <em>146</em>, 110704. (<a
href="https://doi.org/10.1016/j.asoc.2023.110704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is one of the leading causes of high mortality rates among women. An early disease diagnosis is crucial in breast cancer’s treatment for improving the survival rate of people who have been afflicted. Human error, incorrect diagnoses, and insufficient time make manual techniques for breast cancer diagnosis ineffective and inefficient. An automated breast cancer diagnosis system assists the medical expert in diagnosing the cancer at its earliest stage. When attempting to investigate all aspects (high dimensionality) of the medical records, the automated diagnosis system faces significant challenges. It reduces the success rate of the diagnosis system. This study presents a two-stage feature subset selection approach using quasi-opposition self-adaptive coati optimization algorithm for breast cancer classification. The proposed scheme’s fitness function aims to maximize the success rate and minimize the feature selection ratio. We used the three benchmark datasets (Breast Cancer Wisconsin Diagnostic Dataset (WDBC), Breast Cancer Wisconsin Prognostic Dataset (WPBC), and Breast Cancer Wisconsin Original Dataset (WBCD)) to examine the proposed scheme’s performance. The comparison analysis results show that the proposed scheme outperforms the other competitor schemes.},
  archive      = {J_ASOC},
  author       = {Karpagalingam Thirumoorthy and Jerold John Britto J.},
  doi          = {10.1016/j.asoc.2023.110704},
  journal      = {Applied Soft Computing},
  pages        = {110704},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage feature selection approach using hybrid quasi-opposition self-adaptive coati optimization algorithm for breast cancer classification},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto weighted robust dual graph nonnegative matrix
factorization for multiview clustering. <em>ASOC</em>, <em>146</em>,
110702. (<a href="https://doi.org/10.1016/j.asoc.2023.110702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent multi-data fusion techniques have inspired multi-view clustering study, which has become a popular topic in machine learning . One successful way of solving multi-view clustering is nonnegative matrix factorization . This paper proposes an auto weighted robust dual graph nonnegative matrix factorization (ARDNMF), where ℓ 2 , 1 ℓ2, 1 norm is adopted to make ARDNMF robust as data contain noise and outliers. Through dual graphs, structure information in data space and feature space is considered. Moreover, an auto weighted assignment method is introduced in ARDNMF to assign appropriate weights to views. Effective updating rules are given. Experiments on real datasets show ARDNMF superiors to state-of-the-art algorithms in terms of clustering results .},
  archive      = {J_ASOC},
  author       = {Mengxue Jia and Sanyang Liu and Yiguang Bai},
  doi          = {10.1016/j.asoc.2023.110702},
  journal      = {Applied Soft Computing},
  pages        = {110702},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Auto weighted robust dual graph nonnegative matrix factorization for multiview clustering},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fusion algorithm based on whale and grey wolf optimization
algorithm for solving real-world optimization problems. <em>ASOC</em>,
<em>146</em>, 110701. (<a
href="https://doi.org/10.1016/j.asoc.2023.110701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to better understand and analyze population-based meta-heuristic optimization algorithms , this paper proposed a new hybrid algorithm combined Lévy flight with modified Whale Optimization Algorithm (WOA) and Grey Wolf Optimizer (GWO) , which is called LMWOAGWO to discard the dross and select the essence. Firstly, the population is initialized by using the uniform distribution space combined with the pseudo-reverse learning strategy, which lays the foundation for global search. Then, modifications were made to both WOA and GWO. For WOA algorithm, random adjustment control parameters strategy and different chaotic maps are used to adjust the main parameters of WOA to avoid the algorithm falling into local optimum in the later stage. For GWO algorithm, a new optimal solution is added to the grey wolf population to increase the optimal update position of the algorithm. On this basis, the dynamic weighting strategy is introduced to improve the convergence accuracy and convergence speed of the algorithm. Subsequently, new conditions were added during the WOA exploitation phase to formulate LMWOAGWO and the greedy strategy is used to retain better iteration update locations. Finally, Lévy flight is used to improve the global search ability of the algorithm. Extensive numerical experiments were conducted using 23 standard test benchmark functions , 25 CEC2005 functions, 15 popular benchmark functions and 10 CEC2019 functions to test the performance of LMWOAGWO compared with other well-known swarm optimization algorithms . Experimental and statistical results show that the performance of LMWOAGWO algorithm is better than many state-of-the-art algorithms. Then, 22 real-world optimization problems were used to further study the effectiveness of LMWOAGWO. Winners of CEC2020 Real World Single Objective Constraint Optimization Competition, such as iLSHADE ϵ ϵ algorithm, sCMAgES algorithm, COLSHADE algorithm and EnMODE algorithm are selected as four comparison algorithms in real world optimization problems . Experimental results show that the proposed LMWOAGWO has the capability to solve real-world optimization problems. Finally, the application efficiency of LMWOAGWO in solving two basic optimization problems in wireless networks is briefly introduced, and compared with the original WOA and GWO. Simulation results show that the performance of the LMWOAGWO is better than WOA and GWO.},
  archive      = {J_ASOC},
  author       = {Qian Yang and Jinchuan Liu and Zezhong Wu and Shengyu He},
  doi          = {10.1016/j.asoc.2023.110701},
  journal      = {Applied Soft Computing},
  pages        = {110701},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fusion algorithm based on whale and grey wolf optimization algorithm for solving real-world optimization problems},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Market index price prediction using deep neural networks
with a self-similarity approach. <em>ASOC</em>, <em>146</em>, 110700.
(<a href="https://doi.org/10.1016/j.asoc.2023.110700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock indexes are of vital importance to understand financial and economic markets of sectors and nations. Given the importance of market indexes, researchers, investors, and policy makers are continuously working to improve models to forecast market movements; small improvements in modeling have the potential for large financial gains. To this end, machine learning approaches have gained in popularity as both software and hardware performance have increased. In this work, we apply machine learning applications to the S&amp;P 500, DAX, AEX and the SMI indexes to improve forecasting performance. In particular, we take advantage of the fractal and Self-Similarity behaviors that exist in the time series of these indexes using simple Recurrent Neural Network , Multilayer Perceptron , and Long-Short Term Memory architectures. We apply the architecture using 60-, 30-, and 15-min windows of daily series for each approach and index. The results indicate that for the S&amp;P 500 the proposed self-similarity models outperform all base approaches. However, results are not uniform across all models and indexes. Overall, performance differences across models and indexes are presented. To check the difference between the forecasts of the self-similarity and base models, Model Confidence Set was used.},
  archive      = {J_ASOC},
  author       = {Carlos Mendoza and Werner Kristjanpoller and Marcel C. Minutolo},
  doi          = {10.1016/j.asoc.2023.110700},
  journal      = {Applied Soft Computing},
  pages        = {110700},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Market index price prediction using deep neural networks with a self-similarity approach},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving non-linear fixed-charge transportation problems
using nature inspired non-linear particle swarm optimization algorithm.
<em>ASOC</em>, <em>146</em>, 110699. (<a
href="https://doi.org/10.1016/j.asoc.2023.110699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this study is to formulate and solve the model of non-linear fixed-charge transportation problem (NFCTP), which is one of the NP-hard problems. This type of problem is challenging to solve using conventional methods because these methods are usually inefficient, costly to compute, and may stuck at the local optimum solution. To overcome this issue, a non-linear particle swarm optimization (NPSO) algorithm with new non-linear acceleration parameters is proposed in this study. Also, to preserve the feasibility condition (non-negative integer solution) of the transportation problem, two novel negative repair and fraction repair strategies have been incorporated into the proposed NPSO. The efficiency of the NPSO algorithm is tested on both the small-scale and large-scale NFCTPs. The dataset are considered from the existing studies. The obtained results are compared with those obtained by the spanning tree-based genetic algorithm (st-GA), priority-based genetic algorithm (pb-GA), and minimum cost flow-based genetic algorithm. The comparative study reveals that for all the considered problems, the proposed NPSO provides better feasible solutions in lesser computational time. Furthermore, the effectiveness of the NPSO algorithm is shown by comparing it to other seven existing variants of PSO and is found to outperformed these variants for the small scale as well as large scale NFCTPs.},
  archive      = {J_ASOC},
  author       = {Shivani and Deepika Rani},
  doi          = {10.1016/j.asoc.2023.110699},
  journal      = {Applied Soft Computing},
  pages        = {110699},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving non-linear fixed-charge transportation problems using nature inspired non-linear particle swarm optimization algorithm},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning by competing: Competitive multi-generator based
adversarial learning. <em>ASOC</em>, <em>146</em>, 110698. (<a
href="https://doi.org/10.1016/j.asoc.2023.110698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) have been extensively used for dozens of image enhancement and image translation applications, where several traditional and novel architectures have been and are still being introduced. However, the classical training protocols do not fully explore the significant potential of such architectures. In this paper, we propose a novel conditional GAN (cGAN) framework called competitive multi-generator based cGAN (CMcGAN) for more effective training progress and enhanced image generation . Different from classical adversarial learning protocols, the generators of a CMcGAN are not only challenged by the discriminator for improved performance, but also by other generators. This is achieved by a sharing mechanism, where the current performances of the competing generators are shared among each other, motivating each generator to perform better than the rest. Extensive experiments in several conditional image generation applications show the significant improvements achieved by several traditional architectures when trained following our proposed learning strategy. Moreover, these architectures, trained by our learning strategy, outperformed the state-of-the-art approaches proposed in each tested application.},
  archive      = {J_ASOC},
  author       = {I. Kajo and M. Kas and A. Chahi and Y. Ruichek},
  doi          = {10.1016/j.asoc.2023.110698},
  journal      = {Applied Soft Computing},
  pages        = {110698},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning by competing: Competitive multi-generator based adversarial learning},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual adversarial network with meta-learning for
domain-generalized few-shot text classification. <em>ASOC</em>,
<em>146</em>, 110697. (<a
href="https://doi.org/10.1016/j.asoc.2023.110697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning-based methods prevail in few-shot text classification . Current methods perform meta-training and meta-testing on two parts of a dataset in the same or similar domains. This results in a significant limit in model performance when faced with data from different domains, limiting the generalization of few-shot models. To solve this problem, this study proposes a new setting, namely, domain-generalized few-shot text classification . First, meta-training is conducted on a multi-domain dataset to learn a generalizable model. Subsequently, the model is meta-tested on a target dataset. In addition, a domain-generalized model, namely, a dual adversarial network, is designed to improve the meta-learning-based methods under domain drift between different datasets and domains. Unlike previous meta-learning methods, two N-way-K-shot tasks were input from different domains for a dual adversarial network at each episode. Dual adversarial networks leverage the features from two different domains for adversarial training to improve the domain adaptability of the model. The proposed model utilizes a domain-knowledge generator during adversarial training to produce domain-specific knowledge, and a domain discriminator to recognize the domain label of the produced knowledge. Extensive experiments are conducted to verify the effectiveness of the proposed settings and model. The experimental results show that the model performance in our proposed setting is improved by an average of 3.84\% compared to that in cross-domain few-shot text classification. Furthermore, the dual adversarial network significantly outperforms the five competitive baseline models , with an average improvement of 7.20\%. The proposed model achieves an average performance improvement of 2.69\% compared with the best baseline method .},
  archive      = {J_ASOC},
  author       = {Xuyang Wang and Yajun Du and Danroujing Chen and Xianyong Li and Xiaoliang Chen and Yongquan Fan and Chunzhi Xie and Yanli Li and Jia Liu and Hui Li},
  doi          = {10.1016/j.asoc.2023.110697},
  journal      = {Applied Soft Computing},
  pages        = {110697},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual adversarial network with meta-learning for domain-generalized few-shot text classification},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Q-learning-based hyper-heuristic evolutionary algorithm for
the distributed assembly blocking flowshop scheduling problem.
<em>ASOC</em>, <em>146</em>, 110695. (<a
href="https://doi.org/10.1016/j.asoc.2023.110695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed shop scheduling problems (DSSPs) have attracted increasing interest in recent years due to the technical trends of smart manufacturing and Industry 4.0 . The distributed assembly blocking flowshop scheduling problem (DABFSP) is a critical class of DSSPs with widespread applications in modern supply chains and manufacturing systems . In this paper, a Q -learning-based hyper-heuristic evolutionary algorithm (QLHHEA) is proposed to solve DABFSP with the objective of minimizing the makespan. Firstly, a mathematical model of DABFSP is formulated, and two insertion-based speedup strategies are devised to conserve the computational cost of evaluating solutions and to accelerate the search efficiency. Secondly, a problem-specific constructive heuristic is developed to produce high-quality initial solutions. Thirdly, twelve efficient heuristics are designed to construct low-level heuristics (LLHs). The Q -learning-based evolutionary algorithm is applied as a high-level strategy to manipulate the LLHs, which are then executed in order to search the solution space. Moreover, suitable solution encoding and decoding schemes are provided to produce feasible scheduling schedules. The design of experiments is implemented to investigate the impact of the parameters. Finally, a comprehensive comparison campaign is carried out based on a total of 1710 well-known instances to evaluate the efficacy of the proposed algorithm against several state-of-the-art algorithms. Experimental results and statistical analysis show that QLHHEA significantly outperforms the existing algorithms by a significant margin, demonstrating the effectiveness and efficiency of QLHHEA in solving DABFSP.},
  archive      = {J_ASOC},
  author       = {Zi-Qi Zhang and Bin Qian and Rong Hu and Jian-Bo Yang},
  doi          = {10.1016/j.asoc.2023.110695},
  journal      = {Applied Soft Computing},
  pages        = {110695},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Q-learning-based hyper-heuristic evolutionary algorithm for the distributed assembly blocking flowshop scheduling problem},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate 3D contrast-free myocardial infarction delineation
using a 4D dual-stream spatiotemporal feature learning framework.
<em>ASOC</em>, <em>146</em>, 110694. (<a
href="https://doi.org/10.1016/j.asoc.2023.110694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D contrast-free myocardial infarction (MI) delineation has the potential to eliminate the need for toxic injections, thereby significantly advances diagnosis and treatment of MI. In this study, we propose a 4D dual-stream spatiotemporal feature learning framework (4D-DSS) that enables learning of 4D (3D + T) representation of the heart to accurately map the 3D MI regions, thereby directly delineating of 3D MI without contrast agent. This framework creatively introduces a dual-stream 3D spatiotemporal point cloud architecture enables to learn the myocardial 4D representation in both local and global aspects, and improve the comprehension and precision of the representation. Specifically, the framework utilizes the local spatiotemporal variation of individual point clouds to characterize minute distortions in myocardial regions and the global spatiotemporal variation of point cloud sequences to represent the overall myocardial motion between frames, thereby enables comprehensive learning of 3D myocardial motion and leverages these features to classify myocardial tissue into MI regions and normal regions. 4D-DSS significantly improved performance (with a precision increase of at least 4\%) compared to four advanced methods. The results support the impact of our 4D-DSS framework on the development and implementation of 3D contrast-free myocardial infarction region delineation technology.},
  archive      = {J_ASOC},
  author       = {Jinhao Liu and Xinglai Zhu and Chenchu Xu and Lei Xu and Zhifan Gao and Kemal Polat and Fayadh Alenezi},
  doi          = {10.1016/j.asoc.2023.110694},
  journal      = {Applied Soft Computing},
  pages        = {110694},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Accurate 3D contrast-free myocardial infarction delineation using a 4D dual-stream spatiotemporal feature learning framework},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary multi-objective optimisation in neurotrajectory
prediction. <em>ASOC</em>, <em>146</em>, 110693. (<a
href="https://doi.org/10.1016/j.asoc.2023.110693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has rapidly evolved during the last decade, achieving expert human performance on notoriously challenging problems such as image classification . This success is partly due to the re-emergence of bio-inspired modern artificial neural networks (ANNs) along with the availability of computation power, vast labelled data and ingenious human-based expert knowledge as well as optimisation approaches that can find the correct configuration (and weights) for these networks. Neuroevolution is a term used for the latter when employing evolutionary algorithms . Most of the works in neuroevolution have focused their attention in a single type of ANNs, named Convolutional Neural Networks (CNNs). Moreover, most of these works have used a single optimisation approach. This work makes a progressive step forward in neuroevolution for vehicle trajectory prediction, referred to as neurotrajectory prediction, where multiple objectives must be considered. To this end, rich ANNs composed of CNNs and Long-short Term Memory Network are adopted. Two well-known and robust Evolutionary Multi-objective Optimisation (EMO) algorithms, named Non-dominated Sorting Genetic Algorithm-II (NSGA-II) and Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D) are also adopted. The completely different underlying mechanism of each of these algorithms sheds light on the implications of using one over the other EMO approach in neurotrajectory prediction. In particular, the importance of considering objective scaling is highlighted, finding that MOEA/D can be more adept at focusing on specific objectives whereas, NSGA-II tends to be more invariant to objective scaling. Additionally, certain objectives are shown to be either beneficial or detrimental to finding valid models, for instance, inclusion of a distance feedback objective was considerably detrimental to finding valid models, while a lateral velocity objective was more beneficial.},
  archive      = {J_ASOC},
  author       = {Edgar Galván and Fergal Stapleton},
  doi          = {10.1016/j.asoc.2023.110693},
  journal      = {Applied Soft Computing},
  pages        = {110693},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary multi-objective optimisation in neurotrajectory prediction},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Short-term power load forecasting system based on rough
set, information granule and multi-objective optimization.
<em>ASOC</em>, <em>146</em>, 110692. (<a
href="https://doi.org/10.1016/j.asoc.2023.110692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately forecasting power load is essential for utilities to effectively manage their resources, reduce operational costs, and provide improved customer service. However, the current load forecasting lacks the ability to deeply explore data, thus failing to accurately predict both short-term trends and volatility ranges. To address this issue, we construct a novel combined forecasting system based on rough sets, information granulation, deep learning , and multi-objective optimization. In this study, we follow the reasonable granulation criterion for granular computing , which aims to improve the reasonableness and specificity of granular interval prediction under the determination of granularity level, and innovatively propose a novel multi-objective optimization algorithm that can simultaneously constrain the reasonable granulation criterion and theoretically demonstrate the obtained Pareto-optimal solution. Four simulation experiments were conducted using the Australian dataset to evaluate the performance of our proposed system in predicting trend changes and fluctuation ranges of power load. Our results demonstrate that the developed system effectively predicts the trend changes and fluctuation range of power load. Specifically, our system showed a deterministic prediction performance improvement of 13.39\% and a granularity interval prediction performance improvement of 6.67\% compared to the baseline model . Moreover, we conducted a series of discussion tests to validate the superiority of our system, which further confirmed the effectiveness of our proposed approach.},
  archive      = {J_ASOC},
  author       = {Jianzhou Wang and Kang Wang and Zhiwu Li and Haiyan Lu and He Jiang},
  doi          = {10.1016/j.asoc.2023.110692},
  journal      = {Applied Soft Computing},
  pages        = {110692},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term power load forecasting system based on rough set, information granule and multi-objective optimization},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new logarithmic fuzzy full consistency method for
prioritizing critical success factors of technology start-ups in
thailand. <em>ASOC</em>, <em>146</em>, 110691. (<a
href="https://doi.org/10.1016/j.asoc.2023.110691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to societal changes and modernization in the world, various technologies and businesses have changed. Technology start-up enterprises are currently increasingly gaining attention in Thailand. However, for a technology start-up to be successful is not an easy path. Critical success factors (CSFs) from experts are necessary at the start. In this research two sectors: general technology (GeneralTech) and deep technology (DeepTech) are investigated. Entrepreneurs who wish to shift their businesses from GeneralTech to DeepTech can also understand what they have to focus on in running a relevant and successful business. Fuzzy Logarithmic Full Consistency Method (FUCOM-LF) has been applied to rank the CSFs for technology start-ups. This newly developed method has been compared with the existing methods; Logarithmic Fuzzy Preference Programming method (LFPP) and Fuzzy Logarithmic Full Consistency Method (FUCOM-F). The results have shown that the FUCOM-LF has advantages over existing methods because it uses a smaller number of pairwise comparisons with full consistency and it can get reliable results based on evaluated data. It is also not necessary to defuzzify the final value like FUCOM-F. The findings reveal that the three main CSFs for GeneralTech start-ups in Thailand are market and customer, key persons, and value propositions, while CSFs for DeepTech start-ups are market and customer, value propositions, and key persons, respectively. To shift from GeneralTech start-ups to DeepTech start-ups entrepreneurs should enhance their value propositions.},
  archive      = {J_ASOC},
  author       = {Busaba Phruksaphanrat and Saruntorn Panjavongroj},
  doi          = {10.1016/j.asoc.2023.110691},
  journal      = {Applied Soft Computing},
  pages        = {110691},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new logarithmic fuzzy full consistency method for prioritizing critical success factors of technology start-ups in thailand},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical implementation of computationally-efficient
machine learning-based control performance assessment system for a class
of closed loop systems. <em>ASOC</em>, <em>146</em>, 110690. (<a
href="https://doi.org/10.1016/j.asoc.2023.110690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the concept of the computationally-efficient reduced order control performance assessment (RO-CPA) system for automatic detection of industrial single closed loop control systems whose performance could be not acceptable due to a poor tuning of the controller. The proposed machine learning (ML) based classification system is derived for a broad class of industrial control closed loops based on the proportional–integral–derivative (PID) controller. Apart from a high accuracy of the classification of the closed loop performance, the proposed RO-CPA system is derived to meet the requirements of low computational complexity and low memory usage. These requirements allow for practical implementation of RO-CPA in devices with low computational and memory resources, such as industrial programmable logic controllers (PLC) operating in manufacturing systems or embedded autonomous systems with dedicated PID-based control loops. This paper shows the rigorous ML-based design of the proposed RO-CPA system and the results of the validation of its classification accuracy . Additionally, an example of a practical implementation within a PLC in the form of a general purpose library function block is presented and the final experimental validation is made using a part of laboratory heat exchange and distribution setup.},
  archive      = {J_ASOC},
  author       = {Patryk Grelewicz and Pawel Nowak and Thanh Tung Khuat and Jacek Czeczot and Tomasz Klopot and Bogdan Gabrys},
  doi          = {10.1016/j.asoc.2023.110690},
  journal      = {Applied Soft Computing},
  pages        = {110690},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Practical implementation of computationally-efficient machine learning-based control performance assessment system for a class of closed loop systems},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LAnoBERT: System log anomaly detection based on BERT masked
language model. <em>ASOC</em>, <em>146</em>, 110689. (<a
href="https://doi.org/10.1016/j.asoc.2023.110689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The system log generated in a computer system refers to large-scale data that are collected simultaneously and used as the basic data for determining errors, intrusion and abnormal behaviors. The aim of system log anomaly detection is to promptly identify anomalies while minimizing human intervention, which is a critical problem in the industry. Previous studies performed anomaly detection through algorithms after converting various forms of log data into a standardized template using a parser. Particularly, a template corresponding to a specific event should be defined in advance for all the log data using which the information within the log key may get lost. In this study, we propose LAnoBERT, a parser free system log anomaly detection method that uses the BERT model, exhibiting excellent natural language processing performance. The proposed method, LAnoBERT, learns the model through masked language modeling , which is a BERT-based pre-training method, and proceeds with unsupervised learning-based anomaly detection using the masked language modeling loss function per log key during the test process. In addition, we also propose an efficient inference process to establish a practically applicable pipeline to the actual system. Experiments on three well-known log datasets, i.e., HDFS, BGL, and Thunderbird, show that not only did LAnoBERT yield a higher anomaly detection performance compared to unsupervised learning-based benchmark models , but also it resulted in a comparable performance with supervised learning-based benchmark models .},
  archive      = {J_ASOC},
  author       = {Yukyung Lee and Jina Kim and Pilsung Kang},
  doi          = {10.1016/j.asoc.2023.110689},
  journal      = {Applied Soft Computing},
  pages        = {110689},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LAnoBERT: System log anomaly detection based on BERT masked language model},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biparty multiobjective optimal power flow: The problem
definition and an evolutionary approach. <em>ASOC</em>, <em>146</em>,
110688. (<a href="https://doi.org/10.1016/j.asoc.2023.110688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiobjective optimal power flow (MOOPF) problem consists of adjusting the generator power and the voltage state of each node within the feasible range in the process of power transmission and, finally of achieving the objectives of optimizing the cost, loss and stability, etc. In the MOOPF, two key decision makers are usually involved, which are the power generation sector and the transmission sector. Thus, it is more suitable to model an MOOPF as a biparty multiobjective optimal power flow (BPMOOPF) problem. However, so far, there is no work on treating and solving the MOOPF problem from the perspective of biparty multiobjective optimization . In this paper, we propose the definition of the BPMOOPF problem as well as a novel evolutionary biparty multiobjective optimization algorithm for solving the BPMOOPF problem, which we call BPMOOPF-EA. Our experimental results show that, compared two state-of-the-art algorithms (C-MOEA/D and A-NSGA-III), our proposed BPMOOPF-EA has a better performance when solving the BPMOOPF problem.},
  archive      = {J_ASOC},
  author       = {Yatong Chang and Wenjian Luo and Xin Lin and Zhen Song and Carlos A. Coello Coello},
  doi          = {10.1016/j.asoc.2023.110688},
  journal      = {Applied Soft Computing},
  pages        = {110688},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Biparty multiobjective optimal power flow: The problem definition and an evolutionary approach},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A sample subspace optimization-based framework for
addressing mislabeling in self-labeled semi-supervised classification.
<em>ASOC</em>, <em>146</em>, 110687. (<a
href="https://doi.org/10.1016/j.asoc.2023.110687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The self-labeled methods can enlarge the labeled set by continuously adding pseudo-labeled data from the unlabeled set and predicted by base classifiers . Mislabeling is a great challenge for self-labeled methods due to their generalization performance depending on the correct prediction of pseudo-labeled data. Existing solutions of self-labeled methods or frameworks employ data editing techniques based on traditional instance selection methods or differential evolution to overcome mislabeling. Nevertheless, they still suffer from the following issues: (a) data editing techniques based on traditional instance selection methods heavily rely on specific assumptions and some are not easily extended to other self-labeled methods; (b) the differential evolution may distort the original data distribution. To overcome mislabeling and the above issues in existing solutions, a novel framework based on sample subspace optimization for self-labeled semi-supervised classification (SSO-SLSSC) is proposed. SSO-SLSSC is a wrapping framework and can be highly compatible with most existing self-labeled methods. First, SSO-SLSSC can employ almost any self-labeled method to perform the iterative self-labeled process. Second, during the iterative self-labeled process, a binary particle swarm optimization-based sample subspace optimization (BPSOSSO) is innovatively proposed to select a subset containing correctly predicted pseudo-labeled data from newly predicted data and filter out the subset containing mislabeled data from newly predicted samples. Experimental results have proven that SSO-SLSSC outperforms 2 representative self-labeled frameworks and 5 advanced data editing techniques in overcoming mislabeling of 4 popular self-labeled methods on extensive benchmark data sets from UCI and Kaggle with different ratios of initial labeled samples and noise.},
  archive      = {J_ASOC},
  author       = {Junnan Li and Tingting Li},
  doi          = {10.1016/j.asoc.2023.110687},
  journal      = {Applied Soft Computing},
  pages        = {110687},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sample subspace optimization-based framework for addressing mislabeling in self-labeled semi-supervised classification},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “adaptive stick–slip friction and backlash
compensation using dynamic fuzzy logic system” [appl. Soft comput. 6
(2005) 26–37]. <em>ASOC</em>, <em>146</em>, 110685. (<a
href="https://doi.org/10.1016/j.asoc.2023.110685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Suraneni S. and Kar I.N. and Ramana Murthy O.V. and Bhatt R.K.P.},
  doi          = {10.1016/j.asoc.2023.110685},
  journal      = {Applied Soft Computing},
  pages        = {110685},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Adaptive stick–slip friction and backlash compensation using dynamic fuzzy logic system” [Appl. soft comput. 6 (2005) 26–37]},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sensitivity analysis of parameters in an agent-based model
for crowd simulations. <em>ASOC</em>, <em>146</em>, 110684. (<a
href="https://doi.org/10.1016/j.asoc.2023.110684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research paper, we present a sensitivity analysis of parameters utilized in an agent-based model for crowd simulations. The model is made up of two types of agents that explore a virtual environment to reach an exit from a specified starting point. They behave differently depending on their type, and they may be collaborative , acting carefully to help others reach the exit, or defectors , acting independently and wildly. To simulate the agents’ and environment dynamics we used the Ant Colony Optimization algorithm principles. Three metrics to evaluate the effects of these behaviors have been used: the number of exited agents, the path cost, and the exit time. Furthermore, two different types of analyses have been carried out and are presented: group analysis , in which the performance of the groups into which the agents are split are compared; and types’ analysis , where the performance of the two types of agents, i.e., collaborators and defectors, are investigated and compared.},
  archive      = {J_ASOC},
  author       = {Carolina Crespi and Rocco A. Scollo and Georgia Fargetta and Mario Pavone},
  doi          = {10.1016/j.asoc.2023.110684},
  journal      = {Applied Soft Computing},
  pages        = {110684},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sensitivity analysis of parameters in an agent-based model for crowd simulations},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BP-CapsNet: An image-based deep learning method for medical
diagnosis. <em>ASOC</em>, <em>146</em>, 110683. (<a
href="https://doi.org/10.1016/j.asoc.2023.110683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning is one of the promising branches of artificial intelligence and has made phased progress in medical diagnosis. Widely used Convolution Neural Network (CNN) methods require considerable data to train the models to be comparable to professionals. However, it is difficult to acquire medical data in real life to support the application of the CNN methods due to its industrial particularity. Besides, CNN is easy to lose the spatial relationships among the features, which is caused by pooling. The emergence of Capsule Network (CapsNet) effectively alleviates these problems. Nonetheless, CapsNet has an obvious drawback of overlearning: it is prone to learning the noise latent in the image. The named dynamic routing algorithm may lead to the degeneration of capsules on some datasets as the routing iteration increases, making inaccurate diagnoses. This paper proposes a Bayes–Pearson routing CapsNet (BP-CapsNet) with a Singular Value Decomposition (SVD) module to solve the above problems and process medical images to achieve an adequate diagnosis. The proposed method’s effectiveness was evaluated by training and testing the entire model on seven distinct medical image datasets. It achieves state-of-the-art performance on two datasets and the greatest accuracy on another two datasets. The simulation results show that the method can significantly reduce the impact of noise, achieving effective diagnosis and potent generality.},
  archive      = {J_ASOC},
  author       = {Yongjia Lei and Zujian Wu and Zhiying Li and Yuer Yang and Zhongming Liang},
  doi          = {10.1016/j.asoc.2023.110683},
  journal      = {Applied Soft Computing},
  pages        = {110683},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BP-CapsNet: An image-based deep learning method for medical diagnosis},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning algorithm for supervision process in
production using acoustic signal. <em>ASOC</em>, <em>146</em>, 110682.
(<a href="https://doi.org/10.1016/j.asoc.2023.110682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an industrial environment, accurate fault diagnosis of machines is crucial to prevent shutdowns, failures, maintenance costs, and production downtime. Existing methods for system failure prevention are often unsatisfactory and expensive, prompting the need for alternative approaches. Acoustic signals have emerged as a new method for predicting machine component lifespan, but recognizing relevant features and distinguishing them from noise remains challenging. To address the aforementioned challenges, we present a comprehensive model that integrates various components to enhance the accuracy and effectiveness of machine process identification. The proposed model incorporates a deep learning algorithm, which enables the forecasting of machine operation based on acoustic signals. In addition, we employ a customized Continuous Wavelet Transformation (CWT) technique to convert the acoustic signals into CWT images, preserving vital information such as signal amplitude . This transformation allows for a more comprehensive analysis and representation of the acoustic data. Furthermore, a Convolutional Neural Network (CNN) is utilized as a powerful classifier to accurately classify and differentiate between different machine processes based on the extracted features from the CWT images. By combining these elements, our model provides a robust and efficient framework for machine process identification using acoustic signals. Testing our model on a dataset generated from the Institute for Manufacturing Technology and Machine Tools (IFW) for the Gildemeister machine (CTX420 linear), we achieve over 97\% accuracy in discovering and early detecting emerging faults and machine processes based on acoustic signals.},
  archive      = {J_ASOC},
  author       = {Mahmood Safaei and Seyed Ahmad Soleymani and Mitra Safaei and Hassan Chizari and Mehrbakhsh Nilashi},
  doi          = {10.1016/j.asoc.2023.110682},
  journal      = {Applied Soft Computing},
  pages        = {110682},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning algorithm for supervision process in production using acoustic signal},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive-fidelity computation of the genetic algorithm
for energy-efficient virtual machine placement in cloud data centers.
<em>ASOC</em>, <em>146</em>, 110681. (<a
href="https://doi.org/10.1016/j.asoc.2023.110681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy efficiency is a critical issue in the management and operation of data centers , which form the backbone of cloud computing . Virtual machine placement has a significant impact on the energy efficiency of virtualized data centers. Among various methods to solve the virtual-machine placement problem, the genetic algorithm has been well accepted for its quality of solution. However, it is computationally demanding largely due to the complex form of fitness function, limiting its applications in data centers. To enhance the computational efficiency of the genetic algorithm while maintaining its quality of solution, a progressive-fidelity approach is developed in this paper for genetic-algorithm computation. It starts with a low-fidelity genetic algorithm with a simple fitness function. Then, for solution refinement, it switches to a medium-fidelity genetic algorithm with a more complicated fitness function. Finally, it progresses to the fine tuning of solution through a high-fidelity genetic algorithm with the energy consumption of data centers as fitness function. Heuristics are presented for the adaptive switching of genetic-algorithm computation from low fidelity to medium fidelity and finally to high fidelity. Experiments show that compared with the standard genetic algorithm of high fidelity, our progressive-fidelity approach of genetic algorithm computation is 50\% faster for large-scale data centers while maintaining similar quality of solution in terms of the energy consumption of data centers.},
  archive      = {J_ASOC},
  author       = {Zhe Ding and Yu-Chu Tian and You-Gan Wang and Weizhe Zhang and Zu-Guo Yu},
  doi          = {10.1016/j.asoc.2023.110681},
  journal      = {Applied Soft Computing},
  pages        = {110681},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Progressive-fidelity computation of the genetic algorithm for energy-efficient virtual machine placement in cloud data centers},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeuroCrossover: An intelligent genetic locus selection
scheme for genetic algorithm using reinforcement learning.
<em>ASOC</em>, <em>146</em>, 110680. (<a
href="https://doi.org/10.1016/j.asoc.2023.110680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have been studying genetic algorithms (GAs) extensively in recent decades and employing them to address extremely challenging combinatorial optimization problems (COPs). Although GAs achieve superior performance, they are less efficient because most GAs are designed manually without intelligent parameter configuration to support scalable problem-solving strategies and learnable evolutionary operators. To address this issue, machine learning (ML) techniques have been integrated with GAs for operator and parameter selection, however, few studies have focused on intelligent genetic locus selection for influential operators in GAs. To fill this gap, this paper proposes an intelligent genetic locus selection algorithm that serves as the foundation of parameter configuration for critical operators. With the established framework, the Cross Information Synergistic Attention (CISA) model and the n-step proximal policy optimization (PPO) have been utilized to intelligently select the appropriate genetic locus for the most influential phase, i.e., crossover, during the evolutionary process. The proposed NeuroCrossover algorithm is validated on extensive COPs, including the traveling salesman problem , capacitated vehicle routing problem, and bin packing problem . The results demonstrate the efficiency and effectiveness of our algorithm, which outperforms other methods in terms of solution quality, convergence speed, and generalization. For instance, with the CISA, the average percentage gaps of our algorithm are 3.64\% and 6.38\% for instances in TSPLIB and CVRPLIB, respectively, obtaining gains of about 0.47\% and 1.20\% compared to those of GA. The proposed algorithm provides a novel solution to lead GAs to an efficient search and improve their scalability and learnability.},
  archive      = {J_ASOC},
  author       = {Haoqiang Liu and Zefang Zong and Yong Li and Depeng Jin},
  doi          = {10.1016/j.asoc.2023.110680},
  journal      = {Applied Soft Computing},
  pages        = {110680},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NeuroCrossover: An intelligent genetic locus selection scheme for genetic algorithm using reinforcement learning},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Feature selection in threes: Neighborhood relevancy,
redundancy, and granularity interactivity. <em>ASOC</em>, <em>146</em>,
110679. (<a href="https://doi.org/10.1016/j.asoc.2023.110679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental granular computing strategy, neighborhood granulation has been acknowledged as an intuitive and effective approach to feature evaluation and selection. However, such an approach always has a bias towards a fixed neighborhood granularity , while ignoring the observations across different levels of granularity . To this end, a novel algorithm based on Neighborhood relevancY , redundancY, and granularity interactivitY (N3Y) is proposed. Technically, N3Y adheres well to the rudiment of three-way decision, evaluating and selecting features in threes: 1) feature-to-class relevancy; 2) feature-to-feature redundancy; 3) granularity-to-granularity interactivity. Specifically, firstly, the neighborhood symmetrical uncertainty induced by neighborhood measures is adopted to evaluate the relevancy and redundancy of candidate feature subset; secondly, the proposed neighborhood granularity interactivity allows an uncertainty quantification for finer-to-coarser granularity, and is leveraged as a supplemental factor to guide the relevancy and redundancy, making our procedure more comprehensive; thirdly, a forward-greedy selector is devised, which is required to maximize the evaluation criterion integrating neighborhood relevancy, redundancy, and granularity interactivity. Extensive experiments demonstrate that N3Y outperforms several other advanced feature selectors.},
  archive      = {J_ASOC},
  author       = {Keyu Liu and Tianrui Li and Xibei Yang and Hengrong Ju and Xin Yang and Dun Liu},
  doi          = {10.1016/j.asoc.2023.110679},
  journal      = {Applied Soft Computing},
  pages        = {110679},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection in threes: Neighborhood relevancy, redundancy, and granularity interactivity},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight multi-sensory field-based dual-feature fusion
residual network for bird song recognition. <em>ASOC</em>, <em>146</em>,
110678. (<a href="https://doi.org/10.1016/j.asoc.2023.110678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bird song recognition plays an important function in ecosystem balance monitoring, biodiversity detection, and biodiversity conservation. Due to the complexity of the natural environment, based on deep learning , there is a problem of information loss in extracting audio features with a single filter. Identifying bird sounds efficiently and quickly is still a challenge. To address this problem, a lightweight multi-sensory field dual-feature fusion residual network (LDFSRE-NET) is proposed in this paper. Firstly, using the feature extraction filters based on Mel and SincNet to extract birdsong’s low-frequency and timbre information. The proposed dual-feature fusion module (FFMS) is used to fuse the low-frequency and timbre information with the differences between the two feature sets. Secondly, the double-layer residual module (DBNet), connected by basicblock and downblock, is used as the backbone network for bird song recognition to improve the training speed. To improve the different perceptual fields of the backbone network , the 3 × 3 convolutional modules in the basicblocks of the two residual modules are replaced with a Diverse Branch Block. They make the network performs better on recognition tasks under complex situations of multiple branches. Then, the ShuffleAttention attention module is embedded between the two layers of the residual module for transferring its valid information, enhancing the spectrogram ripple feature, and further improving the network’s recognition performance. Finally, extensive experiments are conducted on three datasets: the self-built 30-class bird song dataset (Birdselfdata), the public datasets Birdsdata and Urbansound8K. The model proposed in this paper surpasses the state-of-the-art sound recognition model methods in terms of efficiency and accuracy. The recognition accuracy on these three datasets of this model are 96.75\%, 96.46\%, and 97.98\%, with the F1-score of 96.79\%, 96.39\%, and 97.88\%.},
  archive      = {J_ASOC},
  author       = {Shipeng Hu and Yihang Chu and Lu Tang and Guoxiong Zhou and Aibin Chen and Yurong Sun},
  doi          = {10.1016/j.asoc.2023.110678},
  journal      = {Applied Soft Computing},
  pages        = {110678},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight multi-sensory field-based dual-feature fusion residual network for bird song recognition},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PPFLHE: A privacy-preserving federated learning scheme with
homomorphic encryption for healthcare data. <em>ASOC</em>, <em>146</em>,
110677. (<a href="https://doi.org/10.1016/j.asoc.2023.110677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare data are characterized by explosive growth and value, which is the private data of patients, and its characteristics and storage environment have brought significant issues of data privacy and security. People are reluctant to share their data for privacy concerns during machine learning . To balance this contradiction, Federated Learning was proposed as a solution to train on private data without sharing it. However, many studies show that there is still the possibility of privacy leakage during the training process of federated learning. In light of this, we propose a privacy-preserving federated learning scheme with homomorphic encryption(PPFLHE). Specifically, on the client side, homomorphic encryption technology is used to encrypt the training model shared by users to ensure its security and privacy. In addition, to prevent internal attacks, Access Control (AC) technology is used to confirm the user’s identity and judge whether it is trusted; on the server side, the Acknowledgment (ACK) mechanism is designed to remove the dropped or unresponsive users temporarily, which reduces the waiting delay and communication overhead , and solves the problem of user’s exiting during training. Theoretical analysis and experimental results show that the proposed scheme achieves high data utility and classification accuracy (81.53\%), and low communication delay while achieving privacy preserving, compared to state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Bo Wang and Hongtao Li and Yina Guo and Jie Wang},
  doi          = {10.1016/j.asoc.2023.110677},
  journal      = {Applied Soft Computing},
  pages        = {110677},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PPFLHE: A privacy-preserving federated learning scheme with homomorphic encryption for healthcare data},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDIF-CNN: Stacking deep image features using fine-tuned
convolution neural network models for real-world malware detection and
classification. <em>ASOC</em>, <em>146</em>, 110676. (<a
href="https://doi.org/10.1016/j.asoc.2023.110676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of malware is a complex problem in the area of Internet security. Developing a malware defense system that is less costly to detect large-scale malware is needed. This paper proposes a novel malware detection and classification architecture based on image visualization as SDIF-CNN: Stacking deep image features using fine-tuned convolution neural networks. The hybrid methodology of transfer learning as fine-tuning and feature extractor of deep convolution neural network models is designed. At first, the pre-trained VGG16 CNN model is deeply fine-tuned with different hyperparameters, including the number of layers, learning rate, momentum, etc. The transfer learning-based fine-tuned VGG16 model is used as a feature extractor along with the three similar pre-trained CNN models, VGG19 , ResNet50, and InceptionV3, to obtain the diverse feature map. The extracted features are horizontally concatenated to construct a single feature map. The different feature selection methodologies, including filter-based methods and embedded methods, such as linear regression and random forest , are designed to discard the irrelevant features from a stacked feature map. After that, this study uses six machine learning and deep learning classifiers- K-Nearest Neighbor (K-NN), Support Vector Machine (SVM), Random Forest (RF), Multi-Layer Perceptron (MLP), Extra Tree (ET), and Gaussian Naive Bayes (GNB) by using the stacked feature map as a training feature vector . The hyperparameter optimization of the MLP model as the best classifier is performed using a randomized search algorithm to devise an optimal classifier. The experiments are performed using a publicly benchmarked MalImg dataset of 9339 images from 25 families. The model is also validated on real-world and packed malicious programs to prove the generalization of the proposed methodology in detecting real-world malware. In the proposed system, the MLP model obtained the best performance results as 98.55\% accuracy, 99\% precision, 99\% recall, and 99\% F1-score for MalImg datasets, and accuracy of 94.78\% for real-world malware datasets. The proposed methodology is resilient to commonly used obfuscation techniques and does not depend upon code disassembly, reverse engineering analysis, and highly resource-intensive dynamic analysis.},
  archive      = {J_ASOC},
  author       = {Sanjeev Kumar and Kajal Panda},
  doi          = {10.1016/j.asoc.2023.110676},
  journal      = {Applied Soft Computing},
  pages        = {110676},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SDIF-CNN: Stacking deep image features using fine-tuned convolution neural network models for real-world malware detection and classification},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-source adversarial transfer learning for ultrasound
image segmentation with limited similarity. <em>ASOC</em>, <em>146</em>,
110675. (<a href="https://doi.org/10.1016/j.asoc.2023.110675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lesion segmentation of ultrasound medical images based on deep learning techniques is a widely used method for diagnosing diseases. Although there is a large amount of ultrasound image data in medical centers and other places, labeled ultrasound datasets are a scarce resource, and it is likely that no datasets are available for new tissues/organs. Transfer learning provides the possibility to solve this problem, but there are too many features in natural images that are not related to the target domain. As a source domain, redundant features that are not conducive to the task will be extracted. Migration between ultrasound images can avoid this problem, but there are few types of public datasets, and it is difficult to find sufficiently similar source domains. Compared with natural images, ultrasound images have less information, and there are fewer transferable features between different ultrasound images, which may cause negative transfer. To this end, a multi-source adversarial transfer learning network for ultrasound image segmentation is proposed. Specifically, to address the lack of annotations, the idea of adversarial transfer learning is used to adaptively extract common features between a certain pair of source and target domains, which provides the possibility to utilize unlabeled ultrasound data . To alleviate the lack of knowledge in a single source domain, multi-source transfer learning is adopted to fuse knowledge from multiple source domains. In order to ensure the effectiveness of the fusion and maximize the use of precious data, a multi-source domain independent strategy is also proposed to improve the estimation of the target domain data distribution, which further increases the learning ability of the multi-source adversarial migration learning network in multiple domains. The effectiveness of multi-source adversarial transfer learning is demonstrated through experiments on three datasets of ultrasound image datasets.},
  archive      = {J_ASOC},
  author       = {Yifu Zhang and Hongru Li and Tao Yang and Rui Tao and Zhengyuan Liu and Shimeng Shi and Jiansong Zhang and Ning Ma and Wujin Feng and Zhanhu Zhang and Xinyu Zhang},
  doi          = {10.1016/j.asoc.2023.110675},
  journal      = {Applied Soft Computing},
  pages        = {110675},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-source adversarial transfer learning for ultrasound image segmentation with limited similarity},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge acquisition of multi-granularity ordered
information systems. <em>ASOC</em>, <em>146</em>, 110674. (<a
href="https://doi.org/10.1016/j.asoc.2023.110674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge reduction is an essential issue in data mining. However, most of the existing studies consider the problem of knowledge reduction in a single-scale system. When the system expands to a multi-scale information system (MIS), the existing methods are not available. To overcome this challenge, we design a universal framework to reduce attributes and select optimal scales (OSs) based on the dominance relation in ordered information systems (OISs). Firstly, we analyze the effect of scale on rough approximations and correlation measures in multi-scale ordered information systems (MOISs). Secondly, the relation between attribute reduction and OS selection is explored in OISs. Moreover, the corresponding algorithms are designed around both tasks. Due to our universal framework considering the decisions at the finest, optimal and coarsest scale, our study is an extension of the three-way decision. Thirdly, a novel MOIS construction algorithm is explored to transform an OIS into an MIS. Thus, the proposed reduction algorithms can be operated in any OIS. Experiments substantiate that models trained by data at the OS can improve their stability and the attribute reduction task is also suitable for the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Zhiyong Hu and Mingwen Shao and Weizhi Wu and Leijun Li},
  doi          = {10.1016/j.asoc.2023.110674},
  journal      = {Applied Soft Computing},
  pages        = {110674},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge acquisition of multi-granularity ordered information systems},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Duel-based neuroevolutionary method for stackelberg security
games with boundedly rational attacker. <em>ASOC</em>, <em>146</em>,
110673. (<a href="https://doi.org/10.1016/j.asoc.2023.110673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of finding optimal strategies in Stackelberg Security Games when playing against a non-perfectly rational Attacker. To this end, a novel Duel-based NeuroEvolutionary approach to Security Games (DNESG) is proposed, which utilizes the Strategy Comparison Neural Network (SCNN) as a surrogate model to compare pairs of Defender’s strategies. SCNN is trained on historical data (past attack attempts) and does not require any direct information about the Attacker’s preferences regarding targets, payoff distribution, or decision-making model. SCNN is embedded in the Evolutionary Algorithm framework and implements a tournament-based selection method in place of a time-consuming direct strategy evaluation. The effectiveness of DNESG is assessed on a set of 90 benchmark Deep Packet Inspection games inspired by real cybersecurity scenarios. The proposed method provides high-quality solutions and outperforms state-of-the-art approaches (both exact and approximate) with statistical significance when playing against non-perfectly rational Attacker. Moreover, DNESG offers excellent time scalability, being two orders of magnitude faster than the state-of-the-art Mixed-Integer Linear Programming method.},
  archive      = {J_ASOC},
  author       = {Jacek Mańdziuk and Adam Żychowski},
  doi          = {10.1016/j.asoc.2023.110673},
  journal      = {Applied Soft Computing},
  pages        = {110673},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Duel-based neuroevolutionary method for stackelberg security games with boundedly rational attacker},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for inverse surrogate modeling for fitness
estimation applied to multi-objective evolutionary algorithms.
<em>ASOC</em>, <em>146</em>, 110672. (<a
href="https://doi.org/10.1016/j.asoc.2023.110672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many-Objective Optimization Problems , or MaOPs, are complex optimization problems with more than three objective functions. Traditional Multi-Objective Evolutionary Algorithms (MOEAs) have shown poor scalability in solving this problem. Neuroevolution techniques can be used to overdraw that problem. The use of machine learning techniques to enhance optimization algorithms applied to MaOPs has drawn attention due to their capacity to add domain knowledge during the search process. One method of this kind is inverse surrogate modeling for fitness estimation, which uses machine learning models to enhance MOEAs, mapping the objective function values to the decision variables. Some inverse modeling methods in literature have performed well in various optimization problems. Despite the promising results, new strategies using the generated knowledge during the search can be further improved. The main goal of this work is to propose a framework that uses an inverse modeling approach coupled with an MOEA. The inverse surrogate modeling is used to sample solutions that are combined with the evolutionary search of the MOEA. The framework is evaluated in different scenarios, including multi-objective benchmark problems and real-world problems. The results show that the proposed framework had statistically better or equivalent results in 86\% scenarios in the benchmark problems and 100\% scenarios in the real-world problem.},
  archive      = {J_ASOC},
  author       = {Artur Leandro da Costa Oliveira and André Britto and Renê Gusmão},
  doi          = {10.1016/j.asoc.2023.110672},
  journal      = {Applied Soft Computing},
  pages        = {110672},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A framework for inverse surrogate modeling for fitness estimation applied to multi-objective evolutionary algorithms},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient self-learning evolutionary neural architecture
search. <em>ASOC</em>, <em>146</em>, 110671. (<a
href="https://doi.org/10.1016/j.asoc.2023.110671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolutionary algorithm has become a major method for neural architecture search recently. However, the fixed probability distribution employed by the traditional evolutionary algorithm may lead to structural complexity and redundancy due to its inability to control the size of individual architectures, and it cannot learn from empirical information gathered during the search process to guide the subsequent search more effectively and efficiently. Moreover, evaluating the performance of all the searched architectures requires significant computing resources and time overhead. To overcome these challenges, we present the Efficient Self-learning Evolutionary Neural Architecture Search (ESE-NAS) method. Firstly, we propose an Adaptive Learning Strategy for Mutation Sampling, composed of a Model Size Control module and a Credit Assignment method for Mutation Candidates, to guide the search process by learning from the model size information and evaluation results of the architectures and adjusting the probability distributions for evolution sampling accordingly. Additionally, we developed a neural architecture performance predictor to further improve the efficiency of NAS. Experiments on CIFAR-10 and CIFAR-100 datasets show that ESE-NAS significantly brings forward the first hitting time of the optimal architectures and reaches a competitive performance level with classic manual-designed and NAS models while maintaining structural simplicity and efficiency.},
  archive      = {J_ASOC},
  author       = {Zhengzhong Qiu and Wei Bi and Dong Xu and Hua Guo and Hongwei Ge and Yanchun Liang and Heow Pueh Lee and Chunguo Wu},
  doi          = {10.1016/j.asoc.2023.110671},
  journal      = {Applied Soft Computing},
  pages        = {110671},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient self-learning evolutionary neural architecture search},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of strip section shape for hot-rolled based on
mechanism fusion data model. <em>ASOC</em>, <em>146</em>, 110670. (<a
href="https://doi.org/10.1016/j.asoc.2023.110670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise and vigorous development of artificial intelligence (AI) and data mining methodologies, machine learning (ML) has been successfully and widely used in industrial fields. For hot-rolled strips, both the rolling mechanism analyzing and multi-indicators integrated optimization are difficulties in hot rolling process. To overcome the limitations of multi-variable, nonlinear, and strong coupling in the process of hot rolling, and to improve the prediction accuracy of multi-output regression model, a novel model of rolling mechanism guided ML was developed. Mechanism data were calculated as added input features to participate in training process by way of dimensionality processing (DP). Simultaneously, genetic algorithm (GA) was adopted to realize multi-objective optimization of multi-output support vector regression (M-SVR) to further enhance the performance of model ultimately. Especially, the experimental results of root mean square error (RMSE) and correlation coefficient (R) of strip crown and thickness are 2.5218, 0.9891 and 2.1302, 0.9902, respectively, which fully demonstrates that this method does not limit the prediction accuracy but greatly improved the generalization ability .},
  archive      = {J_ASOC},
  author       = {Yafeng Ji and Lebao Song and Hao Yuan and Huaying Li and Wen Peng and Jie Sun},
  doi          = {10.1016/j.asoc.2023.110670},
  journal      = {Applied Soft Computing},
  pages        = {110670},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction of strip section shape for hot-rolled based on mechanism fusion data model},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent fault diagnosis scheme for rolling bearing based
on domain adaptation in one dimensional feature matching. <em>ASOC</em>,
<em>146</em>, 110669. (<a
href="https://doi.org/10.1016/j.asoc.2023.110669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of intelligent manufacturing technology has made data-driven fault diagnosis technology an important research at present. Most existing intelligent approaches, on the other hand, have a flaw: training and testing data are collected under the same operational conditions. As a result, a feature knowledge transfer learning method for rotating machinery fault detection is presented in this study to encourage the effective implementation of intelligent fault diagnosis. This method combines the domain adaptive ability of transfer learning and the ability of deep learning to automatically extract features Firstly, unsupervised convolutional auto-encoder is used for learning information from multiple source domains to construct feature subspace. Then adaptive learning rate , weight initialization and weight change algorithm are used to construct feature matching algorithm and incorporated into the stack-encoder. Finally, the encoder is used for diagnostic recognition of feature subspace information. Three datasets from various experimental platforms are used to validate the proposed method’s efficacy. The experimental findings demonstrate the method’s capacity to diagnose and identify multi-domain faults in bearings while having strong generalization and flexibility. Comparatively speaking, the proposed model may attain higher diagnostic accuracy with a rate of accuracy of above 98\%. In addition, two additional analytical experiments were conducted. The findings demonstrate that the method may provide over-task transfer learning and reliable diagnostic results. And, it demonstrates how improved diagnostic models may be built with a good dataset of unlabeled source domains.},
  archive      = {J_ASOC},
  author       = {Dengyun Sun and Zong Meng and Yang Guan and Jingbo Liu and Wei Cao and Fengjie Fan},
  doi          = {10.1016/j.asoc.2023.110669},
  journal      = {Applied Soft Computing},
  pages        = {110669},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent fault diagnosis scheme for rolling bearing based on domain adaptation in one dimensional feature matching},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural TV program recommendation based on dynamic long-short
term interest. <em>ASOC</em>, <em>146</em>, 110668. (<a
href="https://doi.org/10.1016/j.asoc.2023.110668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {TV program recommendation can help user find interested programs and improve user experience . The heterogeneous information of programs is important for alleviating the problem of data sparsity . In addition, the existing TV program recommendation methods are lacking in dynamics. This paper proposes a neural TV program recommendation based on dynamic long-short term interest (NPR-DLSTI), which mainly includes two modules: program and user encoder. In the program encoder module, we use convolutional neural network and attention mechanism to learn the heterogeneous information of the program and realize program representation. In the user encoder module, we use gated recurrent unit and personalized attention to learn the dynamic change law of users’ interests. Experiments on real data sets show that our method can effectively improve the effectiveness and dynamics of TV program recommendation than other existing models.},
  archive      = {J_ASOC},
  author       = {Fulian Yin and Xiaoli Feng and Ruiling Fu and Tongtong Xing and Sitong Li},
  doi          = {10.1016/j.asoc.2023.110668},
  journal      = {Applied Soft Computing},
  pages        = {110668},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural TV program recommendation based on dynamic long-short term interest},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-subgroup decision-making method for design selection
based on subjective reports and objective physiological index data.
<em>ASOC</em>, <em>146</em>, 110667. (<a
href="https://doi.org/10.1016/j.asoc.2023.110667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting an optimal product design scheme requires considering multiple criteria and engaging the wisdom of crowds. However, existing studies regarding design selection rarely considered the participation of group wisdom, and few studies considered subjective reports and physiological index data comprehensively for evaluation. This study developed a multi-subgroup decision-making method based on the participation of designers, experts, and customers. Multi-modal aggregation strategies are applied to aggregate subjective evaluations of both individuals and groups represented in probabilistic linguistic term sets (PLTSs), objective eye movement, and Electroencephalogram feature data . Specifically, a probabilistic linguistic information aggregation operator based on the Choquet integral is proposed to aggregate the values of subjective criteria with correlations, and the double normalization-based multi-aggregation method is then applied to aggregate objective criteria values, which can avoid information loss in the normalization and aggregation. Finally, the Choquet integral-based Borda operator is developed to integrate the evaluation results of multiple subgroups. An example of evaluating intelligent shared vehicle design schemes is presented to validate the proposed method.},
  archive      = {J_ASOC},
  author       = {Han Lai and Jiahuan Tian and Zheng Wu and Huchang Liao and Xingli Wu},
  doi          = {10.1016/j.asoc.2023.110667},
  journal      = {Applied Soft Computing},
  pages        = {110667},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-subgroup decision-making method for design selection based on subjective reports and objective physiological index data},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AUQantO: Actionable uncertainty quantification optimization
in deep learning architectures for medical image classification.
<em>ASOC</em>, <em>146</em>, 110666. (<a
href="https://doi.org/10.1016/j.asoc.2023.110666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms have the potential to automate the examination of medical images obtained in clinical practice. Using digitized medical images, convolution neural networks (CNNs) have demonstrated their ability and promise to discriminate among different image classes. As an initial step towards explainability in clinical diagnosis, deep learning models must be exceedingly precise, offering a measure of uncertainty for their predictions. Such uncertainty-aware models can help medical professionals in detecting complicated and corrupted samples for re-annotation or exclusion. This paper proposes a new model and data-agnostic mechanism, called Actionable Uncertainty Quantification Optimization ( AUQantO ) to improve the performance of deep learning architectures for medical image classification . This is achieved by optimizing the hyperparameters of the proposed entropy-based and Monte Carlo (MC) dropout uncertainty quantification techniques escorted by single- and multi-objective optimization methods, abstaining from the classification of images with a high level of uncertainty. This helps in improving the overall accuracy and reliability of deep learning models. To support the above claim, AUQantO has been validated with four deep learning architectures on four medical image datasets and using various performance metric measures such as precision, recall, Area Under the Receiver Operating Characteristic (ROC) Curve score (AUC), and accuracy. The study demonstrated notable enhancements in deep learning performance, with average accuracy improvements of 1.76\% and 2.02\% for breast cancer histology and 5.67\% and 4.24\% for skin cancer datasets, utilizing two uncertainty quantification techniques, and AUQantO further improved accuracy by 1.41\% and 1.31\% for brain tumor and 4.73\% and 1.83\% for chest cancer datasets while allowing exclusion of images based on confidence levels.},
  archive      = {J_ASOC},
  author       = {Zakaria Senousy and Mohamed Medhat Gaber and Mohammed M. Abdelsamea},
  doi          = {10.1016/j.asoc.2023.110666},
  journal      = {Applied Soft Computing},
  pages        = {110666},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AUQantO: Actionable uncertainty quantification optimization in deep learning architectures for medical image classification},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive differential evolution algorithm with DBSCAN for
the integrated slab allocation problem in steel industry. <em>ASOC</em>,
<em>146</em>, 110665. (<a
href="https://doi.org/10.1016/j.asoc.2023.110665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, an integrated slab allocation problem (ISAP) is investigated, aiming to optimise inventory management in the steel production process of hot rolling. Different from previous studies, three sets of decisions are made simultaneously. The first is to allocate open-order slabs to orders to reduce inventory slabs and improve slab utilisation. The second is to reallocate customer-order slabs to orders to shorten the order completion time and improve customer satisfaction. The third is to group the remaining open-order slabs into virtual orders to ensure continuous production and reduce inventory pressure. The problem is formulated as an original integer programming mathematical model. A novel improved differential evolution algorithm with self-adaptive mutation and parameter selection strategies is proposed to solve the ISAP. In particular, a special encoding and decoding method is designed to address the issue of transformation between the slab allocation scheme and algorithm individual. A density-based spatial clustering of applications with noise algorithm and random swap local search are embedded into the framework of the differential evolution algorithm . Finally, experiments are conducted to evaluate the proposed algorithm, and the numerical results demonstrate its superiority in solving the ISAP.},
  archive      = {J_ASOC},
  author       = {Lulu Song and Yun Dong and Qingxin Guo and Ying Meng and Guodong Zhao},
  doi          = {10.1016/j.asoc.2023.110665},
  journal      = {Applied Soft Computing},
  pages        = {110665},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive differential evolution algorithm with DBSCAN for the integrated slab allocation problem in steel industry},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OCRUN: An oppositional runge kutta optimizer with cuckoo
search for global optimization and feature selection. <em>ASOC</em>,
<em>146</em>, 110664. (<a
href="https://doi.org/10.1016/j.asoc.2023.110664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed swarm intelligence algorithm, Runge–Kutta Optimization (RUN), is rooted in the fourth-order Runge–Kutta method. Compared with its counterparts, RUN boasts an advantage of having a more concrete theoretical foundation embodying a more powerful optimization efficacy, free from any metaphor. However, RUN still has its shortcomings. The compelling enhanced solution function leads to insufficient exploration ability of the algorithm, and resulting in an imbalance between exploration and exploitation that cannot be mitigated. An improved version based on opposition-based learning and cuckoo search is proposed to compensate for the above deficiencies, called OCRUN. OCRUN is tested on 30 test functions of CEC2014 with 10 classical metaheuristics and 9 advanced metaheuristics , respectively. Combining the experimental results and the Wilcoxon signed-rank test, OCRUN exhibits excellent performance. At the same time, parameter sensitivity analysis experiments are also carried out on this test set. Furthermore, a binary implementation of the algorithm was constructed specifically for feature selection cases, labeled as BOCRUN. BOCRUN is compared with 5 existing binary metaheuristics on 15 public datasets. The experimental results show that the improved algorithm performs well in feature selection. Therefore, OCRUN is an effectively improved optimizer. Finally, the OCRUN method offers high-quality solutions to engineering problems and contributes significantly to engineering design under practical constraints. The method has been successfully applied to various design scenarios, such as reducer design, cantilever beam design, and tension/compression spring design. The OCRUN method outperforms other similar products in terms of performance and effectiveness.},
  archive      = {J_ASOC},
  author       = {Meilin Zhang and Huiling Chen and Ali Asghar Heidari and Zhennao Cai and Nojood O. Aljehane and Romany F. Mansour},
  doi          = {10.1016/j.asoc.2023.110664},
  journal      = {Applied Soft Computing},
  pages        = {110664},
  shortjournal = {Appl. Soft. Comput.},
  title        = {OCRUN: An oppositional runge kutta optimizer with cuckoo search for global optimization and feature selection},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crack detection of continuous casting slab by evolutionary
topology backbone search. <em>ASOC</em>, <em>146</em>, 110663. (<a
href="https://doi.org/10.1016/j.asoc.2023.110663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, computer vision-based methods have been widely used in steel defect detection . Traditional image detection methods mainly rely on manually extracted features, resulting in poor generalization. Deep learning methods are sensitive to the number of samples, and the network structure design relies heavily on manual experience. To address these problems, a backbone network search algorithm based on evolutionary topology is proposed in this paper for crack detection on continuous casting surfaces. Firstly, a variable-length genetic encoding scheme is designed for industrial defect problems with different data complexity, which can improve the applicability of the algorithm and extend the search space. Secondly, to effectively solve the channel redundancy problem in densely connected CNNs , a random pruning strategy for network connection channels is proposed to reduce the topological space and the complexity of the model. Finally, a computational resource allocation mechanism based on a dynamic surrogate model is devised. The surrogate model predicts the individual performance to ensure that computational resources can be concentrated on individuals with better quality. In addition to the steel crack image dataset, the proposed method also uses the workpiece crack image dataset for a supplementary experiment. Experimental results show that the proposed algorithm can achieve better detection performance with fewer computational resources compared to manually designed deep learning algorithms and classical approaches that use evolutionary algorithms to search network architectures .},
  archive      = {J_ASOC},
  author       = {Tianchen Zhao and Xianpeng Wang and Xiangman Song and Chang Liu},
  doi          = {10.1016/j.asoc.2023.110663},
  journal      = {Applied Soft Computing},
  pages        = {110663},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Crack detection of continuous casting slab by evolutionary topology backbone search},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A competitive learning scheme for deep neural network
pattern classifier training. <em>ASOC</em>, <em>146</em>, 110662. (<a
href="https://doi.org/10.1016/j.asoc.2023.110662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce the computational complexity of training a deep neural network architecture using large data sets of 3D scenes , a competitive learning scheme was devised. The proposed algorithm pits a neural network learning algorithm, in this case the standard Adam optimiser, against an evolutionary algorithm that is used to select the most difficult training examples. The overall scheme is similar to a predator–prey system, where the predator (the neural network) strives to optimise its ability to capture (identify) the prey (the training patterns), and the evolutionary procedure selects the prey that so far evaded capture. As a consequence of the evolutionary process, the neural network is presented only a fraction of the training examples, and the computational complexity of the learning procedure is reduced. Experimental evidence showed that the proposed scheme allows reducing the deep neural network training time on different model sets, sometimes significantly, without affecting the recognition accuracy. The proposed predator–prey scheme is fairly independent of the ANN type and training algorithm employed, and has the potential to be beneficial to a wide range of deep learning applications, where practical implementations are often hindered by the time complexity of the training process.},
  archive      = {J_ASOC},
  author       = {Senjing Zheng and Feiying Lan and Marco Castellani},
  doi          = {10.1016/j.asoc.2023.110662},
  journal      = {Applied Soft Computing},
  pages        = {110662},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A competitive learning scheme for deep neural network pattern classifier training},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic evolutionary multiobjective optimization for
open-order coil allocation in the steel industry. <em>ASOC</em>,
<em>146</em>, 110661. (<a
href="https://doi.org/10.1016/j.asoc.2023.110661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In terms of improving coil resource utilization and customer satisfaction, most steel companies are still far behind in these quality objectives and need significant improvements. Scientific approaches to production and operations planning are critical to improve its situation. Motivated by this phenomenon, this study investigates a challenging operations decision problem on allocating open-order coils to customer orders in the steel industry. In this article, a dynamic multiobjective optimization model is formulated to optimize the total mismatching costs, surplus inventory and coil utilization considering various practical dynamisms, such as production changes and unscheduled arrivals of new customer orders. To address this problem, we propose a multiobjective evolutionary algorithm based on knee-driven change response strategy and Pareto local search mechanism, so called KPLSEA . This proposed approach, which combines the information of decision-making stages with the evolutionary search, significantly reduces the computational cost and promotes the evolutionary search to converge quickly. Extensive experiments are performed on real-world production benchmark instances. Computational comparisons with other state-of-art algorithms validate that the proposed algorithm could generate effective and practical solutions in dynamic environments as well as provide the decision makers with a satisfied near-optimal solution.},
  archive      = {J_ASOC},
  author       = {Fei Zou and Qingxin Guo and Gary G. Yen},
  doi          = {10.1016/j.asoc.2023.110661},
  journal      = {Applied Soft Computing},
  pages        = {110661},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic evolutionary multiobjective optimization for open-order coil allocation in the steel industry},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time path planning of controllable UAV by subgoals
using goal-conditioned reinforcement learning. <em>ASOC</em>,
<em>146</em>, 110660. (<a
href="https://doi.org/10.1016/j.asoc.2023.110660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional path planning problem for an unmanned aerial vehicle (UAV) typically involves a pre-defined environment and mission, with the objective of reaching a single target point. However, in order to perform different missions, the agent must be trained from scratch. In this paper, we propose a new path planning algorithm for UAVs by training them to be controlled by subgoals, which enhances their degree of freedom to perform various maneuvers. The subgoals can be defined by the user and given to the agent in real-time, allowing the UAV to perform diverse flight missions without prior knowledge of the environment. To achieve this, we utilize goal-conditioned reinforcement learning to train the UAV agent to reach various goals by learning different flight maneuvers. In experiments, we designed specific scenarios to test the UAV agent’s ability to perform concrete missions, such as high-flying, low-flying, penetrating, and bypassing. The experimental results show that the same UAV agent trained in a simple environment can accomplish difficult missions in various scenarios. The pre-trained UAV agent can be utilized in other environments as it can be controlled by the subgoals.},
  archive      = {J_ASOC},
  author       = {GyeongTaek Lee and KangJin Kim and Jaeyeon Jang},
  doi          = {10.1016/j.asoc.2023.110660},
  journal      = {Applied Soft Computing},
  pages        = {110660},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time path planning of controllable UAV by subgoals using goal-conditioned reinforcement learning},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Software defects prediction by metaheuristics tuned extreme
gradient boosting and analysis based on shapley additive explanations.
<em>ASOC</em>, <em>146</em>, 110659. (<a
href="https://doi.org/10.1016/j.asoc.2023.110659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software testing represents a crucial component of software development, and it is usually making the difference between successful and failed projects. Although it is extremely important, due to the fast pace and short deadlines of contemporary projects it is often neglected or not detailed enough due to the lack of available time, leading to the potential loss of reputation, private users’ data, money, and even lives in some circumstances. In such situations, it would be vital to have the option to predict what modules are error-prone according to the collection of software metrics, and to focus testing on them, and that task is a typical classification task . Machine learning models have been frequently employed within a wide range of classification problems with significant success, and this paper proposes eXtreme gradient boosting (XGBoost) model to execute the defect prediction task. A modified variant of the well-known reptile search optimization algorithm has been suggested to carry out the calibrating of the XGBoost hyperparameters. The enhanced algorithm was named HARSA and evaluated on the collection of challenging CEC2019 benchmark functions , where it exhibited excellent performance. Later, the introduced XGBoost model that uses the proposed algorithm has been evaluated on two benchmark software testing datasets , and the simulation outcomes have been compared to other powerful swarm intelligence metaheuristics that were used in the identical experimental environment, where the proposed approach attained superior classification accuracy on both datasets. Finally, Shapley Additive Explanations analysis was conducted to discover the impact of various software metrics on the classification results .},
  archive      = {J_ASOC},
  author       = {Tamara Zivkovic and Bosko Nikolic and Vladimir Simic and Dragan Pamucar and Nebojsa Bacanin},
  doi          = {10.1016/j.asoc.2023.110659},
  journal      = {Applied Soft Computing},
  pages        = {110659},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Software defects prediction by metaheuristics tuned extreme gradient boosting and analysis based on shapley additive explanations},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reinforcement learning-artificial bee colony algorithm for
flexible job-shop scheduling problem with lot streaming. <em>ASOC</em>,
<em>146</em>, 110658. (<a
href="https://doi.org/10.1016/j.asoc.2023.110658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a typical production model in manufacturing industry, Flexible Job-shop Scheduling Problem (FJSP) has an important impact on enhancing the productivity of enterprises. Flexible Job-shop Scheduling Problem with Lot Streaming (FJSP-LS) is an extension of FJSP that allows jobs to be split into multiple sublots so they can be processed and transported separately. Since FJSP-LS has a large solution space and it is difficult and unstable for many algorithms to find a high-quality solution, this paper proposes a hybrid algorithm combining Reinforcement Learning and Artificial Bee Colony (RL-ABC) algorithm. In RL-ABC, the utilities for solving FJSP-LS are divided into 2 stages: (1) determining the best dispatch scheme and (2) determining the best scheme of sublots. For stage 1, an algorithm with different initialization and local search strategies is proposed. For stage 2, reinforcement learning is developed by building mappings between the environment and schemes of sublots. The effectiveness and robustness of RL-ABC algorithm and its components are compared with five algorithms including three types (traditional heuristic algorithm , improved heuristic algorithm and new evolutionary algorithm) on nineteen benchmark instances and three real instances. The results show that although RL-ABC algorithm exhibits inferior performance in terms of CPU time, its effectiveness and robustness surpass all the other compared algorithms on all instances. Moreover, both components of the RL-ABC algorithm effectively reduce the Makespan. Therefore, it can be used as a new technique to solve large-scale and complex problems in scheduling domain.},
  archive      = {J_ASOC},
  author       = {Yibing Li and Cheng Liao and Lei Wang and Yu Xiao and Yan Cao and Shunsheng Guo},
  doi          = {10.1016/j.asoc.2023.110658},
  journal      = {Applied Soft Computing},
  pages        = {110658},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reinforcement learning-artificial bee colony algorithm for flexible job-shop scheduling problem with lot streaming},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based density peak merging for identifying multi-peak
clusters. <em>ASOC</em>, <em>146</em>, 110657. (<a
href="https://doi.org/10.1016/j.asoc.2023.110657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC), which is short for clustering by fast search-and-find of density peaks, is a recently developed density-based clustering method that is widely used because of its effective detection of isolated high-density regions. However, it often fails to identify true cluster structures from data owing to its intrinsic assumption that a cluster has a unique and high-density center, because a single cluster can contain several peaks. We call this the “multi-peak problem”. To overcome this, we propose a peak merging method for clustering. In the proposed algorithm, a valley and its local density are defined to identify the intersection between two adjoined peaks. These are used to construct directed and connected subgraphs , using which we merge multiple peaks if needed. Unlike DPC and its variants, the proposed method is capable of identifying highly complex shaped clusters with no interpretation of the decision graph. Numerical experiments based on synthetic and real datasets demonstrated that our method outperformed the benchmarking methods.},
  archive      = {J_ASOC},
  author       = {Minseok Han and Jong-Seok Lee},
  doi          = {10.1016/j.asoc.2023.110657},
  journal      = {Applied Soft Computing},
  pages        = {110657},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph-based density peak merging for identifying multi-peak clusters},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning and clustering analysis of epileptic EEG
signals on riemannian manifold. <em>ASOC</em>, <em>146</em>, 110656. (<a
href="https://doi.org/10.1016/j.asoc.2023.110656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the nonlinearity and non-stationarity of EEG signals, the manifold becomes a new direction to analyze EEG signals. However, the problem of imbalanced epileptic seizure data and limited labeling data results in insufficient training data. The significant difference in feature distribution between cross-subject EEG signals also affects model generalization. A manifold transfer learning method based on divergence alignment (MTLDA) was proposed to overcome the few-shot problem. By constructing an objective function based on domain divergence, the transformation matrix is obtained to reduce the feature distribution difference between source and target domains. Additionally, for less labeled data on the manifold, a Riemannian manifold fuzzy clustering (RMFC) was proposed to realize unsupervised pattern recognition of the sample on the manifold. MTLDA and RMFC use Riemannian distance and Riemannian mean rather than the classical Euclidean distance and mean. Both methods do not require labeling information from subjects. The experiment results on two public datasets and one private dataset show that MTLDA and RMFC are effective and outperform other comparison algorithms.},
  archive      = {J_ASOC},
  author       = {Hui Xu and Hong He and Wei Xue and Zhuangzhuang Dai and Yong Hao},
  doi          = {10.1016/j.asoc.2023.110656},
  journal      = {Applied Soft Computing},
  pages        = {110656},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transfer learning and clustering analysis of epileptic EEG signals on riemannian manifold},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-driven adaptive evolutionary multi-objective
scheduling algorithm for cloud workflows. <em>ASOC</em>, <em>146</em>,
110655. (<a href="https://doi.org/10.1016/j.asoc.2023.110655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workflow scheduling in cloud platforms is a highly challenging issue because it faces multiple conflicting optimization objectives and large-scale decision variables. Most of the existing multi-objective workflow scheduling algorithms regard the focused problems as black boxes, and optimize large-scale decision variables as a whole. This leads to inefficiency in searching solution spaces that grow exponentially with the increase of decision variables. To compensate the above deficiency, this paper proposes a knowledge-driven adaptive evolutionary multi-objective scheduling algorithm, KAMSA for short, to optimize makespan and cost of workflow execution in cloud platforms. Specifically, we excavate the knowledge that adjustment of a task’s execution only affects its successor tasks to divide large-scale decision variables into a series of groups, so as to give play to the strengths of divide-and-conquer technology to improve the evolutionary search efficiency. Moreover, we develop an adaptive resource allocation scheme to reward more evolution opportunities for groups with high contributions to further improve the evolutionary search efficiency. We compare the proposed KAMSA with five state-of-the-art competitors in the context of 20 real-world workflows and the Amazon elastic compute cloud (EC2). The comparison results verify the KAMSA’s advantages by prevailing over the five competitors on 18 out of the 20 test cases with respect to the metric hypervolume.},
  archive      = {J_ASOC},
  author       = {Hui Zhang and Xiaojuan Zheng},
  doi          = {10.1016/j.asoc.2023.110655},
  journal      = {Applied Soft Computing},
  pages        = {110655},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge-driven adaptive evolutionary multi-objective scheduling algorithm for cloud workflows},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multistage deep imputation framework for missing values
large segment imputation with statistical metrics. <em>ASOC</em>,
<em>146</em>, 110654. (<a
href="https://doi.org/10.1016/j.asoc.2023.110654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of missing values is a pervasive and unavoidable phenomenon in sensor data. Despite numerous efforts from researchers to address this issue through imputation techniques, particularly in deep learning models, the unique data distributions and periods inherent in real-world sensor data are often neglected. This paper presents a novel, multistage deep learning-based imputation framework with adaptability to missing value imputation. The framework incorporates a mixture measurement index that accounts for both low- and higher-order statistical aspects of data distribution and a more adaptive evaluation metric , which improves upon traditional mean squared error . Additionally, a multistage imputation strategy and dynamic data length adjustment are integrated into the imputation process to account for variations in data periods. Empirical results on diverse sensor data demonstrate the superiority of the proposed framework, particularly in addressing large segment imputation issues, as evidenced by improved imputation performance. The implementation and experimental results have been made publicly available on GitHub. 1},
  archive      = {J_ASOC},
  author       = {JinSheng Yang and YuanHai Shao and ChunNa Li and WenSi Wang},
  doi          = {10.1016/j.asoc.2023.110654},
  journal      = {Applied Soft Computing},
  pages        = {110654},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multistage deep imputation framework for missing values large segment imputation with statistical metrics},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of influential nodes with shapley influence
maximization extremal optimization algorithm. <em>ASOC</em>,
<em>146</em>, 110653. (<a
href="https://doi.org/10.1016/j.asoc.2023.110653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Influence Maximization Problem is a challenging computational task with multiple real-world applications. A new approach to this problem based on cooperative game theory and optimization called the Shapley Influence Maximization Extremal Optimization approach is proposed. The influence maximization problem for the independent cascade model is considered as a cooperative game, where players seek to choose seeder nodes to maximize the size of the influence set of their cascade model by maximizing their average marginal contribution to all possible coalitions. Numerical experiments conducted on both synthetic and real-world networks and comparisons with state-of-the-art algorithms show the potential of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Noémi Gaskó and Tamás Képes and Rodica Ioana Lung and Mihai Suciu},
  doi          = {10.1016/j.asoc.2023.110653},
  journal      = {Applied Soft Computing},
  pages        = {110653},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identification of influential nodes with shapley influence maximization extremal optimization algorithm},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge-based constructive estimation of distribution
algorithm for bi-objective portfolio optimization with cardinality
constraints. <em>ASOC</em>, <em>146</em>, 110652. (<a
href="https://doi.org/10.1016/j.asoc.2023.110652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio optimization is an essential and practical model for financial decision making. With the consideration of some real-world constraints, especially the cardinality constraints, the problem becomes much more challenging as it converts to a mixed-integer quadratic multi-objective optimization problem . To solve this problem, we propose a knowledge-based constructive estimation of distribution algorithm (KC-EDA) with the following three features. First, a hybrid design of Ant colony optimization (ACO) and Estimation distribution algorithm (EDA) is used to solve this mixed-variable optimization problem based on knowledge information. Second, a knowledge accumulation mechanism is designed to discover the internal relationship among the assets. The mechanism can not only guide the selection of assets effectively but also enable the use of historical information during evolution to direct the allocation of investment proportion. Third, a constructive approach is applied to construct portfolios under the constraints. This hybrid and constructive approach is incorporated into the multi-objective evolutionary framework and the experiment has been performed on the SZ50, SZ180, and SZ380 datasets (from January 2014 to December 2018). The experimental results demonstrate the effectiveness of KC-EDA in solving the portfolio optimization problem with cardinality constraints.},
  archive      = {J_ASOC},
  author       = {Zhi-Xuan Zhang and Wei-Neng Chen and Xiao-Min Hu},
  doi          = {10.1016/j.asoc.2023.110652},
  journal      = {Applied Soft Computing},
  pages        = {110652},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knowledge-based constructive estimation of distribution algorithm for bi-objective portfolio optimization with cardinality constraints},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convex-hull based robust evolutionary optimization approach
for ROC maximization under label noise. <em>ASOC</em>, <em>146</em>,
110651. (<a href="https://doi.org/10.1016/j.asoc.2023.110651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convex-hull based receiver operating characteristic (ROC) maximization has become a hot research topic due to its significance in the study of imbalance binary classification . Recently, a series of multi-objective evolutionary algorithms (MOEAs) have been proposed to maximize ROC convex hull by regarding it as a multi-objective optimization problem. However, in real applications, the ubiquitous label noises degrade the performance of the existing MOEAs. To address this issue, in this paper, we propose a robust evolutionary optimization approach, named REO, to enhance the robustness of the existing MOEAs. In the proposed approach, we firstly design a distance-based samples selection method to extract a “clean” data subset, aiming to obtain an ideal individual. Second, with the ideal individual, a problem-oriented two-stage adaptive updating strategy is designed to guide the population evolution and enhance the robustness of MOEAs. Specifically, in the first stage, based on the achieved ideal individual, a bi-level evolution direction is constructed to provide the guidance for the evolution of population. In the second stage, we utilize the cosine similarity to assign different step sizes to adaptively update the inferior individuals. Experimental results on 19 complicated datasets with different noise levels show that the proposed REO approach can effectively enhance the robustness of the existing MOEAs for ROC convex hull maximization under the label noises.},
  archive      = {J_ASOC},
  author       = {Jianfeng Qiu and Shengda Shu and Qiangqiang Zhang and Chao Wang and Fan Cheng and Xingyi Zhang},
  doi          = {10.1016/j.asoc.2023.110651},
  journal      = {Applied Soft Computing},
  pages        = {110651},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convex-hull based robust evolutionary optimization approach for ROC maximization under label noise},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective multi-verse optimizer algorithm to solve
environmental and economic dispatch. <em>ASOC</em>, <em>146</em>,
110650. (<a href="https://doi.org/10.1016/j.asoc.2023.110650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combustion and emission of coal have always been a concern. A class of multi-objective Environmental Economic Dispatch (EED) problems has been widely studied to reduce the pollution problem of fossil fuel power plants. In this study, a multi-objective Multi-Verse Optimization algorithm based on Gridded Knee Points and Plane Measurement technique (GKPPM-MVO) is proposed for the multi-objective EED problems. Knee points are usually considered as the most critical points in unbiased decision-making, while plane measurement can find the largest distant points in the population neighborhood. We apply the knee and maximum plane distance points in the local search phase . The original mechanism of parameter control in local search is replaced by using the two above points to exploit the pretty information and inherit it to the next generation. The algorithm is applied to various EED problems. The four algorithms, including MOMVO, NSGA-ii, MOABC, and MOEGO are also used to compare the performance of the algorithms thoroughly. Results show that the GKPPM-MVO algorithm has good convergence performance, high stability, and high uniformity of the Pareto Front .},
  archive      = {J_ASOC},
  author       = {Wangying Xu and Xiaobing Yu},
  doi          = {10.1016/j.asoc.2023.110650},
  journal      = {Applied Soft Computing},
  pages        = {110650},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective multi-verse optimizer algorithm to solve environmental and economic dispatch},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biomarker identification and cancer survival prediction
using random spatial local best cat swarm and bayesian optimized DNN.
<em>ASOC</em>, <em>146</em>, 110649. (<a
href="https://doi.org/10.1016/j.asoc.2023.110649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying cancer biomarkers is crucial for improving patient outcomes and reducing cancer-related deaths. This research proposes BioSurv, a framework for biomarker identification and cancer survival prediction, using machine learning and deep learning techniques. Multi-omics data from breast cancer (BRCA) and lung adenocarcinoma (LUAD), including mRNA, miRNA, CNV, and DNA methylation , are analyzed. The collected dataset is passed to statistical tests and the random spatial local best cat swarm optimization (RSLBCSO) algorithm for feature selection, followed by KEGG and survival analyses to identify prognostic markers. Thirteen BRCA and fifteen LUAD poor prognostic markers are identified. A Bayesian optimized deep neural network (DNN) is used for cancer survival prediction, achieving high accuracy of 90\% and 91\% for BRCA and LUAD, respectively.},
  archive      = {J_ASOC},
  author       = {Arwinder Dhillon and Ashima Singh and Vinod Kumar Bhalla},
  doi          = {10.1016/j.asoc.2023.110649},
  journal      = {Applied Soft Computing},
  pages        = {110649},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Biomarker identification and cancer survival prediction using random spatial local best cat swarm and bayesian optimized DNN},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual secret sharing of gray and color images using fuzzy
random grids. <em>ASOC</em>, <em>146</em>, 110648. (<a
href="https://doi.org/10.1016/j.asoc.2023.110648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual secret sharing is a method used to secure images by converting a secret image into multiple shares so that the original image cannot be recognized from individual shares, and no calculations are required during the image decryption, but the original image can be visually seen by stacking the shares. However, one of the significant challenges in this method is the increased size of the shares and the recovered image, which is called pixel expansion. Researchers have proposed several methods to solve this problem, but many of them are limited to binary images , and they cannot handle gray and color images without converting them to binary. This study proposes a novel fuzzy random grid-based method to directly encrypt gray and color images without any pixel expansions. In this method, fuzzy random grids with decimal values between 0.0 to 1.0 are generated during the encryption stage, and fuzzy operators are used for the decryption stage. The evaluation results demonstrate the ability of the proposed solution in encrypting gray and color images without converting them to binary and without any pixel expansions. The individual shares of the proposed method do not reveal any information from the original image, making it a secure method. The quality of the decrypted images has been evaluated using both subjective and objective evaluation methods such as PSNR and SSIM metrics, which show state-of-the-art results.},
  archive      = {J_ASOC},
  author       = {Mostafa MokhtariArdakan and Reza Ramezani and AliMohammad Latif},
  doi          = {10.1016/j.asoc.2023.110648},
  journal      = {Applied Soft Computing},
  pages        = {110648},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Visual secret sharing of gray and color images using fuzzy random grids},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolutionary artificial neural network approach for
spatio-temporal wave height time series reconstruction. <em>ASOC</em>,
<em>146</em>, 110647. (<a
href="https://doi.org/10.1016/j.asoc.2023.110647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel methodology for recovering missing time series data , a crucial task for subsequent Machine Learning (ML) analyses. The methodology is specifically applied to Significant Wave Height (SWH) time series in the field of marine engineering. The proposed approach involves two phases. Firstly, the SWH time series for each buoy is independently reconstructed using three transfer function models: regression-based, correlation-based, and distance-based. The distance-based transfer function exhibits the best overall performance. Secondly, Evolutionary Artificial Neural Networks (EANNs) are utilised for the final recovery of each time series, using as inputs highly correlated buoys that have been intermediately recovered. The EANNs are evolved considering two metrics, the novel squared error relevance area, which balances the importance of extreme and around-mean values, and the well-known mean squared error . The study considers SWH time series data from 15 buoys in two coastal zones in the United States. The results demonstrate that the distance-based transfer function is generally the best transfer function, and that EANNs outperform a range of state-of-the-art ML techniques in 12 out of the 15 buoys, with a number of connections comparable to linear models. Furthermore, the proposed methodology outperforms the two most popular approaches for time series reconstruction, BRITS and SAITS, for all buoys except one. Therefore, the proposed methodology provides a promising approach, which may be applied to time series from other fields, such as wind or solar energy farms in the field of green energy.},
  archive      = {J_ASOC},
  author       = {David Guijo-Rubio and Antonio M. Durán-Rosal and Antonio M. Gómez-Orellana and Juan C. Fernández},
  doi          = {10.1016/j.asoc.2023.110647},
  journal      = {Applied Soft Computing},
  pages        = {110647},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary artificial neural network approach for spatio-temporal wave height time series reconstruction},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying optimal architectures of physics-informed neural
networks by evolutionary strategy. <em>ASOC</em>, <em>146</em>, 110646.
(<a href="https://doi.org/10.1016/j.asoc.2023.110646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-Informed Neural Networks (PINNs) are artificial neural networks that encode Partial Differential Equations (PDEs) as an integral component of the ML model. PINNs are successfully used nowadays to solve PDEs, fractional equations, and integral–differential equations, including direct and inverse problems . Just as in the case of other kinds of artificial neural networks , the architecture, including the number and sizes of layers, activation functions , and other hyperparameters can significantly influence the network performance. Despite the serious work in this field, there are still no clear directions on how to choose an optimal network architecture in a consistent manner. In practice, expertise is required, with a significant number of manual trial and error cycles. In this paper, we propose PINN/GA (PINN/Genetic Algorithm), a fully automatic design of a PINN by an evolutionary strategy with specially tailored operators of selection, crossover, and mutation, adapted for deep neural network architecture and hyperparameter search. The PINN/GA strategy starts from the population of simple PINNs, adding new layers only if it brings clear accuracy benefits, keeping PINNs in the population as simple as possible. Since the examination of dozens of neural networks through the evolutionary process implies enormous computational costs, it employs a scalable computational design based on containers and Kubernetes batching orchestration. To demonstrate the potential of the proposed approach, we chose two non-trivial direct problems. The first is 1D Stefan transient model with time-dependent Dirichlet boundary conditions , describing the melting process, and the second is the Helmholtz wave equation over a 2D square domain. The authors found that PINNs accuracy gradually improves throughout the evolutionary process, exhibiting better performance and stability than parallel random search and Hyperopt Tree of Parzen Estimators, while keeping the network design reasonably simple.},
  archive      = {J_ASOC},
  author       = {Ana Kaplarević-Mališić and Branka Andrijević and Filip Bojović and Srđan Nikolić and Lazar Krstić and Boban Stojanović and Miloš Ivanović},
  doi          = {10.1016/j.asoc.2023.110646},
  journal      = {Applied Soft Computing},
  pages        = {110646},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identifying optimal architectures of physics-informed neural networks by evolutionary strategy},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty measurement for single cell RNA-seq data based
on class-consistent technology with application to semi-supervised gene
selection. <em>ASOC</em>, <em>146</em>, 110645. (<a
href="https://doi.org/10.1016/j.asoc.2023.110645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the high cost of label collection, people are now faced with a large number of partially labeled gene expression data ( p l g e plge -data). Single cell RNA-seq data ( s c r s scrs -data) are a kind of important p l g e plge -data and reflect the abundance of gene transcript mRNA measured directly or indirectly in cells. For convenience, a decision information system (DIS) based on s c r s scrs -data is called a single cell gene decision space ( s c g d scgd -space). Due to the high dimensionality of s c r s scrs -data, feature selection must be done before clustering and classification. The existing feature selection methods based on equivalence relation are ineffective for the s c g d scgd -space owing to the strictness of equality between information values. To solve the above problems, this paper studies the uncertainty measurement of the s c g d scgd -space based on class-consistent technology and considers its application to semi-supervised gene selection. Class-consistent technology replaces equality with approximate equality between two expression values at a gene. Based on the proposed technology, class-consistent and non-class-consistent relations on the cell set of the s c g d scgd -space are established first. Then, the s c g d scgd -space ( O , A , d ) (O, A, d) is divided into labeled space ( O l , A , d ) (Ol, A, d) and unlabeled space ( O u , A , d ) (Ou, A, d) . Next, four metrics of importance on each gene subset of ( O , A , d ) (O, A, d) are defined. They are the weighted sum of ( O l , A , d ) (Ol, A, d) and ( O u , A , d ) (Ou, A, d) determined by the missing rate of labels and the established relations and can be used to measure the uncertainty of ( O , A , d ) (O, A, d) . In addition, as an application of four metrics to the s c g d scgd -space, a semi-supervised gene selection algorithm is designed. Finally, the experimental results and statistical tests on 16 large-scale s c r s scrs -data sets show that the defined metrics can effectively measure the uncertainty of the s c g d scgd -space. The designed algorithm with a high reduction rate outperforms some state-of-the-art feature selection algorithms in terms of eight performance evaluation indicators.},
  archive      = {J_ASOC},
  author       = {Qinli Zhang and Zhengwei Zhao and Fang Liu and Zhaowen Li},
  doi          = {10.1016/j.asoc.2023.110645},
  journal      = {Applied Soft Computing},
  pages        = {110645},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty measurement for single cell RNA-seq data based on class-consistent technology with application to semi-supervised gene selection},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Everything is varied: The surprising impact of instantial
variation on ML reliability. <em>ASOC</em>, <em>146</em>, 110644. (<a
href="https://doi.org/10.1016/j.asoc.2023.110644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instantial variation (IV) refers to variation that is due not to population differences or errors, but rather to within-subject variation, that is the intrinsic and characteristic patterns of variation pertaining to a given instance or the measurement process. Although taking into account IV is critical for the proper analysis of the results, this source of uncertainty and its impact on robustness have so far been neglected in Machine Learning (ML). To fill this gap, we look at how IV affects ML performance and generalization, and how its impact can be mitigated. Specifically, we provide a methodological contribution to formalize the problem of IV in the statistical learning framework. To prove the relevance of our contribution, we focus on one of the most critical domains, healthcare, and take individual (analytical and biological) variation as a specific kind of IV; in this domain, we use one of the largest real-world laboratory medicine datasets for the task of COVID-19 detection, to show that: (1) common state-of-the-art ML models are severely impacted by the presence of IV in data; and (2) advanced learning strategies, based on data augmentation and soft computing methods (data imprecisiation), and proper study designs can be effective at improving robustness to IV. Our findings demonstrate the critical relevance of correctly accounting for IV to enable safe deployment of ML in real-world settings.},
  archive      = {J_ASOC},
  author       = {Andrea Campagner and Lorenzo Famiglini and Anna Carobene and Federico Cabitza},
  doi          = {10.1016/j.asoc.2023.110644},
  journal      = {Applied Soft Computing},
  pages        = {110644},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Everything is varied: The surprising impact of instantial variation on ML reliability},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-additive robust ordinal regression: A multicriteria
decision model based on hierarchical-level-bidirectional choquet
integral. <em>ASOC</em>, <em>146</em>, 110643. (<a
href="https://doi.org/10.1016/j.asoc.2023.110643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to the stable development of the traditional face to face teaching, distance education has become a considerable portion in higher education today and it is still rapidly growing. The mixed modes in education promote a great challenge for the professors and administrators to accurately and fairly evaluate the performance of students. Indeed, students’ performance evaluation is a multicriteria decision problem. To obtain a robust auxiliary decision support scheme compatible with the preferences of decision maker (DM), challenges often confronted in multicriteria decision analysis include dealing with complex structure of criteria, handling interaction between criteria and reducing the cognitive efforts of the DM. Based on the disaggregation–aggregation principle, complex preference is refined into smaller, manageable modules from two dimensions of alternative evaluation and criteria hierarchy. Then, a hierarchical-level-bidirectional Choquet integral preference method is developed to reproduce the refined preference and obtain the comprehensive evaluation of the alternatives. Further, a class of non-additive robust ordinal regression is performed by utilizing the characteristic indicators of the new Choquet integral and the preference relations given by the DM. Moreover, a representative preference method is put forward to rule out some preconceived possible relations, which enriches the necessary relations among alternatives. Finally, an illustrative example is provided to verify the practicability of the above multicriteria decision support methods.},
  archive      = {J_ASOC},
  author       = {Huanhuan Song and Zaiwu Gong and Guo Wei and Weiwei Guo and Xiujuan Ma and Enrique Herrera-Viedma},
  doi          = {10.1016/j.asoc.2023.110643},
  journal      = {Applied Soft Computing},
  pages        = {110643},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Non-additive robust ordinal regression: A multicriteria decision model based on hierarchical-level-bidirectional choquet integral},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing inconsistencies of FAHP in structural safety
assessment of diversion tunnels. <em>ASOC</em>, <em>146</em>, 110642.
(<a href="https://doi.org/10.1016/j.asoc.2023.110642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fuzzy Analytic Hierarchy Process (FAHP) is an effective method for assessing risks and has great potential for assessing the structural safety of diversion tunnels . However, this type of professional assessment can become more complicated due to the presence of multiple factors that influence the safety of the tunnel. This complexity not only necessitates the establishment of a comprehensive index system but also introduces uncertainties related to inconsistencies in expert judgments and unknown expert weights in group decision-making process. To address these issues, this study proposed an improved FAHP method for the structural safety assessment of diversion tunnels . This method incorporates a proposed comprehensive index system and incorporates a preceding ordering process before pairwise comparisons in the questionnaire to enhance the consistency in expert judgment. The consistency and compatibility of expert judgment matrices are quantified using a distance vector rather than a single number, and a reasonable comprehensive weight is calculated based on the Dempster–Shafer (D–S) evidence theory. We applied this method to the structure safety assessment of the Houziyan hydropower station diversion tunnel and found an overall low risk for the tunnel, but significant issues were identified regarding lining crack. Furthermore, we conducted a comparative evaluation of the proposed method against other related methods, thereby demonstrating its high quality. The study contributes an improved FAHP method for the structural safety analysis of diversion tunnels and encourages the wider application of FAHP in more complicated systems.},
  archive      = {J_ASOC},
  author       = {Kang Liu and Yongcan Chen and Haoran Wang and Hui Xie and Zhaowei Liu},
  doi          = {10.1016/j.asoc.2023.110642},
  journal      = {Applied Soft Computing},
  pages        = {110642},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reducing inconsistencies of FAHP in structural safety assessment of diversion tunnels},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A negative survey based privacy preservation method for
topology of social networks. <em>ASOC</em>, <em>146</em>, 110641. (<a
href="https://doi.org/10.1016/j.asoc.2023.110641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of social platforms, the social network has aroused wide attention. Since social networks contain a lot of personal sensitive information , many privacy preservation methods have been designed for social networks to allay concerns about privacy disclosure of people. However, most of the existing methods disturb the social networks too much to ensure the utility of social networks. To this end, we propose a negative survey based privacy preservation method, called NetNS, to preserve the topology privacy of social networks, where a dedicated negative survey model is developed to disturb edges in social networks in order to preserve the topology privacy of them. The theoretical analysis indicates that the developed NetNS is efficient, and can resist two common graph structure attacks including friendship attack and subgraph attack, while empirical studies conducted on three real-world social networks show that compared to six existing privacy preservation algorithms tailored for the topology of social networks, the developed NetNS can provide disturbed social networks with better utility while achieving same privacy preservation level.},
  archive      = {J_ASOC},
  author       = {Hao Jiang and Yuerong Liao and Dongdong Zhao and Yiheng Li and Kehang Mu and Qianwei Yu},
  doi          = {10.1016/j.asoc.2023.110641},
  journal      = {Applied Soft Computing},
  pages        = {110641},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A negative survey based privacy preservation method for topology of social networks},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Robust echo state network with cauchy loss function and
hybrid regularization for noisy time series prediction. <em>ASOC</em>,
<em>146</em>, 110640. (<a
href="https://doi.org/10.1016/j.asoc.2023.110640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy time series prediction is a hot research topic in practical applications. Echo state networks (ESNs) have superior performance on time series prediction. However, the ill-posed problem may weaken the generalization performance of ESNs when dealing with real data, because of the noise and the overlarge reservoir. To solve this problem, this paper proposes a robust ESN with a Cauchy loss function and hybrid regularizations for noisy time series prediction. First, the Cauchy loss function is used to suppress the large noise mixed in the real data. Then, an improved L 1 L1 regularization and the L 2 L2 regularization are combined to shrink the larger output weights and produce the sparse solution . Particularly, a simple iterative algorithm is proposed to calculate the output weights of the network. Finally, simulation results on both synthetic and real-world data sets show the super performance of the proposed model.},
  archive      = {J_ASOC},
  author       = {Fanjun Li and Ying Li},
  doi          = {10.1016/j.asoc.2023.110640},
  journal      = {Applied Soft Computing},
  pages        = {110640},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust echo state network with cauchy loss function and hybrid regularization for noisy time series prediction},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ASMEvoNAS: Adaptive segmented multi-objective evolutionary
network architecture search. <em>ASOC</em>, <em>146</em>, 110639. (<a
href="https://doi.org/10.1016/j.asoc.2023.110639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network architecture search (NAS) has attracted much attention as an automatic design technique of network architecture. In particular, multi-objective evolutionary algorithms (MOEAs) have been a popular kind of optimizer in NAS due to their global optimization capability. However, as a population-based iterative search method, MOEAs are subject to the unbearable computational cost of individual evaluation on multiple objectives at each generation, which affects their generalization ability and transferability of MOEA-based NAS. Therefore, an adaptive segmented multi-objective evolutionary network architecture search (ASMEvoNAS) method is proposed in this paper. Firstly, an adaptive segmented evaluation strategy is designed to adaptively select different but more suitable objectives to efficiently assess the candidate architectures at different evolutionary stages, instead of evaluating them by all the considered objectives simultaneously. Thus, the computational cost and complexity of the search process can be controlled and reduced to some extent. Secondly, a preference-based pre-selection strategy is designed to filter out the initialized architectures with high parameter quantities to reduce the total parameter scale of the whole population and memory consumption. Last, a novel desirable gene reservation-based crossover and a directed connection-based mutation are proposed to produce offspring. Experimental results show that ASMEvoNAS shows promising performance on CIFAR-10, CIFAR-100, and ImageNet with error rates of 2.21\%, 15.57\%, and 24.43\% top-1, respectively. The proposed method reduces the search cost to 0.36 GPU-Days on CIFAR-10 while maintaining competitive classification performance compared to state-of-the-art networks. In addition, ASMEvoNAS presents superior performance when dealing with the considered transfer tasks, as well as the benchmark dataset of NAS.},
  archive      = {J_ASOC},
  author       = {Li Yan and Zhipeng Zhang and Jing Liang and Boyang Qu and Kunjie Yu and Kongyuan Wang},
  doi          = {10.1016/j.asoc.2023.110639},
  journal      = {Applied Soft Computing},
  pages        = {110639},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ASMEvoNAS: Adaptive segmented multi-objective evolutionary network architecture search},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A genetic algorithm for balancing and sequencing of
mixed-model two-sided assembly line with unpaced synchronous transfer.
<em>ASOC</em>, <em>146</em>, 110638. (<a
href="https://doi.org/10.1016/j.asoc.2023.110638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on the assembly line balancing problem has been continuously incorporated the characteristics of actual production lines without considering the mixed-model two-sided assembly line with unpaced synchronous transfer, which is common in body shops of cars or trucks. This study attempts to model a mixed-model two-sided assembly line balancing and sequencing problem with unpaced synchronous transfer, and designs a genetic algorithm to solve this problem. First, to describe the process whereby model sequence and its execution, station and its number, and the process time caused by task allocation lead to the variable output of an unpaced synchronous line, a model without an observation point restriction is proposed. Based on this, a mathematical model of a balancing and sequencing problem of a mixed-model two-sided assembly line with an unpaced synchronous transfer is established, in which the balancing problem is aimed at Type-II. Then, an improved genetic algorithm adapted to the complexity of the problem is proposed. The algorithm designs a combination and evaluation mechanism to ensure that the solution populations of the balancing and sequencing problems are sufficiently combined and evaluated in each iteration to improve efficiency. The proposed algorithm is illustrated using a small numerical example. Finally, the algorithm is experimentally investigated using a set of test cases and several comparison algorithms. The experimental results show that the combination and evaluation mechanism is suitable for these problems, especially for large-sized cases, and that the proposed algorithm is effective and has better performance than the comparison algorithms.},
  archive      = {J_ASOC},
  author       = {Shi-Gen Liao and Yi-Bo Zhang and Chun-Yan Sang and Hui Liu},
  doi          = {10.1016/j.asoc.2023.110638},
  journal      = {Applied Soft Computing},
  pages        = {110638},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic algorithm for balancing and sequencing of mixed-model two-sided assembly line with unpaced synchronous transfer},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing supervised deep learning MRI reconstruction to
multiple and unseen contrasts using meta-learning hypernetworks.
<em>ASOC</em>, <em>146</em>, 110633. (<a
href="https://doi.org/10.1016/j.asoc.2023.110633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning has recently been an emerging data-efficient learning technique for various medical imaging operations and has helped advance contemporary deep learning models. Furthermore, meta-learning enhances the knowledge generalization of the imaging tasks by learning both shared and discriminative weights for various configurations of imaging tasks during training. However, existing meta-learning models attempt to learn a single set of weight initializations of a neural network that might be fundamentally restrictive under the heterogeneous (multimodal) data scenario. This work aims to develop a multimodal meta-learning model for image reconstruction, which augments meta-learning with evolutionary capabilities to encompass diverse acquisition settings of heterogeneous data . Our proposed model called KM-MAML ( K ernel M odulation-based M ultimodal M eta- L earning), has hypernetworks (auxiliary learners) that evolve to generate mode-specific (or context-specific) weights. These weights provide the mode-specific inductive bias for multiple modes by re-calibrating each kernel of the base network for image reconstruction via a low-rank kernel modulation operation. Furthermore, we incorporate gradient-based meta-learning (GBML) in the contextual space to update the weights of the hypernetworks based on different modes. The hypernetworks and the base reconstruction network in the GBML setting provide discriminative mode-specific features and low-level image features , respectively. We extensively evaluate our model for multi-contrast magnetic resonance image reconstruction considering the essential research directions in fastMRI for multimodal and rich transfer learning capabilities across various MRI contrasts . Our comparative studies show that the proposed model (i) exhibits superior reconstruction performance over joint training, other meta-learning methods, and various context-specific MRI reconstruction architectures, and (ii) better adaptation to 80\% and 92\% of unseen multi-contrast data contexts with improvement margins of 0.1 to 0.5 dB in PSNR and around 0.01 in SSIM, respectively. Besides, a representation analysis with U-Net as the base network shows that kernel modulation infuses 80\% of mode-specific representation changes in the high-resolution layers. Our source code is available at https://github.com/sriprabhar/KM-MAML/ .},
  archive      = {J_ASOC},
  author       = {Sriprabha Ramanarayanan and Arun Palla and Keerthi Ram and Mohanasankar Sivaprakasam},
  doi          = {10.1016/j.asoc.2023.110633},
  journal      = {Applied Soft Computing},
  pages        = {110633},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generalizing supervised deep learning MRI reconstruction to multiple and unseen contrasts using meta-learning hypernetworks},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bipolar fuzzy p-competition graph based ARAS technique for
prioritizing COVID-19 vaccines. <em>ASOC</em>, <em>146</em>, 110632. (<a
href="https://doi.org/10.1016/j.asoc.2023.110632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bipolar fuzzy competition graph helps to examine the competition between two vertices when they have a common out-neighbourhood. With this idea, the bipolar fuzzy p p -competition graph (BF p p CG) can be defined when two vertices have p p common out-neighbourhood. In this study, the above-said concepts are defined along with certain characteristics such as low, high, semi-perfect and perfect competitive competition graphs based on the number of components, and certain important results are derived. Moreover, the extensions of BF p p CG are discussed by considering various situations and illustrated by a food web problem of 13 species. Besides, an application is taken to find the competition among the COVID-19 vaccines against five mutations: Alpha, Beta, Gamma, Delta and Omicron. Multi-criteria decision-making (MCDM) techniques utilize to access the prioritization problems. The additive ratio assessment (ARAS) is a robust MCDM technique and it simplifies complex decision-making problems through the utility degree. An efficient outcome can be attained by this technique when it works with positive and negative perceptions. Therefore, this study constructs a novel bipolar fuzzy ARAS system based on the BF p p CG. To demonstrate the applicability of the proposed system, an application is taken in prioritizing COVID-19 vaccines, where Moderna and CoronaVac resulted in first and last rank, respectively. For this purpose, the efficacy rate of the vaccines against severe disease is considered. Finally, the outcomes are validated through sensitivity and comparative analyses. Then, the managerial implications, future works and limitations of the study are provided to explore the proposed research.},
  archive      = {J_ASOC},
  author       = {Deva Nithyanandham and Felix Augustin},
  doi          = {10.1016/j.asoc.2023.110632},
  journal      = {Applied Soft Computing},
  pages        = {110632},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bipolar fuzzy p-competition graph based ARAS technique for prioritizing COVID-19 vaccines},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust your neighbours: Handling noise in multi-objective
optimisation using kNN-averaging. <em>ASOC</em>, <em>146</em>, 110631.
(<a href="https://doi.org/10.1016/j.asoc.2023.110631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimisation (MOO) is a popular approach for finding solutions to many types of complex problems with large search spaces and conflicting search objectives. In the past, MOO algorithms have been shown to reliably produce good optimisation results. With the rise of cyber–physical systems, however, emerges the new challenge of non-deterministic system executions, caused by e.g. imperfect sensor readings or synchronisation in multi-process/multi-agent system architectures . These systems produce varying output on each execution, causing the algorithms’ observations to be noisy. Naturally, MOO algorithms favour the fittest solutions, which may have been measured with great inaccuracy. The end results are therefore not trustworthy. In this paper, we propose kNN-averaging, a new method to address this issue by identifying the k-nearest neighbours (kNN) of a solution, and using their weighted average as an estimate for its true fitness. Our experiments demonstrate the viability of kNN-averaging on 40 synthetic benchmark problems and on a real-world case study system. In the process, we compare kNN-averaging to the noisy baseline as well as two resampling-based methods and one spectral sampling approach on a range of algorithm settings. The results show that kNN-averaging approximates the fitness of solutions more accurately than the noisy baseline, leading to more trustworthy results.},
  archive      = {J_ASOC},
  author       = {Stefan Klikovits and Cédric Ho Thanh and Ahmet Cetinkaya and Paolo Arcaini},
  doi          = {10.1016/j.asoc.2023.110631},
  journal      = {Applied Soft Computing},
  pages        = {110631},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Trust your neighbours: Handling noise in multi-objective optimisation using kNN-averaging},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A short-term residential load forecasting scheme based on
the multiple correlation-temporal graph neural networks. <em>ASOC</em>,
<em>146</em>, 110629. (<a
href="https://doi.org/10.1016/j.asoc.2023.110629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate residential load forecasting (RLF) is of great significance for the decision-making and operation of modern power system . In literature, deep neural network (DNN) based RLF schemes have witnessed great development due to the advantage of automatically extracting features and capturing complex non-linear pattern in the presented data. However, most existed works separately exploit the historical data of a specific residential house to forecast its load. However, the electricity consumption behaviors among residential users are not independent, and implicitly have some correlations, which can be explicitly characterized and exploited to improve the accuracy of RLF forecasting. Inspired by this idea, through exploiting the multiple correlations among households, this paper proposes a novel residential load forecasting framework based on multiple correlation-temporal graph neural networks , RLF-MGNN. Specifically, the novelty of our work includes three aspects. First, multiple graphs are explicitly constructed to represent both linear and nonlinear correlations among temporal load series of households. That is, the synchronization graph is built to describe the degree of linear correlation between two households using Pearson correlation coefficient, which characterizes the similarity of their consumption behaviors, and the causality graph is built to describe their nonlinear correlation using transfer entropy , which characterizes the amount of directional information transfer from one time series to another, and models the mutual influence between households. Second, the multiple correlation-temporal graph convolutional networks (GCNs) are designed to forecast the residential users’ loads. In detail, at each timestep, latent features are first extracted by corresponding GCNs to embed multiple correlations among households, and then are sent to Long Short-Term Memory (LSTM) for further learning the latent temporal features. Finally, thorough experiments on real datasets demonstrate that our proposed RLF-MGNN outperforms the state-of-the-art independent DNN based schemes and other GNN based schemes.},
  archive      = {J_ASOC},
  author       = {Yufeng Wang and Lingxiao Rui and Jianhua Ma and Qun jin},
  doi          = {10.1016/j.asoc.2023.110629},
  journal      = {Applied Soft Computing},
  pages        = {110629},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A short-term residential load forecasting scheme based on the multiple correlation-temporal graph neural networks},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving the multi-robot task allocation with functional
tasks based on a hyper-heuristic algorithm. <em>ASOC</em>, <em>146</em>,
110628. (<a href="https://doi.org/10.1016/j.asoc.2023.110628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-robot task allocation (MRTA) problem is a classical problem in multi-robot systems. The most common assumption in MRTA is that all the tasks need to be completed with the time cost as small as possible. It is worth noting that some tasks may be optional in some real-world situations, i.e., the robots do not necessarily need to complete all tasks. These optional tasks do not limit the achievement of the goal, but the completion of these tasks will lead to some functional effects (e.g., making other tasks easier to be completed). Therefore, these optional tasks can be called “functional task”. Different from functional tasks, if a task must be completed, this task can be called “compulsory task”. In this paper, we study the problem where the robots need to minimize the time cost of completing all compulsory tasks and can selectively complete some functional tasks. The existence of the functional tasks greatly increases the solution space of MRTA, because the functional tasks should be firstly suitably selected and then suitably allocated. Existing related algorithms are usually based on the assumption that all tasks must be allocated. Thus, these algorithms cannot suitably deal with functional tasks. We analyze the characteristics of this problem and present a new hyper-heuristic algorithm. The low-level heuristic (LLH) in hyper-heuristic is designed to score the functional tasks by using influence diffusion model . The high-level strategy (HLS) seeks the optimal values of the key parameters in the influence diffusion model based on particle swarm optimization (PSO). Extensive simulated experiments are presented to comprehensively analyze the proposed algorithm. The proposed hyper-heuristic is compared with greedy algorithm and two meta-heuristic algorithms. Based on the simulated data, it is known that the hyper-heuristic algorithm can outperform the benchmark algorithms especially when the number of functional tasks is large.},
  archive      = {J_ASOC},
  author       = {Fuhan Yan and Kai Di},
  doi          = {10.1016/j.asoc.2023.110628},
  journal      = {Applied Soft Computing},
  pages        = {110628},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving the multi-robot task allocation with functional tasks based on a hyper-heuristic algorithm},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convergence analysis of ABC algorithm based on difference
model. <em>ASOC</em>, <em>146</em>, 110627. (<a
href="https://doi.org/10.1016/j.asoc.2023.110627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the cross-mutation phase of the Artificial Bee Colony optimization algorithm (ABC algorithm), the value of parameter φ φ is very important to the stability of ABC algorithm, however, most literatures did not specify the origin of the value range of parameter φ φ . Through the theoretical analysis of the stability of the ABC algorithm, appropriate parameters can be selected for the algorithm under different convergence conditions and different application conditions, and then the rapid convergence of the algorithm can be achieved, so the stability analysis of the algorithm is very worth exploring. This paper firstly introduces the ABC algorithm, and then differentiates the motion trajectory equations of employed bees in the ABC algorithm to obtain a differential model ; then uses the discrete system stability theory to analyze the stability of the system, and obtains the relationship between the value range of the parameters in the cross-mutation stage and the stability of the algorithm. Finally, the correctness of the conclusion will be verified by experiments using the test function. The results obtained show that ABC algorithm with φ ∈ ( − 2 , 0 ) φ∈(−2, 0) converges faster, and has better global optimization performance than ABC algorithm with other values of φ φ , which are investigated in this paper.},
  archive      = {J_ASOC},
  author       = {Ye Jiang and Hanxiao Qian and Yili Chu and Jian Liu and Zhaoneng Jiang and Feibiao Dong and Lu Jia},
  doi          = {10.1016/j.asoc.2023.110627},
  journal      = {Applied Soft Computing},
  pages        = {110627},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convergence analysis of ABC algorithm based on difference model},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EST-NAS: An evolutionary strategy with gradient descent for
neural architecture search. <em>ASOC</em>, <em>146</em>, 110624. (<a
href="https://doi.org/10.1016/j.asoc.2023.110624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using weight-sharing and continuous relaxation strategies, the gradient descent-based differential architecture search has achieved great success in automatically designing neural network architectures . However, unresolved issues, i.e., the local optimum dilemma of the gradient descent method , and the network performance collapse of the searched architecture with too many unreasonable operations, are still frustrating for researchers and practitioners. To address these two issues, a novel and efficient neural architecture search approach based on a hybrid evolutionary strategy, termed EST-NAS, is proposed in this paper. In particular, we propose using a new evolutionary strategy to explore various search directions based on the gradient descent-based neural network architecture search, aiming at obtaining a more excellent architecture. In the proposed EST-NAS, the gradient descent architecture search is performed first, and then the best architecture obtained is utilized to design an efficient initialization for the following evolutionary strategy-based architecture search. By hybridizing evolutionary strategy with gradient descent-based search, EST-NAS can improve the performance of the searched architecture with better search efficiency. Meanwhile, the validation accuracy is applied to directly measure the importance of operations, which reduces the error in the relationship between operation and task performance. Extensive experimental results in the various datasets on different search spaces show that the proposed EST-NAS achieves remarkably competitive performance with less search cost, compared to other state-of-the-art NAS approaches.},
  archive      = {J_ASOC},
  author       = {Zicheng Cai and Lei Chen and Shaoda Zeng and Yutao Lai and Hai-lin Liu},
  doi          = {10.1016/j.asoc.2023.110624},
  journal      = {Applied Soft Computing},
  pages        = {110624},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EST-NAS: An evolutionary strategy with gradient descent for neural architecture search},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On optimizing healthcare waste routing systems using waste
separation policies: A case study. <em>ASOC</em>, <em>146</em>, 110615.
(<a href="https://doi.org/10.1016/j.asoc.2023.110615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the transportation task of Health-care waste (HCW). The particularity of HCW, compared to ordinary waste, is that it may contain infectious and/or toxic substances. The disposal of this waste involves transporting it from healthcare facilities (HCF) to treatment centers. This step can be threatening to humans and the environment if not managed with the utmost care. To ensure the safe management of HCW, the separation policy between dangerous and normal waste should be honored. Hence, a vehicle routing problem with multi-compartment vehicles is adopted with the underlying objective of minimizing the total travel distance. Given the NP-hardness of the problem, an adaptive evolutionary algorithm is proposed based on the results of pre–post statistical tests of the population. The proposed algorithm is compared to the best so far solutions on medium and large instances using benchmarks from the literature. The experimental study confirms the efficiency of the proposed approach against the competing algorithms. Finally, a case study is provided to illustrate the managerial insights of the studied problem.},
  archive      = {J_ASOC},
  author       = {Hajer Ben-Romdhane and Nasreddine Ouertani and Saoussen Krichen and Issam Nouaouri},
  doi          = {10.1016/j.asoc.2023.110615},
  journal      = {Applied Soft Computing},
  pages        = {110615},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On optimizing healthcare waste routing systems using waste separation policies: A case study},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). An EEG-based brain cognitive dynamic recognition network
for representations of brain fatigue. <em>ASOC</em>, <em>146</em>,
110613. (<a href="https://doi.org/10.1016/j.asoc.2023.110613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fatigue driving will seriously threaten public safety and health, so monitoring the brain’s cognitive state accurately and exploring the fatigue process is essential. This paper proposes a 5-D Brain Cognitive Dynamic Recognition Network (E-5-D-BCDRNet) that synchronizes the brain’s cognitive changes with global features, local features , and sequence changes. The global feature refers to the 3-D brain global cognitive power maps (3-D-BGCM) constructed based on the new fatigue cognitive indicators. Local features refer to the 1-D brain local cognitive array (1-D-BLCA) created based on fatigue recognition contribution of different rhythms in each brain region. This paper uses a non-invasive wearable device to collect electroencephalogram (EEG) data from 14 subjects to evaluate the model’s performance. Compared with the existing advanced methods, the proposed method achieves the best detection effect (88.5\%). In addition, this study finds that the features learned based on different indicators are different and distributed in various brain regions by presenting the learned characteristics. Moreover, by comparing the detection results based on the local features of a single rhythm, this study finds that the frontal and occipital regions of the driver are significantly different before and after. More importantly, only a single region can also achieve significant detection effects, which shows that our method can achieve satisfactory results based on small data. In a word, the research in this paper can lay a theoretical foundation for real-time brain fatigue detection and promote the development of intelligent transportation systems .},
  archive      = {J_ASOC},
  author       = {Pengrui Li and Yongqing Zhang and Shihong Liu and Liqi Lin and Haokai Zhang and Tian Tang and Dongrui Gao},
  doi          = {10.1016/j.asoc.2023.110613},
  journal      = {Applied Soft Computing},
  pages        = {110613},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An EEG-based brain cognitive dynamic recognition network for representations of brain fatigue},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NSGA-III integrating eliminating strategy and dynamic
constraint relaxation mechanism to solve many-objective optimal power
flow problem. <em>ASOC</em>, <em>146</em>, 110612. (<a
href="https://doi.org/10.1016/j.asoc.2023.110612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing optimizing criteria in modern power systems promotes the birth of many-objective optimal power flow (Ma-OPF) problems in which more than three optimizing objective functions are considered. The increasing optimizing objectives as well as complex constraints of power balance and other system limits bring a decrease in selection pressure and challenges of constraint handling techniques for traditional multi-objective evaluation algorithms. Aiming at the solving difficulties of Ma-OPF problems, an improved NSGA-III integrating eliminating strategy and dynamic constraint relaxation mechanism (NSGA-III-EDR) is proposed. In the proposed approach, a new elimination strategy that eliminates the associated points having the largest angle with the corresponding reference line is employed to increase the selection pressure of the algorithm. An integrated constraint handling method, which employs a repair strategy based on assigning decision variables to feasible values and a penalty function approach together to handle power flow equality constraints, is also introduced into the NSGA-III-EDR. Aiming at lacking feasible solutions of the algorithm in the early searching stage , a strategy of relaxing constraint violations as well as a dynamic updating strategy for the tolerated threshold value of distinguishing feasible and infeasible solutions at the early evolution is proposed. Moreover, an improved domination sorting rule based on the proposed constraint handling method and the relaxing strategy to constraint violations is employed to promote the generation of feasible solutions. The effectiveness and feasibility of the proposed improved NSGA-III-EDR approach are studied and evaluated on different test cases of a famous standard IEEE 30-bus power system as well as the larger power systems of IEEE 57-bus and IEEE 118-bus. The numerical results show that the proposed NSGA-III-EDR method can provide solutions to many-objective OPF problems with tremendous potential compared with the traditional NSGA-III and other algorithms illustrated in the literature.},
  archive      = {J_ASOC},
  author       = {Jingrui Zhang and Junfeng Cai and Hongcai Zhang and Tengpeng Chen},
  doi          = {10.1016/j.asoc.2023.110612},
  journal      = {Applied Soft Computing},
  pages        = {110612},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NSGA-III integrating eliminating strategy and dynamic constraint relaxation mechanism to solve many-objective optimal power flow problem},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete grey wolf optimization algorithm for solving
k-coverage problem in directional sensor networks with network lifetime
maximization viewpoint. <em>ASOC</em>, <em>146</em>, 110609. (<a
href="https://doi.org/10.1016/j.asoc.2023.110609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike the traditional wireless sensor networks (WSNs) which apply omni-directional sensors, the directional sensor networks (DSNs) utilize directional sensor nodes . Each directional sensor in DSN is a self-configured in one direction and one coverage range at a same time. In critical industries, the target coverage problem in which every target should be observed by k different sensors increases the reliability and leads the fault tolerant observation. In this case, the huge amount of energy is consumed in comparison with one coverage problem. Since the business continuity in such industries strongly depends on the network lifetime, the energy management and lifetime maximization of such resource-limited networks are still of the most important challenges. To address the issue, this paper formulates the k -coverage challenge to a discrete optimization problem with the network lifespan expansion viewpoint which is an NP-Hard problem. To solve this combinatorial problem , a discrete grey wolf optimization algorithm (D-GWA) is presented. This D-GWA proposes abundant novel exploration and exploitation procedures which permutes discrete search space efficiently. In this regard, the new fitness function is also defined which steers solutions in the directions to engage sensors uniformly in different rounds. It potentially improves network lifetime because of uniform battery usage. During the course of optimization, it utilizes temperature concept of simulated annealing algorithm to running away from local trap. To verify the proposed algorithm in solving k -coverage problem in DSNs, twelve scenarios are conducted. The simulation results experimentally prove that the totally dominance of the proposed algorithm against state-of-the-arts HTOH, LA-based, GWA, and hybrid PSO-GA approaches respectively are the amount of 31.37\%, 20.97\%, 14.12\%, and 7.93\% improvement in term of network lifetime expansion. Furthermore, the behavior of D-GWA shows the high potential of scalability in the larger search space.},
  archive      = {J_ASOC},
  author       = {Vahid Reza Ekhlas and Mirsaeid Hosseini Shirvani ( Ph.D. ) and Arash Dana and Nima Raeisi},
  doi          = {10.1016/j.asoc.2023.110609},
  journal      = {Applied Soft Computing},
  pages        = {110609},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete grey wolf optimization algorithm for solving k-coverage problem in directional sensor networks with network lifetime maximization viewpoint},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequence-to-sequence digital twin model in chemical plants
with internal rolling training algorithm. <em>ASOC</em>, <em>146</em>,
110608. (<a href="https://doi.org/10.1016/j.asoc.2023.110608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-driven digital twin based on the sequence-to-sequence (StS) model is introduced in this study. The rolling training algorithm is presented as a plug-in in StS training to achieve long-term rolling predictions. The model performs long-term horizontal predictions over 8-days based on initial observed data and manipulation sequence conditions without any new historical inputs. The digital twin model is validated using the dynamic simulation of the vapor-recompression C 3 C3 process. When the rolling training algorithm is used, the StS model demonstrates the best predictions than the traditional artificial neural network model and simple StS model for 1- and 8-days tests. Moreover, noise added cannot affect the prediction performance of StS with rolling training, and the step-change test validated the StS with rolling training containing physical meaning. Hence, the proposed StS model is a data-driven digital twin having physical interpretability . The StS model can implement long-term rolling predictions for days.},
  archive      = {J_ASOC},
  author       = {Jia-Lin Kang and Somayeh Mirzaei and Zi Hang Yang},
  doi          = {10.1016/j.asoc.2023.110608},
  journal      = {Applied Soft Computing},
  pages        = {110608},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sequence-to-sequence digital twin model in chemical plants with internal rolling training algorithm},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy centric reputation index and fuzzy-based clustering
for wireless sensor networks. <em>ASOC</em>, <em>146</em>, 110602. (<a
href="https://doi.org/10.1016/j.asoc.2023.110602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy conservation is essential for a wireless sensor network to enhance its network lifetime. Clustering is one of the most important energy conservation techniques that prolong the WSN by providing balanced energy depletion across the network. Various clustering strategies have been proposed in the past to enhance the efficiency of the network till now. In the last two decades, fuzzy inference system based clustering solution has been proven to provide promising solutions. The existing fuzzy-based clustering algorithms consider the naïve network parameters as input that saves only a limited amount of energy in the network. However, in wireless sensor networks, the requirement is to make them highly energy efficient through an effective clustering strategy. To achieve effective clustering there is a need for energy-centric network parameters for the clustering instead of naïve network parameters. In this article, a novel energy-centric reputation index is proposed in which the naïve network parameters are unified with energy parameters in the background. Further, the reputation index scores are passed into a fuzzy-based approach to obtain the fitness value of the node chosen as the cluster head. Thus, in this paper, an energy-centric reputation index and fuzzy-based clustering(ECRIF) model is proposed for wireless sensor networks. The proposed model provides a highly energy-efficient wireless sensor network and is designed to work in two phases. In phase 1, three reputation indexes concerning internodal distance, distance to the sink, and the degree of the node is calculated while considering their energy consumption pattern in the background. In phase 2, the three reputation indexes are passed onto the fuzzy system as fuzzy input parameters to obtain the fitness value of the nodes for cluster head selection. Simulation results confirm that the proposed protocol enhances the overall lifetime of the network on an average by 70\% in terms of the first node dies, 61\% in terms of half node dies, and 78\% in terms of the last node dies when compared with two other existing clustering models.},
  archive      = {J_ASOC},
  author       = {Srishti Tyagi and Vivekanand Jha},
  doi          = {10.1016/j.asoc.2023.110602},
  journal      = {Applied Soft Computing},
  pages        = {110602},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy centric reputation index and fuzzy-based clustering for wireless sensor networks},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gamified teaching–learning based optimization algorithm
for a three-echelon supply chain scheduling problem in a two-stage
assembly flow shop environment. <em>ASOC</em>, <em>146</em>, 110598. (<a
href="https://doi.org/10.1016/j.asoc.2023.110598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The supply chain scheduling problem, which is an integrated production scheduling and distribution through the batch delivery system, has been studied by researchers for more than two decades. So far, all researches in this regard have studied the two-echelon supply chain network. The first novelty of this research is that, for the first time, the three-echelon supply chain scheduling problem, including supplier, manufacturer and customer, is investigated considering two separated batch delivery systems for transportation, simultaneously. To do so, a two-stage assembly flow shop environment is used. In the first stage, there are a number of decentralized suppliers who are responsible for manufacturing components. The components are dispatched to the second stage (assembly stage) with the first batch delivery system and after assembly, the orders are dispatched to the customer through secondary batch delivery system. The purpose of this problem is to minimize the cost of total completion time plus batch delivery costs. First, a mixed integer linear programming model (MILP) is presented that is able to solve small-size instances in a logical time. The second novelty of this research is that, to solve large-size instances, a novel gamified teaching–learning based optimization (GTLBO) algorithm is developed with applying a number of dominance rules and gamification tools. The computational results indicate the superiority of the new TLBO variant over its standard form and two well-known methods, i.e. GA and PSO , especially by applying the dominance rules.},
  archive      = {J_ASOC},
  author       = {Mohammad Rostami and Ali Yousefzadeh},
  doi          = {10.1016/j.asoc.2023.110598},
  journal      = {Applied Soft Computing},
  pages        = {110598},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gamified teaching–learning based optimization algorithm for a three-echelon supply chain scheduling problem in a two-stage assembly flow shop environment},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale network via progressive multi-granularity
attention for fine-grained visual classification. <em>ASOC</em>,
<em>146</em>, 110588. (<a
href="https://doi.org/10.1016/j.asoc.2023.110588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) is challenging due to the subtle inter-class variations. Key region location and discriminative feature extraction are the crucial aspects of FGVC. In this paper, we carry out research from the perspective of multi-granularity and multi-scale, explore the information in different granularities and scales, and propose a novel method called Multi-Scale Network via Progressive Multi-Granularity Attention (MSMGA-Net), which locates the key discriminative regions while avoiding recognition confusion caused by subtle inter-class variations. In order to achieve discriminative region locating, we adopt a progressive training strategy to make the network work in steps during training process, and the jigsaw puzzle generator (JPGen) is constructed to generate images with different granularity for different training steps. The focus of each step of training is to locate the key discriminative regions of specific granularity image at the corresponding stage of the network through the multi-granularity attention (MGA) module. Aiming to obtain the discriminative features, we also construct a multi-scale convolution (MSC) module for feature extraction corresponding to each step respectively, enabling the capture of different scales of details. We validated our MSMGA-Net through extensive experiments on CUB-200-2011, FGVC Aircraft and Stanford Cars. The experiments demonstrate that MSMGA-Net can get promising results. In particular, the accuracy on two publicly competitive benchmark datasets, CUB-200-2011 and FGVC Aircraft, reached 90.30\% and 93.84\%, respectively.},
  archive      = {J_ASOC},
  author       = {Chen An and Xiaodong Wang and Zhiqiang Wei and Ke Zhang and Lei Huang},
  doi          = {10.1016/j.asoc.2023.110588},
  journal      = {Applied Soft Computing},
  pages        = {110588},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale network via progressive multi-granularity attention for fine-grained visual classification},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BE-GWO: Binary extremum-based grey wolf optimizer for
discrete optimization problems. <em>ASOC</em>, <em>146</em>, 110583. (<a
href="https://doi.org/10.1016/j.asoc.2023.110583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most metaheuristic algorithms for continuous search space have been developed, a number of transfer functions have been proposed including S-shaped, V-shaped, linear, U-shaped, and X-shaped to convert the continuous solution to the binary one. However, most existing transfer functions do not provide exploration and exploitation required to solve complex discrete problems. Thus, in this study, an improved binary GWO named extremum-based GWO (BE-GWO) algorithm is introduced. The proposed algorithm proposes a new cosine transfer function (CTF) to convert the continuous GWO to the binary form and then introduces an extremum (Ex) search strategy to improve the efficiency of converted binary solutions. The performance of the BE-GWO was evaluated through solving two binary optimization problems , the feature selection and the 0-1 multidimensional knapsack problem (MKP). The results of feature selection problems were compared with several well-known binary metaheuristic algorithms such as BPSO, BGSA, BitABC, bALO, bGWO, BDA, BSSA, and BinABC. Moreover, the results were compared with four versions of the binary GWO, the binary PSO , and the binary ABC. In addition, the BE-GWO algorithm was evaluated to solve the 0-1 MKP with difficult and very difficult benchmark instances and the results were compared with several binary GWO variants. The results of two binary problems were statistically analyzed by the Friedman test. The experimental results showed that the proposed BE-GWO algorithm enhances the performance of binary GWO in terms of solution accuracy, convergence speed, exploration, and balancing between exploration and exploitation.},
  archive      = {J_ASOC},
  author       = {Mahdis Banaie-Dezfouli and Mohammad H. Nadimi-Shahraki and Zahra Beheshti},
  doi          = {10.1016/j.asoc.2023.110583},
  journal      = {Applied Soft Computing},
  pages        = {110583},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BE-GWO: Binary extremum-based grey wolf optimizer for discrete optimization problems},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comment on “deep reinforcement learning approach for MPPT
control of partially shaded PV systems in smart grids.” <em>ASOC</em>,
<em>146</em>, 110577. (<a
href="https://doi.org/10.1016/j.asoc.2023.110577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a recent work, (Avila et al., 2020), an environment was developed and reported for partial shading conditions (PSC) in the open-source OpenAI Gym platform. This work presented deep reinforcement learning (DRL) techniques to address the maximum power point tracking (MPPT) problem of a photovoltaic (PV) array under PSC. Two DRL algorithms, namely, Deep Deterministic Policy Gradient (DDPG) and Twin Delayed DDPG (TD3) were investigated. A deviation of less than 1\%, compared to the theoretical maximum power, was claimed for the DDPG algorithm. Based on the presented fact-based investigations, this comment highlights the issues in the reported approach. The main issues are found to be the approximate PV array modeling, erroneous performance metric and erroneous choice of DRL algorithms. Of the presented PSCs, the proposed technique in Avila et al. (2020) always attained the very first peak of the PV characteristic of the PV system . It may be noted that the first peak may not always be the global maximum power point . Overall, based on the presented investigations this comment demonstrates that the DDPG technique used in Avila et al. (2020) is not able to effectively address the MPPT problem in PV array under PSC.},
  archive      = {J_ASOC},
  author       = {Vicky Jaiswal and Archit Wadehra and Siddhant Bhalla and K.P.S. Rana and Vineet Kumar},
  doi          = {10.1016/j.asoc.2023.110577},
  journal      = {Applied Soft Computing},
  pages        = {110577},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comment on “Deep reinforcement learning approach for MPPT control of partially shaded PV systems in smart grids”},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A study of crude oil futures price volatility based on
multi-dimensional data from event-driven and deep learning perspectives.
<em>ASOC</em>, <em>146</em>, 110548. (<a
href="https://doi.org/10.1016/j.asoc.2023.110548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two unique challenges in current research of price volatility studies in the crude oil futures market, which is how to capture the impact of interactivity between data as much as possible when multi-dimensional data are fused, and how to solve the problem of temporal heterogeneity of research samples existing in multi-dimensional data. We proposed a tensor-based event-driven gated recurrent unit (Event-GRU) deep learning model that helps to solve the above dilemmas. Our model can effectively prevent the loss of interaction relationship of multi-dimensional data by taking the advantage of spatial data representation of tensor, as well as using news occurrence as a trigger to construct the event-driven mechanism based on GRU model, in order to overcome the feature space distortion between discrete news data and continuous trading fundamentals data due to the uneven sample time matching. Regarding the current insufficient research of crude oil prices in China, we addressed the crude oil futures trading data from the Shanghai Futures Exchange and 31, 422 news data of the crude oil futures market from April 2018 to August 2020. We used the XGBoost algorithm (eXtreme Gradient Boosting) to evaluate and filter. Besides, we built a tensor-based event-GRU deep learning model to finally analyze the closing price on the day after. This research contributes to improving the accuracy of crude oil futures price analysis compared with using benchmark models such as DT , SVM , BP, LSTM, and GRU, and laying the foundation for the subsequent opening of corresponding quantitative investment strategies.},
  archive      = {J_ASOC},
  author       = {Jun Wang and Wenjin Zhao and Fu-Sheng Tsai and Hanlei Jin and Jinghua Tan and Chao Su},
  doi          = {10.1016/j.asoc.2023.110548},
  journal      = {Applied Soft Computing},
  pages        = {110548},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A study of crude oil futures price volatility based on multi-dimensional data from event-driven and deep learning perspectives},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An antigravity-based fuzzy gravitational search algorithm
for economic dispatch problems. <em>ASOC</em>, <em>145</em>, 110630. (<a
href="https://doi.org/10.1016/j.asoc.2023.110630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an antigravity-based fuzzy Gravitational Search Algorithm (AFGSA) to solve the economic dispatch problem (EDP) with various constraints. Based on an antigravity mechanism combined with an adaptive fuzzy parameter controller AFGSA is designed. By doing so, it ensures a proper balance between exploration and exploitation of GSA. Furthermore, the simulated annealing (SA) process is introduced to control the application of anti-gravitation forces and guarantee the convergence rate. A fuzzy optimizing system is applied to adaptively control the parameters in GSA. The success of the suggested algorithm is firstly shown in eight various test EDPs including static and dynamic economic load problems as well as hydrothermal scheduling problems by varying the complex constraints. Then, the efficiency of the proposed optimizer is validated through ten CEC2020 test functions. The performance of AFGSA is verified by comparison with numerous state-of-the-art techniques. The experimental results show that AFGSA provides a more competing and robust solution to the EDPs than the compared algorithms.},
  archive      = {J_ASOC},
  author       = {Xianrui Yu and Qiuhong Zhao and Tongyu Wang and Yuanrui Li},
  doi          = {10.1016/j.asoc.2023.110630},
  journal      = {Applied Soft Computing},
  pages        = {110630},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An antigravity-based fuzzy gravitational search algorithm for economic dispatch problems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ameliorated graph sample and aggregate network and
convolutional neural network for stock trading decisions. <em>ASOC</em>,
<em>145</em>, 110626. (<a
href="https://doi.org/10.1016/j.asoc.2023.110626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both graph neural network and convolutional neural network (CNN) perform well in predicting the trading signals of time series data . However, these models often encounter challenges in profitability due to either inadequate information or algorithmic limitations. A model AG-MCNN is proposed by combining ameliorated graph sample and aggregate network (GraphSAGE) and multi-CNN for stock trading decisions. Ameliorated GraphSAGE (AG) is used to learn graph embedding in daily market network constructed from comprehensive market information, with neighbors having higher correlation exerting stronger influence on the target node . Moreover, a more reasonable graph modeling approach is adopted to preserve the important features of time series while filtering out noise. Multi-scale time series and comments with sentiment information are collected to analyze short-term and long-term trends. The short-term features of the weighted graph are extracted by a dual concatenated CNN. The pre-trained model is fine-tuned based on transfer learning to address the issue of insufficient sentiment information in the long-term trend. Experimental results demonstrate that AG-MCNN achieves an average profit rate of 68.4094\%, compared to the highest value of 46.1091\% for comparison models and 18.2234\% for the buy-and-hold strategy. AG-MCNN’s high level of profitability in stock investments makes it a promising choice for investors.},
  archive      = {J_ASOC},
  author       = {Xi Chen and Kaoru Hirota and Yaping Dai and Xiangdong Wu},
  doi          = {10.1016/j.asoc.2023.110626},
  journal      = {Applied Soft Computing},
  pages        = {110626},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ameliorated graph sample and aggregate network and convolutional neural network for stock trading decisions},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolving fuzzy prediction interval for fault detection in a
heat exchanger. <em>ASOC</em>, <em>145</em>, 110625. (<a
href="https://doi.org/10.1016/j.asoc.2023.110625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this work is to design a novel evolving fuzzy prediction interval for modeling a nonlinear system and to implement a novel interval-based algorithm for fault detection. To achieve these goals, three frameworks are studied to provide the basis for the ideas to be proposed. First, the prediction intervals developed in the literature to characterize the uncertainties in modeling systems. Second, the evolving intelligence systems presented in the literature to improve the predictive models through adaptability to temporal changes. Third, the model-based error detection algorithms that detect anomalies in system behavior based on the error of the models. This work proposes a complete recursive design of the evolving fuzzy prediction interval based on interval coverage estimation. This estimation is based on the accumulated number of past time points in which the interval contained the measurements of the modeled system. Additionally, a new interval-based fault detection algorithm is proposed in this work that determines the activation of alarms based on the increment of the proposed interval failure index. This novel index is similar in structure to the interval coverage estimation. The proposed recursively evolving fuzzy prediction interval and interval-based fault detection algorithm were tested on a heat exchanger system whose characteristics were affected by external changes. The experimental results show the effectiveness of the proposed interval model in representing the dynamics of the heat exchanger . Furthermore, the experiment shows that the proposed fault detection algorithm achieves comparable accurate performance to the results of a previous model-based fault detection method.},
  archive      = {J_ASOC},
  author       = {Oscar Cartagena and Miha Ožbot and Doris Sáez and Igor Škrjanc},
  doi          = {10.1016/j.asoc.2023.110625},
  journal      = {Applied Soft Computing},
  pages        = {110625},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving fuzzy prediction interval for fault detection in a heat exchanger},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gene expression programming with dual strategies and
neighborhood search for symbolic regression problems. <em>ASOC</em>,
<em>145</em>, 110616. (<a
href="https://doi.org/10.1016/j.asoc.2023.110616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an evolutionary algorithm for automatic generation of computer programs, Gene expression programming (GEP) has received a lot of attention because it can generate concise and highly accurate solutions. However, GEP also has some unsatisfactory aspects, such as too many control parameters and a lack of balance between exploration and exploitation. Therefore, a new GEP with dual strategies and neighborhood search (DNSGEP) is proposed in this paper for symbolic regression problems . As the core part, a new multi-parent crossover operator with neighborhood search is proposed to replace the original transposition and recombination operators in GEP, which greatly reduces the excessive control parameters in GEP. To assist the multi-parent crossover operator in selecting parents, selection weight is used to assess the quality of individuals in the population. The multi-parent crossover operator combines selection weight to learn more from the better parents. In order to satisfy the different needs of global search and local search of the algorithm, two different crossover strategies are extended on the basis of the multi-parent crossover operator. Furthermore, a population distribution entropy strategy to GEP is proposed for determining when the algorithm should choose which crossover strategy to use. DNSGEP is compared with GEP and three GEP variants on 14 symbolic regression problems and 6 even parity problems. Experimental results show that DNSGEP has the success rate and accuracy that are fully superior to GEP, GEPADF and OGEP, not inferior to SLGEP.},
  archive      = {J_ASOC},
  author       = {Hu Peng and Lin Li and Changrong Mei and Changshou Deng and Xuezhi Yue and Zhijian Wu},
  doi          = {10.1016/j.asoc.2023.110616},
  journal      = {Applied Soft Computing},
  pages        = {110616},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gene expression programming with dual strategies and neighborhood search for symbolic regression problems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy robust flexible programming with me measure for
electric sustainable supply chain. <em>ASOC</em>, <em>145</em>, 110614.
(<a href="https://doi.org/10.1016/j.asoc.2023.110614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity as a source of energy, is one of the most essential part in our life. In recent time, it is noted that the temperature of earth increases in compare to the previous years. It is caused due to burning of the conventional energy sources such as gas, oil, and coal from thermal power plant and other sources, which brings about global warming. Bases on this fact, we propose a multi-objective mixed-integer programming model to design an electric sustainable supply chain network by considering resiliency and responsiveness. In order to formulate the environmental issues, a power-to-gas (P2G) technology is considered as an environment-friendly system. The presence of P2G can reduce the carbon level and it has some major advantages, i.e., it collects carbon dioxide ( CO 2 ) (CO2) from thermal power plant and then converts CO 2 CO2 to hydrogen ( H 2 ) (H2) with the help of water ( H 2 O ) (H2O) and methane ( CH 4 ) (CH4) gas. The drawbacks in addressing the uncertainty of the parameters are overcome by proposing a novel fuzzy robust flexible programming approach based on M e Me measure. Thereafter, a new approach namely, multi-choice conic goal programming with utility function is introduced to solve the proposed multi-objective model. Damodar Valley Corporation (DVC) in India is considered as a case study about electric network that envisages the sustainable issues of an electric supply chain. Finally, experimental results show that one bio-fuel plant, one photovoltaic plant, one air turbine plant, and two mixed-cycle power plants are build at the end of the scheduled period.},
  archive      = {J_ASOC},
  author       = {Binoy Krishna Giri and Sankar Kumar Roy and Muhammet Deveci},
  doi          = {10.1016/j.asoc.2023.110614},
  journal      = {Applied Soft Computing},
  pages        = {110614},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy robust flexible programming with me measure for electric sustainable supply chain},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to drive the participation willingness of supply chain
members in metaverse technology adoption? <em>ASOC</em>, <em>145</em>,
110611. (<a href="https://doi.org/10.1016/j.asoc.2023.110611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaverse integrates a variety of digital technologies, which can significantly reduce production costs, improve product quality and consumer experience, and has become an important direction of digital transformation of manufacturing industry. However, manufacturing enterprises need to bear too high costs and risks to develop metaverse, which hinders the development of metaverse in manufacturing industry. Therefore, on the perspective of supply chain, this study establishes the evolutionary game system in which the manufacturer, the retailer and the government participate in the development of metaverse, so as to analyze the conditions of strategy selection and system equilibrium of all parties, and uses MATLAB R2021b to conduct numerical simulation to analyze the influence of various factors on system equilibrium. Results indicate a positive correlation between the willingness of manufacturer and retailer to participate, and when both sides tend to participate in the development of metaverse, the government will also adopt subsidy strategy; The promotion degree of metaverse to product attributes will positively affect the willingness of all parties to participate; The participation of retailers positively affects the willingness of manufacturer to develop metaverse; The price sensitivity of consumers to product prices is negatively correlated with the willingness of manufacturer and retailer to participate. The incentive effect of government subsidies positively affects the willingness of the three parties to participate. Finally, this paper provides suggestions for all participants on how to strengthen the cooperative relationship so as to give full play to the functions of metaverse.},
  archive      = {J_ASOC},
  author       = {Xiaole Wan and Guixian Zhang and Ye Yuan and Shousheng Chai},
  doi          = {10.1016/j.asoc.2023.110611},
  journal      = {Applied Soft Computing},
  pages        = {110611},
  shortjournal = {Appl. Soft. Comput.},
  title        = {How to drive the participation willingness of supply chain members in metaverse technology adoption?},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An experimental comparison of evolved neural network models
for controlling simulated modular soft robots. <em>ASOC</em>,
<em>145</em>, 110610. (<a
href="https://doi.org/10.1016/j.asoc.2023.110610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voxel-based soft robots (VSRs) are a type of modular robots composed by interconnected soft and deformable blocks, i.e., voxels. Thanks to the softness of their bodies, VSRs may exhibit rich dynamic behaviors. One open question is what type of neural controller is most suitable for a given morphology and sensory apparatus in a given environment. One observation is that artificial neural networks with state may be able to cope with the dynamical nature of VSR bodies and their morphological computation. In this work, we consider four types of controllers, i.e., multilayer perceptrons (MLPs, stateless), recurrent neural networks (RNNs), spiking neural networks (SNNs) without homeostasis, and SNNs with homeostasis. We consider three robot morphologies tested for locomotion, where each morphology is investigated in simulation with three different types and number of sensors. Neural network controllers are optimized with neuroevolution, and the experimental results are compared in terms of effectiveness, efficiency, and generalization ability. In addition, we analyze the resulting behavior of the robots systematically. Our results show that RNNs are typically more effective while MLPs are often the weakest controllers, particularly for robots with few sensors. However, SNNs are more capable in terms of generalization and the mechanism of homeostasis is often beneficial. Finally, we show that RNNs and SNNs with homeostasis produce a more wide variety of behaviors.},
  archive      = {J_ASOC},
  author       = {Giorgia Nadizar and Eric Medvet and Stefano Nichele and Sidney Pontes-Filho},
  doi          = {10.1016/j.asoc.2023.110610},
  journal      = {Applied Soft Computing},
  pages        = {110610},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An experimental comparison of evolved neural network models for controlling simulated modular soft robots},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fourier transform layer: A proof of work in different
training scenarios. <em>ASOC</em>, <em>145</em>, 110607. (<a
href="https://doi.org/10.1016/j.asoc.2023.110607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work two established methods were merged: Fourier Transform and Convolutional Neural Network to classify images in several datasets. Fourier Transform is commonly used in signal processing as well as in image processing . Convolutional Neural Networks are well-suited to image analysis tasks, yet require a lot of processing power and/or time during training process. Fourier Transform Layer is introduced to increase the processing speed without sacrificing accuracy. The motivation is to present and compare an alternative approach to Convolutional Neural Networks, which could reduce the need for GPU training. Models containing only the novel layer were trained to classify images from widely accepted datasets and compared to classification results of simple models containing one convolutional layer . The comparison was performed in terms of test accuracy, Area-Under-Curve and training times. The results showed that, for images of size 128 × 128 and larger, models with the proposed layer reached test accuracy comparable to that reached by convolutional models (accuracy: 96\% and 98\% respectively), with at least 27\% decrease in training time per one epoch on Central Processing Unit .},
  archive      = {J_ASOC},
  author       = {Jakub Zak and Anna Korzynska and Antonina Pater and Lukasz Roszkowiak},
  doi          = {10.1016/j.asoc.2023.110607},
  journal      = {Applied Soft Computing},
  pages        = {110607},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fourier transform layer: A proof of work in different training scenarios},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developing sustainable management strategies in construction
and demolition wastes using a q-rung orthopair probabilistic hesitant
fuzzy set-based decision modelling approach. <em>ASOC</em>,
<em>145</em>, 110606. (<a
href="https://doi.org/10.1016/j.asoc.2023.110606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainable management of construction and demolition wastes (CDWs) has become a pressing global issue in social, environmental and economic contexts, and it involves complex technological, engineering, management and regulatory challenges. Recently, many CDW management strategies have been developed based on the barrier attributes of reuse distribution. However, no strategy can simultaneously address all barrier attributes of reuse distribution. Furthermore, no research has assessed and modelled the identified CDW management strategies to determine optimality. On this basis, the presence of multiple barrier attributes, varying attribute priority and a wide range of data allow for the modelling of CDW management strategies under complex multiple-attribute decision-making (MADM) problems. This study develops the fuzzy-weighted zero inconsistency (FWZIC) and fuzzy decision by opinion score method (FDOSM)-based multiplicative multiple objective optimisation by ratio analysis (MULTIMOORA) with the q-rung orthopair probabilistic hesitant fuzzy set (q-ROPHFS) to address this problem. The developed q-ROPHFS–FWZIC method prioritised and weighted the main and sub-barrier attributes of reuse distribution in CDW management strategies. The developed q-ROPHFS–FDOSM is used to score the CDW management strategies. Then, the MULTIMOORA method is used to model 51 CDW management strategies to determine the optimum one. Results showed that Strategy 46 modelled first in six q values because it had the most essential attributes (i.e. cost, market, value-for-money, experience, infrastructure, management, risk and trust). Strategy 17 and Strategy 20 are the least sustainable strategies because they had only one attribute (i.e. experience). Sensitivity analysis, systematic modelling and comparison analysis are conducted to validate and evaluate the stability and robustness of the proposed methods. The implications of this study would likely benefit various stakeholders involved in the construction industry, including construction companies, architects, engineers, policy-makers and members of the public.},
  archive      = {J_ASOC},
  author       = {Hend Ghailani and A.A. Zaidan and Sarah Qahtan and Hassan A. Alsattar and Mostafa Al-Emran and Muhammet Deveci and Dursun Delen},
  doi          = {10.1016/j.asoc.2023.110606},
  journal      = {Applied Soft Computing},
  pages        = {110606},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing sustainable management strategies in construction and demolition wastes using a q-rung orthopair probabilistic hesitant fuzzy set-based decision modelling approach},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary multitasking for bidirectional adaptive codec:
A case study on vehicle routing problem with time windows.
<em>ASOC</em>, <em>145</em>, 110605. (<a
href="https://doi.org/10.1016/j.asoc.2023.110605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem (VRP), an NP-hard problem of considerable complexity, pertains to the determination of optimal sequences of customer visits in an effort to minimize the total operational cost of vehicles. Conventional approaches to tackling VRP have primarily focused on optimizing each task independently, utilizing mathematical methodologies or evolutionary algorithms . Nevertheless, research exploring the application of the advanced concept of evolutionary multitasking (EMT) to address VRP remains limited. To enrich the body of knowledge in this field, our study presents an innovative EMT algorithm, dubbed EMT-RD. This algorithm employs a bidirectional adaptive codec to facilitate the transfer of information between real and discrete solutions. The EMT-RD algorithm permits the transfer of real and discrete solutions across varying tasks, utilizing an encoding rule that transforms information from real to discrete solutions for each task. Further, a decoding rule from discrete to real solutions generates search experiences, facilitating the transfer of knowledge between real solutions. The mechanism is capped off with an adaptive transfer strategy that instigates transfer operations across different tasks. Each task is tackled using a single-task optimization algorithm that employs an artificial bee colony algorithm . Through comprehensive experimentation, we found that our proposed EMT-RD algorithm outperforms competitors on 168 VRP with time windows multitasking instance pairs and four real-world problem pairs that were developed for this study. These results underscore the efficacy of the adaptive transfer strategy and bidirectional adaptive codec in addressing inter-task correlations when solving the VRP.},
  archive      = {J_ASOC},
  author       = {Yanlin Wu and Yanguang Cai and Chuncheng Fang},
  doi          = {10.1016/j.asoc.2023.110605},
  journal      = {Applied Soft Computing},
  pages        = {110605},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary multitasking for bidirectional adaptive codec: A case study on vehicle routing problem with time windows},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Autonomous target tracking of multi-UAV: A two-stage deep
reinforcement learning approach with expert experience. <em>ASOC</em>,
<em>145</em>, 110604. (<a
href="https://doi.org/10.1016/j.asoc.2023.110604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep reinforcement learning (DRL) has developed rapidly and has been applied to multi-UAV target tracking (MTT) research. However, DRL still faces challenges in data utilization and learning speed. To better solve the above problems, a novel two-stage DRL-based multi-UAV decision-making method is proposed in this paper. Specifically, a sample generator combining artificial potential field with proportional–integral–derivative is used to produce expert experience data. On this basis, a two-stage reinforcement learning training method is introduced. For the first stage, the policy network and critic network are pre-trained using expert data, combined with behavior cloning loss and additional Q-value loss, which reduces ineffective exploration and speeds up learning. For the second RL stage, by calculating the average return of the last recent k k excellent episodes, the excellent experience generated by the agent itself is screened out and used to guide the policy network to choose the actions with high reward, thus improving the efficiency of data utilization. Extensive simulation experiments show that our method not only enables multi-UAV to continuously track the target in obstacle environments but also significantly improves the learning speed and convergence effect.},
  archive      = {J_ASOC},
  author       = {Jiahua Wang and Ping Zhang and Yang Wang},
  doi          = {10.1016/j.asoc.2023.110604},
  journal      = {Applied Soft Computing},
  pages        = {110604},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Autonomous target tracking of multi-UAV: A two-stage deep reinforcement learning approach with expert experience},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-sensor data fusion method based on divergence measure
and probability transformation belief factor. <em>ASOC</em>,
<em>145</em>, 110603. (<a
href="https://doi.org/10.1016/j.asoc.2023.110603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dempster–Shafer evidence theory is widely used in multi-sensor data fusion. However, how to manage the counterintuitive result generated by the highly conflicting evidence remains an open question. To solve the problem, a novel multi-sensor data fusion method is proposed, which analyses the credibility of evidence from both the discrepancy between evidences and the factors of evidence itself. Firstly, a new Belief Kullback–Leibler divergence is put forward, which evaluates the credibility of evidence from the discrepancy between evidences. Secondly, another credibility measure called the Probability Transformation Belief Factor is defined, which assesses the credibility of evidence from the evidence itself. These two credibilities are combined as the comprehensive credibility of evidence. Furthermore, considering the uncertainty of evidence, a new belief entropy based on the cross-information within the evidence is presented, which is applied to quantify the information volume of evidence and to adjust the comprehensive credibility of evidence. The adjusted comprehensive credibility is regarded as the final weight to modify the body of evidence. Finally, the Dempster’s combination rule is applied for fusion. Experiment and applications show that the proposed method is effective and superior.},
  archive      = {J_ASOC},
  author       = {Zhentao Hu and Yujie Su and Wei Hou and Xing Ren},
  doi          = {10.1016/j.asoc.2023.110603},
  journal      = {Applied Soft Computing},
  pages        = {110603},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-sensor data fusion method based on divergence measure and probability transformation belief factor},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Immune deep reinforcement learning-based path planning for
mobile robot in unknown environment. <em>ASOC</em>, <em>145</em>,
110601. (<a href="https://doi.org/10.1016/j.asoc.2023.110601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new deep deterministic policy gradient (DDPG) integrating kinematics analysis and immune optimization (KAI-DDPG) is proposed to address the drawbacks of DDPG in path planning . An orientation angle reward component, linear velocity reward factor, and safety performance reward factor are added to the DDPG reward function based on kinematic modeling and analysis of mobile robots. A multi-objective performance index turns the path planning problem into one of multi-objective optimization. We propose KA-DDPG, which uses the orientation angle, linear speed, and safety degree as evaluation indices, and information entropy to alter the influence coefficient of the multi-objective function in the reward function. KAI-DDPG is proposed to address the low learning and training efficiency of KA-DDPG, using immune optimization to optimize the experience samples in the experience buffer pool. Performance indices of traditional path planning and the proposed techniques are compared on a gazebo simulation platform, and the results suggest that KAI-DDPG can mitigate the drawbacks of DDPG, such as a protracted training cycle and poor path planning technique, and can broaden the range of application.},
  archive      = {J_ASOC},
  author       = {Chengliang Yan and Guangzhu Chen and Yang Li and Fuchun Sun and Yuanyuan Wu},
  doi          = {10.1016/j.asoc.2023.110601},
  journal      = {Applied Soft Computing},
  pages        = {110601},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Immune deep reinforcement learning-based path planning for mobile robot in unknown environment},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-growth learning-based machine scheduler to minimize
setup time and tardiness in OLED display semiconductor manufacturing.
<em>ASOC</em>, <em>145</em>, 110600. (<a
href="https://doi.org/10.1016/j.asoc.2023.110600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on a scheduling problem of the evaporation and encapsulation (EVEN) line in organic light-emitting diode (OLED) display manufacturing towards minimizing total setup time and tardiness of allocated jobs. This scheduling problem is an unrelated dedicated parallel machine scheduling (UDPMS) problem with practical complex constraints including machine availability, family setup, preventive maintenance (PM), and job splitting. There are three major points to this problem. First, the processing of a job on a machine should be completed before its due date. Second, setups should be reduced to minimize labor costs. Finally, a job is able to be split into one or more sub-jobs for processing on machines independently. To obtain a schedule to minimize total setup time and tardiness of allocated jobs, we suggest two novel machine schedulers such as deep learning-based and self-growth learning-based schedulers, called DLS and SGLS, respectively. Although the structure of the proposed schedulers is identical, especially, SGLS incrementally learns to explore more effective allocation patterns by preserving the obtained knowledge from the trained DLS based on self-growth learning. The comprehensive experiments show that SGLS produces satisfactory schedules in terms of minimizing total setup time and tardiness. In particular, when the number of jobs and the ratio of job sizes are larger, SGLS achieves a schedule with both less setup time and tardiness compared to others.},
  archive      = {J_ASOC},
  author       = {Donghun Lee and Dongjin Lee and Kwanho Kim},
  doi          = {10.1016/j.asoc.2023.110600},
  journal      = {Applied Soft Computing},
  pages        = {110600},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-growth learning-based machine scheduler to minimize setup time and tardiness in OLED display semiconductor manufacturing},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three-stage fuzzy-metaheuristic algorithm for smart cities:
Scheduling mobile charging and automatic rule tuning in WRSNs.
<em>ASOC</em>, <em>145</em>, 110599. (<a
href="https://doi.org/10.1016/j.asoc.2023.110599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent breakthroughs in wireless power transfer technology have been promising for empowering and enabling seamless operation of wireless sensors and developing sustainable systems such as wireless rechargeable sensor networks (WRSNs). Relying on this technology and focusing on WRSNs, this paper proposes an adaptive charging scheduling algorithm called the three-stage fuzzy metaheuristic algorithm (TSFM) by using the whale optimization algorithm (WOA) and a multiobjective function designed to meet the application requirements of WRSNs in smart cities. The TSFM algorithm benefits from the WOA for the automated configuration and optimization of the fuzzy rule-base table in a three-stage fuzzy inference system for clustering, routing, and scheduling mobile charging simultaneously. Unlike the other methods, the TSFM algorithm determines the ratio of effect of fuzzy input parameters of every stage based on application and environment in addition to considering effective parameters for every stage, especially for charging scheduling ( e.g. , the energy of a charge-request node, the distance between a charge-request node and a mobile charging (MC), the density of a charge-request node, the average energy consumption rate of a charge-request node, and the time elapsed after a charge request). Furthermore, for the proper distribution of energy in a network, better responsiveness, and optimal use of a charging robot, two energy thresholds are defined in the TSFM algorithm, namely as the charging request issue threshold and the charging operation completion threshold. The TSFM algorithm is adaptable to applications, environments, and requirements of WRSNs not only by using the features of nodes (as the fuzzy input parameters) but also by considering the application-specific features, environments, and infrastructure ( e.g. , network size, number of nodes, base station (BS) positions, and designer’s goals) within the process of optimizing tunable parameters. The TSFM algorithm was evaluated and compared with some common methods such as FCFS , NJNP, and ESS in terms of functional metrics such as energy utility, charging latency, survival rate, death rate of nodes, system stability, number of packets received by BS, number of request nodes, number of alive nodes, average residual energy , adaptability analysis, and statistical validation in two scenarios and six applications. According to the simulation results, the proposed TSFM algorithm improved the performance criteria and outperformed the other methods by far in terms of application. For instance, the TSFM algorithm improved the minimum system stability rate by 167\%, 0.73\%, and 123\% compared with FCFS, NJNP, and ESS methods, respectively.},
  archive      = {J_ASOC},
  author       = {Fakhrosadat Fanian and Marjan Kuchaki Rafsanjani},
  doi          = {10.1016/j.asoc.2023.110599},
  journal      = {Applied Soft Computing},
  pages        = {110599},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-stage fuzzy-metaheuristic algorithm for smart cities: Scheduling mobile charging and automatic rule tuning in WRSNs},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NLBBODE optimizer for accurate and fast modeling of
photovoltaic module/string generator and its application to solve
real-world constrained optimization problems. <em>ASOC</em>,
<em>145</em>, 110597. (<a
href="https://doi.org/10.1016/j.asoc.2023.110597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new optimizer is presented to quickly and accurately identify parameters of the photovoltaic (PV) module/string models. This optimizer is named Nested Loop Biogeography-based Optimization - Differential Evolution referred to as (NLBBODE). It has been developed to identify the PV parameters with reasonable computational effort and minimum execution time, despite the nonlinearity of the PV system dynamics and the insufficiency of data. In addition, the NLBBODE optimizer is used to solve some engineering design problems known as highly constrained, nonlinear, and non-convex. The weaknesses of the original versions of BBO and DE approaches have been overpassed. Furthermore, the proposed optimizer is compared to the state-of-the-art metaheuristic methods using performance evaluation metrics . The computational resources needed to obtain the optimum solution using NLBBODE are significantly reduced due to the nested loop design. The results obtained prove that the NLBBODE optimizer is a suitable candidate to solve the problem of the PV modeling as well as to solve various real-world constrained optimization problems , with high accuracy and low processor runtime, which is a necessary condition for online applications. For instance, the well-known Photowatt-PWP-201 module model represented in the single diode model , NLBBODE registers a standard deviation value of 1.4682E-17 within a time of 13.9 s. For the STM6-40/36 module model represented in the single diode model , the NLBBODE optimizer records a standard deviation value of 6.191583E-18 within an execution time of 6 s. For the speed reducer design problem, the standard deviation obtained by the NLBBODE is 1.515824E-13 and needs a time of less than 1 s to obtain the optimum solution.},
  archive      = {J_ASOC},
  author       = {Belkacem Aoufi and Oussama Hachana and Mohamed Amine Sid and Giuseppe Marco Tina},
  doi          = {10.1016/j.asoc.2023.110597},
  journal      = {Applied Soft Computing},
  pages        = {110597},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NLBBODE optimizer for accurate and fast modeling of photovoltaic module/string generator and its application to solve real-world constrained optimization problems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolution strategies-based optimized graph reinforcement
learning for solving dynamic job shop scheduling problem. <em>ASOC</em>,
<em>145</em>, 110596. (<a
href="https://doi.org/10.1016/j.asoc.2023.110596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job shop scheduling problem (JSSP) with dynamic events and uncertainty is a strongly NP-hard combinatorial optimization problem (COP) with extensive applications in the manufacturing system . Recently, growing interest has been aroused in utilizing machine learning techniques to solve the JSSP. However, most prior arts cannot handle dynamic events and barely consider uncertainties. To close this gap, this paper proposes a framework to solve a dynamic JSSP (DJSP) with machine breakdown and stochastic processing time based on Graph Neural Network (GNN) and deep reinforcement learning (DRL). To this end, we first formulate the DJSP as a Markov Decision Process (MDP), where disjunctive graph represent the states. Secondly, we propose a GNN-based model to effectively extract the embeddings of the state by considering the features of the dynamic events and the stochasticity of the problem, e.g., the machine breakdown and stochastic processing time. Then, the model constructs solutions by dispatching optimal operations to machines based on the learned embeddings. Notably, we propose to use the evolution strategies (ES) to find optimal policies that are more stable and robust than conventional DRL algorithms. The extensive experiments show that our method substantially outperforms existing reinforcement learning-based and traditional methods on multiple classic benchmarks.},
  archive      = {J_ASOC},
  author       = {Chupeng Su and Cong Zhang and Dan Xia and Baoan Han and Chuang Wang and Gang Chen and Longhan Xie},
  doi          = {10.1016/j.asoc.2023.110596},
  journal      = {Applied Soft Computing},
  pages        = {110596},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolution strategies-based optimized graph reinforcement learning for solving dynamic job shop scheduling problem},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge graph–GCN–community detection integrated model
for large-scale stock price prediction. <em>ASOC</em>, <em>145</em>,
110595. (<a href="https://doi.org/10.1016/j.asoc.2023.110595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to uncertainty in the stock market, stock price prediction has always been a challenging research hotspot. In recent years, many stock prediction methods have used stock price series and technical indicators as inputs and the time series algorithm to predict, but they often ignore the influence of deeper factors such as the situation of the stock company and current situation of the stock industry. In addition, most of them predict based on small-scale stock datasets with limited characteristics and have certain defects such as bias, poor stability of prediction results, and lack of statistical significance tests on experimental results. To solve these problems, we propose a new method for stock price prediction based on knowledge graph (KG) and graph convolution neural network (GCN) models. First, stock KG is constructed, and the semantic relationships between stocks are described in the form of triples. Second, the correlations between stocks are quantified by fully utilizing their explicit/implicit relationships in the KG. Third, K-means, community detection (CD), and GCNs are merged to obtain accurate clustering results for similar stocks. Finally, the historical prices of similar stocks are used as the input characteristics of the time series models to predict stock price trends. We collect 4684 A-share market stocks in China from 2013 to 2019 and predicted the stock price trends for 762 of them. The experimental results and significance test show that the proposed method achieve the best accuracy, precision, and F1-measure on large-scale stock datasets and have the best stability, proving that the overall prediction effect outperforms that by state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Ting Wang and Jiale Guo and Yuehui Shan and Yueyao Zhang and Bo Peng and Zhuang Wu},
  doi          = {10.1016/j.asoc.2023.110595},
  journal      = {Applied Soft Computing},
  pages        = {110595},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knowledge graph–GCN–community detection integrated model for large-scale stock price prediction},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A matheuristic for flowshop scheduling with batch processing
machines in textile manufacturing. <em>ASOC</em>, <em>145</em>, 110594.
(<a href="https://doi.org/10.1016/j.asoc.2023.110594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by a real-world textile packing process, this paper investigates a novel hybrid flowshop scheduling problem with batch processing machines to minimize the makespan. Geometry sizes of textile products , non-identical capacities of packing machines, and the capacity of the needle detector are addressed in parallel-batching modes. To make batching and sequencing decisions simultaneously, a mixed integer linear programming model (MILP) is formulated. Then, a matheuristic method, Tabu Search with Fixing and Optimization (TSFO), is developed. Using tabu search to determine which variables to be fixed, the MILP is solved through a fix-and-optimize process. The results of extensive computational experiments show that the TSFO yields better solutions within much less computation time than solving the model directly in a commercial solver. Furthermore, the TSFO also outperforms an existing method in a special case from literature.},
  archive      = {J_ASOC},
  author       = {Shijin Wang and Hanyu Zhang},
  doi          = {10.1016/j.asoc.2023.110594},
  journal      = {Applied Soft Computing},
  pages        = {110594},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A matheuristic for flowshop scheduling with batch processing machines in textile manufacturing},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using radial basis function and back propagation to
predicate fault in a railway dangerous goods transportation system
considering the markov correction. <em>ASOC</em>, <em>145</em>, 110593.
(<a href="https://doi.org/10.1016/j.asoc.2023.110593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, we can obtain a huge amount of information and data of railway dangerous goods transportation system through various technical documents, accident reports, monitoring technologies etc. For the space composed of massive historical information and data, a data-driven fault predication approach that can better grasp some important local information while ignoring the negative impact of other data in the global might be practical. However, we have the following challenges: (i) How to reduce the impact of unimportant information on prediction results in advance? (ii) What kind of local approximation data-driven approach should be applied, and (iii) How to correct the results and improve the reliability and accuracy after fault predication based on the local approximation data-driven approach? In this paper, two data-driven approaches including Back Propagation with Markov Correction and Radial Basis Function with Markov Correction are proposed to predicate the fault in a railway dangerous goods transportation system. In order to solve challenge (i), we decompose the whole transportation process into multiple sub-processes based on Work Breakdown Structure with clear duration boundary on the time axis, and use Risk Breakdown Structure to detect the possible faults in each sub-process. In order to solve challenge (ii), we use and compare Back Propagation/Radial Basis Function with global approximation/local approximation and nonlinear prediction ability to learn and predicate the fault based on the collected historical data. In order to solve challenge (iii), we correct the prediction results with fluctuating deviation by using Markov Correction due to its non-aftereffect. Finally, a case study is conducted based on the collected historical fault data of railway dangerous goods transportation system in China. The results show that, for the prediction results based on Back Propagation , the average error increases from 1.43 to 1.88 and the Mean Square Error increases from 3.27 to 5.60 after Markov Correction. For the prediction results based on Radial Basis Function , the average error decreases from 1.13 to 1.11 and the Mean Square Error decreases from 1.81 to 1.76 after Markov Correction. The non-aftereffect of Markov chain well corresponds to the local approximation of Radial Basis Function. When the prediction value is determined by some data of the series, Radial Basis Function with Markov Correction can be applied. When the prediction value is determined by all the data (small-scale) of the series, Back Propagation with Markov Correction is better.},
  archive      = {J_ASOC},
  author       = {Wencheng Huang and Luohao Sun and Zhenlong Yang and Yanhui Yin},
  doi          = {10.1016/j.asoc.2023.110593},
  journal      = {Applied Soft Computing},
  pages        = {110593},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using radial basis function and back propagation to predicate fault in a railway dangerous goods transportation system considering the markov correction},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mass movement susceptibility prediction and infrastructural
risk assessment (IRA) using GIS-based meta classification algorithms.
<em>ASOC</em>, <em>145</em>, 110591. (<a
href="https://doi.org/10.1016/j.asoc.2023.110591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mountainous areas, mass movements are among the most dangerous natural hazards. Infrastructure is a crucial component and is thought of as human wealth. This infrastructure is frequently impacted by mass movements, whose frequency and size are anticipated to rise in the future due to the unequal distribution of rainfall events brought on by climate change. To deal with the anticipated repercussions, the study area, the Northern part of Morocco, needs to implement new management and maintenance practices. Thus, the main motivation of this study was to examine the mass movement vulnerability and assess risk on infrastructures in two provinces of North Morocco, i.e. Chefchaouen and Tetouan. The present study employed Reduced Error Pruning Tree (REPTree) and its ensemble with Bagging, AdaBoost , and Random SubSpace (i.e. REPTree Bagging Bagging , REPTree AdaBoost AdaBoost , and REPTree RandomSubSpace RandomSubSpace ) for mass movement susceptibility mapping (MMSM) based on a comprehensive dataset of 100 mass movements locations which include debris flow, landslide, and rock fall during past 20 years (2000–2020) as well as 12 MM conditioning factors. The result revealed that REPTree RandomSubSpace RandomSubSpace is the most viable model for MMSM with AUC = 0.8656. In addition, REPTree Bagging Bagging , REPTree AdaBoost AdaBoost , and REPTree models also offer acceptable results with AUC of 0.8338, 0.8269, and 0.7942, respectively. After MMSM, the infrastructural risk was assessed and the result showed that among the six infrastructural features considered, buildings and forests have a greater risk of mass movement in the study area. Most of the mass movement and infrastructural risk-prone areas are found in the central and north-eastern parts of the study area. The results further revealed that elevation, land use, lithology, rainfall, and distance from roads are important variables for mass movement and infrastructure risk assessment (IRA). The results of this study offer a systematic sight for decision-makers to mitigate natural disasters and infrastructural risk in the study region.},
  archive      = {J_ASOC},
  author       = {Sk Ajim Ali and Meriame Mohajane and Farhana Parvin and Antonietta Varasano and Sliman Hitouri and Ewa Łupikasza and Quoc Bao Pham},
  doi          = {10.1016/j.asoc.2023.110591},
  journal      = {Applied Soft Computing},
  pages        = {110591},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mass movement susceptibility prediction and infrastructural risk assessment (IRA) using GIS-based meta classification algorithms},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient evolutionary algorithm for high-speed train
rescheduling under a partial station blockage. <em>ASOC</em>,
<em>145</em>, 110590. (<a
href="https://doi.org/10.1016/j.asoc.2023.110590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the high-speed train rescheduling (HSTR) problem under a partial station blockage and proposes an efficient problem-specific strengthen elitist genetic algorithm (PS-SEGA) for HSTR. Firstly, a HSTR model subject to train operation constraints is established to minimize the total train delay. A permutation-based encoding method is developed to define an efficient search space based on the train departure sequence. A heuristic decoding method is employed to eliminate all train operation constraints and output the rescheduled timetable. Moreover, a hybrid initialization method involving an efficient heuristic strategy (EHS) is put forward to accelerate the convergence speed of PS-SEGA. Using problem-specific knowledge, EHS generates an efficient and feasible solution for the initial population. Finally, a restart strategy is presented to maintain genetic diversity. Compared with other advanced evolutionary algorithms and their improved variants also using the improvements of PS-SEGA, experimental results demonstrate the effectiveness of the proposed PS-SEGA for addressing HSTR scenarios under the partial station blockage. As for the scenarios that CPLEX cannot obtain optimal solutions within 10 min, PS-SEGA can provide quasi-optimal solutions in real time. Furthermore, compared with the other two heuristics algorithms (i.e., First-Scheduled-First-Served and EHS), PS-SEGA can give the train departure sequence with a smaller total train delay.},
  archive      = {J_ASOC},
  author       = {Rongsheng Wang and Qi Zhang and Xuewu Dai and Zhiming Yuan and Tao Zhang and Shuxin Ding and Yaochu Jin},
  doi          = {10.1016/j.asoc.2023.110590},
  journal      = {Applied Soft Computing},
  pages        = {110590},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient evolutionary algorithm for high-speed train rescheduling under a partial station blockage},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Minimum spanning tree niching-based differential evolution
with knowledge-driven update strategy for multimodal optimization
problems. <em>ASOC</em>, <em>145</em>, 110589. (<a
href="https://doi.org/10.1016/j.asoc.2023.110589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal optimization problems (MMOPs) aim to simultaneously locate as many global optima as possible with high accuracy. Recently, many niching strategies have been wildly used in evolutionary algorithms to solve MMOPs due to the advantage of maintaining the diversity of population. However, most multimodal algorithms are sensitive to the niching parameters and lack effective methods to update the “stagnant individuals”, including individuals that have trapped into local optima or converged to the same optima. In this paper, a minimum spanning tree niching-based differential evolution (TNDE) with knowledge-driven update (KDU) strategy is proposed to better solve the above challenges, where the “knowledge” includes historical evolutionary information, fitness distribution information, and individual distribution information. In TNDE, a minimum spanning tree niching (MSTN) strategy is proposed to adaptively divide the population, which can adjust the number of niches dynamically. Besides, the KDU strategy is proposed to utilize the knowledge to locate and update stagnant individuals. Lastly, an improved differential evolution with local stage-based mutation (LSM) and directional guidance selection (DGS) strategies is proposed to accelerate convergence and refine the accuracy of solutions, respectively. The comparison results with 16 state-of-the-art algorithms on CEC’2013 show that TNDE achieves significant advantages on high-dimensional problems or problems with many global optima.},
  archive      = {J_ASOC},
  author       = {Xiangqian Li and Hong Zhao and Jing Liu},
  doi          = {10.1016/j.asoc.2023.110589},
  journal      = {Applied Soft Computing},
  pages        = {110589},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimum spanning tree niching-based differential evolution with knowledge-driven update strategy for multimodal optimization problems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using GAN-generated market simulations to guide genetic
algorithms in index tracking optimization. <em>ASOC</em>, <em>145</em>,
110587. (<a href="https://doi.org/10.1016/j.asoc.2023.110587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Index tracking is the problem of building a portfolio that replicates the performance of a market index. The recent applications involving deep learning in index tracking are more focused on the learned information rather than on the framework that consumes this information. The problem is that, until now, a way to enable the extension of the index tracking models that adopt machine learning was not yet proposed. Nowadays, the mathematical programming framework is more flexible when considering model extensions to build realistic portfolios. Thus, this study presents ways to combine generative adversarial networks (GANs) within this framework to generate market simulations incorporated into a base index tracking model. It was verified how the simulations generated by GANs can impact the out-of-sample performance of the portfolio and how to deal with their instability, by using real data from the Brazilian market. To achieve this, two evolutionary metaheuristics were proposed to solve the multiple scenario index tracking problem. The proposed evolutionary algorithms minimize the tracking errors of a portfolio in multiple market simulations generated by GANs. The performance of the proposed algorithms was compared against another evolutionary metaheuristic that solves the index tracking problem using historical data. It was possible to observe that the scenario-based dominance genetic algorithm (SDM-SBDGA-GAN) was able to perform better than the real data genetic algorithm (RDM-GA) and the sample average approximation genetic algorithm (SDM-SAAGA-GAN). These results open doors for new applications of synthetic data in the construction of portfolios using more realistic constraints and other tracking objectives. This work also brings discussions about problems related to the application of GANs in this context.},
  archive      = {J_ASOC},
  author       = {Julio Cezar Soares Silva and Adiel Teixeira de Almeida Filho},
  doi          = {10.1016/j.asoc.2023.110587},
  journal      = {Applied Soft Computing},
  pages        = {110587},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using GAN-generated market simulations to guide genetic algorithms in index tracking optimization},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Criteria system construction for sustainable supplier
selection: A product-category-oriented intelligent model. <em>ASOC</em>,
<em>145</em>, 110586. (<a
href="https://doi.org/10.1016/j.asoc.2023.110586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, awareness of environmental protection and social responsibility has increased, making sustainable supply chain development and the selection of sustainable suppliers more important. The construction of a criteria system is the premise of sustainable supplier selection (SSS), which determines the final decision. Recognizing the limitations of existing research works, including subjectively determined evaluation criteria based only on the literature and personal experience, and neglecting the characteristics of different product categories and the combination effect of different criteria, this research designs a product-category-oriented intelligent SSS criteria system construction model by applying the random forest algorithm and the recursive feature elimination cross-validation method. The novelties and contributions of this paper include, first, a systematic SSS criteria system construction model from a new perspective of product category is designed to help buyers construct a comprehensive, reliable, and efficient SSS criteria system. Second, the appropriate integration of human experience and machine learning fully exploits the advantages of both subjective judgement and objective analysis. Third, the ideas and methods of group decision making are introduced for the first time into the SSS criteria system construction process , and the relative importance of different decision-makers are taken into account. Finally, the proposed model can construct the optimal SSS criteria system while considering the combination effect of the whole criteria system. The model was validated in a Chinese forklift company, helping the company to construct a criteria system containing nine criteria with 86.68\% accuracy rate. The results of comparative and sensitivity analyses show that the accuracy of the proposed model can achieve an 8.11\% improvement compared with other existing methods.},
  archive      = {J_ASOC},
  author       = {Chong Wu and Yiqun Jia and David Barnes},
  doi          = {10.1016/j.asoc.2023.110586},
  journal      = {Applied Soft Computing},
  pages        = {110586},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Criteria system construction for sustainable supplier selection: A product-category-oriented intelligent model},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). An effective multi-objective bald eagle search algorithm
for solving engineering design problems. <em>ASOC</em>, <em>145</em>,
110585. (<a href="https://doi.org/10.1016/j.asoc.2023.110585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multi-objective bald eagle search algorithm (MOBES) is proposed. The MOBES introduces an archive mechanism to store the non-dominated solutions obtained by the algorithm. When the archive overflows, remove the most crowded solutions by using the roulette method. The MOBES also adds elite selection strategy to guide other individuals to optimize by selecting elite individuals in the population. The efficiency of MOBES is validated on CEC 2020 benchmark functions , and the results demonstrate that the proposed algorithm is more efficient than its competitors in terms of convergence, diversity and distribution of solutions. The MOBES is also applied to two-objective, tri-objective and four-objective engineering design problems in real world. The results show its superiority in handling challenging multi-objective optimization problems with unknown true Pareto optimal solutions and fronts, and it is more competitive than other algorithms.},
  archive      = {J_ASOC},
  author       = {Yunhui Zhang and Yongquan Zhou and Guo Zhou and Qifang Luo},
  doi          = {10.1016/j.asoc.2023.110585},
  journal      = {Applied Soft Computing},
  pages        = {110585},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective multi-objective bald eagle search algorithm for solving engineering design problems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interval type-2 fuzzy set induced fuzzy rank-level fusion
for face recognition. <em>ASOC</em>, <em>145</em>, 110584. (<a
href="https://doi.org/10.1016/j.asoc.2023.110584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Type-2 fuzzy set is extensively studied in the past due to its superiority over type-1 fuzzy set, especially while dealing with data with higher uncertainty and higher association amongst them. This paper proposes a framework by introducing interval type-2 fuzzy set induced fuzzy rank-level fusion for face recognition utilizing multi-feature vectors. It utilizes the outputs of a classifier as confidence factors . We address the wide intra-class variability issue by introducing interval type-2 fuzzy sets by using these confidence factors to generate secondary membership values from the intra-class face images, which are then reduced to a primary membership value. It also mitigates the influence of inter-class similarity by excluding the inter-class face images in the computation. Furthermore, multi-feature vectors for a face image are used to utilize the underlying different discriminant features, which also in turn address the issues of inter-class similarity. For each feature vector, we generate interval type-2 fuzzy set based fuzzy ranks corresponding to all classes. To reduce the complexity, top k fuzzy ranks are fused with corresponding membership values to obtain the fuzzy ranks. Likewise, the interval type-2 fuzzy set based multiple fuzzy ranks are obtained using multiple feature vectors. Finally, these fuzzy ranks are fused with the corresponding complemented confidence factors to get the final fuzzy ranks, based on which a face image is classified and recognized. The method is evaluated on several face databases and found to be superior to the many state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Manas Ghosh and Jamuna Kanta Sing},
  doi          = {10.1016/j.asoc.2023.110584},
  journal      = {Applied Soft Computing},
  pages        = {110584},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval type-2 fuzzy set induced fuzzy rank-level fusion for face recognition},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Variational eligibility trace meta-reinforcement recurrent
network for residual life prediction of space rolling bearings.
<em>ASOC</em>, <em>145</em>, 110582. (<a
href="https://doi.org/10.1016/j.asoc.2023.110582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional sequence recurrent neural networks (SRNNs) have the defect of long time dependence in the prediction of time series, resulting in their poor generalization ability . Moreover, it is required to traverse the whole training data set to realize supervised learning by SRNNs, which increases the time complexity and leads to their low prediction accuracy and high computation cost in the residual life prediction of space rolling bearings in the ground simulated space environment. In view of this, a novel SRNN named variational eligibility trace meta-reinforcement recurrent network (VETMRRN) is proposed for achieving higher residual life prediction accuracy and lower computation cost. In the proposed VETMRRN, a new sequence recurrent network structure is constructed to increase the memory amount of historical information, thus improving the long-term memory capacity of VETMRRN. Then, a hyperparameter self-initialization meta-learning network with an oracle gate mechanism is designed to self-initialize the hyperparameters of VETMRRN for fast determination of the optimal review sequence length. Hence, VETMRRN can adapt to different input sequence lengths and avoid the defect of long time dependence of traditional SRNNs. Furthermore, a variational auto-encoding meta policy gradient learning algorithm with an eligibility trace operator is designed to improve the training speed and enhance the global optimization effect for VETMRRN parameters. Based on the above advantages of VETMRRN, a new residual life prediction method of space rolling bearings in the ground simulated space environment is proposed. Firstly, the time-frequency fusion features are extracted by Shapely-value feature fusion from the vibration acceleration data of space rolling bearing as the performance degradation features. Then, the performance degradation features are input into VETMRRN to predict the performance degradation feature trends of space rolling bearings. Finally, a Weibull-distribution reliability model is established based on the performance degradation feature trend values to predict the residual life of space rolling bearings. The effectiveness of the proposed VETMRRN-based prediction method is verified by the vibration acceleration data collected from the self-built vibration monitoring platform of space rolling bearings in the ground simulated space environment. The results indicate that compared to traditional SRNNs, deep sparse auto-encoding neural network (DSAE-NN), and multi-kernel least-square support vector machine (MK-LSSVM), the proposed method can improve the prediction accuracy and reduce the computation cost in the residual life prediction of space rolling bearings. In the future, the generalization performance of VETMRRN still needs to be further improved.},
  archive      = {J_ASOC},
  author       = {Feng Li and Peixuan Jiang and Ling Luo and Baoping Tang and Yongchao Wang},
  doi          = {10.1016/j.asoc.2023.110582},
  journal      = {Applied Soft Computing},
  pages        = {110582},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Variational eligibility trace meta-reinforcement recurrent network for residual life prediction of space rolling bearings},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A direction vector-guided multi-objective evolutionary
algorithm for variable linkages problems. <em>ASOC</em>, <em>145</em>,
110581. (<a href="https://doi.org/10.1016/j.asoc.2023.110581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recombination is a key component for the evolutionary algorithm to provide promising offspring solutions. However, conventional recombination operators cannot generate high-quality solutions for variable linkages problems due to the particularity of the Pareto optimal set (PS). To tackle this problem, a two-stage multi-objective evolutionary algorithm based on direction vector guidance (DSMOEA) is proposed in this paper. Firstly, a portion of the population is transformed by the eigenmatrix of the covariance matrix to increase the probability of generating high-quality offspring. Then the representative solutions are selected in the transformed population to create the direction vectors. Under the guidance of the direction vectors, the population rapidly approaches PS and generates promising offspring solutions. Finally, Differential Evolution (DE) is performed for searching globally to increase the diversity of the population. The proposed algorithm is tested on three classes of variable linkages problems with 30, 50, and 100 dimensions to verify its performance. The results show that the algorithm is promising for variable linkages problems.},
  archive      = {J_ASOC},
  author       = {Qinghua Gu and Shaopeng Zhang and Qian Wang and Neal N. Xiong},
  doi          = {10.1016/j.asoc.2023.110581},
  journal      = {Applied Soft Computing},
  pages        = {110581},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A direction vector-guided multi-objective evolutionary algorithm for variable linkages problems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction using multi-objective slime mould algorithm
optimized support vector regression model. <em>ASOC</em>, <em>145</em>,
110580. (<a href="https://doi.org/10.1016/j.asoc.2023.110580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from the traditional neural network models based on empirical risk minimization principle, support vector regression (SVR) minimizes the upper limit of generalization error by the structural risk minimization principle. SVR is suitable for solving nonlinear, small-sample, high-dimensional modelling, and pattern recognition problems. However, the selection of hyperparameters of SVR can significantly affect the prediction accuracy and computational time. This paper proposes a method to optimize the SVR hyperparameters through an intelligent optimization algorithm , specifically multi-objective slime mould algorithm (MOSMA). MOSMA is a new multi-objective optimization algorithm with strong global search ability and fast convergence. The principle and calculation process of MOSMA are first described in detail. The analytical method for transforming the SVR hyperparameter optimization problem into a multi-objective problem is then presented. The process of optimizing two SVR hyperparameters (the penalty coefficient and radial basis function kernel parameter) based on MOSMA is described in steps. Meanwhile, the procedures of algorithm parameter setting, fitness function design, and population update are given. To investigate the characteristics and performance of MOSMA-SVR, two datasets are used to predict the vibration trend of the spindle of CNC milling machine in one and the maximum bending normal stress in the cutting process in the other. The performance of MOSMA-SVR is evaluated by multiple statistical indexes and compared with seven other prediction models (GA-SVR, PSO-SVR, ABC-SVR, GWO-SVR, SSA-SVR, SMA-SVR and MOWOA-SVR).},
  archive      = {J_ASOC},
  author       = {Chong Peng and Zhongyuan Che and T.W. Liao and Zhongwen Zhang},
  doi          = {10.1016/j.asoc.2023.110580},
  journal      = {Applied Soft Computing},
  pages        = {110580},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction using multi-objective slime mould algorithm optimized support vector regression model},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous intuitionistic fuzzy sets (CINFUS) and their
AHP&amp;TOPSIS extension: Research proposals evaluation for grant
funding. <em>ASOC</em>, <em>145</em>, 110579. (<a
href="https://doi.org/10.1016/j.asoc.2023.110579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitionistic fuzzy sets are the most widely used fuzzy set extension in the literature. It is a fuzzy set extension in which decision makers specify the degree of membership of the elements in the set as well as their non-membership to the set. Thus, the indecision of the decision makers about the membership of the elements to the set also emerges spontaneously. It is known that discrete intuitionistic fuzzy sets or linear continuous intuitionistic fuzzy sets are used in the literature. In this study, it is aimed to develop non-linear continuous intuitionistic​ fuzzy sets and to use them in multi-criteria decision making models. CINFUSs consisting of membership and non-membership degrees represented by second-order non-linear functions, have been used to develop the analytical hierarchy process (AHP) and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) methodology in a fuzzy environment. This study is a milestone in showing how higher order non-linear functions can be used in intuitionistic fuzzy sets. The CINFUS-AHP&amp;TOPSIS methodology has been applied to the solution of the multi-criteria research proposals evaluation for grand funding problem and has been tested for validity and robustness with sensitivity analysis and comparative analysis.},
  archive      = {J_ASOC},
  author       = {Nurşah Alkan and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2023.110579},
  journal      = {Applied Soft Computing},
  pages        = {110579},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Continuous intuitionistic fuzzy sets (CINFUS) and their AHP&amp;TOPSIS extension: Research proposals evaluation for grant funding},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Steel surface defect detection based on self-supervised
contrastive representation learning with matching metric. <em>ASOC</em>,
<em>145</em>, 110578. (<a
href="https://doi.org/10.1016/j.asoc.2023.110578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect detection is crucial in the quality control of industrial applications. Existing supervised methods are heavily reliant on the large amounts of labeled data. However, labeled data in some specific fields are still scarce, and it requires professionals to do expensive manual annotations. In this paper, we construct a novel self-supervised steel surface defect detection model by learning better embedding feature representation of the defect on large amounts of unlabeled data , which can achieve excellent results in downstream detection tasks. Commonly used image embeddings strategies in self-supervised contrastive learning methods destroy the spatial structures of the image and are not suitable for pre-training of object detection. To address the aforementioned issue, we preserve convolutional feature maps to mine robust data structures and local features , which can enhance the representation capability of the upstream model and make it applicable for transfer to object detection tasks. Besides, in order to eliminate the effect of random augmentations of contrastive learning , which can introduce noise on multi-target coexistence datasets, the Earth Mover’s Distance (EMD) metric is employed to evaluate the contrastive matching similarity. Finally, a Self-supervised Contrastive Representation Learning framework with EMD (SCRL-EMD) is constructed through learning on large-scale unlabeled data and then transferred to Faster R-CNN and RetinaNet for detection performance validation on two public steel defect datasets. Comparative experimental results show that our method can achieve superior results than the state-of-the-art approaches. Compared to the baseline model , it achieves 4.1\% and 6.8\% mAP improvement on the two datasets, respectively. More importantly, a further improvement can be achieved on a smaller downstream dataset, revealing the meaningful potential of our method in exploiting more readily available unlabeled data.},
  archive      = {J_ASOC},
  author       = {Xuejin Hu and Jing Yang and Fengling Jiang and Amir Hussain and Kia Dashtipour and Mandar Gogate},
  doi          = {10.1016/j.asoc.2023.110578},
  journal      = {Applied Soft Computing},
  pages        = {110578},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Steel surface defect detection based on self-supervised contrastive representation learning with matching metric},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recursive surrogate model based on generalized regression
neural network. <em>ASOC</em>, <em>145</em>, 110576. (<a
href="https://doi.org/10.1016/j.asoc.2023.110576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate models have attracted considerable interest as approximation tools that can save considerable computational resources in various applications. In this study, a recursive surrogate model was developed based on a generalized regression neural network and variable-fidelity surrogate method. The proposed model can continuously improve its prediction accuracy using a novel recursive correction method. Ultimately, a model with sufficient predictive accuracy can be obtained. To verify the performance of the proposed model, we conducted a series of comparative experiments using test functions and an engineering problem. The results showed that the proposed model has better predictive accuracy and robustness than the other benchmark models . Additionally, the impacts of the stopping criteria and spread factor on the performance of the proposed model were investigated, and the time cost associated with the modeling process was analyzed. This model presents a novel option for engineering design optimization. The recursive correction method also provides a new approach for other regression models to further improve their prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Shuai Zhang and Kunpeng Li and Shuo Wang and Jianji Li and Yong Pang and Xueguan Song},
  doi          = {10.1016/j.asoc.2023.110576},
  journal      = {Applied Soft Computing},
  pages        = {110576},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recursive surrogate model based on generalized regression neural network},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast 3D-graph convolutional networks for skeleton-based
action recognition. <em>ASOC</em>, <em>145</em>, 110575. (<a
href="https://doi.org/10.1016/j.asoc.2023.110575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on human action recognition based on skeletons has received much attention. But most of the research focuses on improving the model’s generalization ability , while ignoring significant efficiency issues. This leads to developing heavy models with poor scalability and cost-effectiveness in practical use. This paper, we investigate the under-studied but practically critical recognition model efficiency problem. To this end, we present a new Fast Recognition Distillation (FRD) model learning strategy. Specifically, FRD trains a lightweight recognition neural network structure that can be quickly executed at a low computational cost. It can be achieved by effectively disseminating the identification probability information of the teacher network to the lightweight network. We call the probability information of the teacher network as soft-target, and FRD can learn more potential information from soft-target. In addition, we also used a particular loss function for soft-target. Through the FRD network, while basically maintaining the recognition accuracy, we minimized the network structure. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that our model (FRD) is more lightweight and refined than others. Therefore, our model FRD is efficient.},
  archive      = {J_ASOC},
  author       = {Guohao Zhang and Shuhuan Wen and Jiaqi Li and Haijun Che},
  doi          = {10.1016/j.asoc.2023.110575},
  journal      = {Applied Soft Computing},
  pages        = {110575},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fast 3D-graph convolutional networks for skeleton-based action recognition},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of long short-term memory neural networks for
electric arc furnace modeling. <em>ASOC</em>, <em>145</em>, 110574. (<a
href="https://doi.org/10.1016/j.asoc.2023.110574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world steel industry is highly dependent on the use of electric arc furnaces (EAFs). The application of the electric arc phenomenon causes many power quality (PQ) problems, such as harmonics or voltage flickering. An adequate EAF model is useful for the design and control of EAFs and PQ improvement systems. In this paper, we propose an approach to EAF modeling based on a deterministic differential equation that is enhanced with stochastic ingredients. The identification of time series that represent equation coefficients is carried out using a genetic algorithm. The final solution includes two models of the electric arc furnace, both based on long short-term memory (LSTM) networks. They recreate the time series of the coefficients with given stochastic properties. The first model uses LSTM to generate the main component of the output signal, while the second applies LSTM to include the high frequency component. The potential of LSTM models to reflect different stages of the EAF work cycle, that is, the melting and refining stages, has been investigated. The results indicate that the LTSM model outperforms chaotic or stochastic models in both stages considered.},
  archive      = {J_ASOC},
  author       = {Maciej Klimas and Dariusz Grabowski},
  doi          = {10.1016/j.asoc.2023.110574},
  journal      = {Applied Soft Computing},
  pages        = {110574},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of long short-term memory neural networks for electric arc furnace modeling},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-heuristic search algorithms in truss optimization:
Research on stability and complexity analyses. <em>ASOC</em>,
<em>145</em>, 110573. (<a
href="https://doi.org/10.1016/j.asoc.2023.110573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although they are among the most researched real world engineering design problems , it is encountered with significant problems in the optimization of structural truss bar problems (TPs). The main reasons for these problems are (i) the studies on the optimization of TPs being carried out with different experimental settings, (ii) the competitor algorithms used in the optimization process being insufficient, and (iii) the stability analysis and computational complexity information of the algorithms are not investigated. It is designed a simulation environment with defined standards and a benchmarking suite consisting of nine TPs of three different types in order to eliminate these problems. It is found optimum solutions for all problems in the benchmark suite and it is introduced feasible solutions for the first time. According to the statistical analysis results, among the seventy-seven competing algorithms, the best ten algorithms showed competitive performance in the optimization of TPs are LSHADE-EpSin, LSHADE-CnEpSin, SHADE, LSHADE, GSK, FDB-AGDE, MPA, LRFDB-COA and BES. According to the results of the stability analysis carried out on feasible solutions, it has been determined the algorithms with the best success rate for planar type; LSHADE, LSHADE-CnEpSin, LSHADE-EpSin and GSK with 100\% success rate, LSHADE-CnEpSin, LSHADE-EpSin, LSHADE and SHADE for Space type with 100\% success rate, and LSHADE-EpSin with 69\% success rate for frequency constrained type. When the performances of the algorithms are evaluated regardless of the problem type, LSHADE-EpSin has been the most stable algorithm with an overall success rate of 90\% on the TPs. Click here for the simulation environment containing the source codes of the ten competitive algorithms and the TP benchmark suite proposed in the article : https://www.mathworks.com/matlabcentral/fileexchange/131668-structural-truss-problems-benchmark-suite .},
  archive      = {J_ASOC},
  author       = {Hasan Tahsin Öztürk and Hamdi Tolga Kahraman},
  doi          = {10.1016/j.asoc.2023.110573},
  journal      = {Applied Soft Computing},
  pages        = {110573},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Meta-heuristic search algorithms in truss optimization: Research on stability and complexity analyses},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fuzzy preference programming and weighted influence
non-linear gauge system for mission architecture assessment at NASA.
<em>ASOC</em>, <em>145</em>, 110572. (<a
href="https://doi.org/10.1016/j.asoc.2023.110572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of a mission architecture is complex due to conflicting and intertwined mission operation criteria. In addition, the uncertainties inherent in human exploration missions make the evaluation process more challenging. This study presents a combination of improved fuzzy preference programming (FPP) and weighted influence non-linear gauge system (WINGS) to efficiently and effectively weigh conflicting and intertwined decision criteria and prioritize mission architecture scenarios at NASA. The improved FPP determines the weights of decision criteria, and the WINGS method considers criteria interdependencies and evaluates the alternative mission architecture scenarios . A numerical example demonstrates the proposed approach’s performance compared to three multi-criteria methods. A case study at the Johnson Space Center shows the applicability and efficacy of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Madjid Tavana and Mohammad Saeed Heidary and Hassan Mina},
  doi          = {10.1016/j.asoc.2023.110572},
  journal      = {Applied Soft Computing},
  pages        = {110572},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy preference programming and weighted influence non-linear gauge system for mission architecture assessment at NASA},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous multi-project multi-task allocation in mobile
crowdsensing using an ensemble fireworks algorithm. <em>ASOC</em>,
<em>145</em>, 110571. (<a
href="https://doi.org/10.1016/j.asoc.2023.110571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Internet of Things (IoT), Mobile CrowdSensing (MCS) platform will release projects consisting of heterogeneous tasks, requiring participants with different skills to collaborate to develop such systems. In this paper, a heterogeneous multi-project multi-task allocation model is proposed based on the group collaboration mode to cater for this problem state. Our method would distinguish the roles of members within the group, and incorporate the inherent attributes of participants like skill level and social competence. With the constraints of skill matching and completion time, one needs to simultaneously maximize the sensing quality and to minimize the platform cost by finding an optimal task-participant allocation schedule. To solve the established model, a multi-objective fireworks algorithm with dual-feedback ensemble learning framework is proposed. The weight of the weak optimizer would be adjusted automatically by the evolutionary significance, for which the individual generation method more suitable for the current state would be chosen. The individual evaluation mechanism is updated by the objective exploration degree, so that the evolutionary direction can be adaptively adjusted. To experimentally evaluate the proposed approach, it would be compared with five representative algorithms on 12 real-world instances. Experimental results show that our algorithm can assist platform managers in making better decisions.},
  archive      = {J_ASOC},
  author       = {Xiaoning Shen and Di Xu and Liyan Song and Yuchi Zhang},
  doi          = {10.1016/j.asoc.2023.110571},
  journal      = {Applied Soft Computing},
  pages        = {110571},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogeneous multi-project multi-task allocation in mobile crowdsensing using an ensemble fireworks algorithm},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Not just select samples, but exploration: Genetic
programming aided remote sensing target detection under deep learning.
<em>ASOC</em>, <em>145</em>, 110570. (<a
href="https://doi.org/10.1016/j.asoc.2023.110570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data of target detection in remote sensing images are diverse, and the detection results of some categories with a small number of samples are poor. In order to solve this problem, most of the existing methods focus on the category with a small number of samples through data augmentation , but this will bring huge loss of original information, resulting in the decline of the effectiveness of some categories when improving the effectiveness. Additionally, since remote sensing image targets are small, numerous and densely distributed, the mixing degree of target and background is high, making them hardly distinguished. Therefore, a loss-based sample selection mechanism is proposed to enhance the category samples with low proportion. In the training process, we select between the original samples and enhanced samples through loss feedback, so as to retain the original sample information as much as possible and improve the detection performance. On this basis, an auxiliary feature detection module is proposed. First, the module detects the highly mixed area between the object to be detected and the background, and uses a series of image enhancement operations to build a genetic programming (GP) tree to separate the object from the background as much as possible, so that the detector can better extract and detect target features. Compared with other latest related algorithms, the loss-based sample selection mechanism and evolutionary auxiliary feature detection method proposed in this paper can improve the detection performance of low proportion categories through the sample selection mechanism, and improve the robustness to background clutter interference through evolutionary auxiliary feature detection. The proposed approach effectively improves the detection performance and performs well in remote sensing target detection.},
  archive      = {J_ASOC},
  author       = {Shuai Wang and Shichen Huang and Shuai Liu and Ying Bi},
  doi          = {10.1016/j.asoc.2023.110570},
  journal      = {Applied Soft Computing},
  pages        = {110570},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Not just select samples, but exploration: Genetic programming aided remote sensing target detection under deep learning},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A method for finding numerical solutions to diophantine
equations using spiral optimization algorithm with clustering (SOAC).
<em>ASOC</em>, <em>145</em>, 110569. (<a
href="https://doi.org/10.1016/j.asoc.2023.110569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diophantine equations are equations containing two or more unknowns, such that only the integer solutions are required. To find solutions of these equations numerically, we can be performed by solving an optimization problem using a metaheuristic method. In this paper, the Spiral Optimization Algorithm with Clustering (SOAC) method is proposed to find solutions to Diophantine equations in the form of polynomial, exponential, and also linear and nonlinear systems of equations. In the implementation of the method on solving some existing benchmark problems, the goal of simulation is to find all solutions only in a single run and in a short period of time. Appropriate values of required parameters are selected during the simulation. Results shows satisfactory in solving four problems in polynomial equations , four problems in exponential equations, and three problems in systems of linear and nonlinear equations. In most of cases, the results yield the same with the analytical or numerical solutions in the reference papers, and in some cases the results give more solutions.},
  archive      = {J_ASOC},
  author       = {Novriana Sumarti and Kuntjoro Adji Sidarto and Adhe Kania and Tiara Shofi Edriani and Yudi Aditya},
  doi          = {10.1016/j.asoc.2023.110569},
  journal      = {Applied Soft Computing},
  pages        = {110569},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A method for finding numerical solutions to diophantine equations using spiral optimization algorithm with clustering (SOAC)},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent cryptocurrency trading system using integrated
AdaBoost-LSTM with market turbulence knowledge. <em>ASOC</em>,
<em>145</em>, 110568. (<a
href="https://doi.org/10.1016/j.asoc.2023.110568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bitcoin market is firmly positioned as a global asset market. However, due to its extremely high volatility and the lack of a custom Bitcoin trading system, investors find it difficult to establish an effective investment strategy in this market. In this study, we build an intelligent Bitcoin trading system to maximize profitability by predicting the market. We propose a fusion approach combining technology with economic knowledge to achieve accurate predictions. Firstly, we provide an ensemble prediction framework that integrates the AdaBoost algorithm with the LSTM deep learning model as a form of technological convergence. This dramatically improves the predictive performance due to the high representational capacity of LSTM and the overfitting minimization capability of AdaBoost . Secondly, to account for the repeated structural volatility in the market, we combine the econometrics Markov regime-switching model with the AdaBoost-LSTM model to predict the period of market turbulence caused by structural breaks in the regime. Our prediction model suggests that market turbulence lasting longer than 21 days could be a potentially high-risk investment in the future. Finally, we have developed a Bitcoin-customized trading algorithm that maximizes rewards by predicting upward movements in prices and warning of high-risk investments during a long-lasting turbulent regime. In trading tests, our system yielded a cumulative return of 2.4 times and 4.0 times on 1-day and 5-day forecasts, respectively, compared to other representative trading strategies such as the stochastic oscillator .},
  archive      = {J_ASOC},
  author       = {Sangjin Park and Jae-Suk Yang},
  doi          = {10.1016/j.asoc.2023.110568},
  journal      = {Applied Soft Computing},
  pages        = {110568},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent cryptocurrency trading system using integrated AdaBoost-LSTM with market turbulence knowledge},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized linguistic gained and lost dominance score
method for landslide hazard treatment. <em>ASOC</em>, <em>145</em>,
110567. (<a href="https://doi.org/10.1016/j.asoc.2023.110567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing the risk of landslide hazards necessitates a prompt emergency decision regarding landslides, particularly when a hasty investigation lacks abundant data. Thus, the experience and opinions of experts, which are typically expressed in linguistic terms rather than exact values, can be utilized to the fullest extent to facilitate this process. Here I extend the gained and lost dominance score (GLDS) method to a generalized linguistic setting for modeling collective cognition in the selection of landslide treatment schemes for an open-pit mine. To do so, a generalized qualitative scale considering two cognitive bias arguments and one granularity parameter of linguistic term sets (LTS), which constitutes a generalized LTS that allows experts to express their evaluations in a flexible way, is presented to capture precise semantics of linguistic evaluations. Furthermore, several aggregation operators are given to deal with the generalized LTS for information fusion. To promote the application of the GLDS method within the context of multi-expert heterogeneous linguistic environment, I develop the framework of a generalized GLDS method by improving three aspects (i.e., the construction of dominance flow, the computing of lost dominance scores and the establishment of final aggregation function) of the classical one. Lastly, the effectiveness of the proposed method is examined by solving the multiple criteria decision-making problem we encountered as well as comparing with other outranking methods. The lattice anchored technique coupled with real-time monitoring is picked out to treat the landslide, which mitigates the production pressure in this high-slope open-pit mine.},
  archive      = {J_ASOC},
  author       = {Ziguo Fu},
  doi          = {10.1016/j.asoc.2023.110567},
  journal      = {Applied Soft Computing},
  pages        = {110567},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generalized linguistic gained and lost dominance score method for landslide hazard treatment},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting long-term stock prices of global indices: A
forward-validating genetic algorithm optimization approach for support
vector regression. <em>ASOC</em>, <em>145</em>, 110566. (<a
href="https://doi.org/10.1016/j.asoc.2023.110566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting long-term stock index prices is a challenging and debatable task. Most of the studies focus on predicting next-day stock prices. However, those are not useful to long-term investors and traders. In this paper, we attempt to predict up to a year’s daily prices of global stock indices using daily close prices data. This study fills a gap in the existing literature by focusing on long-term stock index price forecasting, which is crucial for practical applications in the financial markets. Moreover, The empirical analysis highlights the superior performance of a rolling forward-validation approach over cross-validation in predicting long-term stock prices. A forward-validating Genetic Algorithm Optimization for Support Vector Regression (OGA-SVR) is used to efficiently forecast multi-step ahead long-term global stock indices. Further, the performance of the model is compared with that of Support Vector Regression (SVR), Grid Search based Support Vector Regression (GS-SVR), Genetic algorithm-based Support Vector regression (GA-SVM), and state-of-the-art Long Short-Term Memory (LSTM) algorithms. The models are empirically tested on five global stock indices time series daily data, namely Nifty, Dow Jones Industrial Average (DJIA), DAX performance index (DAX), Nikkei 225 (NI225), and Shanghai Stock Exchange composite index (SSE). Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE) are used for the evaluation The result shows the OGA-SVR model outperforms other models in predicting the long-term prices of global indices. Further, the OGA-SVR model has the potential to forecast the long-term underlying future pattern of index prices which can be used to build trading and risk mitigation systems by investors and traders.},
  archive      = {J_ASOC},
  author       = {Mohit Beniwal and Archana Singh and Nand Kumar},
  doi          = {10.1016/j.asoc.2023.110566},
  journal      = {Applied Soft Computing},
  pages        = {110566},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting long-term stock prices of global indices: A forward-validating genetic algorithm optimization approach for support vector regression},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A 2-phase prediction of a non-stationary time-series by
taylor series and reinforcement learning. <em>ASOC</em>, <em>145</em>,
110565. (<a href="https://doi.org/10.1016/j.asoc.2023.110565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of a non-stationary time-series is hard as the frequency components and their amplitudes in the series vary randomly over time. This paper proposes a 2-phase approach for prediction of such non-stationary time-series. The first phase employs Taylor series to approximately predict the next time-point value in the series from its current and last two preceding sample values. The Taylor series based prediction, however, presumes that the time-series is locally stationary. The second phase employs reinforcement learning to refine the Taylor series based prediction further, particularly at the juncture of structural changes in the time-series. The reinforcement learning based prediction is realized with the help of an adaptive probabilistic learning matrix that evolves to encode the mapping between current prediction error and the error-compensation for the next sample. On convergence of the matrix, the saved probabilities are used to determine the error-compensation for the next sample from the estimated error at the current sample. The additive error-compensation is then utilized to rectify the results of prediction obtained by Taylor series. Experiments undertaken confirm that the proposed 2-phase prediction outperforms the state-of-the-art prediction techniques by a significant margin of prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Debolina Dey and Lidia Ghosh and Diptendu Bhattacharya and Amit Konar},
  doi          = {10.1016/j.asoc.2023.110565},
  journal      = {Applied Soft Computing},
  pages        = {110565},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A 2-phase prediction of a non-stationary time-series by taylor series and reinforcement learning},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dimensionless model and ant colony optimization fusion
temperature prediction in tunnel fires. <em>ASOC</em>, <em>145</em>,
110564. (<a href="https://doi.org/10.1016/j.asoc.2023.110564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior studies have noted the importance of ceiling temperature distribution prediction of tunnel fires. To overcome the limitations of precise modeling difficulties for model-driven methods and biased toward the convergence for the data-driven methods due to loss of physical interpretations, a hybrid model-driven and data-driven fusion algorithm is established to predict ceiling temperature distribution in tunnel fires based on dimensional analysis method and ant colony optimization algorithm. Thus, full-scale tunnel fire experiments with different ventilation speeds are conducted to support the algorithm. In contrast to the experimental data as well as the known theoretical model, data-driven back propagation neural network algorithm and hybrid ant colony optimization and back propagation neural network algorithm, the effectiveness and ability of the algorithm are verified, which can be used for the tunnel fire prediction.},
  archive      = {J_ASOC},
  author       = {Bin Sun},
  doi          = {10.1016/j.asoc.2023.110564},
  journal      = {Applied Soft Computing},
  pages        = {110564},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dimensionless model and ant colony optimization fusion temperature prediction in tunnel fires},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An artificial immune differential evolution algorithm for
scheduling a distributed heterogeneous flexible flowshop. <em>ASOC</em>,
<em>145</em>, 110563. (<a
href="https://doi.org/10.1016/j.asoc.2023.110563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a distributed heterogeneous flexible flowshop problem (DHFFP) with sequence-dependent setup and transport times, where each job has a release time and each production stage has multiple unrelated parallel machines. An integer linear programming (ILP) model and a new artificial immune differential evolution (AIDE) algorithm are proposed aiming to minimize the makespan and total tardiness simultaneously. Firstly, the AIDE employs a two-dimensional vector encoding scheme including the information of job permutation and factory allocation for solution representation. In addition, a dynamic decoding scheme is designed to construct a feasible schedule. Secondly, three distributed Nawaz–Enscore–Ham (NEH) based constructive heuristics are presented to provide the initial antibody population. Thirdly, a discrete differential evolution algorithm is introduced to mutate the cloned antibodies. Further, clonal suppression is applied by eliminating the antibodies with lower stimulation values. Simulated annealing (SA) based local search is incorporated with four problem-specific neighborhood structures to enhance the algorithm. Finally, the optimal values of the algorithmic parameters are determined by the orthogonal test. Some numerical experiments of the ILP model on small-scale instances are conducted to show the effectiveness of the proposed AIDE algorithm. The AIDE algorithm is also compared with several existing algorithms for different scale instances. The results demonstrate that the AIDE algorithm generates average relative percentage deviation of 12.51\% and 7.99\% respectively for small &amp; medium-scale instances and large-scale instances, which is the best-performing among all algorithms.},
  archive      = {J_ASOC},
  author       = {Hua Xuan and Wenting Li and Bing Li},
  doi          = {10.1016/j.asoc.2023.110563},
  journal      = {Applied Soft Computing},
  pages        = {110563},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An artificial immune differential evolution algorithm for scheduling a distributed heterogeneous flexible flowshop},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-label text message classification method designed
for applications in call/contact centre systems. <em>ASOC</em>,
<em>145</em>, 110562. (<a
href="https://doi.org/10.1016/j.asoc.2023.110562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a system for multi-label classification of text data processed in Call/Contact Centre (CC) systems. The solution presented herein constitutes a significant innovation and an advantage in relation to the solutions used so far in CC systems, as the contents can be automatically routed directly to even several agents with different competences (depending on the number of classes recognised in the message). The proposed approach combines a set of vectorisation methods, dimensionality reduction methods, and a classifier based on artificial neural networks . Analyses were performed using data from real databases of a large commercial CC system and data extracted from the publicly available Stackoverflow database to evaluate the effectiveness of the developed classification method. The proposed approach was compared with the existing text data classification methods. The method enables classification of text messages belonging to one or more classes and can be used to automatically route contents to agents with appropriate competences.},
  archive      = {J_ASOC},
  author       = {Katarzyna Poczeta and Mirosław Płaza and Tomasz Michno and Maria Krechowicz and Michał Zawadzki},
  doi          = {10.1016/j.asoc.2023.110562},
  journal      = {Applied Soft Computing},
  pages        = {110562},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-label text message classification method designed for applications in call/contact centre systems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manta ray foraging optimization based on mechanics game and
progressive learning for multiple optimization problems. <em>ASOC</em>,
<em>145</em>, 110561. (<a
href="https://doi.org/10.1016/j.asoc.2023.110561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms are currently being studied in depth by many scholars, and it is an important task to improve the learning and adaptive capabilities of the algorithms so that they can be of great use in a wide range of optimization problems . The Manta ray foraging optimization (MRFO) has made certain achievements in optimization problems , but it still has shortcomings, such as its insufficient intra-population communication and poor learning ability, so the optimization capability of MRFO needs further improvement. To address these shortcomings, this paper proposes a manta ray foraging optimization based on mechanics game and progressive learning for multiple optimization problems, which is abbreviated as MGL-MRFO, introducing variable spiral factors and dynamic adjustment of internal parameters; then proposes progressive learning to enhance the learning ability of MRFO; and finally proposes a matching game-based update mechanism to eliminate intra-population repetition as a feasible solution. It is validated in terms of move step size and time complexity. The results are compared with variants of the algorithm proposed in recent years in 18 benchmark test functions and the CEC 2022 test set, as well as with the TOP algorithm in the CEC competition, and show that MGL-MRFO has strong advantages, validating the novelty and competitiveness of MGL-MRFO. At the end of the paper, the feasibility and practicality of MGL-MRFO is further validated by three engineering optimization problems.},
  archive      = {J_ASOC},
  author       = {Donglin Zhu and Siwei Wang and Changjun Zhou and Shaoqiang Yan},
  doi          = {10.1016/j.asoc.2023.110561},
  journal      = {Applied Soft Computing},
  pages        = {110561},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Manta ray foraging optimization based on mechanics game and progressive learning for multiple optimization problems},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum-inspired optimization algorithm with adaptive
correction of energy position: Methodology and a case study.
<em>ASOC</em>, <em>145</em>, 110560. (<a
href="https://doi.org/10.1016/j.asoc.2023.110560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and stable global optimizers constitute a noteworthy arena of academic study and real-world applications. Since Multi-scale Quantum Harmonic Oscillator Algorithm inspired by the quantum motion for solving optimization problems was proposed, considerable contributions regarding this algorithm have been achieved in recent years. Nevertheless, issues such as the aggregation effect during sampling as well as recurrence and blindness in random searches hinder the performance of the algorithm. Motivated by this situation, a variant of Multi-scale Quantum Harmonic Oscillator Algorithm is put forward to improve the efficiency of the system convergence while maintaining the solution diversity. The measurement of the solution position through the collapse of the quantum state to the classical state is realized by means of quantum Monte Carlo simulations , and the energy position is established as a metric for energy observation. Then, the adaptive correction of the energy position is explored to improve algorithm performance. The core idea of our mechanism is to adaptively guide the candidate solutions toward convergence to the ground state by means of attractive factors based on the relationship among the energy positions of several reference points. Experimental results obtained on the CEC2013 benchmark functions and a real-world application indicate that the performance of our scheme is competitive and that it achieves prominence among the compared algorithms as the dimensionality increases .},
  archive      = {J_ASOC},
  author       = {Lei Mu and Peng Wang},
  doi          = {10.1016/j.asoc.2023.110560},
  journal      = {Applied Soft Computing},
  pages        = {110560},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum-inspired optimization algorithm with adaptive correction of energy position: Methodology and a case study},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-organizing modular neural network based on empirical
mode decomposition with sliding window for time series prediction.
<em>ASOC</em>, <em>145</em>, 110559. (<a
href="https://doi.org/10.1016/j.asoc.2023.110559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series is mostly with a chaotic nature and non-stationary characteristic in real-word, which makes it difficult to be modeled and predicted accurately. To solve this problem, we introduce a novel self-organizing modular neural network based on the empirical mode decomposition with the sliding window mechanism (SWEMD-MNN) for time series prediction. In SWEMD-MNN, the improved empirical mode decomposition with sliding window (SWEMD) is developed to decompose time series online, which can effectively alleviate the limitation that the traditional EMD-based models cannot handle the long term or online problem and end effect. Thus, SWEMD-MNN can decompose time series based on time characteristic effectively and dynamically, and improve the prediction accuracy of the classical modular neural networks dividing time series based on sample space. Then time subseries are dynamically assigned to the subnetworks with a single layer feedforward neural network using the sample entropy and Euclidean distance for learning. Experimental investigations using benchmark chaotic and real-world time series show that SWEMD-MNN can decompose time series effectively and dynamically, and provides a better prediction accuracy than the fully coupled networks and other MNN models for time series prediction.},
  archive      = {J_ASOC},
  author       = {Xin Guo and Wen-jing Li and Jun-fei Qiao},
  doi          = {10.1016/j.asoc.2023.110559},
  journal      = {Applied Soft Computing},
  pages        = {110559},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-organizing modular neural network based on empirical mode decomposition with sliding window for time series prediction},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective binary grey wolf optimization for feature
selection based on guided mutation strategy. <em>ASOC</em>,
<em>145</em>, 110558. (<a
href="https://doi.org/10.1016/j.asoc.2023.110558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims to choose a subset of features with minimal feature-feature correlation and maximum feature-class correlation, which can be considered as a multi-objective problem. Grey wolf optimization mimics the leadership hierarchy and group hunting mechanism of grey wolves in nature. However, it can easily fall into local optimization in multi-objective optimization. To address this, a novel multi-objective binary grey wolf optimization based on a guided mutation strategy (GMS), called MOBGWO-GMS, is proposed. In the initialization phase, the population is initialized based on feature correlation, and features are selected using a uniform operator. The proposed GMS uses the Pearson correlation coefficient to provide direction for local search, improving the local exploration ability of the population. Moreover, a dynamic agitation mechanism is used for perturbation to prevent population stagnation due to the use of a single strategy. The strategy is dynamically adjusted to maintain population diversity and improve detection ability. To evaluate the classification ability of quasi-optimal subsets, a wrapper-based k-nearest neighbor classifier was employed. The effectiveness of the proposed algorithm was demonstrated through an extensive comparison with eight well-known algorithms on fourteen benchmark datasets. Experimental results showed that the proposed approach is superior in the optimal trade-off between the two fitness evaluation criteria and can easily jump out of local optima compared to other algorithms.},
  archive      = {J_ASOC},
  author       = {Xiaobo Li and Qiyong Fu and Qi Li and Weiping Ding and Feilong Lin and Zhonglong Zheng},
  doi          = {10.1016/j.asoc.2023.110558},
  journal      = {Applied Soft Computing},
  pages        = {110558},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective binary grey wolf optimization for feature selection based on guided mutation strategy},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local double quantitative fuzzy rough sets over two
universes. <em>ASOC</em>, <em>145</em>, 110556. (<a
href="https://doi.org/10.1016/j.asoc.2023.110556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important expanded quantification fuzzy rough set model, the local fuzzy rough set model is used to measure relative quantitative information between the fuzzy similarity classes and the basic concept. Although many studies have taken on this topic, the existing local fuzzy rough sets ignore the absolute quantitative information in the fuzzy information system over two universes. Therefore, based on this observation, we propose a local double quantitative fuzzy rough set model over two universes, which considers the problems from the double quantization form of the local fuzzy rough sets over two universes. Furthermore, the corresponding properties and decision rules of the local double quantitative “logical and” fuzzy rough set model over two universes are studied. Then, in order to improve the applicability of the model, an effective reduction method is introduced. Finally, we conduct experimental comparisons showing the computational efficiency and approximate accuracy of the model in concept approximation and reduction.},
  archive      = {J_ASOC},
  author       = {Guoping Lin and Linlin Xie and Jinjin Li and Jinkun Chen and Yi Kou},
  doi          = {10.1016/j.asoc.2023.110556},
  journal      = {Applied Soft Computing},
  pages        = {110556},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local double quantitative fuzzy rough sets over two universes},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). POPNASv3: A pareto-optimal neural architecture search
solution for image and time series classification. <em>ASOC</em>,
<em>145</em>, 110555. (<a
href="https://doi.org/10.1016/j.asoc.2023.110555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing demand for machine learning applications in industry has created a need for fast and efficient methods to develop accurate machine learning models. Automated Machine Learning (AutoML) algorithms have emerged as a promising solution to this problem, designing models without the need for human expertise. Given the effectiveness of neural network models , Neural Architecture Search (NAS) specialises in designing their architectures autonomously, with results that rival the most advanced hand-crafted models. However, this approach requires significant computational resources and hardware investment, making it less attractive for real-world applications. This article presents the third version of Pareto-Optimal Progressive Neural Architecture Search (POPNASv3), a new NAS algorithm that employs Sequential Model-Based Optimisation and Pareto optimality . This choice makes POPNASv3 flexible to different hardware environments, computational budgets and tasks, as the algorithm can efficiently explore user-defined search spaces of varying complexity. Pareto optimality extracts the architectures that achieve the best trade-off with respect to the metrics considered, reducing the number of models sampled during the search and dramatically improving time efficiency without sacrificing accuracy. The experiments performed on image and time series classification datasets provide evidence that POPNASv3 can explore a large set of different operators and converge to optimal architectures suited to the type of data provided under different scenarios. 1},
  archive      = {J_ASOC},
  author       = {Andrea Falanti and Eugenio Lomurno and Danilo Ardagna and Matteo Matteucci},
  doi          = {10.1016/j.asoc.2023.110555},
  journal      = {Applied Soft Computing},
  pages        = {110555},
  shortjournal = {Appl. Soft. Comput.},
  title        = {POPNASv3: A pareto-optimal neural architecture search solution for image and time series classification},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-series prediction using a regularized self-organizing
long short-term memory neural network. <em>ASOC</em>, <em>145</em>,
110553. (<a href="https://doi.org/10.1016/j.asoc.2023.110553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial process data are naturally in the form of complex time-series with high nonlinearities and dynamics. Long short-term memory (LSTM) networks are suitable for developing prediction models to handle nonlinear and dynamic process. However, LSTM neural networks have typically large and predefined structures, which may result in overfitting, and an optimal hidden neurons for a given problem cannot be automatically obtained. For this reason, a regularized self-organizing LSTM (RSO-LSTM) is proposed to optimize both the structure and the parameters of the network. First, an adaptive learning algorithm based on l 2-norm regularization is introduced for parameter adjustment. Thereafter, both the prediction accuracy and weight dispersion are considered to avoid overfitting. Second, a growing strategy is designed based on hidden neuronal sensitivity. The structure of the LSTM can then be determined automatically with improved compactness. Finally, a convergence analysis is performed to ensure the feasibility of RSO-LSTM. To demonstrate the merits of the proposed RSO-LSTM for time-series​ prediction, its results for three benchmark experiments and real industrial data of a municipal solid waste incineration process were examined and compared with those of other methods. The results indicated the superiority and potential of RSO-LSTM for industrial applications.},
  archive      = {J_ASOC},
  author       = {Hao-shan Duan and Xi Meng and Jian Tang and Jun-fei Qiao},
  doi          = {10.1016/j.asoc.2023.110553},
  journal      = {Applied Soft Computing},
  pages        = {110553},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time-series prediction using a regularized self-organizing long short-term memory neural network},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Countering malicious content moderation evasion in online
social networks: Simulation and detection of word camouflage.
<em>ASOC</em>, <em>145</em>, 110552. (<a
href="https://doi.org/10.1016/j.asoc.2023.110552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content moderation is the process of screening and monitoring user-generated content online. It plays a crucial role in stopping content resulting from unacceptable behaviors such as hate speech, harassment, violence against specific groups, terrorism, racism, xenophobia, homophobia, or misogyny, to mention some few, in Online Social Platforms. These platforms make use of a plethora of tools to detect and manage malicious information; however, malicious actors also improve their skills, developing strategies to surpass these barriers and continuing to spread misleading information. Twisting and camouflaging keywords are among the most widely used techniques to evade platform content moderation systems. In response to this recent ongoing issue This paper presents an innovative approach to address this linguistic trend in social networks through the simulation of different content evasion techniques and a multilingual transformer model for content evasion detection. In this way a multilingual public tool Named “ pyleetspeak ” is shared with the scientific community, enabling the generation and simulation of content evasion through automatic word camouflage in a customizable way. Additionally a multilingual named-entity recognition (NER) transformer-based model is provided Designed for the recognition and detection of such evasion technique. The developed tool is multilingual Supporting over 20 languages (ar, az, da, de, el, en, es, fi, fr, hu, id, it, kk, nb, ne, nl, pt, ro, ru, sl, sv, tg, tr) and the NER model has been tested in English, Spanish, French, Italian, and German. This multilingual NER model is evaluated in different textual scenarios Detecting different types and mixtures of camouflage techniques Achieving an overall weighted F1 score of 0.8795. This article contributes significantly to countering malicious information by developing multilingual tools to simulate and detect new methods of evasion of content on social networks Making the fight against information disorders more effective},
  archive      = {J_ASOC},
  author       = {Álvaro Huertas-García and Alejandro Martín and Javier Huertas-Tato and David Camacho},
  doi          = {10.1016/j.asoc.2023.110552},
  journal      = {Applied Soft Computing},
  pages        = {110552},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Countering malicious content moderation evasion in online social networks: Simulation and detection of word camouflage},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-driven ant colony optimization algorithm for
vehicle routing problem in instant delivery peak period. <em>ASOC</em>,
<em>145</em>, 110551. (<a
href="https://doi.org/10.1016/j.asoc.2023.110551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instant delivery is an important part of urban logistics distribution, which realizes point-to-point distribution between merchants and customers. During the peak period of orders, instant delivery is a large-scale variable NP-hard combinatorial optimization problem , which increases the difficulty and complexity of scheduling greatly. To solve the large-scale vehicle routing problem of instant delivery in peak periods, a knowledge-driven ant colony optimization (KDACO) algorithm is proposed in this paper. First, the knowledge base is established to guide evolutionary search, including the knowledge of order priority and the feature knowledge of feasible schemes. Second, the pheromone supplementation strategy is designed based on the knowledge of order priority, enhancing the guiding ability of the pheromone table. Third, the adaptive evolutionary operator is designed based on the feature knowledge of feasible schemes, improving the optimization efficiency of the algorithm. Finally, numerical experiments on extensive classical datasets show that the proposed KDACO can obtain superior performance to other state-of-the-art algorithms in the instant delivery peak period.},
  archive      = {J_ASOC},
  author       = {Ying Hou and Xinyu Guo and Honggui Han and Jingjing Wang},
  doi          = {10.1016/j.asoc.2023.110551},
  journal      = {Applied Soft Computing},
  pages        = {110551},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge-driven ant colony optimization algorithm for vehicle routing problem in instant delivery peak period},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new class of robust and predefined-time consensus protocol
based on noise-tolerant ZNN models. <em>ASOC</em>, <em>145</em>, 110550.
(<a href="https://doi.org/10.1016/j.asoc.2023.110550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zeroing neural network (ZNN) is a powerful tool in designing suitable control schemes since it is a systematic approach. It has been used in fields like robot manipulator control and tracking control , but few researchers have investigated the possible application of the ZNN in multi-agent systems. Based on the elegant zeroing neural network (ZNN) scheme, in this paper, two novel noise-tolerant ZNN (NTZNN) models are proposed to achieve consensus, which is a crucial problem in the field of cooperative control of the multi-agent systems. Besides, the novel noise-tolerant sign-bi-power (NTSBP) and noise-tolerant sign-exp-power (NTSEP) activation functions are used in this study. The NTZNN models activated by NTSBP and NTSEP are more robust than traditional ZNN models activated by the traditional sign-bi-power (SBP) and sign-exp-power (SEP) activation functions , respectively. The detailed mathematical analysis is presented to prove the robustness and predefined-time stability of the NTZNN models, and the upper bounds of the settling-time function are also estimated by a novel method based on improper integral . Combining the traditional Polyakov method and the proposed method based on improper integral , we can estimate the upper bounds of the settling-time function in a more precise way. Then, the robustness of the NTZNN models under both dynamic bounded vanishing noise and dynamic bounded non-vanishing noise is further evaluated by numerical experiments, and results show that the models are effective at both situations. We also present several practical examples of formation control , and parallel experiments are provided to further demonstrate that our results are general. All the theoretical and numerical verification results show that the NTZNN models are more robust than the traditional ZNN models activated by SBP or SEP activation functions, and it is also predefined-time stable.},
  archive      = {J_ASOC},
  author       = {Jiajie Luo and Lin Xiao and Penglin Cao and Xiaopeng Li},
  doi          = {10.1016/j.asoc.2023.110550},
  journal      = {Applied Soft Computing},
  pages        = {110550},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new class of robust and predefined-time consensus protocol based on noise-tolerant ZNN models},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mass customization with reinforcement learning: Automatic
reconfiguration of a production line. <em>ASOC</em>, <em>145</em>,
110547. (<a href="https://doi.org/10.1016/j.asoc.2023.110547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of efficient automation system configuration for mass customization in industrial manufacturing. Due to the various demands from customers, production lines need to adjust the process parameters of the machines based on specific quality parameters. Reinforcement learning , which learns from samples, can tackle the problem more efficiently than the currently used methods. Based on the proximal policy optimization and centralized training with decentralized execution, a multi-agent reinforcement learning method (MARL) is proposed to reconfigure process parameters of machines based on the changed specifications. The proposed method has the actor of each agent observing only its own state, the agents are made to collaborate by a centralized critic which observes all the states. To evaluate the method, a steel strip rolling line with six collaborating mills is studied. Simulation results show that the proposed method outperforms the existing methods and state-of-the-art multi-agent reinforcement learning methods in terms of accuracy and computing costs.},
  archive      = {J_ASOC},
  author       = {Jifei Deng and Seppo Sierla and Jie Sun and Valeriy Vyatkin},
  doi          = {10.1016/j.asoc.2023.110547},
  journal      = {Applied Soft Computing},
  pages        = {110547},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mass customization with reinforcement learning: Automatic reconfiguration of a production line},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What makes evolutionary multi-task optimization better: A
comprehensive survey. <em>ASOC</em>, <em>145</em>, 110545. (<a
href="https://doi.org/10.1016/j.asoc.2023.110545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-task optimization (EMTO) is a new branch of evolutionary algorithm (EA) that aims to optimize multiple tasks simultaneously within a same problem and output the best solution for each task. EMTO utilizes the strengths of EA to perform global optimization without relying on the mathematical properties of the problem. Therefore, EMTO is particularly suitable for complex, non-convex and nonlinear problems. Unlike traditional single-task EA, EMTO can deal with multiple optimization problems at once and can automatically transfer knowledge among these different problems. EMTO provides a novel approach for solving multi-task optimization problems and has attracted the attention of many researchers in the field of evolution. Due to the strong parallel search capability of EMTO, many excellent theoretical and applied research has been proposed on EMTO. To better organize these respectable research works and inspire future researchers, this paper reviews the related works on EMTO in the following three aspects. Firstly, many works focus on improving the performance of EMTO through various optimization strategies . Through an in-depth analysis and review of the current literature on this topic, we provide a comprehensive summary of these strategies. Secondly, we provide examples of real-world applications of EMTO, as well as its combination with other optimization paradigms. These examples demonstrate the wide applicability of EMTO. Finally, we propose some potential directions for future research in EMTO to inspire researchers in this field.},
  archive      = {J_ASOC},
  author       = {Hong Zhao and Xuhui Ning and Xiaotao Liu and Chao Wang and Jing Liu},
  doi          = {10.1016/j.asoc.2023.110545},
  journal      = {Applied Soft Computing},
  pages        = {110545},
  shortjournal = {Appl. Soft. Comput.},
  title        = {What makes evolutionary multi-task optimization better: A comprehensive survey},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correlation-split and recombination-sort interaction
networks for air quality forecasting. <em>ASOC</em>, <em>145</em>,
110544. (<a href="https://doi.org/10.1016/j.asoc.2023.110544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air quality prediction is a crucial issue in air pollution control and plays a vital role in environmental preservation and the promotion of sustainable development. A novel air quality prediction framework, referred to as Correlation-split and Recombination-sort Interaction Networks (CRINet), is proposed in this paper. Firstly, the data is divided into smaller segments based on the Correlation-split strategy, and is followed by a convolution operation to enhance the capability of extracting essential features. Secondly, Recombination-sort strategy is used to facilitate the interaction between temporal features. The CRINet model utilizes a dual-layer CRINet network structure to extract internal dependencies and temporal features from air quality data for prediction. Finally, the Beijing PM2.5 dataset and the Beijing Shunyi-station Air Quality dataset are used for experimental evaluation. The outcomes demonstrate that the CRINet model surpasses other models in prediction accuracy, particularly in the prediction of multiple pollutant concentrations. This provides theoretical and methodological support for accurately assessing air quality and formulating treatment strategies for relevant departments.},
  archive      = {J_ASOC},
  author       = {Yigui Feng and Yemei Qin and Shen Zhao},
  doi          = {10.1016/j.asoc.2023.110544},
  journal      = {Applied Soft Computing},
  pages        = {110544},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Correlation-split and recombination-sort interaction networks for air quality forecasting},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Gas-expensive patterns detection to optimize smart
contracts. <em>ASOC</em>, <em>145</em>, 110542. (<a
href="https://doi.org/10.1016/j.asoc.2023.110542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are programmable protocols that run on Ethereum and require gas to be deployed and used. Gas-expensive operations in some smart contracts can cause users to consume extra gas in transactions. There are already several methods to detect gas-expensive patterns in smart contracts. However, some problems still need to be solved: most static analysis methods are based on specialist knowledge, and the patterns used to detect gas-expensive patterns must be manually summarized before the detection methods can be applied. Furthermore, due to the explosive growth of smart contracts, which generate a large amount of data, it is challenging to reuse these methods across different patterns. To address these issues, this work first proposes a new learning-based method, ExpenGas, based on the idea of evolutionary computation-based machine learning to detect Expensive Operation patterns of smart contracts through pre-trained techniques and multi-crucial data flow graphs . The low complexity of the multi-crucial data flow graph enables the model to focus on key features. Finally, by testing on 21981 smart contract files, ExpenGas has 83.05\% accuracy and 91.96\% recall in detecting the Expensive Operation patterns of gas-expensive patterns, which is significantly more optimal than the current state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Jiliang Li and Ziyi Zhao and Zhou Su and Weizhi Meng},
  doi          = {10.1016/j.asoc.2023.110542},
  journal      = {Applied Soft Computing},
  pages        = {110542},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gas-expensive patterns detection to optimize smart contracts},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AGORA: An intelligent system for the anonymization,
information extraction and automatic mapping of sensitive documents.
<em>ASOC</em>, <em>145</em>, 110540. (<a
href="https://doi.org/10.1016/j.asoc.2023.110540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public institutions, such as law enforcement agencies or health centers, have a vast volume of unstructured text documents, e.g. police reports. Currently, before this data can be shared (e.g. with research institutions), it must go through a lengthy and costly human anonymization procedure. This paper addresses this issue by presenting AGORA, a cutting-edge tool that automatically identifies key entities and anonymizes sensitive data in text documents. AGORA has been developed in partnership with the Spanish National Office Against Hate Crimes and validated in the police and medical domains. This tool allows to export both anonymized texts and identified entities to structured files, thus, simplifying its exploitation for analysis purposes. Also, AGORA is capable of plotting the location entities identified in the documents, as well as obtaining and displaying relevant information from their geographical surroundings. Thus, it simplifies the task of generating comprehensive datasets for subsequent data analysis or predictive tasks. Its main goal is to foster cooperation between public institutions and research centers by easing document sharing as well as serving as a foundation for addressing succeeding phases in data science. The paper conducts a comprehensive assessment of the literature on Named Entity Recognition methodologies and technologies. Followed by extensive computational experiments to identify the best configuration for the NER models embedded in AGORA which include both successful state-of-the-art model setups and novelly proposed ones. Finally, the methodology, conclusions and software provided can be easily reused in similar application scenarios.},
  archive      = {J_ASOC},
  author       = {Rodrigo Juez-Hernandez and Lara Quijano-Sánchez and Federico Liberatore and Jesús Gómez},
  doi          = {10.1016/j.asoc.2023.110540},
  journal      = {Applied Soft Computing},
  pages        = {110540},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AGORA: An intelligent system for the anonymization, information extraction and automatic mapping of sensitive documents},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A regret theory-based multi-granularity three-way decision
model with incomplete t-spherical fuzzy information and its application
in forest fire management. <em>ASOC</em>, <em>145</em>, 110539. (<a
href="https://doi.org/10.1016/j.asoc.2023.110539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forest fires are an abrupt and highly destructive meteorological disaster that can occur in all regions of the world, resulting in significant ecological, economic and social losses. Moreover, the causes of forest fire disasters are usually complex, involving several uncertain factors such as temperature, relative humidity, wind speed and rainfall. All of those pose the greatest challenge to the study of forest fire management (FRM). In order to efficiently explore FRM via valid intelligent decision-making techniques, a novel model of regret theory (RT)-based multi-granularity (MG) three-way decisions (TWD) in incomplete T-spherical fuzzy (T-SF) environments has been constructed, where incomplete T-spherical fuzzy sets (T-SFSs) have been employed to describe diverse types of uncertain information in FRM, and RT-based MG TWD is conducive to analyzing multi-source T-SF information via reducing decision risks and modeling bounded rationality owned by decision-makers (DMs). Specifically, the concept of MG T-SF incomplete information systems (IISs) has been first constructed for information depictions of FRM. Then, MG T-SF IISs have been processed via the presented T-SF similarity principles for developing adjustable MG T-SF probabilistic rough sets (PRSs). Afterwards, an RT-based MG TWD approach has been built with the support of adjustable MG T-SF PRSs. Finally, a real-world FRM case analysis has been performed by using the built RT-based MG TWD approach, and extensive comparative and experimental analyses have been performed to validate the practicability of the presented methodology. To sum up, the presented methodology has simultaneously incorporated MG T-SF IISs, MG TWD and RT to model various uncertainties, valid information fusion processes and bounded rationality for FRM, which serves as a valid intelligent decision-making technique in processing incomplete and imprecise multi-source information with plentiful decision risks and regret emotions.},
  archive      = {J_ASOC},
  author       = {Chao Zhang and Jingjing Zhang and Wentao Li and Witold Pedrycz and Deyu Li},
  doi          = {10.1016/j.asoc.2023.110539},
  journal      = {Applied Soft Computing},
  pages        = {110539},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A regret theory-based multi-granularity three-way decision model with incomplete T-spherical fuzzy information and its application in forest fire management},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A universal large-scale many-objective optimization
framework based on cultural learning. <em>ASOC</em>, <em>145</em>,
110538. (<a href="https://doi.org/10.1016/j.asoc.2023.110538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When solving large-scale many-objective optimization problems (LMaOPs), due to the large number of variables and objectives involved, the algorithm is faced with a very high-dimensional and complex search space, which is difficult to be explored with limited resources. To address these issues, this paper proposes a universal large-scale many-objective optimization framework based on cultural learning (UCLMO). First, a universal framework is proposed, and multi-objective optimizers can be embedded into the framework to accelerate the convergence. Moreover, inspired by cultural learning, an individual selection strategy based on historical knowledge is proposed to promote the diversity of the population, and an assisted evolution strategy based on normative knowledge is presented to accelerate the convergence of the algorithm. Experiments have been conducted on multi-objective knapsack problems and LMaOPs with decision variables ranging from 500 to 1500, and the number of objectives ranging from 5 to 15. The experimental results verify the superiority and competitiveness of the proposed UCLMO framework in solving LMaOPs compared with state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Xia Wang and Hongwei Ge and Naiqiang Zhang and Yaqing Hou and Liang Sun},
  doi          = {10.1016/j.asoc.2023.110538},
  journal      = {Applied Soft Computing},
  pages        = {110538},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A universal large-scale many-objective optimization framework based on cultural learning},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel particle swarm optimization framework based on a
fork-join thread pool using a work-stealing mechanism. <em>ASOC</em>,
<em>145</em>, 110537. (<a
href="https://doi.org/10.1016/j.asoc.2023.110537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle Swarm Optimization (PSO) is one of the most popular optimization algorithms that has been adopted in various fields, including design, scheduling, and biochemistry. However, the algorithm is time-consuming when facing high-dimensional or multi-objective optimizing problems. Parallel PSO is thus proposed to improve its computing efficiency, and many studies have been conducted. However, the low-level system design is seldom considered in parallel programming, which may have a nonnegligible impact on computing efficiency, creating a gap between low-level optimization and high-level algorithm design . Therefore, this paper proposes a Parallel Asynchronous PSO (PAPSO) framework based on thread pools utilizing multicore processors and adopts a cross-level approach to bridge the gap. A series of experiments are designed and conducted to examine how the aforementioned method can improve the parallel execution efficiency of PSO compared with the OpenMP framework and nonparallel PSO. Results indicate that PAPSO can significantly improve PSO computing efficiency by reducing the elapsed time, approaching approximately 20\% optimization compared with OpenMP. Additionally, it achieves an approximately linear speedup of up to 4.5 threads. In addition, the particle communication experiment shows that the nonblocking communication protocol is effective for maintaining the computing elapsed time in the same level facing different neighborhood sizes. Finally, the work-stealing mechanism achieves an average of 16\% improvement for general computing scenarios and maintain up to 16\% improvement for imbalanced workload scenarios. Generally, the major contribution of this paper is that we innovate the thread pooling management concept with the fork-join model in parallelizing PSO to make sufficient use of multiple core CPUs for parallel computing through multithread programming.},
  archive      = {J_ASOC},
  author       = {Ming Li and Linhao Huang and Gangyan Xu and Kong Biao},
  doi          = {10.1016/j.asoc.2023.110537},
  journal      = {Applied Soft Computing},
  pages        = {110537},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel particle swarm optimization framework based on a fork-join thread pool using a work-stealing mechanism},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stream global–local motion fusion network for
skeleton-based action recognition. <em>ASOC</em>, <em>145</em>, 110536.
(<a href="https://doi.org/10.1016/j.asoc.2023.110536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition is widely used in varied areas such as human–machine interaction and virtual reality. Benefit from the powerful expression ability to depict structural data, graph convolutional networks (GCNs) have been developed to address this task by modeling the human body skeletons as spatial–temporal graphs. However, most existing GCN-based methods usually ignore the diversity of the motion information between channels of the input feature. And how to enhance the ability to capture the long-term global correlations in spatial and temporal dimensions is also a fundamental challenge. In this work, we propose a novel multi-stream framework Global–Local Motion Fusion Network (GLMFN), which integrates the global and local motion information of spatial–temporal dimensions. Specifically, we design a grouping graph convolution module to enforce the ability to aggregate local spatial motion information. Besides, to learn richer semantic features , we propose two modules based on the self-attention operator: a spatial self-attention module and a temporal self-attention module. The former is responsible for extracting spatial long-term motion relationships, while the latter aims to capture temporal long-term motion relationships. Moreover, we present a multi-stream fusion strategy with a series of treatments for body joints to achieve a better recognition effect. To validate the efficacy and efficiency of the proposed model, we perform exhaustive experiments on the NTU-RGBD dataset and NTU-RGBD-120 dataset, and our method achieves the state-of-the-art performance on both datasets.},
  archive      = {J_ASOC},
  author       = {Yanpeng Qi and Chen Pang and Yiliang Liu and Lei Lyu},
  doi          = {10.1016/j.asoc.2023.110536},
  journal      = {Applied Soft Computing},
  pages        = {110536},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-stream Global–Local motion fusion network for skeleton-based action recognition},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep convolutional neural network for salt-and-pepper
noise removal using selective convolutional blocks. <em>ASOC</em>,
<em>145</em>, 110535. (<a
href="https://doi.org/10.1016/j.asoc.2023.110535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an unprecedented upsurge in applying deep learning approaches, specifically convolutional neural networks (CNNs), to solve image denoising problems, owing to their superior performance. However, CNNs mostly rely on Gaussian noise , and there is a conspicuous lack of exploiting CNNs for salt-and-pepper (SAP) noise reduction. In this paper, we proposed a deep CNN model, namely SeConvNet, to suppress SAP noise in gray-scale and color images. To meet this objective, we introduce a new selective convolutional (SeConv) block. SeConvNet is compared to state-of-the-art SAP denoising methods using extensive experiments on various common datasets. The results illustrate that the proposed SeConvNet model effectively restores images corrupted by SAP noise and surpasses all its counterparts at both quantitative criteria and visual effects, especially at high and very high noise densities.},
  archive      = {J_ASOC},
  author       = {Ahmad Ali Rafiee and Mahmoud Farhang},
  doi          = {10.1016/j.asoc.2023.110535},
  journal      = {Applied Soft Computing},
  pages        = {110535},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep convolutional neural network for salt-and-pepper noise removal using selective convolutional blocks},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leaf disease detection using machine learning and deep
learning: Review and challenges. <em>ASOC</em>, <em>145</em>, 110534.
(<a href="https://doi.org/10.1016/j.asoc.2023.110534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of leaf disorder plays an important role in the economic prosperity of any country. Many parts of a plant can be infected by a virus, fungal, bacteria, and other infectious organisms but here we mainly considered the detection of leaf disease of a plant as a research topic. We have performed an in-depth study of this topic from 2010 to 2022 and found that many researchers use multispectral or hyperspectral imaging to study crop diseases. Machine learning (ML) and deep learning (DL) models are used to classify different types of leaf diseases. We made a workflow mechanism to help researchers in this field. Support vector machine (SVM), Random Forest , and multiple twin SVM (MTSVM) are popular ML models for predicting leaf disease, while convolutional neural networks (CNN), visual geometry group (VGG), ResNet (RNet), GoogLeNet, deep CNN (DCNN), back propagation neural networks (BPNN), DenseNet (DNet), LeafNet (LN), and LeNet are common deep learning models used for detecting leaf disease. Among these deep learning models, it is evident that models like CNN, VGG, and ResNet are highly capable at finding diseases in leaves. The performance of the algorithms is generally evaluated using F1 score, precision, accuracy and others. This review will be helpful for the researchers who are working in this area and looking for various efficient ML and DL-based classifiers for leaf disease detection.},
  archive      = {J_ASOC},
  author       = {Chittabarni Sarkar and Deepak Gupta and Umesh Gupta and Barenya Bikash Hazarika},
  doi          = {10.1016/j.asoc.2023.110534},
  journal      = {Applied Soft Computing},
  pages        = {110534},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Leaf disease detection using machine learning and deep learning: Review and challenges},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph sample and aggregate attention network optimized with
barnacles mating algorithm based sentiment analysis for online product
recommendation. <em>ASOC</em>, <em>145</em>, 110532. (<a
href="https://doi.org/10.1016/j.asoc.2023.110532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data analytics is important in many businesses that use computing applications like real-time shopping and e-commerce. Big data is employed for promoting goods and improve connectivity betwixt retailers and customers. People today frequently utilize online promotions to learn about the best stores to visit in order to purchase higher-quality goods. This purchasing experience along opinion regarding the shopper’s shop can be observing through the customer-experience published across social media. A new customer requires information’s about manufacturing date (MRD), manufacturing price (MRP), offers, quality, and suggestions while searching a shop that are presented by prior customer experience. The product cover or label already has MRP and MRD. The product details have been predicted using a variety of methods, however the information is not correct. To overcome these issues, Graph Sample and Aggregate Attention Network (GSAAN) Optimized with Barnacles Mating Algorithm (BMA) based Sentiment Analysis (SA) is proposed for Online Product Recommendation (GSAAN-BMA-SA-OPR). The data’s are collected from the data set of Amazon Product recommendation. With the help of GSAAN with BMA classify product recommendation as excellent, good, very good, bad, very bad. The proposed model is implemented in MATLAB, its efficiency of the proposed system is examined under performance metrics, such as mean absolute error (MAE), mean squared error (MSE), mean absolute percentage error (MAPE), accuracy, F-Score, recall, precision. The proposed technique attains higher MAPE 23.77\%, 65.3\% and 10.42\% and lower MSE 42.15\%, 29.64\% and 32.48\% comparing to the existing models.},
  archive      = {J_ASOC},
  author       = {N. Pughazendi and P.V. Rajaraman and Muzammil Hussain Mohammed},
  doi          = {10.1016/j.asoc.2023.110532},
  journal      = {Applied Soft Computing},
  pages        = {110532},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph sample and aggregate attention network optimized with barnacles mating algorithm based sentiment analysis for online product recommendation},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FER-CHC: Facial expression recognition with cross-hierarchy
contrast. <em>ASOC</em>, <em>145</em>, 110530. (<a
href="https://doi.org/10.1016/j.asoc.2023.110530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) tasks with convolutional neural networks (CNNs) have seen remarkable progress. However, these CNN-based approaches do not well capture detailed and crucial features that can distinguish different facial expressions from a global perspective. There is still much room for improvement in the performance of existing CNN-based models for FER. To address this, we propose a novel cross-hierarchy contrast (CHC) framework called FER-CHC for FER tasks. FER-CHC employs a contrastive learning mechanism to utilize these crucial features in improving the performance of CNN-based models for FER. Specifically, FER-CHC utilizes CHC to regularize the feature learning of the backbone network and enhance global representations of facial expressions. The CHC captures common and differential features from different facial expressions with a cross-hierarchy contrast mechanism. Furthermore, a fusion network globally integrates the features learned from both the backbone network and CHC to learn a more robust feature representation. We conducted comprehensive experiments on six popular datasets: CK+, FER2013, FER+, RAF-DB, AffectNet, and JAFFE. The results show that our proposed FER-CHC achieves state-of-the-art performances on these datasets. Additionally, an ablation study was conducted to demonstrate the effectiveness of the proposed components in FER-CHC.},
  archive      = {J_ASOC},
  author       = {Xuemei Wu and Jie He and Qionghao Huang and Changqin Huang and Jia Zhu and Xiaodi Huang and Hamido Fujita},
  doi          = {10.1016/j.asoc.2023.110530},
  journal      = {Applied Soft Computing},
  pages        = {110530},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FER-CHC: Facial expression recognition with cross-hierarchy contrast},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online evolutionary neural architecture search for
multivariate non-stationary time series forecasting. <em>ASOC</em>,
<em>145</em>, 110522. (<a
href="https://doi.org/10.1016/j.asoc.2023.110522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting (TSF) is one of the most important tasks in data science. TSF models are usually pre-trained with historical data and then applied on future unseen datapoints. However, real-world time series data is usually non-stationary and models trained offline usually face problems from data drift. Models trained and designed in an offline fashion can not quickly adapt to changes quickly or be deployed in real-time. To address these issues, this work presents the Online NeuroEvolution-based Neural Architecture Search (ONE-NAS) algorithm, which is a novel neural architecture search method capable of automatically designing and dynamically training recurrent neural networks (RNNs) for online forecasting tasks. Without any pre-training, ONE-NAS utilizes populations of RNNs that are continuously updated with new network structures and weights in response to new multivariate input data. ONE-NAS is tested on real-world, large-scale multivariate wind turbine data as well as the univariate Dow Jones Industrial Average (DJIA) dataset. Results demonstrate that ONE-NAS outperforms traditional statistical time series forecasting methods, including online linear regression, fixed long short-term memory (LSTM) and gated recurrent unit (GRU) models trained online, as well as state-of-the-art, online ARIMA strategies. Additionally, results show that utilizing multiple populations of RNNs which are periodically repopulated provide significant performance improvements, allowing this online neural network architecture design and training to be successful.},
  archive      = {J_ASOC},
  author       = {Zimeng Lyu and Alexander Ororbia and Travis Desell},
  doi          = {10.1016/j.asoc.2023.110522},
  journal      = {Applied Soft Computing},
  pages        = {110522},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online evolutionary neural architecture search for multivariate non-stationary time series forecasting},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interval-valued spherical fuzzy MABAC method based on dombi
aggregation operators with unknown attribute weights to select plastic
waste management process. <em>ASOC</em>, <em>145</em>, 110516. (<a
href="https://doi.org/10.1016/j.asoc.2023.110516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of plastic products worldwide generates an enormous amount of waste. Plastic is a substance that is not biodegradable and is permanent in the environment. The environment and living beings are negatively affected by this waste if it is not managed properly. However, the assessment and selection of effective plastic waste management methods is a complex and uncertain decision-making problem due to the involvement of multiple conflicting attributes, the imprecise human mind, and a lack of information. Recently, interval-valued spherical fuzzy (IVSF) sets have been recognized as one of the suitable tools to handle vague information. In this paper, we introduce a hybrid multi-attribute group decision-making methodology based on the entropy method , deviation-based method, and Multi-Attribute Border Approximation area Comparison (MABAC) method with IVSF sets to choose the most effective plastic waste management process in which the decision experts’ and attribute weights are completely unknown. In this model, the entropy and deviation-based methods are utilized to calculate the decision experts’ and attribute weights, respectively. Also, the MABAC method is used to evaluate the priority order of the plastic waste management options. To do this, Dombi weighted averaging and Dombi weighted geometric operators are developed in the IVSF environment. In the proposed model, the Dombi weighted averaging operator is used to fuse several decision experts’ information, and the Dombi weighted geometric operator is used to determine the border approximation area matrix. Then, to confirm the applicability of the suggested model, we look at a numerical example of choosing the most effective method for managing plastic waste with IVSF information. In the numerical example, we consider six alternatives and four important attributes with ten sub-attributes by consulting four decision experts. The result of this study shows that ‘Recycling ( A 2 ) (A2) ’ is the most effective method to reduce plastic waste. Finally, we compare the proposed method with the existing ones to demonstrate its validity.},
  archive      = {J_ASOC},
  author       = {Utpal Mandal and Mijanur Rahaman Seikh},
  doi          = {10.1016/j.asoc.2023.110516},
  journal      = {Applied Soft Computing},
  pages        = {110516},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval-valued spherical fuzzy MABAC method based on dombi aggregation operators with unknown attribute weights to select plastic waste management process},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust hesitant fuzzy partitional clustering algorithms and
their applications in decision making. <em>ASOC</em>, <em>145</em>,
110212. (<a href="https://doi.org/10.1016/j.asoc.2023.110212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hesitant fuzzy sets (HFSs) are a powerful tool to describe uncertain and vague information, whose relationship can be analyzed and mined by clustering algorithms . The partitional idea is widely used in clustering analysis for real-number data but little for hesitant fuzzy data. After pointing out the weakness of the existing partitional clustering algorithms between HFSs, we modify the existing partitional clustering algorithms and propose four new algorithms with two types under hesitant fuzzy environment. One type, including the hesitant fuzzy K-medians (HFKME) and the hesitant fuzzy C-medians (HFCME) algorithms, is based on median but not mean. The other type, including the improved hesitant fuzzy K-means (IHFKM) and the improved hesitant fuzzy C-means (IFHCM) algorithms, is on the basis of a new and more robust distance measure which will be developed by HFSs. We find that these four new clustering algorithms are more robust than the existing ones and have better performance under the noise environment. After that, we compare our proposed partitional clustering algorithms with each other and also compare them with the existing methods, and then apply them to multi-attribute decision making and cluster analysis. Numerical results show that HFCME has better performance than HFKME, and IHFCM is better than IHFKM.},
  archive      = {J_ASOC},
  author       = {Kun Chao and Hua Zhao and Zeshui Xu and Feng Cui},
  doi          = {10.1016/j.asoc.2023.110212},
  journal      = {Applied Soft Computing},
  pages        = {110212},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust hesitant fuzzy partitional clustering algorithms and their applications in decision making},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of consensus in group decision making under the CWW
environment. <em>ASOC</em>, <em>144</em>, 110557. (<a
href="https://doi.org/10.1016/j.asoc.2023.110557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing an evaluation matrix and selecting the optimal alternative are two crucial stages in any decision-making process. The development of an evaluation matrix involves two key steps: ensuring individual consistency and reaching group consensus. The latter is particularly effective in resolving decision-making problems, as it seeks to identify the best alternative based on the satisfaction levels of various decision-makers. This survey comprises three parts that outline current research and provide insights into future directions for group consensus measurements. Firstly, it revisits the two-part group consensus method, which involves the distance and similarity degree. Then, it highlights the challenges associated with the current developments in group consensus under the computing with words (CWW) environment. Finally, it offers suggestions for future research topics in the group consensus process.},
  archive      = {J_ASOC},
  author       = {Xuan Yao and Zeshui Xu},
  doi          = {10.1016/j.asoc.2023.110557},
  journal      = {Applied Soft Computing},
  pages        = {110557},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey of consensus in group decision making under the CWW environment},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A synergy of the adaptive whale optimization algorithm and
differential evolution for abrupt motion tracking. <em>ASOC</em>,
<em>144</em>, 110554. (<a
href="https://doi.org/10.1016/j.asoc.2023.110554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem that conventional trackers are difficult to adapt to abrupt motion, a tracking algorithm based on a synergy of the adaptive whale optimization algorithm and differential evolution (AWOA-DE) is proposed in this paper. Firstly, the non-linear dynamic feedback strategy based on the one-fifth principle is employed to replace the linear mechanism, which is the variation of exploring step in the basic whale optimization algorithm (WOA), and an adaptive WOA (AWOA) is generated to improve the global optimization ability of the WOA. Secondly, AWOA adopts a random sampling strategy to generate new samples, which could ensure the diversity of solutions, but may lead to instability. By mixing AWOA and DE algorithms , the population is divided into two groups, which are updated by AWOA and DE algorithms respectively, to ensure the diversity of the populations and improve the optimization accuracy of the algorithm. Finally, a visual tracker based on AWOA-DE is proposed, which can deal with the problem of abrupt motion in tracking. In addition, the effectiveness of the hybrid optimization algorithm is evaluated using 23 benchmark functions . In the experiment of visual tracking, extensive experimental results show that the AWOA-DE tracker (AWOADET) has much stronger competitiveness compared to the WOA tracker (WOAT) and the other 10 state-of-the-art trackers, especially for abrupt motion tracking.},
  archive      = {J_ASOC},
  author       = {Huanlong Zhang and Zeng Gao and Youmei Pan and Guosheng Yang and W.J. (Chris) Zhang and Jianing Wang},
  doi          = {10.1016/j.asoc.2023.110554},
  journal      = {Applied Soft Computing},
  pages        = {110554},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A synergy of the adaptive whale optimization algorithm and differential evolution for abrupt motion tracking},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A local binary social spider algorithm for feature
selection in credit scoring model. <em>ASOC</em>, <em>144</em>, 110549.
(<a href="https://doi.org/10.1016/j.asoc.2023.110549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Borrowers’ default is the main risk of online lending. Using credit scoring model to assess borrowers’ credit is an important means to reduce default risk. The existing related work focus on the improvement of assessment methods, and do not study the quality of credit data enough. Actually, there are usually some noisy, redundant or irrelevant features in online credit data, which increase the computational complexity and reduce the evaluation accuracy of the model. Well-performing feature selection method is the key premise to improve accuracy of the credit evaluation model. At present, the feature selection methods applied to online credit scoring generally have shortcomings such as subjectivity, time consuming, low accuracy, etc. So it is urgent to introduce new schemes. Among the feature selection schemes in many fields, heuristic algorithm is widely recognized because of its advantages of high efficiency and accuracy, and especially BinSSA is an outstanding representative. However, BinSSA has the defects that it is easy to form extreme distribution in initialization, and is inefficient because there is a high probability of falling into local optimum during iteration. So it is still not the best choice for feature selection in online lending credit scoring model. In this paper, we propose a local binary social spider algorithm (LBSA), which introduces two local optimization strategies into BinSSA: opposition-based learning (OBL) and improved local search algorithm (ILSA). These strategies can help to improve the above defects. We conduct comparative experiments based on three typical online credit data sets and different algorithms, to verify the superior performance of LBSA. The experimental results validate that LBSA greatly reduces the redundancy of returned feature subsets, improves the iterative stability, and makes the credit scoring model more accurate and effective.},
  archive      = {J_ASOC},
  author       = {Zaimei Zhang and Yitan Li and Yan Liu and Siming Liu},
  doi          = {10.1016/j.asoc.2023.110549},
  journal      = {Applied Soft Computing},
  pages        = {110549},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A local binary social spider algorithm for feature selection in credit scoring model},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Apis-prime: A deep learning model to optimize beehive
monitoring system for the task of daily weight estimation.
<em>ASOC</em>, <em>144</em>, 110546. (<a
href="https://doi.org/10.1016/j.asoc.2023.110546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents Apis-Prime , 2 a hybrid deep learning model for soft sensing and time series forecasting, to estimate the daily weight variations of honeybee hives. Apis-Prime improves the state-of-the-art of earlier proposed WE-Bee (Anwar et al., 2022), and also helps optimize the beehive monitoring systems for the task of daily weight variation estimation. Weight variations of a honeybee hive are the most important indicator of hive productivity, and the health and strength of a bee colony. Currently, precise measurement of the weight of a hive requires an expensive weighing scale under each hive. On the other hand, sensors deployed inside the hive are cheaper than a weighing scale, and are shielded from the extreme weather variations outside the hive. In this work, honeybee activity is monitored using data from sensors inside the hive, along with monitoring the information related to the seasons, time of the day, weather, and the size of the hive. Apis-Prime’s deep learning algorithm is based on two self-attention encoders, which collectively transform the sensor data into daily weight variations of the hive. Two parallel encoders simultaneously pay attention to time-based relationships and feature-based relationships within the daily sensor data and generate daily hive weight estimates with better accuracy. The comparison shows an average error of 19.7 grams/frame for Apis-Prime , compared to 21.05 grams/frame for the earlier proposed model WE-Bee . For system optimization, this work uses the attention weights of trained encoders of Apis-Prime to evaluate the sensor features collected by the monitoring system. This evaluation is used to identify and remove the unnecessary sensors/features from the dataset, reducing the number of features from 36 to 23, hence providing a significant optimization of cost, power, and data bandwidth. We provide a performance analysis of beehive weight estimations by Apis-Prime using the complete, as well as the optimized dataset on 2, 170 days of beehive sensor recordings. Equally good results of daily weight estimations using the optimized feature set demonstrate the efficacy of the proposed model for the optimization of the beehive monitoring system for the task of hive weight estimation.},
  archive      = {J_ASOC},
  author       = {Omar Anwar and Adrian Keating and Rachel Cardell-Oliver and Amitava Datta and Gino Putrino},
  doi          = {10.1016/j.asoc.2023.110546},
  journal      = {Applied Soft Computing},
  pages        = {110546},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Apis-prime: A deep learning model to optimize beehive monitoring system for the task of daily weight estimation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Deep reinforcement learning for optimal rescue path
planning in uncertain and complex urban pluvial flood scenarios.
<em>ASOC</em>, <em>144</em>, 110543. (<a
href="https://doi.org/10.1016/j.asoc.2023.110543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An urban pluvial flood is a devastating, costly natural disaster requiring effective rescue path planning to mitigate the loss of lives and property. The inherent uncertainty and complexity of the risks associated with urban flooding limit the ability to plan optimal rescue paths that prioritize both timeliness and safety. This study addresses the challenge by proposing an innovative assessment methodology to output risk values and probability representing safety and timeliness in each passable area while simulating real-world flood scenarios. Furthermore, the paper develops a pioneering path-planning algorithm based on deep reinforcement learning , incorporating improved stochastic reward exploitation and heterogeneous reward exploration mechanisms to function in a simulation rescue path-planning scenario with uncertainty and complexity. According to the findings, the proposed algorithm outperforms current state-of-the-art algorithms in converging to the optimal path, fully sampling, and running efficiency. The study contributes to theoretical progress on urban pluvial flood rescue, deep reinforcement learning , risk assessment, and decision intelligence while offering practical implications for smart cities, emergency management, and optimizing real-world problems by employing artificial intelligence .},
  archive      = {J_ASOC},
  author       = {Xiaoyan Li and Xuedong Liang and Xia Wang and Rong Wang and Lingli Shu and Wentao Xu},
  doi          = {10.1016/j.asoc.2023.110543},
  journal      = {Applied Soft Computing},
  pages        = {110543},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning for optimal rescue path planning in uncertain and complex urban pluvial flood scenarios},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient lexicographic approach to solve multi-objective
multi-port fabric dyeing machine planning problem. <em>ASOC</em>,
<em>144</em>, 110541. (<a
href="https://doi.org/10.1016/j.asoc.2023.110541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the fabric dyeing process in a towel manufacturing factory is investigated, focusing on the batching of towel fabric bolts with different quantities and features belonging to different orders in multi-port dyeing machines. This research differs from previous studies in several aspects. Firstly, it is considered three different objectives: total tardiness, total number of washes (sequence-dependent setups), and total machine fixed cost. These objectives have not been simultaneously optimized in previous studies, making our approach unique. Secondly, we adopt a lexicographic optimization approach to prioritize these objectives according to the company’s requirements. This allows to find solutions that are optimal in terms of one objective while satisfying the constraints imposed by the other objectives. Lastly, it is proposed a novel lexicographic multi-objective genetic algorithm , which extends the traditional single-objective genetic algorithm , to effectively solve large-sized problems. Overall, our study not only addresses a real-life problem in the textile industry but also introduces innovative techniques and approaches for optimizing multiple objectives in a scheduling context.},
  archive      = {J_ASOC},
  author       = {Yunus Demi̇r},
  doi          = {10.1016/j.asoc.2023.110541},
  journal      = {Applied Soft Computing},
  pages        = {110541},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient lexicographic approach to solve multi-objective multi-port fabric dyeing machine planning problem},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from hybrid labels with partial labels via
hybrid-grained contrast regularization. <em>ASOC</em>, <em>144</em>,
110533. (<a href="https://doi.org/10.1016/j.asoc.2023.110533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from hybrid labels is suitable for dealing with the real-world scenario, where the labels of the training dataset include fine-grained labels and coarse-grained labels. Unfortunately, obtaining fine-grained labels with strong supervision information is difficult. Compared with the more difficult-to-obtain fine-grained labels, it is labor-saving and efficient for the annotator to remove several impossible options to obtain partial labels, which also provide more information than coarse-grained labels. Nonetheless, the goal of existing methods of PLL is only to learn from partially labeled data but cannot utilize fine-grained labels, coarse-grained labels, and partial labels to train an ordinary supervised classifier. In this paper, we propose a novel setting called learning from hybrid labels with partial labels (LHLP) and propose a method to learn a classifier for identifying difficult-to-recognize images by using hybrid labels with partial labels. Specifically, we drive hybrid-grained contrast regularization (HGCR) for exploring the relationship between sample labels, which uses fine-grained label information and coarse-grained label information to disambiguate labels respectively. Extensive experiments on benchmark validate the effectiveness of the proposed method HGCR in LHLP, compare to the state-of-art methods on the CIFAR-100, Kuzushiji-MNIST and Fashion-MNIST datasets, our method achieves promising results.},
  archive      = {J_ASOC},
  author       = {Xinzheng Xu and Jian Zhang and Zhongnian Li},
  doi          = {10.1016/j.asoc.2023.110533},
  journal      = {Applied Soft Computing},
  pages        = {110533},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning from hybrid labels with partial labels via hybrid-grained contrast regularization},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative emergency decision-making for public health
events: An integrated BWM-TODIM approach with multi-granularity extended
probabilistic linguistic term sets. <em>ASOC</em>, <em>144</em>, 110531.
(<a href="https://doi.org/10.1016/j.asoc.2023.110531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carrying out emergency rescue in coordination with multiple organizations, is of great significance for the effectiveness and timeliness of emergency response. Therefore, this study aims to extend a novel practical tool for distinguishing the optimal combination of different emergency plans in multiple organizations. In a multi-granularity extended probabilistic linguistic term sets (MGEPLTSs) environment, we propose a new collaborative emergency decision-making (CEDM) approach in the inspiration of the best–worst method (BWM) and TOmada de Decisão Iterativa Multicritério (TODIM) method. Firstly, a combined multi-granularity and extended probabilistic linguistic term sets, namely MGEPLTSs, are proposed to quantify the preferences given by decision makers (DMs) to address the issues on potential ambiguity and uncertainty in actual CEDM. Then, the BWM is introduced to the MGEPLTSs environment to compute the index weights of the individual and collaborative performance evaluation for multi-plan combinations, by building the fuzzy mathematical programming model respectively. Finally, we develop the multi-granularity extended probabilistic linguistic TODIM method to calculate the overall dominance of indexes considering the psychological behavior of DMs, thereby achieving the ranking of multi-plan combinations. A CEDM case on COVID-19 epidemic is used to illustrate the feasibility of the proposed approach, and the sensitivity analysis and comparative analysis with other similar approaches are presented to demonstrate its effectiveness and superiority.},
  archive      = {J_ASOC},
  author       = {Xia Wang and Xue-dong Liang and Xiao-yan Li and Peng Luo},
  doi          = {10.1016/j.asoc.2023.110531},
  journal      = {Applied Soft Computing},
  pages        = {110531},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative emergency decision-making for public health events: An integrated BWM-TODIM approach with multi-granularity extended probabilistic linguistic term sets},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPEA2 based on grid density search and elite guidance for
multi-objective operation optimization of wastewater treatment process.
<em>ASOC</em>, <em>144</em>, 110529. (<a
href="https://doi.org/10.1016/j.asoc.2023.110529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is challenging to introduce an optimization method to improve the operation performance of wastewater treatment process (WWTP) on account of its high energy consumption and poor water quality characteristics. In this paper, an improved Strength Pareto Evolutionary Algorithm 2 (SPEA2) based on grid density search and elite guidance (GDSEG-SPEA2) is proposed for multi-objective operation optimization of WWTP. First, the external archive is divided by an improved adaptive grid method to determine the distribution density of the solutions, and a neighborhood circle strategy as well as a mixed perturbation strategy are designed to search the neighborhood of sparse and crowded solutions, resulting in a more uniformly distributed Pareto front . Then, in order to avoid SPEA2 falling into local optimum after adding the neighborhood search strategies, the crossover and mutation operations based on individual information are proposed to generate new individuals. Finally, an elite guidance strategy, in which the poor-performing individuals of the population learn from the best-performing individuals to update their positions, is introduced into the algorithm to improve convergence. The test function verification shows that the proposed algorithm can obtain the Pareto front with better distribution and convergence than other existing algorithms. The operation optimization control experiment of WWTP shows that the proposed algorithm can better purify water quality and reduce more energy consumption on the premise of ensuring that effluent quality meets the discharge standards, which is better than other optimization algorithms in comparison.},
  archive      = {J_ASOC},
  author       = {Ping Zhou and Hongpeng Li and Tianyou Chai},
  doi          = {10.1016/j.asoc.2023.110529},
  journal      = {Applied Soft Computing},
  pages        = {110529},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SPEA2 based on grid density search and elite guidance for multi-objective operation optimization of wastewater treatment process},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Daily prediction method of dust accumulation on photovoltaic
(PV) panels using echo state network with delay output. <em>ASOC</em>,
<em>144</em>, 110528. (<a
href="https://doi.org/10.1016/j.asoc.2023.110528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dust accumulation over time can be one of the main causes of uncertainty in the output of photovoltaic (PV) systems. In order to better understand these losses, this paper established a daily dust accumulation prediction model for PV panels based on the delay output echo state network (DESN). A pigeon-inspired optimization (PIO) algorithm with adaptive Cauchy (AC) mutation strategy was proposed, which can optimize the reservoir parameters (such as leakage rate, spectral radius , and input scaling) of DESN, shorten the solution time, and improve search speed. Based on typical meteorological and air quality data, as well as daily accumulated dust weight recorded from the experimental platform, model training and testing were carried out. According to the Pearson correlation coefficient, the relationship between the environment parameters (humidity, wind speed, wind direction, PM2.5, PM10, and rainfall) and the dust accumulation was obtained. The results show that the prediction accuracy of AC-PIO-DESN is better than other methods for meteorological and air quality data. The mean absolute percentage error (MAPE) for humidity, irradiance, PM2.5 and PM10 were 4.0743\%, 4.4958\%, 10.6231\% and 12.8402\%, respectively. In addition, the proposed daily dust prediction model has good robustness for 10-day samples, with an average relative error ranging from 0.65\% to 54\%. This method can provide data support for grid scheduling and PV panel cleaning strategy of PV power plants.},
  archive      = {J_ASOC},
  author       = {Siyuan Fan and Mingyue He and Zhenhai Zhang},
  doi          = {10.1016/j.asoc.2023.110528},
  journal      = {Applied Soft Computing},
  pages        = {110528},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Daily prediction method of dust accumulation on photovoltaic (PV) panels using echo state network with delay output},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel mayfly algorithm for the α-neighbor p-center
problem. <em>ASOC</em>, <em>144</em>, 110527. (<a
href="https://doi.org/10.1016/j.asoc.2023.110527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a competitive algorithm in which the mayfly optimization algorithm (MA) is combined with a local search procedure to provide high-quality solutions to the α α -neighbor p -center problem ( α α – p CP). The aim of this problem is to locate p facilities on a plane such that the maximum distance between each demand point and its α th αth closest facility is minimized. Although α α – p CP is a discrete optimization problem , it is addressed in this study from the perspective of a continuous optimization problem . The major reason for this choice is that the MA is a very suitable technique for finding the optimum point on a plane. However, it is very difficult to successfully apply this technique to a discrete optimization problem. Hence, additional efforts are made to reduce the computational burden that arises from approaching this from the perspective of a continuous problem. Thus, an effective solution method based on the MA for finding the optimum point on a plane is presented for the discrete α α – p CP. The proposed MA method is compared to the best alternatives found in the literature, which are an extremely efficient exact procedure for the continuous variant of the problem, and a GRASP algorithm that includes a tabu search with strategic oscillation (SO) post-processing. A comparative analysis shows that the performance of the proposed algorithm is similar to that of these state-of-the-art methods. A more extensive comparison shows that the two best performing algorithms, the MA and the SO, complement each other. In addition, the proposed method performs very well for cases where the number of facilities are low. If we consider only the cases where p ≤ ≤ 50, we see that the MA performs best 5 times, 14 times and 14 times for α α = = 1, 2, 3, respectively, and loses twice, three times, and three times.},
  archive      = {J_ASOC},
  author       = {Tunchan Cura},
  doi          = {10.1016/j.asoc.2023.110527},
  journal      = {Applied Soft Computing},
  pages        = {110527},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel mayfly algorithm for the α-neighbor p-center problem},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous attention based transformer for sign language
translation. <em>ASOC</em>, <em>144</em>, 110526. (<a
href="https://doi.org/10.1016/j.asoc.2023.110526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language translation (SLT) has attracted significant interest both from research and industry, enabling convenient communications with the deaf-mute community. While recent transformer-based models have shown improved sign translation performance, it is still under-explored how to design an efficient transformer-based deep network architecture that effectively extracts joint visual-text features by exploiting multi-level spatial and temporal contextual information. In this paper, we propose heterogeneous attention based transformer(HAT), a novel SLT model to generate attentions from diverse spatial and temporal contextual levels. Specifically, the proposed light dual-stream sparse attention-based module yields more effective visual-text representations compared to conventional transformers. Extensive experiments demonstrate that our HAT achieves state-of-the-art performance on the challenging PHOENIX2014T benchmark dataset with a BLEU-4 score of 25.33 on the test set.},
  archive      = {J_ASOC},
  author       = {Hao Zhang and Yixiang Sun and Zenghui Liu and Qiyuan Liu and Xiyao Liu and Ming Jiang and Gerald Schafer and Hui Fang},
  doi          = {10.1016/j.asoc.2023.110526},
  journal      = {Applied Soft Computing},
  pages        = {110526},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogeneous attention based transformer for sign language translation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective chaos game optimization algorithm based on
decomposition and random learning mechanisms for numerical optimization.
<em>ASOC</em>, <em>144</em>, 110525. (<a
href="https://doi.org/10.1016/j.asoc.2023.110525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaos Game Optimization (CGO) is a heuristic optimization approach that estimates global optima for optimization problems using operators based on chaos theory. This paper first proposes a multi-objective variant of this recent algorithm using decomposition. The proposed algorithm is called Multi-Objective CGO based on Decomposition (MOCGO/D), in which the decomposition step employs a Normalized Boundary Intersection (NBI) technique for decomposing the multi-objective problem into single-objective sub-problems. A novel Random Learning (RL) strategy based on the combination of multiple strategies such as Opposition-based learning, Levy flight operator, Orthogonal learning, and the tangent flight operator, is incorporated in the MOCGO/D to propose an enhanced version called MOCGO/DR algorithm. The RL strategy aims to improve the balance between exploitation and exploration of the conventional CGO, leading to better convergence behavior and avoiding getting trapped in local optima. The first set of experimental results demonstrates that MOCGO/DR can perform better than three other variants of MOCGO, namely archive-based MOCGO (MOCGO/A), crowding-distance based MOCGO (MOCGO/CD) and decomposition-based MOCGO (MOCGO/D) on 62\% of test cases. A second set of experiments and evaluation shows that the proposed approach provides better results than well-regarded algorithms, including strength pareto evolutionary algorithm (SPEA2), multiobjective evolutionary algorithm based on decomposition (MOEA/D), multi-objective particle swarm optimization algorithm based on decomposition (MPSO/D), multistage evolutionary algorithm (MSEA), and a fast and elitist multi- objective genetic algorithm (NSGAII) when using performance measures such as GD, IGD, HV, Spacing, Spread, and Hausdorff distance on 65\% test cases. This two-stage evaluation was conducted on three different benchmark test sets: the Deb–Thiele–Laumanns–Zitzler (DTLZ) test suite, the Zitzler–Deb–Thiele (ZDT) test suite, and the bias test suite (BT). Overall, the Friedman test results for all performance measures show that MOCGO/DR is demonstrated to be a competitive candidate as a multi-objective optimization algorithm in this space.},
  archive      = {J_ASOC},
  author       = {Salma Yacoubi and Ghaith Manita and Amit Chhabra and Ouajdi Korbaa and Seyedali Mirjalili},
  doi          = {10.1016/j.asoc.2023.110525},
  journal      = {Applied Soft Computing},
  pages        = {110525},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective chaos game optimization algorithm based on decomposition and random learning mechanisms for numerical optimization},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point cloud registration based on the dark forest algorithm
and its application in coal industry. <em>ASOC</em>, <em>144</em>,
110524. (<a href="https://doi.org/10.1016/j.asoc.2023.110524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Being sensitive to initial position and noise is the key problem that must be solved in point cloud registration. To improve the overall performance of registration methods, a point cloud registration method based on the Dark Forest Algorithm is proposed in this paper. The introduction of swarm intelligence enhances the adaptability of the registration method to the initial position and improves the robustness of the point cloud registration system. Furthermore, the application of the Dark Forest Algorithm with four capability enhancement strategies: the hierarchical elite, civilization self-decision, suspicion chain, and technology explosion has improved the comprehensive competitiveness in point cloud registration. The mentioned strategies have brought a great positive impact on the registration methods in solving the far initial position, breaking away from the local optimal solution , and weakening the noise interference. The paper aims to provide the Dark Forest Algorithm and the point cloud registration method based on it, which are elaborated on in detail and compared to other existing algorithms to prove the comprehensive performance, respectively. Then, the point cloud registration experiment for coal-wall point clouds was carried out and the results indicate that the proposed registration algorithm is better than other algorithms for standard point cloud dataset and coal-wall point cloud, which demonstrates that the proposed registration method has strong potential competitiveness.},
  archive      = {J_ASOC},
  author       = {Dong Wei and Houzhe Wang and Lei Si and Chao Tan and Xinhua Liu and Haifeng Yan},
  doi          = {10.1016/j.asoc.2023.110524},
  journal      = {Applied Soft Computing},
  pages        = {110524},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Point cloud registration based on the dark forest algorithm and its application in coal industry},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-scale contrastive learning for multi-behavior
recommendation. <em>ASOC</em>, <em>144</em>, 110523. (<a
href="https://doi.org/10.1016/j.asoc.2023.110523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-behavior recommendation (MBR) aims to improve the prediction of the target behavior (i.e., purchase) by exploiting multi-typed auxiliary behaviors, such as page view, cart and favorite. Recently, leveraging Graph Neural Networks (GNNs) to capture collaborative signals has been the mainstream paradigm for MBR. However, GNN-based MBR suffers from data sparsity in real-world scenarios and thus performs mediocrely. Excitingly, contrastive learning which can mine additional self-supervised signals from raw data, holds great potential to alleviate this problem. Naturally, we seek to exploit contrastive learning to enhance MBR, while two key challenges have yet to be addressed: (i) Difficult to learn reliable representations under different behaviors; (ii) Sparse supervised signals under target behavior. To tackle the above challenges, in this paper, we propose a novel D ual- S cale C ontrastive L earning (DSCL) framework. Unlike traditional contrastive learning methods that artificially construct two views through data augmentation , we comprehensively consider two different views for MBR, including the collaborative view and the semantic view. Specifically, we regard the user–item graph as a collaborative view and the user–user graph as a semantic view. In particular, we develop two novel contrastive learning objectives at two scales. For the first challenge, we devise local-to-context contrastive learning within behaviors on collaborative view, which enhances the representation learning by incorporating potential neighbors into the contrastive learning from the graph topological space and the semantic space , respectively. As for the second challenge, we design local-to-local contrastive learning across behaviors on a semantic view, which has the benefit of capturing commonalities between different behaviors and integrating them into the target behavior to alleviate the sparse supervised signal problem of the target behavior. In addition, we also propose an adaptive weight network to efficiently customize the integration of all losses. Extensive experiments on three real-world benchmark datasets show that our proposed DSCL is significantly superior to various state-of-the-art recommendation methods.},
  archive      = {J_ASOC},
  author       = {Qingfeng Li and Huifang Ma and Ruoyi Zhang and Wangyu Jin and Zhixin Li},
  doi          = {10.1016/j.asoc.2023.110523},
  journal      = {Applied Soft Computing},
  pages        = {110523},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-scale contrastive learning for multi-behavior recommendation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intuitionistic fuzzy EM-SWARA-TOPSIS approach based on new
distance measure to assess the medical waste treatment techniques.
<em>ASOC</em>, <em>144</em>, 110521. (<a
href="https://doi.org/10.1016/j.asoc.2023.110521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the potential harm to public health as well as the environment, medical waste treatment have emerged as a critical concern, especially in developing nations. The selection of a suitable medical waste treatment technique (MWTT) is now characterized as a critical multi-criteria decision-making (MCDM) problem because of the incorporation of several competing criteria. This study provides a hybrid MCDM technique for assessing MWTTs concerning social, environmental, economic, and technical criteria in an intuitionistic fuzzy environment. The intuitionistic fuzzy set (IFS) is a potent tool for expressing ambiguous and vague information efficiently and has proven effective for MCDM problems. In IFS environment a new distance measure and an entropy measure (EM) are introduced by employing the generalized Csiszar f-divergence measure, which are further used to construct the MCDM algorithm. The introduced entropy measure is used to examine the objective weights of criteria, and the distance measure is employed to construct the technique for order performance by similarity to ideal solution (TOPSIS) for the assessment of the alternatives. Furthermore, the significance of the criteria for the proposed MCDM technique is evaluated by integrated weights (combining objective and subjective weights). The Stepwise Weight Assessment Ratio Analysis (SWARA) technique is used to obtain the subjective weights for criteria. Thus, a hybrid MCDM technique, namely IF-EM-SWARA-TOPSIS, is developed. To exhibit the effectiveness and applicability of the suggested framework, a MWTT selection problem in Indian context is depicted. The comparative study and sensitivity assessment are conducted to show the sustainability and stability of the proposed IF-EM-SWARA-TOPSIS technique for ranking the preference order of the MWTTs. Moreover, the practical implications of the proposed method are discussed.},
  archive      = {J_ASOC},
  author       = {Anjali Patel and Subhankar Jana and Juthika Mahanta},
  doi          = {10.1016/j.asoc.2023.110521},
  journal      = {Applied Soft Computing},
  pages        = {110521},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intuitionistic fuzzy EM-SWARA-TOPSIS approach based on new distance measure to assess the medical waste treatment techniques},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascade forest regression algorithm for non-invasive blood
pressure estimation using PPG signals. <em>ASOC</em>, <em>144</em>,
110520. (<a href="https://doi.org/10.1016/j.asoc.2023.110520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The regression and classification performances of deep-learning algorithms depend on the tuning of the model and require significant intervention to train the model. A blood pressure prediction (BP) method – which is an improved cascade forest regression method based on a deep forest framework – is proposed in this study to mitigate the adverse effects of hyperparameters on deep learning algorithms and human errors. The impact of hyperparameters on the model can be reduced using random forest regression and extra tree regression as base estimators and by applying them to bootstrap aggregation strategies for learning data. The photoplethysmography (PPG) data and ambulatory blood pressure obtained from the MIMIC II database were evenly divided into 5 s segments to accurately estimate blood pressure. Nonlinear indices of the time domain, frequency domain, and heart rate variability were obtained from the PPG signal as training characteristic values. This process is similar to collecting data directly from a wearable device for rapid blood pressure prediction. This improves the predictive performance of the model and reduces additional memory consumption. Our proposed algorithm achieved mean absolute errors of 1.760 and 2.896 mmHg for systolic blood pressure (SBP) and diastolic blood pressure (DBP), respectively. Additionally, our best model achieved R 2 scores of 0.948 and 0.926 for SBP and DBP, respectively. According to the Association for the Advancement of Medical Instrumentation standards, the standard deviation and mean error of the predicted results for systolic and diastolic BPs were within the standard range. According to the British Hypertension Society standard, the results of the proposed algorithm reached grade A. This study confirmed the possibility of developing an algorithm that can accurately estimate blood pressure.},
  archive      = {J_ASOC},
  author       = {Gengjia Zhang and Siho Shin and Jaehyo Jung},
  doi          = {10.1016/j.asoc.2023.110520},
  journal      = {Applied Soft Computing},
  pages        = {110520},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cascade forest regression algorithm for non-invasive blood pressure estimation using PPG signals},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic evolutionary game and simulation with embedded
pricing model for channel selection in shipping supply chain.
<em>ASOC</em>, <em>144</em>, 110519. (<a
href="https://doi.org/10.1016/j.asoc.2023.110519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the “Internet +” era, the shipping industry , which is struggling to survive due to economic weakness and excess capacity , is also actively seeking transformation to enhance core competitiveness of enterprises. This paper embeds the evolutionary game theory in the pricing model to explore the dynamic interaction mechanism between shipping companies and freight forwarders in channel selection. The conditions for shipping companies to choose different channel strategies in the long-term dynamic evolution process are derived. Then, we apply system dynamics to simulate the evolution model, and analyzes the influence of factors such as channel preference, operating cost and perceived loss on the dynamic evolution process. It is concluded that shipping companies and forwarders have different channel choices at different development stages, and when shipping companies choose to invade online platform, forwarders are motivated to choose online channel to obtain lower freight rates . When the channel preference of forwarders is moderate, the two channels coexist; when the channel preference of forwarders is high, the final stabilization strategy evolves to only the online channel. Furthermore, the channel preference of shippers has a greater impact on the evolutionary mechanism than that of forwarders.},
  archive      = {J_ASOC},
  author       = {Qiaoyu Peng and Chuanxu Wang},
  doi          = {10.1016/j.asoc.2023.110519},
  journal      = {Applied Soft Computing},
  pages        = {110519},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic evolutionary game and simulation with embedded pricing model for channel selection in shipping supply chain},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative recommendation model based on multi-modal
multi-view attention network: Movie and literature cases. <em>ASOC</em>,
<em>144</em>, 110518. (<a
href="https://doi.org/10.1016/j.asoc.2023.110518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing collaborative recommendation models that use multi-modal information emphasize the representation of users’ preferences but easily ignore the representation of users’ dislikes. Nevertheless, modelling users’ dislikes facilitates comprehensively characterizing user profiles. Thus, the representation of users’ dislikes should be integrated into the user modelling when we construct a collaborative recommendation model. In this paper, we propose a novel C ollaborative R ecommendation M odel based on M ulti-modal multi-view A ttention N etwork (CRMMAN), in which the users are represented from both preference and dislike views. Specifically, the users’ historical interactions are divided into positive and negative interactions, used to model the user’s preference and dislike views, respectively. Furthermore, the semantic and structural information extracted from the scene is employed to enrich the item representation. We validate CRMMAN by designing contrast experiments based on two benchmark MovieLens-1M and Book-Crossing datasets. Movielens-1 m has about a million ratings, and Book-Crossing has about 300, 000 ratings. Compared with the state-of-the-art knowledge-graph-based and multi-modal recommendation methods, the AUC, NDCG@5 and NDCG@10 are improved by 2.08\%, 2.20\% and 2.26\% on average of two datasets. We also conduct controlled experiments to explore the effects of multi-modal information and multi-view mechanism. The experimental results show that both of them enhance the model’s performance.},
  archive      = {J_ASOC},
  author       = {Zheng Hu and Shi-Min Cai and Jun Wang and Tao Zhou},
  doi          = {10.1016/j.asoc.2023.110518},
  journal      = {Applied Soft Computing},
  pages        = {110518},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative recommendation model based on multi-modal multi-view attention network: Movie and literature cases},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A space-reduction based three-phase approach for
large-scale optimization. <em>ASOC</em>, <em>144</em>, 110517. (<a
href="https://doi.org/10.1016/j.asoc.2023.110517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of science and technology, large-scale global optimization (LSGO) has received more and more attention. However, large-scale problem is very challenging due to its high dimensionality , nonlinearity , and countless local optimal solutions . In this paper, a space-reduction based three-phase approach (SRTP) is proposed to solve this problem. In the first phase, a space-reduction based line search method is designed to roughly locate good solutions as well as to explore important information such as promising search regions and characteristics of each variable (dimension). In the second phase, a multi-grouping strategy is proposed for fully non-separable large-scale problems. This new decomposition method provides dozens of grouping results for non-separable large-scale problems that not only reduces the dimensionality but also maintains the correlation of variables to the most extent. Then in the third phase, a space-reduction based group search method is designed to optimize the sub-groups. By decomposing the large-scale problem as well as reducing the huge search space to focus on promising areas, this group search method can gain better efficiency. Experiments are conducted on 35 widely used benchmark functions and the proposed SRTP is compared with state-of-the-art LSGO algorithms . Experimental results show that SRTP is effective and efficient.},
  archive      = {J_ASOC},
  author       = {Haiyan Liu and Yuan Cheng and Siyan Xue and Shouheng Tuo},
  doi          = {10.1016/j.asoc.2023.110517},
  journal      = {Applied Soft Computing},
  pages        = {110517},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A space-reduction based three-phase approach for large-scale optimization},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-criteria group decision-making method in disposal of
municipal solid waste based on cubic pythagorean fuzzy EDAS approach
with incomplete weight information. <em>ASOC</em>, <em>144</em>, 110515.
(<a href="https://doi.org/10.1016/j.asoc.2023.110515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Municipal solid waste management (MSWM) has always been a typical issue in India. The rapid population growth, impetuous adoption of modern lifestyle, and fast urbanization resulted in India’s swift generation of complicated municipal solid waste (MSW). Consequently, municipalities of different cities in India face various difficulties in MSW collection, treatment and disposal methodology. Inefficient or poorly managed MSW causes numerous troubles, including air pollution, soil fouling, contamination in drinking water and health hazard, and it also impacts socio-economic deterioration. MSWM technique selection is based on several criteria, which are generally imprecise, uncertain, and vague. This paper uses the cubic Pythagorean fuzzy number to comprise the fuzzy characteristics of the criteria value concerning the five alternatives (Thermochemical methods in MSW treatment and disposal). The best MSW treatment and disposal method is selected amongst the five alternatives under the novel multi-criteria group decision-making (MCGDM) approach, the cubic Pythagorean fuzzy EDAS (Evaluation based on Distance from Average Solution) in the Indian context. The weight information of the criteria has yet to be wholly known. A non-linear optimization scheme is proposed to determine the criteria weights that are partially known to maximize the generalized total distance measure between alternatives corresponding to all criteria. The present method is explained with a case study. Finally, a comparative analysis is drawn between the present and some existing methods, showing that the present method is consistent and robust enough to apply in practice.},
  archive      = {J_ASOC},
  author       = {Tapas Kumar Paul and Chiranjibe Jana and Madhumangal Pal},
  doi          = {10.1016/j.asoc.2023.110515},
  journal      = {Applied Soft Computing},
  pages        = {110515},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria group decision-making method in disposal of municipal solid waste based on cubic pythagorean fuzzy EDAS approach with incomplete weight information},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). A hybrid biogeography-based optimization algorithm to solve
high-dimensional optimization problems and real-world engineering
problems. <em>ASOC</em>, <em>144</em>, 110514. (<a
href="https://doi.org/10.1016/j.asoc.2023.110514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to our extensive investigation, Biogeography-based optimization (BBO) and its variants have not been applied to solve high-dimensional optimization problems . To make a breakthrough in this field, a new BBO variant with hybrid migration operator and feedback differential evolution mechanism, HFBBO, is proposed. Firstly, the example learning method is used to ensure the inferior solutions cannot destroy the superior solutions. Secondly, the hybrid migration operator is presented to balance the exploration and exploitation. It enables the algorithm to switch freely between local search and global search. Finally, the feedback differential evolution mechanism is designed to replace the random mutation operator . HFBBO can select the mutation mode intelligently by this mechanism to avoid getting stuck in local optima. Meanwhile, the Markov model is established to prove the convergence of HFBBO, and the complexity is also discussed. A series experiments are carried out on 24 benchmark functions , CEC2017 test suite and 12 real-world engineering problems. The results of the Wilcoxon’s rank-sum test and Friedman’s test show that HFBBO has better competitiveness and stability than the 27 compared algorithms. Furtherly, the performance of HFBBO is compared on 1000, 2000, 5000 and 10000 dimensions, respectively. Experimental results show that this method can effectively solve high-dimensional optimization problems .},
  archive      = {J_ASOC},
  author       = {Ziyu Zhang and Yuelin Gao and Yingchun Liu and Wenlu Zuo},
  doi          = {10.1016/j.asoc.2023.110514},
  journal      = {Applied Soft Computing},
  pages        = {110514},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid biogeography-based optimization algorithm to solve high-dimensional optimization problems and real-world engineering problems},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random following ant colony optimization: Continuous and
binary variants for global optimization and feature selection.
<em>ASOC</em>, <em>144</em>, 110513. (<a
href="https://doi.org/10.1016/j.asoc.2023.110513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous ant colony optimization was a population-based heuristic search algorithm inspired by the pathfinding behavior of ant colonies with a simple structure and few control parameters. However, in the case of multimodal and high-dimensional optimization problems , it was often limited to local regions in the feasible domain space, negatively affecting the computational effort required to find the optimal solution point. To alleviate its limitations in this regard, a random following strategy is proposed to enhance communication among the ant colony search agent and other ant colony members within the search dimension. The proposed algorithm that incorporates this strategy is called Random Following Ant Colony Optimization. Then, to evaluate the global optimization performance of the proposed algorithm, the well-known numerical optimization problem, namely the Congress on Evolutionary Computation 2017 test suite, is used. First, the proposed algorithm’s parameters are analyzed for sensitivity, scalability experiments, and balanced diversity. Second, it is compared experimentally with 11 state-of-the-art algorithms in dimensions 10, 30, 50, and 100, respectively, and Wilcoxon signed-rank test, Friedman test, and Bonferroni-Dunn post-hoc statistical test are used to synthesize the experimental comparison results. Finally, to evaluate the ability of the proposed algorithm to handle discrete feature selection problems, comparative experiments are conducted on 24 datasets with eight well-known classification methods and five high-performance classification methods. The benchmark test results show that the global optimization performance of the proposed algorithm is comparable to the winners of the test suite in 50 and 100 dimensions. The results of the feature selection experiments show that the proposed algorithm is much stronger than the well-known and high-performance classification methods on high-dimensional datasets.},
  archive      = {J_ASOC},
  author       = {Xinsen Zhou and Wenyong Gui and Ali Asghar Heidari and Zhennao Cai and Guoxi Liang and Huiling Chen},
  doi          = {10.1016/j.asoc.2023.110513},
  journal      = {Applied Soft Computing},
  pages        = {110513},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Random following ant colony optimization: Continuous and binary variants for global optimization and feature selection},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online auto-tuning of multiresonant current controller with
nature-inspired optimization algorithms and disturbance in the loop
approach. <em>ASOC</em>, <em>144</em>, 110512. (<a
href="https://doi.org/10.1016/j.asoc.2023.110512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel tuning method of a complex multiresonant current controller (MRCC). An online auto-tuning process is proposed to provide optimal coefficients suitable for the real-time operation of a grid-connected power converter. The time-consuming and inaccurate simulation stage based on the model of the plant is omitted. A novel function of constraints has been introduced into the optimization scheme to ensure the desired behavior of the control system. In this solution, the noise level of a control signal and the rise time of controlled currents are directly defined. To the author’s best knowledge, it is the first time when such a solution is proposed. Next, the most commonly used meta-heuristic optimization algorithms were applied to solve this particular optimization problem . Another original concept presented in the paper is the application of the disturbance-in-the-loop (DiL) approach. In this method, the distorted phase voltages are emulated during the tuning stage, and optimal coefficients of MRCC are selected in the off-grid operation of the power converter. The proposed solution ensures safe and efficient online tuning of the MRCC for an extremely simple performance index. As a result, the high-performance operation of the grid-connected inverter is achieved. A brief stability analysis based on Lyapunov’s stability theory is included. Experimental results obtained for grid-connected inverter indicate superior disturbance attenuation and current tracking. Advantages of the proposed approach are exhibited when compared to two analytical reference solutions.},
  archive      = {J_ASOC},
  author       = {Tomasz Tarczewski and Djordje Stojic and Rafal Szczepanski and Lukasz Niewiara and Lech M. Grzesiak and Xiaosong Hu},
  doi          = {10.1016/j.asoc.2023.110512},
  journal      = {Applied Soft Computing},
  pages        = {110512},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online auto-tuning of multiresonant current controller with nature-inspired optimization algorithms and disturbance in the loop approach},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic diagnosis of COVID-19 from CT images using
CycleGAN and transfer learning. <em>ASOC</em>, <em>144</em>, 110511. (<a
href="https://doi.org/10.1016/j.asoc.2023.110511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of the corona virus disease (COVID-19) has changed the lives of most people on Earth. Given the high prevalence of this disease, its correct diagnosis in order to quarantine patients is of the utmost importance in the steps of fighting this pandemic. Among the various modalities used for diagnosis, medical imaging , especially computed tomography (CT) imaging, has been the focus of many previous studies due to its accuracy and availability. In addition, automation of diagnostic methods can be of great help to physicians. In this paper, a method based on pre-trained deep neural networks is presented, which, by taking advantage of a cyclic generative adversarial net (CycleGAN) model for data augmentation , has reached state-of-the-art performance for the task at hand, i.e., 99.60\% accuracy. Also, in order to evaluate the method, a dataset containing 3163 images from 189 patients has been collected and labeled by physicians. Unlike prior datasets, normal data have been collected from people suspected of having COVID-19 disease and not from data from other diseases, and this database is made available publicly. Moreover, the method’s reliability is further evaluated by calibration metrics, and its decision is interpreted by Grad-CAM also to find suspicious regions as another output of the method and make its decisions trustworthy and explainable.},
  archive      = {J_ASOC},
  author       = {Navid Ghassemi and Afshin Shoeibi and Marjane Khodatars and Jonathan Heras and Alireza Rahimi and Assef Zare and Yu-Dong Zhang and Ram Bilas Pachori and J. Manuel Gorriz},
  doi          = {10.1016/j.asoc.2023.110511},
  journal      = {Applied Soft Computing},
  pages        = {110511},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic diagnosis of COVID-19 from CT images using CycleGAN and transfer learning},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble federated learning: An approach for collaborative
pneumonia diagnosis. <em>ASOC</em>, <em>144</em>, 110500. (<a
href="https://doi.org/10.1016/j.asoc.2023.110500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a very convenient approach for scenarios where (i) the exchange of data implies privacy concerns and/or (ii) a quick reaction is needed. In smart healthcare systems, both aspects are usually required. In this paper, we work on the first scenario, where preserving privacy is key and, consequently, building a unique and massive medical image data set by fusing different data sets from different medical institutions or research centers (computation nodes) is not an option. We propose an ensemble federated learning (EFL) approach that is based on the following characteristics: First, each computation node works with a different data set (but of the same type). They work locally and apply an ensemble approach combining eight well-known CNN models (densenet169, mobilenetv2, xception, inceptionv3, vgg16, resnet50, densenet121, and resnet152v2) on Chest X-ray images. Second, the best two local models are used to create a local ensemble model that is shared with a central node. Third, the ensemble models are aggregated to obtain a global model, which is shared with the computation nodes to continue with a new iteration. This procedure continues until there are no changes in the best local models. We have performed different experiments to compare our approach with centralized ones (with or without an ensemble approach). The results conclude that our proposal outperforms these ones in Chest X-ray images (achieving an accuracy of 96.63\%) and offers very competitive results compared to other proposals in the literature. A source code is provided at the Code Ocean repository: https://codeocean.com/capsule/0530602/tree .},
  archive      = {J_ASOC},
  author       = {Alhassan Mabrouk and Rebeca P. Díaz Redondo and Mohamed Abd Elaziz and Mohammed Kayed},
  doi          = {10.1016/j.asoc.2023.110500},
  journal      = {Applied Soft Computing},
  pages        = {110500},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble federated learning: An approach for collaborative pneumonia diagnosis},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel particle swarm optimization algorithm based on
GPU/CUDA. <em>ASOC</em>, <em>144</em>, 110499. (<a
href="https://doi.org/10.1016/j.asoc.2023.110499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel computing is the main way to improve the computational efficiency of metaheuristic algorithms for solving high-dimensional, nonlinear optimization problems . Previous studies have typically only implemented local parallelism for the particle swarm optimization (PSO) algorithm. In this study, we proposed a new parallel particle swarm optimization algorithm (GPU-PSO) based on the Graphics Processing Units (GPU) and Compute Unified Device Architecture (CUDA), which uses a combination of coarse-grained parallelism and fine-grained parallelism to achieve global parallelism. In addition, we designed a data structure based on CUDA features and utilized a merged memory access mode to further improve data-parallel processing and data access efficiency. Experimental results show that the algorithm effectively reduces the solution time of PSO for solving high-dimensional, large-scale optimization problems . The speedup ratio increases with the dimensionality of the objective function, where the speedup ratio is up to 2000 times for the high-dimensional Ackley function.},
  archive      = {J_ASOC},
  author       = {Yanhong Zhuo and Tao Zhang and Feng Du and Ruilin Liu},
  doi          = {10.1016/j.asoc.2023.110499},
  journal      = {Applied Soft Computing},
  pages        = {110499},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel particle swarm optimization algorithm based on GPU/CUDA},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPS-LCNN: A significant point sampling-based lightweight
convolutional neural network for point cloud processing. <em>ASOC</em>,
<em>144</em>, 110498. (<a
href="https://doi.org/10.1016/j.asoc.2023.110498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud data have very promising applications, but the irregularity and disorder make it a challenging problem how to use them. In recent years, an increasing number of new and excellent research solutions have been proposed, which focus on exploring local feature extractors. Over-engineered feature extractors lead to saturating the performance of current methods and often introduce unfavorable latency and additional overhead. This defeats the original purpose of using point cloud data, which is simplicity and efficiency. In this paper, we construct a learnable pipeline by designing two core modules with a small number of parameters – significant point sampling (SPS) and multiscale significant feature extraction (MS-SFE) – to balance accuracy and overhead. Our pipeline demonstrates comparable performance to state-of-the-art methods while requiring fewer parameters, making it well-suited for real-time applications.},
  archive      = {J_ASOC},
  author       = {Haojun Xu and Jing Bai},
  doi          = {10.1016/j.asoc.2023.110498},
  journal      = {Applied Soft Computing},
  pages        = {110498},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SPS-LCNN: A significant point sampling-based lightweight convolutional neural network for point cloud processing},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Determining the influence and correlation for parameters of
flexible forming using the random forest method. <em>ASOC</em>,
<em>144</em>, 110497. (<a
href="https://doi.org/10.1016/j.asoc.2023.110497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-point incremental forming (SPIF) enables the forming of fix-clamped sheet metal by moving a relatively small geometrically simple tool along the trajectory, producing the desired shape of the final product. Excessive thinning of the sheet results in fracture, determining the limit of formability. This characteristic of the forming process can be improved by upgrading the basic SPIF process to two-step forming, whereby a more even distribution of the sheet thickness can be achieved by pre-bulging with a hemispherical punch. This study focused on analysing the SPIF process and a hybrid two-step forming consisting of sequential bulging and SPIF. The analysis focused on the output parameters of sheet metal thinning and maximum forming force components and was conducted with Abaqus simulation software. An innovative new approach for influence analysis of technological, material and geometrical input parameters and correlation analysis between the mentioned parameters was performed using the random forest (RF) method, which allows the determination of individual parameter influence by analysing tree-shaped models obtained through the training process. The analysis results show a significant influence of the workpiece wall angle and part depth on thinning and initial sheet thickness on values of the forming force components. The results also show a great correlation between the parameters of the bulging depth and the part depth after SPIF and the significant influence of the appropriate choice of the backing plate geometry for the target product geometry.},
  archive      = {J_ASOC},
  author       = {Luka Sevšek and Sandi Baressi Šegota and Zlatan Car and Tomaž Pepelnjak},
  doi          = {10.1016/j.asoc.2023.110497},
  journal      = {Applied Soft Computing},
  pages        = {110497},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Determining the influence and correlation for parameters of flexible forming using the random forest method},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoOC: Automated multi-objective design of deep
autoencoders and one-class classifiers using grammatical evolution.
<em>ASOC</em>, <em>144</em>, 110496. (<a
href="https://doi.org/10.1016/j.asoc.2023.110496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-Class Classification (OCC) corresponds to a subclass of unsupervised Machine Learning (ML) that is valuable when labeled data is non-existent. In this paper, we present AutoOC, a computationally efficient Grammatical Evolution (GE) approach that automatically searches for OCC models . AutoOC assumes a multi-objective optimization, aiming to increase the OCC predictive performance while reducing the ML training time. AutoOC also includes two execution speedup mechanisms, a periodic training sampling, and a multi-core fitness evaluation. In particular, we study two AutoOC variants: a pure Neuroevolution (NE) setup that optimizes two types of deep learning models, namely dense Autoencoder (AE) and Variational Autoencoder (VAE); and a general Automated Machine Learning (AutoML) ALL setup that considers five distinct OCC base learners, specifically Isolation Forest (IF), Local Outlier Factor (LOF), One-Class SVM (OC-SVM), AE and VAE. Several experiments were conducted, using eight public OpenML datasets and two validation scenarios (unsupervised and supervised). The results show that AutoOC requires a reasonable amount of execution time and tends to obtain lightweight OCC models . Moreover, AutoOC provides quality predictive results, outperforming a baseline IF for all analyzed datasets and surpassing the best supervised OpenML human modeling for two datasets.},
  archive      = {J_ASOC},
  author       = {Luís Ferreira and Paulo Cortez},
  doi          = {10.1016/j.asoc.2023.110496},
  journal      = {Applied Soft Computing},
  pages        = {110496},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AutoOC: Automated multi-objective design of deep autoencoders and one-class classifiers using grammatical evolution},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A customized multi-neighborhood search algorithm using the
tabu list for a sustainable closed-loop supply chain network under
uncertainty. <em>ASOC</em>, <em>144</em>, 110495. (<a
href="https://doi.org/10.1016/j.asoc.2023.110495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental pollution and social inequalities have prompted the agricultural industry to implement sustainable supply chain management practices. However, supply chain operations and planning in developing countries such as Iran are often not economically, environmentally, and socially sustainable. To address this challenge, this paper presents a practical optimization model for sustainable closed-loop supply chain (SCLSC) management in the agricultural industry of Iran, with a focus on the olive crop. Based on the triple bottom line concept, which considers economic, environmental, and social sustainability, the proposed model makes location, allocation, and inventory decisions under uncertainty by developing a scenario-based robust optimization model. To solve this complex network design problem , we propose a metaheuristic algorithm with a multi-neighborhood procedure that efficiently handles the complexity of the problem. Specifically, we develop a customized Simulated Annealing (SA) algorithm using a tabu list to improve the initial solution found by a constructive heuristic algorithm . Our extensive analysis and comparison of our metaheuristic algorithm against the exact solver and two other powerful metaheuristic algorithms demonstrate the applicability of our SCLSC model for the agricultural industry in Iran and the high performance of the proposed metaheuristic algorithm for solving large-scale networks.},
  archive      = {J_ASOC},
  author       = {Pourya Seydanlou and Mohammad Sheikhalishahi and Reza Tavakkoli-Moghaddam and Amir M. Fathollahi-Fard},
  doi          = {10.1016/j.asoc.2023.110495},
  journal      = {Applied Soft Computing},
  pages        = {110495},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A customized multi-neighborhood search algorithm using the tabu list for a sustainable closed-loop supply chain network under uncertainty},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-based multimodal sentiment analysis and emotion
recognition using deep neural networks. <em>ASOC</em>, <em>144</em>,
110494. (<a href="https://doi.org/10.1016/j.asoc.2023.110494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a growing interest in multimodal sentiment analysis and emotion recognition in recent years due to its wide range of practical applications. Multiple modalities allow for the integration of complementary information, improving the accuracy and precision of sentiment and emotion recognition tasks. However, working with multiple modalities presents several challenges, including handling data source heterogeneity, fusing information, aligning and synchronizing modalities, and designing effective feature extraction techniques that capture discriminative information from each modality. This paper introduces a novel framework called “Attention-based Multimodal Sentiment Analysis and Emotion Recognition (AMSAER)” to address these challenges. This framework leverages intra-modality discriminative features and inter-modality correlations in visual, audio, and textual modalities. It incorporates an attention mechanism to facilitate sentiment and emotion classification based on visual, textual, and acoustic inputs by emphasizing relevant aspects of the task. The proposed approach employs separate models for each modality to automatically extract discriminative semantic words, image regions, and audio features. A deep hierarchical model is then developed, incorporating intermediate fusion to learn hierarchical correlations between the modalities at bimodal and trimodal levels. Finally, the framework combines four distinct models through decision-level fusion to enable multimodal sentiment analysis and emotion recognition. The effectiveness of the proposed framework is demonstrated through extensive experiments conducted on the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. The results confirm a notable performance improvement compared to state-of-the-art methods, attaining 85\% and 93\% accuracy for sentiment analysis and emotion classification, respectively. Additionally, when considering class-wise accuracy, the results indicate that the “angry” emotion and “positive” sentiment are classified more effectively than the other emotions and sentiments, achieving 96.80\% and 93.14\% accuracy, respectively.},
  archive      = {J_ASOC},
  author       = {Ajwa Aslam and Allah Bux Sargano and Zulfiqar Habib},
  doi          = {10.1016/j.asoc.2023.110494},
  journal      = {Applied Soft Computing},
  pages        = {110494},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-based multimodal sentiment analysis and emotion recognition using deep neural networks},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A centrality based genetic algorithm for the graph burning
problem. <em>ASOC</em>, <em>144</em>, 110493. (<a
href="https://doi.org/10.1016/j.asoc.2023.110493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information spread is an intriguing topic to study in network science, which investigates how information, influence, or contagion propagate through networks. Graph burning is a simplified deterministic model for how information spreads within networks. The complicated NP-complete nature of the problem makes it computationally difficult to solve using exact algorithms. Accordingly, a number of heuristics and approximation algorithms have been proposed in the literature for the graph burning problem. In this paper, we propose an efficient genetic algorithm called Centrality BAsed Genetic-algorithm (CBAG) for solving the graph burning problem. Considering the unique characteristics of the graph burning problem, we introduce novel genetic operators, chromosome representation , and evaluation method. In the proposed algorithm, the well-known betweenness centrality is used as the backbone of our chromosome initialization procedure . The proposed algorithm is implemented and compared with previous heuristics and approximation algorithms on 15 benchmark graphs of different sizes. Based on the results, it can be seen that the proposed algorithm achieves better performance in comparison to the previous state-of-the-art heuristics. The complete source code is available online and can be used to find optimal or near-optimal solutions for the graph burning problem.},
  archive      = {J_ASOC},
  author       = {Mahdi Nazeri and Ali Mollahosseini and Iman Izadi},
  doi          = {10.1016/j.asoc.2023.110493},
  journal      = {Applied Soft Computing},
  pages        = {110493},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A centrality based genetic algorithm for the graph burning problem},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential evolution with wavelet basis function based
parameter control and dimensional interchange for diversity enhancement.
<em>ASOC</em>, <em>144</em>, 110492. (<a
href="https://doi.org/10.1016/j.asoc.2023.110492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential Evolution (DE) is a potent population-based global optimization algorithm which has already proven efficient for optimization demands in engineering applications . However, even the state-of-the-art DE variants still suffer premature convergence and lack of diversity. In order to overcome the above mentioned weaknesses, this paper proposes a brand-new DE variant, namely zDE algorithm, for single-objective numerical optimization . The main contributions can be summarized as follows: First, an improved wavelet basis function is incorporated into the generation of the scale factor F F and a Minkowski Distance based adaptation scheme is proposed for the adaptation of it. Second, a new trial vector generation strategy is first proposed as a supplementary of the existing strategies, and t-distribution is incorporated into this strategy. Third, a novel diversity enhancement technique is firstly proposed by changing the dimensional parameters of the individuals in the population. The zDE algorithm is validated under 88 benchmarks from the CEC2013, CEC2014, and CEC2017 test suites for real-parameter single-objective optimization, and the results show the superiority of our algorithm in comparison with the recent state-of-the-art DE variants.},
  archive      = {J_ASOC},
  author       = {Zhenghao Song and Zhenyu Meng},
  doi          = {10.1016/j.asoc.2023.110492},
  journal      = {Applied Soft Computing},
  pages        = {110492},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution with wavelet basis function based parameter control and dimensional interchange for diversity enhancement},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A knowledge-based multi-objective evolutionary algorithm for
solving home health care routing and scheduling problems with multiple
centers. <em>ASOC</em>, <em>144</em>, 110491. (<a
href="https://doi.org/10.1016/j.asoc.2023.110491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, population aging is becoming more and more serious. The elderly occupies a great many of public medical resources, which brings great pressure to the public and governments. Home health care (HHC) is treated as an alternative answer to hospitalization, and plays positive roles in alleviating the stress from the shortage of medical resources incurred by the population aging. Hence, how to effectively manage and organize the operations of HHCs becomes an all-important problem in reality. HHC routing and scheduling problems attract a great deal of interest from modeling and optimization areas. Nevertheless, most of the existing studies just focus on the HHC routing and scheduling problems with a single HHC center. This study focuses on a multi-objective HHC routing and scheduling problem with multiple centers for minimizing total service cost and total tardiness incurred by delay service while meeting caregivers’ workload and resource constraints. To cope with it, we establish a mixed integer programming model to formulate the concerned problem. Then, a knowledge-based multi-objective evolutionary algorithm (KMoEA) is specially designed by employing two local search operators with the properties of the problem. Via comparing KMoEA with five algorithms and a mathematical programming solver (CPLEX), we validate that KMoEA is an effective approach for solving the studied problem.},
  archive      = {J_ASOC},
  author       = {Xiaomeng Ma and Yaping Fu and Kaizhou Gao and Hui Zhang and Jianhui Mou},
  doi          = {10.1016/j.asoc.2023.110491},
  journal      = {Applied Soft Computing},
  pages        = {110491},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knowledge-based multi-objective evolutionary algorithm for solving home health care routing and scheduling problems with multiple centers},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New approach for quality function deployment based on
multi-granular unbalanced linguistic information and consensus reaching
process. <em>ASOC</em>, <em>144</em>, 110490. (<a
href="https://doi.org/10.1016/j.asoc.2023.110490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely used product improvement technique, the quality function deployment (QFD) can satisfy customer requirements and realize higher customer satisfaction by translating them into corresponding engineering characteristics. Nonetheless, two basic challenges hinder the effective application of QFD are the imprecise relationship assessments between customer requirements and engineering characteristics because of the uncertainty inherent in human judgements and the unreasonable importance ranking of engineering characteristics due to the heterogeneity of domain experts. In response, this paper puts forward a new hybrid QFD approach for the priority of engineering characteristics to satisfy customer requirements. First, multi-granular unbalanced linguistic term sets are utilized to describe the vague relational evaluations between customer requirements and engineering characteristics. Then, the opinion evolution social network consensus reaching model is employed to assist QFD experts in deriving consensual relational evaluations. Taking the conflict customer requirements into consideration, the combined compromise solution method is adopted and extended to derive the priority orders of engineering characteristics. To illustrate the practicality and effectiveness of the proposed QFD approach, a product development case about low pressure pulse filter is provided with comparative analysis and simulation experiments. The results show that the proposed approach can represent complex linguistic relationship assessments of experts and determine more accurate priority orders of engineering characteristics in QFD.},
  archive      = {J_ASOC},
  author       = {Ya-Juan Han and Miao-Miao Cao and Hu-Chen Liu},
  doi          = {10.1016/j.asoc.2023.110490},
  journal      = {Applied Soft Computing},
  pages        = {110490},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New approach for quality function deployment based on multi-granular unbalanced linguistic information and consensus reaching process},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fault location method based on ensemble complex
spatio-temporal attention network for complex systems under fluctuating
operating conditions. <em>ASOC</em>, <em>144</em>, 110489. (<a
href="https://doi.org/10.1016/j.asoc.2023.110489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, traditional fault diagnosis methods achieve fault identification by establishing a sample set covering all fault degrees of different fault components in complex systems. However, the uncertainties including environmental stresses and own physical and chemical variations lead to an infinite number of fault degrees of fault components, and the fault identification of complex systems in practical engineering applications faces the challenge of fluctuating operating conditions due to variations of rotational speed and load. To address the above problem, a fault location method based on ensemble complex spatio-temporal attention network (ECSAN) is proposed in this paper, which can identify the critical fault components of complex systems by combining an ensemble learning mechanism with excellent basic estimators. A basic estimator consists of a feature extraction module, a feature enhancement module , and a classification module. In the first module, a complex spatio-temporal backbone network with strong generalization is developed to provide spatio-temporal features containing the inherent information of the sample data for complex systems. In the feature enhancement module , a lightweight complex attention layer is constructed to enhance the effective structural information of the features and reduce the interference of their redundant information. The classification module then adopts a Softmax layer to perform fault classification. Finally, an ensemble learning mechanism is designed to integrate the basic estimators. By constructing sample weights and introducing a knowledge transfer strategy, the generalization is further improved while saving training expenses. Two datasets from different experimental platforms are concerned to verify the effectiveness and superiority of this method under various operating conditions and fault degrees. The experimental results indicate that this method achieves 99.81\% accuracy on a standard dataset of the mechanical system and 98.88\% accuracy on a real dataset of the closed-loop control system of the water jet propulsion device, which is superior to comparison approaches.},
  archive      = {J_ASOC},
  author       = {Jingli Yang and Tianyu Gao and Ge Yan and Cheng Yang and Gangqiang Li},
  doi          = {10.1016/j.asoc.2023.110489},
  journal      = {Applied Soft Computing},
  pages        = {110489},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fault location method based on ensemble complex spatio-temporal attention network for complex systems under fluctuating operating conditions},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A composite weighted human learning network and its
application for modeling of the intermediate point temperature in USC.
<em>ASOC</em>, <em>144</em>, 110488. (<a
href="https://doi.org/10.1016/j.asoc.2023.110488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra supercritical power plant (USC) is a complex system associated with nonlinearity , uncertainties and multivariable couplings. Generally, it is difficult to build an accurate model to approximate the dynamic behavior of USC. This paper presents a novel composite weighted human learning optimization network (CWHLO) to tackle the above-mentioned problem. Firstly, by fully using of the statistic characteristic of the history operating data, K-means clustering algorithm is applied to partition the raw date, which extremely reduces the operating nonlinearity . Then, an improved real-coded human learning optimization (HLO) is adopted to built linear models in local regions. Different from conventional methods, the advantage of the proposed CWHLO is that the nonlinear model of the object is effectively replaced by a real-time dynamic linear model, which is more suitable for other control methods . Finally, the CWHLO model is compared with the traditional recursive least square method (RLS), and four other meta-heuristic algorithms, to show the advantages in approximating the dynamic behavior of USC.},
  archive      = {J_ASOC},
  author       = {Chuanliang Cheng and Chen Peng and Miao Rong},
  doi          = {10.1016/j.asoc.2023.110488},
  journal      = {Applied Soft Computing},
  pages        = {110488},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A composite weighted human learning network and its application for modeling of the intermediate point temperature in USC},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A floating offshore platform motion forecasting approach
based on EEMD hybrid ConvLSTM and chaotic quantum ALO. <em>ASOC</em>,
<em>144</em>, 110487. (<a
href="https://doi.org/10.1016/j.asoc.2023.110487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of floating offshore platform motion (FOPM) is very important to control the movement of the platform and the normal operation of the equipment on the platform. However, the accurate prediction of FOPM is very difficult, due to the coupling effect of the mooring system, operation system, wind, wave, and current. Therefore, in order to obtain more accurate prediction results, firstly, the Convolutional LSTM (ConvLSTM) network is introduced to simulate FOPM nonlinear dynamical system , design input vector coding rules, considering the characteristics of complex time-varying nonlinear and space non-stationary of FOPM. The EEMD is applied for modal decomposition for time series of FOPM to reduce the nonlinearity of time series. Then the FOPM-EEMD-ConvLSTM forecasting model is proposed by designing the input sequence matrix and prediction architecture. Considering the defects of ALO algorithm, based on quantum computing and chaotic mapping, the quantum global search algorithm (QRS) and ant lion trap Chaos reconstruction mechanism (ALTCR)are designed, and chaotic quantum ant lion optimization algorithm (CQALO) is proposed and use to optimate the hyperparametric of FOPM-EEMD- ConvLSTM. Consequently, a hybrid forecasting approach of FOPM integrating FOPM-EEMD- ConvLSTM and CQALO was established, namely FOPM-EEMD-ConvLSTM-CQALO. Finally, the sway and heave data of a floating offshore platform serving in the ocean is used to carry out prediction experiments to test the performance of the proposed new prediction approach. The test results indicate that the prediction model established in this paper has higher prediction accuracy and stronger robustness than the comparison model selected in this paper and the CQALO obtains more appropriate hyperparameters than the comparison algorithm applied in selecting the hyperparameters of FOPM-EEMD- ConvLSTM.},
  archive      = {J_ASOC},
  author       = {Da Li and Mei-Rong Jiang and Ming-Wei Li and Wei-Chiang Hong and Rui-Zhe Xu},
  doi          = {10.1016/j.asoc.2023.110487},
  journal      = {Applied Soft Computing},
  pages        = {110487},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A floating offshore platform motion forecasting approach based on EEMD hybrid ConvLSTM and chaotic quantum ALO},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective genetic algorithm for compression of
weighted graphs to simplify epidemic analysis. <em>ASOC</em>,
<em>144</em>, 110486. (<a
href="https://doi.org/10.1016/j.asoc.2023.110486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of computational intelligence based approaches for the compression of graphs is an under-explored area of research. Further, compression of weighted graphs is significantly more complicated than compression of unweighted graphs. In this paper a multi-objective approach using NSGA-II is applied to the problem of weighted graph compression. The approach is designed to find a balance between the level of compression and the distortion created by the compression. Distortion is measured using two fitness functions that each evaluate changes both in graph structure and in edge weights. The methodology is applied to three weighted contact networks with differing characteristics. It was found that the multi-objective approach is useful in identifying suitable compression ratios based upon defined levels of acceptable distortion, with a single-objective genetic algorithm then applied to focus on this target compression ratio to further reduce distortion.},
  archive      = {J_ASOC},
  author       = {Emilia Rutkowski and Sheridan Houghten and Joseph Alexander Brown},
  doi          = {10.1016/j.asoc.2023.110486},
  journal      = {Applied Soft Computing},
  pages        = {110486},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective genetic algorithm for compression of weighted graphs to simplify epidemic analysis},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A surrogate evolutionary neural architecture search
algorithm for graph neural networks. <em>ASOC</em>, <em>144</em>,
110485. (<a href="https://doi.org/10.1016/j.asoc.2023.110485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the unique construction module and design of graph neural networks (GNNs), neural architecture search (NAS) methods specifically for GNNs have become a promising research hotspot in recent years. Among the existing methods, one class of methods microscopically searches for the constituent components of network layers. However, most of them ignore the topology connections between network layers or the feature fusion strategies. Another class of methods, called differentiable architecture search methods, has the advantage of searching topology connections and feature fusion strategies. However, constrained by the requirement of predefining all candidate operations, these methods can only sample a limited number of network layers. In this paper, we propose a surrogate evolutionary graph neural architecture search (GNAS) algorithm whose search space contains not only the microscopic network layer c omponents but also t opology connections and feature f usion strategies (called CTFGNAS). The GNN sampled in CTFGNAS is represented by a simple one-dimensional vector and does not fix the network depth. To address the problem that traditional crossover and mutation operators applied to GNAS may produce illegal solutions, we design a repair operation to guarantee the legitimacy of the solutions. The network depth is also increased with a large probability in the mutation operation to alleviate the oversmoothing problem. In addition, to cope with the challenge of computational resources due to the increased search space, we form a surrogate model with three classical regression models, where only a small number of solutions are truly evaluated for their fitness, and the remaining large number of solutions are predicted for their fitness by the surrogate model . Finally, experiments are executed on six widely used real-world datasets. The experimental results illustrate that CTFGNAS obtains more effective results than the state-of-the-art handcrafted GNNs and GNAS methods on all datasets. CTFGNAS is now available on the following website: https://github.com/chnyliu/CTFGNAS .},
  archive      = {J_ASOC},
  author       = {Yang Liu and Jing Liu},
  doi          = {10.1016/j.asoc.2023.110485},
  journal      = {Applied Soft Computing},
  pages        = {110485},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate evolutionary neural architecture search algorithm for graph neural networks},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge augmentation-based soft constraints for
semi-supervised clustering. <em>ASOC</em>, <em>144</em>, 110484. (<a
href="https://doi.org/10.1016/j.asoc.2023.110484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis discovers natural structures from different perspectives of data objects and has become an effective method in data mining. The emergence of semi-supervised clustering techniques has improved the performance of unsupervised clustering algorithms . Clustering with guidance information is a clustering method variant that uses pairwise constraints based on background knowledge. This method increases the interpretability of the results through a knowledge-guided perspective but simultaneously suffers from the problem of constraint conflict. This paper designs knowledge augmentation-based soft constraints as a new pairwise constraint representation and proposes a Soft Constraints Kmeans (SCop-Kmeans) method to resolve constraint conflicts. By describing constraint knowledge from multiple perspectives, the association strength of pairwise constraints is calculated to obtain the assignment basis of objects. SCop-Kmeans can solve the sample allocation conflict problem caused by the contradiction between different constraints and improve the clustering stability. Finally, experiments are performed using UCI public standard datasets. The proposed method further improves the accuracy of clustering and performs well in experiments with different numbers of constraints, which shows that the proposed method has advantages in using constraint information to guide clustering.},
  archive      = {J_ASOC},
  author       = {Zhanhu Zhang and Xia Yu and Rui Tao and Xinyu Zhang and Hongru Li and Jingyi Lu and Jian Zhou},
  doi          = {10.1016/j.asoc.2023.110484},
  journal      = {Applied Soft Computing},
  pages        = {110484},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge augmentation-based soft constraints for semi-supervised clustering},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cultural evolution with a modified selection function and
adaptive α-cognition procedure for numerical optimization.
<em>ASOC</em>, <em>144</em>, 110483. (<a
href="https://doi.org/10.1016/j.asoc.2023.110483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, several population-based evolutionary and swarm algorithms have been developed and used in the literature. This work introduces an improved Cultural Algorithm with a modified selection function and a dynamic α α -cognition procedure to handle a variety of challenging numerical optimization problems . The modified selection function is used to support a balanced evolutionary search. A process that starts with a clearer exploration early in the search process and gradually begins to focus on exploitation towards the end of the search process. This work uses the elites of each knowledge source that are at a certain distance from each other. The dynamic α α -cognition procedure assists in providing effective learning of individuals through preserving the diversity of the population during the evolution process. In this procedure, each individual is able to learn from the top α α\% individuals controlled by its knowledge source in the belief space, where the proportion of the affecting subpopulation ( α α ) is adaptively modified during the evolution. The performance of the proposed work has been evaluated on the CEC’2010 and CEC’2013 benchmark suites developed for the special sessions on large-scale global optimization problems . An appropriate comparative study with the best results in the literature is presented. The results confirm how the merits of the improved Cultural Algorithm can achieve superior performance over other cutting-edge algorithms for these data sets.},
  archive      = {J_ASOC},
  author       = {Mostafa Z. Ali and Heba Abdel-Nabi and Rami Alazrai and Bushra AlHijawi and Mazen G. AlWadi and Amer F. Al-Badarneh and Ponnuthurai N. Suganthan and Mohammad I. Daoud and Robert G. Reynolds},
  doi          = {10.1016/j.asoc.2023.110483},
  journal      = {Applied Soft Computing},
  pages        = {110483},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cultural evolution with a modified selection function and adaptive α-cognition procedure for numerical optimization},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and analysis of cardioimpedance signals using
polynomial models and fuzzy rule-based models. <em>ASOC</em>,
<em>144</em>, 110482. (<a
href="https://doi.org/10.1016/j.asoc.2023.110482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of characteristic points of bioimpedance curves is crucial to the analysis of bioimpedance signals. Traditional methods need to calculate the derivative of the bioimpedance curve and rely on the help of other simultaneously recorded signals. However, these methods are sensitive to noise while some auxiliary signal may be unrecorded. A novel approach is proposed in this study to automatically determine the positions of characteristic points. The overall development process is realized as a two-phase construct. The first stage of the process involves a fitting of the impedance change curves using nonlinear regression models . Then, a prediction model is trained to predict the positions of characteristic points through a linear combination of the parameters of the nonlinear regression model. Experimental studies constructed on a collection of real-world bioimpedance signals help quantify the performance of the algorithm and gain a deep insight into the superiority of the proposed methodology. It is shown that the proposed method offers a substantial improvement in the prediction accuracy in comparison with the baseline method . Moreover, the proposed characteristic point determination method is robust to noise since its performance is not affected in the presence of noise.},
  archive      = {J_ASOC},
  author       = {Dan Wang and Xiubin Zhu and Witold Pedrycz and Adam Gacek and Aleksander Sobotnicki and Zhiwu Li},
  doi          = {10.1016/j.asoc.2023.110482},
  journal      = {Applied Soft Computing},
  pages        = {110482},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling and analysis of cardioimpedance signals using polynomial models and fuzzy rule-based models},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive learning with frequency domain for sequential
recommendation. <em>ASOC</em>, <em>144</em>, 110481. (<a
href="https://doi.org/10.1016/j.asoc.2023.110481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation has recently played an important role on various platforms due to its ability to understand users’ intentions from their historical interactions. However, modeling user’s intention on time-based representations poses challenges, such as fast-evolving interests, noisy interactions, and sparse data. While contrastive self-supervised learning can mitigate these issues, the complexity of time-based interactions limits the utility of understanding the user’s intention. Motivated by this limitation, we posit that intent representations need to accommodate different domains. To this end, we expect both the frequency-domain augmented view and the time-domain augmented view of the same sample should be maximally consistent with their original input in their corresponding domains. Inspired by this idea, we propose Contrastive Learning with Frequency Domain for Sequential Recommendation (CLF4SRec), where a learnable Fourier layer provides the frequency-based self-supervised signal. Instead of pre-training, we employ a multi-task learning framework jointly with contrastive learning and recommendation learning to optimize the user representation encoder. We conduct comprehensive experiments on four real-world datasets, where CLF4SRec outperforms the recent strongest baselines from 7.58\% to 67.85\%, showing its effectiveness for sequential recommendation tasks. Specifically, CLF4SRec can achieve a boost of 41.88\% to 67.85\% on the dense dataset, which might be attributed to the ability of frequency domain technology to handle dense signals. The implementation code is available at https://github.com/zhangyichi1Z/CLF4SRec .},
  archive      = {J_ASOC},
  author       = {Yichi Zhang and Guisheng Yin and Yuxin Dong and Liguo Zhang},
  doi          = {10.1016/j.asoc.2023.110481},
  journal      = {Applied Soft Computing},
  pages        = {110481},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning with frequency domain for sequential recommendation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Medical waste disposal planning for healthcare units using
spherical fuzzy CRITIC-WASPAS. <em>ASOC</em>, <em>144</em>, 110480. (<a
href="https://doi.org/10.1016/j.asoc.2023.110480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare services create medical waste that may be dangerous to healthcare personnel , patients, the general public, and the environment. Medical waste disposal method selection is among the most important decisions that must be made by healthcare organizations, and such a problem has a number of contradictory criteria and alternatives. On the other hand, decision experts may have considerable uncertainty while evaluating these alternatives. In this paper, new fuzzy multi-criteria decision-making (MCDM) methodologies are provided for assessing the medical waste disposal alternatives. The CRiteria Importance Through Intercriteria Correlation (CRITIC) is used for obtaining criterion weights in an objective manner, and the Weighted Aggregated Sum Product ASsessment (WASPAS) approach is utilized to rank the alternatives. For modeling the uncertainty in the nature of the problem, the proposed methodology is developed in single and interval-valued spherical fuzzy environments. Single-valued spherical fuzzy sets enable users to model the membership, non-membership, and hesitancy parameters independently. On the other hand, interval-valued spherical fuzzy sets provide increased fuzziness modeling capacity. The step-by-step solution of the proposed methodologies are followed by sensitivity and comparative analyses, and a discussion. This study contributes to the work of both academics and practitioners in the healthcare industry , as well as other sectors facing similar types of decision-making problems.},
  archive      = {J_ASOC},
  author       = {Akın Menekşe and Hatice Camgöz Akdağ},
  doi          = {10.1016/j.asoc.2023.110480},
  journal      = {Applied Soft Computing},
  pages        = {110480},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Medical waste disposal planning for healthcare units using spherical fuzzy CRITIC-WASPAS},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fitness-distance-constraint (FDC) based guide selection
method for constrained optimization problems. <em>ASOC</em>,
<em>144</em>, 110479. (<a
href="https://doi.org/10.1016/j.asoc.2023.110479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the optimization of constrained type problems, the main difficulty is the elimination of the constraint violations in the evolutionary search process. Evolutionary algorithms are designed by default according to the requirements of unconstrained and continuous global optimization problems . Since there are no constraint functions in these type of problems, the constraint violations are not considered in the design of the guiding mechanism of evolutionary algorithms. In this study, two new methods were introduced to redesign the evolutionary algorithms in accordance with the requirements of constrained optimization problems . These were (i) constraint space-based, called Fitness-Distance-Constraint (FDC), selection method and (ii) dynamic guiding mechanism. Firstly, thanks to the FDC guide selection method, the constraint violation values of the individuals in the population were converted into score values and the individuals who increase the diversity in the search process were selected as guide. On the other hand, in dynamic guiding mechanism, the FDC method was applied in case of constraint violation, otherwise the default guide selection method was used The proposed methods were used to redesign the guiding mechanism of adaptive guided differential evolution (AGDE), a current evolutionary algorithm, and the FDC-AGDE algorithm was designed. The performance of the FDC-AGDE was tested on eleven different constrained real-world optimization problems. The results of the FDC-AGDE and AGDE were evaluated using the Friedman and Wilcoxon test methods. According to Wilcoxon pairwise results, the FDC-AGDE showed better performance than the AGDE in nine of the eleven problems and equal performance in two of the eleven problems. Moreover, the proposed algorithm was compared with the competitive and up-to-date MHS algorithms in terms of the results of Friedman test, Wilcoxon test, feasibility rate, and success rate. According to Friedman test results, the first three algorithms were the FDC-AGDE, LSHADE-SPACMA, and AGDE algorithms with the score of 2.69, 4.05, and 4.34, respectively. According to the mean values of the success rates obtained from the eleven problems, the FDC-AGDE, LSHADE-SPACMA, and AGDE algorithms ranked in the first three with the success rates of 67\%, 48\% and 28\%, respectively. Consequently, the FDC-AGDE algorithm showed a superior performance comparing with the competing MHS algorithms. According to the results, it is expected that the proposed methods will be widely used in the constrained optimization problems in the future.},
  archive      = {J_ASOC},
  author       = {Burcin Ozkaya and Hamdi Tolga Kahraman and Serhat Duman and Ugur Guvenc},
  doi          = {10.1016/j.asoc.2023.110479},
  journal      = {Applied Soft Computing},
  pages        = {110479},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fitness-distance-constraint (FDC) based guide selection method for constrained optimization problems},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient e-mail spam filtering approach combining logistic
regression model and orthogonal atomic orbital search algorithm.
<em>ASOC</em>, <em>144</em>, 110478. (<a
href="https://doi.org/10.1016/j.asoc.2023.110478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phishing emails called spam have created a need for reliable and intelligent spam filters . Machine-learning techniques are effective, but current methods such as Logistic Regression (LR), Support Vector Machine (SVM), Decision Trees (DT), and Naive Bayes (NB) sometimes produce low detection rates and struggle with large amounts of data. Motivated by these concerns, we propose an efficient spam-filtering approach, OAOS-LR, combining an improved Atomic Orbital Search (AOS) algorithm with an LR classification model . To remove the deficiency of low detection rate produced by the standard LR method due to the utilization of the gradient descent technique, we train it with our proposed OAOS approach, which uses AOS and Orthogonal learning to enhance the search capabilities of the conventional algorithm. In the experimental study, we first evaluated the performance of OAOS over the IEEE Congress on Evolutionary Computation (CEC’20) benchmarks against five different metaheuristics to prove its effectiveness in improving its convergence rate and reducing the probability of falling in local optima. After that, the proposed technique LR-OAOS was applied to the spam filtering problem using CSDMC2010 and Enron datasets and tested against the most recent machine learning and metaheuristic approaches using standard statistical measures and plots. OAOS-LR significantly outperformed other methods with an average F1-score success rate of 95.45\% and 96.30\% on CSDMC2010, and 74.80\% and 78.33\% on Enron, respectively with the number of feature spaces equal to 500 and 1000.},
  archive      = {J_ASOC},
  author       = {Ghaith Manita and Amit Chhabra and Ouajdi Korbaa},
  doi          = {10.1016/j.asoc.2023.110478},
  journal      = {Applied Soft Computing},
  pages        = {110478},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient e-mail spam filtering approach combining logistic regression model and orthogonal atomic orbital search algorithm},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hesitant convolutional neural networks and intelligent drive
algorithm fused subjective guidance. <em>ASOC</em>, <em>144</em>,
110477. (<a href="https://doi.org/10.1016/j.asoc.2023.110477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the low-carbon development strategy, the new energy vehicle has become one crucial and recognized tool to satisfy the emission reduction requirements, the representative advance of the new energy vehicle is its intelligent drive technique. This paper introduces the driver’s subjective guidance to further improve the intelligent drive technique. To do this, the hesitant convolutional neural networks (HCNN) are proposed to deal with the above issue, in which the hesitant fuzzy set is used to fully describe the subjective guidance information. Thus, the different driver’s personalized needs can be considered and fused into the intelligent drive algorithm to achieve the intelligent drive technique of new energy vehicles. Note that the innovation and difference of the proposed method are the subjective guidance fusion and its generalized presentation but not the intelligent drive calculation. After that, the data downward partition model and weight secondary matching calculation are developed based on the HCNN to fully describe and process the hesitant fuzzy subjective guidance information. Furthermore, the intelligent drive algorithm considering the driver’s subjective guidance is provided according to the above methods. Finally, an illustrative example is given to show the effectiveness of the proposed algorithm in the given drive scenario.},
  archive      = {J_ASOC},
  author       = {Wei Zhou and Yi Lu and Man Liu and Zeshui Xu},
  doi          = {10.1016/j.asoc.2023.110477},
  journal      = {Applied Soft Computing},
  pages        = {110477},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hesitant convolutional neural networks and intelligent drive algorithm fused subjective guidance},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A constrained cooperative guidance algorithm based on gray
wolf optimization against highly maneuvering target. <em>ASOC</em>,
<em>144</em>, 110476. (<a
href="https://doi.org/10.1016/j.asoc.2023.110476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of cooperative guidance of two pursuers against an evader equipped with higher maneuverability . The goal is that the distance between the Evader and at least one of the pursuers becomes less than a predetermined threshold at the end of flight time . To achieve this goal, firstly, the roles of pursuers are divided into two units, which include (1) pursuing the Evader and (2) Observing the Evader’s escape space. Secondly, a new cooperative guidance algorithm based on the separation of roles of the pursuers is proposed and formulated into a constrained nonlinear model predictive control problem. An objective function with time-variant weighting factors is presented to evaluate the constrained guidance commands. The proposed guidance algorithm uses a gray wolf-based optimization algorithm to calculate the guidance commands.},
  archive      = {J_ASOC},
  author       = {Saeed Nasrollahi},
  doi          = {10.1016/j.asoc.2023.110476},
  journal      = {Applied Soft Computing},
  pages        = {110476},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A constrained cooperative guidance algorithm based on gray wolf optimization against highly maneuvering target},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable column-generation-based genetic algorithm for
knapsack-like energy aware nanosatellite task scheduling. <em>ASOC</em>,
<em>144</em>, 110475. (<a
href="https://doi.org/10.1016/j.asoc.2023.110475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Offline Nanosatellite Task Scheduling (ONTS) problem poses a complex optimization challenge, focused on maximizing the number of tasks executed by a satellite in orbit while adhering to Quality of Service constraints such as priority, execution time-frames, and resource management. Based on mixed integer programming , existing methods rely on branch-and-bound aided algorithms and can struggle to achieve satisfactory computational time performance. In order to avoid the computational burden of branch-and-bound, this work introduces the Column-Generation-based Genetic Algorithm (CGbGA) as a heuristic approach to the ONTS problem. The method, based on branch-and-price principles, combines Genetic Algorithm (GA) and Dynamic Programming (DP) to solve the problem of interest efficiently. We generate solution vectors for each job using DP and adapt mutation and crossover operators to work on a column-wide scale. This ensures that every solution is valid for the given job. Also, a novel pseudo-shadow pricing strategy is employed to mimic the pricing procedure of the branch-and-price algorithm. To better understand the impact of the number of available columns on the incumbent solution, we employ Local Interpretable Model-Agnostic Explanations (LIME). Our results, based on a set of representative literature instances, demonstrate the potential of CGbGA in terms of solution value and computational solving time compared to commercially available solvers.},
  archive      = {J_ASOC},
  author       = {Laio Oriel Seman and Cezar Antônio Rigo and Eduardo Camponogara and Eduardo Augusto Bezerra and Leandro dos Santos Coelho},
  doi          = {10.1016/j.asoc.2023.110475},
  journal      = {Applied Soft Computing},
  pages        = {110475},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable column-generation-based genetic algorithm for knapsack-like energy aware nanosatellite task scheduling},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic generation of a hybrid algorithm for the maximum
independent set problem using genetic programming. <em>ASOC</em>,
<em>144</em>, 110474. (<a
href="https://doi.org/10.1016/j.asoc.2023.110474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of graph optimization’s fundamental and most challenging problems is determining the maximum set of unconnected vertices in a graph, called the maximum independent set problem. This problem consists of finding the largest independent set in a graph, where an independent set is a set of vertices such that no two vertices are adjacent. This paper presents a new artificially generated algorithm for the maximum independent set problem. The new algorithm is generated by the automatic generation of algorithms, a technique that allows the construction of new hybrid algorithms, taking advantage of existing algorithms. Thus, the automatic generation of algorithms combines basic heuristics for the problem, a tabu search method selected from the literature, and an exact method that solves the problem’s mathematical formulation working for a limited computational time. With these components, the space of possible algorithms is traversed by employing genetic programming . Algorithms of small sizes are generated to study their structure and discover new algorithmic combinations. Then, we select the algorithm that finds solutions with the best computational performance among all the generated algorithms. This best algorithm is compared with three state-of-the-art algorithms for the problem, presenting the best computational performance for the 131 instances in the literature.},
  archive      = {J_ASOC},
  author       = {Moisés Silva-Muñoz and Carlos Contreras-Bolton and Carlos Rey and Victor Parada},
  doi          = {10.1016/j.asoc.2023.110474},
  journal      = {Applied Soft Computing},
  pages        = {110474},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic generation of a hybrid algorithm for the maximum independent set problem using genetic programming},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust estimation of distribution algorithms via fitness
landscape analysis for optimal low-thrust orbital maneuvers.
<em>ASOC</em>, <em>144</em>, 110473. (<a
href="https://doi.org/10.1016/j.asoc.2023.110473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One particular kind of evolutionary algorithms known as Estimation of Distribution Algorithms (EDAs) has gained the attention of the aerospace industry for its ability to solve nonlinear and complicated problems, particularly in the optimization of space trajectories during on-orbit operations of satellites. This article describes an effective method for optimizing the trajectory of a spacecraft using an evolutionary approach based on EDAs, incorporated with fitness landscape analysis (FLA). The approach utilizes flexible operators that are paired with seeding and selection mechanisms of EDAs. Initially, the orbit transfer problem is mathematically modeled and the objectives and constraints are identified. The landscape feature of the search space is analyzed via the dispersion metric to measure the modality and ruggedness of the search domain. The obtained information are used as feedback in developing adaptive operators for truncation factor and constraints separation threshold of the employed EDA. A framework for spacecraft trajectory optimization has been presented where the dispersion value for a space mission is estimated using a k -nearest neighbors ( k -NN) algorithm. The suggested method is used to solve several problems related to low-thrust orbit transfer of satellites in Earth’s orbit. Results demonstrate that the suggested framework for trajectory design and optimization of space transfers is effective enough to offer fuel-efficient and energy-efficient maneuvers for different thrust levels of the propulsion system . Moreover, the performance of the proposed approach is evaluated against non-adaptive EDA and other advanced evolutionary algorithms . The obtained results certify that the proposed adaptive evolutionary approach is superior in identifying feasible minimum-fuel and minimum-energy transfer trajectories .},
  archive      = {J_ASOC},
  author       = {Abolfazl Shirazi},
  doi          = {10.1016/j.asoc.2023.110473},
  journal      = {Applied Soft Computing},
  pages        = {110473},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust estimation of distribution algorithms via fitness landscape analysis for optimal low-thrust orbital maneuvers},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparative study on evolutionary multi-objective
algorithms for next release problem. <em>ASOC</em>, <em>144</em>,
110472. (<a href="https://doi.org/10.1016/j.asoc.2023.110472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next release problem (NRP) refers to implementing the next release of software in the software industry regarding the expected revenues; specifically, constraints like limited budgets indicate that the total cost corresponding to the next software release should be minimized. This paper uses and investigates the comparative performance of nineteen state-of-the-art evolutionary multi-objective algorithms, including NSGA-II, rNSGA-II, NSGA-III, MOEAD, EFRRR, tDEA , KnEA, MOMBIII, SPEA2, RVEA, NNIA, HypE, ANSGA-III, BiGE, GrEA, IDBEA, SPEAR, SPEA2SDE, and MOPSO , that can tackle this problem. The problem was designed to maximize customer satisfaction and minimize the total required cost. Three indicators, namely hyper-volume (HV), spread, and runtime, were examined to compare the algorithms. Two types of datasets, i.e., classic and realistic data, from small to large scale, were also examined to verify the applicability of the results. Overall, NSGA-II exhibited the best CPU run time in all test scales, and, also, the results show that the HV and spread values of 1st and 2nd best algorithms (NNIA and SPEAR), for which most HV values for NNIA are bigger than 0.708 and smaller than 1, while the HV values for SPEAR vary between 0.706 and 0.708. Finally, the conclusion and direction for future works are discussed.},
  archive      = {J_ASOC},
  author       = {Iman Rahimi and Amir H. Gandomi and Mohammad Reza Nikoo and Fang Chen},
  doi          = {10.1016/j.asoc.2023.110472},
  journal      = {Applied Soft Computing},
  pages        = {110472},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comparative study on evolutionary multi-objective algorithms for next release problem},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). LKDPNet: Large-kernel depthwise-pointwise convolution
neural network in estimating coal ash content via data augmentation.
<em>ASOC</em>, <em>144</em>, 110471. (<a
href="https://doi.org/10.1016/j.asoc.2023.110471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Product quality monitoring is one of the most critical demands in the coal industry. Conventional coal quality analysis is offline, laborious, and lagging behind coal production. Using machine vision for determining ash content in coal has been recently developed. However, there are some challenges in the model design due to its task complexity. A data augmentation method for a specific task was proposed to deal with the peculiarities of a small, unbalanced industrial dataset. For estimating ash content in coal from an image without background, we provided a trajectory going from a ResNet to an LKDPNet under the guidance of receptive field size . First, the model depth was determined by changing the stage ratio. Second, patch embedding was employed to substitute the stem cell of ResNet for downsampling. Third, a residual connection block of depthwise convolution followed by pointwise convolution was designed to replace the ResNet identify and conv blocks. The receptive field of the model was adjusted by increasing the kernel size of the depthwise convolution layer and removing the downsampling block. The results and visualization revealed that the proposed LKDPNet could significantly improve performance with an accuracy of 98.91\% and an MAE of 0.082. It could pave the way to an online analysis of coal, thus accelerating intelligent coal production.},
  archive      = {J_ASOC},
  author       = {Kanghui Zhang and Weidong Wang and Ziqi Lv and Junda Feng and Huixuan Li and Chenglian Zhang},
  doi          = {10.1016/j.asoc.2023.110471},
  journal      = {Applied Soft Computing},
  pages        = {110471},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LKDPNet: Large-kernel depthwise-pointwise convolution neural network in estimating coal ash content via data augmentation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel similarity-based multigranulation three-way decision
approach to hypertension risk assessment with multi-source and
multi-level structure data. <em>ASOC</em>, <em>144</em>, 110470. (<a
href="https://doi.org/10.1016/j.asoc.2023.110470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early intervention and prevention of chronic diseases is a fundamental way to reduce disease incidence. Due to the complex pathogenic factors of disease, the assessment of disease risk requires consideration of the influence of multiple factors. In clinical practice, disease risk assessment may face data with multi-source, multi-type and multiple indicators of examination items. Mining useful information and rules in this complex information is the foundation for further decisions. However, the existing knowledge representation methods cannot comprehensively and accurately describe these characteristic of data. Therefore, we introduce the concept of multiple hybrid attribute information systems (MHAISs). Then, we defined the binary relations over MHAIS by fusing different types of kernel functions . On this basis, we construct a variable precision multigranulation kernel rough set (VPMGKRS), and propose a multigranulation three-way decision method over MHAIS. In addition, considering the individual difference between decision objects and the diversity of data characteristic among sources, we calculate the loss functions under different granular with the help of conditional probabilities , and then obtain the thresholds for the three-way decisions. Finally, we use 712 clinical random samples to conduct a simulation analysis applying the proposed method to assess the risk of hypertension. The experiment result confirmed the applicability and validity of the model.},
  archive      = {J_ASOC},
  author       = {Ting Wang and Bingzhen Sun and Chao Jiang},
  doi          = {10.1016/j.asoc.2023.110470},
  journal      = {Applied Soft Computing},
  pages        = {110470},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Kernel similarity-based multigranulation three-way decision approach to hypertension risk assessment with multi-source and multi-level structure data},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid wavelet-neural network models for time series.
<em>ASOC</em>, <em>144</em>, 110469. (<a
href="https://doi.org/10.1016/j.asoc.2023.110469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of wavelet analysis contributes to better modeling for financial time series in the sense of both frequency and time. In this study, S&amp;P500 and NASDAQ data are separated into several components utilizing multiresolution analysis (MRA). Subsequently, using an appropriate neural network structure , each component is modeled. In addition, wavelets are used as an activation function in long short-term memory (LSTM) networks to form a hybrid model. The hybrid model is merged with MRA as a proposed method in this paper. Four distinct strategies are employed: LSTM, LSTM+MRA, hybrid LSTM-Wavenet, and hybrid LSTM-Wavenet+MRA. Results show that the use of MRA and wavelets as an activation function together reduces the error the most.},
  archive      = {J_ASOC},
  author       = {Deniz Kenan Kılıç and Ömür Uğur},
  doi          = {10.1016/j.asoc.2023.110469},
  journal      = {Applied Soft Computing},
  pages        = {110469},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid wavelet-neural network models for time series},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient (2N+1) selective harmonic elimination in modular
multilevel converters using an evolutionary many-tasking approach with
prior knowledge. <em>ASOC</em>, <em>144</em>, 110468. (<a
href="https://doi.org/10.1016/j.asoc.2023.110468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {(2N+1) selective harmonic elimination pulse with modulation (SHE-PWM) is an important technique for a modular multi-level converter (MMC) to enhance the control ability and improve the fundamental wave amplitude accuracy of the output waveform . While finding optimal switch angles by solving the complex non-linear equation system is one of its main challenges. A current popular solution is to run a metaheuristic or evolutionary algorithm repeatedly on various modulation indexes . These methods ignore the fact that some of the optimization tasks are similar, and the optimization experiences could be shared among these similar tasks. To fill this gap, this study proposes an evolutionary many-tasking approach with prior knowledge (EMT-PK). EMT-PK facilitates positive knowledge transfer between similar tasks and is capable to solve all the sub SHE-PWM problems simultaneously through a single run. Numerical results on 7-, 9- and 11-level MMCs show that EMT-PK provides superior results for the (2N+1) SHE-PWM in comparison with 8 other widely used metaheuristics/evolutionary algorithms in this field including differential evolution (DE), ant colony optimization (ACO), teaching and learning-based optimization (TLBO), bee algorithm (BA), genetic algorithm (GA), particle swarm optimization (PSO), generalized pattern search (GPS) and asynchronous particle swarm optimization-genetic algorithm(APSO-GA). The superior performance of EMT-PK is further validated by the MATLAB Simulink simulation and the laboratory experiment on a 7-level MMC.},
  archive      = {J_ASOC},
  author       = {Huayan Pu and Zexin Bai and Yayun Xin and Jinglei Zhao and Ruqing Bai and Jun Luo and Jin Yi},
  doi          = {10.1016/j.asoc.2023.110468},
  journal      = {Applied Soft Computing},
  pages        = {110468},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient (2N+1) selective harmonic elimination in modular multilevel converters using an evolutionary many-tasking approach with prior knowledge},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast sharpness-aware training for periodic time series
classification and forecasting. <em>ASOC</em>, <em>144</em>, 110467. (<a
href="https://doi.org/10.1016/j.asoc.2023.110467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various deep learning architectures have been developed to capture long-term dependencies in time series data , but challenges such as overfitting and computational time still exist. The recently proposed optimization strategy called Sharpness-Aware Minimization (SAM) optimization prevents overfitting by minimizing a perturbed loss within the nearby parameter space. However, SAM requires doubled training time to calculate two gradients per iteration, hindering its practical application in time series modeling such as real-time assessment. In this study, we demonstrate that sharpness-aware training improves generalization performance by capturing trend and seasonal components of time series data . To avoid the computational burden of SAM, we leverage the periodic characteristics of time series data and propose a new fast sharpness-aware training method called Periodic Sharpness-Aware Time series Training (PSATT) that reuses gradient information from past iterations. Empirically, the proposed method achieves both generalization and time efficiency in time series classification and forecasting without requiring additional computations compared to vanilla optimizers.},
  archive      = {J_ASOC},
  author       = {Jinseong Park and Hoki Kim and Yujin Choi and Woojin Lee and Jaewook Lee},
  doi          = {10.1016/j.asoc.2023.110467},
  journal      = {Applied Soft Computing},
  pages        = {110467},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fast sharpness-aware training for periodic time series classification and forecasting},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reduced bayesian optimized stacked regressor (RBOSR): A
highly efficient stacked approach for improved air pollution prediction.
<em>ASOC</em>, <em>144</em>, 110466. (<a
href="https://doi.org/10.1016/j.asoc.2023.110466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, stacked generalization has emerged as a promising approach for predicting the concentration of PM 2.5 2.5 , but its use is often associated with reduced efficiency due to computational demands. In this study, we propose a new approach called Reduced Bayesian Optimized Stacked Regressor (RBOSR) that optimizes both the performance and efficiency of the PM 2.5 2.5 stacked model. RBOSR incorporates Bayesian optimization for hyperparameter tuning, ensemble-based feature selection, dimensionality reduction via single-link hierarchical clustering , and recursive base estimator eliminations. The RBOSR model is significantly more efficient when compared to the original stacked model, with 5.7 times shorter training time. Additionally, the RBOSR model outperformed the unreduced stacked model with an R 2 value of 0.91 and RMSE of 26.46, resulting in a 3.5\% improvement. Compared to other PM 2.5 2.5 stacked models proposed in recent studies, the RBOSR model demonstrates superior efficiency, with up to 47 times shorter training time. While the RBOSR approach has been developed specifically for PM 2.5 2.5 prediction, it has the potential for broader applications in other regression problems . Although the approach has not yet been applied to other datasets, future work could explore its applicability to a broader range of datasets and the development of more efficient strategies for optimizing base estimator selection.},
  archive      = {J_ASOC},
  author       = {Danny Hartanto Djarum and Zainal Ahmad and Jie Zhang},
  doi          = {10.1016/j.asoc.2023.110466},
  journal      = {Applied Soft Computing},
  pages        = {110466},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reduced bayesian optimized stacked regressor (RBOSR): A highly efficient stacked approach for improved air pollution prediction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evo-GUNet3++: Using evolutionary algorithms to train
UNet-based architectures for efficient 3D lung cancer detection.
<em>ASOC</em>, <em>144</em>, 110465. (<a
href="https://doi.org/10.1016/j.asoc.2023.110465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The early detection of malignant lung nodules can strongly increase the chances of life in lung cancer patients . A computer tomography scan represents an effective way to identify and locate malignant nodules in the body and monitor their growth. However, the reading and interpretation of tomography scans are subject to errors that can be reduced with a second reader. The adoption of image processing systems can reduce the possibility of errors and can support radiologists in ensuring multiple readings of tomography scans. This study proposes a new approach for accurate 3D lung nodule detection starting from computer tomography scans. This work exploits an evolutionary algorithm to build variants of a UNet-based architecture, called GUNet3++, to detect patients affected by lung cancer, from the analysis of CT-scan images of lungs. The approach is validated on the LIDC-IDRI real dataset and results show that it improves segmentation quality metrics (IoU and Dice) over baselines, leading to better 3D models reconstruction of lesions.},
  archive      = {J_ASOC},
  author       = {Pasquale Ardimento and Lerina Aversano and Mario Luca Bernardi and Marta Cimitile and Martina Iammarino and Chiara Verdone},
  doi          = {10.1016/j.asoc.2023.110465},
  journal      = {Applied Soft Computing},
  pages        = {110465},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evo-GUNet3++: Using evolutionary algorithms to train UNet-based architectures for efficient 3D lung cancer detection},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint-optimized feature selection and classifier
hyperparameters by salp swarm algorithm in piano score difficulty
measurement problem. <em>ASOC</em>, <em>144</em>, 110464. (<a
href="https://doi.org/10.1016/j.asoc.2023.110464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes the semantic-explicit features that characterize difficulty, and jointly optimizes feature selection and classifier hyperparameters by the salp swarm algorithm (SSA) to deal with the corresponding mixed-integer programming problem with constructing large-scale piano score difficulty level datasets. The difficulty level of piano scores is essential for piano learners to choose the appropriate piece, especially for beginners and amateurs. However, the previous studies lack an open-access baseline dataset and sufficient difficulty-related features, as well as the separate optimization of and feature and model hyperparameter. To address such problems, this study constructs large-scale difficulty-level datasets, proposes novel difficulty-related features, and jointly optimizes feature selection and classifier hyperparameters due to the coupled effect of feature selection and model optimization. The search space of the joint optimization is complex due to the strong mutual constraint relationship of difficulty levels in the piano-score difficulty measurement (PSDM) problem. SSA is adapted to the joint optimization scheme of the PSDM problem with the advantages of only one main controlling parameter and less computation complexity involving the gradual SSA movement approach to balance global exploration and local exploitation in an unknown and complex search space. The joint-optimization mechanism by SSA achieves an overall accuracy of 78.80\% and 60.68\% on two datasets of 677 and 2040 piano pieces with difficulty levels of four and nine, respectively. The results of recognition accuracy obviously validate the distinguished performance of our joint-optimization scheme compared to the successive optimization and joint optimization by other seven optimization algorithms in terms of the PSDM problem.},
  archive      = {J_ASOC},
  author       = {Hanhan Yan and Qiang Li and Ming-Lang Tseng and Xin Guan},
  doi          = {10.1016/j.asoc.2023.110464},
  journal      = {Applied Soft Computing},
  pages        = {110464},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint-optimized feature selection and classifier hyperparameters by salp swarm algorithm in piano score difficulty measurement problem},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary echo state network: A neuroevolutionary
framework for time series prediction. <em>ASOC</em>, <em>144</em>,
110463. (<a href="https://doi.org/10.1016/j.asoc.2023.110463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From one side, Evolutionary Algorithms have enabled enormous progress over the last years in the optimization field. They have been applied to a variety of problems, including optimization of Neural Networks’ architectures. On the other side, the Echo State Network (ESN) model has become increasingly popular in time series prediction, for instance when modeling chaotic sequences . The network has numerous hidden neurons forming a recurrent topology, so-called reservoir , which is fixed during the learning process. Initial reservoir design has mostly been made by human experts; as a consequence, it is prone to errors and bias, and it is a time consuming task. In this paper, we introduce an automatic general neuroevolutionary framework for ESNs, on which we develop a computational tool for evolving reservoirs, called EVOlutionary Echo State Network (EvoESN). To increase efficiency, we represent the large matrix of reservoir weights in the Fourier space , where we perform the evolutionary search strategy. This frequency space has major advantages compared with the original weight space. After updating the Fourier coefficients , we go back to the weight space and perform a conventional training phase for full setting the reservoir architecture. We analyze the evolutionary search employing genetic algorithms and particle swarm optimization , obtaining promising results with the latter over three well-known chaotic time series. The proposed framework leads fast to very good results compared with modern ESN models. Hence, this contribution positions an important family of recurrent systems in the promising neuroevolutionary domain.},
  archive      = {J_ASOC},
  author       = {Sebastián Basterrech and Gerardo Rubino},
  doi          = {10.1016/j.asoc.2023.110463},
  journal      = {Applied Soft Computing},
  pages        = {110463},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary echo state network: A neuroevolutionary framework for time series prediction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive sequential sampling method based on ANN_MCD and
RF: Application in geotechnical problems. <em>ASOC</em>, <em>144</em>,
110462. (<a href="https://doi.org/10.1016/j.asoc.2023.110462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In geotechnical engineering , surrogate modeling is a vital technique for enhancing computational efficiency when faced with limited and costly samples. However, most adaptive sequential sampling (ASS) methods use the Kriging model , which directly evaluates the uncertainty of the model at unsampled points, but they are not efficient in solving multiple output problems. A few studies use cross-validation to indirectly evaluate the uncertainty of other surrogate models , which is time-consuming to repeatedly train multiple models. In addition, few methods consider input feature sensitivity, which further reduces the efficiency of ASS methods. To address these issues, this study proposes a hybrid ASS method that combines artificial neural networks with Monte Carlo dropout (ANN_MCD) and random forests (RF). The ANN is suitable for multiple output problems, and the MCD enables the ANN architecture to stochastically transform and efficiently predict uncertainty at unsampled points with a single model training. The RF evaluates input feature sensitivity, assigns weights to the sampling space based on feature sensitivity, and effectively avoids the interference of low-sensitivity features. Furthermore, two active learning functions are proposed to regulate the sampling areas on either a global scale or close to the limit state function , depending on the problem type. The study validates the efficacy of the proposed method through three representative geotechnical engineering examples: system reliability analysis of soil slope, back-analysis of soil constitutive parameters, and estimating the penetration rate in diamond drilling. The outcomes demonstrate that the proposed method has less computational cost than related studies and has more potential in addressing geotechnical sampling.},
  archive      = {J_ASOC},
  author       = {Weihang Chen and Jianwen Ding and Tengfei Wang and Zi Ying and Xing Wan},
  doi          = {10.1016/j.asoc.2023.110462},
  journal      = {Applied Soft Computing},
  pages        = {110462},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive sequential sampling method based on ANN_MCD and RF: Application in geotechnical problems},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual LSTM based short-term load forecasting.
<em>ASOC</em>, <em>144</em>, 110461. (<a
href="https://doi.org/10.1016/j.asoc.2023.110461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the modern energy systems is becoming more complex and flexible, accurate load forecasting has been the key to scheduling power to meet customers’ needs, load switching, and infrastructure development. In this paper, we propose a neural network framework based on a modified deep residual network (DRN) and a long short-term memory (LSTM) recurrent neural network (RNN) for addressing the short-term load forecasting (STLF) problem. The proposed model not only inherits the DRN’s excellent characteristic to avoid vanishing gradient for training deeper neural networks , but also continues the LSTM’s strong ability to capture nonlinear patterns for time series forecasting. Moreover, through the dimension weighted units based on attention mechanism , the dimension-wise feature response is adaptively recalibrated by explicitly modeling the interdependencies between dimensions, so that we can jointly improve the performance of the model from three aspects: depth, time and feature dimension. The snapshot ensemble method has also been applied to improve the accuracy and robustness of the proposed model. By implementing multiple sets of experiments on two public datasets, we demonstrate that the proposed model has high accuracy, robustness and generalization capability, and can perform STLF better than the existing mainstream models.},
  archive      = {J_ASOC},
  author       = {Ziyu Sheng and Zeyu An and Huiwei Wang and Guo Chen and Kun Tian},
  doi          = {10.1016/j.asoc.2023.110461},
  journal      = {Applied Soft Computing},
  pages        = {110461},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Residual LSTM based short-term load forecasting},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified total bregman divergence driven picture fuzzy
clustering with local information for brain MRI image segmentation.
<em>ASOC</em>, <em>144</em>, 110460. (<a
href="https://doi.org/10.1016/j.asoc.2023.110460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research work discusses a noise-robust picture fuzzy clustering method with an application to the MRI image segmentation problem. The MRI images suffer from the problem of noise and non-linear structures. Although there are many variants of fuzzy set theory and intuitionistic fuzzy set theory-based clustering approaches to handle the problem of noise and non-linearity present in the image during the segmentation process , still they lack in achieving accurate segmentation. To overcome this problem, we have suggested using the picture fuzzy set theoretic approach which enhances the representational capability of the data and helps in handling the non-linear structures present in the image, and in our proposed work, the picture fuzzy Euclidean distance is replaced with modified picture fuzzy total Bregman divergence using the spatial neighborhood information around the sample. Squared Euclidean distance is itself a special case of Bregman Divergence, hence Bregman divergence helps to explore the details in a better way thus resulting in noise suppression . Furthermore, the proposed algorithm proves to be robust in preserving the image details using picture fuzzy set theory and is free from any parameter selection due to the incorporation of a local information factor. The algorithm was performed on a synthetic image corrupted with “Gaussian”, “salt and pepper”, “mixture of Gaussian and salt and pepper noise” of different intensities, “Brainweb” datasets corrupted with noise, six real “IBSR” datasets along with seven “MRbrainS18” datasets. Performance measures used were partition coefficient, partition entropy, dice score (DS), average segmentation accuracy (ASA), and XB index, and the proposed method was robust when compared to a series of algorithms stated in the literature.},
  archive      = {J_ASOC},
  author       = {Himanshi Lohit and Dhirendra Kumar},
  doi          = {10.1016/j.asoc.2023.110460},
  journal      = {Applied Soft Computing},
  pages        = {110460},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified total bregman divergence driven picture fuzzy clustering with local information for brain MRI image segmentation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum healthcare computing using precision based granular
approach. <em>ASOC</em>, <em>144</em>, 110458. (<a
href="https://doi.org/10.1016/j.asoc.2023.110458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previously, doctors interpreted diseases and their outcomes according to their experience in diagnosis. However, with the rapid increase in technology and population, the task of examining the patient becomes cumbersome and sometimes human efforts produce inconsistent results. Several research is being done for healthcare in terms of improving visualization and accuracy by using machine learning models. The current research targets to explore quantum computing as a different way of processing information compared to classical computer systems such as the use of quantum bits (qubits) along with superposition and entanglement for extending the computation capabilities at an unprecedented level of thinking in the healthcare domain. Quantum computing systems provide exponential benefits in terms of high-speed processing, faster and easier diagnostic assistance, unimaginable reduction in processing throughput, and many more. An extensive comparative analysis of existing approaches has been made which benchmarks the need for quantum healthcare computing. The objective of this work is to interpret whether Quantum computers prove to be more trusted when it comes to patient diagnosis, and faster analysis leading to cost optimization. In order to accelerate patient diagnosis, different approaches have been presented. The authors have proposed a precision-based granular approach for patient diagnosis that incorporates diagnosing the disease with enhanced precision and granularity . It involves reporting symptoms by the patient, encountering by healthcare expert on multiple factors, precise examination, granular health status (understanding past and present medical history), followed by a precise intervention by understanding biomolecular simulations. The algorithm has been presented to describe the flow process for patient diagnosis modeling using quantum computing . It involves qubits initialization, pairing the values, assigning probabilistic values, cross-validation, and quantum circuit formation. Precision-based granular approach has been implemented for a scenario (consisting of medical parameters such as oxygen and heart rate level, with the functionality of diagnosing oxygen level and heart range which lies as either normal or not normal (high/low)). Precision-based granular approach deals specifically with the individual ‘biomolecular simulation by understanding variations in the individual body whereas the umbrella-based approach does not deal with specifically to individual mechanisms. Granular level of encounter is not possible in umbrella-based treatment. Python Jupyter notebook and IBM Composer tool is used for the implementation of results. Bloch sphere and computational state graph are obtained as an output for better visualization and understanding. Falcon r5.11H processor is used with the version of 1.0.24 of IBM Composer to simulate the experiment. The methodology using precision based granular approach provides timely encounter of disease along with umbrella diagnosis and precise treatment. The time is taken and frequency of qubits have been presented with promising results. The diagnosis process and optimizing cost efficiency can aid in an early detection of the disease.},
  archive      = {J_ASOC},
  author       = {Lakshita Aggarwal and Shelly Sachdeva and Puneet Goswami},
  doi          = {10.1016/j.asoc.2023.110458},
  journal      = {Applied Soft Computing},
  pages        = {110458},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum healthcare computing using precision based granular approach},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New coding scheme to compile circuits for quantum
approximate optimization algorithm by genetic evolution. <em>ASOC</em>,
<em>144</em>, 110456. (<a
href="https://doi.org/10.1016/j.asoc.2023.110456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compiling quantum circuits on target quantum hardware architectures is one of the key issues in the development of quantum algorithms , and the related problem is known as the Quantum Circuit Compilation Problem (QCCP). This paper presents a genetic algorithm for solving QCCP instances for Quantum Approximate Optimization Algorithms (QAOA). In particular, such instances represent quantum circuits for the resolution of both MaxCut and Graph-Coloring combinatorial problems . The presented algorithm represents a significant improvement over an already existing genetic algorithm called Decomposition Based Genetic Algorithm (DBGA), and is characterized by a completely new coding scheme that allows to reduce the number of SWAP gates introduced in the decoding step, consequently reducing the circuit depth. After providing a description of the problem, this paper presents the newly produced genetic algorithm (termed DBGA-X) in detail, especially focusing on the new coding/decoding scheme. Subsequently, a set of results will be presented that demonstrate the superior performance of the new method compared with the results obtained from recent literature against the same benchmark. In addition, new benchmarks characterized by larger quantum architectures and by a higher number of compilation passes are proposed in this paper, to the aim of testing the scalability of the proposed method in more realistic scenarios.},
  archive      = {J_ASOC},
  author       = {Lis Arufe and Riccardo Rasconi and Angelo Oddi and Ramiro Varela and Miguel A. González},
  doi          = {10.1016/j.asoc.2023.110456},
  journal      = {Applied Soft Computing},
  pages        = {110456},
  shortjournal = {Appl. Soft. Comput.},
  title        = {New coding scheme to compile circuits for quantum approximate optimization algorithm by genetic evolution},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consensus reaching process with noncooperative behaviors in
large-scale group social network environment. <em>ASOC</em>,
<em>144</em>, 110454. (<a
href="https://doi.org/10.1016/j.asoc.2023.110454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale group decision-making (LSGDM) problems have drawn general attention from scholars. The distribution linguistic preference relation (DLPR) is a flexible and practical tool to describe the preferences of decision makers (DMs) in the decision-making process. Opinion conflict is inevitable among large-scale DMs due to self-interest and individual perception. Thus, it is essential to establish a consensus reaching process (CRP), but there may be non-cooperative behaviors for DMs when they are required to accept the opinions adjustment. As the social network becomes more ubiquitous, moreover, the trust relationship among decision members has implications for the consensus reaching process. Hence, this study proposes a new consensus model that manages non-cooperative behaviors from the cooperative degree, trust relationship, and individual self-confidence level three aspects, and discusses the specific influence of these factors on the penalty for non-cooperative behaviors. In addition, hesitancy-based similarity measure of linguistic distributed assessment is proposed for clustering and measuring consensus level. Finally, a numerical example of a price hearing system demonstrates the feasibility and efficacy of the proposed method, and a comparative analysis illustrates its features and advantages.},
  archive      = {J_ASOC},
  author       = {Xinli You and Fujun Hou and Francisco Chiclana},
  doi          = {10.1016/j.asoc.2023.110454},
  journal      = {Applied Soft Computing},
  pages        = {110454},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consensus reaching process with noncooperative behaviors in large-scale group social network environment},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expanded feature space-based gradient boosting ensemble
learning for risk prediction of type 2 diabetes complications.
<em>ASOC</em>, <em>144</em>, 110451. (<a
href="https://doi.org/10.1016/j.asoc.2023.110451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a metabolic disease, diabetes is a serious threat to human health, in which type 2 diabetes (T2D) constitutes about 90\% of diabetic cases. As the disease progresses, T2D often comes with various complications such as kidney disease and retinopathy, which may cause huge threats to patients’ health. In recent years, complication prediction of T2D has gained wide research interest as early disease detection and management can effectively reduce the risk of death. In this paper, to break through the limitation of the available data and achieve higher overall prediction accuracy, an improved feature space-based gradient boosting regression tree ensemble (IFS-GBRTE) approach is proposed for the risk prediction of developing T2D complications. Specifically, the original feature space is expanded based on existing feature generation theories. Then the new feature space is refined by comparing the total contribution of each feature during the construction of a classification and regression tree (CART) under a cross-validation scheme. Finally, a gradient boosting ensemble algorithm using CART as base learners is utilized for model training. The prediction model is validated on the T2D data provided by the China National Clinical Medical Science Data Center . The experimental results show that the proposed IFS-GBRTE achieves 82.49\% accuracy and realizes better generalization ability than compared single models and ensemble learning models, meanwhile it is helpful to obtain more effective predictive variables for improving the accuracy of risk prediction of T2D complications, which is of great significance for achieving the early prevention, screening and nursing of complications, and further reducing mortality and save medical resources.},
  archive      = {J_ASOC},
  author       = {Yuyan Wang and Sutong Wang and Xiutian Sima and Yu Song and Shaoze Cui and Dujuan Wang},
  doi          = {10.1016/j.asoc.2023.110451},
  journal      = {Applied Soft Computing},
  pages        = {110451},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Expanded feature space-based gradient boosting ensemble learning for risk prediction of type 2 diabetes complications},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective optimization ensembled generative design for
large-diameter tunnel passing underneath existing tunnels.
<em>ASOC</em>, <em>144</em>, 110448. (<a
href="https://doi.org/10.1016/j.asoc.2023.110448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated and intelligent design in the construction industry has drawn considerable attention in these years along with the modern industrial revolutions. However, the achievement is strongly held up by the high unreliability in the underground space. To ameliorate this situation, this study develops a fuzzy robust multi-objective optimization (FRMOO) ensembled generative design system based on a large-diameter tunnel construction project with existing tunnels lying above. The robust part of the algorithm focuses on dealing with the unavoidable uncertainties from geotechnical conditions by seeking more conservative solutions. The fuzzy logic transforms the objective from minimizing the damage to seeking the most satisfactory solution by balancing all requirements from objectives and constraints. From evaluating the improvement in safety and the designer’s satisfaction degree simultaneously, the FRMOO approach reaches an overall improvement of 29.15\%, which is found significantly superior to merely using robust optimization (18.93\%) or simply using deterministic optimization (19.13\%). Besides the FRMOO segment, the generative design system is also ensembled with a parametric modeling segment, which automatically builds a model based on the optimal solutions, and a user interface for the designer to interact with.},
  archive      = {J_ASOC},
  author       = {Penghui Lin and Limao Zhang and Robert L.K. Tiong},
  doi          = {10.1016/j.asoc.2023.110448},
  journal      = {Applied Soft Computing},
  pages        = {110448},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization ensembled generative design for large-diameter tunnel passing underneath existing tunnels},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive dual-population based evolutionary algorithm for
industrial cut tobacco drying system. <em>ASOC</em>, <em>144</em>,
110446. (<a href="https://doi.org/10.1016/j.asoc.2023.110446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial cut tobacco drying is one of the most important processes in cigarette production, which affects the taste, cut tobacco consumption and other indicators of cigarette products. Due to the complicated process and parameters involved, the production of drying system is difficult to improve. In this paper, the model of tobacco drying system is established and optimized. First, an eighth-order nonlinear first-principle model is established, and its corresponding constrained multi-objective optimization problem is constructed based on the multiple requirements in industrial production. Furthermore, an adaptive dual-population based evolutionary algorithm (ADPEA) is proposed in which an assistant population is introduced to balance the feasibility, diversity and convergence. Feasible solutions are preferentially reserved to the next generation in the main population, while diversity and convergence are considered more in the assistant population. The ADPEA is used to optimize the tobacco drying system and is compared with four state-of-the-art multi-objective evolution algorithms. The experimental results reveal that ADPEA has a better performance, and the optimization results could help engineers adjust the process parameters according to the requirements of different batches and brands of cigarette products to ensure that the whole production process can meet the technological requirements while saving energy and reducing emissions.},
  archive      = {J_ASOC},
  author       = {Xue Feng and Anqi Pan and Zhengyun Ren and Juchen Hong and Zhiping Fan and Yinghao Tong},
  doi          = {10.1016/j.asoc.2023.110446},
  journal      = {Applied Soft Computing},
  pages        = {110446},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive dual-population based evolutionary algorithm for industrial cut tobacco drying system},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A game-theoretical constructive approach for the
multi-objective frequency assignment problem. <em>ASOC</em>,
<em>144</em>, 110444. (<a
href="https://doi.org/10.1016/j.asoc.2023.110444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents the game modeling of the multi-objective frequency assignment problem (FAP) in cellular networks considering two conflicting objectives: interference and separation costs, while respecting separation constraints, and thus improving the QoS for end users. Two types of cooperative games are suggested depending on the TRX and frequency selection strategies each using two players, one for each considered objective. A feasible solution consisting of a frequency assignment plan is built during the constructive process. At each step of the game, a player assigns a frequency to a TRX according to a given strategy. The proposed solution being part of both multi-objective optimization (MOO) and game theory (GT), we used the most important measures in these two fields: the hypervolume (HV) and the Nash value, to evaluate the performance of our solutions to able to compare our results with those of these two communities: MOO and GT. The proposed games achieve a good trade-off between the two considered objectives corroborated by tests on two real-world instances, Denver and Seattle, demonstrating the performance of the proposed approach in solving the problem. Furthermore, the proposed performance evaluation method is innovative and also generic, since it can be adapted to any algorithm based on both GT and MOO, taking into account these two aspects.},
  archive      = {J_ASOC},
  author       = {Fatma Laidoui and Malika Bessedik and Fatima Benbouzid-Si Tayeb},
  doi          = {10.1016/j.asoc.2023.110444},
  journal      = {Applied Soft Computing},
  pages        = {110444},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A game-theoretical constructive approach for the multi-objective frequency assignment problem},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lexicographic maximum dynamic evacuation modeling with
partial lane reversal based on hesitant fuzzy TOPSIS. <em>ASOC</em>,
<em>144</em>, 110435. (<a
href="https://doi.org/10.1016/j.asoc.2023.110435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making under evacuation environment is extremely complex due to the variety of aspects to be considered as well as the necessity of high-speed reaction. Therefore, the elaboration of the evacuation pattern, particularly, finding the priority order of shelters for evacuation, requires multiple experts to be involved in the decision-making process. Moreover, decision-makers cannot provide precise assessments of alternatives due to the uncertainty inherent in the network parameters, dynamic nature of transportation, complexity of the task. In this regard, the method of finding the priority order of terminals for the evacuation based on the fuzzy hesitant TOPSIS method is proposed. The method handles a modified ranking index to find the priority order based on assigned weights of separations. In this method, experts’ assessments are presented as linguistic terms , which are further converted to fuzzy triangular numbers. This leads to the hesitant fuzzy TOPSIS decision-making with completely unknown attribute weights. The existing state of the art of evacuation modeling lacks fuzzy statements of evacuation flow problems. Our work aims to contribute to the design of the algorithm for the lexicographic maximum flow finding in the evacuation dynamic network with fuzzy transit arc capacities and transit traversal time parameters with partial contraflow. The approach reverses only necessary arcs by reversing the traffic in vacant segments, which allows using these segments for movement towards the safe areas as well. In addition, the method based on linear combinations of spreads is proposed to handle fuzzy values, which does not lead to the blurring of a fuzzy number. A case study to find the priority order of four destinations d 1 , d 2 , d 3 , d 4 d1, d2, d3, d4 along with the maximum dynamic flow determining is provided to illustrate the proposed method. The hesitant fuzzy TOPSIS evaluated the priority order of destinations for evacuation the maximum number of aggrieved as { d 4 , d 3 , d 1 , d 2 } {d4, d3, d1, d2} along with maximum lexicographic flow with partial lane reversal 62 ˜ 62˜ flow units. The method for finding the spreads without blurring a fuzzy number was used to determine the final triangular number (50, 62, 75) units. The sensitivity analysis shows that in all ten cases, the alternatives have unique rank: A 4 A4 is the best, A 2 A2 is the worst. In the pessimistic case, DMs prefers the alternative that is furthest to FNIS. In the optimistic case, DMs prefer the alternative that is closest to FPIS. We proved that the proposed algorithm is reliable because despite the changes in parameters ω ω , the same priority order of alternatives is chosen. The obtained results are confirmed by efficiency frontier and coincide with the results of original TOPSIS, Doukas et al.’s, method, Kuo’s method, Yoon and Kim’s behavioral TOPSIS.},
  archive      = {J_ASOC},
  author       = {Janusz Kacprzyk and Alexander Bozhenyuk and Evgeniya Gerasimenko},
  doi          = {10.1016/j.asoc.2023.110435},
  journal      = {Applied Soft Computing},
  pages        = {110435},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lexicographic maximum dynamic evacuation modeling with partial lane reversal based on hesitant fuzzy TOPSIS},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A modified artificial bee colony algorithm based on a
non-dominated sorting genetic approach for combined economic-emission
load dispatch problem. <em>ASOC</em>, <em>144</em>, 110433. (<a
href="https://doi.org/10.1016/j.asoc.2023.110433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-objective optimization algorithms (MOO) are used to obtain the best compromising solutions when two or more objective functions need to be optimized simultaneously. The convergence and diversity are critical factors to consider while solving the MOO problems because they determine the possibility of obtaining an evenly distributed Pareto front . This paper proposes a hybrid optimization algorithm to solve a multi-objective economic emission dispatch (MOEED) problem of electrical power systems. The electrical power generated by consuming fossil fuels is very costly and also burning of these fuels contributes to global warming. Hence, electrical power is generated at the least cost and emission. The MOEED problems have been solved in the past by using swarm intelligence and evolutionary algorithms . However, the solutions reported in the literature, are either inferior or the constraints are violated. The algorithm proposed in this paper is an integration of an Artificial Bee Colony optimization algorithm and a Non-Dominated Sorting Genetic Algorithm-II. The effectiveness of the proposed algorithm is evaluated by applying it to three test systems having 6, 10, and 40 coal-based generators. Additionally, various multi-criteria decision-making algorithms are used to identify the best non-dominated solutions obtained by the proposed algorithm and compared with previously reported results. The best fuel costs obtained by the proposed approach, for a 6-unit test system with and without transmission losses are found to be 605.9983 and 600.11140 $/h respectively. While the best emission values, for this test system, with and without transmission losses are found to be 0.1941 and 0.1942 tons/h. Moreover, the best fuel costs obtained by the proposed approach, for 10 and 40-unit test systems are found to be 111181.9871 and 121369.0838 $/h respectively. Furthermore, the best emission values for these test systems are found to be 3932.24322 and 176682.264 tons/h respectively. All these results are obtained without constraint violations and within 10–600 iterations.},
  archive      = {J_ASOC},
  author       = {Maneesh Sutar and H.T. Jadhav},
  doi          = {10.1016/j.asoc.2023.110433},
  journal      = {Applied Soft Computing},
  pages        = {110433},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A modified artificial bee colony algorithm based on a non-dominated sorting genetic approach for combined economic-emission load dispatch problem},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Error feedback method (EFM) based dimension synthesis
optimisation for four-bar linkage mechanism. <em>ASOC</em>,
<em>144</em>, 110424. (<a
href="https://doi.org/10.1016/j.asoc.2023.110424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose an error-feedback method (EFM) for constraint handling of the path-generation problem, which replaces the traditional penalty-functional method. Moreover, it proportionally redistributes solutions that do not satisfy the constraint conditions, such that each offspring can effectively participate in the iterative process and avoid additional computing resource wastage. To test the performance, the EFM, penalty, and self-adaptive–penalty-functional methods were applied to heuristic algorithms , such as the differential evolution (DE), teaching learning-based optimisation (TLBO), whale optimisation algorithm (WOA), and gaining–sharing knowledge (GSK). Three four-bar mechanisms of dimensional synthesis and one path-generation problem of a pickup manipulator was considered as examples. The numerical results showed that the algorithm processed by the new constraint-processing method exhibited improved performance compared to other methods.},
  archive      = {J_ASOC},
  author       = {Kai Zhang and Mingwei Yang and Yimin Zhang and Qiujun Huang},
  doi          = {10.1016/j.asoc.2023.110424},
  journal      = {Applied Soft Computing},
  pages        = {110424},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Error feedback method (EFM) based dimension synthesis optimisation for four-bar linkage mechanism},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated multi-criteria decision-making approach for
evaluating e-waste mitigation strategies. <em>ASOC</em>, <em>144</em>,
110420. (<a href="https://doi.org/10.1016/j.asoc.2023.110420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for effective electronic waste (e-waste) management practices has received wide attention due to increased societal awareness; however, the existence of challenges adds difficulties. This study identifies these critical e-waste management challenges through extensive literature reviews and targeted interaction with e-waste recycling units, and our work suggests strategies for e-waste mitigation. Then, an integrated framework of four methods — Fermatean fuzzy set (FFS), analytic hierarchy process (AHP), Decision-Making Trial and Evaluation Laboratory (DEMATEL), and Technique for order performance by similarity to ideal solution (TOPSIS) – calculates the weightage of the challenges, depicts causal interrelationships among the challenges, and ranks the mitigation strategies . For India, the top five most critical challenges to e-waste management practices include the booming informal sector, illegal import of e-waste, lack of technical expertise, absence of guidelines for e-waste recycling, and lack of data inventory. India’s governmental establishment of an infrastructure for modern e-waste management represents the best potential strategy. Understanding the causal interrelationship and weightage of the challenges may help policymakers, industrial practitioners, and government agencies streamline e-waste management activities.},
  archive      = {J_ASOC},
  author       = {Koppiahraj Karuppiah and Bathrinath Sankaranarayanan},
  doi          = {10.1016/j.asoc.2023.110420},
  journal      = {Applied Soft Computing},
  pages        = {110420},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated multi-criteria decision-making approach for evaluating e-waste mitigation strategies},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuromorphic deep learning frequency regulation in
stand-alone microgrids. <em>ASOC</em>, <em>144</em>, 110418. (<a
href="https://doi.org/10.1016/j.asoc.2023.110418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency instability has been a growing problem in recent years due to the rising penetration of distributed generating systems in the format of Microgrids (MGs), which are powered by renewable energy sources (RESs) with an unpredictable nature. The provision of an effective load frequency control (LFC) to an MG model under such conditions contributes significantly to the restoration regarding the unconfigured power system’s stability. An MG structure is employed, which combines renewable energy sources like solar and wind, as well as biologically renewable sustainable energy sources like wastewater, agricultural and domestic wastes, and contains appropriate battery systems and is suited to achieve both energy and waste control in this study. In addition, redox flow battery (RFB), an innovative battery storage system with a fast dynamic response in this MG structure, has been considered as one of the energy storage devices . A nonlinear integral backstepping (NIB) controller is adopted to stabilize the frequency deviation of the integrated MG system with RFB under various level of load disturbances and randomness of RESs. In particular, a spike neural network (SNN) based on neuromorphic platform is proposed to adjust the coefficients of NIB controller. Furthermore, real time analyses are performed with OPAL-RT to validate the feasibility of the proposed strategy from a systematical viewpoint. As a result of this study, the proposed controller for load disturbance, RES power changes, and contingency circumstances in bio renewable MG is determined to be flexible enough to satisfy the efficient frequency regulation for these grids.},
  archive      = {J_ASOC},
  author       = {Burak Yildirim and Peyman Razmi and Arman Fathollahi and Meysam Gheisarnejad and Mohammad Hassan Khooban},
  doi          = {10.1016/j.asoc.2023.110418},
  journal      = {Applied Soft Computing},
  pages        = {110418},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neuromorphic deep learning frequency regulation in stand-alone microgrids},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A representation learning framework for stock movement
prediction. <em>ASOC</em>, <em>144</em>, 110409. (<a
href="https://doi.org/10.1016/j.asoc.2023.110409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learning of high-quality stock representations is one of the keys to predicting stock movements effectively. Current studies have been negatively impacted by stochasticity in stock prices, resulting in inadequate representation learnt by the models. We present an end-to-end stock movement prediction framework (CLSR) utilizing contrastive learning to exploit the correlation between intra-day data and enhance stock representation in order to improve the accuracy of stock movement prediction. In addition, a hybrid encoding network is developed to extract long-range dependencies and local contextual features in stock data, making the feature representation more complete. To further improve the prediction accuracy of the model, historical state information is added to the intra-day stock data. Our experiments on CSI-500 show that the proposed method outperforms state-of-the-art solutions. The proposed method is also validated by analyzing the representation space thoroughly.},
  archive      = {J_ASOC},
  author       = {Wenzhi Feng and Xiang Ma and Xuemei Li and Caiming Zhang},
  doi          = {10.1016/j.asoc.2023.110409},
  journal      = {Applied Soft Computing},
  pages        = {110409},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A representation learning framework for stock movement prediction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal weighted GAN and u-net based segmentation for
phenotypic trait estimation of crops using taylor coot algorithm.
<em>ASOC</em>, <em>144</em>, 110396. (<a
href="https://doi.org/10.1016/j.asoc.2023.110396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and robust collection of plant phenotypic data offers theoretical as well as technical support to support the growth of crop science and to ensure ecological security, agricultural growth, and food security. Identifying phenotypic traits of crops refers to the detection of difference exists in plant features caused due to interaction of the environment and plant genetics. It is an important research discussed in plant breeding as it permits breeders to find a variety of crops with physical features, like stress resistance, and high yield. Manual measurement of phenotypic traits in the area is labor intensive and causes inaccurate results and these issues are resolved by developing a method based on the Taylor Coot algorithm for segmenting plant regions and biomass area to detect emergence counting and to estimate the biomass of crops. The process of counting emergence and estimating the biomass is performed in a parallel way using a Deep Residual Network (DRN) that is trained by developed optimization. The segmentation framework is done using Generative Adversarial Network (GAN) and U-Net to segment the plant regions and biomass area. For instance, the extraction of vegetation indices makes the process of biomass estimation to generate more optimal features using a deep learning model. Moreover, the proposed model obtains minimal Mean Absolute Difference (MAD), Standard Absolute Difference (SDAD),\%Difference (\%D) as 0.073, 0.074, and 16.45 for emergence counting. Moreover, the DRN shows higher performance by attaining minimum MAD, SDAD, and\%D as 0.069, 0.096, and 14.85 for biomass estimation.},
  archive      = {J_ASOC},
  author       = {Sandip Debnath and Anusha Preetham and Shankar Vuppu and Sanjay Nakharu Prasad Kumar},
  doi          = {10.1016/j.asoc.2023.110396},
  journal      = {Applied Soft Computing},
  pages        = {110396},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal weighted GAN and U-net based segmentation for phenotypic trait estimation of crops using taylor coot algorithm},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diagnosis-based domain-adaptive design using designable data
augmentation and bayesian transfer learning: Target design estimation
and validation. <em>ASOC</em>, <em>143</em>, 110459. (<a
href="https://doi.org/10.1016/j.asoc.2023.110459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on prognosis and health management (PHM) technology is actively being conducted. Existing diagnostic technologies focus on qualitative results, i.e., anomaly detection and classification for maintenance. Therefore, quantified diagnostic solutions are required to allow users to take clear actions in advance. In this paper, we propose a diagnostic design solution methodology from a system design perspective, considering the degradation and uncertainty of the system via combined data-driven and model-based approaches using domain adaptation and designable data augmentation . When designing a new target system similar to an existing developed source system, the uncertainty of the diagnostic knowledge of the existing source system is quantified to adapt the domain knowledge to the new target system through Bayesian transfer learning . Additionally, with small amount of target data, a deep-learning-based design algorithm is used to estimate the system design solutions. To validate the proposed method, a case study was conducted using mathematical and Modelica-based physical system models. We verified that the system response obtained from the estimated design solution is more likely to belong to the normal class than to the initial design.},
  archive      = {J_ASOC},
  author       = {Myeongsun Kwak and Jongsoo Lee},
  doi          = {10.1016/j.asoc.2023.110459},
  journal      = {Applied Soft Computing},
  pages        = {110459},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diagnosis-based domain-adaptive design using designable data augmentation and bayesian transfer learning: Target design estimation and validation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdamR-GRUs: Adaptive momentum-based regularized GRU for HMER
problems. <em>ASOC</em>, <em>143</em>, 110457. (<a
href="https://doi.org/10.1016/j.asoc.2023.110457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten Mathematical Expression Recognition (HMER) is essential to online education and scientific research. However, discerning the length and characters of handwritten mathematical expressions remains a formidable challenge. In this study, we propose an encoder–decoder architecture incorporating a novel Adaptive momentum-based Regularized Gated Recurrent Unit (AdamR-GRU) to solve this problem. It incorporates a novel gate within the GRU framework to maintain input histories, enabling extended memory retention and faster convergence. To assess the performance of AdamR-GRU, we conducted a series of comprehensive experiments on the CROHME 2014, 2016, and 2019 datasets. The results indicate that the AdamR-GRU model exhibits competitive performance with existing state-of-the-art methods regarding Expression Recognition Rate (exprate). Moreover, our model achieves state-of-the-art accuracy concerning 1, 2, and 3 symbol-level errors (top 1, 2, 3 error accuracy) on the CROHME 2014 and 2016 datasets. For the CROHME 2019 dataset, the model attains state-of-the-art performance in top 2 and 3 error accuracy . AdamR-GRU demonstrates reduced computational complexity and memory usage compared to alternative contemporary models.},
  archive      = {J_ASOC},
  author       = {Aniket Pal and Krishna Pratap Singh},
  doi          = {10.1016/j.asoc.2023.110457},
  journal      = {Applied Soft Computing},
  pages        = {110457},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AdamR-GRUs: Adaptive momentum-based regularized GRU for HMER problems},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Requirements engineering framework for human-centered
artificial intelligence software systems. <em>ASOC</em>, <em>143</em>,
110455. (<a href="https://doi.org/10.1016/j.asoc.2023.110455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360° videos intended for virtual reality (VR) users. We found that our proposed approach helped the project team fully understand the human-centered needs of the project to deliver. Furthermore, the framework helped to understand what requirements need to be captured at the initial stages against later stages in the engineering process of AI-based software.},
  archive      = {J_ASOC},
  author       = {Khlood Ahmad and Mohamed Abdelrazek and Chetan Arora and Arbind Agrahari Baniya and Muneera Bano and John Grundy},
  doi          = {10.1016/j.asoc.2023.110455},
  journal      = {Applied Soft Computing},
  pages        = {110455},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Requirements engineering framework for human-centered artificial intelligence software systems},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FGSS: Federated global self-supervised framework for
large-scale unlabeled data. <em>ASOC</em>, <em>143</em>, 110453. (<a
href="https://doi.org/10.1016/j.asoc.2023.110453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the unique advantages of collaborative learning on isolated yet unlabeled data , federated self-supervised learning has received increasing attention from both academic and industrial researchers. Most of the existing federated self-supervised approaches concentrate on the classical scenario, i.e., a large amount of unlabeled data is stored on the clients. However, in many real-world applications, partial labels may be available to the client user, while a large amount of unlabeled data remains on the server side. The existing federated self-supervised methods may usually have difficulty in addressing this scenario. In this paper, we propose a creative federated global self-supervised framework (FGSS) for large-scale unlabeled data that innovatively uses self-supervised learning on the server side, and during every round of communication, we use a small amount of labeled data from the client to facilitate the performance of self-supervised learning. To address the heterogeneity of local data from different clients, we designed an aggregation approach that can adjust the weight of each local model based on the frequency of participation in the communication and the size of its dataset. Experimental results show that our framework outperforms the most existing state-of-the-art methods in both IID and non-IID settings under certain conditions.},
  archive      = {J_ASOC},
  author       = {Chen Zhang and Zixuan Xie and Bin Yu and Chao Wen and Yu Xie},
  doi          = {10.1016/j.asoc.2023.110453},
  journal      = {Applied Soft Computing},
  pages        = {110453},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FGSS: Federated global self-supervised framework for large-scale unlabeled data},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An advanced stratified decision-making strategy to explore
viable plastic waste-to-energy method: A step towards sustainable dumped
wastes management. <em>ASOC</em>, <em>143</em>, 110452. (<a
href="https://doi.org/10.1016/j.asoc.2023.110452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dumping waste and dumping sites pose significant environmental issues with serious implications for human health, the environment, and wildlife. Dumping grounds can also cause difficulties such as leaching, which occurs when waste chemicals seep into the soil and damage groundwater resources. The vast quantity of dumped waste plastic is a major source of energy in the form of gas, fuel, and electricity. The selection of the most appropriate waste-to-energy (WtE) technology affects the dumped waste because it allows for reduced greenhouse gas emissions, diverts dumpsites, generates energy, supports the circular economy, and lowers post-recycling pollution. This complex selection problem requires careful consideration of uncertainties from multi-perspective analysis, vague information, and imprecision in decisions. The stratified fuzzy multi-criteria decision making approach is suitable for addressing different states of events, uncertain data, and sustainable choice selection. Hence, we improved the intuitionistic analytical hierarchical process with a stratified targeting concept for multi-level criteria importance and coupled it with weighted aggregated sum product assessment for ranking WtE techniques. The preferences of experts are characterized using intuitionistic fuzzy preference relations . An empirical study on dumped plastic waste and dumpsite management discovered that plasma technology becomes a more feasible option by satisfying stratification and is more effective in clearing landfills by recovering energy. The comparative analysis, sensitive analysis , and Spearman’s rank correlation validate the proposed methodology, and this method seems to be a new dimension to handle ecological issues.},
  archive      = {J_ASOC},
  author       = {Daekook Kang and Thangaraj Manirathinam and Selvaraj Geetha and Samayan Narayanamoorthy and Massimiliano Ferrara and Ali Ahmadian},
  doi          = {10.1016/j.asoc.2023.110452},
  journal      = {Applied Soft Computing},
  pages        = {110452},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An advanced stratified decision-making strategy to explore viable plastic waste-to-energy method: A step towards sustainable dumped wastes management},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prophet-EEMD-LSTM based method for predicting energy
consumption in the paint workshop. <em>ASOC</em>, <em>143</em>, 110447.
(<a href="https://doi.org/10.1016/j.asoc.2023.110447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy conservation and preventive maintenance of equipment require the ability to accurately predict future trends in shop floor power consumption to keep track of equipment operation and abnormalities. Due to the non-linearity and uncertainty of the workshop electrical energy consumption data , it is very difficult to establish an accurate energy consumption prediction model. For the above problems, an energy consumption prediction model based on Prophet-EEMD-LSTM was proposed. To identify the period features in the energy consumption time series data , the Prophet algorithm was introduced to identify and extract the period and trend features of the energy consumption data . Then for the feature of the uncertainty of the remaining data, this paper used the ensemble empirical mode decomposition (EEMD) method to decompose the data to form the components with specific modalities. Finally, all the extracted feature data were put into the input layer of the Long short-term memory (LSTM), and the final energy consumption values were predicted using the excellent prediction performance of the LSTM on time-series data. The experimental results showed that the model proposed in this paper has high prediction performance for energy consumption data.},
  archive      = {J_ASOC},
  author       = {Yingkang Lu and Buyun Sheng and Gaocai Fu and Ruiping Luo and Geng Chen and Yuzhe Huang},
  doi          = {10.1016/j.asoc.2023.110447},
  journal      = {Applied Soft Computing},
  pages        = {110447},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prophet-EEMD-LSTM based method for predicting energy consumption in the paint workshop},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical multi-UAVs task assignment based on dominance
rough sets. <em>ASOC</em>, <em>143</em>, 110445. (<a
href="https://doi.org/10.1016/j.asoc.2023.110445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task assignment of multi-UAVs is important to realize the effective cooperation and enhance the comprehensive performance of multi-UAVs. Heuristic algorithms have become popular methods to solve this problem. As random searching algorithms , the performance of existing heuristic algorithms degrades when solving large-scale or high-dimensional problems, and depends on additional parameter settings. Unlike the idea of heuristic algorithms approximating the optimal solution by searching for the solution space, this paper takes a new approach of gradually acquiring the assignment result based on human cognition, called a hierarchical multi-UAVs task assignment by the optimal ranking strategy based on dominance rough sets (HORD). HORD first establishes the optimal ranking strategy for task assignment of all UAVs based on the dominance rough set theory . Then, a hierarchical multi-UAV task assignment algorithm is built. Comparative experimental results verify that the HORD algorithm speeds up the assignment and increases the total utility without relying on additional parameters.},
  archive      = {J_ASOC},
  author       = {Haihuan Jiang and Guoyin Wang and Qun Liu and Peng Gao and Xin Huang},
  doi          = {10.1016/j.asoc.2023.110445},
  journal      = {Applied Soft Computing},
  pages        = {110445},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical multi-UAVs task assignment based on dominance rough sets},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving job shop scheduling problems via deep reinforcement
learning. <em>ASOC</em>, <em>143</em>, 110436. (<a
href="https://doi.org/10.1016/j.asoc.2023.110436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning (DRL), as a promising technique, is a new approach to solve the job shop scheduling problem (JSSP). Although DRL method is effective for solving JSSP, there are still deficiencies in state representation, action space definition, and reward function design, which make it difficult for the agent to learn effective policy. In this paper, we model JSSP as a Markov decision process (MDP) and design a new state representation using the state features of bidirectional scheduling, which can not only enable the agent to capture more effective state information, improve its decision-making ability, but also effectively avoid the phenomenon of multiple optimal action selections in candidate action set. Invalid action masking (IAM) technique is employed to narrow the search space, which helps the agent avoid exploring suboptimal solutions. We evaluate the performance of the policy model on eight public test datasets : ABZ, FT, ORB, YN, SWV, LA, TA, and DMU. Extensive experimental results show that the proposed method on the whole has better optimization ability than the existing state-of-the-art models and priority dispatching rules .},
  archive      = {J_ASOC},
  author       = {Erdong Yuan and Shuli Cheng and Liejun Wang and Shiji Song and Fang Wu},
  doi          = {10.1016/j.asoc.2023.110436},
  journal      = {Applied Soft Computing},
  pages        = {110436},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving job shop scheduling problems via deep reinforcement learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interval type-2 fuzzy set based block-SBU for image fusion
technique. <em>ASOC</em>, <em>143</em>, 110434. (<a
href="https://doi.org/10.1016/j.asoc.2023.110434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion is the process of examining one or more obscure images, combining only the critical information in those images to make them a good quality image. Due to the large gap between the high dynamic range of landscape scenes and the low quality of consumer quality cameras, a single-shot image cannot record the information of a single set. A part of image processing called image fusion is used to overcome the above problem. In this method, authors propose a new technology for image fusion with intuitionistic fuzzy sets. During the processing, the given images are converted into fuzzy images and then intuitionistic fuzzy images ( I F I s ) (IFIs) . Thus, a significant change occurs between the given image and I F I s IFIs . The resulting I F I s IFIs are converted into interval type-2 fuzzy images ( I T 2 F I s ) (IT2FIs) to overcome this problem. The proposed technique is compared to other procedures such as discrete cosine harmonic wavelet transform ( D C H W T ) (DCHWT) , multi-resolution single value decay ( M S V D ) (MSVD) , primary component analysis ( P C A ) (PCA) , standard wavelet transform ( S W T ) (SWT) , two scale image fusion ( T S I F ) (TSIF) , and wavelet transform ( W T ) (WT) . The proposed method gives the best results based on performance analysis such as Entropy, standard deviation ( S T D ) (STD) , average gradient ( A G ) (AG) , mutual information ( M I F ) (MIF) , and blind referenceless image spatial quality evaluator ( B R I S Q U E ) (BRISQUE) . Using the proposed technique, one can obtain the best results for entropy, STD, AG, MIF, and BRISQUE respectively as 7.6241, 49.6745, 69.2100, 5.7276, and 3.3091. Hence it reveals that the proposed method performs better than other methods regarding overall visual quality and performance measurements.},
  archive      = {J_ASOC},
  author       = {J. Reegan Jebadass and P. Balasubramaniam},
  doi          = {10.1016/j.asoc.2023.110434},
  journal      = {Applied Soft Computing},
  pages        = {110434},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval type-2 fuzzy set based block-SBU for image fusion technique},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate extended time horizon drought prediction via
wavelet-season-fuzzy models. <em>ASOC</em>, <em>143</em>, 110432. (<a
href="https://doi.org/10.1016/j.asoc.2023.110432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improved accurate prediction of the Palmer drought severity index (PDSI) is crucial for sustainable water supply demand, flood mitigation, management of hydraulic structures , sustainability of the ecosystem and, significant economic and social benefits. In this study, a novel predictive ASA-fuzzy model based on additive season algorithm (ASA) and fuzzy logic is proposed to enhance prediction accuracy with extended future lead times. For the first time, additive season algorithm (ASA) is introduced as an alternative data preprocessing algorithm in prediction of PDSI data obtained from measurement stations that exhibit distinct meteorological characteristics. The results showed that the newly proposed hybrid ASA-fuzzy approach can satisfactorily be utilized to predict monthly PDSI data up to 24-month time horizon. In comparison, for all stations’ data, the newly proposed ASA-fuzzy is found to be superior in accuracy to the stand-alone fuzzy and widely used W-fuzzy models for all lead time predictions based on the quantitative diagnostic measures, root mean squared error (RMSE) and the Nash–Sutcliffe coefficient of efficiency (CE). The remarkable performance of the introduced ASA-fuzzy model in this study clearly shows that the season algorithm can decompose the original data into trend cycle, seasonality and error components more effectively than wavelet technique. Therefore ASA-Fuzzy model has been advocated as a new prediction tool in predicting monthly PDSI data with superior accuracy when compared to conventional methods. In addition, when hybridized with a proper preprocessing algorithm, power of the fuzzy modeling approach in time-series forecasting is manifested.},
  archive      = {J_ASOC},
  author       = {Abdüsselam Altunkaynak and Anıl Çelik},
  doi          = {10.1016/j.asoc.2023.110432},
  journal      = {Applied Soft Computing},
  pages        = {110432},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Accurate extended time horizon drought prediction via wavelet-season-fuzzy models},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble feature selection using bonferroni, OWA and induced
OWA aggregation operators. <em>ASOC</em>, <em>143</em>, 110431. (<a
href="https://doi.org/10.1016/j.asoc.2023.110431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection becomes inevitable owing to a rapid increase in digital technology which permits the generation of high dimensional data in a large quantity within a short time. Feature selection techniques not only improves classification accuracy but also decreases time complexity as well as computation cost and storage. Ensemble feature selection has lately emerged as a potential approach to data mining. Identifying numerous optimal features is one of the key advantages of ensemble feature selection. The objective of this study is to introduce an ensemble technique based on the rank aggregation procedure. Four operators are used for the purpose of aggregation, namely the induced ordered weighted averaging (IOWA) operator, the normalized Bonferroni weighted mean operator, the Bonferroni-ordered weighted averaging operator and the Bonferroni-induced ordered weighted averaging operator . These operators enable the evaluation of continuous aggregations, multiple assessments between each input and distance measurements in the same formulation. An objective weighting approach denoted as the entropy weight method, is utilized to measure the degree of disorder. A total of 10 benchmark data sets are employed to evaluate the superiority of the proposed methodologies. The effectiveness of the proposed method is evaluated and compared using the accuracy, F-measure, precision and recall performance metrics, and better results than those from other existing techniques are obtained.},
  archive      = {J_ASOC},
  author       = {K. Janani and S.S. Mohanrasu and Chee Peng Lim and Balachandran Manavalan and R. Rakkiyappan},
  doi          = {10.1016/j.asoc.2023.110431},
  journal      = {Applied Soft Computing},
  pages        = {110431},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble feature selection using bonferroni, OWA and induced OWA aggregation operators},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Service-oriented model-based fault prediction and
localization for service compositions testing using deep learning
techniques. <em>ASOC</em>, <em>143</em>, 110430. (<a
href="https://doi.org/10.1016/j.asoc.2023.110430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As service-oriented computing systems become more buoyant and complex, the occurrence of faults dramatically increases. Fault prediction plays a crucial role in the service-oriented computing paradigm, aiming to reduce testing cost while maximizing testing quality to utilize testing resources effectively and increase the reliability of service compositions. Although various fault prediction techniques were considered in software testing, service-oriented systems were less fortunate, in which most of the studies have focused on single web services testing rather than service compositions. Moreover, mainly the detection of faulty/non-faulty services was addressed, ignoring the estimate of faults count, their severity, as well as predicting when and where such faults would occur. In this paper, a multilateral model-based fault prediction and localization approach is proposed using deep learning techniques for web service compositions testing rather than single web service testing, which uniquely predicts not only faulty services, but also their count and severity level , location of faults, and time at which faults would occur. Three deep learning models are investigated: Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) and a proposed hybrid model based on both CNN and RNN. The proposed approach is language-independent, as it adopts process metrics rather than code metrics to overcome the code unavailability concern of services. The experimental analysis adopted main performance metrics on multiple public datasets to evaluate its efficiency and effectiveness. The results indicated that the hybrid CNN_RNN model achieves an average accuracy range of 84\%–95.7\%, where the RNN and CNN models individually achieve 75\%–90\% and 70\%-79.3\% respectively. Thus, the hybrid model increases the accuracy level by 5\%–10\% and 15\%–20\%, while achieving the least mean square error of 30\% and 60\% compared to the RNN and CNN models respectively. In terms of time, the RNN model consumes less average time as of 30–50 ms for the different datasets of variant sizes compared to the CNN and hybrid CNN_RNN models that consume 79–102 and 177–224 ms respectively. Thus, RNN model consumes around 50\%–80\% less time than those of the CNN and hybrid models respectively.},
  archive      = {J_ASOC},
  author       = {Roaa ElGhondakly and Sherin M. Moussa and Nagwa Badr},
  doi          = {10.1016/j.asoc.2023.110430},
  journal      = {Applied Soft Computing},
  pages        = {110430},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Service-oriented model-based fault prediction and localization for service compositions testing using deep learning techniques},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Examining the role of class imbalance handling strategies in
predicting earthquake-induced landslide-prone regions. <em>ASOC</em>,
<em>143</em>, 110429. (<a
href="https://doi.org/10.1016/j.asoc.2023.110429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study was undertaken to propose a comprehensive prediction scheme containing the hybrid use of class imbalance handling strategies and machine learning methods to assess the earthquake-induced landslide susceptibility for the North Sikkim region. It is worth to mention that taking the class imbalance handling techniques into account is essential to mimic real-world conditions. To tackle this issue, this research for the first time focused on the comprehensive evaluation of nine scenarios comprising four oversampling, four undersampling, and a RAW data analysis techniques. The predictions were conducted with the stochastic gradient boosting (SGB) algorithm. Analysis results depicted that the SVM-SMOTE-SGB outperformed its counterparts (with an AUROC of 0.9878), followed by the models subjected to the pre-processing with BL-SMOTE (AUROC: 0.9876) and RUS (AUROC: 0.9859), respectively. Also, the major drawback of the black-box models, i.e., lack of interpretability , was overcome with a game-theoretical SHapley Additive explanation (SHAP) analysis. The SHAP application with respect to the best-performed model ensured the importance of distance to road, distance to stream, and elevation in the identification of earthquake-induced landslide prone regions.},
  archive      = {J_ASOC},
  author       = {Quoc Bao Pham and Ömer Ekmekcioğlu and Sk Ajim Ali and Kerim Koc and Farhana Parvin},
  doi          = {10.1016/j.asoc.2023.110429},
  journal      = {Applied Soft Computing},
  pages        = {110429},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Examining the role of class imbalance handling strategies in predicting earthquake-induced landslide-prone regions},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RaGMAN: A relativistic average generative multi-adversarial
network for pansharpening. <em>ASOC</em>, <em>143</em>, 110428. (<a
href="https://doi.org/10.1016/j.asoc.2023.110428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening aims to reconstruct a high-resolution multi-spectral image (HRMS) from a low spatial resolution multi-spectral (MS) image and a high spatial resolution single band panchromatic (PAN) image acquired from the same satellite. To solve the spatial information and spectral information of the fused image that are not evenly preserved, this paper proposes a relativistic average generative multi-adversarial network (RaGMAN) for pansharpening. The RaGMAN consists of two parts. One builds a two-stream generator network to generate HRMS images by extracting features from PAN and MS images. The other is a dual discriminator to preserve the spectral and spatial information of the input when performing fusion. To make the generated image contain more spatial information, we propose multiple residual dense block in the generator. At the same time, in order to improve the overall quality of the fused images, two relativistic average discriminators are used in the network. Furthermore, a novel hybrid loss function is introduced to optimize training. Compared with seven state-of-the-art methods on GF-2, QB and WV-3 datasets, experimental results show that the proposed RaGMAN method can produce excellent pansharpening performance in terms of spatial and spectral fidelity.},
  archive      = {J_ASOC},
  author       = {Yu Wang and Xiaoli Zhang and Bo Huang and Xiongfei Li and Amit Abu Sadat Mohammad Salehin and Rui Zhu},
  doi          = {10.1016/j.asoc.2023.110428},
  journal      = {Applied Soft Computing},
  pages        = {110428},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RaGMAN: A relativistic average generative multi-adversarial network for pansharpening},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive review of automatic programming methods.
<em>ASOC</em>, <em>143</em>, 110427. (<a
href="https://doi.org/10.1016/j.asoc.2023.110427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic programming (AP) is one of the most attractive branches of artificial intelligence because it provides effective solutions to problems with limited knowledge in many different application areas. AP methods can be used to determine the effects of a system’s inputs on its outputs. Although there is increasing interest in solving many problems using these methods for a variety of applications, there is a lack of reviews that address the methods. Therefore, the goal of this paper is to provide a comprehensive literature review of AP methods. At the same time, we mention the main characteristics of the methods by grouping them according to how they represent solutions. We also try to give an outlook on the future of the field by highlighting possible bottlenecks and perspectives for the benefit of the researchers involved.},
  archive      = {J_ASOC},
  author       = {Sibel Arslan and Celal Ozturk},
  doi          = {10.1016/j.asoc.2023.110427},
  journal      = {Applied Soft Computing},
  pages        = {110427},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive review of automatic programming methods},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TLBO merged with studying effect for economic environmental
energy management in high voltage AC networks hybridized with
multi-terminal DC lines. <em>ASOC</em>, <em>143</em>, 110426. (<a
href="https://doi.org/10.1016/j.asoc.2023.110426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, different types of non-linear and multi-modal hybridized alternating current (AC) power grids with multi-terminal high voltage direct current (HVDC) are of great importance. Energy management aims to reduce costs, network power losses, and environmental pollutants in AC-HVDC electricity grids. This paper describes an improved variant of TLBO (teaching–learning-based optimization) entitled teaching–learning studying-based optimizer (TLSBO) that improves TLBO’s global optimization performance. Nevertheless, an advanced model of preventive strategy action is presented that offers an appropriate margin in power output and power transfer through transmission lines to plan for possible contingencies. Following that, a contingency study is conducted to guarantee the system’s operation. Also, a corrective strategy action via Re-performing the energy management in the high voltage AC–DC network considering the most critical lines that prevents the system from converging. The suggested upgrade is formed by the inclusion of a change initiative to TLBO known as the studying approach, wherein one individual borrows additional data from some other randomized member in improving its situation. The suggested TLSBO is assessed and contrasted to TLBO and various existing approaches on modified IEEE 30-bus, 57-bus, and large-scale 118-bus AC-HVDC systems. Also, the efficiency of the proposed TLSBO is declared versus the top three techniques in CEC2020 Competition. Compared to the standard TLBO, the proposed TLSBO achieves improvements of 0.04, 4.25, 15.65 and 65.84\% for the best, mean, worst and standard deviation metrics, respectively. The simulation outcomes show that the developed TLSBO outperforms the others in terms of efficacy and resilience. TLSBO, on the other hand, has convergence speed and superior quality for the ultimate optimal solution, as well as greater power for escape from converging to local optima than basic TLBO.},
  archive      = {J_ASOC},
  author       = {Shahenda Sarhan and Ragab A. El-Sehiemy and Abdullah M. Shaheen and Mona Gafar},
  doi          = {10.1016/j.asoc.2023.110426},
  journal      = {Applied Soft Computing},
  pages        = {110426},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TLBO merged with studying effect for economic environmental energy management in high voltage AC networks hybridized with multi-terminal DC lines},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Set algebra — based algebraic evolutionary algorithm for
binary optimization problems. <em>ASOC</em>, <em>143</em>, 110425. (<a
href="https://doi.org/10.1016/j.asoc.2023.110425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to design algebraic evolutionary algorithms by set algebra, the intersection, union, complement, difference and symmetric difference operations of 0-1 vectors on { 0 , 1 } n {0, 1}n are firstly defined based on set operations. Then, the isomorphism between algebraic system defined on { 0 , 1 } n {0, 1}n and set algebra defined on power set P ( Ω ) P(Ω) of set Ω Ω is proved. Therefrom a simple and fast implement method of set algebra is proposed. Third, symmetric difference operator and asymmetric mutation operator are successively proposed based on set algebra, they have global exploration and local exploitation capabilities respectively. On this basis, a novel algebraic evolutionary algorithm, named set algebra-based heuristic algorithm (SAHA), is proposed based on the operations of 0-1 vectors on { 0 , 1 } n {0, 1}n for solving binary optimization problems . For verifying the performance of SAHA, it is used to solve 0-1 knapsack problem (0-1KP) and knapsack problem with single continuous variable (KPC), respectively. The comparison with the state-of-the-art algorithms of solving 0-1KP and KPC shows that SAHA can not only obtain excellent calculation results, but also is faster speed, it is most competitive for solving binary optimization problems .},
  archive      = {J_ASOC},
  author       = {Yichao He and Hailu Sun and Yuan Wang and Xinlu Zhang and Seyedali Mirjalili},
  doi          = {10.1016/j.asoc.2023.110425},
  journal      = {Applied Soft Computing},
  pages        = {110425},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Set algebra — based algebraic evolutionary algorithm for binary optimization problems},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning based source identification of environmental
audio signals using optimized convolutional neural networks.
<em>ASOC</em>, <em>143</em>, 110423. (<a
href="https://doi.org/10.1016/j.asoc.2023.110423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research in the field of environmental sounds is a growing area due to its enormous potential and its applications. One of the major factors that affect the model performance is the noisy, redundant, or irrelevant features. Deep learning models have shown promise in this area, but the extraction of optimal features from audio signals and classification efficiency of the model are still challenging issues in this field. To address the challenges faced by existing methods, this research proposes a unique deep learning framework-based model that employs an enhanced bio-inspired algorithm for feature extraction and environmental sound classification. The quality and relevance of the training features are essential for the model’s accuracy, and a novel algorithm is introduced to select optimal features for improved performance. The algorithm is further improved for weight optimization to address overfitting and accuracy issues. Additionally, a modified version of the Discrete Fourier Transform is introduced to reduce computational complexity , which makes the model more suitable for real-time applications or resource-limited devices. This research emphasizes the necessity for improved algorithms for feature selection and weight optimization. The proposed model exhibits excellent accuracy and efficiency, making it suitable for real-time applications.},
  archive      = {J_ASOC},
  author       = {Krishna Presannakumar and Anuj Mohamed},
  doi          = {10.1016/j.asoc.2023.110423},
  journal      = {Applied Soft Computing},
  pages        = {110423},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning based source identification of environmental audio signals using optimized convolutional neural networks},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HiRN: Hierarchical recurrent neural network for video
super-resolution (VSR) using two-stage feature evolution. <em>ASOC</em>,
<em>143</em>, 110422. (<a
href="https://doi.org/10.1016/j.asoc.2023.110422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of video super-resolution (VSR) is generate the high-resolution (HR) frames from their low-resolution (LR) counterparts. As one of the fundamental module of VSR, propagation process provides the path of feature map and specifies how the feature map is leveraged. In the recurrent propagation, the latent features can be propagated and aggregated. Therefore, adopting the recurrent strategy can resolve the limitation of sliding-window-based local propagation. Recently, bi-directional recurrent propagation-based latest methods have achieved powerful performance in VSR. However, existing bi-directional frameworks have structured by combining forward and backward branches. These structures cannot propagate and aggregate previous and future latent features of current branch. In this study, we suggest the hierarchical recurrent neural network (HiRN) based on feature evolution. The proposed HiRN is designed based on the hierarchical recurrent propagation and residual block-based backbone with temporal wavelet attention (TWA) module. The hierarchical recurrent propagation consists of two stages to combine advantages of low frame rate-based forward and backward schemes, and multi-frame rate-based bi-directional access structure. The proposed methods are compared with state-of-the-art (SOTA) methods on the benchmark datasets. Experiments show that the proposed scheme achieves superior performance compared with SOTA methods. In particular, the proposed HiRN achieves better performance than all compared methods in terms of SSIM on Vid4 benchmark. In addition, the proposed HiRN surpasses the existing GBR-WNN by a significant 3.03 dB in PSNR on REDS4 benchmark with fewer parameters.},
  archive      = {J_ASOC},
  author       = {Young-Ju Choi and Byung-Gyu Kim},
  doi          = {10.1016/j.asoc.2023.110422},
  journal      = {Applied Soft Computing},
  pages        = {110422},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HiRN: Hierarchical recurrent neural network for video super-resolution (VSR) using two-stage feature evolution},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Requirements practices and gaps when engineering
human-centered artificial intelligence systems. <em>ASOC</em>,
<em>143</em>, 110421. (<a
href="https://doi.org/10.1016/j.asoc.2023.110421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engineering Artificial Intelligence (AI) software is a relatively new area with many challenges, unknowns, and limited proven best practices. Big companies such as Google, Microsoft, and Apple have provided a suite of recent guidelines to assist engineering teams in building human-centered AI systems. The practices currently adopted by practitioners for developing such systems, especially during Requirements Engineering (RE), are little studied and reported to date. This paper presents the results of a survey conducted to understand current industry practices in RE for AI (RE4AI) and to determine which key human-centered AI guidelines should be followed. Our survey is based on mapping existing industrial guidelines, best practices, and efforts in the literature. We surveyed 29 professionals and found most participants agreed that all the human-centered aspects we mapped should be addressed in RE. Further, we found that most participants were using UML or Microsoft Office to present requirements. We identify that most of the tools currently used are not equipped to manage AI-based software, and the use of UML and Office may pose issues with the quality of requirements captured for AI. Also, all human-centered practices mapped from the guidelines should be included in RE.},
  archive      = {J_ASOC},
  author       = {Khlood Ahmad and Mohamed Abdelrazek and Chetan Arora and Muneera Bano and John Grundy},
  doi          = {10.1016/j.asoc.2023.110421},
  journal      = {Applied Soft Computing},
  pages        = {110421},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Requirements practices and gaps when engineering human-centered artificial intelligence systems},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-based gate recurrent unit for remaining useful
life prediction in prognostics. <em>ASOC</em>, <em>143</em>, 110419. (<a
href="https://doi.org/10.1016/j.asoc.2023.110419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential process in prognostics and health management (PHM) is remaining useful life (RUL) prediction. The traditional Recurrent Neural Networks (RNNs) and their variants are not very efficient at solving the regression problems of RUL prediction. Given this problem, an attention-based Gate Recurrent Unit (ABGRU) for RUL prediction is proposed in this paper. Firstly, the dataset is preprocessed, and the RUL labels are modeled using the piecewise linear degradation method. Then, a GRU network based on an encoder–decoder framework with an attention mechanism is proposed. The network can assign weights according to the importance of feature information and effectively use the feature information to predict RUL. The validity of the proposed framework is verified in the NASA C-MAPSS benchmark dataset. The results show that the presented method outperforms the existing state-of-the-art approaches and provides a new solution for RUL Prediction.},
  archive      = {J_ASOC},
  author       = {Ruiguan Lin and Huawei Wang and Minglan Xiong and Zhaoguo Hou and Changchang Che},
  doi          = {10.1016/j.asoc.2023.110419},
  journal      = {Applied Soft Computing},
  pages        = {110419},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-based gate recurrent unit for remaining useful life prediction in prognostics},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of metaheuristics algorithm on a multi-objective
container loading problem considering container’s utilization and
vehicle’s balance. <em>ASOC</em>, <em>143</em>, 110417. (<a
href="https://doi.org/10.1016/j.asoc.2023.110417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The container loading problem (CLP) intends to arrange a set of boxes into a container by considering different aspects, such as the dimension and weight of the box, capacity of the container, etc. In the multi-destination delivery, box arrangement should also consider routing, or delivery order. In addition, it is crucial to balance the boxes for safety in order to avoid traffic accident . Therefore, in order to overcome the aforementioned problems, this study proposes a mathematical model with two objectives, maximizing container’s utilization and minimizing truck’s unbalance force penalty. The proposed model also considers some practical constraints including geometry, maximum weight, and delivery order, or loading priority. Two metaheuristics , non-dominated sorting genetic algorithm II (NSGA II) and multi-objective particle swarm optimization algorithm (MOPSO) are employed to solve the proposed model and verified by using seven benchmark datasets in terms of hypervolume and spacing. The experimental results show that both algorithms have their own merits.},
  archive      = {J_ASOC},
  author       = {R.J. Kuo and Pei-Cheng Ho and Ferani E. Zulvia},
  doi          = {10.1016/j.asoc.2023.110417},
  journal      = {Applied Soft Computing},
  pages        = {110417},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of metaheuristics algorithm on a multi-objective container loading problem considering container’s utilization and vehicle’s balance},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DCARS: Deep context-aware recommendation system based on
session latent context. <em>ASOC</em>, <em>143</em>, 110416. (<a
href="https://doi.org/10.1016/j.asoc.2023.110416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems (RSs) usually create suggestions based on users’ prior intentions. Users’ interests may evolve due to context change or user-mode change. Discovering such a change is crucial for producing personalized suggestions. Traditional approaches assume that each user has a fixed preference. On the contrary, context-aware recommendation systems (CARSs) use contextual information to detect user intention changes. However, applying contextual information is the main challenge in CARSs, because it is not always feasible to achieve all the users’ contextual information. Furthermore, adding different contexts to RSs grows its dimensionality in multiple applications. Besides, existing CARSs cannot precisely obtain the hierarchical relationships between items and contexts items that influence users’ intentions. They often use short-term interest with either static long-term preference in the recommendation process. To alleviate the mentioned challenges, we propose a novel deep context-aware recommendation system (DCARS) to capture and incorporate user preferences changes in the recommendation process. The proposed method models the latent context among selected items in each session throughout users’ historical interactions and combines users’ short-term and long-term preferences to generate recommendations. Specifically, we suggest a DCARS based on latent representations of sessions derived from users’ activities. The experiment results on benchmark context-aware data sets show that the proposed DCARS model surpasses state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Javad Sohafi-Bonab and Mehdi Hosseinzadeh Aghdam and Kambiz Majidzadeh},
  doi          = {10.1016/j.asoc.2023.110416},
  journal      = {Applied Soft Computing},
  pages        = {110416},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DCARS: Deep context-aware recommendation system based on session latent context},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A broad review on class imbalance learning techniques.
<em>ASOC</em>, <em>143</em>, 110415. (<a
href="https://doi.org/10.1016/j.asoc.2023.110415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imbalanced learning issue is related to the performance of learning algorithms in the presence of asymmetrical class distribution. Due to the complex characteristics of imbalanced datasets, learning from such data need new algorithms and understandings to convert efficient large amounts of initial data into suitable datasets. Although several review papers can be found about imbalanced classification problems, none of them contributed an in-depth review of SVM for imbalanced classification problems. To fill this gap, we present an exhaustive review of existing methods to deal with issues linked with class imbalance learning. The majority of the existing survey addresses only classification tasks . We also describe methods to deal with similar problems in regression tasks . A new taxonomy for class imbalanced learning techniques is proposed and classified into three parts: (1) Data pre-processing, (2) Algorithmic structures, and (3) Hybrid techniques. The advantages and disadvantages of each type of imbalanced learning technique are discussed. Moreover, we explain the main difficulties in distributions of imbalanced datasets and discuss the main approaches that have been proposed to tackle these issues. Finally, to stimulate the next research in this area, we emphasize the main opportunities and challenges, which can be useful in research directions for learning algorithms from imbalanced data .},
  archive      = {J_ASOC},
  author       = {Salim Rezvani and Xizhao Wang},
  doi          = {10.1016/j.asoc.2023.110415},
  journal      = {Applied Soft Computing},
  pages        = {110415},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A broad review on class imbalance learning techniques},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An area optimization approach taking into account polarity
conversion sequence. <em>ASOC</em>, <em>143</em>, 110414. (<a
href="https://doi.org/10.1016/j.asoc.2023.110414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, area has become one of the main bottlenecks restricting the development of EDA. The area optimization for XNOR/OR-based fixed polarity Reed–Muller (FPRM) circuits aims to find an FPRM circuit with a minimum area. Because the area optimization is a combinatorial optimization problem , we first propose an adaptive bacterial foraging algorithm based on tabu search (ABFA-TS), which includes fuzzy control theory and tabu search strategy. Few studies have considered the problem of polarity conversion sequence. In order to solve the problem of conversion sequence, we propose a hybrid genetic algorithm (HGA) based on the nearest neighbor. Moreover, based on the proposed ABFA-TS and proposed HGA, we propose an area optimization approach for FPRM circuits, which searches for an FPRM circuit with a minimum area. The experimental results confirmed that the maximum time saving rate of HGA reached 78\%, and confirmed the superiority of the FPRM area optimization approach in optimizing the FPRM circuits area.},
  archive      = {J_ASOC},
  author       = {Yuhao Zhou and Zhenxue He and Chen Chen and Limin Xiao and Xiang Wang},
  doi          = {10.1016/j.asoc.2023.110414},
  journal      = {Applied Soft Computing},
  pages        = {110414},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An area optimization approach taking into account polarity conversion sequence},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fault-tolerant adaptive genetic algorithm for service
scheduling in internet of vehicles. <em>ASOC</em>, <em>143</em>, 110413.
(<a href="https://doi.org/10.1016/j.asoc.2023.110413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, a range of Internet of Vehicles services has emerged, along with improved quality parameters. However, the field still faces several limitations, including resource constraints and the time response requirement. This paper extracts cost, energy, processing power, service management, and resource allocation parameters. Mathematical equations are then defined based on these parameters. To simplify the process complexity and ensure scalability, we propose an algorithm that uses the genetic algorithm for fault and cost management during resource allocation to services. The main concept is to pick resources for services using a genetic algorithm . We discuss the processing and energy costs associated with this function, which is the algorithm’s objective function and is created to optimize cost. Our approach goes beyond the conventional genetic algorithm in two stages. In the first step, services are prioritized, and resources are allocated in accordance with those priorities; in the second step, load balancing in message transmission paths is ensured, and message failures are avoided. The algorithm’s performance is evaluated using various parameters, and it was shown to outperform other metaheuristic algorithms like the classic genetic algorithm, particle swarm , and mathematical models. Different scenarios with various nodes and service variables are defined in various system states, including fault occurrences to various percentages of 10, 20, and 30. To compare methods, we consider different parameters, the most significant being performance success rate. Moreover, the cost optimization has a good convergence after iterations, and the rate of improvement in the big scenario has slowed down after 150 iterations. Besides, it provides acceptable performance in response time for services.},
  archive      = {J_ASOC},
  author       = {Shirin Abbasi and Amir Masoud Rahmani and Ali Balador and Amir Sahafi},
  doi          = {10.1016/j.asoc.2023.110413},
  journal      = {Applied Soft Computing},
  pages        = {110413},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fault-tolerant adaptive genetic algorithm for service scheduling in internet of vehicles},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic design of machine learning via evolutionary
computation: A survey. <em>ASOC</em>, <em>143</em>, 110412. (<a
href="https://doi.org/10.1016/j.asoc.2023.110412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML), as the most promising paradigm to discover deep knowledge from data, has been widely applied to practical applications, such as recommender systems , virtual reality, and semantic segmentation . However, building a high-quality ML system for given tasks requires expert knowledge and high computation cost. This poses a significant challenge to the further development of ML in large-scale practical applications. The automatic design of ML has become an increasingly popular research trend. At the same time, evolutionary computation (EC), as an excellent heuristic search technique, has been widely employed in ML optimization, so-called evolutionary machine learning (EML). In this paper, we offer a comprehensive review of the literature (more than 500 references) for EML methods. We first introduce the concepts related to ML and EC. After that, we propose a taxonomy criterion based on the ML and EC perspectives. The important research problems of EML, e.g., ML algorithms , solution representations, search paradigms, acceleration strategies and applications, are reviewed systematically. Lastly, we analyze EML limitations and discuss potential trends that are promising to address in the future.},
  archive      = {J_ASOC},
  author       = {Nan Li and Lianbo Ma and Tiejun Xing and Guo Yu and Chen Wang and Yingyou Wen and Shi Cheng and Shangce Gao},
  doi          = {10.1016/j.asoc.2023.110412},
  journal      = {Applied Soft Computing},
  pages        = {110412},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic design of machine learning via evolutionary computation: A survey},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian incremental learning paradigm for online monitoring
of dam behavior considering global uncertainty. <em>ASOC</em>,
<em>143</em>, 110411. (<a
href="https://doi.org/10.1016/j.asoc.2023.110411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced data-driven methods for real-time monitoring of dam structural behavior , especially displacement, are obtaining increasing popularity. Most existing data-driven models are offline and static, however, which cannot be dynamically updated with the increase of monitoring data. Additionally, the capability of monitoring models to identify anomalies, which is the ultimate goal of dam behavior modeling, has received little attention. This study proposes a novel Extreme Learning Machine (ELM)-based Bayesian incremental learning methodology to address these challenges. The proposed model is termed Incremental Global Bayesian ELM (I-GBELM), which couples ELM with global Bayesian and incremental learning for online monitoring of dam displacements. Firstly, a global Bayesian approach to the ELM is presented to account for both input and target uncertainties and to infer the network parameters analytically. Secondly, an incremental learning paradigm is theoretically derived, which aims to directly learn new knowledge from additional data to update model parameters without retraining from scratch. The I-GBELM model integrates the strengths of both methods so that it allows accurately predicting displacements, identifying anomalies, and efficiently updating parameters. For online deployment purposes, finally, a non-iterative pruning algorithm is developed to adaptively discard the redundant hidden nodes, yielding a more compact model. The performance of the proposed model is fully validated through comprehensive verification cases using benchmark and real-world monitoring datasets. The empirical results indicate that the proposed I-GBELM outperforms all other models in most cases, and hence, is a competitive modeling tool for online structural health monitoring .},
  archive      = {J_ASOC},
  author       = {Qiubing Ren and Heng Li and Mingchao Li and Ting Kong and Runhao Guo},
  doi          = {10.1016/j.asoc.2023.110411},
  journal      = {Applied Soft Computing},
  pages        = {110411},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian incremental learning paradigm for online monitoring of dam behavior considering global uncertainty},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep incremental random vector functional-link network: A
non-iterative constructive sketch via greedy feature learning.
<em>ASOC</em>, <em>143</em>, 110410. (<a
href="https://doi.org/10.1016/j.asoc.2023.110410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incremental version of randomized neural networks provides a greedy constructive algorithm for the shallow network, which adds new nodes through different stochastic methods rather than gradient optimization. However, the potential of the random incremental mechanism is still underutilized in deep structures. To address this research gap, we propose an unsupervised algorithm termed the incremental randomization-based autoencoder (IR-AE) for greedy feature learning , which applies an integrated optimized constructive algorithm to train the feature extractor. Using IR-AE as a hierarchical stacked block, we synthesize the deep incremental random vector functional-link (DI-RVFL) network that builds a deep structure with overall feature-output links through a feedforward approach. Furthermore, it is a novel data-driven initialization to implement the feedforward constructive sketch (CoSketch) as a pre-trained model for multi-layer perceptron . The simulation results empirically demonstrate that the proposed IR-AE can realize a higher reconstruction efficiency than AE and randomization-based AE. Moreover, DI-RVFL shows the advantages of deep structures in higher-level feature extraction compared to other stacked random structures. The overall performance of deep RVFLs outperforms those of multi-layer extreme learning machines . As data-driven initialization, CoSketch significantly improves the convergence performance of gradient descent .},
  archive      = {J_ASOC},
  author       = {Siyuan Zhang and Linbo Xie},
  doi          = {10.1016/j.asoc.2023.110410},
  journal      = {Applied Soft Computing},
  pages        = {110410},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep incremental random vector functional-link network: A non-iterative constructive sketch via greedy feature learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combined machine-learning and optimization models for
predicting carbon dioxide trapping indexes in deep geological
formations. <em>ASOC</em>, <em>143</em>, 110408. (<a
href="https://doi.org/10.1016/j.asoc.2023.110408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emissions of carbon dioxide (CO 2 ) are a major source of atmospheric pollution contributing to global warming. Carbon geological sequestration (CGS) in saline aquifers offers a feasible solution to reduce the atmospheric buildup of CO 2 . The direct determination of the trapping efficiency of CO 2 in potential storage formations requires extensive, time-consuming simulations. Machine-learning (ML) models offer a complementary means of determining trapping indexes, thereby reducing the number of simulations required. However, ML models have to date found it difficult to accurately predict two specific reservoir CO2 indexes: residual-trapping index (RTI) and solubility-trapping index (STI). Hybridizing ML models with optimizers (HML) demonstrate better RTI and STI prediction performance by selecting the ML model’s hyperparameters more precisely. This study develops and evaluates six HML models, combining a least-squares-support-vector machine (LSSVM) and a radial-basis-function neural network (RBFNN) with three effective optimizer algorithms: genetic (GA), cuckoo optimization (COA), and particle-swarm optimization (PSO). 6810 geological-formation simulation records for RTI and STI were compiled from published studies and evaluated with the six HML models. Error and score analysis reveal that the HML models outperform standalone ML models in predicting RTI and STI for this dataset, with the LSSVM-COA model achieving the lowest root mean squared errors of 0.00421 and 0.00067 for RTI and STI, respectively. Sensitivity analysis identifies residual gas saturation and permeability as the most influential input variables on STI and RTI predictions. The high RTI and STI prediction accuracy achieved by the HML models offers to reduce the uncertainties associated with CGS projects substantially.},
  archive      = {J_ASOC},
  author       = {Shadfar Davoodi and Hung Vo Thanh and David A. Wood and Mohammad Mehrad and Valeriy S. Rukavishnikov},
  doi          = {10.1016/j.asoc.2023.110408},
  journal      = {Applied Soft Computing},
  pages        = {110408},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combined machine-learning and optimization models for predicting carbon dioxide trapping indexes in deep geological formations},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Natural image matting based on surrogate model.
<em>ASOC</em>, <em>143</em>, 110407. (<a
href="https://doi.org/10.1016/j.asoc.2023.110407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matting is an important process in digital image processing , with pixel pair optimization-based methods having distinct advantages in parallelization and handling mislabeled trimaps or spatially disconnected foregrounds. Nevertheless, such methods cannot provide high-quality alpha mattes under a limited computing time, limiting their computing time-sensitive applications. Thus, this paper presents a natural image matting method based on surrogate models to address this problem. Specifically, the surrogate models for pixel pair optimization is established to approximate a pixel pair evaluation function, and its optimal solution obtained efficiently is used as the approximate optimal solution of the pixel pair optimization problem , saving much computing time. Experimental results demonstrate that the image matting based on surrogate models provides high-quality matting mattes with a little computing time and a competitive image matting performance compared to state-of-the-art pixel pair optimization-based methods that impose an excessive computing time.},
  archive      = {J_ASOC},
  author       = {Yihui Liang and Hongshan Gou and Fujian Feng and Guisong Liu and Han Huang},
  doi          = {10.1016/j.asoc.2023.110407},
  journal      = {Applied Soft Computing},
  pages        = {110407},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Natural image matting based on surrogate model},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mega trend diffusion-siamese network oversampling for
imbalanced datasets’ SVM classification. <em>ASOC</em>, <em>143</em>,
110406. (<a href="https://doi.org/10.1016/j.asoc.2023.110406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced class distribution is a frequent and problematic issue in the domains of data engineering and machine learning . Traditional classification algorithms or machine learning models frequently fail, in this difficult situation, to provide accurate classification results for the minority class. To improve classification performance for the minority class, some research has advised creating artificial instances in the minority class using the oversampling technique. However, on the support vectors machines (SVM) model, as new minority class instances were created in an original data space, classification improvements made using these popular oversampling techniques may not hold true. In this paper, we develop a novel oversampling method termed DB-MTD-SN (for distance-based mega-trend-diffusion Siamese Network) for generating artificial minority class instances to increase the SVM model’s classification accuracy for imbalanced datasets. In the proposed method, we utilize distance-based mega-trend-diffusion (DB-MTD) technique to estimate data domain for tiny support vectors to generate synthetic examples with multi-model distributions of minority class. Further, we construct a novel Siamese network model based on membership function (MF-based SN) to find the most representative synthetic minority class examples. The proposed SN model is used to map the original data onto a high-dimensional space to easily learn complicated patterns between majority and minority classes. In our proposed SN model, the MF-based contrastive loss function is used to measure the similarity between a new synthetic example and original examples to avoid generating noise. To demonstrate efficacy of the proposed DB-MTD-SN approach, ten benchmark datasets are used in this study. On two types of SVM models, we compare the proposed method with three state-of-the-art oversampling methods. Further, three evaluation metrics : G-mean, F1, and index of balanced accuracy (IBA) are utilized to measure SVM classification performance on imbalanced datasets. The experimental datasets are set at different imbalanced ratios (IRs) as 15, 20, and 30 to test classification performance using five methods. For a high IR value of 30, on the two SVM models, the proposed method achieved the best average in terms of G-mean (0.865 and 0.865), F1 (0.785 and 0.772), and IBA (0.681 and 0.689) metrics, respectively. The paired Wilcoxon test is used to evaluate whether the suggested approach has statistically significant differences from the four other methods on the three evaluation metrics . The test results demonstrate that classification results using the proposed DB-MTD-SN method enjoys significant improvements (i.e. p p -value &lt;0.05) in terms of G-mean, F1, and IBA indicators when compared to the other four methods. Our experimental results indicate the suggested DB-MTD-SN method outperforms other oversampling methods for imbalanced datasets.},
  archive      = {J_ASOC},
  author       = {Liang-Sian Lin and Yao-San Lin and Der-Chiang Li and Yi-Ting Chen},
  doi          = {10.1016/j.asoc.2023.110406},
  journal      = {Applied Soft Computing},
  pages        = {110406},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mega trend diffusion-siamese network oversampling for imbalanced datasets’ SVM classification},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEvoU-net: An ensemble of evolutionary deep fully
convolutional neural networks for medical image segmentation.
<em>ASOC</em>, <em>143</em>, 110405. (<a
href="https://doi.org/10.1016/j.asoc.2023.110405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a Deep Convolutional Neural Network (DCNN) is a challenging research topic which needs extensive efforts and computation to find a proper network structure and a precise set of hyper-parameters. The problem is that in most of the cases the achieved network works well on the specific application or dataset, and a small to significant changes are required to adapt it for a new one. Besides, the limited number of available labelled images and the required computational infrastructure, make this task even more challenging. Therefore, developing an automatic method that is able to find a network structure and its parameters, while using minimum computation, seems necessary. Evolutionary computation is an optimisation method that can be used to address the mentioned difficulties. This paper proposes an evolutionary-based framework to find a set of precise and small networks for medical image segmentation , and also, an ensemble model to improve the quality of segmentation. To the best of our knowledge, EEvoU-Net is the first ensemble method that utilises a set of evolutionary U–Net–based deep networks for medical image segmentation . In the proposed model, a Genetic Algorithm (GA) is applied to design a set of optimal network structures , along with their parameters, using a new fixed-length encoding strategy to create variable length networks, for medical image segmentation. The proposed model is evaluated using five different, publicly available medical image segmentation datasets. The best found evolutionary networks, outperformed U-Net, ResU-Net, DenseU-Net, NAS U-Net, AdaResU-Net, EvoU-Net, DenseRes, 2D to 3D EvoU-Net, and Attention EvoU-Net in the most of the cases using considerable less trainable parameters. Furthermore, EEvoU-Net as an ensemble of evolutionary networks, has also substantially improved over those previous results.},
  archive      = {J_ASOC},
  author       = {Tahereh Hassanzadeh and Daryl Essam and Ruhul Sarker},
  doi          = {10.1016/j.asoc.2023.110405},
  journal      = {Applied Soft Computing},
  pages        = {110405},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEvoU-net: An ensemble of evolutionary deep fully convolutional neural networks for medical image segmentation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning-based sentiment analysis approach
(MF-CNN-BILSTM) and topic modeling of tweets related to the
ukraine–russia conflict. <em>ASOC</em>, <em>143</em>, 110404. (<a
href="https://doi.org/10.1016/j.asoc.2023.110404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter, one of the most significant social media platforms , can be used as data sources to research public opinion on various topics, including political conflicts. People worldwide have expressed their opinions about the war between Russia and Ukraine since it began. The motivation of this study is to use deep learning techniques to reveal qualitative and quantitatively narrow-scoped situational awareness and emotional tendencies during crisis periods. In order to achieve this, a sizable dataset of geotagged tweets was gathered with specific terms associated with the conflict between Ukraine and Russia. Then, deep learning-based text mining techniques like topic modeling and sentiment analysis were applied to investigate people’s perspectives on conflict and emotional tendencies. The study will establish an impartial data source for the reports and articles of unbiased press members. This study used Valence Aware Dictionary and sEntiment Reasoner (VADER) to categorize the emotions related to the conflict between Russia and Ukraine in tweets. In addition, Latent Dirichlet Allocation (LDA) was used to extract various discussion topics. These techniques reveal the role of Twitter and compare and analyze the emotions and attitudes expressed on Twitter by different countries during the Ukraine–Russia conflict. In addition, a new deep learning-based sentiment classification “MF-CNN-BiLSTM” model that predicts and analyzes sentiments was proposed in this study. The proposed model stands for Multistage Feature Extraction using Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM). By combining various qualities and benefits of CNNs and BiLSTM , the suggested model makes it possible to identify both short- and long-term dependencies in ordinal data . The proposed model includes MF steps that strengthen feature extraction by identifying local dependencies. Experimental results demonstrate that it is possible to obtain more accurate sentiment classification results by the proposed method.},
  archive      = {J_ASOC},
  author       = {Serpil Aslan},
  doi          = {10.1016/j.asoc.2023.110404},
  journal      = {Applied Soft Computing},
  pages        = {110404},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learning-based sentiment analysis approach (MF-CNN-BILSTM) and topic modeling of tweets related to the Ukraine–Russia conflict},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-criteria group decision-making for optimal management
of water supply with fuzzy ELECTRE-based outranking method.
<em>ASOC</em>, <em>143</em>, 110403. (<a
href="https://doi.org/10.1016/j.asoc.2023.110403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study elevates the potential of ELECTRE IV method using the logical backgrounds of fuzzy sets in the proposed fuzzy ELECTRE IV method to capture the water supply problem of Iran. ELECTRE IV method, being an advanced variant of ELECTRE family, operates on the outranking principle to achieve the results using three types of preferences and five dominance relations that exhibit different levels of superiorities among alternatives. The proposed procedure is designed to exhibit the linguistic preferences via triangular fuzzy numbers which are compared on the basis of their corresponding magnitudes. The proposed approach successfully captures the pseudo criteria using three threshold values to filter out the preferences and then classifying the level of dominance. Moreover, ranking is derived by combining two pre-orders, i.e., ascending and descending pre-orders. The main contribution of this study is the implementation of outranking caliber of proposed fuzzy ELECTRE IV method to address the water supply problem in the Kermanshah province of Iran, concerning the selection of optimal site for Kandoleh dam. The comparisons with two existing techniques, fuzzy TOPSIS method and integrated fuzzy delphi and fuzzy ELECTRE method, are conducted to verify the authenticity and competency of the proposed technique.},
  archive      = {J_ASOC},
  author       = {Muhammad Akram and Kiran Zahid and Muhammet Deveci},
  doi          = {10.1016/j.asoc.2023.110403},
  journal      = {Applied Soft Computing},
  pages        = {110403},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria group decision-making for optimal management of water supply with fuzzy ELECTRE-based outranking method},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density-based approach for fuzzy rule interpolation.
<em>ASOC</em>, <em>143</em>, 110402. (<a
href="https://doi.org/10.1016/j.asoc.2023.110402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse rule base is one of the common problems in fuzzy rule-based systems, and fuzzy rule interpolation (FRI) could derive interpolated results for the input based on the neighbor fuzzy rules when the input is not matched by any of the fuzzy rules. The core idea of FRI is that similar inputs would lead to similar results, and several FRI methods that use a pre-defined number of closest rules to obtain the interpolated results have been presented. However, this could lead to the loss of some information as selecting a given number of rules without considering the exact distance between them and the input could lead to the selection of unwanted rules or the ignoring of similar rules. This paper presents a density-based fuzzy rule interpolation method that uses a density-based approach to search and select the closest rules for unmatched inputs. Instead of selecting a given number of rules, the proposed method adaptively selects the closest rules that are within a certain range of the unmatched inputs, thus assuring the selected rules are with high similarity to the inputs. The performance of the proposed method is verified through fifteen classification benchmarks, showing the effectiveness and efficiency of the proposed method.},
  archive      = {J_ASOC},
  author       = {Fei Gao},
  doi          = {10.1016/j.asoc.2023.110402},
  journal      = {Applied Soft Computing},
  pages        = {110402},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Density-based approach for fuzzy rule interpolation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A particle swarm optimization algorithm with novelty search
for combustion systems with ultra-low emissions and minimum fuel
consumption. <em>ASOC</em>, <em>143</em>, 110401. (<a
href="https://doi.org/10.1016/j.asoc.2023.110401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The particle swarm optimization algorithm is primarily inspired by the natural behaviour of swarms and achieves important results in different applications. However, it is not exempt from stagnation in local optima and has a tendency to prematurely converge to them. Novelty Search is a concept that appeared recently in different fields of computational intelligence. It aims at exploring non-visited areas of the search space through solutions that bring novelty to already discovered solutions. The novelty of this work can be divided into two steps: on one side, this article proposes a variant of the particle swarm optimization algorithm which uses Novelty Search concepts to improve the algorithm’s performance. Our proposal is first checked and compared using the CEC 2005 benchmark suite and then, we apply it to solve a real-world optimization problem : the design of a combustion system targeting the reduction of pollutant emissions and fuel consumption. The combustion chamber design phase usually is a complex and time-consuming process even with advanced supercomputers , since it depends on several input variables which are highly non-linear and with crossed interaction. Then, the second contribution of this work is to develop a methodology that couples a computational fluid dynamics (CFD) simulation tool with the new optimization algorithm for minimizing the specific fuel consumption of a compression-ignited engine, while constraining the NO x x and soot emissions. A 3D-CFD model of the combustion system was built to predict and analyse the performance of the combustion system and hence, select the parameters with a higher impact on the system. The method reduces the computational time and includes tools for the automatic preparation of the input parameters and geometry of the system. The input parameters correspond to geometrical variables that control the bowl shape, the number of holes in the injector , the injection pressure , the swirl number and the exhaust gas recirculation rate. Results show how the simulation tool and the new PSO with Novelty Search algorithm allow us to obtain a new combustion system that minimizes the fuel consumption by 3\%, simultaneously reducing NO x x and soot emissions.},
  archive      = {J_ASOC},
  author       = {David Martínez-Rodríguez and Ricardo Novella and Gabriela Bracho and Josep Gomez-Soriano and Cassio Fernandes and Tommaso Lucchini and Augusto Della Torre and Rafael-J. Villanueva and J. Ignacio Hidalgo},
  doi          = {10.1016/j.asoc.2023.110401},
  journal      = {Applied Soft Computing},
  pages        = {110401},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle swarm optimization algorithm with novelty search for combustion systems with ultra-low emissions and minimum fuel consumption},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neutrosophic LOPCOW-ARAS model for prioritizing industry
4.0-based material handling technologies in smart and sustainable
warehouse management systems. <em>ASOC</em>, <em>143</em>, 110400. (<a
href="https://doi.org/10.1016/j.asoc.2023.110400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry 4.0 technologies embedded in the warehouse management system (WMS) are needed to improve the automation of material handling activities such as receiving, storing, picking, sorting, packaging, and delivering. This research aims to introduce a neutrosophic multi-criteria group decision-making tool that is intelligible in supporting the transition and upgrading of WMS with Industry 4.0-based solutions. This advanced two-stage model is based on the integration of the logarithmic percentage change-driven objective weighting (LOPCOW) method and the additive ratio assessment (ARAS) method under the type-2 neutrosophic number (T2NN) environment. In the first stage, T2NN-LOPCOW generates an objective importance vector of decision-making criteria. In the second stage, T2NN-ARAS based on the generalized weighted Heronian mean operator provides an advantageous order of Industry 4.0-based material handling technologies. T2NN-LOPCOW-ARAS brings the following novelties: (( i ) to straightforwardly represent and explore interconnection levels between weights of criteria, (( ii ) to provide wide-scoping insight into the stability of initial priority order, as well as a broad spectrum of flexible solutions, (( iii ) to control the normalization procedure and minimize distortions due to the double-normalization backbone. The real-life case study of a logistics company from the Serbian grocery retail sector illustrates the practical applicability of T2NN-LOPCOW-ARAS. A practical evaluation framework is defined to comprehensively assess automated guided vehicles (AGVs), collaborative robotics, and drones. The sensitivity analyses show the high robustness of the proposed framework. The comparative investigation shows that T2NN-LOPCOW-ARAS is superior to the extant methods. The research findings show that AGVs are the most favorable Industry 4.0-based material handling solution.},
  archive      = {J_ASOC},
  author       = {Vladimir Simic and Svetlana Dabic-Miletic and Erfan Babaee Tirkolaee and Željko Stević and Ali Ala and Arash Amirteimoori},
  doi          = {10.1016/j.asoc.2023.110400},
  journal      = {Applied Soft Computing},
  pages        = {110400},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neutrosophic LOPCOW-ARAS model for prioritizing industry 4.0-based material handling technologies in smart and sustainable warehouse management systems},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neutrosophic CEBOM-MACONT model for sustainable management
of end-of-life tires. <em>ASOC</em>, <em>143</em>, 110399. (<a
href="https://doi.org/10.1016/j.asoc.2023.110399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Management of end-of-life tires (ELTs) has evolved into an important sustainability requirement that should follow circular economy principles. It is increasingly important to find environmentally-friendly and cost-effective solutions for ELT management, particularly in the context of large freight transportation companies. Selecting the most sustainable solution from the set of available ELT management strategies, such as retreading, recycling, energy recovery, and landfilling, presents a decision-making challenge for authorities. This study aims to introduce a practical evaluation framework comprised of strategy alternatives and key decision-making criteria to support transportation companies in managing ELT flows. Also, the research introduces an advanced two-stage neutrosophic decision support model to solve the addressed problem and reveal the most sustainable strategy . The model is based on the integration of the cross-entropy-based optimization model (CEBOM) method and mixed aggregation by comprehensive normalization technique (MACONT) under the type-2 neutrosophic number (T2NN) environment. Prominent features of T2NN-CEBOM are hybrid weighting sub-framework and processing controllability. Distinguished characteristics of T2NN-MACONT are average referencing, triple-normalization support, modeling of risk attitude behavior, adjustable ordering schemes, and an advanced scoring system. The real-life study of one of the largest German freight transportation companies that operates along an important European transit route offers practical insights for decision-makers when evaluating ELT management strategies. The research findings show that retreading is the most sustainable solution. The eight sensitivity analyses confirm the high robustness of the introduced decision-support model. The comparative analysis reveals the superiority of T2NN-CEBOM-MACONT for employment in practical settings.},
  archive      = {J_ASOC},
  author       = {Vladimir Simic and Svetlana Dabic-Miletic and Erfan Babaee Tirkolaee and Željko Stević and Muhammet Deveci and Tapan Senapati},
  doi          = {10.1016/j.asoc.2023.110399},
  journal      = {Applied Soft Computing},
  pages        = {110399},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neutrosophic CEBOM-MACONT model for sustainable management of end-of-life tires},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization models and solving approaches in relief
distribution concerning victims’ satisfaction: A review. <em>ASOC</em>,
<em>143</em>, 110398. (<a
href="https://doi.org/10.1016/j.asoc.2023.110398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relief distribution is one of the most widely studied topics in the domain of emergency logistics. Optimization models and solving approaches have become one of the most powerful tools for tackling relief distribution problems. In this context, victims’ satisfaction should be considered as one significant indicator to evaluate relief distribution operations. Therefore, this survey addresses some of the most representative publications working with optimization models and solving approaches in relief distribution concerning victims’ satisfaction. Firstly, collected models are discussed from the commonly used objectives for describing victims’ satisfaction: the shortest distribution time, the lowest unmet demand, and the maximum fairness. Second, gathered solving approaches are analyzed from exact algorithms, heuristic algorithms , and machine learning algorithms respectively. Heuristic algorithms are further studied into four groups: genetic algorithm , ant colony optimization , particle swarm optimization , and others. Finally, development trends of models and approaches in relief distribution concerning victims’ satisfaction and potential interest in the cross-disciplinary are showcased.},
  archive      = {J_ASOC},
  author       = {Jia Luo and Lei Shi and Rui Xue and Didier El-baz},
  doi          = {10.1016/j.asoc.2023.110398},
  journal      = {Applied Soft Computing},
  pages        = {110398},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization models and solving approaches in relief distribution concerning victims’ satisfaction: A review},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One evolutionary algorithm deceives humans and ten
convolutional neural networks trained on ImageNet at image recognition.
<em>ASOC</em>, <em>143</em>, 110397. (<a
href="https://doi.org/10.1016/j.asoc.2023.110397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are widely used in computer vision , but can be deceived by carefully crafted adversarial images. In this paper, we propose an evolutionary algorithm (EA) based adversarial attack against CNNs trained on ImageNet. Our EA-based attack aims to generate adversarial images that not only achieve a high confidence probability of being classified into the target category (at least 75\%), but also appear indistinguishable to the human eye in a black-box setting. These constraints are implemented to simulate a realistic adversarial attack scenario. Our attack has been thoroughly evaluated on 10 CNNs in various attack scenarios, including high-confidence targeted, good-enough targeted, and untargeted. Furthermore, we have compared our attack favorably against other well-known white-box and black-box attacks. The experimental results revealed that the proposed EA-based attack is superior or on par with its competitors in terms of the success rate and the visual quality of the adversarial images produced.},
  archive      = {J_ASOC},
  author       = {Ali Osman Topal and Raluca Chitic and Franck Leprévost},
  doi          = {10.1016/j.asoc.2023.110397},
  journal      = {Applied Soft Computing},
  pages        = {110397},
  shortjournal = {Appl. Soft. Comput.},
  title        = {One evolutionary algorithm deceives humans and ten convolutional neural networks trained on ImageNet at image recognition},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic conditional score model-based weighted incremental
fuzzy clustering of consumer power load data. <em>ASOC</em>,
<em>143</em>, 110395. (<a
href="https://doi.org/10.1016/j.asoc.2023.110395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a weighted incremental fuzzy C-mean power load clustering algorithm based on the dynamic conditional score model to solve the problems that the predominant power load data mining method only captures the mean characteristics of time series and ignores the heteroscedasticity and sequence correlations. Consequently, this method has unsatisfactory time series clustering and low clustering accuracy. A dynamic conditional score model is constructed to analyze and extract statistical characteristic parameters of a time series to calculate the autocorrelation value of the parameter series. A weighted fuzzy C-mean clustering analysis is performed, and the obtained data weight information is used as input for incremental clustering to improve the clustering accuracy. The DCS model parameter dataset and data weight information are combined, and the clustering analysis of the consumer power load data stream is performed. The power load time series of different companies is given, and the clustering validity indices are defined for the performance analysis to verify the proposed clustering algorithm. The experimental results show that the proposed algorithm achieves satisfactory clustering and improves the performance.},
  archive      = {J_ASOC},
  author       = {Yong Zhang and Xinyue Li and Shuhao Jiang and Ming-Lang Tseng and Li Wang and Shurui Fan},
  doi          = {10.1016/j.asoc.2023.110395},
  journal      = {Applied Soft Computing},
  pages        = {110395},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic conditional score model-based weighted incremental fuzzy clustering of consumer power load data},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). MimicNet: Mimicking manual delineation of human expert for
brain tumor segmentation from multimodal MRIs. <em>ASOC</em>,
<em>143</em>, 110394. (<a
href="https://doi.org/10.1016/j.asoc.2023.110394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep neural networks for brain tumor segmentation from multimodal MRIs rely predominantly on standard segmentation architectures, overlooking the underlying rules in clinical scenarios. To address this gap, we propose a novel deep multimodal network that architecturally mimics these rules and fully replicates the process of manually annotating brain tumors by human experts in machines. Three significant manual annotation rules are mimicked. The three sub-tasks are introduced first to segment three regions sequentially. We incorporate a fine-grained modality attention module in each sub-task to mimic region-aware multimodal fusion. Finally, we propose the deep feature propagation module for multi-scale context exploitation and repurpose deep cascaded attention for location information exploitation. These exploits imitate earlier segmentation knowledge exploitation. We use a new curriculum loss to train the nested network and a stratified training approach to reduce the intramodality domain shift caused by the dataset’s distribution. The proposed approach is evaluated and compared to state-of-the-art methods on three public datasets: BraTS2018, BraTS2019 and BraTS2020. The Dice similarity coefficient on Brats2020 is 0.800, 0.930 and 0.857 for enhancing tumor, whole tumor, and tumor core, respectively, ranking sixth top among 285 participating methods. These findings suggest the potential applicability of our approach in brain tumor segmentation from multimodal MRIs.},
  archive      = {J_ASOC},
  author       = {Zean Liu and Yuanzhi Cheng and Tao Tan and Tamura Shinichi},
  doi          = {10.1016/j.asoc.2023.110394},
  journal      = {Applied Soft Computing},
  pages        = {110394},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MimicNet: Mimicking manual delineation of human expert for brain tumor segmentation from multimodal MRIs},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated fuzzy-BWM, fuzzy-LBWA and v-fuzzy-CoCoSo-LD
model for gateway selection in fog-bolstered internet of things.
<em>ASOC</em>, <em>143</em>, 110393. (<a
href="https://doi.org/10.1016/j.asoc.2023.110393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a fog gateway selection strategy based on a hybrid Multi-criteria Decision-Making (MCDM) optimization model to avoid overwhelming the fog–cloud network in fog computing architecture. The study suggests that only authorized fog gateways should communicate with the cloud to prevent prolonged response times and significant data loss. To prioritize the selection criteria, the proposed model integrates subjective weights obtained from Fuzzy Best–Worst Method (Fuzzy-BWM) and Fuzzy Level Based Weight Assessment (Fuzzy-LBWA) using an extended nonlinear weighted synthesis approach. A Vector-normalized Fuzzy Combined Compromise Solution with Later Defuzzification (V-Fuzzy-CoCoSo-LD) model is used to rank the alternative gateways. The study constructs a taxonomy of selection criteria and finds that Quality-of-service and Level-of-security are the most important criteria with weights of 0.3731 and 0.3452, respectively. The hybridized weighting method addresses the weaknesses of individual methods. The study also conducts sensitivity analysis of the impact of fuzzy weighting parameters α ̃ α̃ and β ̃ β̃ on the decision-making process, and the results show that the most viable alternative remains the same regardless of the changes. The sensitivity analysis of the most sensitive criterion as well as comparative and correlation analyses also validate the effectiveness and robustness of the proposed model. The study concludes that the vector normalization method is the best for Fuzzy-CoCoSo MCDM model, with Spearman’s and Pearson’s correlation coefficient mean values of 0.9404 and 0.9868, respectively, justifying the choice in this study. Finally, the study provides design implications for the practical application of the proposed model in fog computing services. The proposed model can assist fog service providers in making informed decisions, and this study presents a case study to demonstrate the effectiveness of the model.},
  archive      = {J_ASOC},
  author       = {Sunday Oyinlola Ogundoyin and Ismaila Adeniyi Kamil},
  doi          = {10.1016/j.asoc.2023.110393},
  journal      = {Applied Soft Computing},
  pages        = {110393},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated fuzzy-BWM, fuzzy-LBWA and V-fuzzy-CoCoSo-LD model for gateway selection in fog-bolstered internet of things},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Local reversible transformer for semantic segmentation of
grape leaf diseases. <em>ASOC</em>, <em>143</em>, 110392. (<a
href="https://doi.org/10.1016/j.asoc.2023.110392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grape leaf diseases segmentation is an essential basis for achieving precise diagnosis and identification of diseases. However, the complex background renders it difficult for small disease areas to be precisely segmented. The existing Transformer mainly focuses on utilizing key and value downsampling to improve model performance while neglecting that downsampling is irreversible with the loss of contextual information. To this end, this paper proposed a novel Locally Reversible Transformer (LRT) segmentation model for grape leaf diseases in natural scene images, whose representation is learned in a reversible downsampling manner. Specifically, a Local Learning Bottleneck (LLB) is developed to enhance local perception and extract richer semantic information of grape leaf diseases via inverted residual convolution. Furthermore, motivated by the wavelet theory , the Reversible Attention (RA) is designed to replace the original downsampling operation by introducing wavelet transform into the multi-headed attention and solving the problem of difficult detection and segmentation of small disease targets with complex backgrounds. Extensive experiments demonstrate that the segmentation performance of LRT outperforms state-of-the-art models with comparable GFLOPs and parameters. Moreover, LRT can retain more multi-grain information and can increase the receptive field to focus on small disease regions with complex backgrounds.},
  archive      = {J_ASOC},
  author       = {Xinxin Zhang and Fei Li and Haibin Jin and Weisong Mu},
  doi          = {10.1016/j.asoc.2023.110392},
  journal      = {Applied Soft Computing},
  pages        = {110392},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Local reversible transformer for semantic segmentation of grape leaf diseases},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Value aided optimal load shedding accounting voltage
stability consideration employing crow search algorithm with
modification based on lampinen’s criterion. <em>ASOC</em>, <em>143</em>,
110391. (<a href="https://doi.org/10.1016/j.asoc.2023.110391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a novel load-shedding strategy that assures adequate voltage stability margin in post-load-shedding conditions. Minimum numbers of buses are selected for load shedding based on the incremental voltage of the load buses. An objective function has been formed that is to be minimized based on inequality constraints on load bus voltage and line flow. This objective function is the weighted sum of the slope of the PV-curve of the weakest bus, transmission losses, and the total load shed amount, which has been normalized with respect to the pre-load shed condition. A modified Crow Search Algorithm (CSA) has been used to obtain optimal load shed. The developed methodology has been implemented on standard IEEE 14 and 25-bus test systems, and comparisons based on statistical inferences have been carried out with the Sine–Cosine, Jaya, and Self-adaptive differential evolution (SaDE) algorithms.},
  archive      = {J_ASOC},
  author       = {Pushpendra Singh and Rajesh Arya and L.S. Titare and Pradeep Purey and L.D. Arya},
  doi          = {10.1016/j.asoc.2023.110391},
  journal      = {Applied Soft Computing},
  pages        = {110391},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Value aided optimal load shedding accounting voltage stability consideration employing crow search algorithm with modification based on lampinen’s criterion},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A differential evolution algorithm based on accompanying
population and piecewise evolution strategy. <em>ASOC</em>,
<em>143</em>, 110390. (<a
href="https://doi.org/10.1016/j.asoc.2023.110390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) is a simple and effective stochastic search algorithm , but its convergence speed and population diversity often decline catastrophically with the evolution process. In this paper, a differential evolution algorithm based on accompanying population and piecewise evolution strategy (APPDE) is proposed. The accompanying population is used to store suboptimal solutions, and its initialization, reinitialization and renewal mechanisms are designed to maintain the characteristics of suboptimal solutions and enhance the population diversity. The mutation operators are improved based on the accompanying population to balance the exploration and exploitation ability. In view of the phenomenon that the evolution speed slows down or even stagnates, the mutation strategies and control parameters are optimized by combining with piecewise evolution. The performance of APPDE is evaluated on CEC2014, CEC2015 and CEC2017 benchmark problem suites, and compared with the state-of-the-art optimization algorithms . The results show that APPDE has better performance than the competitive algorithms.},
  archive      = {J_ASOC},
  author       = {Minghao Wang and Yongjie Ma},
  doi          = {10.1016/j.asoc.2023.110390},
  journal      = {Applied Soft Computing},
  pages        = {110390},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A differential evolution algorithm based on accompanying population and piecewise evolution strategy},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Universal backdoor attack on deep neural networks for
malware detection. <em>ASOC</em>, <em>143</em>, 110389. (<a
href="https://doi.org/10.1016/j.asoc.2023.110389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor attacks targeting the deep neural network are flourishing recently and are more stealthy than existing adversarial attacks . A deep understanding of the backdoor attacks targeting malware detection models is still missing. We design a highly transferable backdoor attack targeting three benchmark convolutional neural networks (CNNs) for malware detection. The designed backdoor attack involves two steps: trigger generation and trigger insertion. Firstly, based on the computation of the most significant byte sub-sequence from samples of a chosen target label, the trigger patterns are generated by training a class activation mapping-based deep neural network (CAM-DNN). Then, the byte sequence with the maximum class activation mapping score is chosen as the candidate trigger pattern. The computed trigger pattern is then inserted into an index-based place that satisfies the minimum distance between a predefined feature space to the target label. Through detailed experiments, the CAM-DNN-based backdoor considers many influential factors, including the number of backdoor triggers, the degree of perturbations applied on a single trigger pattern, the length of the inserted trigger, etc. The experiments demonstrate that the CAM-DNN-based backdoor attack achieves an 89.58\% success rate on average at the cost of a 2.25\% accuracy drop on clean inputs. More importantly, the poisoned malware ensures high integrity because the original malicious functions are preserved to a large extent.},
  archive      = {J_ASOC},
  author       = {Yunchun Zhang and Fan Feng and Zikun Liao and Zixuan Li and Shaowen Yao},
  doi          = {10.1016/j.asoc.2023.110389},
  journal      = {Applied Soft Computing},
  pages        = {110389},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Universal backdoor attack on deep neural networks for malware detection},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning for heterogeneous graph via
structure information based on metapath. <em>ASOC</em>, <em>143</em>,
110388. (<a href="https://doi.org/10.1016/j.asoc.2023.110388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are the dominant paradigm for modeling and handling graph structural data by learning universal node representation. The traditional way of training GNNs depends on a great many labeled data, which is time-consuming and money-consuming. In some special scenes, it is even unavailable and impracticable. Self-supervised representation learning , which can generate labels by graph structural data itself, is a potential approach to tackle this problem. And turning to research self-supervised learning problems for heterogeneous graphs is more challenging than dealing with homogeneous graphs, there are fewer studies about it as well. In this paper, we propose a SE lf-supervised learning method for heterogeneous graph via S tructure I nformation based on M etapath (SESIM). Firstly, the pseudo-labels are constructed to train pretext tasks, using data itself and avoiding time-consuming manual labeling. Afterward, we use traditional graph neural networks to aggregate node features, obtaining the node embeddings . And then, the primary task and pretext tasks are designed by these node embeddings . The pretext tasks, i.e., jump numbers prediction between nodes in each metapath, can improve the representation ability of the primary task. Moreover, predicting jump numbers in each metapath can effectively utilize graph structural information, which is the essential property of nodes. Therefore, SESIM deepens the understanding of models for graph structure. At last, we train the primary task and pretext tasks jointly and balance the contributions of pretext tasks for the primary task. The key advantage of our proposed model is that we research self-supervised learning for the heterogeneous graph to address the time-consuming and money-consuming problem of obtaining labels. And we design a novel pretext task, i.e., jump numbers prediction in each metapath, via graph structural information based on the metapath. Empirical results validate the performance of the SESIM method and demonstrate that this method can improve the representation ability of traditional neural networks on link prediction tasks and node classification tasks.},
  archive      = {J_ASOC},
  author       = {Shuai Ma and Jian-wei Liu and Xin Zuo},
  doi          = {10.1016/j.asoc.2023.110388},
  journal      = {Applied Soft Computing},
  pages        = {110388},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-supervised learning for heterogeneous graph via structure information based on metapath},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable neighborhood search for weighted total domination
problem and its application in social network information spreading.
<em>ASOC</em>, <em>143</em>, 110387. (<a
href="https://doi.org/10.1016/j.asoc.2023.110387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weighted total domination problem (WTDP) is a practical extension of the well-known total domination problem. The most efficient literature approaches to tackle this problem are based on branch and cut or genetic algorithm . In this work, we propose a different strategy to solve WTDP that relies on the popular variable neighborhood search ( VNS ) metaheuristic . VNS is equipped with a carefully designed fitness function that allows for evaluation of both feasible and infeasible solutions, which further allows for a thorough search of the promising regions of the solution space. The method also utilizes two effective first-improvement local search procedures. The effectiveness of VNS is demonstrated on a wide range of benchmark sets compared to all three competing methods from the literature. For small-to-middle-sized instances (up to 100 nodes), VNS can obtain solutions that match the quality of optimal solutions in almost all cases (134 out of 135). For middle-to-large-sized instances, VNS can outperform all comparison algorithms in terms of solution quality, which is verified by statistical hypothesis tests. Another key aspect of this paper is presenting a potential application of the WTDP for boosting information spreading across social networks. Experiments confirmed that information spreading is accelerated when informed nodes (spreaders) are set to be solutions of the WTDP obtained by VNS . Although our method has been successfully applied to samples of real social networks of up to approximately 81 thousand nodes and 1.34 million edges, further research might enhance the method to be used on various (unsampled) social network datasets.},
  archive      = {J_ASOC},
  author       = {Stefan Kapunac and Aleksandar Kartelj and Marko Djukanović},
  doi          = {10.1016/j.asoc.2023.110387},
  journal      = {Applied Soft Computing},
  pages        = {110387},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Variable neighborhood search for weighted total domination problem and its application in social network information spreading},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L-SHADE with parameter decomposition for photovoltaic
modules parameter identification under different temperature and
irradiance. <em>ASOC</em>, <em>143</em>, 110386. (<a
href="https://doi.org/10.1016/j.asoc.2023.110386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of photovoltaic (PV) system relies on the accurate determination of unknown parameters in PV models, such as photo-generated current, diode current, and resistance. These unknown parameters may vary with different temperature and irradiance conditions, which greatly increases the difficulty of identifying these unknown parameters. Although some parameter identification methods have been proposed to solve such a problem, the accuracy and reliability of solutions obtained by these methods suffers from great challenges when temperature and irradiance are changing. In this paper, a simple and effective approach called success-history adaptation differential evolution with linear population size reduction (L-SHADE) and decomposition referred as (L-SHADED), is presented to accurately and reliably identify the unknown parameters of PV models under different temperature and irradiance. In L-SHADED, firstly, an unknown parameters decomposition technique is used to reduce the complexity of the problem. Then an advanced evolutionary algorithm L-SHADE is employed to identify the optimal unknown parameter values. The performance of proposed L-SHADED has been investigated by testing two single-diode-based PV modules (multi-crystalline KC200GT and mono-crystalline SM55) under different temperature and irradiance. The experimental comparison results demonstrate that proposed L-SHADED is almost 10 times smaller in RMSE than other methods. Furthermore, the coefficient of determination of L-SHADED reaches 1.0 in almost all conditions, which indicates that the parameters identified by L-SHADED are quite accurate.},
  archive      = {J_ASOC},
  author       = {Qiong Gu and Shuijia Li and Wenyin Gong and Bin Ning and Chunyang Hu and Zuowen Liao},
  doi          = {10.1016/j.asoc.2023.110386},
  journal      = {Applied Soft Computing},
  pages        = {110386},
  shortjournal = {Appl. Soft. Comput.},
  title        = {L-SHADE with parameter decomposition for photovoltaic modules parameter identification under different temperature and irradiance},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive archive-based multifactorial evolutionary algorithm
for constrained multitasking optimization. <em>ASOC</em>, <em>143</em>,
110385. (<a href="https://doi.org/10.1016/j.asoc.2023.110385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multitasking has attracted much attention in the field of evolutionary computing. Most of the existing multitasking evolutionary algorithms aim at solving unconstrained multitasking optimization problems . The study on constrained multitasking optimization problems is scarce. However, in practical applications, lots of optimization problems contain constraints. In this paper, an adaptive archive-based multifactorial evolutionary algorithm is proposed to solve constrained multitasking optimization problems. First, an archiving strategy is proposed to store infeasible solutions with better objective function values. With this strategy, useful information on infeasible solutions can be exploited to accelerate the convergence rate. Second, the random mating probability is adjusted through an adaptive strategy to facilitate positive knowledge transfer. Finally, a new mutation strategy is proposed to promote convergence by mutating some random individuals and replacing the individuals with the largest constraint violation. By comparing existing constrained multitasking evolutionary algorithms and some constrained single-task evolutionary algorithms, the results reveal the effectiveness of the proposed algorithm in solving constrained multitasking optimization problems.},
  archive      = {J_ASOC},
  author       = {Caixiao Xing and Wenyin Gong and Shuijia Li},
  doi          = {10.1016/j.asoc.2023.110385},
  journal      = {Applied Soft Computing},
  pages        = {110385},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive archive-based multifactorial evolutionary algorithm for constrained multitasking optimization},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast electrical impedance tomography based on sparse
bayesian learning. <em>ASOC</em>, <em>143</em>, 110384. (<a
href="https://doi.org/10.1016/j.asoc.2023.110384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrical impedance tomography (EIT) is a severely under-determined and ill-posed inverse problem . Therefore, based on the compressed sensing theory and the block sparse Bayesian learning (BSBL) model, a novel accelerated EIT sparse imaging algorithm is presented. The key feature of the proposed algorithm is that the convex–concave procedure (CCP) method is used to optimize the non-convex cost function of the block sparse Bayesian learning model which improves the convergence speed and reduces the time complexity of the algorithm. The proposed method can adaptively explore and utilize the inter-block sparsity and intra-block structural correlation of non-sparse signals without any a priori information , thereby sparse imaging is performed on non-sparse physiological data effectively. The results comparing with other methods show that the proposed algorithm not only reconstruct inclusions with different shapes and conductivity applicably and effectively but also reduce the running time of the algorithm.},
  archive      = {J_ASOC},
  author       = {Nan Wang and Yang Li and Peng-Fei Zhao and Lan Huang and Zhong-Yi Wang},
  doi          = {10.1016/j.asoc.2023.110384},
  journal      = {Applied Soft Computing},
  pages        = {110384},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fast electrical impedance tomography based on sparse bayesian learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random vector functional link network: Recent developments,
applications, and future directions. <em>ASOC</em>, <em>143</em>,
110377. (<a href="https://doi.org/10.1016/j.asoc.2023.110377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have been successfully employed in various domains such as classification, regression and clustering, etc. Generally, the back propagation (BP) based iterative approaches are used to train the neural networks, however, it results in the issues of local minima, sensitivity to learning rate and slow convergence. To overcome these issues, randomization based neural networks such as random vector functional link (RVFL) network have been proposed. RVFL model has several characteristics such as fast training speed, direct links, simple architecture, and universal approximation capability, that make it a viable randomized neural network. This article presents the first comprehensive review of the evolution of RVFL model, which can serve as the extensive summary for the beginners as well as practitioners. We discuss the shallow RVFLs, ensemble RVFLs, deep RVFLs and ensemble deep RVFL models. The variations, improvements and applications of RVFL models are discussed in detail. Moreover, we discuss the different hyperparameter optimization techniques followed in the literature to improve the generalization performance of the RVFL model. Finally, we present potential future research directions/opportunities that can inspire the researchers to improve the RVFL’s architecture and learning algorithm further.},
  archive      = {J_ASOC},
  author       = {A.K. Malik and Ruobin Gao and M.A. Ganaie and M. Tanveer and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2023.110377},
  journal      = {Applied Soft Computing},
  pages        = {110377},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Random vector functional link network: Recent developments, applications, and future directions},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic safety risk assessment in large-diameter
tunnel construction using an interactive and explainable tree-based
pipeline optimization method. <em>ASOC</em>, <em>143</em>, 110376. (<a
href="https://doi.org/10.1016/j.asoc.2023.110376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to knowledge alienation, the application of artificial intelligence (AI) techniques in tunnel construction has been greatly stunted in recent years. In order to motivate the development of techniques in tunneling works, we propose an approach that combines automated machine learning , explainable AI , and building information modeling in this paper. Considering geotechnical uncertainties from soil properties and aleatoric uncertainties from the predicting model, this approach is used to evaluate the system risk during a large-diameter tunnel excavation taking into account the two sources of uncertainties simultaneously. The geotechnical uncertainties are evaluated by the confidence interval of the mean of samples in the Monte-Carlo simulation, and the aleatoric uncertainties are evaluated by computing the prediction interval of the machine learning models. It is found that the number of Monte-Carlo samples significantly affects the evaluation of the system’s reliability. Based on our results, a generalized setup with 1000 samples performs the best and is recommended considering the efficiency of computation, as well as the reliability and conservativeness of the result. This paper effectively assists with system reliability evaluation in large-diameter tunnel constructions, where the interactive and explainable AI approach largely motivates the application of AI techniques in the field of tunnel construction.},
  archive      = {J_ASOC},
  author       = {Penghui Lin and Maozhi Wu and Limao Zhang},
  doi          = {10.1016/j.asoc.2023.110376},
  journal      = {Applied Soft Computing},
  pages        = {110376},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probabilistic safety risk assessment in large-diameter tunnel construction using an interactive and explainable tree-based pipeline optimization method},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Digital technology implementation and impact of artificial
intelligence based on bipolar complex fuzzy schweizer–sklar power
aggregation operators. <em>ASOC</em>, <em>143</em>, 110375. (<a
href="https://doi.org/10.1016/j.asoc.2023.110375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital technology refers to any technology that uses digital signals or electronic data to process, store, and transmit information. Some examples of digital technologies include social media platforms , cloud computing , artificial intelligence , virtual and augmented reality , and blockchain technology . Digital technology has the potential to play a significant role in achieving sustainable development goals by providing solutions for a wide range of environmental, social, and economic challenges. In this manuscript, we investigate digital technology implementation under sustainable development and would find which area of sustainable development is most in need of digital technology. Further, we investigate the operational laws based on Schweizer–Sklar t-norm and t-conorm and originate aggregation operators based on these deduced operational laws under the environment of bipolar complex fuzzy set that is bipolar complex fuzzy Schweizer–Sklar power averaging, bipolar complex fuzzy Schweizer–Sklar power weighted averaging, bipolar complex fuzzy Schweizer–Sklar power geometric and bipolar complex fuzzy Schweizer–Sklar power weighted geometric operators and then we deduce techniques of decision-making utilizing these originated operators. Afterward, we tackle a numerical example related to the digital technology implementation under sustainable development by considering artificial data and finding the area of sustainable development which is most in need of digital technology. Moreover, we reveal the impact of one of the digital technologies that are artificial intelligence in the field of healthcare and study a numerical example by considering hypothetical data by employing the originated technique of decision-making. At the last, we do a comparison of the deduced operators with numerous current operators to reveal the superiority and benefits of the deduced operators.},
  archive      = {J_ASOC},
  author       = {Tahir Mahmood and Ubaid ur Rehman},
  doi          = {10.1016/j.asoc.2023.110375},
  journal      = {Applied Soft Computing},
  pages        = {110375},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Digital technology implementation and impact of artificial intelligence based on bipolar complex fuzzy Schweizer–Sklar power aggregation operators},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A secondary decomposition–ensemble approach to interval
predicting china’s railway container volume. <em>ASOC</em>,
<em>143</em>, 110374. (<a
href="https://doi.org/10.1016/j.asoc.2023.110374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise forecasts of railway container demand is crucial for railway infrastructure construction and daily operations management. Nevertheless, temporal variations in railway container traffic are subject to a diversity of factors, and such dynamics are therefore extremely difficult to capture in forecasts. Decomposing the temporal data into a number of subseries with distinct qualities is a prevalent technique for reducing the complexity of data. Yet, a few of sub-series resulted from such decomposition tend to remain irregular and unstable. In this study, a novel secondary decomposition–ensemble method is proposed for railroad container volume forecasting. Specifically, CEEDMAN and VMD are employed successively to decompose the time series into various components. The sub-series are processed by different models, according to their distinct features, to yield their respective independent predictions, which are then ensembled to give the final railway container traffic forecast. Additionally, we incorporate the DEEPAR model for interval predictions, providing valuable uncertainty information that assists decision-makers in preparing for potential challenges. Our method demonstrates superior accuracy and stability compared to other benchmark models , even in the presence of disruptive events such as the big epidemic. We further validate the applicability and robustness of the proposed method by incorporating an additional dataset with a finer-time data. Our findings contribute to the field of railway container demand forecasting and offer practical insights for decision-makers in the railway industry .},
  archive      = {J_ASOC},
  author       = {Shuang Yuan and Peng Jia and Shouyang Wang},
  doi          = {10.1016/j.asoc.2023.110374},
  journal      = {Applied Soft Computing},
  pages        = {110374},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A secondary decomposition–ensemble approach to interval predicting china’s railway container volume},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-stage MCDM model for reverse logistics network design
of waste batteries in turkey. <em>ASOC</em>, <em>143</em>, 110373. (<a
href="https://doi.org/10.1016/j.asoc.2023.110373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inadequate environmental resources and overpopulation reveal the need to protect and recover natural resources attentively. In this sense, the reverse logistics concept emerged as a key solution since it deals with product flow from the final user to the origin. There are various items that need to be considered in well-planned reverse logistics network designs and one of these items is batteries which include hazardous and precious materials in it. Hence, waste management of batteries via recycling becomes a very significant issue from both economic and environmental benefits. Accordingly, depending on the importance of the topic, a two-stage methodology is proposed in this study for providing a network design under multiple objectives. Within the first stage, the importance weights of objectives are obtained via Spherical Fuzzy Analytical Hierarchy Process (SF-AHP) and they are found as 0.248 for cost minimization , 0.3 for carbon emission minimization, 0.256 for employment rate maximization and 0.196 for development rate maximization. Afterward, in the second stage, a Multi-Objective Mixed Integer Linear Programming Model (MO-MILP) is developed to design the reverse logistics network and an application is performed in Turkey for validation. The model is solved for various scenarios including different quantities to be collected. Hence, it is obtained that the satisfaction degrees for employment and development objectives are 100\% in all of the scenarios. However, the satisfaction degree of carbon emission minimization is around 96\% and the less satisfied objective is the cost minimization having a satisfaction degree of 75\% on average.},
  archive      = {J_ASOC},
  author       = {Huseyin Selcuk Kilic and Zeynep Tugce Kalender and Buse Solmaz and Demet Iseri},
  doi          = {10.1016/j.asoc.2023.110373},
  journal      = {Applied Soft Computing},
  pages        = {110373},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage MCDM model for reverse logistics network design of waste batteries in turkey},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RLTD: A reinforcement learning-based truth data discovery
scheme for decision support systems under sustainable environments.
<em>ASOC</em>, <em>143</em>, 110369. (<a
href="https://doi.org/10.1016/j.asoc.2023.110369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The online world and associated information and communication technologies have generated digital networks by processing massive volumes of data and have a significant impact on the environmental sustainability. Mobile Crowd Source (MCS) is one of the digital technologies that can help humanity to better sense, understand and protect the environment by using vast amounts of data obtained to construct intelligent decision support systems (DSS). However, as the false data submitted by dishonest and malicious workers will cause the data-based DSSs to make wrong decisions and thus cause great harm, it is an urgent issue to propose an effective Truth Data Discovery (TDD) scheme for MCS. To tackle this issue, a Reinforcement Learning-based Truth Data Discovery (RLTD) scheme is proposed to obtain truth data in MCS at low cost in this paper. The main innovations of the RLTD scheme are as follows: (1) A novel trustworthiness-based TDD scheme is proposed to obtain truth data accurately at low cost, which can facilitate data-based DSSs in MCS. (2) Combined with Matrix Factorization (MF), we propose a worker recruitment method that only needs to recruit ϑ n ϑn ( ϑ ≤ 1 ϑ≤1 ) workers for TDD in n n tasks, which reduces the data collection cost significantly than previous TDD schemes. (3) We propose a Reinforcement Learning-based Site Selection (RLSS) method that intelligently selects as few sites as possible for worker recruitment with guaranteed high data quality. Experimental results demonstrate that the RLTD scheme can improve the accuracy of data collection by 1.31\%–21.02\%, reduce the data collection cost by 81.39\%–85.50\% compared to the traditional TDD schemes, and identify workers with accuracy of 86.67\%–94.58\%.},
  archive      = {J_ASOC},
  author       = {Tingxuan Liang and Lingyi Chen and Mingfeng Huang and Xiaoheng Deng and Shaobo Zhang and Neal N. Xiong and Anfeng Liu},
  doi          = {10.1016/j.asoc.2023.110369},
  journal      = {Applied Soft Computing},
  pages        = {110369},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RLTD: A reinforcement learning-based truth data discovery scheme for decision support systems under sustainable environments},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A class-specific feature selection and classification
approach using neighborhood rough set and k-nearest neighbor theories.
<em>ASOC</em>, <em>143</em>, 110366. (<a
href="https://doi.org/10.1016/j.asoc.2023.110366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough set theories are utilized in class-specific feature selection to improve the classification performance of continuous data while handling data uncertainty. However, most of those approaches are converting continuous data into discrete or fuzzy data before applying rough set theories. These data conversions can reduce or change the meaning of data, as well as introduce unnecessary complexity to the feature domain. Therefore, in this study, we use neighborhood rough sets in class-specific feature selection to improve the classification performance without data conversions. As the standard classification algorithms are capable of handling a single feature set, we propose a novel classification algorithm based on the K-Nearest Neighbour algorithm to use class-specific feature subsets. Experimental evaluations prove that the proposed approach outperforms most of the baselines, and the selected feature sets are more effective than using the full feature sets in classification. The approach highly reduces the number of selected features and hence, can be used for effective data analysis of continuous data with high performance.},
  archive      = {J_ASOC},
  author       = {M.A.N.D. Sewwandi and Yuefeng Li and Jinglan Zhang},
  doi          = {10.1016/j.asoc.2023.110366},
  journal      = {Applied Soft Computing},
  pages        = {110366},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A class-specific feature selection and classification approach using neighborhood rough set and K-nearest neighbor theories},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of forest fire using deep convolutional neural
networks with transfer learning approach. <em>ASOC</em>, <em>143</em>,
110362. (<a href="https://doi.org/10.1016/j.asoc.2023.110362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forest fires caused by natural causes such as climate change, temperature increase, lightning strikes, volcanic activity or human effects are among the world’s most dangerous, deadly, and destructive disasters. Detection, prevention, and extinguishing forest fires is challenging. In addition, forest fires can cause habitat destruction that cannot be controlled in time and cause great material and moral losses. Therefore, fast and accurate Detection of forest fires is vital in emergency response. Here, in solving the problem, the transfer learning method from deep learning sub-topics can be used, which allows the application of pre-trained networks to a new problem. The Fire Luminosity Airborne-based Machine learning Evaluation dataset (consisting of forest fire images) obtained by Unmanned Aerial Vehicle was used in this study. In the Detection of forest fire images in the dataset, InceptionV3, DenseNet121, ResNet50V2, NASNetMobile, VGG-19 deep learning algorithms, transfer learning techniques that can produce more successful results than networks trained from scratch, and hybrid proposed with Support Vector Machine , Random Forest , Bidirectional Long Short-Term Memory, Gated Recurrent Unit algorithms methods have been applied. In the classification study with the Fire Luminosity Airborne-based Machine learning Evaluation dataset in performance measurement, 97.95\% accuracy was obtained from the DenseNet121 model, which was started with random weights. In the transfer learning study using ImageNet weights, satisfactory results were obtained with 99.32\% accuracy in the DenseNet121 model. We anticipate that working in forest fire detection and response can be entirely satisfactory.},
  archive      = {J_ASOC},
  author       = {Hatice Catal Reis and Veysel Turk},
  doi          = {10.1016/j.asoc.2023.110362},
  journal      = {Applied Soft Computing},
  pages        = {110362},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detection of forest fire using deep convolutional neural networks with transfer learning approach},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiobjective optimization algorithm with dynamic operator
selection for feature selection in high-dimensional classification.
<em>ASOC</em>, <em>143</em>, 110360. (<a
href="https://doi.org/10.1016/j.asoc.2023.110360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection (FS) is an important technique in data preprocessing that aims to reduce the number of features for training while maintaining a high accuracy for classification. In recent studies, FS has been extended to optimize multiple objectives simultaneously in classification. To better solve this problem, this paper proposes a new multiobjective optimization algorithm with dynamic operator selection for feature selection in high-dimensional classification, called FS-DOS. First, two complementary search operators with different characteristics are designed, where the first operator is a quick search (QS) operator aiming to accelerate the convergence speed, and the other operator is a modified binary differential evolution (BDE) operator that can prevent the algorithm from falling into a local optimum. In addition, a dynamic selection strategy based on the idea of resource allocation is also designed to dynamically select the most suitable operator for each solution according to its corresponding performance improvement on aggregated objective values. The simulation results on fifteen different real-world high-dimensional FS datasets show that FS-DOS can obtain a feature subset with higher quality than several state-of-the-art FS algorithms . Importantly, in terms of error rate, FS-DOS wins 55 out of 75 comparisons. In terms of dimensionality reduction, the number of features selected by FS-DOS is between one hundredth and one thousandth of the original dataset.},
  archive      = {J_ASOC},
  author       = {Wenhong Wei and Manlin Xuan and Lingjie Li and Qiuzhen Lin and Zhong Ming and Carlos A. Coello Coello},
  doi          = {10.1016/j.asoc.2023.110360},
  journal      = {Applied Soft Computing},
  pages        = {110360},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective optimization algorithm with dynamic operator selection for feature selection in high-dimensional classification},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fuzzy robust stochastic model for designing a
pharmaceutical supply chain with sustainability and resiliency
dimensions. <em>ASOC</em>, <em>143</em>, 110357. (<a
href="https://doi.org/10.1016/j.asoc.2023.110357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of pharmaceutical supply chains in ensuring the health of communities is obvious to everyone. In the event of a crisis, the importance of pharmaceutical supply chains becomes more visible, and paying attention to the dimensions of sustainability and resilience in supply chains is an active research issue. However, crisis logistics modeling with these factors for pharmaceutical products is rarely considered. In crises, perishable medicinal products such as drugs should be provided to hospitals at the lowest cost and expediently. This paper develops a new fuzzy robust stochastic model to deal with the crisis logistics of medicinal products by considering the dimensions of sustainability and resilience using multi-objective programming. Because this model contributes to the criteria of stability and flexibility. By defining three types of goals, this model seeks to find the optimal place to cover the demand when establishing field hospitals and the optimal route for transporting pharmaceutical products from the drug collection stands to the pharmacy, as well as the optimal route for transportation from the accident site to the field hospitals. Since the proposal is more complex than other pharmaceutical supply chain studies, to solve the model, a hybrid heuristic formulated by multi-choice objective programming is designed. The application of this model is indicated in a case study in Iran. The efficiency of the developed model is tested with sensitivities. The performance of the solution algorithm is analyzed with numerical results through quality and time measures. Finally, a discussion to evaluate the management insight of the solution of the proposed optimization model and the solution algorithm is concluded. The results indicate that the proposed algorithm reduces supply chain costs and dissatisfaction regarding the fairness of drug distribution and increases demand coverage.},
  archive      = {J_ASOC},
  author       = {Neda Daryanian and Abolfazl Kazemi and Mohammad Amin Adibi},
  doi          = {10.1016/j.asoc.2023.110357},
  journal      = {Applied Soft Computing},
  pages        = {110357},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy robust stochastic model for designing a pharmaceutical supply chain with sustainability and resiliency dimensions},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A bi-level nested heuristic algorithm for divisional seru
order acceptance and scheduling problems. <em>ASOC</em>, <em>143</em>,
110354. (<a href="https://doi.org/10.1016/j.asoc.2023.110354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the order acceptance and scheduling problem with worker–operation assignment considering the precedence constraint in the divisional seru production system, which is a new production type derived from Japan and can achieve responsiveness, flexibility, and efficiency simultaneously. With limited production capacity and workers with different skill sets and skill levels, the problem considered in this paper includes three sub-problems: (i) assigning workers to each seru , (ii) making the order acceptance and scheduling plan, and (iii) allocating workers to operations for each order. Subsequently, a nonlinear integer programming model is established for improving the total net revenue, and a bi-level nested heuristic algorithm is designed due to its intractable computation. Computational experiments are made finally, and results show that the objective value obtained by the bi-level nested heuristic algorithm is 3\% better than the bi-level genetic algorithm , and the running time is shortened by 99.23\%. That is, the proposed bi-level nested heuristic algorithm can achieve better results and higher efficiency for divisional seru order acceptance and scheduling problems.},
  archive      = {J_ASOC},
  author       = {Lili Wang and Zhe Zhang and Yong Yin},
  doi          = {10.1016/j.asoc.2023.110354},
  journal      = {Applied Soft Computing},
  pages        = {110354},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bi-level nested heuristic algorithm for divisional seru order acceptance and scheduling problems},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An attribute fuzzy concept-oriented three-way utility
decision model in multi-attribute environments. <em>ASOC</em>,
<em>143</em>, 110353. (<a
href="https://doi.org/10.1016/j.asoc.2023.110353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, an increasing number of researchers have focused on how to leverage limited information to make more reasonable decisions, thereby reducing the decision-making risk for decision-makers. Three-way decision, as a method for risky decision-making, offers a new approach to address this challenge and has gained significant attention. In view of this, based on the utility theory, this paper proposes a novel three-way utility decision model oriented to attribute fuzzy concept in multi-attribute environments. Firstly, an attribute fuzzy concept is constructed based on the decision-maker’s minimum requirements on each attribute. Subsequently, a new method for calculating the relative utility function is proposed, which is used to calculate the corresponding decision thresholds. Additionally, a generalized conditional probability calculation method is presented based on the attribute fuzzy concept and the basic semantics of conditional probability . Finally, a new three-way decision model is established to obtain the classification and ranking results of all objects. Through application analysis, comparative study and experimental analysis, the proposed method’s rationality and effectiveness are verified.},
  archive      = {J_ASOC},
  author       = {Zhaohui Qi and Hui Li and Kai Zhang and Jianhua Dai},
  doi          = {10.1016/j.asoc.2023.110353},
  journal      = {Applied Soft Computing},
  pages        = {110353},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An attribute fuzzy concept-oriented three-way utility decision model in multi-attribute environments},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A soft set theoretic approach to network complexity and a
case study for turkish twitter users. <em>ASOC</em>, <em>143</em>,
110344. (<a href="https://doi.org/10.1016/j.asoc.2023.110344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a few different angles from which to examine the question of how to determine whether social networks, which belong to the category of complex systems, are neither totally regular nor totally arbitrary. The theory of soft sets, which is a form of soft computing, is used in this study to analyze how the degree of complexity of a social network changes in response to an external stressor. By taking this strategy, conversation subjects within social networks are able to incorporate participation from more than just two individuals. This research also presents the imprecise technique using a Bayesian approach to soft modeling that is used to determine complexity. These soft sets enable complexity computations to be performed on directed networks established by Turkish Twitter users and acquired everyday during the Covid-19 stress period. This study additionally involves the forming of comparisons with the compositional and structural complexity observations of the underlying network. The findings demonstrate that the soft set techniques that were utilized in the complexity computation of the social network are both efficient and trustworthy.},
  archive      = {J_ASOC},
  author       = {Ömer Akgüller},
  doi          = {10.1016/j.asoc.2023.110344},
  journal      = {Applied Soft Computing},
  pages        = {110344},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A soft set theoretic approach to network complexity and a case study for turkish twitter users},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A decision modeling approach for smart e-tourism data
management applications based on spherical fuzzy rough environment.
<em>ASOC</em>, <em>143</em>, 110297. (<a
href="https://doi.org/10.1016/j.asoc.2023.110297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology deployment in smart e-tourism brings high potential in terms of customer data, events, reservations, and others. It acts as an effective and personalized guide to aid travelers. There is an increasing variety of smart e-tourism apps with multiple categories and criteria, but in terms of decision making, this presents a multicriteria complex problem to determine the best app from a group of available options with high criteria subjectivity. Literature reviews have evaluated and modeled the existing smart e-tourism apps alternatives, but informational uncertainty remains. The fuzzy sets and Multi-Attribute Decision Analysis (MADA) were used to handle the subjectivity issue. However, this process includes levels of uncertainty, which affects the decisions made and still an open issues. Spherical fuzzy rough sets (SFRSs) environment are useful in this situation for resolving fuzziness and ambiguity. This paper proposed a decision modeling approach for smart E-Tourism data management applications based on SFRSs environment. For methodology: firstly, a decision matrix is adopted for 5 different categories of Smart E-tourism’s system applications on the basis of the integrated 12 evaluation criteria. Secondly, a new formulation and development formulating a new extension of FWZIC, called a Spherical Fuzzy Rough-Weighted Zero-Inconsistency (SFR-WZIC), for weighting the smart key concept attributes involved in modeling smart e-tourism, whereas a new formulation and development for a new extension of FDOSM, called a Spherical Fuzzy Rough Decision by Opinion Score Method (SFR-DOSM), for modeling the applications of smart e-tourism per each e-tourism category; then, the new developments are integrated. The proposed methods were evaluated using systematic ranking and sensitivity analysis.},
  archive      = {J_ASOC},
  author       = {R.T. Mohammed and A.H. Alamoodi and O.S. Albahri and A.A. Zaidan and H.A. AlSattar and Uwe Aickelin and A.S. Albahri and B.B. Zaidan and Amelia Ritahani Ismail and R.Q. Malik},
  doi          = {10.1016/j.asoc.2023.110297},
  journal      = {Applied Soft Computing},
  pages        = {110297},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decision modeling approach for smart e-tourism data management applications based on spherical fuzzy rough environment},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A digital-twin-based adaptive multi-objective harris hawks
optimizer for dynamic hybrid flow green scheduling problem with dynamic
events. <em>ASOC</em>, <em>143</em>, 110274. (<a
href="https://doi.org/10.1016/j.asoc.2023.110274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many hybrid flow shop production environments, dynamic events (DEs) often occur and seriously impact production schedules. However, the traditional model rarely considers DEs, which makes the model seriously inconsistent with the actual situation. In addition, to reduce carbon emissions, there is an urgent need to carry out green optimization scheduling of production workshops, especially in emerging digital workshops. First, this paper proposes a dynamic digital-twin-based Multi-Objective Hybrid Flowshop Green Scheduling Model with DEs (MOHFGSM-DEs) to fill the above gaps. The MOHFGSM-DEs model’s bi-objectives are the makespan minimization and digital workshop total energy consumption minimization. In addition, the MOHFGSM-DEs model takes account of representative DEs in the actual dynamic production process, including controlled processing time, device dynamic reconfiguration events, and workpiece reworking events, which makes it more adaptive in practical production. Secondly, an Adaptive Multi-Objective Dynamic Harris Hawks Optimizer (AMODHHO) is presented with DEs parameters perceived by the digital twin of the workshop, adjusting the scheduling scheme adaptively in real time to address the MOHFGSM-DEs model efficiently. AMODHHO combines a nonlinear optimization strategy on balancing the exploration and exploitation of Harris Hawks Optimizer (HHO) and introduces the crossover operator of Genetic Algorithm (GA) to improve its optimal global ability. Moreover, a digital-twin-driven dynamic encoding methodology containing many optimization strategies is designed based on problem-specific characteristics. Finally, numerical experiments and application case comparisons are performed among AMODHHO, SPEA2, and NSGA-II. Results show that AMODHHO is superior to SPEA2 and NSGA-II regarding comprehensive performance. Therefore, the proposed model and AMODHHO are feasible to solve the real-world MOHFGSM-DEs adaptively. Furthermore, it performs dynamic adaptive adjustment for the actual scheduling schemes according to the real-time DEs the digital twin perceives.},
  archive      = {J_ASOC},
  author       = {Yankai Wang and Shilong Wang and Wenhan Yang and Chunfeng Shen and Junliang Li},
  doi          = {10.1016/j.asoc.2023.110274},
  journal      = {Applied Soft Computing},
  pages        = {110274},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A digital-twin-based adaptive multi-objective harris hawks optimizer for dynamic hybrid flow green scheduling problem with dynamic events},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolving quantum fuzzy neural network for online
state-of-health estimation of li-ion cell. <em>ASOC</em>, <em>143</em>,
110263. (<a href="https://doi.org/10.1016/j.asoc.2023.110263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement in the battery industry, more accurate and advanced state estimation methods are required to meet the performance requirements. The State of Health (SOH) estimation is performed in the battery management system (BMS), which provides the qualitative measure of the capability of a lithium-ion battery (LIB), in terms of capacity or internal resistance. Theoretically, the cell capacity is obtained by complete charge and discharge of the cell but in practical scenario, complete charge or discharge is never the case. To address this issue of dynamic discharge, this paper presents an evolving model-based SOH estimation, predicting the capacity fade of the cell extracted from the incomplete discharge conditions as in the case of dynamic driving scenarios. The evolving algorithm uses Neural Network , which features an interval fuzzy set, with conjectural jump positions. For better identification of overlaps between the classes, the quantum fuzzy set uses graded membership function. The number of rules are automatically adjusted and evolved, in the quantum fuzzy set using Decoupled Extended Kalman Filter (DEKF) for parameter estimation. The proposed method uses voltage, current and sampling time data to estimate the capacity, over a period of 600 charging-discharging cycles of Nickel Manganese Cobalt Oxide (NMC) chemistry batteries. The dynamic discharge voltage data is obtained from the periodic characterization tests and is used for predict the complete discharge voltage. The full voltage profile has been forecasted using Long-Short Term Memory (LSTM) network and the subsequent capacity has been estimated using evolving Quantum Fuzzy Neural Network (eQFNN) with an RMSE of less than 5\% making it suitable for on-board applications. The results are simulated in MATLAB 2020b and are validated using experimental verification in Battery Testing Lab (BTL), IIT Delhi.},
  archive      = {J_ASOC},
  author       = {Nitika Ghosh and Akhil Garg and B.K. Panigrahi and Jonghoon Kim},
  doi          = {10.1016/j.asoc.2023.110263},
  journal      = {Applied Soft Computing},
  pages        = {110263},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolving quantum fuzzy neural network for online state-of-health estimation of li-ion cell},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing reverse logistics network for healthcare waste
management considering epidemic disruptions under uncertainty.
<em>ASOC</em>, <em>142</em>, 110372. (<a
href="https://doi.org/10.1016/j.asoc.2023.110372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population growth and recent disruptions caused by COVID-19 and many other man-made or natural disasters all around the world have considerably increased the demand for medical services, which has led to a rise in medical waste generation. The improper management of these wastes can result in a serious threat to living organisms and the environment. Designing a reverse logistics network using mathematical programming tools is an efficient and effective way to manage healthcare waste. In this regard, this paper formulates a bi-objective mixed-integer linear programming model for designing a reverse logistics network to manage healthcare waste under uncertainty and epidemic disruptions. The concept of epidemic disruptions is employed to determine the amount of waste generated in network facilities; and a Monte Carlo-based simulation approach is used for this end. The proposed model minimizes total costs and population risk , simultaneously. A fuzzy goal programming method is developed to deal with the uncertainty of the model. A simulation algorithm is developed using probabilistic distribution functions for generating data with different sizes; and then used for the evaluation of the proposed model. Finally, the efficiency of the proposed model and solution approach is confirmed using the sensitivity analysis process on the objective functions’ coefficients.},
  archive      = {J_ASOC},
  author       = {Saeede Nosrati-Abarghooee and Mohammad Sheikhalishahi and Mohammad Mahdi Nasiri and Seyed Mohammad Gholami-Zanjani},
  doi          = {10.1016/j.asoc.2023.110372},
  journal      = {Applied Soft Computing},
  pages        = {110372},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing reverse logistics network for healthcare waste management considering epidemic disruptions under uncertainty},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noise-robust graph-based semi-supervised learning with
dynamic shaving label propagation. <em>ASOC</em>, <em>142</em>, 110371.
(<a href="https://doi.org/10.1016/j.asoc.2023.110371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semi-supervised classification is widely used because it effectively exploits the characteristics of unlabeled data . However, the existing methods have a drawback in that they do not account for the inherent noise of the data. Noise in graph data refers to nodes that are isolated from classes and overlapping nodes between different classes. Therefore, neglecting noise can distort the data manifold, leading to over-smoothing and overall performance degradation . In this paper, we propose a noise-robust model called the dynamic shaving label propagation algorithm . Our proposed method comprises three parts: graph construction , noise definition and cutting, and label propagation. First, in the graph construction process, k is determined at the point when reverse nearest neighbors are identified for the most isolated nodes. Second, the noise definition process identifies the nodes classified as noise based on the number and distance of their reverse nearest neighbors. Finally, label propagation is performed dynamically and iteratively by adjusting the noise removal intensity. Using various simulated and real-world datasets, we evaluate the accuracy and noise robustness of the proposed method with those of existing methods to evaluate its effectiveness and applicability. The comparison results demonstrate that the proposed method outperforms the existing alternatives.},
  archive      = {J_ASOC},
  author       = {Jiyoon Lee and Younghoon Kim and Seoung Bum Kim},
  doi          = {10.1016/j.asoc.2023.110371},
  journal      = {Applied Soft Computing},
  pages        = {110371},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Noise-robust graph-based semi-supervised learning with dynamic shaving label propagation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian evolutionary optimization for crafting high-quality
adversarial examples with limited query budget. <em>ASOC</em>,
<em>142</em>, 110370. (<a
href="https://doi.org/10.1016/j.asoc.2023.110370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the importance of security, the adversarial attack has become an increasingly popular area for deep learning , especially the black-box adversarial attack , which can only obtain the input and output of the target model, resulting in closer to the real world. Query-based methods are a common strategy for performing black-box attacks, and they can find a slight perturbation to make target model misclassify by continuously querying the target model to obtain the output of the target model. However, query-based methods usually suffer from a serious flaw that needs massive queries, which is unaccepted in the real world. Although some query-efficient methods have been proposed to alleviate the above problems, they greatly sacrifice the quality of adversarial examples due to their problem formulations. To generate high-quality adversarial examples with a limited query budget, we propose a Bayesian evolutionary optimization (BEO) based black-box attack method using differential evolution, where a Gaussian processes model is employed to approximate the real objective function. As a key component of the BEO, we use seven acquisition functions to sample the new solution to update the Gaussian processes model , and an information entropy based selection strategy is proposed to adaptively choose the acquisition function. Finally, an effectiveness validation study is carried out comparing the proposed method with five other black-box attack methods and one Bayesian optimization (BO) method using the CIFAR-10 and ImageNet datasets. Experimental results demonstrate that the proposed method can effectively generate adversarial examples using only 200 queries.},
  archive      = {J_ASOC},
  author       = {Chao Li and Wen Yao and Handing Wang and Tingsong Jiang and Xiaoya Zhang},
  doi          = {10.1016/j.asoc.2023.110370},
  journal      = {Applied Soft Computing},
  pages        = {110370},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian evolutionary optimization for crafting high-quality adversarial examples with limited query budget},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Limited budget-based consensus model for large group
decision making with hesitant fuzzy linguistic information.
<em>ASOC</em>, <em>142</em>, 110368. (<a
href="https://doi.org/10.1016/j.asoc.2023.110368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the uncertainty of large group decision-making and the limitation of resources, this paper constructs a large group decision-making method with hesitant fuzzy linguistic term set (HFLTS) based on limited budget. Firstly, by studying the basic theory of HFLTS, a weighted aggregation model of HFLTS is proposed, which can ensure the minimum weighted distance between aggregated HFLTS and HFLTSs before aggregation. Secondly, a decision makers’ weight determination algorithm is proposed by considering the quality of trust sources in the social trust network, which is calculated by the LeaderRank centrality measure . Furthermore, a maximum group consensus level model (MGCM) based on limited budget is constructed, in which the adjustment cost of decision makers is determined by the hesitancy degree in the evaluation and trust out-degree. For the situation of insufficient budget, the order correlation between the alternatives’ ranking before and after the budget increase is introduced to judge whether to increase the budget to achieve a higher group consensus level. Then, a limited budget-based large group decision-making method with hesitant fuzzy linguistic information is proposed. And this method is applied to the selection of equipment suppliers for hydrogen refueling station . Finally, the effectiveness of the proposed method is verified by comparison with existing research methods and sensitivity analysis.},
  archive      = {J_ASOC},
  author       = {Peide Liu and Xin Dong and Peng Wang},
  doi          = {10.1016/j.asoc.2023.110368},
  journal      = {Applied Soft Computing},
  pages        = {110368},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Limited budget-based consensus model for large group decision making with hesitant fuzzy linguistic information},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fuzzy decision-making approach for testing activity
prioritisation and its application in an engine company. <em>ASOC</em>,
<em>142</em>, 110367. (<a
href="https://doi.org/10.1016/j.asoc.2023.110367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prioritising testing activities is essential for new product development to identify the most valuable tests and distinguish the critical ones from the trivial ones. Most research solves this prioritisation problem based on the high risks identified in Failure Mode and Effect Analysis (FMEA). However, an empirical case study on a UK-based engine manufacturer highlights that the aim of testing is not only to detect technical failures but also to validate customer requirements. Based on the findings identified from the empirical study, this paper proposes a systematic approach for testing activity prioritisation, where fuzzy relational decision matrices are established to link customer requirements, technical objectives and testing activities. An XOR Best–Worst Method (BWM) and an extended simple additive weighting (SAW) method are developed to calculate the weights of customer requirements and score the testing activities. The approach also handles two uncertainty scenarios during the prioritisation process, i.e. the subjective judgements of choosing from options and the linguistic vagueness of the decisions. The application in a case from the engine manufacturing company illustrates the methodological improvement in the prioritisation process. The proposed method ranks the testing activities, which suggests an order of effort distribution for the test plan.},
  archive      = {J_ASOC},
  author       = {Yan Liu and Khadija Tahera},
  doi          = {10.1016/j.asoc.2023.110367},
  journal      = {Applied Soft Computing},
  pages        = {110367},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy decision-making approach for testing activity prioritisation and its application in an engine company},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From risks to rewards: A comprehensive guide to sustainable
investment decisions in renewable energy using a hybrid facial
expression-based fuzzy decision-making approach. <em>ASOC</em>,
<em>142</em>, 110365. (<a
href="https://doi.org/10.1016/j.asoc.2023.110365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are some risks in renewable energy investments, such as legal, technology and financial issues. Hence, appropriate actions should be taken to minimize these risks. However, each of the measures to be taken for the management of risks creates new costs for businesses. Therefore, to ensure the financial sustainability of the projects, measures for the most important risks should be taken at the first stage. Because of this situation, it is vital to make a priority analysis for the risks faced by renewable energy projects so that it can be possible to increase the effectiveness of risk management . Accordingly, in this study, it is aimed to examine the risks and rewards regarding sustainable investment decisions for renewable energy projects. In this scope, a novel model is generated by integrating different techniques. In this process, the evaluations of six different experts are taken into consideration. First, missing evaluations for the sustainable investment decisions in renewable energy are estimated with neuro decision-making and collaborative filtering. Secondly, the weights of the risk factors of sustainable energy investments are computed with neuro quantum spherical fuzzy (QNSLF) DEMATEL with golden cut. Thirdly, reward alternatives for sustainable decision making are analyzed with Neuro QNSLF TOPSIS with golden cut. This study contributes to literature by determining the most important risks for renewable energy projects by an original decision-making model. Another important novelty of this study is that a new technique called neuro decision-making has also been developed. In this technique, the facial expressions of the experts who evaluate are taken into consideration. It is defined that technological changes have the greatest significance. Additionally, governmental incentives are found as the most essential reward to improve sustainable investments in renewable energy. Thus, for the effective management of these risks, appropriate actions should be taken. It is also obvious that the competitiveness of companies that fall behind this new technology will decrease. In this context, these enterprises need to take the necessary measures to manage the technology risk to be sustainable in the long term.},
  archive      = {J_ASOC},
  author       = {Gang Kou and Dragan Pamucar and Hasan Dinçer and Serhat Yüksel},
  doi          = {10.1016/j.asoc.2023.110365},
  journal      = {Applied Soft Computing},
  pages        = {110365},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From risks to rewards: A comprehensive guide to sustainable investment decisions in renewable energy using a hybrid facial expression-based fuzzy decision-making approach},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The knowledge trajectory and thematic evolution of the rough
sets research: A main path and scientific mapping analysis.
<em>ASOC</em>, <em>142</em>, 110364. (<a
href="https://doi.org/10.1016/j.asoc.2023.110364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough sets (RS) are a set of theories that explore the expression, learning, and induction of incomplete data and inaccurate knowledge. Given the high profile and influence of RS research in the academic world, it is crucial to investigate and summarize the field’s development status and knowledge structure. This study employs the main path analysis (MPA) and scientific mapping methods to analyze 2803 retrieved papers and explore the knowledge diffusion and topic evolution in the RS field. The findings suggest that the RS field initially focused on practical applications before transitioning to extended research as the mainstream direction. Two main methods for RS extension research emerged: the constructional method and algebraic (axiomatic) method. Key research topics include neighborhood-based RS, decision-theoretic RS, and covering RS. Scientific mapping reveals six thematic areas across three stages, highlighting the theme distribution and evolution in the RS field. Overall, this paper provides insights into the knowledge structure and conceptual evolution of the RS field by tracing the trajectory of knowledge development and the thematic evolution.},
  archive      = {J_ASOC},
  author       = {Dejian Yu and Anran Fang and Zeshui Xu},
  doi          = {10.1016/j.asoc.2023.110364},
  journal      = {Applied Soft Computing},
  pages        = {110364},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The knowledge trajectory and thematic evolution of the rough sets research: A main path and scientific mapping analysis},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A residual graph convolutional network with spatio-temporal
features for autism classification from fMRI brain images.
<em>ASOC</em>, <em>142</em>, 110363. (<a
href="https://doi.org/10.1016/j.asoc.2023.110363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing autism spectrum disorder (ASD) remains a challenge due to its complexity and insufficient evidence available for its diagnosis. Recent research in the field of psychiatry suggests that there are no clear causes for ASD, but a hypothesis is raised that abnormalities in the superior temporal sulcus (STS), which is connected to visual cortex regions, can serve as a critical indicator of ASD. Inspired by this hypothesis, this paper proposes a deep learning model with two parts to diagnose ASD by utilizing functional brain connectivity between STS and visual cortex. The first part is a residual attention network that selectively extracts the structural and temporal features from 4D brain images while maintaining dynamic connectivity between the two regions. The second part is a graph convolutional network that determines ASD from the graph with 39 nodes constructed by the residual attention network . Experiments with the fMRI data from 800 patients known as ABIDE (Autism Brain Imaging Data Exchange) and 10-fold cross-validation show that the proposed model outperforms the state-of-the-art methods by achieving 11.37\%p improvement in the ASD classification. Additional analyses justify each part of the proposed model through an ablation study and various visualizations.},
  archive      = {J_ASOC},
  author       = {Kyoung-Won Park and Sung-Bae Cho},
  doi          = {10.1016/j.asoc.2023.110363},
  journal      = {Applied Soft Computing},
  pages        = {110363},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A residual graph convolutional network with spatio-temporal features for autism classification from fMRI brain images},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sampling technique for noisy and borderline examples problem
in imbalanced classification. <em>ASOC</em>, <em>142</em>, 110361. (<a
href="https://doi.org/10.1016/j.asoc.2023.110361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance Learning (CIL) is an important machine learning branch. Due to an imbalanced dataset, the efficiency of the classifiers is impacted. Various under/oversampling approaches are applied to the dataset to solve the class imbalance problem . The most successful among all the present solutions for data imbalance is the Synthetic Minority Oversampling Technique (SMOTE) which has a broad range of handling real-world applications. Noise and borderline examples are the two factors that degrade the performance of SMOTE and its variants. To address these issues, filtering-based methods have been developed. However, there are drawbacks associated with the filtering method. Firstly, error detection approaches in filtering methods are highly dependent on parameter settings. Secondly, samples identified during error detection are deleted or filtered from the sampling process which leads to abnormality of obtaining decision boundary and thus problem of again the class imbalance problem . To fix the problems associated with state-of-the-art filtering-based approaches, a novel oversampling filter-based method SMOTE-TLNN-DEPSO is proposed in this paper. In this hybrid variant SMOTE-TLNN-DEPSO, using SMOTE method the synthetic samples are generated to enhance the original class-imbalance data. Next, the two-layer natural neighbors’ technique is used for error detection which identifies the noisy and borderline examples. Lastly, instead of deleting the identified noisy and borderline examples, the hybrid variant of the differential evolution (DE) algorithm based on particle swarm optimization (PSO) called DEPSO is applied to optimize and modify iteratively the position (attributes). SMOTE-TLNN-DEPSO technique shows the advantage over other state of art SMOTE-based filtering-based approaches by solving the noise problem; the error detection technique using the nearest neighbor is parameter-free; Utilizing DEPSO approach the identified noisy samples by error detection technique are optimized instead of removing them. This helps in maintaining the imbalance ratio and improving the boundary; this approach is very appropriate for data sets having more noisy attributes especially class attributes the efficiency and usefulness of the proposed SMOTE-TLNN-DEPSO are demonstrated by exhaustive comparison experiments on artificial and real data sets .},
  archive      = {J_ASOC},
  author       = {Abhishek Dixit and Ashish Mani},
  doi          = {10.1016/j.asoc.2023.110361},
  journal      = {Applied Soft Computing},
  pages        = {110361},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sampling technique for noisy and borderline examples problem in imbalanced classification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic multi-objective evolutionary algorithm based on
niche prediction strategy. <em>ASOC</em>, <em>142</em>, 110359. (<a
href="https://doi.org/10.1016/j.asoc.2023.110359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reality, many multi-objective optimization problems are dynamic. The Pareto optimal front (PF) or Pareto optimal solution (PS) of these dynamic multi-objective problems (DMOPs) changes as the environment change. Therefore, solving such problems requires an optimization algorithm that can quickly track the PF or PS after an environment change. Prediction-based response mechanism is a common method used to deal with environmental changes, which is commonly known as center point-based prediction. However, if the predicted direction of the center point is inaccurate, the predicted population will be biased towards one side. In this paper, we propose a niche prediction strategy based on center and boundary points (PCPB) to solve the dynamic multi-objective optimization problems, which consists of three steps. After environmental changes are detected, the first step is to divide the niche, dividing different individuals in the PS into different niche populations. The second step is to independently predict different niches, and select individuals with good convergence and distribution in the niche to predict the individuals that will produce the next generation. Finally, some different individuals are randomly generated in the next possible PS area to ensure the diversity of the population. To verify whether our proposed strategy is effective and competitive, PCPB was compared with five state-of-the-art strategies. The experimental results show that PCPB performed competitively in solving dynamic multi-objective optimization problems, which proves that our algorithm has good competitiveness.},
  archive      = {J_ASOC},
  author       = {Jinhua Zheng and Bo Zhang and Juan Zou and Shengxiang Yang and Yaru Hu},
  doi          = {10.1016/j.asoc.2023.110359},
  journal      = {Applied Soft Computing},
  pages        = {110359},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic multi-objective evolutionary algorithm based on niche prediction strategy},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale reduction clustering of vibration signals for
unsupervised diagnosis of machine faults. <em>ASOC</em>, <em>142</em>,
110358. (<a href="https://doi.org/10.1016/j.asoc.2023.110358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis is of great importance for the intelligent health management of mechanical systems . For engineering applications , it is very difficult to collect and label vibration signals corresponding to machine faults. Due to the complicated operational environment , moreover, useful and critical features are often covered by surrounding noise. For those reasons, a multiscale reduction clustering (MRC) method is proposed for the unsupervised diagnosis of machine faults. In the present approach, vibration signals were collected to generate multiscale convolutional representation through a one-dimensional convolutional neural network without prior knowledge of signal processing techniques . Chosen by a convolutional encoder , the dimensionality of the multiscale convolutional representation was reduced for improving the clustering capability. During this dimensionality reduction, a loss function was proposed to optimize the network and speed up the convergence of the diagnostics. The proposed method was evaluated by two benchmark datasets and an experimental setup. With the present method, the clustering accuracy and normalized mutual information for three datasets are all over 0.91 and 0.82, respectively. Results show that the addressed MRC has superior diagnosis ability under the unsupervised fashion compared to other state-of-the-art models. It is proved that MRC is robust for the diagnosis tasks under different working conditions.},
  archive      = {J_ASOC},
  author       = {Yifan Wu and Chuan Li and Shuai Yang and Yun Bai},
  doi          = {10.1016/j.asoc.2023.110358},
  journal      = {Applied Soft Computing},
  pages        = {110358},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiscale reduction clustering of vibration signals for unsupervised diagnosis of machine faults},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stock index forecasting based on multivariate empirical mode
decomposition and temporal convolutional networks. <em>ASOC</em>,
<em>142</em>, 110356. (<a
href="https://doi.org/10.1016/j.asoc.2023.110356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and forecasting of stock indexes is an important and challenging work in the field of financial research, which is of great significance for investors to reduce risk on investment and improve investment returns. However, the basic data of the stock index includes five indicators, namely, opening price, highest price, lowest price, closing price and trading volume (COHLV), each of which contains some information related to the future trend. And these indicators are affected by different factors such as politics, economy, psychology and so on, so they have nonlinear, high noise and other complex characteristics. These reasons make the existing methods cannot effectively improve the accuracy of stock index forecasting. In order to solve this problem, we propose a hybrid stock index forecasting model named MEMD-TCN, which is based on multivariate empirical mode decomposition (MEMD) and temporal convolutional networks (TCN). The novelty of the proposed model focuses on decomposition of multivariate time series and using only fully convolutional layers to forecast stock index. The proposed model first uses MEMD to decompose the data of COHLV to obtain subsequences of different fluctuation frequencies of each time series, then inputs subsequences of the same frequency into TCN to predict the subsequences of closing price in the next period. Ultimately, the forecasted values for the closing price are obtained by reconstructing the prediction results of all the subsequences of closing price. To evaluate the performance of the proposed MEMD-TCN model, stock index of several countries that can reflect the overall changes of the market are used. The experimental results verify the notable effectiveness and necessity of the decomposition of multivariate time series . Meanwhile, the results demonstrate the significant superiority of the proposed MEMD-TCN model on accuracy and stability.},
  archive      = {J_ASOC},
  author       = {Yuan Yao and Zhao-yang Zhang and Yang Zhao},
  doi          = {10.1016/j.asoc.2023.110356},
  journal      = {Applied Soft Computing},
  pages        = {110356},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stock index forecasting based on multivariate empirical mode decomposition and temporal convolutional networks},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “least learning machine and its experimental
studies on regression capability” [applied soft computing 21 (2014)
677–684]. <em>ASOC</em>, <em>142</em>, 110355. (<a
href="https://doi.org/10.1016/j.asoc.2023.110355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Shitong Wang and Fu-lai Chung and Jun Wu and Jun Wang},
  doi          = {10.1016/j.asoc.2023.110355},
  journal      = {Applied Soft Computing},
  pages        = {110355},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Least learning machine and its experimental studies on regression capability” [Applied soft computing 21 (2014) 677–684]},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ISOMA swarm intelligence algorithm in synthesis of quantum
computing circuits. <em>ASOC</em>, <em>142</em>, 110350. (<a
href="https://doi.org/10.1016/j.asoc.2023.110350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present paper, we demonstrate the possibilities of designing quantum computing circuits using a specific swarm intelligence algorithm — iSOMA in the form of three experiments. All simulations are based on a simple sample of a quantum computing circuit from the Qiskit environment, which was used as a comparison circuit with the results of the three experiments already mentioned. In the first experiment, we try to find an arbitrary functional solution using iSOMA with minimal constraints on this circuit’s design. It can be said that in this experiment, iSOMA showed the highest degree of “creativity”. In the second experiment, we focused on whether iSOMA can be used to find a circuit identical to the one designed by a human or equivalent with the positions of the measurement gates fixed. In the last experiment, we highlight iSOMA’s ability to avoid unnecessary qubit usage by adding redundant qubits to a possible circuit and fixing the measurement gates to the last two qubits in the scheme. In all three experiments, we see that iSOMA can find efficient functional and often astonishing solutions — the proposed method applied to a classical circuit founded a new one preserving required properties while saving one ancilla (redundant, useless, non-used) 1 qubit. All computations are implemented in the IBM Qiskit 2 environment. Although these are relatively simple experiments, the results show that evolutionary algorithms can successfully design more complex quantum circuits .},
  archive      = {J_ASOC},
  author       = {Ivan Zelinka and Lumír Kojecký and Marek Lampart and Jana Nowaková and Jan Plucar},
  doi          = {10.1016/j.asoc.2023.110350},
  journal      = {Applied Soft Computing},
  pages        = {110350},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ISOMA swarm intelligence algorithm in synthesis of quantum computing circuits},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-subpopulation parallel computing genetic algorithm for
the semiconductor packaging scheduling problem with auxiliary resource
constraints. <em>ASOC</em>, <em>142</em>, 110349. (<a
href="https://doi.org/10.1016/j.asoc.2023.110349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the scheduling is established in a semiconductor packaging factory, frequent machine-related changes can pose a serious problem because of the large number of products processed using different machines with auxiliary resources. Thus, the efficiency of the scheduling algorithm is crucial for addressing the semiconductor packaging scheduling problem (SPSP). This study proposed a novel multi-subpopulation parallel computing genetic algorithm (MSPCGA) to solve the SPSP under practical production constraints. The MSPCGA uses a multithreaded central processing unit to perform parallel computing . The graphics processing unit (GPU) grid computing method was applied to modify the genetic algorithm computing architecture to increase the efficiency of the algorithm. Finally, the proposed MSPCGA outperformed two other metaheuristic algorithms in 12 evaluation scenarios. Additionally, the existing factory method was compared with the proposed MSPCGA to verify the effectiveness of the algorithm in practical applications.},
  archive      = {J_ASOC},
  author       = {Hung-Kai Wang and Yu-Chun Lin and Che-Jung Liang and Ya-Han Wang},
  doi          = {10.1016/j.asoc.2023.110349},
  journal      = {Applied Soft Computing},
  pages        = {110349},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-subpopulation parallel computing genetic algorithm for the semiconductor packaging scheduling problem with auxiliary resource constraints},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NCC: Neural concept compression for multilingual document
recommendation. <em>ASOC</em>, <em>142</em>, 110348. (<a
href="https://doi.org/10.1016/j.asoc.2023.110348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a novel method for generating inter-lingual document representations using neural network concept compression. The presented approach is intended to improve the quality of content-based multilingual document recommendation and information retrieval systems by creating a language-independent representation. The main idea is to use mappings to align monolingual representation spaces, using concept compression, to create inter-lingual representations. The proposed approach outperforms traditional cross-lingual retrieval and recommendations methods in experiments conducted on JRC-Acquis and EU bookshop multilingual corpora. Our dataset and code are publicly available at https://github.com/Tsegaye-misikir/NCC .},
  archive      = {J_ASOC},
  author       = {Tsegaye Misikir Tashu and Marc Lenz and Tomáš Horváth},
  doi          = {10.1016/j.asoc.2023.110348},
  journal      = {Applied Soft Computing},
  pages        = {110348},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NCC: Neural concept compression for multilingual document recommendation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrated adversarial long short-term memory deep networks
for reheater tube temperature forecasting of ultra-supercritical
turbo-generators. <em>ASOC</em>, <em>142</em>, 110347. (<a
href="https://doi.org/10.1016/j.asoc.2023.110347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important equipment of the ultra-supercritical turbine systems, real-time and accurate control of reheater tube temperature is the key to the normal operation of the system. To improve the accuracy of reheater tube temperature prediction, an integrated adversarial long short-term memory deep network is proposed in this study. Considering the correlation and dynamics between reheater temperature features, different types of single long short-term memory deep networks are batch mixed and stacked in parallel to integrate adversarial training . The training process shares network layer weight parameters to synthesize the decision boundary of each network and complete the dichotomous integration. The proposed networks predict information for the next ten time steps, wirelessly approximate the mapping relationship between multi-source input and output data, and maximize the prediction accuracy and generalization. The proposed networks with single long short-term memory deep networks and 21 algorithms are compared for temperature prediction on the reheater tubes of thermal power plants in Guangdong Province , China, from March 2020 to May 2020. Compared with 90 networks without an integrated adversary, the mean square error obtained by the integrated networks have a 90.85\% reduction. Furthermore, compared with the algorithm with the second smallest error among the 22 algorithms, the mean absolute percentage error obtained by the integrated networks is significantly reduced by 99.66\%, and the mean absolute error is reduced by 98.36\%.},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Xiaoying Wei},
  doi          = {10.1016/j.asoc.2023.110347},
  journal      = {Applied Soft Computing},
  pages        = {110347},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated adversarial long short-term memory deep networks for reheater tube temperature forecasting of ultra-supercritical turbo-generators},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ranking loss and sequestering learning for reducing image
search bias in histopathology. <em>ASOC</em>, <em>142</em>, 110346. (<a
href="https://doi.org/10.1016/j.asoc.2023.110346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has started to play an essential role in healthcare applications, including image search in digital pathology. Despite the recent progress in computer vision , significant issues remain for image searching in histopathology archives. A well-known problem is AI bias and lack of generalization. A more particular shortcoming of deep models is the ignorance toward search functionality . The former affects every model, the latter only search and matching. Due to the lack of ranking-based learning, researchers must train models based on the classification error and then use the resultant embedding for image search purposes. Moreover, deep models appear to be prone to internal bias even if using a large image repository of various hospitals. This paper proposes two novel ideas to improve image search performance. First, we use a ranking loss function to guide feature extraction toward the matching-oriented nature of the search. By forcing the model to learn the ranking of matched outputs, the representation learning is customized toward image search instead of learning a class label. Second, we introduce the concept of sequestering learning to enhance the generalization of feature extraction. By excluding the images of the input hospital from the matched outputs, i.e., sequestering the input domain, the institutional bias is reduced. The proposed ideas are implemented and validated through the largest public dataset of whole slide images. The experiments demonstrate superior results compare to the-state-of-art.},
  archive      = {J_ASOC},
  author       = {Pooria Mazaheri and Azam Asilian Bidgoli and Shahryar Rahnamayan and H.R. Tizhoosh},
  doi          = {10.1016/j.asoc.2023.110346},
  journal      = {Applied Soft Computing},
  pages        = {110346},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ranking loss and sequestering learning for reducing image search bias in histopathology},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection for multiset-valued data based on fuzzy
conditional information entropy using iterative model and matrix
operation. <em>ASOC</em>, <em>142</em>, 110345. (<a
href="https://doi.org/10.1016/j.asoc.2023.110345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiset-valued data is a powerful tool to deal with missing feature values. Classical rough set theory is sensitive to noise in classification learning due to the stringent condition of equivalence relation. Thus, a class of fuzzy relations can be introduced to describe the similarity between samples in multiset-valued data. However, these fuzzy relations have deficiencies when they are used in the computation of fuzzy conditional information entropy. This paper proposes a new model for multiset-valued data by introducing two variable parameters: one controls the similarity between samples, the other dominates the distance between feature values. This model employs the iterative computation strategy to define fuzzy conditional information entropy that is computing by matrix. The notions of three fuzzy information entropies in a multiset-valued decision information system (MVDIS) are put forward. For measuring any added feature, the significance based on fuzzy conditional information entropy is introduced. Moreover, fuzzy conditional information entropy iterative model ( F C I E I FCIEI -model) is presented. Finally, feature selection in an MVDIS based on F C I E I FCIEI -model and matrix operation is researched, and the relevant algorithm (denoted as FCIEIM-MO algorithm) is proposed. The experimental results demonstrate that FCIEIM-MO algorithm possesses good robustness and is superior to some feature selection algorithms . The experimental results demonstrate that FCIEIM-MO algorithm possesses good robustness and exhibit better performance compared with some feature selection algorithms .},
  archive      = {J_ASOC},
  author       = {Dan Huang and Yiying Chen and Fang Liu and Zhaowen Li},
  doi          = {10.1016/j.asoc.2023.110345},
  journal      = {Applied Soft Computing},
  pages        = {110345},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection for multiset-valued data based on fuzzy conditional information entropy using iterative model and matrix operation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational-based partitioning and strong α, β-cut based
novel method for intuitionistic fuzzy time series forecasting.
<em>ASOC</em>, <em>142</em>, 110336. (<a
href="https://doi.org/10.1016/j.asoc.2023.110336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a computational-based partitioning (CBP) and Strong α , β α, β -cut based novel intuitionistic fuzzy time series (IFTS) forecasting method. Construction of intervals, intuitionistic fuzzification of time series data , appropriate intuitionistic fuzzy logical relationships (IFLRs), and procedure of defuzzification are critical issues that affect the forecasting accuracy of any IFTS method. A Computational-based partitioning approach uses basic statistical parameters to determine the number of intervals and constructing of intervals without specialized knowledge of the domain. For intuitionistic fuzzification of time series data , all those intuitionistic fuzzy sets are taken having both non-zero membership and non-membership grade. A Strong α , β α, β -cut are used to choose apposite IFLRs that deliver importance in investigating the tendency of time series data. We also propose a defuzzification approach to get the numerical values. In this article, two popular historical time series datasets are used to demonstrate the supremacy of the proposed forecasting method. Root mean square error (RMSE) and mean absolute percentage error (MAPE) are used to verify the performance of the proposed method. Verification of the validity of the proposed forecasting method is authenticated by using the values of the Theil inequality coefficient (U) and tracking signal (TS).},
  archive      = {J_ASOC},
  author       = {Manish Pant and Kamlesh Bisht and Seema Negi},
  doi          = {10.1016/j.asoc.2023.110336},
  journal      = {Applied Soft Computing},
  pages        = {110336},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Computational-based partitioning and strong α, β-cut based novel method for intuitionistic fuzzy time series forecasting},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized Seq2Seq model based on multiple methods for
short-term power load forecasting. <em>ASOC</em>, <em>142</em>, 110335.
(<a href="https://doi.org/10.1016/j.asoc.2023.110335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate power load prediction plays a key role in reducing resource waste and ensuring stable and safe operations of power systems . To address the problems of poor stability and unsatisfactory prediction accuracy of existing prediction methods, in this paper, we propose a novel approach for short-term power load prediction by improving the sequence to sequence (Seq2Seq) model based on bidirectional long-short term memory (Bi-LSTM) network. Different from existing prediction models, we apply convolutional neural network , attention mechanism , and Bayesian optimization for the improvement of the Seq2Seq model. Moreover, in the data processing stage, we use the random forest algorithm for feature selection, and also adopt the weighted grey relational projection algorithm for holiday load processing to process the data and thereby overcome the difficulty of holiday load prediction. To validate our model, we choose the power load dataset in Singapore and Switzerland as experimental data and compare our prediction results with those by other models to show that our method can generate a higher prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Yeming Dai and Xinyu Yang and Mingming Leng},
  doi          = {10.1016/j.asoc.2023.110335},
  journal      = {Applied Soft Computing},
  pages        = {110335},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized Seq2Seq model based on multiple methods for short-term power load forecasting},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time DC faults diagnosis in a DC ring microgrid by
using derivative current based optimal weighted broad learning system.
<em>ASOC</em>, <em>142</em>, 110334. (<a
href="https://doi.org/10.1016/j.asoc.2023.110334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for DC faults diagnosis in renewables based DC-ring microgrid (DC-RM). The proposed novel approach consists of a second-order derivative current (SODC) approach, which is adequate for isolation of faults due to its accurate fault detection and faster response during fault. Moreover, this approach is also used to estimate the fault location using cable parameters. On the other hand, most powerful weighted broad learning system (WBLS) is developed for faults classification. However, WBLS has a major challenge i.e., random generation of weights or fixed weights for the input data, which is connected to WBLS network. To address that and generate the suitable weights according to the statistical feature data, optimized WBLS is developed by hybridizing the sine–cosine algorithm (SCA) and chaotic salp swarm algorithm (CSSA), called SC-CSSA-WBLS. Here, the most influential features are used to extract the data from the simulated DC fault current signals. The role of SSA and SCA in the WBLS is to enhance the initialization of population, convergence speed, and search capabilities of both local exploitation and global exploration. Further, the efficacy of the proposed algorithm is validated during different case studies and its superiority is evidenced in terms of detection time, relative computational time, classification accuracy , and relative error when compared to state-of-the-art techniques in MATLAB/Simulink environment. Finally, a real-time hardware setup is implemented using dSPACE DS1104 embedded processor to isolate the various faults that occurred on the DC microgrid accurately using adaptive threshold strategy of SODC approach.},
  archive      = {J_ASOC},
  author       = {Kanche Anjaiah and Smruti Rekha Pattnaik and P.K. Dash and Ranjeeta Bisoi},
  doi          = {10.1016/j.asoc.2023.110334},
  journal      = {Applied Soft Computing},
  pages        = {110334},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A real-time DC faults diagnosis in a DC ring microgrid by using derivative current based optimal weighted broad learning system},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic multi-objective evolutionary algorithm based on
two-stage dimensionality reduction and a region gauss adaptation
prediction strategy. <em>ASOC</em>, <em>142</em>, 110333. (<a
href="https://doi.org/10.1016/j.asoc.2023.110333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multi-objective optimization evolutionary algorithms (DMOEAs) have attracted increasing attention due to their superior performance in real-world applications. It is challenging to rapidly track the changing Pareto set (PS) and Pareto front (PF) in DMOEAs. However, most existing studies only focus on the position change in the PS. To address this problem, we define a series of new PS region changes. Based on these definitions, we propose a dynamic multi-objective evolutionary algorithm based on two-stage dimensionality reduction and a region Gauss adaptation prediction strategy (DRPS). Specifically, we estimate the possible type of PS changes at the next moment by processing historical information. Afterward, the basic scaling population P S B PSB is generated by dimensionality reduction, while parameters including the scaling factor and density change rate are calculated. Combining these parameters with a Gaussian distribution, the algorithm adaptively adjusts the sampling probability of individuals in P S B PSB , achieving prediction with different types of PS changes. We conducted extensive experiments and compared classical algorithms, the latest algorithms, and algorithms with the same type in 25 test functions. The results demonstrate that the proposed algorithm outperforms the compared algorithms in most cases, while implying that the newly defined PS region change is reasonable.},
  archive      = {J_ASOC},
  author       = {Yue Yang and Yongjie Ma and Peidi Wang and Yang Xu and Minghao Wang},
  doi          = {10.1016/j.asoc.2023.110333},
  journal      = {Applied Soft Computing},
  pages        = {110333},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic multi-objective evolutionary algorithm based on two-stage dimensionality reduction and a region gauss adaptation prediction strategy},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of type-2 pythagorean fuzzy set with its
application to sustainable transport system selection. <em>ASOC</em>,
<em>142</em>, 110332. (<a
href="https://doi.org/10.1016/j.asoc.2023.110332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In resolving multicriteria decision-making (MCDM) problems, it is observed that the solution process is frequently affected by the available vague information together with intra-personal uncertainties associated with expert’s opinion. In most of the time fuzzy set or its existing variants fail to capture such uncertainties for making reasonable balance in decision making processes. To reduce the effects of uncertainties, this paper introduces a new variant of fuzzy set, viz., type-2 Pythagorean fuzzy set (T2PFS). The main advantage of using T2PFS in MCDM is that the membership and non-membership values of each argument are presented by Pythagorean fuzzy numbers rather than using fuzzy number or intuitionistic fuzzy number. In model formulation process, at first, T2PFS is explicitly formulated and some basic operations are defined. Subsequently, four aggregation operators are constructed to aggregate the arguments, and their properties are verified. Afterwards, two information measures and a score function are defined. Moreover, two MCDM algorithms based on the developed aggregation operators and TOPSIS are framed. To show the application potentiality of the proposed method, a case study relating to the selection of sustainable transport system in India is considered and solved. In the case study five different fuel-based vehicles, mostly used in India in recent times, are evaluated by the experts as transport alternatives with the consideration of thirteen sustainability related criteria. From the achieved result it is found that in future electric cars would become the most sustainable vehicle in India. Sensitivity analysis of the developed models and comparative study with existing fuzzy and non-fuzzy MCDM methods establish effectiveness of the developed models. This study may facilitate transport authority to select appropriate transport system in connection with sustainable development of a region.},
  archive      = {J_ASOC},
  author       = {Biswajit Sarkar and Debjani Chakraborty and Animesh Biswas},
  doi          = {10.1016/j.asoc.2023.110332},
  journal      = {Applied Soft Computing},
  pages        = {110332},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of type-2 pythagorean fuzzy set with its application to sustainable transport system selection},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-objective and multi-objective optimization for
variance counterbalancing in stochastic learning. <em>ASOC</em>,
<em>142</em>, 110331. (<a
href="https://doi.org/10.1016/j.asoc.2023.110331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks have proved to be useful in a host of demanding applications, therefore becoming increasingly important in science and engineering. Large-scale problems constitute a challenging task for training neural networks using the stochastic gradient descent method and variations, which are based on the random selection of mini-batches of training points at every iteration. The challenge lies on the mandatory use of diminishing search step sizes in order to retain mild error fluctuations throughout the training set preserving so the quality of the network’s generalization capability. Variance counterbalancing was recently proposed as a remedy for addressing the diminishing step sizes in neural network training using stochastic gradient methods . It is based on the concurrent minimization of the average mean squared error of the network along with the error variance over random sets of mini-batches. Also, it promotes the use of advanced optimization algorithms instead of the slowly convergent gradient descent . The present work aims at enriching our understanding of the original variance counterbalancing approach, as well as reformulating it as a multi-objective problem by taking advantage of its bi-objective nature. Experimental analysis reveals the performances of the studied approaches and their competitive edge over the established Adam method.},
  archive      = {J_ASOC},
  author       = {Dimitra G. Triantali and Konstantinos E. Parsopoulos and Isaac E. Lagaris},
  doi          = {10.1016/j.asoc.2023.110331},
  journal      = {Applied Soft Computing},
  pages        = {110331},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Single-objective and multi-objective optimization for variance counterbalancing in stochastic learning},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective optimization for improving machining benefit
based on WOA-BBPN and a deep double q-network. <em>ASOC</em>,
<em>142</em>, 110330. (<a
href="https://doi.org/10.1016/j.asoc.2023.110330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To scientifically and reasonably set and adjust the machining parameters, and improve the machining benefit of the batch machining process including machining quality, performance and efficiency, this paper proposes a low cost and high accurate multi-objective optimization framework by integrating intelligent technologies from three perspectives of physical experiment design, objective function construction and multi-objective optimization. In this framework, multilayer design (MLD) which is a computer experiment design method with low redundancy, good uniformity and filling is applied to the machining physics experiment design to reduce the experiment cost. Whale optimization algorithm (WOA) optimized backpropagation neural network (BBPN) is utilized to construct the optimization objective functions to improve the accuracy of functions. And Deep Double Q-Network (DDQN) is employed to achieve multi-objective optimization of machining quality, efficiency, and performance. The effectiveness of MLD, WOA-BBPN and DDQN in multi-objective optimization of machining process are verified by comparisons of common modeling and multi-objective optimization algorithms based on the milling data. These techniques provide a good guidance for the selection of machining parameters to improve the machining benefit and demonstrates some advantages. The research of this paper also offers a good example of applying intelligent technology in machining.},
  archive      = {J_ASOC},
  author       = {Juan Lu and Zhiheng Chen and Xiaoping Liao and Chaoyi Chen and Haibin Ouyang and Steven Li},
  doi          = {10.1016/j.asoc.2023.110330},
  journal      = {Applied Soft Computing},
  pages        = {110330},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization for improving machining benefit based on WOA-BBPN and a deep double Q-network},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel global best-worst particle swarm optimization
algorithm for solving optimization problems. <em>ASOC</em>,
<em>142</em>, 110329. (<a
href="https://doi.org/10.1016/j.asoc.2023.110329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The range of applications of swarm optimization algorithms is very vast. However, high dimensions and more number of decision variables make these optimization problems more complex. Particle Swarm Optimization (PSO) is the most popular optimizer for performing such types of optimization. PSO is motivated from the movement and intelligence of swarms. However, the primary constraint with the PSO and other swarm algorithms is enormous computational time (CT) due to more number of decision variables in complex problem. The number of steps inside Swarm Intelligence Algorithms (SIAs) also increase the complexity of computation in the process of optimization. Many iterations of the procedure of SIA need more CT since these algorithms are iterative in nature. In this study, a new Global Best-Worst Particle Swarm Optimization (GBWPSO) algorithm has been proposed so as to provide a fully version of parallel algorithm . GBWPSO algorithm is the combination of PSO and Jaya algorithm that provides a refined version of parallel algorithm having more parallelism . The proposed algorithm is executed on three different computational hardware with various combinations of population size and maximum number of iteration on five different standard benchmark functions . The evaluation is done on the basis of performance metrics such as speedup (S), real speedup (RS), maximum speedup (MS), efficiency (E), and scalability. The proposed parallel algorithm (P-GBWPSO) outperforms both parallel version of PSO and Jaya algorithm in terms of less CT and better optimal solution. Based on the results, we found that system 03 (S3) is best on proposed GBWPSO algorithm with an efficiency of 1.2518 compare with system 01 (S1) and system 02 (S2).},
  archive      = {J_ASOC},
  author       = {Lalit Kumar and Manish Pandey and Mitul Kumar Ahirwal},
  doi          = {10.1016/j.asoc.2023.110329},
  journal      = {Applied Soft Computing},
  pages        = {110329},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel global best-worst particle swarm optimization algorithm for solving optimization problems},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of a parallel physics-informed neural network to
solve the multi-body dynamic equations for full-scale train collisions.
<em>ASOC</em>, <em>142</em>, 110328. (<a
href="https://doi.org/10.1016/j.asoc.2023.110328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prohibitive cost of acquiring data from full-scale train collision experiments limits the applicability of data-driven machine learning methods in train collision simulation. Physics-informed neural networks (PINN) attempt to address this challenge by incorporating physics equations as part of the loss function construction. However, the PINN approach is relatively time-consuming when it comes to solving a large number of physical equations. In this paper, a parallel physics-informed neural network (PPINN) methodology is developed for the solution of multibody dynamics equations to further reduce the computational cost. As well, a PPINN-based framework for engineering applications is proposed, investigating the dynamic responses, absorbed energy, collision forces, and wheel vertical rises of the full-scale train collision. Automatic differentiation and parallelization algorithms are applied to the multi-body dynamic equations including mass, damping, and stiffness matrices, as well as the. The residuals and the initial conditions are included in the loss function. The dynamic responses, absorbed energy, collision forces, and wheel vertical rise of the full-scale train collision are simulated and studied in detail. The results obtained from the PPINN method are in excellent agreement with those obtained from the finite element method (FEM), Newmark- β β , and fourth-order Runge–Kutta methods for all four train collision scenarios. Additionally, the PPINN methodology keeps better stability even under large time steps which implies a big potential for computational cost reduction.},
  archive      = {J_ASOC},
  author       = {Zhao Tang and Shaodi Dong and Xiaosong Yang and Jianjun Zhang},
  doi          = {10.1016/j.asoc.2023.110328},
  journal      = {Applied Soft Computing},
  pages        = {110328},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of a parallel physics-informed neural network to solve the multi-body dynamic equations for full-scale train collisions},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Block-segmentation vectors for arousal prediction using
semi-supervised learning. <em>ASOC</em>, <em>142</em>, 110327. (<a
href="https://doi.org/10.1016/j.asoc.2023.110327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To handle emotional expressions in computer applications, Russell’s circumplex model has been useful for representing emotions according to valence and arousal. In SentiWordNet, the level of valence is automatically assigned to a large number of synsets (groups of synonyms in WordNet) using semi-supervised learning. However, when assigning the level of arousal, the existing method proposed for SentiWordNet reduces the accuracy of sentiment prediction . In this paper, we propose a block-segmentation vector for predicting the arousal levels of many synsets from a small number of labeled words using semi-supervised learning. We analyze the distribution of arousal and non-arousal words in a corpus of sentences by comparing it with the distribution of valence words. We address the problem that arousal level prediction fails when arousal and non-arousal words are mixed together in some sentences. To capture the features of such arousal and non-arousal words, we generate word vectors based on inverted indexes by block IDs, where the corpus is divided into blocks in the flow of sentences. In the evaluation experiment, we show that the results of arousal prediction with the block-segmentation vectors using semi-supervised learning outperform the results of the previous methods in SentiWordNet and SocialSent.},
  archive      = {J_ASOC},
  author       = {Yuki Odaka and Ken Kaneiwa},
  doi          = {10.1016/j.asoc.2023.110327},
  journal      = {Applied Soft Computing},
  pages        = {110327},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Block-segmentation vectors for arousal prediction using semi-supervised learning},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An accelerating convolutional neural networks via a 2D
entropy based-adaptive filter search method for image recognition.
<em>ASOC</em>, <em>142</em>, 110326. (<a
href="https://doi.org/10.1016/j.asoc.2023.110326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of CNNs for various vision tasks has been accompanied by a significant increase in required FLOPs and parameter quantities, which has impeded the deployment of CNNs on devices with limited computing resources and power budgets. Network pruning, which compresses and accelerates CNN models, is an effective solution to this issue. Some studies have considered pruning as a special case of neural network search (NAS) in recent years. However, existing techniques are often computationally complex or prone to sub-optimal pruning results. As such, this paper proposes a novel acceleration method via a 2 D E ntropy based- A daptive F ilter S earch (2EAFS). The importance of corresponding filters, measured by utilizing the amount of information contained in feature maps, is employed as a theoretical guide to simplify the complex exhaustive search process. Information entropy is then normalized layer by layer and the resulting value is used to calculate a layer-wise importance score in a single step. Additionally, a sparse constraint equation is constructed based on the negative correlation between filter pruning rates and the importance of convolutional layers . The Nelder–Mead search algorithm is then adopted to quickly and adaptively determine the optimal pruning architecture. Finally, importance weights are inherited using the pruning rate and 2D entropy and model performance are restored through fine-tuning. Extensive experiments conducted with the CIFAR-10/100, ILSVRC-2012, NWPU-RESISC45 and CUB-200-2011 datasets showed this approach achieved considerable accuracy increases, with significant reductions in FLOPs and required parameters that surpassed current state-of-the-art methods by a wide margin. For example, 2EAFS achieved a 44.1\% reduction in FLOPs over ResNet-50, with only a 0.53\% Top-5 accuracy decrease for ILSVRC-2012.},
  archive      = {J_ASOC},
  author       = {Chunlei Li and Huanyu Li and Guangshuai Gao and Zhoufeng Liu and Pengcheng Liu},
  doi          = {10.1016/j.asoc.2023.110326},
  journal      = {Applied Soft Computing},
  pages        = {110326},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An accelerating convolutional neural networks via a 2D entropy based-adaptive filter search method for image recognition},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and analytics of multi-factor disease evolutionary
process by fusing petri nets and machine learning methods.
<em>ASOC</em>, <em>142</em>, 110325. (<a
href="https://doi.org/10.1016/j.asoc.2023.110325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, informatization methods have been gradually applied to medical treatment, in which machine learning and evolutionary computation play an important role. However, the effective methods for the study of multi-factor disease evolutionary process are still largely open. There are some issues in the field of disease analysis, such as the lack of visual multi-factor disease evolution model and effective analysis methods. For a universal method of data analysis and medical diagnosis, the machine learning algorithms should be combined with the formal modeling methods to fully realize the complementary advantages, make model has the advantages of visualization and efficient data analysis. This work proposes a novel research idea for the modeling analysis of current multi-factor diseases and reveal its feasibility, so as to explore potential pharmaceutical targets and enable doctors and patients to better understand the evolution process of multi-factor diseases. It is worth mentioning that, in order to verify the feasibility of the proposed idea, we applied it to the analysis of the role of monoamine hormones in depression. The model incorporates the machine learning algorithms, and it finally outputs the pathogenic probability under different hormone levels, reflecting the importance of different factors on depression. The application case proved that we provide a clear process model and a novel research method for multi-factor disease evolutionary process analysis.},
  archive      = {J_ASOC},
  author       = {Wangyang Yu and Xuyue Wang and Xianwen Fang and Xiaojun Zhai},
  doi          = {10.1016/j.asoc.2023.110325},
  journal      = {Applied Soft Computing},
  pages        = {110325},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling and analytics of multi-factor disease evolutionary process by fusing petri nets and machine learning methods},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An explainable AI (XAI) model for landslide susceptibility
modeling. <em>ASOC</em>, <em>142</em>, 110324. (<a
href="https://doi.org/10.1016/j.asoc.2023.110324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landslides are among the most devastating natural hazards, severely impacting human lives and damaging property and infrastructure. Landslide susceptibility maps, which help to identify which regions in a given area are at greater risk of a landslide occurring, are a key tool for effective mitigation. Research in this field has grown immensely, ranging from quantitative to deterministic approaches , with a recent surge in machine learning (ML)-based computational models . The development of ML models, in particular, has undergone a meteoritic rise in the last decade, contributing to the successful development of accurate susceptibility maps. However, despite their success, these models are rarely used by stakeholders owing to their “black box” nature. Hence, it is crucial to explain the results, thus providing greater transparency for the use of such models. To address this gap, the present work introduces the use of an ML-based explainable algorithm, SHapley Additive exPlanations (SHAP), for landslide susceptibility modeling. A convolutional neural network model was used conducted in the CheongJu region in South Korea. A total of 519 landslide locations were examined with 16 landslide-affected variables, of which 70\% was used for training and 30\% for testing, and the model achieved an accuracy of 89\%. Further, the comparison was performed using Support Vector Machine mode, which achieved an accuracy of 84\%. The SHAP plots showed variations in feature interactions for both landslide and non-landslide locations, thus providing more clarity as to how the model achieves a specific result. The SHAP dependence plots explained the relationship between altitude and slope, showing a negative relationship with altitude and a positive relationship with slope. This is the first use of an explainable ML model in landslide susceptibility modeling, and we argue that future works should include aspects of explainability to open up the possibility of developing a transferable artificial intelligence model.},
  archive      = {J_ASOC},
  author       = {Biswajeet Pradhan and Abhirup Dikshit and Saro Lee and Hyesu Kim},
  doi          = {10.1016/j.asoc.2023.110324},
  journal      = {Applied Soft Computing},
  pages        = {110324},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explainable AI (XAI) model for landslide susceptibility modeling},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis on population-based algorithm optimized filter for
non-invasive fECG extraction. <em>ASOC</em>, <em>142</em>, 110323. (<a
href="https://doi.org/10.1016/j.asoc.2023.110323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaheuristic algorithms (MAs) have become one of the primary tools for optimization in diverse domains, including non-invasive fetal electrocardiogram (fECG) extraction. Research reveals that hyperparameters affect the performance of MAs differently to problems, and some algorithms are problem-specific designed and may produce bad results for different problems. Hence, three questions arise, (1) how much can we trust MAs when solving boxed-constrained fECG extraction problems, and (2) which type of MAs are suitable and adequate for fECG extraction? (3) do MAs find acceptable solutions to a problem that does not formulate the problem comprehensively with an imperfect objective function? This paper focuses on these three inquiries and proposes a framework providing an MA-assisted adaptive filter for non-invasive fECG extraction. The proposed framework has three key components: data pre-processing, MA-based adaptive filter , and post-processing. The pre-processing starts to process the signals detected by pregnant women and extract the desired signal by independent component analysis . Such output signals are then passed to a filter and optimized by population-based algorithms, producing an optimal solution. After that, this solution, as filter weights, will be used for signal extraction and be processed by post-processing. Importantly, the proposed framework is user-friendly that can import any MA algorithm to run and disassemble as a separate assisting tool. This work investigated eight classic and disparate MA algorithms on the Abdominal and Direct Fetal ECG Database (ADFECGDB) dataset, offering comprehensive experimental analysis. We infer from the results that the recordings r 01 , r 02 , r 03 , r 05 , r 08 r01, r02, r03, r05, r08 , and r 09 r09 , where the signals are less noisy and can be solved well. While for recordings r 04 , r 07 , r 06 r04, r07, r06 , and r 10 r10 , the performance can be influenced by hyperparameters for any test algorithms, such as the population size, window size, and other parameters in MAs. Meanwhile, we found that MA may not be the primary target influencing the performance vary.},
  archive      = {J_ASOC},
  author       = {Lingping Kong and Seyedali Mirjalili and Václav Snášel and Jeng-Shyang Pan and Akshaya Raj and Radana Vilimkova Kahankova and Martinek Radek},
  doi          = {10.1016/j.asoc.2023.110323},
  journal      = {Applied Soft Computing},
  pages        = {110323},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis on population-based algorithm optimized filter for non-invasive fECG extraction},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consensus-based failure mode and effect analysis in group
decision-making with incomplete weights of risk factors: Case study of
stereotactic body radiation therapy for lung cancer. <em>ASOC</em>,
<em>142</em>, 110322. (<a
href="https://doi.org/10.1016/j.asoc.2023.110322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure mode and effect analysis (FMEA) is a widely used management technique to identify potential failure modes in products, processes, systems, and services to assign limited resources for implementing improvement efforts. For the FMEA with a number of experts, the experts may take different influences on the decision-making process due to the difference between individual rationality and cognition and the influence of social relations. Besides, incomplete weights of risk factors are vital to capturing the fuzziness of experts’ evaluation. Therefore, this paper proposes a consensus-based FMEA method to explore that how cooperative relationship network of experts, the consensus process, and incomplete weight information of risk factors in FMEA influence the decision-making process. First, a consensus reaching process considering cooperative relationships between experts, the self-confidence of experts, and minimum adjustment of expert opinion is proposed to derive a collective evaluation of group. Afterward, the stochastic multicriteria acceptability analysis method is integrated with the gained and lost dominance score method to deal with incomplete weights of risk factors in the FMEA. The proposed method is employed in a case study about ranking the failure modes of stereotactic body radiation therapy for lung cancer patients . The robustness and feasibility of the proposed method are validated by comparative analysis.},
  archive      = {J_ASOC},
  author       = {Fan Liu and Huchang Liao and Xingli Wu and Abdullah Al-Barakati},
  doi          = {10.1016/j.asoc.2023.110322},
  journal      = {Applied Soft Computing},
  pages        = {110322},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consensus-based failure mode and effect analysis in group decision-making with incomplete weights of risk factors: Case study of stereotactic body radiation therapy for lung cancer},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An entropy-based density peak clustering for numerical gene
expression datasets. <em>ASOC</em>, <em>142</em>, 110321. (<a
href="https://doi.org/10.1016/j.asoc.2023.110321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In molecular biology, gene expression analysis is one of the important research areas which deals with identifying the genes having similar functionality known as co-expressed genes. Data mining techniques like clustering are frequently employed for grouping gene expressions with similar functional characteristics. Numerous such clustering techniques are available for gene expression analysis. Usually, gene expression datasets are a result of millions of measurements due to which they possess high dimensionality and noise which makes the conventional distance measures ineffective. On the other hand, entropy-based distance computation is much more efficient to capture the inhomogeneity in large dimensional data and is also quite insensitive to noise. To exploit these advantages, we propose a novel method to compute the density distribution of data points in high-dimensional and noisy gene expression datasets using the concept of entropy. After obtaining the density distribution, an existing technique known as “Extreme Clustering” is used to obtain the desired clusters present in the gene expressions dataset. The proposed technique is implemented and evaluated on diversified microarray gene expression datasets. Experiment results show that the proposed technique outperforms other popular density-based techniques in terms of cluster quality, robustness against noise, and biological significance of the genes within the clusters.},
  archive      = {J_ASOC},
  author       = {Rashmi Maheshwari and Amaresh Chandra Mishra and Sraban Kumar Mohanty},
  doi          = {10.1016/j.asoc.2023.110321},
  journal      = {Applied Soft Computing},
  pages        = {110321},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An entropy-based density peak clustering for numerical gene expression datasets},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A local opposition-learning golden-sine grey wolf
optimization algorithm for feature selection in data classification.
<em>ASOC</em>, <em>142</em>, 110319. (<a
href="https://doi.org/10.1016/j.asoc.2023.110319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification problem is an important research topic in machine learning and data mining. Feature selection can remove irrelevant and redundant features and improve classification accuracy . The traditional Grey Wolf algorithm (GWO) has the defects of low convergence efficiency and easy to falls into local extremes in solving the feature selection process, leading to ineffective removal of irrelevant and redundant features. This paper proposes a binary version of the local opposing learning golden sine grey wolf optimization algorithm (OGGWO). First, the OGGWO algorithm uses local opposing learning mapping to initialize the positions of individual grey wolves to enrich population diversity and improve convergence speed. Secondly, mix the golden sine algorithm and the grey wolf optimization algorithm to control the direction and distance of α α wolves by using the golden mean coefficient to improve the autonomous search ability of individual grey wolves and avoid the algorithm from falling into the local optimum. Finally, the updated grey wolf position is binary converted by pre-setting the threshold value to reduce the feature subset’s size and improve the classification effect. To verify the effectiveness of the OGGWO algorithm, 18 international standard datasets were selected, and compare the OGGWO algorithm with the improved Grey Wolf algorithm and the popular metaheuristic algorithm for the fitness value comparison test and the simulation comparison experiment for classification accuracy . The results show that: (1) the OGGWO algorithm has good convergence and high search accuracy on all 18 test data; (2) the improved strategy of the OGGWO algorithm can effectively improve the classification accuracy and reduce the number of selected features compared with the traditional GWO algorithm in the classification accuracy simulation. The experimental results show that the superiority and robustness of the OGGWO algorithm in feature selection are verified.},
  archive      = {J_ASOC},
  author       = {Zhang Li},
  doi          = {10.1016/j.asoc.2023.110319},
  journal      = {Applied Soft Computing},
  pages        = {110319},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A local opposition-learning golden-sine grey wolf optimization algorithm for feature selection in data classification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interval reliability sensitivity analysis using monte carlo
simulation and mouth brooding fish algorithm (MBF). <em>ASOC</em>,
<em>142</em>, 110316. (<a
href="https://doi.org/10.1016/j.asoc.2023.110316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reliability analysis and reliability-based design approach, sensitivity analysis establishes a relation between the variation in random variable parameters and that in reliability. The analysis of sensitivity is also employed to determine key random variables with the greatest contribution to reliability. The sensitivity analysis of a non-linear limit state function is relatively complicated and time-consuming. Therefore, a novel approach is proposed in this study to calculate the reliability sensitivity parameter in terms of an interval. In the proposed algorithm, the non-linear limit state function is initially converted to a linear limit state function by the Monte Carlo simulation method. Then, the interval random variables are introduced to include both epistemic and aleatory uncertainties so as to obtain a realistic result. In the second step of the proposed methodology, the mouth-brooding fish algorithm (MBF) is applied to capture the upper and lower bounds of the reliability sensitivity parameters. Several illustrative examples were then presented, showing the capability of the proposed approach in obtaining results accurately and efficiently. According to the results, the percentage of improvement in the sensitivity analysis of random variables in the proposed method, compared to the classical method, by using the MBF optimization algorithm to the parameters of random variables due to the variation Coefficient is more than 40 percents. This means that the length of the response interval has increased.},
  archive      = {J_ASOC},
  author       = {M. Babazadeh and O. Rezayfar and E. Jahani},
  doi          = {10.1016/j.asoc.2023.110316},
  journal      = {Applied Soft Computing},
  pages        = {110316},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval reliability sensitivity analysis using monte carlo simulation and mouth brooding fish algorithm (MBF)},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Policy gradient empowered LSTM with dynamic skips for
irregular time series data. <em>ASOC</em>, <em>142</em>, 110314. (<a
href="https://doi.org/10.1016/j.asoc.2023.110314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series modelling has been successfully handled by Long Short-Term Memory (LSTM) models. Yet their performance can be severely inhibited by the occurrence of missing values prevalent in many real-life datasets. Many previous studies have been dedicated to imputation methods for generating a complete time series sequence, which have their limitations in terms of imputation bias and inaccuracies. In this paper, we propose a new LSTM model incorporating policy gradient (PG) based reinforcement learning called PG-LSTM, which can mitigate the effect of missing data and capture time-based input feature patterns more effectively to improve prediction performance. Inspired by numerous sequence models’ successes in improving the efficiency of processing language data by skipping irrelevant tokens, the PG-LSTM introduces dynamic skip connections between LSTM cell states for time series data for classification and regression tasks for the first time. Specifically, the proposed model comprises a modified LSTM cell architecture that can internally call a policy-based reinforcement learning agent to generate a skipping action, allowing the model to dynamically select the optimal subset of hidden and cell states from past states to capture periodic and non-periodic patterns within a time series sequence. Moreover, the PG-LSTM also designs a lightweight imputation layer using a simple missing value imputation strategy while incorporating missing indicators and skipping segments of unimportant data to reduce the limitations associated with imputed data for handling missing values. Our experimental results on regression and classification tasks on time series data with high rates of missing values demonstrate that the PG-LSTM improves performance against current gated recurrent neural networks (RNN) and conventional non-neural network algorithms. The PG-LSTM can enhance AUC by up to 18.5\% in the classification task and RMSE by up to 19.3\% in the regression task over gated RNN models, respectively. Our findings are also statistically analysed using statistical significance testing with post hoc analysis.},
  archive      = {J_ASOC},
  author       = {Philip B. Weerakody and Kok Wai Wong and Guanjin Wang},
  doi          = {10.1016/j.asoc.2023.110314},
  journal      = {Applied Soft Computing},
  pages        = {110314},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Policy gradient empowered LSTM with dynamic skips for irregular time series data},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient flow-based meta generative adversarial network for
data augmentation in fault diagnosis. <em>ASOC</em>, <em>142</em>,
110313. (<a href="https://doi.org/10.1016/j.asoc.2023.110313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date, various meta-learning methods have been explored to face the data-scarcity problem in fault diagnosis. Almost without exception, these methods work on the premise that an auxiliary dataset close or related to the target data exists and can be employed as a background set to pre-train a deep network. However, in diversified industrial applications, such an appropriate dataset is not always available or requires heavy time and resource consumption for search. To address this problem, a Gradient Flow-based Meta Generative Adversarial Network (GFMGAN) is proposed for fault diagnosis of rotating machinery under the condition of insufficient training data. In the proposed method, a novel architecture taking an Unconditional Generative Model from A Single Natural Image (SinGAN) as the backbone is developed to capture the data distribution from several signal patches, permitting the training on few training data. Then, a gradient flow-based meta-learning technique is introduced into the GFMGAN to modify its learning property and further boost its generative ability. The well-trained GFMGAN can produce sufficient imitated samples. Thereafter, the quality of the generated data is evaluated by three methods, namely Fast Fourier transform (FFT)-based spectrum analysis, t-distributed stochastic neighbor embedding (t-SNE)-based feature visualization, and novel Gramian Angular Summation Fields (GASF)-based signal imaging . Ultimately, these newly generated vibration signals are employed as supplementary data to train the classification model and to finish diagnosis tasks downstream. Extensive experiments illustrate the effectiveness and superiority of the proposed framework over other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Rugen Wang and Zhuyun Chen and Weihua Li},
  doi          = {10.1016/j.asoc.2023.110313},
  journal      = {Applied Soft Computing},
  pages        = {110313},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gradient flow-based meta generative adversarial network for data augmentation in fault diagnosis},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining genetic local search into a multi-population
imperialist competitive algorithm for the capacitated vehicle routing
problem. <em>ASOC</em>, <em>142</em>, 110309. (<a
href="https://doi.org/10.1016/j.asoc.2023.110309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vehicle Routing Problem (VRP) is one of the most significant problems in operational research today. VRP has a vast range of application fields such as transportation, logistics, manufacturing, relief systems and communication. To suit the needs of different real-world VRP scenarios, many models of VRP have been developed — CVRP (Capacitated VRP) being the classical form. In this article, a hybrid metaheuristic algorithm , ICAHGS, is proposed for solving CVRP. The present study proposes a refined Imperialist Competitive Algorithm (ICA) as the primary evolutionary and multi-population method for addressing the Capacitated Vehicle Routing Problem (CVRP). In order to further optimize the search process, a Hybrid Genetic Search (HGS-CVRP) algorithm is applied as an enhanced local search and population management strategy within the ICA framework. Additionally, the internal restart step of the HGS-CVRP algorithm is replaced with a multi-step restart mechanism for intensification improvement. One notable aspect of the proposed method is its ability to facilitate parallel processing , with each empire able to be processed on a separate processor. This structure allows for increased computational efficiency in addressing the CVRP. To assess the effectiveness of the proposed algorithm, it has been compared to several state-of-the-art algorithms from the literature. The results of this comparison, which include both classical benchmark instances and real-world applications, demonstrate the competitive performance of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Babak Rezaei and Frederico Gadelha Guimaraes and Rasul Enayatifar and Pauline C. Haddow},
  doi          = {10.1016/j.asoc.2023.110309},
  journal      = {Applied Soft Computing},
  pages        = {110309},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combining genetic local search into a multi-population imperialist competitive algorithm for the capacitated vehicle routing problem},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic clustering and feature selection using
multi-objective crow search algorithm. <em>ASOC</em>, <em>142</em>,
110305. (<a href="https://doi.org/10.1016/j.asoc.2023.110305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s real-world data is frequently significant in size, with many redundant, missing, and noise-based features and data instances must be addressed before applying various data-mining-based algorithms for further knowledge discovery. Excessive dimensionality may be mitigated by carefully excluding unnecessary characteristics and selecting a reasonable subset of features. When presented as an optimization issue, choosing the best clusters using the most suitable subset of attributes is a challenge that may be handled using practical meta-heuristic approaches. Besides this, the automatic finding of the appropriate cluster number is another challenging task for the real-world dataset in the unsupervised machine-learning study. The present work proposes a multi-objective crow search algorithm for clustering and feature selection (MO-CSACFS) by modifying the crow search algorithm and introducing a levy flight-based two-point cross-over mechanism for a better exploration phase of the crow and further making it suitable for multi-objective optimization problems. MO-CSACFS addresses both issues using the three objective functions to find appropriate cluster numbers and features. MO-CSACFS is implemented over several real-life and synthetic datasets with varying instances, features, and cluster numbers to assess the algorithm’s performance; apart from that, the present work is also applied over several gene-expression datasets. MO-CSACFS is compared with two similar recently proposed multi-objective optimization processes used over an automatic, unsupervised machine learning task. The results show that the MO-CSACFS has produced a compact and robust cluster comparable to other similar works from the literature.},
  archive      = {J_ASOC},
  author       = {Rajesh Ranjan and Jitender Kumar Chhabra},
  doi          = {10.1016/j.asoc.2023.110305},
  journal      = {Applied Soft Computing},
  pages        = {110305},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic clustering and feature selection using multi-objective crow search algorithm},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stacking ensemble method for personal credit risk assessment
in peer-to-peer lending. <em>ASOC</em>, <em>142</em>, 110302. (<a
href="https://doi.org/10.1016/j.asoc.2023.110302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, China’s Peer-to-Peer (P2P) lending industry has been seen as an important credit source but it has recently suffered from a wave of bankruptcies . Using 126, 090 P2P loan deals from RenRen Dai, one of the biggest online P2P websites in China, this paper attempts to predict credit default probabilities for P2P lending by implementing machine-learning techniques. More specifically, this study proposes a stacking ensemble machine-learning model to assess credit default risk for P2P lending platforms. A Max-Relevance and Min-Redundancy (MRMR) method is used for feature selection and then irrelevant features are eliminated by using k-means clustering method . Finally, the stacking ensemble model is performed to produce accurate and stable predictions in the feature subset. Experimental results show that stacking ensemble model yields high performance, not only in prediction accuracy but also in precision and recall. In comparison to single classifiers, the stacking ensemble machine-learning model has a minimum error rate and provides more accurate credit default risk prediction. The results also confirm the efficiency of the proposed stacking ensemble model through the area under the ROC curve.},
  archive      = {J_ASOC},
  author       = {Wei Yin and Berna Kirkulak-Uludag and Dongmei Zhu and Zixuan Zhou},
  doi          = {10.1016/j.asoc.2023.110302},
  journal      = {Applied Soft Computing},
  pages        = {110302},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stacking ensemble method for personal credit risk assessment in peer-to-peer lending},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertain interval TOPSIS and potentially regrettable
decisions within ICT evaluation environments. <em>ASOC</em>,
<em>142</em>, 110301. (<a
href="https://doi.org/10.1016/j.asoc.2023.110301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally assumed that the rankings provided by Multi-Attribute Decision-Making (MADM) techniques are definitive. Once the ranking is delivered, decision makers (DMs) are expected to choose the first alternative and dismiss the remaining ones, concluding the application of the corresponding model. The MADM literature has incorporated fuzziness and imprecision to its models to deal with evaluation uncertainties but has not accounted for its consequences defined in terms of regrettable choices. That is, MADM models do not consider the possible consequences of having chosen an alternative whose actual characteristics do not correspond to those expected by the DM. This paper aims at designing an integrated MADM framework with interval variables where the DM is allowed to modify the initial alternative chosen after observing the realizations of its characteristics. In order to do so, sequences of alternatives including the initial choice as well as subsequent alternate choices should be ranked in place of single alternatives. We analyze the combinatorial decision environment that arises from defining and evaluating sequences of choices by accounting for the whole set of potential realizations and any subsequent change in the alternatives selected. The TOPSIS method is used to design the integrate evaluation framework producing the final ranking. A case study analyzing the entry decision of a firm within a group of European countries based on their levels of ICT development is presented. We illustrate how the countries selected and their order may differ substantially when accounting for the complementarities existing among them. Moreover, the selection process and any subsequent decision vary with the number of modifications considered relative to the initial country selected. The results obtained are of interest not only to firms facing a similar problem, but also to DMs or managers dealing with strategic selection processes where the wrong choice of alternatives may lead to increasingly complex sequential disruptions.},
  archive      = {J_ASOC},
  author       = {Debora Di Caprio and Francisco J. Santos-Arteaga},
  doi          = {10.1016/j.asoc.2023.110301},
  journal      = {Applied Soft Computing},
  pages        = {110301},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertain interval TOPSIS and potentially regrettable decisions within ICT evaluation environments},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-channel multi-tower GNN model for job transfer
prediction based on academic social network. <em>ASOC</em>,
<em>142</em>, 110300. (<a
href="https://doi.org/10.1016/j.asoc.2023.110300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of scholar job transfer is beneficial to both individual career success and talent recruitment of institutions. For such a complicated problem, the diversity and objectivity of the obtained information are essentially significant. However, previous works focus on the general job-hopping problems and utilize the information from the recruitment or social platform, which is somehow limited by data privacy. In this paper, we define the scholar job transfer prediction task by introducing more diversified information (e.g., career sequence, collaboration graph), which is obtained from their publications, for more comprehensive modeling without privacy data. Moreover, we design a M ulti-channel M ulti-tower E nhanced F ramework (MMEF) to integrate the heterogeneous inputs in a complementary manner, which can capture the temporal pattern from career trajectories, leverage the academic collaboration information considering the influence from co-authors, and deal with extra descriptions and estimate the relevance scores between the scholars and institutions. Extensive experiments on two real-world datasets demonstrate the superiority of the proposed framework, which outperforms the state-of-the-art approaches by about 20\% overall for making better use of the potential patterns in heterogeneous data . More intensive studies explore how and the degree to different collaborators impact scholar job transfer.},
  archive      = {J_ASOC},
  author       = {Ruoyan Zhao and Zhou Shao and Wenhu Zhang and Jiachen Zhang and Chunming Wu},
  doi          = {10.1016/j.asoc.2023.110300},
  journal      = {Applied Soft Computing},
  pages        = {110300},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-channel multi-tower GNN model for job transfer prediction based on academic social network},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privileged information learning with weak labels.
<em>ASOC</em>, <em>142</em>, 110298. (<a
href="https://doi.org/10.1016/j.asoc.2023.110298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privileged information learning is proposed to construct the classifier by incorporating privileged knowledge. At present, most of the privileged information learning methods assume that the instance is accurately labeled. However, in real-world applications, an instance may be weakly labeled. In this paper, we propose a novel privileged information learning method with weak labels (PLWB). The hypothesis of our work is that an instance may be annotated by a number of labelers and different labelers may give different labels to this instance due to distinct professional knowledge and subjective factors. It leads to ambiguous labels of instances, namely weak labels. To solve this problem, our methodology is to give each labeler a weight and incorporate these weights into a privileged information learning model. Our technique is to employ a heuristic framework to optimize the labeler weights and the privileged information learning model jointly. The existing privileged information learning methods do not consider the weak label problem, and assign an equal or random weight to each labeler. Our work is different from these methods. The novelty and theoretical contribution is that this is the first work to deal with the weak label problem in privileged information learning. The merit is that we assign an unknown weight to each labeler and solve the optimal values of these weights in the optimization process, such that the performance of the learning model can be improved with the optimal labeler weights. In the experiments, the tool that we use is MATLAB, in which we implement our algorithm. The experimental datasets include one handwritten categorization dataset, two image classification datasets (i.e., Animals-with-Attributes dataset and Caltech-101 dataset), and one disease diagnosis dataset (i.e., Alzheimer’s Disease Neuroimaging Initiative dataset), in which the number of instances used is 2000, 6180, 8677 and 202, respectively. The obtained results are that: (1) by optimizing the labeler weights, the proposed PLWB method obtains explicitly higher classification accuracy than the existing privileged information learning methods; (2) PLWB has relatively higher training time since it needs to solve the labeler weights in the optimization process.},
  archive      = {J_ASOC},
  author       = {Yanshan Xiao and Zexin Ye and Liang Zhao and Xiangjun Kong and Bo Liu and Kemal Polat and Adi Alhudhaif},
  doi          = {10.1016/j.asoc.2023.110298},
  journal      = {Applied Soft Computing},
  pages        = {110298},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Privileged information learning with weak labels},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Genetic algorithms as classical optimizer for the quantum
approximate optimization algorithm. <em>ASOC</em>, <em>142</em>, 110296.
(<a href="https://doi.org/10.1016/j.asoc.2023.110296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is one of the research areas where quantum computing could bring significant benefits. In this scenario, a hybrid quantum–classical variational algorithm, the Quantum Approximate Optimization Algorithm (QAOA), is receiving much attention for its potential to efficiently solve combinatorial optimization problems . This approach works by using a classical optimizer to identify appropriate parameters of a problem-dependent quantum circuit , which ultimately performs the optimization process. Unfortunately, learning the most appropriate QAOA circuit parameters is a complex task that is affected by several issues, such as search landscapes characterized by many local optima. Moreover, gradient-based optimizers, which have been pioneered in this context, tend to waste quantum computing resources. Therefore, gradient-free approaches are emerging as promising methods to address this parameter-setting task. Following this trend, this paper proposes, for the first time, the use of genetic algorithms as gradient-free methods for optimizing the QAOA circuit. The proposed evolutionary approach has been evaluated in solving the MaxCut problem for graphs with 5 to 9 nodes on a noisy quantum device . As the results show, the proposed genetic algorithm statistically outperforms the state-of-the-art gradient-free optimizers by achieving solutions with a better approximation ratio.},
  archive      = {J_ASOC},
  author       = {Giovanni Acampora and Angela Chiatto and Autilia Vitiello},
  doi          = {10.1016/j.asoc.2023.110296},
  journal      = {Applied Soft Computing},
  pages        = {110296},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Genetic algorithms as classical optimizer for the quantum approximate optimization algorithm},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breast cancer classification using deep q learning (DQL) and
gorilla troops optimization (GTO). <em>ASOC</em>, <em>142</em>, 110292.
(<a href="https://doi.org/10.1016/j.asoc.2023.110292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is a primary reason for death among the female population around the world. Early identification can aid in decreasing the mortality rates associated with this disease all over the world. The application of big data in healthcare not only saves lives but also saves time and money. Hospital records, patient medical information, and medical exam results are big data sources in the healthcare sector . In this research, a big data-based two-class (i.e., Benign or Cancer) BC classification model is developed using the Deep Reinforcement Learning (DRL) method. The model stages are big data collection, preprocessing, feature selection, classification, and explanations. In the preprocessing step , the data is normalized, and the missing values are replaced. The gorilla troops optimization (GTO) algorithm is employed for the feature selections. Deep reinforcement learning-based Deep Q learning (DQL) is used for the classification, and LIME is used to explain the predicted output. Three different datasets such as WBCD, WDBC, and WPBC from the UCI repository, are used for evaluation. We verify the proposed model against the radial basis function ensemble boosting learning method (RBF-ELB), the Particle Swarm Optimization Multilayer Perceptron (PSO-MLP), and the Genetic Algorithm Multilayer Perceptron (GA-MLP). The results of the experiments show that our method outperforms the traditional methods. The proposed GTO-DQL model achieves 98.90\% accuracy for the WBCD dataset, 99.02\% for WDBC, and 98.88\% for the WPBC dataset, respectively.},
  archive      = {J_ASOC},
  author       = {Saad Almutairi and Manimurugan S. and Byung-Gyu Kim and Majed M. Aborokbah and Narmatha C.},
  doi          = {10.1016/j.asoc.2023.110292},
  journal      = {Applied Soft Computing},
  pages        = {110292},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Breast cancer classification using deep q learning (DQL) and gorilla troops optimization (GTO)},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A conditional generative adversarial network-based synthetic
data augmentation technique for battery state-of-charge estimation.
<em>ASOC</em>, <em>142</em>, 110281. (<a
href="https://doi.org/10.1016/j.asoc.2023.110281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a battery data augmentation approach to enrich training data for state-of-charge (SOC) estimation algorithm . The approach is evaluated on battery datasets collected under various conditions to test its effectiveness. Visual comparison and low Kullback–Leibler divergence values prove that synthetic data is indistinguishable from real battery data. The computation results show that the performance of the SOC estimator can be greatly improved by adding synthetic data to the training data, and the accuracy of the estimator is even better than that of our previously proposed advanced white-box method. This data augmentation approach provides a credible way to enrich training data for SOC estimation algorithm and we have confidence that it will further accelerate the development of accurate SOC estimators. The proposed generative method is also an universal method to generate multi-type time series, rather than a method only applicable to battery data augmentation.},
  archive      = {J_ASOC},
  author       = {Xianghui Qiu and Shuangfeng Wang and Kai Chen},
  doi          = {10.1016/j.asoc.2023.110281},
  journal      = {Applied Soft Computing},
  pages        = {110281},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A conditional generative adversarial network-based synthetic data augmentation technique for battery state-of-charge estimation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NSGA-III-SD based fuzzy energy management system
optimization for lithium battery/supercapacitor HEV. <em>ASOC</em>,
<em>142</em>, 110280. (<a
href="https://doi.org/10.1016/j.asoc.2023.110280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid energy storage system (HESS), a combination of lithium battery (LIB) and supercapacitor (SC) in hybrid electric vehicles (HEV), can slow down the degradation of battery performance and improve the peak current characteristics by using an optimal energy management system (EMS). A fuzzy EMS controller is first designed to allocate the energy between LIB and SC, then an improved NSGA-III algorithm based on similarity and diversity selection (NSGA-III-SD) is proposed to reduce energy consumption, lithium battery output current, peak power and prolong the life of LIB as well as to meet the SOC of SC constraint by optimizing the parameters of the fuzzy rule base and membership functions (MFs). Four typical driving patterns including busy city, urban, suburban and highway are selected to demonstrate the efficiency of the proposed method. The experimental results indicate that the optimal fuzzy EMS exhibits good adaptive power distribution capability under all driving conditions.},
  archive      = {J_ASOC},
  author       = {Ruibin Gao and Jili Tao and Jingyi Zhang and Longhua Ma and Ming Xu},
  doi          = {10.1016/j.asoc.2023.110280},
  journal      = {Applied Soft Computing},
  pages        = {110280},
  shortjournal = {Appl. Soft. Comput.},
  title        = {NSGA-III-SD based fuzzy energy management system optimization for lithium battery/supercapacitor HEV},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Set-based integer-coded fuzzy granular evolutionary
algorithms for high-dimensional feature selection. <em>ASOC</em>,
<em>142</em>, 110240. (<a
href="https://doi.org/10.1016/j.asoc.2023.110240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays a pivotal role in handling today’s high-dimensional databases by keeping only the most valuable features, leading to less computation, improved performance, and higher transparency in decision-making processes. Despite the considerable advances in combinatorial optimization , this data-preprocessing step is computationally NP-hard and continues to pose critical challenges, particularly for very high-dimensional (VHD) databases. Here, we propose integer coding and fuzzy granulation (FG) as an integral part of evolutionary wrapper-based feature selection. Based on this integer coding, we further propose crossover and mutation operators that employ set operations such as ‘union, ’ ‘intersection, ’ and ‘complement’ for higher transparency in their evolutionary explorative and exploitative search processes. In addition to its common use as a surrogate technique to avoid unnecessary computations by recognizing similarities, the fuzzy granulation concept also operates as a repulsive strategy that searches for dissimilarities in the elitist and population initialization routines to reach higher population diversity. An ablation study is implemented to discover the role of individual components of this multi-prong approach. The results are then compared on 22 benchmark problems, ranging from 64 to 138672 attributes, with nine competing methods. Superior performance is shown for the proposed approach in terms of accuracy (in 15 of 22 cases) and achieving a substantially smaller (as much as six times less) feature set with considerably less computational cost (by an average of 30 percent), particularly for VHD feature selection},
  archive      = {J_ASOC},
  author       = {H. Saadatmand and M.-R. Akbarzadeh-T},
  doi          = {10.1016/j.asoc.2023.110240},
  journal      = {Applied Soft Computing},
  pages        = {110240},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Set-based integer-coded fuzzy granular evolutionary algorithms for high-dimensional feature selection},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A coevolutionary algorithm based on reference line guided
archive for constrained multiobjective optimization. <em>ASOC</em>,
<em>142</em>, 110169. (<a
href="https://doi.org/10.1016/j.asoc.2023.110169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective space of the constrained multiobjective optimization problem (CMOP) is constantly torn by the applied constraints. This makes evolutionary algorithms , which are driven by objectives, face greater difficulties in feasibility, convergence, and diversity. Most evolutionary algorithms will be trapped in local optimums such as a fake Pareto-optimal front or a mutilated Pareto-optimal front. To address this issue, this paper proposes an archive-assisted evolutionary framework with a novel archive structure and cooperative mechanism. A reference line guided archive (RA) is established to record the evolution of the unconstrained solutions. The updated criteria of RA are based on the distance from the individual to the reference line and the unconstrained dominance relation. A specially designed adaptive mating selection operator will select mating parents from RA and the main population according to the convergence of the main population and RA, respectively. The participation of RA in offspring reproduction is conducive to skipping infeasible regions for extensive searches. The performance of the archive-assisted nondominated sorting genetic algorithm (AA-NSGA), which embeds the proposed archive strategy into the nondominated sorting genetic algorithm II, is compared with four state-of-the-art constrained multiobjective evolutionary algorithms (MOEAs). The experimental results on 38 benchmark CMOPs and a reactor network design problem show that the proposed AA-NSGA has a high performance among the existing MOEAs in terms of feasibility, convergence, and diversity.},
  archive      = {J_ASOC},
  author       = {Pengbo Wang and Houxiu Xiao and Xiaotao Han and Fan Yang and Liang Li},
  doi          = {10.1016/j.asoc.2023.110169},
  journal      = {Applied Soft Computing},
  pages        = {110169},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A coevolutionary algorithm based on reference line guided archive for constrained multiobjective optimization},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental particle swarm optimization for large-scale
dynamic optimization with changing variable interactions. <em>ASOC</em>,
<em>141</em>, 110320. (<a
href="https://doi.org/10.1016/j.asoc.2023.110320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative coevolutionary algorithms have been developed for large-scale dynamic optimization problems via divide-and-conquer mechanisms. Interacting decision variables are divided into the same subproblem for optimization. Their performance greatly depends on problem decomposition and response abilities to environmental changes. However, existing algorithms usually adopt offline decomposition and hence are insufficient to adapt to changes in the underlying interaction structure of decision variables. Quick online decomposition then becomes a crucial issue, along with solution reconstruction for new subproblems. This paper proposes incremental particle swarm optimization to address the two issues. In the proposed method, the incremental differential grouping obtains accurate groupings by iteratively performing edge contractions on the interaction graph of historical groups. A recombination-based sampling strategy is developed to generate high-quality solutions from historical solutions for new subproblems. In order to coordinate with the multimodal property of the problem, swarms are restarted after convergence to search for multiple high-quality solutions. Experimental results on problem instances up to 1000-D show the superiority of the proposed method to state-of-the-art algorithms in terms of solution optimality . The incremental differential grouping can obtain accurate groupings using less function evaluations.},
  archive      = {J_ASOC},
  author       = {Xiao-Fang Liu and Zhi-Hui Zhan and Jun Zhang},
  doi          = {10.1016/j.asoc.2023.110320},
  journal      = {Applied Soft Computing},
  pages        = {110320},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incremental particle swarm optimization for large-scale dynamic optimization with changing variable interactions},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bio-inspired emergent control approach for distributed
processes. <em>ASOC</em>, <em>141</em>, 110318. (<a
href="https://doi.org/10.1016/j.asoc.2023.110318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of industrial processes has grown exponentially, with high degrees of dependency, non-linearity, imprecision , among other aspects, which motivates the interest in developing distributed control systems for their management. In this sense, this work proposes a bio-inspired distributed control approach, where control actions emerge from the component interactions. The distributed control approach is based on the response threshold model to solve the control problem by imitating the behavior of ants. Particularly, our approach is inspired by the way as the ants carry out the division of labor in a colony. Thus, our control approach based on the threshold response model refers to the possibility of reacting to stimuli associated with the distributed control tasks. It has the ability to stabilize the process in the presence of abrupt/successive changes and various initial conditions, with a minimum effort of the actuators to achieve the objectives. Also, it has shown its versatility in different operational contexts with the same parameter tuning. The bio-inspired control approach is proved in a quadruple tank process, a complex system due to its multivariate nature. In this way, our paper introduces a new domain of application of the response threshold model in industrial processes. Several experiments were carried out in different contexts to evaluate its stability, robustness, etc., and compare it with other similar works. In general, the control performance metrics show satisfactory results, which reflects its ability to adapt to changes in the dynamics of the process, which encourages additional studies.},
  archive      = {J_ASOC},
  author       = {Marcel García and Jose Aguilar},
  doi          = {10.1016/j.asoc.2023.110318},
  journal      = {Applied Soft Computing},
  pages        = {110318},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bio-inspired emergent control approach for distributed processes},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial example detection using semantic graph matching.
<em>ASOC</em>, <em>141</em>, 110317. (<a
href="https://doi.org/10.1016/j.asoc.2023.110317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have recently been found to be vulnerable to adversarial examples , which can deceive attacked models with high confidence. This has given rise to significant security threats and raised doubts about the reliability of deploying deep learning models in security-critical domains. Therefore, effectively dealing with various adversarial examples has become an essential but challenging requirement. Adversarial example detection can predict the existence of adversarial examples in advance. However, existing detection methods are usually restricted to specific attacks, lacking generalization and decision-making bases. In this paper, we discover that the common characteristic of adversarial examples is to alter the semantic information recognized by models, which can effectively distinguish adversarial examples and provide interpretability . Based on this perspective, we propose a semantic graph matching (SeMatch) method to execute attack-agnostic adversarial example detection. SeMatch detects adversarial examples by comparing the constructed semantic graphs with the semantic graph prototypes of their predicted classes and further corrects their classification results . Experimental results demonstrate that SeMatch can effectively detect and classify adversarial examples in several attack settings, achieving an average detection and classification accuracy of 96.01\% and 89.71\%, respectively. When applied to unknown attack scenarios, SeMatch is more effective and interpretable than state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Yuxin Gong and Shen Wang and Xunzhi Jiang and Liyao Yin and Fanghui Sun},
  doi          = {10.1016/j.asoc.2023.110317},
  journal      = {Applied Soft Computing},
  pages        = {110317},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial example detection using semantic graph matching},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic segmentation based on deep learning for the
detection of cyanobacterial harmful algal blooms (CyanoHABs) using
synthetic images. <em>ASOC</em>, <em>141</em>, 110315. (<a
href="https://doi.org/10.1016/j.asoc.2023.110315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyanobacterial Harmful Algal Blooms (CyanoHABs) in lakes and reservoirs have increased substantially in recent decades due to different environmental factors. Its early detection is a crucial issue to minimize health effects, particularly in potential drinking and recreational water bodies. The use of Autonomous Surface Vehicles (ASVs) equipped with machine vision systems (cameras) onboard, represents a useful alternative at this time. In this regard, we propose an image Semantic Segmentation approach based on Deep Learning with Convolutional Neural Networks (CNNs) for the early detection of CyanoHABs considering an ASV perspective. The use of these models is justified by the fact that with their convolutional architecture, it is possible to capture both, spectral and textural information considering the context of a pixel and its neighbors. To train these models it is necessary to have data, but the acquisition of real images is a difficult task, due to the capricious appearance of the algae on water surfaces sporadically and intermittently over time and after long periods of time, requiring even years and the permanent installation of the image capture system. This justifies the generation of synthetic data so that sufficiently trained models are required to detect CyanoHABs patches when they emerge on the water surface. The data generation for training and the use of the semantic segmentation models to capture contextual information determine the need for the proposal, as well as its novelty and contribution. Three datasets of images containing CyanoHABs patches are generated: (a) the first contains real patches of CyanoHABs as foreground and images of lakes and reservoirs as background, but with a limited number of examples; (b) the second, contains synthetic patches of CyanoHABs generated with state-of-the-art Style-based Generative Adversarial Network Adaptive Discriminator Augmentation (StyleGAN2-ADA) and Neural Style Transfer as foreground and images of lakes and reservoirs as background, and (c) the third set, is the combination of the previous two. Four model architectures for semantic segmentation (UNet++, FPN, PSPNet , and DeepLabV3+), with two encoders as backbone (ResNet50 and EfficientNet-b6), are evaluated from each dataset on real test images and different distributions. The results show the feasibility of the approach and that the UNet++ model with EfficientNet-b6, trained on the third dataset, achieves good generalization and performance for the real test images.},
  archive      = {J_ASOC},
  author       = {Fredy Barrientos-Espillco and Esther Gascó and Clara I. López-González and María J. Gómez-Silva and Gonzalo Pajares},
  doi          = {10.1016/j.asoc.2023.110315},
  journal      = {Applied Soft Computing},
  pages        = {110315},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semantic segmentation based on deep learning for the detection of cyanobacterial harmful algal blooms (CyanoHABs) using synthetic images},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CFNet: Facial expression recognition via constraint fusion
under multi-task joint learning network. <em>ASOC</em>, <em>141</em>,
110312. (<a href="https://doi.org/10.1016/j.asoc.2023.110312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In facial expression recognition (FER), global and local features obtained from the same face may have different recognition accuracy, indicating that they have different advantages in recognition. Existing FER works usually focus on extracting and fusing global and local features to obtain better recognition results. Instead of evaluating the advantages of global and local features before fusion, these methods default to fusing them in the same proportion, which probably leads to mutual suppression of information representation between the two features, and then causes worse recognition ability and scene adaptability. To overcome this weakness, this paper proposes a multi-task joint learning network with a constraint fusion (called CFNet). To leverage the key features extracted from different tasks, CFNet adopts a multi-loss mechanism and a constraint fusion method to automatically assign corresponding weights based on the importance of global and local facial information. Compared with existing models that employ the direct fusion strategy, CFNet has better adaptability for FER in complex scenes. Extensive evaluations show the superior effectiveness of CFNet over state-of-the-art methods on real-world emotion datasets. Specifically, the accuracy scores of CFNet on CK＋, MMI, and RAF-DB datasets are 99.07\%, 84.62\%, and 87.52\%, respectively. The robustness of CFNet is also verified in noisy and blurred scenes.},
  archive      = {J_ASOC},
  author       = {Junhao Xiao and Chenquan Gan and Qingyi Zhu and Ye Zhu and Gang Liu},
  doi          = {10.1016/j.asoc.2023.110312},
  journal      = {Applied Soft Computing},
  pages        = {110312},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CFNet: Facial expression recognition via constraint fusion under multi-task joint learning network},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic dual-population co-evolution multi-objective
evolutionary algorithm for constrained multi-objective optimization
problems. <em>ASOC</em>, <em>141</em>, 110311. (<a
href="https://doi.org/10.1016/j.asoc.2023.110311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many multi-objective evolutionary algorithms are proposed to handle constrained multi-objective optimization problems . Nevertheless, they often fail to appropriately balance feasibility, convergence and diversity of the population. This paper proposed a dynamic dual-population co-evolution multi-objective evolutionary algorithm (DDCMEA) to solve this issue. In DDCMEA, a dynamic dual-population co-evolution strategy is employed to balance the convergence and the feasibility by dynamically adjusting the offspring number of the two populations. In the early stage of evolution, the algorithm mainly focuses on the convergence and more offspring of the first population are generated. In the late stage of evolution, the algorithm mainly focuses on the feasibility and more offspring of the second population are generated. Finally, feasible solutions with good convergence could be obtained. To further enhance the diversity of the offspring and obtain feasible solutions with a wide spread of distribution, the evolution operators of the genetic algorithm and the differential evolution are chosen as the search engines for the first population and the second population, respectively. The performance of DDCMEA is further tested through thirty-one bench-mark test problems and two real-world problems in comparison with other five state-of-the-art algorithms. The results show the proposed algorithm DDCMEA achieves competitive performance when handling constrained multi-objective optimization problems .},
  archive      = {J_ASOC},
  author       = {Xiangsong Kong and Yongkuan Yang and Zhisheng Lv and Jing Zhao and Rong Fu},
  doi          = {10.1016/j.asoc.2023.110311},
  journal      = {Applied Soft Computing},
  pages        = {110311},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic dual-population co-evolution multi-objective evolutionary algorithm for constrained multi-objective optimization problems},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An optimized deep nonlinear integrated framework for wind
speed forecasting and uncertainty analysis. <em>ASOC</em>, <em>141</em>,
110310. (<a href="https://doi.org/10.1016/j.asoc.2023.110310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate wind speed prediction improves the efficiency of electricity and also increases the economic benefits of wind farms . However, the previous studies do not fully consider the chronological feature of wind speed, and do not properly identify and use the characteristic decomposed components of wind speed series in different time periods. The phenomenon of insufficient and excessive decomposition of components weakens the prediction accuracy of the model and destroys the power generation plan of the wind farm. In this paper, a segmented multi-modal deep learning integrated model based on periodicity is proposed. Firstly, the time series is reconstructed into matrix according to date; the matrix is divided into different segment matrices according to chronological characteristics; each sub-matrix is spliced into sub-sequence according to chronological order . Secondly, the contribution extremum method is used to determine the optimal feature components of all sub-sequences. Thirdly, the optimized deep learning model integrates all sub-sequences and then obtains the prediction results of the current date. Finally, the uncertainty analysis of the predicted value further improves the reliability of wind speed prediction. The data of wind farm in Hexi corridor area of China are used to simulate the experiment, and the results show that the proposed model has good performance.},
  archive      = {J_ASOC},
  author       = {Jujie Wang and Dongming Gao and Zhenzhen Zhuang},
  doi          = {10.1016/j.asoc.2023.110310},
  journal      = {Applied Soft Computing},
  pages        = {110310},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized deep nonlinear integrated framework for wind speed forecasting and uncertainty analysis},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classical simulation of variational quantum classifiers
using tensor rings. <em>ASOC</em>, <em>141</em>, 110308. (<a
href="https://doi.org/10.1016/j.asoc.2023.110308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, Variational Quantum Circuits (VQC) have been widely adopted to different tasks in machine learning such as Combinatorial Optimization and Supervised Learning. With the growing interest, it is pertinent to study the boundaries of the classical simulation of VQCs to effectively benchmark the algorithms. Classically simulating VQCs can also provide the quantum algorithms with a better initialization reducing the amount of quantum resources needed to train the algorithm. Even though Matrix Product State representations have been extensively used for quantum state approximation , their capacity is limited in simulating quantum circuits due to the exponential complexity in circuit depth. This manuscript proposes an algorithm that compresses the quantum state within a circuit using a noisy tensor ring representation which allows for the implementation of VQC based algorithms on a classical simulator at a fraction of the usual storage and computational complexity . Using the tensor ring approximation of the input quantum state, we propose a method that applies the parametrized unitary operations while retaining the low-rank structure of the tensor ring corresponding to the transformed quantum state, providing an exponential improvement of storage and computational time in the number of qubits and layers. This approximation is used to implement the tensor ring VQC (TRVQC) for the task of supervised learning on Iris and MNIST datasets to demonstrate the performance of the proposed method compared with the implementations from classical simulator using Matrix Product States (MPS). TRVQC has a test accuracy of 82.63\% compared to the benchmark of 83.68\% on Iris dataset whereas the former outperforms the latter on a reduced MNIST dataset with TRVQC having an accuracy of 83.73\% compared to the benchmark 81.02\%, showcasing the comparable performance of the proposed algorithm with the MPS framework.},
  archive      = {J_ASOC},
  author       = {Dheeraj Peddireddy and Vipul Bansal and Vaneet Aggarwal},
  doi          = {10.1016/j.asoc.2023.110308},
  journal      = {Applied Soft Computing},
  pages        = {110308},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classical simulation of variational quantum classifiers using tensor rings},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum angle encoding with learnable rotation applied to
quantum–classical convolutional neural networks. <em>ASOC</em>,
<em>141</em>, 110307. (<a
href="https://doi.org/10.1016/j.asoc.2023.110307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum Machine Learning (QML) has experienced rapid progress in recent years due to the development of Noisy Intermediate-Scale Quantum (NISQ) devices and quantum simulators. Two key elements must be minimized To maintain acceptable computational complexity in QML: the number of qubits required to encode classical data and the number of quantum gates . This paper proposes a novel angle encoding with learnable rotation to drastically reduce the qubits and circuit depth from O ( N ) O(N) to O ( ⌈ log 2 ( N ) ⌉ ) O(⌈log2(N)⌉) qubits, and only N N parameterized gates, where N N is the input size. Additionally, an extended quantum convolutional layer is introduced with multiple quantum circuits (quantum kernel) that allow for the configuration of any arbitrary size, stride, and dilation analogous to a classical convolutional layer . The proposed quantum convolutional layer learns multiple feature maps with a single quantum kernel while reducing computational cost by employing angle encoding with learnable rotation. Extensive experiments were performed by comparing diverse types of quantum convolutional configurations in a Quantum Convolutional Neural Network (QCNN) over a balanced subset of the MNIST and Fashion-MNIST datasets, achieving an accuracy of 0.90 and 0.7850, respectively.},
  archive      = {J_ASOC},
  author       = {Emmanuel Ovalle-Magallanes and Dora E. Alvarado-Carrillo and Juan Gabriel Avina-Cervantes and Ivan Cruz-Aceves and Jose Ruiz-Pinales},
  doi          = {10.1016/j.asoc.2023.110307},
  journal      = {Applied Soft Computing},
  pages        = {110307},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum angle encoding with learnable rotation applied to quantum–classical convolutional neural networks},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A chaotic local search-based LSHADE with enhanced memory
storage mechanism for wind farm layout optimization. <em>ASOC</em>,
<em>141</em>, 110306. (<a
href="https://doi.org/10.1016/j.asoc.2023.110306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The search for clean energy alternatives to fossil fuels has been a major effort by researchers all over the world. Wind energy is one of the most optimal choices because of its cleanliness and renewability. However, the existence of the wake effect leads to a decrease in conversion efficiency. Finding the best wind turbine layout has become an important factor in the wind power generation system. Inspired by the excellent optimization capability of meta-heuristic algorithms, they are increasingly applied to solve complex constraints and design objectives in the wind farm layout optimization problems . It is reported that LSHADE, which is an advanced variant of differential evolution, provides a more efficient configuration of wind turbines than other meta-heuristic algorithms. This motivates us to conduct research in this direction and design an effective meta-heuristic algorithm with a chaotic local search strategy and an enhanced memory storage mechanism, which contributes to the reduction of global carbon emissions. The proposed new algorithm is called CLSHADE. The validity of the proposed algorithm is verified by the simulation of different constraints and wind field distribution profiles. Compared to four state-of-the-art meta-heuristic algorithms, the average conversion rate of the proposed algorithm is 92.87\%, 89.13\%, and 96.86\% for three wind distribution profiles, respectively. The results show that the proposed algorithm has superiorities and effectiveness in wind farm layout optimization.},
  archive      = {J_ASOC},
  author       = {Yang Yu and Tengfei Zhang and Zhenyu Lei and Yirui Wang and Haichuan Yang and Shangce Gao},
  doi          = {10.1016/j.asoc.2023.110306},
  journal      = {Applied Soft Computing},
  pages        = {110306},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A chaotic local search-based LSHADE with enhanced memory storage mechanism for wind farm layout optimization},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dueling deep q-network method for low-carbon traffic
signal control. <em>ASOC</em>, <em>141</em>, 110304. (<a
href="https://doi.org/10.1016/j.asoc.2023.110304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improper traffic signal control will lead to long delays for vehicles and produce massive carbon emissions. The vast vehicle exhaust emissions will pollute the environment and exacerbate the earth’s greenhouse effect. Intersection signal optimization tends to start from the traditional view of improving traffic efficiency but ignores the perspective of reducing vehicle carbon emissions. Under the framework of a deep reinforcement learning strategy, this study proposes a novel signal control method to minimize the carbon emissions of vehicles at the intersection. To associate with carbon emissions and signal control plans, the method employs the negative value of vehicle’s carbon dioxide emissions as the reward and takes the feature vectors at different time points in the two decision action intervals as the state features. The fully connected neural network , convolutional neural network , and long short-term memory network are respectively adopted to extract the state features of the decision-making period and compare their Q-value estimation effects. Through the SUMO simulation platform, the proposed signal control method is comprehensively evaluated and compared with different baseline models . It has been proved that the proposed signal control approach can not only directly reduce vehicle carbon emissions but also improve the operational efficiency of the intersection.},
  archive      = {J_ASOC},
  author       = {Leilei Kang and Hao Huang and Weike Lu and Lan Liu},
  doi          = {10.1016/j.asoc.2023.110304},
  journal      = {Applied Soft Computing},
  pages        = {110304},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dueling deep Q-network method for low-carbon traffic signal control},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical clustering: Visualization, feature importance
and model selection. <em>ASOC</em>, <em>141</em>, 110303. (<a
href="https://doi.org/10.1016/j.asoc.2023.110303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose methods for the analysis of hierarchical clustering that fully use the multi-resolution structure provided by a dendrogram. Specifically, we propose a loss for choosing between clustering methods , a feature importance score and a graphical tool for visualizing the segmentation of features in a dendrogram. Current approaches to these tasks lead to loss of information since they require the user to generate a single partition of the instances by cutting the dendrogram at a specified level. Our proposed methods, instead, use the full structure of the dendrogram. The key insight behind the proposed methods is to view a dendrogram as a phylogeny. This analogy permits the assignment of a feature value to each internal node of a tree through an evolutionary model. Real and simulated datasets provide evidence that our proposed framework has desirable outcomes and gives more insights than state-of-art approaches. We provide an R package that implements our methods.},
  archive      = {J_ASOC},
  author       = {Luben M.C. Cabezas and Rafael Izbicki and Rafael B. Stern},
  doi          = {10.1016/j.asoc.2023.110303},
  journal      = {Applied Soft Computing},
  pages        = {110303},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical clustering: Visualization, feature importance and model selection},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An embedded hamiltonian dynamic evolutionary neural network
model for high-dimensional data recognition. <em>ASOC</em>,
<em>141</em>, 110299. (<a
href="https://doi.org/10.1016/j.asoc.2023.110299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the limitation of Hamiltonian neural network in high-dimensional data processing with non-observable physical quantity system, an embedded Hamiltonian dynamic evolutionary neural network model (EHDN) is proposed in this paper. First, the dynamic physical quantities of high dimensional nonlinear systems are represented by constructing Hamiltonian functions . Then a symplectic integrator is designed to realize the time-varying evolution of network features based on Hamiltonian regular equation and variational numerical integration. Finally, the Hamiltonian dynamic evolutionary neural network is embedded in the convolutional neural network (CNN) to realize the Hamiltonian simulation evolution of convolutional features. Experimental results on two different datasets, CIFAR and Fashion-Mnist, show that compared with existing state-of-the-art (SOTA) methods, EHDN model can combine the advantages of Hamiltonian dynamics and convolutional network to improve the recognition accuracy and training stability in high-dimensional image classification tasks.},
  archive      = {J_ASOC},
  author       = {Kui Qian and Lei Tian},
  doi          = {10.1016/j.asoc.2023.110299},
  journal      = {Applied Soft Computing},
  pages        = {110299},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An embedded hamiltonian dynamic evolutionary neural network model for high-dimensional data recognition},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamical decomposition and selection based evolutionary
algorithm for many-objective optimization. <em>ASOC</em>, <em>141</em>,
110295. (<a href="https://doi.org/10.1016/j.asoc.2023.110295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposition-based many-objective evolutionary algorithms decompose the objective space into multiple subregions , with the help of a set of predefined reference vectors. These vectors serve to guide coevolution between subproblems , and while they show potential for maintaining the diversity of solutions, they have limited exploration capabilities in complex problems and high-dimensional objective space. The main issue is that the predefined reference vectors cannot maintain uniformity of the intersection points between search directions and the irregular Pareto front (PF). To address this problem, this paper proposes a dynamical decomposition and selection strategy (DDS). In DDS, the predefined reference vectors are replaced by solutions themselves and normal-boundary directions (NBI), which guide the population to automatically adapt to the shape of PF. The subregions adapt to divide the objective space to increase diversity. Then, to adjust the relationship between diversity and convergence, a dynamical selection strategy based on the course of evolution is proposed. The process of dynamical decomposition and selection strategy is repeated until the termination condition is met. The proposed algorithm is compared with the state-of-the-art many-objective optimization algorithms on several benchmark problems with 5 to 15 objectives in evolutionary computation. Experimental results show that it outperforms other algorithms in most test instances and is less sensitive to irregular PFs.},
  archive      = {J_ASOC},
  author       = {Qian Bao and Maocai Wang and Guangming Dai and Xiaoyu Chen and Zhiming Song},
  doi          = {10.1016/j.asoc.2023.110295},
  journal      = {Applied Soft Computing},
  pages        = {110295},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamical decomposition and selection based evolutionary algorithm for many-objective optimization},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic adaptive spatio-temporal graph neural network for
multi-node offshore wind speed forecasting. <em>ASOC</em>, <em>141</em>,
110294. (<a href="https://doi.org/10.1016/j.asoc.2023.110294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-node offshore wind speed forecasting is a challenging task due to the complex dynamic spatial dependencies and highly nonlinear temporal dynamics present in the ocean. As deep learning advances, graph neural networks (GNNs) have great potential to capture spatial dependencies in ocean meteorology. However, existing GNN models usually use predefined or learned static graphs. They lack the ability to model dynamic spatial associations , which can limit the performance of GNNs. In this paper, we propose a dynamic adaptive spatio-temporal graph neural network (DASTGN) that uses dynamic graph convolution (DGCN) to capture dynamic spatial dependencies in offshore wind speed data. Based on the assumption that not only long-term static associations but also short-term dynamic associations exist in the spatial domain and that the importance of these two associations is different, we propose a dynamic adaptive graph generation module to generate static and dynamic graphs to model these two associations. Meanwhile, a matrix fusion mechanism is proposed to fuse them into the optimal dynamic graph, which is fed into the DGCN module. We employ a temporal convolution module to capture the nonlinear temporal dependencies. Finally, the above modules are integrated into a dedicated spatio-temporal convolution module to predict wind speed. Extensive experiments on real wind speed datasets in Chinese seas showed that the DASTGN improved the performance of the optimal baseline model by 3.05\% and 3.69\% in terms of the MAE and RMSE , respectively. To demonstrate that the DASTGN can effectively model dynamic spatial associations , the generated graph structure is visualized and analyzed. Finally, we present policy implications aimed at enhancing the security of the power system .},
  archive      = {J_ASOC},
  author       = {Ziheng Gao and Zhuolin Li and Lingyu Xu and Jie Yu},
  doi          = {10.1016/j.asoc.2023.110294},
  journal      = {Applied Soft Computing},
  pages        = {110294},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic adaptive spatio-temporal graph neural network for multi-node offshore wind speed forecasting},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BrightBox — a rough set based technology for diagnosing
mistakes of machine learning models. <em>ASOC</em>, <em>141</em>,
110285. (<a href="https://doi.org/10.1016/j.asoc.2023.110285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a novel approach to investigating mistakes in machine learning model operations. The considered approach is the basis for BrightBox – a diagnostic technology that can be used for analyzing prediction models and identifying model- and data-related issues. The idea is to generate surrogate rough set-based models from data that approximate decisions made by monitored black-box models. Such approximators are used to compute neighborhoods of instances that undergo the diagnostic process — the neighborhoods consist of historical instances that were processed in a similar way by rough set-based models. The diagnostic process is then based on the analysis of mistakes registered in such neighborhoods. The experiments performed on real-world data sets confirm that such analysis can provide us with efficient and valid insights about the reasons for the poor performance of machine learning models.},
  archive      = {J_ASOC},
  author       = {Andrzej Janusz and Andżelika Zalewska and Łukasz Wawrowski and Piotr Biczyk and Jan Ludziejewski and Marek Sikora and Dominik Ślęzak},
  doi          = {10.1016/j.asoc.2023.110285},
  journal      = {Applied Soft Computing},
  pages        = {110285},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BrightBox — a rough set based technology for diagnosing mistakes of machine learning models},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering time series under trend-oriented fuzzy
information granulation. <em>ASOC</em>, <em>141</em>, 110284. (<a
href="https://doi.org/10.1016/j.asoc.2023.110284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a trend-oriented time series granulation method to transform a long numerical time series into a relatively short granular time series which is consist of a group of unequal-size linear fuzzy information granules (LFIG). The transformed granular time series not only captures the main characteristics like trends and fluctuations of the original time series, but also saves the amount of calculation in time series clustering. Inspired by the distance measure of two equal-size LFIGs and the dynamic time warping, this paper also defines the distance measures for two unequal-size LFIGs and two LFIG time series. Based on such distance measures, the k k -medoids method is employed to cluster the datasets coming from UCR time-series database. The clustering performance expressed in terms of computing time and the Rand Index demonstrates the effectiveness and advantages of the proposed time series granulation method and distance measurement. The main original aspects of this study concern the granular representation of time series with unequal-size granules, and the distance measurement of unequal-length granular time series.},
  archive      = {J_ASOC},
  author       = {Xiyang Yang and Fusheng Yu and Witold Pedrycz and Zhiwei Li},
  doi          = {10.1016/j.asoc.2023.110284},
  journal      = {Applied Soft Computing},
  pages        = {110284},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering time series under trend-oriented fuzzy information granulation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disaggregated retail forecasting: A gradient boosting
approach. <em>ASOC</em>, <em>141</em>, 110283. (<a
href="https://doi.org/10.1016/j.asoc.2023.110283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Demand forecast is a relevant topic for retailers to manage effectively inventories comprising a wide range of Stock Keeping Units (SKUs) at store level. While this disaggregated forecast problem is central to ensure profitability as it supports accurate inventory decisions, thus avoiding either out-of-stock events or overstocks and inventory losses, it is also very complex due to aspects such as the large number of stores and products of modern retailers, the complex marketing and promotional strategies that impact customer demand together with cross-product effects that are all difficult to model. In this study, we propose more effective methods to handle these aspects. More specifically, we employ XGBoost , a non-linear non-parametric ensemble-based model, as the central learning algorithm and a structural change correction method to account for sudden changes in consumer behavior caused by external factors. Our approach also encompasses data cleansing procedures to correct sales observations during out-of-stock days as well as discrepancies between logical and physical inventory counts. Based on real data from a public dataset of a large retailer, we show that our methods outperform the Base-Lift model, a widely used benchmark model for retail forecasting, yielding significant improvements in accuracy metrics together with reductions in stockouts and in stock on hand. The proposed approach has also a high degree of automation, an important requirement for modern retailers.},
  archive      = {J_ASOC},
  author       = {Luiz Augusto C.G. Andrade and Claudio B. Cunha},
  doi          = {10.1016/j.asoc.2023.110283},
  journal      = {Applied Soft Computing},
  pages        = {110283},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Disaggregated retail forecasting: A gradient boosting approach},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A decomposition-based multiobjective evolutionary algorithm
using simulated annealing for the ambulance dispatching and relocation
problem during COVID-19. <em>ASOC</em>, <em>141</em>, 110282. (<a
href="https://doi.org/10.1016/j.asoc.2023.110282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of the COVID-19 epidemic has had a significant impact in increasing the number of emergency calls, which causes significant problems to emergency medical services centers (EMS) in many countries around the world, such as Saudi Arabia, which attracts a huge number of pilgrims during pilgrimage seasons. Among these issues, we address real-time ambulance dispatching and relocation problems (real-time ADRP). This paper proposes an improved MOEA/D algorithm using Simulated Annealing (G-MOEA/D-SA) to handle the real-time ADRP issue. The simulated annealing (SA) seeks to obtain optimal routes for ambulances to cover all emergency COVID-19 calls through the implementation of convergence indicator based dominance relation (CDR). To prevent the loss of good solutions once they are found in the G-MOEA/D-SA algorithm, we employ an external archive population to store the non-dominated solutions using the epsilon dominance relationship. Several experiments are conducted on real data collected from Saudi Arabia during the Covid-19 pandemic to compare our algorithm with three relevant state-of-art algorithms including MOEA/D, MOEA/D-M2M and NSGA-II. Statistical analysis of the comparative results obtained using ANOVA and Wilcoxon test demonstrate the merits and the outperformance of our G-MOEA/D-SA algorithm.},
  archive      = {J_ASOC},
  author       = {Meriem Hemici and Djaafar Zouache and Boualem Brahmi and Adel Got and Habiba Drias},
  doi          = {10.1016/j.asoc.2023.110282},
  journal      = {Applied Soft Computing},
  pages        = {110282},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decomposition-based multiobjective evolutionary algorithm using simulated annealing for the ambulance dispatching and relocation problem during COVID-19},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inception-embedded attention memory fully-connected network
for short-term wind power prediction. <em>ASOC</em>, <em>141</em>,
110279. (<a href="https://doi.org/10.1016/j.asoc.2023.110279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for energy in the world today, wind energy has turned out to be an attractive alternative to traditional fossil energy sources because of the characteristics of being clean, non-polluting, and easily accessible. Reliably predicting wind power is vital to improving energy utilization and ensuring the stability of power system operation . However, because of the uncertainty and instability of wind energy, accurately predicting wind power is still challenging. Therefore, this study proposes an Inception-embedded attention memory fully-connected network short-term wind power prediction model, incorporating improved attention mechanisms . As a result, the Inception-embedded attention memory fully-connected network can give reliable wind power predictions. This study utilizes a dataset of about 400 days from Natal and compares the Inception-embedded attention memory fully-connected network with 23 algorithms including EffiientNet, NasNet, and ResNet . The comparison results show that the Inception-embedded attention memory fully-connected network obtains reliable wind power prediction one day ahead and outperforms all other compared algorithms by more than 40\% in all evaluation metrics .},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Mingshan Zhao},
  doi          = {10.1016/j.asoc.2023.110279},
  journal      = {Applied Soft Computing},
  pages        = {110279},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inception-embedded attention memory fully-connected network for short-term wind power prediction},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-aware JPEG image compression: A multi-objective
approach. <em>ASOC</em>, <em>141</em>, 110278. (<a
href="https://doi.org/10.1016/j.asoc.2023.110278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer satisfaction is crucially affected by energy consumption in mobile devices . One of the most energy-consuming parts of an application is images. This paper, first, investigates that there is a correlation between energy consumption and image quality as well as image file size. Therefore, these two can be considered as a proxy for energy consumption. In the next step, we focused on proposing a multi-objective strategy to enhance image quality and reduce image file size based on the quantisation table (QT) in JPEG image compression . To this end, we have used two general multi-objective approaches: scalarisation and Pareto-based. In this paper, we embed our strategy into five scalarisation algorithms, including energy-aware multi-objective genetic algorithm (EnMOGA), energy-aware multi-objective particle swarm optimisation (EnMOPSO), energy-aware multi-objective differential evolution (EnMODE), energy-aware multi-objective evolutionary strategy (EnMOES), and energy-aware multi-objective pattern search (EnMOPS). Also, two Pareto-based methods, including a non-dominated sorting genetic algorithm (NSGA-II) and a reference-point-based NSGA-II (NSGA-III) are used for the embedding scheme, and two Pareto-based algorithms, EnNSGAII and EnNSGAIII, are presented. With our proposed scalarisation method, user’s preferences can be set before starting the optimisation process and the algorithm generates only one solution based on the preference, while our Pareto-based approaches generate a set of solutions so that a user can select one of the preferred solutions after the optimisation process. Experimental studies show that the performance of the baseline algorithm is improved by embedding the proposed strategy into metaheuristic algorithms . In particular, EnMOGA, EnMOPS, and EnNSGA-II can perform competitively, among others. From the results, the baseline algorithm in all cases and in comparison to all algorithms yields the worst results. Among the scalarisation methods, EnMOGA and EnMOPS can achieve the first rank in 6 and 7 out of 13 cases and the second rank in 7 and 5 cases in terms of objective function. Also, EnMOES achieved the fifth or worst rank among the scalarisation algorithms. Regarding the Pareto-based algorithms, the table shows that EnNSGAII outperforms EnNSGAIII in 10 out of 13 cases in terms of hyper-volume measure, while it fails in 3 cases. Furthermore, we statistically verify the proposed algorithm’s effectiveness based on the Wilcoxon-signed rank test. Finally, a sensitivity analysis of the parameters is provided. The source code for reproducing the results is available in: https://github.com/SeyedJalaleddinMousavirad/MultiobjectiveJPEGImageCompression .},
  archive      = {J_ASOC},
  author       = {Seyed Jalaleddin Mousavirad and Luís A. Alexandre},
  doi          = {10.1016/j.asoc.2023.110278},
  journal      = {Applied Soft Computing},
  pages        = {110278},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy-aware JPEG image compression: A multi-objective approach},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Understanding the selection of intelligent engineering B2B
platform in china through the fuzzy DANP and TOPSIS techniques: A
multi-study analysis. <em>ASOC</em>, <em>141</em>, 110277. (<a
href="https://doi.org/10.1016/j.asoc.2023.110277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the Internet’s popularity and the economy’s rapid development, e-commerce has entered a whole new era in China. This study aims to investigate the factors affecting intelligent engineering B2B platforms and develop an indicator system based on the theory of resource complementarity. In addition, this study used a combination of qualitative and quantitative research to calculate the weights. We used the Fuzzy Decision-Making Trial and Evaluation Laboratory and Analytic Network Process (fuzzy DANP) method to determine the weights of the dimensions and indicators. The Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) method ranks the well-known intelligent engineering B2B platforms in China. The results show that platform transaction volume, relationship stability and durability, and degree of dependence are key elements affecting the platforms. Finally, it is proposed that media and companies can prioritize two aspects of cooperation performance and relationship quality to improve the operational efficiency of both parties. Platforms should pay more attention to three indicators: relationship stability and durability, closeness, and platform traffic increase, to reasonably allocate resources, grasp the development opportunities, attract users to reside, and improve the economic benefits of the companies.},
  archive      = {J_ASOC},
  author       = {Zhichao Zhang and Hongbo Jiang and Ting Shao and Qigan Shao},
  doi          = {10.1016/j.asoc.2023.110277},
  journal      = {Applied Soft Computing},
  pages        = {110277},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Understanding the selection of intelligent engineering B2B platform in china through the fuzzy DANP and TOPSIS techniques: A multi-study analysis},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novelty detection for multi-label stream classification
under extreme verification latency. <em>ASOC</em>, <em>141</em>, 110265.
(<a href="https://doi.org/10.1016/j.asoc.2023.110265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Label Stream Classification (MLSC) is the classification streaming examples into multiple classes simultaneously. Since new classes may emerge during the streaming process (concept evolution) and known classes may change over time (concept drift) it is challenging task. In real situations, concept drift and concept evolution occur in scenarios where the actual labels of arriving examples are never available; hence it is impractical to update decision models in a supervised fashion. This is known as Extreme Verification Latency, a topic that has not been well investigated in MLSC literature. This paper proposes a new method called MultI-label learNing Algorithm for Data Streams with Binary Relevance transformation (MINAS-BR), integrated with a Novelty Detection (ND) procedure for detecting concept evolution and concept drift, updating the model in an unsupervised fashion. Furthermore, since the label space is not static, we propose a new evaluation methodology for MLSC under extreme verification latency. Experiments over synthetic and real-world data sets with different concept drift and concept evolution scenarios confirmed the strategies employed in the MINAS-BR and presented relevant advances for handling streaming multi-label data.},
  archive      = {J_ASOC},
  author       = {Joel D. Costa Júnior and Elaine R. Faria and Jonathan A. Silva and João Gama and Ricardo Cerri},
  doi          = {10.1016/j.asoc.2023.110265},
  journal      = {Applied Soft Computing},
  pages        = {110265},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Novelty detection for multi-label stream classification under extreme verification latency},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph based fuzzy clustering algorithm for crime report
labelling. <em>ASOC</em>, <em>141</em>, 110261. (<a
href="https://doi.org/10.1016/j.asoc.2023.110261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globalization influences the frequency and severity of localized traditional patterns of crime in addition to creating conditions for new types of crime. The scope and makeup of traditional crime have been significantly impacted by the world’s compression. This makes it impossible for law enforcement officials to examine crime reports for the purpose of conducting investigations and taking the necessary preventive measures. Clustering facilitates investigations by grouping crime reports into different categories based on the crime types. In the proposed work, the crime reports are preprocessed and embedded with the help of the BERT model to capture the contextual meaning of the reports. An undirected graph is constructed considering each crime report vector as a node where an edge exists between a pair of nodes only if the cosine similarity between them is more than a threshold value. The constructed graph is partitioned based on the concepts of node betweenness , edge connectivity, and two proposed concepts, namely, cut of a node, and safe node, introduced in the paper. Based on the edge connectivity of the graph, we decide whether the graph needs to be further partitioned or not. If edge connectivity is high, we consider the graph as a cluster; otherwise, we partition the graph with the help of a node, say v v , with the highest node betweenness . During partition, we find the cut of v v to determine whether v v is safe or not. If v v is safe, the algorithm provides two overlapping subgraphs of the graph; on the other hand, if it is not safe, then the algorithm provides a modified graph with a replica of node v v . The algorithm is iterative in nature and terminates when all subgraphs are of higher edge connectivity. As the generated subgraphs are overlapped in nature, a novel graph theory-based fuzzification technique is utilized to measure the membership value of each node in different subgraphs, which allows investigators to focus on the most serious crimes in the reports. The proposed method is assessed using crime report datasets as well as other text datasets and compared with several state-of-the-art methods by utilizing a variety of performance metrics to reflect the effectiveness of the method in different domains.},
  archive      = {J_ASOC},
  author       = {Aparna Pramanik and Asit Kumar Das and Weiping Ding},
  doi          = {10.1016/j.asoc.2023.110261},
  journal      = {Applied Soft Computing},
  pages        = {110261},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph based fuzzy clustering algorithm for crime report labelling},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiobjective evolutionary algorithm using
multi-ecological environment selection strategy. <em>ASOC</em>,
<em>141</em>, 110232. (<a
href="https://doi.org/10.1016/j.asoc.2023.110232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many-objective optimization problems (MaOPs), the conflict between convergence and diversity becomes more and more serious as the number of objectives increases. This paper proposes the evolutionary algorithm MeEA of multi-ecological environment selection strategy and uses this algorithm to solve MaOPs. Firstly, the objective space is divided into several different types of ecological environments. Secondly, the preference for convergence or diversity in the ecological environment is initially determined during environment selection and then the overall diversity maintenance of the population is ensured. Thirdly, the proposed algorithm is compared with five popular evolutionary algorithms on 44 multi-objective benchmark problems. Finally, it is applied to the optimization design of hydrodynamic lubrication radial sliding bearing of crane gearbox. Experimental results show that the performance of this algorithm is better than other algorithms in solving MaOPs.},
  archive      = {J_ASOC},
  author       = {Shuzhi Gao and Leiyu Yang and Yimin Zhang},
  doi          = {10.1016/j.asoc.2023.110232},
  journal      = {Applied Soft Computing},
  pages        = {110232},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multiobjective evolutionary algorithm using multi-ecological environment selection strategy},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auxiliary data structures and techniques to speed up solving
of the p-next center problem: A VNS heuristic. <em>ASOC</em>,
<em>140</em>, 110276. (<a
href="https://doi.org/10.1016/j.asoc.2023.110276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the p p -next center problem and propose an adequate solution approach. The p p -next center problem aims to minimizing the maximum distance from a user to the nearest center plus the distance between the center and its closest center. In this paper we propose a new Variable Neighborhood Search based algorithm to solve the p p -next center problem. It uses refined local search and shaking procedures as well as auxiliary data structures . The implementation consists in filtering out the candidate centers to enter a solution by considering only ones that potentially decrease the objective function value. The same approach has been applied to the classical p p -center problem. Here we show that known properties of an efficient implementation of VNS heuristic developed for the p p -center problem, hold for the new problem as well. More precisely, all the proposals in this work are inspired by other analogous ones used in the literature for similar problems. Hence, the novelty is the adaptation of the known properties that hold for the p p -center problem to the p p -next center problem. The performance of the proposed heuristic is assessed on the benchmark instances from the literature as well as newly generated larger instances with 1000, 1500, 2000 and 2500 vertices and instances defined over graphs up to 1000 vertices with different densities. The obtained results clearly demonstrate the effectiveness and efficiency of the proposed algorithm. Hence, the paper shows that the same observations used to solve p p -center problem may be used to efficiently solve the p p -next center problem.},
  archive      = {J_ASOC},
  author       = {Dalibor Ristić and Nenad Mladenović and Mustapha Ratli and Raca Todosijević and Dragan Urošević},
  doi          = {10.1016/j.asoc.2023.110276},
  journal      = {Applied Soft Computing},
  pages        = {110276},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Auxiliary data structures and techniques to speed up solving of the p-next center problem: A VNS heuristic},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extended belief rule-based system using bi-level joint
optimization for environmental investment forecasting. <em>ASOC</em>,
<em>140</em>, 110275. (<a
href="https://doi.org/10.1016/j.asoc.2023.110275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific investment forecasting can effectively avoid the blind investments of environmental management. Among existing studies in developing investment forecasting models, the extended belief rule-based system (EBRBS) showed its potential to accurately predict environment investments but also exposed two challenges to be further addressed: (1) how to select antecedent attributes from various environmental indicators for the EBRBS; (2) how to optimize basic parameters of the EBRBS based on the selected antecedent attributes. Since these two challenges are connected, a bi-level joint optimization model is proposed to improve the EBRBS for better environmental investment forecasting, in which the selection of antecedent attributes is described as an upper-level optimization model using Akaike information criterion (AIC) and the optimization of basic parameters is as a lower-level optimization model using mean absolute error (MAE). Moreover, a corresponding bi-level joint optimization algorithm is proposed to solve the bi-level joint optimization model, where ensemble feature selection and swarm intelligence optimization are regarded as the engine of upper-level and lower-level optimizations, respectively. The real environmental data collected from 2005 to 2020 of 30 Chinese provinces are studied to verify the effectiveness of the proposed approach. Experimental results show that the EBRBS with bi-level joint optimization not only can effectively predict environmental investments, but also is able to have desired accuracy better than previous investment forecasting models.},
  archive      = {J_ASOC},
  author       = {Long-Hao Yang and Fei-Fei Ye and Ying-Ming Wang and Yi-Xin Lan and Chan Li},
  doi          = {10.1016/j.asoc.2023.110275},
  journal      = {Applied Soft Computing},
  pages        = {110275},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extended belief rule-based system using bi-level joint optimization for environmental investment forecasting},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fppsv-NHSS: Fuzzy parameterized possibility single valued
neutrosophic hypersoft set to site selection for solid waste management.
<em>ASOC</em>, <em>140</em>, 110273. (<a
href="https://doi.org/10.1016/j.asoc.2023.110273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive increase in urban population has created a slew of socioeconomic and environmental issues. Among these problems, the most notable is the disposal of solid waste and the search for an effective system for it. Many scholars employing various fuzzy set-like methodologies have considered it a fuzzy multi-criteria or multi-attribute decision-making problem due to the involvement of criteria and anticipated uncertainty. The goal of the current study is to use an innovative methodology to tackle the expected uncertainties present in the solid waste site selection problem (SOWSSP). Such uncertainties may be seen in decision-makers’ choices of parameters (or subparameters) and the degree to which they accept approximations for various options. Therefore, a novel mathematical structure called fuzzy parameterized possibility single valued neutrosophic hypersoft set (Fppsv-NHSS) is characterized first and then integrated with modified Sanchez’s method to resolve the SOWSSP through the proposal of an intelligent algorithm. The steps of the proposed algorithm are explained with a self-explanatory example. The relationship between the suitability of solid waste management systems and sites is discussed, and their rankings are determined with rich descriptions of their feasibility. The preferential aspects of this study are that it is capable of managing uncertainties depicted by the nature of parameters and approximations of alternatives by using the concept of fuzzy parameterization and possibility grades respectively. It employs particular mathematical formulations to determine the fuzzy parameterized degrees and possibility grades that are missing in the existing literature. It facilitates the decision-makers to evaluate the alternatives independently, with the choice being indeterminate. With the help of comparison, the computed results are found to be consistent and reliable due to their preferential features.},
  archive      = {J_ASOC},
  author       = {Atiqe Ur Rahman and Muhammad Saeed and Mazin Abed Mohammed and Karrar Hameed Abdulkareem and Jan Nedoma and Radek Martinek},
  doi          = {10.1016/j.asoc.2023.110273},
  journal      = {Applied Soft Computing},
  pages        = {110273},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fppsv-NHSS: Fuzzy parameterized possibility single valued neutrosophic hypersoft set to site selection for solid waste management},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering of multi-layer networks with structural relations
and conservation of features. <em>ASOC</em>, <em>140</em>, 110272. (<a
href="https://doi.org/10.1016/j.asoc.2023.110272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-layer networks are omnipresent in nature and society, as complex systems often involve multiple types of relationships among entities. Compared with single-layer graphs, community detection is a challenging task that requires simultaneous characterization of the structure and relations across various layers. Current algorithms have been criticized for their suboptimal performance in describing and quantifying the structure of communities in multi-layer networks. A novel algorithm, called McNMF, is proposed to detect the conserved communities in multi-layer networks, where the shared and layer-specific features of vertices are learned simultaneously by utilizing joint nonnegative matrix factorization . Specifically, McNMF joins the feature learning , feature decomposition, and relation of the shared and layer-specific features into an overall objective function, thereby enhancing the quality of features. To learn the features of vertices, McNMF jointly factorizes matrices associated with multi-layer networks by projecting the topology into different basis spaces. It then decomposes the learned features into shared and layer-specific parts, enabling quantification of multi-layer network specificity at the feature level. Moreover, the relation between the shared and layer-specific features of vertices is measured using trace optimization, providing a comprehensive strategy to characterize the structure of conserved communities in multi-layer networks. Extensive experiments on artificial and real-world multi-layer networks demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of accuracy.},
  archive      = {J_ASOC},
  author       = {Wentao Jia and Xiaoke Ma},
  doi          = {10.1016/j.asoc.2023.110272},
  journal      = {Applied Soft Computing},
  pages        = {110272},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering of multi-layer networks with structural relations and conservation of features},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust color image watermarking algorithm based on
synchronization correction with multi-layer perceptron and cauchy
distribution model. <em>ASOC</em>, <em>140</em>, 110271. (<a
href="https://doi.org/10.1016/j.asoc.2023.110271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust color image watermarking algorithm based on the multi-layer Perceptron (MLP) and Cauchy distribution model is proposed. The algorithm has great performance against geometric attacks . Firstly, the central region of the Y-color channel is extracted, and the watermark is embedded into the low-frequency component by dual-tree complex wavelet transform (DTCWT). A synchronization correction technique based on the MLP model is used prior to the watermark extraction . When the MLP model is trained, the position and scale parameters of the Cauchy distribution are employed to generate the image feature vector after first calculating the quaternion discrete cosine transform (QDCT) of the original color host image. The new 3D hyperchaotic mapping is used during the algorithmic process to increase security, which is employed for scrambling, encryption, and embedding of watermarks, respectively. The experimental results demonstrate that the proposed algorithm has good imperceptibility , accurate model prediction, and excellent robustness against common image processing attacks and geometric attacks .},
  archive      = {J_ASOC},
  author       = {Mei-ru Jiang and Xiu-fang Feng and Chun-peng Wang and Xiao-le Fan and Hao Zhang},
  doi          = {10.1016/j.asoc.2023.110271},
  journal      = {Applied Soft Computing},
  pages        = {110271},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust color image watermarking algorithm based on synchronization correction with multi-layer perceptron and cauchy distribution model},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lévy–cauchy arithmetic optimization algorithm combined with
rough k-means for image segmentation. <em>ASOC</em>, <em>140</em>,
110268. (<a href="https://doi.org/10.1016/j.asoc.2023.110268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough K-Means (RKM) is a well-known unsupervised clustering algorithm based on rough set logic that is utilized in a wide range of applications. However, when dealing with complex problems like image segmentation , it is frequently trapped in local optima during execution, resulting in undesirable segmentation results. To handle the issue in a realistic computing time, this study develops a Lévy–Cauchy Arithmetic Optimization Algorithm (LCAOA), an enhanced form of AOA, for performing rough clustering. The Levy flight and Cauchy distribution help in exploration and exploitation, respectively, in the proposed LCAOA. Therefore, well-balanced exploration and exploitation have been incorporated into LCAOA, which is a major problem of classical AOA. Opposition-based learning is also incorporated into LCAOA to maintain an efficient population during the optimization process. As the segmentation efficacy is somewhat dependent on the selection of color spaces caused by the non-illumination of regions, the suggested method employs the CIELab color space. The suggested method is compared to conventional and Nature-Inspired Optimization Algorithms (NIOA)-based state-of-the-art image segmentation techniques over traditional color images, color pathology images, and leaf images. The proposed clustering methodology outperforms all other examined clustering algorithms, according to the results of the experiments. For example, proposed LCAOA-based rough clustering gives average Feature Similarity Index (FSIM) values of 0.9513, 0.9688, and 0.9769 for traditional color images with 4, 6, and 8 clusters, respectively. The proposed technique is associated with an average FSIM value of 0.9525 for cluster number 2 in images of oral pathology. Lastly, for leaf images, the proposed approach yields a mean FSIM value of 0.9759 with an accuracy of greater than 97\% for cluster number 2.},
  archive      = {J_ASOC},
  author       = {Arunita Das and Amrita Namtirtha and Animesh Dutta},
  doi          = {10.1016/j.asoc.2023.110268},
  journal      = {Applied Soft Computing},
  pages        = {110268},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lévy–Cauchy arithmetic optimization algorithm combined with rough K-means for image segmentation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical parallel multi-scale graph network for 3d human
pose estimation. <em>ASOC</em>, <em>140</em>, 110267. (<a
href="https://doi.org/10.1016/j.asoc.2023.110267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network (GCN) is a widespread architecture for 2D-to-3D human pose estimation (HPE). Vanilla graph convolution is the key in GCN for body joints feature extraction by aggregating features from the first-order neighbors of each joint at single-scale. However, the nodes features updated in graph are gradually assimilated with the network deepens This makes it difficult to model the powerful feature representation for joints and has an adverse effect on resolving the uncertainty caused by occlusion or depth ambiguity. To address these problems, we propose a hierarchical parallel multi-scale graph convolutional network (HPM-GNet) for 3D HPE in this paper. The proposed network is composed by multi-scale sub-graph networks in parallel framework, guided by the geometric constraint of human body. Firstly, a well-designed weight masked graph convolutional (WMGConv) layer is used as a fundamental unit to construct the parallel multi-scale sub-graph convolutional network module (PMGCN) in HPM-GNet, which corporately captures the features of target nodes and neighbor nodes with weight assignment . Then, a cross-scale features exchange fusion block (CFEB) is designed to aggregate multi-scale features from local and global perspective with considering the geometric information from different parts of human body. Finally, we conduct experiments on two challenging 3D human pose benchmark datasets to evaluate the effectiveness of the proposed model. The experimental results demonstrate that our model achieves state-of-the-art performance.},
  archive      = {J_ASOC},
  author       = {Honghong Yang and Hongxi Liu and Yumei Zhang and Xiaojun Wu},
  doi          = {10.1016/j.asoc.2023.110267},
  journal      = {Applied Soft Computing},
  pages        = {110267},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical parallel multi-scale graph network for 3d human pose estimation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geospatial data-driven assessment of earthquake-induced
liquefaction impact mapping using classifier and cluster ensembles.
<em>ASOC</em>, <em>140</em>, 110266. (<a
href="https://doi.org/10.1016/j.asoc.2023.110266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 5.4 M L ML earthquake occurred on November 15, 2017, in Pohang, South Korea. This earthquake was the second largest recorded earthquake in South Korea and had detrimental effects on the ground and infrastructure. Among all the ground deformations, hundreds of liquefaction-induced sand boils and ground failures observed near the epicenter were major issues. However, whether subsurface characteristics and liquefaction vulnerability indices trigger regional liquefaction manifestations and how local liquefaction occurs as a consequence remains elusive. In this study, we present a novel data-driven model for the analysis of site-specific liquefaction triggering that considers the spatial uncertainties of principal liquefaction vulnerability indices. This is achieved by establishing an advanced artificial intelligence technology that assembles optimization-oriented, supervised, and unsupervised machine-learning models. The phased decision-making process could develop unified liquefaction hazard zonation based on the clustering ensemble methodology and help identify feasible liquefaction impact mapping procedures via the optimized classification of their performance evaluation with liquefaction inventory. The alternative three-phase approach, depending on the feasibility of geo-data and geospatial modeling, consists of three zonation methods (macro-, micro-, and nano-zonation) based on a 3D grid network, which assigns the best-fitting machine-learning model. The resulting liquefaction impact map, which has a high resolution and is assigned nano-zonation-based clustered liquefaction indices, can assist in site-specific decision-making to zonate liquefaction-induced ground displacement.},
  archive      = {J_ASOC},
  author       = {Han-Saem Kim},
  doi          = {10.1016/j.asoc.2023.110266},
  journal      = {Applied Soft Computing},
  pages        = {110266},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Geospatial data-driven assessment of earthquake-induced liquefaction impact mapping using classifier and cluster ensembles},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Outlier aware differential evolution for multimodal
optimization problems. <em>ASOC</em>, <em>140</em>, 110264. (<a
href="https://doi.org/10.1016/j.asoc.2023.110264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal optimization problems (MMOPs) target to locate multiple global optima simultaneously, which mainly face two challenges: how to maximize the number of global peaks and how to enhance the accuracy of the found solutions. To deal with these two challenges, an outlier aware differential evolution (OADE) algorithm is proposed in this paper, which includes three novel mechanisms. Firstly, a dimension and guidance-balanced mutation (DGM) strategy is proposed to improve the accuracy of solutions by balancing the information of individuals, niching, and population. Secondly, an outlier-based selection (OBS) strategy is designed to increase the population diversity and further to locate as many peaks as possible, which combines the fitness information and the distribution information of individuals. Thirdly, an inactive outlier-based re-initialization (IOR) strategy is proposed to enable the inactive outliers to jump out of local optima when dealing with high-dimensional MMOPs. The performance of OADE is tested on 20 widely used multimodal benchmarks. The experimental results show that the proposed OADE generally has better or competitive performance compared with some well-performing and state-of-the-art multimodal optimization algorithms .},
  archive      = {J_ASOC},
  author       = {Hong Zhao and Zhi-Hui Zhan and Jing Liu},
  doi          = {10.1016/j.asoc.2023.110264},
  journal      = {Applied Soft Computing},
  pages        = {110264},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Outlier aware differential evolution for multimodal optimization problems},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk assessment model with probabilistic linguistic fuzzy
inference methods for maritime piracy crime and applications.
<em>ASOC</em>, <em>140</em>, 110262. (<a
href="https://doi.org/10.1016/j.asoc.2023.110262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in maritime piracy is becoming a deterrent to international seaborne trade. More attention should be attached to piracy risk analysis and prevention. The complex environments engendering piracy crime make it more difficult to conduct a quantitative judgment. Most of the existing research focuses on the risk assessment of piracy based on historical piracy activities. The dynamic evolution of piracy is not explored in these evaluation models. To solve the piracy evolution pattern analysis and risk control in the uncertain environment, we first propose a novel probabilistic linguistic Markov model to analyze and predict the risk states of piracy accidents. The basic definition of random states and the Markov process in the probabilistic linguistic environment are discussed. Next, we propose the probabilistic linguistic Bayesian network which not only depicts the causal relationships of the risk factors but also provides an efficient tool to calculate the transition probabilities in the Markov model. Then the comprehensive inference risk analysis methods are developed to conduct the risk assessment in the maritime piracy crime. The assessment results can effectively support the decision-making of risk control. A practical case study is conducted to validate the above methods. Sensitivity analyses and method comparisons also illustrate the performance of the proposed risk analysis model.},
  archive      = {J_ASOC},
  author       = {Zhinan Hao and Zeshui Xu and Hua Zhao and Lou Yang},
  doi          = {10.1016/j.asoc.2023.110262},
  journal      = {Applied Soft Computing},
  pages        = {110262},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Risk assessment model with probabilistic linguistic fuzzy inference methods for maritime piracy crime and applications},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Letter: Application of optimization algorithms to
engineering design problems and discrepancies in mathematical formulas.
<em>ASOC</em>, <em>140</em>, 110252. (<a
href="https://doi.org/10.1016/j.asoc.2023.110252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engineering design optimization problems have attracted the attention of researchers since they appeared. Those who work on developing optimization algorithms , in particular, apply their developed algorithms to these problems in order to test their new algorithms’ capabilities. The mathematical discrepancy emerges during the implementation of equations and constraints related to these engineering problems. This is due to an error occurring in writing or transmitting these equations from one paper to another. Maintaining these discrepancies will have a negative impact on the assessment and model performance verification of the newly developed algorithms, as well as the decision-making process. To address this issue, this study investigates the mathematical discrepancies occurred by researchers in four well-known engineering design optimization problems (Welded Beam Design WBD, Speed Reducer Design SRD, Cantilever Beam Design CBD, and Multiple Disk Clutch Brake Design MDCBD). We have investigated some of the recently published papers in the literature, identifying discrepancies in their mathematical formulas, and fixing them appropriately by referring and comparing them to the original problem. Furthermore, all mathematical discrepancies , references, parameters, cost functions, constraints, and constraint errors are highlighted, arranged and organized in tables. As a result, this work can help readers and researchers avoid being confused and wasting time when working on these engineering design optimization problems.},
  archive      = {J_ASOC},
  author       = {Adel Sabry Eesa and Masoud Muhammed Hassan and Wahab Khalaf Arabo},
  doi          = {10.1016/j.asoc.2023.110252},
  journal      = {Applied Soft Computing},
  pages        = {110252},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Letter: Application of optimization algorithms to engineering design problems and discrepancies in mathematical formulas},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A collaborative central domain adaptation approach with
multi-order graph embedding for bearing fault diagnosis under few-shot
samples. <em>ASOC</em>, <em>140</em>, 110243. (<a
href="https://doi.org/10.1016/j.asoc.2023.110243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective fault diagnosis is a prerequisite for ensuring the safe, stable and long-term operation of many rotating machinery . With the rapid development of measurement, sensor and computing technologies, measurement data presents a high-dimensional and massive distribution. This makes the valuable fault information in samples sparse. Moreover, industrial data can only present the distribution state of few-shot unlabeled information. In addition, the vibration signal of bearing faults contains noise interference, leading to poor stability and low efficiency of most models. In this study, we propose an approach for rolling bearing faults diagnosis under few-shot samples. It consists of a multi-order graph embedding stacked denoising auto encoder optimized by an improved sine–cosine​ algorithm (MGE-ISCA-SDAE) and a collaborative central domain adaptation (CCDA). First, a multi-order graph embedding model and an ISCA-based strategy are designed to improve the SDAE, thereby improving the feature extraction effect. To overcome the sparseness of valuable information, we design a CCDA model that learns the fault features using the labeled samples. Subsequently, it is transferred to the target domain of few-shot labeled samples for adaptation. Finally, the intelligent diagnosis is achieved under few-shot samples. We conduct experiments with four datasets. The results show that the MGE-ISCA-SDAE can extract the time–frequency high-level fault features. The CCDA model can transfer the fault samples well. When there are fewer fault samples, our approach has outstanding advantages.},
  archive      = {J_ASOC},
  author       = {Wengang Ma and Ruiqi Liu and Jin Guo and Zicheng Wang and Liang Ma},
  doi          = {10.1016/j.asoc.2023.110243},
  journal      = {Applied Soft Computing},
  pages        = {110243},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A collaborative central domain adaptation approach with multi-order graph embedding for bearing fault diagnosis under few-shot samples},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Protein secondary structure prediction using cascaded
feature learning model. <em>ASOC</em>, <em>140</em>, 110242. (<a
href="https://doi.org/10.1016/j.asoc.2023.110242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protein secondary structure prediction (PSSP) is pivotal for predicting tertiary structure, which is proliferating in demand for drug design and development. Further, it can be used to learn different protein functions. Although there are many computational methods for protein structure prediction, none of them have succeeded 100\% in solving the protein structure prediction problem. This paper proposes a deep learning model, namely Cascaded Feature Learning Model (CFLM) that uses a multi-stage transfer learning approach based on the Residual Dense Network (RDN) for predicting protein secondary structure. The model is trained with different protein datasets at various levels of transfer learning and accepts selected protein features such as solvent accessibility, physicochemical properties , PSSM (position-specific scoring matrix), and PSFM (position-specific frequency matrix) as input. The Q3 and Q8 accuracy obtained on CASP 13 and 14 benchmark datasets with the proposed approach are 91.23\% and 91.45\%, and 76.83\% and 78.04\%, respectively. The performance of CFLM is also compared with some of the recent PSSP approaches and it is observed that the proposed approach improves the prediction accuracy by 2.8\% in terms of Q3 and 1.81\% in terms of Q8 in the CASP 14 dataset.},
  archive      = {J_ASOC},
  author       = {Geethu S. and Vimina E.R.},
  doi          = {10.1016/j.asoc.2023.110242},
  journal      = {Applied Soft Computing},
  pages        = {110242},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Protein secondary structure prediction using cascaded feature learning model},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chaos-based optimal path planning of humanoid robot using
hybridized regression-gravity search algorithm in static and dynamic
terrains. <em>ASOC</em>, <em>140</em>, 110236. (<a
href="https://doi.org/10.1016/j.asoc.2023.110236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work deals with the navigational analysis of humanoids in complex terrains by tuning the classical approach with reactive techniques. Here, the linear regression (LR)-based classical approach is merged with the gravitational search algorithm (GSA), called RGSA, and Chaos is added to achieve an optimum path. To improve upon the local traps, Chaos is introduced into the basic structure of the RGSA model to attain a global solution without effecting the higher convergence rate of the RGSA model. The Chaos further adds ergodicity to the dynamical system , gaining an optimal path length in the shortest time possible. The use of the proposed hybridization technique is found to be effective as compared to its counterparts. Various chaotic maps like the logistic map, Gauss map, piecewise linear chaotic map, and sinusoidal maps are used, and the results are compared. Also, the smoothness of the path is ensured during trajectory planning . The analysis is done in real-time lab conditions and simulation environments using single and multiple humanoids (Nao robots) with static and dynamic obstacles within the search space. The simulation for the path planning is done using the Webot software, while the choregraphe software is used for experimental path planning . The simulation and experimental work presented in the current paper shows good agreement and are within 6\% acceptable limits. The proposed controller is then compared with a vision-based approach and another parallel sensor-based approach for optimum operational cost and smoother trajectory. The current work is a novel approach in which the humanoid’s interaction with the mobile robot is also analyzed. The proposed approach successfully avoided the dynamic obstacle in the form of the KHEPERA-II mobile robot. This concept can find potential applications in soccer scenarios employing mobile robots as opponent players to the Nao robots. Further, because of the lower operational cost, the proposed methodology can also be applied to other areas, such as multitasking, cooperative scheduling, power distribution systems, etc.},
  archive      = {J_ASOC},
  author       = {Vikas and Dayal R. Parhi},
  doi          = {10.1016/j.asoc.2023.110236},
  journal      = {Applied Soft Computing},
  pages        = {110236},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaos-based optimal path planning of humanoid robot using hybridized regression-gravity search algorithm in static and dynamic terrains},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient biobjective evolutionary algorithm for mining
frequent and high utility itemsets. <em>ASOC</em>, <em>140</em>, 110233.
(<a href="https://doi.org/10.1016/j.asoc.2023.110233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining frequent and high utility itemsets (FHUIs) from transactional databases is essential in data mining. From a multiobjective perspective, modelling the task of mining FHUIs in a unified framework requires support and utility to be considered simultaneously. In contrast to traditional algorithms for mining FHUIs, multiobjective evolutionary algorithms (MOEAs) can overcome the difficulty of setting the parameter and can generate multiple solutions in one pass, which brings advantages to mining FHUIs. However, MOEAs may be inefficient when the number of transactions and the number of items in the transaction database are large. To address this problem, we propose an efficient biobjective evolutionary algorithm for obtaining FHUIs (BOEA-FHUI) based on three novel strategies. In BOEA-FHUI, a pruning strategy is proposed to reduce the search space. Based on the pruning results, a repair strategy is proposed to make the generated inferior offspring jump out of the dominated region of the previous Pareto solutions . With the proposed pruning and repair strategies, the search space can be significantly reduced, which helps improve the search efficiency. To increase the number of items with higher support and higher utility values, an improved mutation strategy based on the sparse nature of the FHUI is proposed, which can accelerate the convergence speed of the algorithm. The experimental results on the real-world and synthetic datasets show that the proposed algorithm performs better than state-of-the-art MOEAs in finding FHUIs.},
  archive      = {J_ASOC},
  author       = {Wei Fang and Chongyang Li and Qiang Zhang and Xin Zhang and Jerry Chun-Wei Lin},
  doi          = {10.1016/j.asoc.2023.110233},
  journal      = {Applied Soft Computing},
  pages        = {110233},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient biobjective evolutionary algorithm for mining frequent and high utility itemsets},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An explicit exploration strategy for evolutionary
algorithms. <em>ASOC</em>, <em>140</em>, 110230. (<a
href="https://doi.org/10.1016/j.asoc.2023.110230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bio-inspired algorithms are a trending topic in the field of optimization. In general, evolutionary algorithms start their search process by generating a random population. Although there are some works in the literature that propose a mechanism to generate the initial population of a particular algorithm, there is no general approach to improve exploration in evolutionary algorithms . In this work, a versatile strategy is proposed for improving the exploration of any evolutionary algorithm. An exhaustive set of experiments was designed by applying 4 state-of-the-art algorithms to a well-known benchmark of continuous functions. A comparison based on statistical tests was made to demonstrate the performance of the proposed policy. The results indicate that the explicit exploration strategy is effective.},
  archive      = {J_ASOC},
  author       = {Rogelio Salinas-Gutiérrez and Angel Eduardo Muñoz Zavala},
  doi          = {10.1016/j.asoc.2023.110230},
  journal      = {Applied Soft Computing},
  pages        = {110230},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explicit exploration strategy for evolutionary algorithms},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Survey of machine learning based intrusion detection methods
for internet of medical things. <em>ASOC</em>, <em>140</em>, 110227. (<a
href="https://doi.org/10.1016/j.asoc.2023.110227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Medical Things (IoMT) has revolutionized the healthcare industry by enabling physiological data collection using sensors, which are transmitted to remote servers for continuous analysis by physicians and healthcare professionals. This technology offers numerous benefits, including early disease detection and automatic medication for patients with chronic illnesses. However, IoMT technology also presents significant security risks, such as violating patient privacy or exposing sensitive data to interception attacks due to wireless communication , which could be fatal for the patient. Additionally, traditional security measures, such as cryptography, are challenging to implement in medical equipment due to the heterogeneous communication and their limited computation, storage, and energy capacity. These protection methods are also ineffective against new and zero-day attacks. It is essential to adopt robust security measures to ensure data integrity, confidentiality, and availability during data collection, transmission, storage, and processing. In this context, using Intrusion Detection Systems (IDS) based on Machine Learning (ML) can bring a complementary security solution adapted to the unique characteristics of IoMT systems. Therefore, this paper investigates how IDS based on ML can address security and privacy issues in IoMT systems. First, the generic three-layer architecture of IoMT is provided, and the security requirements of IoMT systems are outlined. Then, the various threats that can affect IoMT security are identified, and the advantages, disadvantages, methods, and datasets used in each solution based on ML at the three layers that make up IoMT are presented. Finally, the paper discusses the challenges and limitations of applying IDS based on ML at each layer of IoMT, which can serve as a future research direction.},
  archive      = {J_ASOC},
  author       = {Ayoub Si-Ahmed and Mohammed Ali Al-Garadi and Narhimene Boustia},
  doi          = {10.1016/j.asoc.2023.110227},
  journal      = {Applied Soft Computing},
  pages        = {110227},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Survey of machine learning based intrusion detection methods for internet of medical things},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resilient supply chain network design without lagging
sustainability responsibilities. <em>ASOC</em>, <em>140</em>, 110225.
(<a href="https://doi.org/10.1016/j.asoc.2023.110225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the 21st century, global supply chains have experienced severe risks due to disruptions caused by crises and serious diseases, such as the great tsunami, SARS, and, more recently, COVID-19. Building a resilient supply chain is necessary for business survival and growth. Similarly, there is increasing regulatory and social pressure for managers to continuously design and implement sustainable supply chain networks, encompassing economic, social, and environmental components. Hence, a panacea approach is required to establish a compromise position between resiliency concerns and sustainability responsibilities. To address this, this work presents a hybrid integrated BWM-CoCoSo-multi-objective programming model (BC-MOPM) formulated to deliver a compromise between resilience and sustainability supply chain network design (RS-SCND). First, a thorough literature review analysis is conducted to explore the relationship and correlation between resilience and sustainability to develop a framework for the resiliency and sustainability criteria, in a supply chain context. Second, four objectives were formulated, including the minimisation of total cost and environmental impact and the maximisation of social and resilience paradigms. A real two-tier supply chain network is deployed to evaluate the applicability of the developed BC-MOPM. Furthermore, sensitivity analysis is conducted to establish the relative importance of the identified criteria to prove the model’s robustness. Results demonstrate the capability of the BC-MOPM in revealing trade-offs between the resiliency and sustainability aspects.},
  archive      = {J_ASOC},
  author       = {Ahmed Mohammed and Nasiru Zubairu and Morteza Yazdani and Ali Diabat and Xiaodong Li},
  doi          = {10.1016/j.asoc.2023.110225},
  journal      = {Applied Soft Computing},
  pages        = {110225},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Resilient supply chain network design without lagging sustainability responsibilities},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated design concept evaluation method based on
best–worst entropy and generalized TODIM considering multiple factors of
uncertainty. <em>ASOC</em>, <em>140</em>, 110165. (<a
href="https://doi.org/10.1016/j.asoc.2023.110165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design concept evaluation (DCE) plays a vital role in the early design stage of new product development because it directly affects the direction of subsequent design and manufacturing activities. However, the previous DCE methods have some limitations, such as (i) they usually only consider one or two factors of uncertainty, while multiple factors of uncertainty are always involved in the group decision-making problems; (ii) the criteria weight calculations usually only consider the subjective expert experience or the objective statistical analysis, which is not complete; and (iii) the psychological behaviors of experts are ignored when considering gains or losses, all of which affect their precision. Hence, this study develops an integrated DCE model to address the above limitations. First, a new uncertain information manipulation method, namely interval-valued intuitionistic fuzzy rough clouds (IVIFRCs), is developed to handle the multiple factors of uncertainty in the group decision-making problems by integrating the interval-valued intuitionistic fuzzy sets (IVIFS)-based trapezium cloud model and the interval rough number (IRN) theory. Then, the arithmetic operations , distance measure, and aggregation operators for IVIFRCs are introduced and discussed. Furthermore, the best–worst and entropy methods are integrated as the best–worst entropy (BWE) to identify the hybrid criteria weights with IVIFRC information . Finally, the generalized TODIM (TOmada de Decisão Iterativa Multicritério) is applied to handle experts’ psychological behaviors as well as rank design concepts. Additionally, a case study, sensitivity analysis, and several comparisons are conducted. Results demonstrate that the developed model is superior to the existing DCE approaches.},
  archive      = {J_ASOC},
  author       = {Guangquan Huang and Liming Xiao and Genbao Zhang},
  doi          = {10.1016/j.asoc.2023.110165},
  journal      = {Applied Soft Computing},
  pages        = {110165},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated design concept evaluation method based on best–worst entropy and generalized TODIM considering multiple factors of uncertainty},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decision field theory-combined multi-attribute group
decision-making method for incomplete linear ordinal ranking.
<em>ASOC</em>, <em>140</em>, 110056. (<a
href="https://doi.org/10.1016/j.asoc.2023.110056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, experts sometimes compare alternatives from different perspectives by rankings to express their preferences. However, it is common that the comparing information they give is incomplete, and experts’ preferences are changing during the decision-making process. In this situation, it is not easy to depict experts’ preferences and making appropriate decisions. Hence, to handle the problems, this paper proposes a multi-attribute group decision-making (MAGDM) method for incomplete linear ordinal ranking (ILOR) information combined with the decision field theory (DFT) from the perspective of process-oriented decision-making. Firstly, the extended preference map and information energy for ILOR are improved. Based on those, the concept of probabilistic utility set (PUS) and some basic operations are proposed to enhance the computability of ILOR, which can convert incomplete ILORs to PUS and depict the experts’ preferences effectively. Then, the framework and the detailed steps of the DFT-combined MAGDM method are presented, in which the psychological difference for PUS is established. The method helps depict the distance felt by experts and the variability of the decision-making process. Finally, the illustrations are conducted to show the usage and features of the proposed method. The illustration shows the good interpretability and accuracy of the method.},
  archive      = {J_ASOC},
  author       = {Nana Liu and Zeshui Xu and Hangyao Wu},
  doi          = {10.1016/j.asoc.2023.110056},
  journal      = {Applied Soft Computing},
  pages        = {110056},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision field theory-combined multi-attribute group decision-making method for incomplete linear ordinal ranking},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolutionary algorithm based on parsimony for the
multiobjective phylogenetic network inference problem. <em>ASOC</em>,
<em>139</em>, 110270. (<a
href="https://doi.org/10.1016/j.asoc.2023.110270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylogenetic networks can represent evolutionary phenomena that phylogenetic trees cannot describe, such as parallelism, convergence, reversion, hybridisation, recombination, and horizontal transference. The phylogenetic inference problem can be seen as an optimisation problem, searching for the most qualified network among the possible topologies, based on an inference criterion. However, different criteria may result in several topologies of networks, which could conflict with each other. Multi-objective optimisation can handle conflicting objectives, reducing the bias associated with the dependency on a specific criterion. In this work, we define the multi-objective phylogenetic inference problem based on networks to consider reticular phenomena and propose an ad-hoc evolutionary algorithm to treat it: MO-PhyNet . This algorithm is based on the Non-dominated Sorting Genetic Algorithm II designed to infer rooted phylogenetic networks by minimising three criteria: (1) parsimony hardwired , (2) parsimony softwired , and (3) the number of reticulations . The formalisation of the phylogenetic inference based on networks as a multi-objective optimisation problem allows us to obtain solutions considering conflicting inference criteria, resulting in different reticulated topologies representing distinct evolutionary hypotheses. The MO-PhyNet results identify Pareto set of solutions that show a relationship between the hardwired parsimony and the minimum reticulations criteria. Additionally, MO-PhyNet obtains better solutions than other strategies in terms of the optimised criteria by allowing to visualise incongruences and horizontal phenomena. This work is the first attempt to address the inference of phylogenetic networks considering multi-objective optimisation concerning the current literature to the best of our knowledge.},
  archive      = {J_ASOC},
  author       = {Manuel Villalobos-Cid and Márcio Dorn and Ángela Contreras and Mario Inostroza-Ponta},
  doi          = {10.1016/j.asoc.2023.110270},
  journal      = {Applied Soft Computing},
  pages        = {110270},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary algorithm based on parsimony for the multiobjective phylogenetic network inference problem},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective butterfly optimization algorithm for
protein encoding. <em>ASOC</em>, <em>139</em>, 110269. (<a
href="https://doi.org/10.1016/j.asoc.2023.110269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of multiple genes to maximize protein expression levels represents an important challenge in synthetic biology . This task relies on the definition of multiple protein-coding sequences, which must be as different as possible to avoid information loss. Proteins can be encoded in different ways, using synonymous codons that translate into the same amino acid. Some codons are better suited to the host than others, thus being preferable the use of the most fitting ones. However, adopting only the most highly adapted codons would lead to very similar coding sequences. An additional criterion is given by the fact that the designed sequences must contain a suitable guanine–cytosine (GC) ratio in accordance with the characteristics of the host organism. Therefore, this biological task requires the simultaneous optimization of several, conflicting objectives. This work proposes a novel multi-objective approach for protein encoding, which tackles the problem according to a new formulation based on three objective functions: codon adaptation index, Hamming distance between sequences, and GC content. Our work extends the recent Butterfly Optimization Algorithm to multi-objective contexts, integrating problem-specific operators to boost solution quality by covering the different aspects required for accurate protein encoding. Two key structures, a taboo list and a best solution list, are defined to conduct improved searches attending to the potential improvements that each solution in the population can promote. Experiments conducted on nine real-world proteins reveal the attainment of relevant solutions from different evaluation perspectives, showing significant improvements over other single and multi-objective methods from the literature.},
  archive      = {J_ASOC},
  author       = {Belen Gonzalez-Sanchez and Miguel A. Vega-Rodríguez and Sergio Santander-Jiménez},
  doi          = {10.1016/j.asoc.2023.110269},
  journal      = {Applied Soft Computing},
  pages        = {110269},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective butterfly optimization algorithm for protein encoding},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The unscented genetic algorithm for fast solution of GA-hard
optimization problems. <em>ASOC</em>, <em>139</em>, 110260. (<a
href="https://doi.org/10.1016/j.asoc.2023.110260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces the Unscented Genetic Algorithm (U-GA), which combines ideas from evolutionary computation and Kalman filters to devise a novel approach to solve GA-hard problems. The approach is justified based on how other Bayesian methods make strong assumptions on data, which could limit their performance in the long run. U-GA applies theory from unscented Kalman filters to relax this assumptions via Monte-Carlo simulation. The algorithm is explained in detail, showing how unscented Kalman filters equations could be adapted for the evolutionary computation framework. In the experiments, the proposed approach is compared to Bayesian optimization algorithm (BOA) and genetic algorithms (GAs) to investigate the strengths and limitations of U-GA. The results show how U-GA attains better performance than the benchmarks, even when the problem size is increased. Also U-GA attained a considerable speed-up (around 400\%) when compared with similar methods.},
  archive      = {J_ASOC},
  author       = {Anton Aguilar-Rivera},
  doi          = {10.1016/j.asoc.2023.110260},
  journal      = {Applied Soft Computing},
  pages        = {110260},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The unscented genetic algorithm for fast solution of GA-hard optimization problems},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Precipitation prediction by integrating rough set on fuzzy
approximation space with deep learning techniques. <em>ASOC</em>,
<em>139</em>, 110253. (<a
href="https://doi.org/10.1016/j.asoc.2023.110253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence has evolved into a mechanism that can predict environmental changes and investigate climatic conditions . In recent days, extreme precipitation and flooding have resulted in huge damage to the society of human beings and farmers. In real-life, the climatic conditions were predicted using huge collected data. However, the collected data are not utilized properly due to the presence of uncertainty. Most of the research was carried out in time series data for classification and prediction. However uncertain data leads to erroneous predictions. Thus, an integrated model is required to overcome the uncertainty and to improve the prediction accuracy. The paper aims in integrating the Rough Set on Fuzzy Approximation Space (RSFAS) with a Deep Learning (DL) techniques to predict the precipitation level in the southern coastal areas of India in a seasonal way. The proposed model can handle uncertainty using an RSFAS whereas the DL technique is utilized for classification and prediction. The developed model is estimated based on various evaluation metrics and optimizers, benchmarked with existing Deep Learning and Machine Learning (ML) techniques to demonstrate the prediction accuracies obtained with minimized error rate. Consequently, the proposed model assists in understanding the climatic conditions in extreme events to provide a prior warning about heavy rainfall to the people who live near the coastal areas.},
  archive      = {J_ASOC},
  author       = {Tishya Manna and Anitha A.},
  doi          = {10.1016/j.asoc.2023.110253},
  journal      = {Applied Soft Computing},
  pages        = {110253},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Precipitation prediction by integrating rough set on fuzzy approximation space with deep learning techniques},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature library-assisted surrogate model for evolutionary
wrapper-based feature selection and classification. <em>ASOC</em>,
<em>139</em>, 110241. (<a
href="https://doi.org/10.1016/j.asoc.2023.110241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, wrapper-based feature selection (FS) using evolutionary algorithms has been widely studied due to its ability to search for and evaluate subsets of features based on populations. However, these methods often suffer from a high computational cost and a long computation time, mainly due to the process of evaluating the feature subsets according to the classification performance. In order to tackle this problem, this paper presents a feature library-assisted surrogate model (FL-SM), which aims to reduce the computational cost but maintain a good prediction accuracy. Unlike the existing surrogate models used in FS, the proposed method focuses on the feature level instead of the sample level: an FL is built by collecting the scores of all the features during the evolutionary search. Specifically, each solution (subset candidate) is pre-evaluated based on the FL using only simple operations to decide whether or not it deserves to be evaluated by the classifier, improving the efficiency of the FS algorithm . Meanwhile, because not evaluating a certain number of solutions may lead to inaccurate solution selection during the evolutionary search, dynamic individual selection criteria are proposed. In addition, an adaptive FL update operator is proposed to handle the dynamics of the evolved population; it ensures the real-time validity of the FL . Furthermore, we incorporate the proposed FL-SM into some state-of-the-art single- and multi-objective evolutionary FS methods. The experimental results on benchmark datasets show that with good flexibility and extendibility, FL-SM can effectively reduce the computational cost of wrapper-based FS and still obtain high-quality feature subsets. Among the five algorithms tested, the average computation time reduction was 34.87\%; at the same time, there was no significant difference in the classification accuracy for 80\% of the tests, and our method even improved the classification accuracy for 6\% of the tests.},
  archive      = {J_ASOC},
  author       = {Hainan Guo and Junnan Ma and Ruiqi Wang and Yu Zhou},
  doi          = {10.1016/j.asoc.2023.110241},
  journal      = {Applied Soft Computing},
  pages        = {110241},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature library-assisted surrogate model for evolutionary wrapper-based feature selection and classification},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigations of color image segmentation based on
connectivity measure, shape priority and normalized fuzzy graph cut.
<em>ASOC</em>, <em>139</em>, 110239. (<a
href="https://doi.org/10.1016/j.asoc.2023.110239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vital ingredient of image analysis and vision-based systems is color image segmentation studies. Because color images contain more information than gray images, they are more fascinating to segment. Finding the features and associated information of a certain object becomes a difficult operation as a result of the underlying data for color images. Segmentation is highly challenging since color images have a diversity of color image and intensities. In this study, we employ shape priority of color image and connectivity measure as just a thresholding theme to distinguish an object from the surrounding. To introduce a normalized fuzzy graph cut measure based on the common S membership function in order to enhance the segmentation of color images. The execution of the suggested technique, also known as the S-fuzzy normalized graph cut (S-FNGC) approach. The structural imperfections in color images have been handled using the S-fuzzy normalized graph cut. In this algorithm, a system of S fuzzy sets is used to provide information about the specific feature of participation of a particular object in the boundary of the image. In the color image segmentation , the trouble of wrongly segmentation and segmentation with low accuracy can identify with the aid of using this approach. This proposed set of rules is as compared with the mask threshold, Gabor filter , some adaptive techniques like Genetic algorithm (GA), and K-Means clustering algorithm. Moreover, we take a look at that during maximum cases, our set of rules offers the lowest error rate and misclassification error values that enhance the segmentation manner of color images.},
  archive      = {J_ASOC},
  author       = {P. Karthick and S.A. Mohiuddine and K. Tamilvanan and S. Narayanamoorthy and S. Maheswari},
  doi          = {10.1016/j.asoc.2023.110239},
  journal      = {Applied Soft Computing},
  pages        = {110239},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Investigations of color image segmentation based on connectivity measure, shape priority and normalized fuzzy graph cut},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient approximating alpha-cut operations approach for
deriving fuzzy priorities in fuzzy multi-criterion decision-making.
<em>ASOC</em>, <em>139</em>, 110238. (<a
href="https://doi.org/10.1016/j.asoc.2023.110238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deriving the fuzzy priorities of criteria from pairwise comparison results is a challenging task in fuzzy multi-criterion decision-making problems. Most past work accomplishes this task by estimating the values of fuzzy priorities or fitting the membership functions of fuzzy priorities using alpha-cut operations (ACO). The former is error-prone, while the latter is time-consuming. To address these issues, this study proposes an efficient approximating ACO (exACO) method by incorporating two novel treatments. First, both logarithmic and exponential functions are used to approximate the membership functions of fuzzy priorities. Second, the enumeration process in ACO is monitored and terminated if the α α cuts of fuzzy priorities converge. The proposed methodology has been applied to two cases in the literature. According to the experimental results, the exACO method reduced the average estimation error by 80\% while increasing the computational efficiency by 98\%. The contribution of this study is to accurately derive fuzzy priorities while maintaining high computational efficiency.},
  archive      = {J_ASOC},
  author       = {Tin-Chih Toly Chen and Yu-Cheng Wang and Min-Chi Chiu},
  doi          = {10.1016/j.asoc.2023.110238},
  journal      = {Applied Soft Computing},
  pages        = {110238},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient approximating alpha-cut operations approach for deriving fuzzy priorities in fuzzy multi-criterion decision-making},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining online reviews for ranking products: A novel method
based on multiple classifiers and interval-valued intuitionistic fuzzy
TOPSIS. <em>ASOC</em>, <em>139</em>, 110237. (<a
href="https://doi.org/10.1016/j.asoc.2023.110237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reviews can assist customers in making purchasing decisions, however, when multiple alternative products need to be compared based on online reviews, a large number of reviews information cannot be processed manually. Therefore, this study develops a novel method for product ranking based on mining online reviews and an interval-valued intuitionistic fuzzy Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). Concretely, it contains three major steps: (1) preprocessing, (2) calculating sentiment scores, and (3) ranking alternative products. First, the results of preprocessing are vectorization representation of online reviews, which are obtained with the Bidirectional Encoder Representations from Transformers (BERT) model. Then, sentiment orientations of online reviews of alternative products regarding different attributes are determined by a multiple classifiers system , which consists of multi-class base classifiers constructed using Support Vector Machine (SVM) with One-Vs-One (OVO) strategy. Lastly, based on the sentiment scores, the ranking result is obtained by the interval-valued intuitionistic fuzzy TOPSIS method. A case study on real-world datasets is provided to illustrate the application of our proposed method, which indicates the validity of our proposal. Thus, the method proposed in this paper can effectively assist consumers in selecting products based on online reviews that meet their preferences.},
  archive      = {J_ASOC},
  author       = {Ke Li and Chou-Yong Chen and Zhong-Liang Zhang},
  doi          = {10.1016/j.asoc.2023.110237},
  journal      = {Applied Soft Computing},
  pages        = {110237},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mining online reviews for ranking products: A novel method based on multiple classifiers and interval-valued intuitionistic fuzzy TOPSIS},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fake news detection: A survey of graph neural network
methods. <em>ASOC</em>, <em>139</em>, 110235. (<a
href="https://doi.org/10.1016/j.asoc.2023.110235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of various social networks has generated vast volumes of data. Efficient methods for capturing, distinguishing, and filtering real and fake news are becoming increasingly important, especially after the outbreak of the COVID-19 pandemic. This study conducts a multiaspect and systematic review of the current state and challenges of graph neural networks (GNNs) for fake news detection systems and outlines a comprehensive approach to implementing fake news detection systems using GNNs. Furthermore, advanced GNN-based techniques for implementing pragmatic fake news detection systems are discussed from multiple perspectives. First, we introduce the background and overview related to fake news, fake news detection, and GNNs. Second, we provide a GNN taxonomy-based fake news detection taxonomy and review and highlight models in categories. Subsequently, we compare critical ideas, advantages, and disadvantages of the methods in categories. Next, we discuss the possible challenges of fake news detection and GNNs. Finally, we present several open issues in this area and discuss potential directions for future research. We believe that this review can be utilized by systems practitioners and newcomers in surmounting current impediments and navigating future situations by deploying a fake news detection system using GNNs.},
  archive      = {J_ASOC},
  author       = {Huyen Trang Phan and Ngoc Thanh Nguyen and Dosam Hwang},
  doi          = {10.1016/j.asoc.2023.110235},
  journal      = {Applied Soft Computing},
  pages        = {110235},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fake news detection: A survey of graph neural network methods},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate approach to uncertainty quantification of neural
networks for regression. <em>ASOC</em>, <em>139</em>, 110234. (<a
href="https://doi.org/10.1016/j.asoc.2023.110234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty quantification is essential in preventing inaccurate predictions of neural networks . A vanilla neural network for regression does not intrinsically provide explicit information about prediction uncertainty. To quantify the prediction uncertainty for regression problems , we can build an alternative prediction model specialized for uncertainty quantification. However, this requires the use of training data, which are inaccessible in many real-world situations. To address such situations, this study presents a surrogate approach to quantify the prediction uncertainty of a regression network without using training data. A regression network tends to have high prediction uncertainty when its output is sensitive to its input. Based on this intuition, we quantify the sensitivity and use it as a surrogate measure of the prediction uncertainty. To do so, we introduce four surrogate measures that capture the sensitivity in different ways: Input perturbation , Gradient norm , MC-dropout , and Knowledge distillation . For a query instance, each surrogate measure can be calculated by using the regression network only to estimate the prediction uncertainty. We demonstrate the respective effectiveness of the proposed surrogate measures on nine regression datasets.},
  archive      = {J_ASOC},
  author       = {Myeonginn Kang and Seokho Kang},
  doi          = {10.1016/j.asoc.2023.110234},
  journal      = {Applied Soft Computing},
  pages        = {110234},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate approach to uncertainty quantification of neural networks for regression},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A three-way decision based multi-attribute decision-making
with intuitionistic fuzzy β-covering. <em>ASOC</em>, <em>139</em>,
110231. (<a href="https://doi.org/10.1016/j.asoc.2023.110231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-attribute decision making (MADM) is an important part of modern decision science, and its theories and methods are widely used in many fields, such as engineering design, economics, management and military. The essence of MADM is to use the existing decision-making information to rank or select a set of (limited) alternatives in a certain way. Using three-way decision (3WD) to solve the MADM problem has become a hot topic today. In classical 3WD models, the equivalence relation of many decision-making methods is too strict, so we have to face a certain decision risk when using classical 3WD to solve real-life problems. In view of this, we aim to put forward a novel 3WD model on the basis of intuitionistic fuzzy β β -covering (IF β β C) in MADM problems. Firstly, we propose a novel ideal positive degree for ranking intuitionistic fuzzy numbers (IFNs). Secondly, we establish a method of conditional probability calculation formula based on the novel ideal positive degree, and obtain the loss functions as per the aggregated operator and the novel ideal positive degree of IFNs. Then a novel 3WD method based on IF β β C is proposed. Finally, we apply the proposed method to solve the teacher training professional certification problem. By comparison and experimental analysis with existing methods, the results show that the proposed method is effective and credible to deal with MADM problems.},
  archive      = {J_ASOC},
  author       = {Haidong Zhang and Deji Selang},
  doi          = {10.1016/j.asoc.2023.110231},
  journal      = {Applied Soft Computing},
  pages        = {110231},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way decision based multi-attribute decision-making with intuitionistic fuzzy β-covering},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional neural network pruning based on
multi-objective feature map selection for image classification.
<em>ASOC</em>, <em>139</em>, 110229. (<a
href="https://doi.org/10.1016/j.asoc.2023.110229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) are widely used for image classification . Deep CNNs often require a large memory and abundant computation resources, limiting their usability in embedded or mobile devices . To overcome this limitation, several pruning methods have been proposed. However, most of the existing methods focus on pruning parameters and cannot efficiently address the computation costs of deep CNNs. Additionally, these methods ignore the connections between the feature maps of different layers. This paper proposes a multi-objective pruning based on feature map selection (MOP-FMS). Unlike previous studies, we use the number of floating point operations (FLOPs) as a pruning objective in addition to the accuracy of the pruned network. First, we propose an encoding method based on feature map selection with a compact and efficient search space. Second, novel domain-specific crossover and mutation operators with reparation are designed to generate new individuals and make them meet the constraint rules. Then, decoding and pruning methods are proposed to prune networks based on the results of feature map selection. Finally, multi-objective optimisation is used for evaluation and individual selection. Our method has been tested with commonly used network structures. Numerical results demonstrate that the proposed method achieves better results than other state-of-the-art methods in terms of pruning rate without decreasing the accuracy rate to a high degree.},
  archive      = {J_ASOC},
  author       = {Pengcheng Jiang and Yu Xue and Ferrante Neri},
  doi          = {10.1016/j.asoc.2023.110229},
  journal      = {Applied Soft Computing},
  pages        = {110229},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convolutional neural network pruning based on multi-objective feature map selection for image classification},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dexterous workspace optimization for a six degree-of-freedom
parallel manipulator based on surrogate-assisted constrained
differential evolution. <em>ASOC</em>, <em>139</em>, 110228. (<a
href="https://doi.org/10.1016/j.asoc.2023.110228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel manipulators are increasingly applied because of their high stiffness, load capacity, and motion precision. However, the workspace of a parallel manipulator is relatively small compared to that of a serial manipulator. Therefore, appropriate structural designs of parallel manipulators for maximizing the workspace are essential. This paper studies the dexterous workspace maximization problem of a six-degree-of-freedom (DOF) parallel manipulator. First, a constrained optimization model is formulated, in which, the objective is to maximize the dexterous workspace, and the constraints include several kinematic and dynamic indicators, and some general structural limitations. To address the highly constrained, computationally-expensive problem, a surrogate-assisted two-phase constrained differential evolution method is proposed. Its effectiveness and efficiency are verified by the superior performance over two surrogate-free evolutionary methods: differential evolution (DE) and genetic algorithm (GA), and two surrogate-assisted state-of-the-art evolutionary methods: modified constrained expected improvement (CEI) and the lower confidence bounding approach (PCLCB), on ten benchmark functions and the established real-word 6-DOF manipulator design optimization problem .},
  archive      = {J_ASOC},
  author       = {Huayan Pu and Hao Cheng and Gang Wang and Jie Ma and Jinglei Zhao and Ruqing Bai and Jun Luo and Jin Yi},
  doi          = {10.1016/j.asoc.2023.110228},
  journal      = {Applied Soft Computing},
  pages        = {110228},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dexterous workspace optimization for a six degree-of-freedom parallel manipulator based on surrogate-assisted constrained differential evolution},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classifying spam emails using agglomerative hierarchical
clustering and a topic-based approach. <em>ASOC</em>, <em>139</em>,
110226. (<a href="https://doi.org/10.1016/j.asoc.2023.110226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware , phishing or hoaxes. Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective. Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative hierarchical clustering into 11 classes. We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words, Word2Vec and BERT- and four classifiers: Support Vector Machine , Näive Bayes, Random Forest and Logistic Regression . Experimental results show that the highest performance is achieved with TF-IDF and LR for the English dataset, with a F1 score of 0.953 and an accuracy of 94.6\%, and while for the Spanish dataset, TF-IDF with NB yields a F1 score of 0.945 and 98.5\% accuracy. Regarding the processing time, TF-IDF with LR leads to the fastest classification, processing an English and Spanish spam email in 2 ms 2ms and 2 . 2 ms 2.2ms on average, respectively.},
  archive      = {J_ASOC},
  author       = {Francisco Jáñez-Martino and Rocío Alaiz-Rodríguez and Víctor González-Castro and Eduardo Fidalgo and Enrique Alegre},
  doi          = {10.1016/j.asoc.2023.110226},
  journal      = {Applied Soft Computing},
  pages        = {110226},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object tracking and detection techniques under GANN threats:
A systemic review. <em>ASOC</em>, <em>139</em>, 110224. (<a
href="https://doi.org/10.1016/j.asoc.2023.110224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current developments in object tracking and detection techniques have directed remarkable improvements in distinguishing attacks and adversaries. Nevertheless, adversarial attacks , intrusions, and manipulation of images/ videos threaten video surveillance systems and other object-tracking applications. Generative adversarial neural networks (GANNs) are widely used image processing and object detection techniques because of their flexibility in processing large datasets in real-time. GANN training ensures a tamper-proof system, but the plausibility of attacks persists. Therefore, reviewing object tracking and detection techniques under GANN threats is necessary to reveal the challenges and benefits of efficient defence methods against these attacks. This paper aims to systematically review object tracking and detection techniques under threats to GANN-based applications. The selected studies were based on different factors, such as the year of publication, the method implemented in the article, the reliability of the chosen algorithms, and dataset size. Each study is summarised by assigning it to one of the two predefined tasks: applying a GANN or using traditional machine learning (ML) techniques. First, the paper discusses traditional applied techniques in this field. Second, it addresses the challenges and benefits of object detection and tracking. Finally, different existing GANN architectures are covered to justify the need for tamper-proof object tracking systems that can process efficiently in a real-time environment.},
  archive      = {J_ASOC},
  author       = {Saeed Matar Al Jaberi and Asma Patel and Ahmed N. AL-Masri},
  doi          = {10.1016/j.asoc.2023.110224},
  journal      = {Applied Soft Computing},
  pages        = {110224},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Object tracking and detection techniques under GANN threats: A systemic review},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive mating selection based on weighted indicator for
multi/many-objective evolutionary algorithm. <em>ASOC</em>,
<em>139</em>, 110223. (<a
href="https://doi.org/10.1016/j.asoc.2023.110223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposition-based Multi/Many-Objective Evolutionary Algorithms (DMOEAs) employ uniformly spaced reference vectors which may be appropriate for Multi-objective Optimization Problems (MOPs) with continuous Pareto Fronts (PFs). However, when solving MOPs which are characterized by discontinuous and/or degenerated PFs, it is essential to identify the regions where no solutions are bound to exist and identify the corresponding reference vectors referred to as ineffective reference vectors. In literature, various frameworks were proposed to classify the reference vectors into effective and ineffective during the process of evolution based on their association with the solutions in the population. However, due to the stochastic nature of the evolutionary process, some of the effective weight vectors may fail to associate with the solutions in the population and are misclassified as ineffective. Therefore, the region corresponding to the particular reference vector should be thoroughly explored before labeling it as ineffective. In this paper, the reference vectors are divided into three classes, namely, effective, possible ineffective and true ineffective vectors. Then, the regions corresponding to the possible ineffective reference vectors are thoroughly explored before finally classifying them as effective or ineffective. In order to facilitate the exploration corresponding to the different regions, an adaptive mating selection based on weighted I SDE + ( I SD E + w ) SDE+(ISDE+w) is proposed. In addition, the adaptive mating selection also facilitates the exploration of sparser regions once all the reference vectors are classified as effective or ineffective. The performance of the MOEA with the proposed Adaptive Mating Selection, referred to as AMS-MOEA significantly outperforms or is comparable to MOEADAWA, NSGA-III, ANSGA-III, MOEA/DD, RVEA, TDEA, 1by1EA, I SDE + ISDE+ and iRVEA in 65\%, 93.75\%, 88.75\%, 76.25\%, 87.5\%, 77.5\%, 93.75\%, 76.25\% and 86.25\% of cases, respectively.},
  archive      = {J_ASOC},
  author       = {Saykat Dutta and Sri Srinivasa Raju M and Rammohan Mallipeddi and Kedar Nath Das},
  doi          = {10.1016/j.asoc.2023.110223},
  journal      = {Applied Soft Computing},
  pages        = {110223},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive mating selection based on weighted indicator for Multi/Many-objective evolutionary algorithm},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Purchasing decision of machine tool by exploiting uncertain
information in nested probabilistic linguistic model. <em>ASOC</em>,
<em>139</em>, 110222. (<a
href="https://doi.org/10.1016/j.asoc.2023.110222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global environment deteriorates further, the decision-makers of enterprises no longer only consider qualitative factors such as yield in the choice of machine tools, but also pay more attention to the green sustainability and intelligent structure. In this study, a two-stage decision-making framework is established and a decision support system that combines quantitative and qualitative analysis is built to handle the machine tool purchasing decision. The first stage focused on quantitative analysis is to propose the mathematical model of the intelligent production system. Two heuristic algorithms that are automatic optimization method and periodic search method are designed to preliminary screen alternatives. The second stage related to qualitative analysis is to propose an improved TOPSIS method with nested probabilistic linguistic term set to obtain the best alternative comprehensively. In the end, we design the production schedule for the best alternative and prove the practicability and validity of the proposed models and algorithms. This study contributes to providing a theoretical perspective of representing uncertain information, as well as a practical scenario for purchasing decisions.},
  archive      = {J_ASOC},
  author       = {Ming Li and Xinxin Wang and Zeshui Xu},
  doi          = {10.1016/j.asoc.2023.110222},
  journal      = {Applied Soft Computing},
  pages        = {110222},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Purchasing decision of machine tool by exploiting uncertain information in nested probabilistic linguistic model},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of risks impeding sustainable mining using
fermatean fuzzy score function based SWARA method. <em>ASOC</em>,
<em>139</em>, 110220. (<a
href="https://doi.org/10.1016/j.asoc.2023.110220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sustainability in the mining and raw materials sector is a key target in the EU Green deal agenda. The aim of this work is to determine the degree of importance of risks that may impede sustainable mining, considering UN Sustainable Development Goals (SDGs) indicators and EU initiatives, taking as a case study the mining sector in Greece . A total of 49 risks for sustainable mining, under six categories, were identified by means of expert consultation and review of the literature. The identification and prioritization of potential risks can provide a pathway towards sustainable mining operations. The risks factors weighting is enhanced using a new Fermatean fuzzy score function with Stepwise Weight Assessment Ratio Analysis (SWARA). The proposed model is a powerful tool to handle the uncertainties and inaccuracies in the information regarding the weights of the risks. The main research findings indicate that the most important risks for sustainable mining in Greece are irresponsible mining, the lack of license to operate, and poor environmental monitoring, which are directly connected to the aim and scope of SDG12: responsible consumption and production. In addition, according to the results the category with the highest risk for sustainable mining is the one of “Risk to Environment”. A complete list of risks and risk categories, and their ranking is presented and discussed creating a priority of actions in the framework of European and international initiatives to set a road map to sustainable mining. This work provides a benchmark for future studies, with the aim of providing a tool for evaluating and ranking global risk factors that may affect sustainable mining development.},
  archive      = {J_ASOC},
  author       = {Muhammet Deveci and Emmanouil A. Varouchakis and Pablo R. Brito-Parada and Arunodaya Raj Mishra and Pratibha Rani and Maria Bolgkoranou and Michail Galetakis},
  doi          = {10.1016/j.asoc.2023.110220},
  journal      = {Applied Soft Computing},
  pages        = {110220},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of risks impeding sustainable mining using fermatean fuzzy score function based SWARA method},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete komodo algorithm for traveling salesman problem.
<em>ASOC</em>, <em>139</em>, 110219. (<a
href="https://doi.org/10.1016/j.asoc.2023.110219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a Discrete Komodo Algorithm (DKA) to solve the traveling salesman problem (TSP). Inspired by the Komodo Mlipir Algorithm, a swarm intelligence algorithm based on the behavior of komodo dragons, DKA introduces an enhanced edge-construction operator and a novel edge-destruction operator to explore the discrete search space of TSP. To prioritize the exploration of potential edges, DKA uses a Priority Queue that stores all possible edges with their weights, and its counts appear on tour as its priority. The edges with smaller weights and counts have higher probabilities of being selected for the tour. DKA is tested with 45 different TSP instances from TSPLIB. Experimental results show that DKA guarantees produced the best-known solution for small test problems for up to 24 nodes, and the best solutions are within 6.92\% of the known optimal solutions. Compared with some of the previous algorithms, DKA outperforms the recent state-of-the-art algorithms and is significantly better than classical metaheuristic algorithms by achieving a lower relative error.},
  archive      = {J_ASOC},
  author       = {Gilang Kusuma Jati and Garry Kuwanto and Tahir Hashmi and Herman Widjaja},
  doi          = {10.1016/j.asoc.2023.110219},
  journal      = {Applied Soft Computing},
  pages        = {110219},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete komodo algorithm for traveling salesman problem},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strengthening evolution-based differential evolution with
prediction strategy for multimodal optimization and its application in
multi-robot task allocation. <em>ASOC</em>, <em>139</em>, 110218. (<a
href="https://doi.org/10.1016/j.asoc.2023.110218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world problems can be considered multimodal optimization problems (MMOPs), which require locating as many global optima as possible and refining the accuracy of the found optima as high as possible. However, there are some issues with existing algorithms for solving MMOPs. For instance, most of the existing methods adopt the greedy selection strategy to select offspring, which may lead some individuals to fall into local optima and the repetitive evaluations for these local optima will exhaust many fitness evaluations ( FEs ). Moreover, many MMOPs tend to be expensive to evaluate, and the rational allocation of evaluation resources to better deal with MMOPs is a critical challenge within a limited number of FEs . How to allocate FEs reasonably in a whole evolution and how to avoid individuals becoming trapped in local optima are two key problems in solving MMOPs. Therefore, this paper proposes a strengthening evolution-based differential evolution with prediction strategy (SEDE-PS) for solving MMOPs and verifies its performance in a multirobot task allocation (MRTA) problem, which has the following three contributions. First, a neighbour-based evolution prediction (NEP) strategy is proposed to predict the position of individuals in the next generation by using the historical information of individuals as much as possible. Second, a prediction-based mutation (PM) strategy is introduced to accelerate convergence by combining it with the NEP strategy. Third, a strengthening evolution (SE) strategy is proposed to select inferior individuals to evolve them unconditionally several times and make them approach global optima or jump out of local optima. We compare the SEDE-PS with state-of-the-art multimodal optimization algorithms on the widely used CEC’2013 benchmark. The experimental results show that SEDE-PS performs better than, or is competitive with these compared algorithms. Moreover, SEDE-PS is applied to a real-world MRTA problem to further verify the effectiveness of SEDE-PS.},
  archive      = {J_ASOC},
  author       = {Hong Zhao and Ling Tang and Jia Rui Li and Jing Liu},
  doi          = {10.1016/j.asoc.2023.110218},
  journal      = {Applied Soft Computing},
  pages        = {110218},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Strengthening evolution-based differential evolution with prediction strategy for multimodal optimization and its application in multi-robot task allocation},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective evolving long–short term memory networks
with attention for network intrusion detection. <em>ASOC</em>,
<em>139</em>, 110216. (<a
href="https://doi.org/10.1016/j.asoc.2023.110216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber security has received increasing attention, as people use more Internet applications in their lives and worry about the security of their personal data on the Internet. Intrusion Detection Systems (IDSs) are critical security tools that can detect and respond to intrusions. In recent years, Deep Learning (DL) techniques have gained popularity in IDS design due to their promising performance in terms of detection accuracy. However, the design of DL architectures usually requires professional knowledge and significantly impacts the performance of the DL model. Furthermore, the existence of a small ratio of abnormal traffic in vast network traffic leads to a serious imbalanced data problem, which negatively affects the performance of the DL model in detecting minority attack classes. To alleviate these problems, this paper proposes a multi-objective evolutionary DL model (called EvoBMF) to detect network intrusion behaviors. The model incorporates bidirectional Long–short Term Memory (BiLSTM) for preliminary feature extraction, Multi-Head Attention (MHA) for further capturing features and global information of the network traffic, and Full-Connected Layer (FCL) module to perform final classification. To deal with the challenge of manually tuning the parameters of the DL model when tackling different tasks, the parameters of the EvoBMF model are first encoded as the chromosome of the Multi-objective Evolutionary Algorithm (MOEA), which aims to optimize the two conflicting objectives (complexity and classification ability) of the model. A state-of-the-art MOEA (MOEA/D-DRA) is then used to optimize the above two objectives, aiming to obtain the optimal architecture for EvoBMF, which can be easily deployed in cloud computing scenarios to detect and respond to network intrusions. Additionally, to alleviate the severe imbalance in routine network traffic, the synthetic minority over-sampling technique is introduced to generate representative samples of minority classes to improve the overall performance of the model. At last, the experimental results conducted on two popular datasets (UNSW-NB15 and CIC-IDS 2018) have demonstrated that the proposed EvoBMF model can provide superior performance for intrusion detection when compared to some state-of-the-art IDSs.},
  archive      = {J_ASOC},
  author       = {Wenhong Wei and Yi Chen and Qiuzhen Lin and Junkai Ji and Ka-Chun Wong and Jianqiang Li},
  doi          = {10.1016/j.asoc.2023.110216},
  journal      = {Applied Soft Computing},
  pages        = {110216},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective evolving long–short term memory networks with attention for network intrusion detection},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A long-term multivariate time series forecasting network
combining series decomposition and convolutional neural networks.
<em>ASOC</em>, <em>139</em>, 110214. (<a
href="https://doi.org/10.1016/j.asoc.2023.110214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multivariate time series forecasting tasks, expanding forecast length and improving forecast efficiency is an urgent need for practical applications. Accurate long-term forecasting of multivariate time series is challenging due to the entangled temporal patterns of multivariate time series and the complex dependencies between variables at different periods. However, it is unreliable for most current models to capture temporal and inter-variable dependencies in intertwined temporal patterns. Furthermore, the Auto-Correlation mechanism cannot precisely capture the local dynamics and long-term dependencies of time series. To address these issues, we propose a concise and efficient model named SDCNet, which integrates time series decomposition and convolutional neural networks (CNNs) into a unified framework. Unlike previous approaches, SDCNet untangles the entangled temporal patterns and uses CNNs to capture the dependencies in both temporal and feature dimensions, respectively. Specifically, SDCNet progressively decomposes seasonal and trend-cyclical components from past time series, and uses temporal and feature convolution modules to extract seasonal patterns and inter-variable dependencies, respectively. Compared to competing methods, SDCNet achieves the best performance on all of four real-world datasets with a relative accuracy improvement of 16.73\%. In addition, SDCNet achieves a relative performance gain of 23.87\% on datasets with no significant periodicity.},
  archive      = {J_ASOC},
  author       = {Xingyu Wang and Hui Liu and Junzhao Du and Xiyao Dong and Zhihan Yang},
  doi          = {10.1016/j.asoc.2023.110214},
  journal      = {Applied Soft Computing},
  pages        = {110214},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A long-term multivariate time series forecasting network combining series decomposition and convolutional neural networks},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A data-driven group emergency decision-making method based
on interval-valued intuitionistic hesitant fuzzy sets and its
application in COVID-19 pandemic. <em>ASOC</em>, <em>139</em>, 110213.
(<a href="https://doi.org/10.1016/j.asoc.2023.110213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of Corona Virus Disease 2019 (COVID-19) makes people more concerned about the validity and timeliness of emergency decision making. When an emergency occurs, it is difficult for decision makers (DMs) to give accurate assessment information in the early stage due to the urgency of time, the incompleteness of information, and the limitations of DMs’ cognition and knowledge. Hence, we use interval-valued intuitionistic hesitant fuzzy sets rather than exact numbers to better characterize the fuzziness and uncertainty of emergencies. In addition, the Internet has become a major platform for the public to express their opinions or concerns, so we can collect the user-generated content on social media to help DMs determine appropriate emergency decision-making criteria which are the premise and basis of scientific decisions. However, there is likely to be some correlation between the obtained criteria. To this end, we first extend the Bonferroni mean (BM) operator to the interval-valued intuitionistic hesitant fuzzy environment, and propose three interval-valued intuitionistic hesitant fuzzy BM operators to capture the interrelation of fuzzy input variables, including an interval-valued intuitionistic hesitant fuzzy BM operator, a simplified interval-valued intuitionistic hesitant fuzzy BM operator, and a simplified interval-valued intuitionistic hesitant fuzzy weighted BM (SIVIHFWBM) operator. Then, a new group emergency decision-making method based on the SIVIHFWBM operator and social media data is proposed, and the specific steps of ranking all emergency plans are put forward. Moreover, our method is applied to evaluate emergency plans for the prevention and control of COVID-19. Finally, the effectiveness and feasibility of the method are verified by the sensitivity analysis, validity test, and comparative analysis.},
  archive      = {J_ASOC},
  author       = {Kang Du and Ruguo Fan and Yuanyuan Wang and Dongxue Wang and Rourou Qian and Bingqing Zhu},
  doi          = {10.1016/j.asoc.2023.110213},
  journal      = {Applied Soft Computing},
  pages        = {110213},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data-driven group emergency decision-making method based on interval-valued intuitionistic hesitant fuzzy sets and its application in COVID-19 pandemic},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-order adams network (HIAN) for image dehazing.
<em>ASOC</em>, <em>139</em>, 110204. (<a
href="https://doi.org/10.1016/j.asoc.2023.110204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNN) are widely used in image dehazing. However, existing network frameworks are built based on manual design from practical experience, lacking interpretable result or theoretical guidelines. Recently, residual networks are regarded as the explicit Euler forward approximation of the ODE (Ordinary Differential Equation), and several ODE-inspired networks are proposed based on the low-order explicit Euler schemes . However, on the issues of system stability and training convergence, high-order Implicit Adams Predictor–Corrector (IAPC) methods have proven to be better than low-order explicit Euler methods . Hence, we extend the IAPC method to the High-order Implicit Adams Network (HIAN). To do so, we design a series of Implicit Adams Predictor–Corrector Blocks (IABs) based on the high-order IAPC methods, all of which give better stability and accuracy than the ones designed using the low-order Euler methods . Given that, we further propose the Implicit Adams Predictor–Corrector Module (IAM) by combining the Non-local Sparse Attention (NSA) and Attention Feature Fusion (AFF) with stacked IABs where the NSA explores the mutual-correlation among intermediate features with low computation cost via a sparse constraint, while the AFF fuses intermediate features by reweighting the features from stacked IABs adaptively. Moreover, because manual network design with IABs limits dehazing performance, the Neural Architecture Search (NAS) is used to find an optimal architecture automatically. This resulting design not only is interpretable for image dehazing but also provides a reliable guideline on future network designs. The experiments demonstrate that the proposed method outperforms most existing methods on both synthetic and real images.},
  archive      = {J_ASOC},
  author       = {Shibai Yin and Shuhao Hu and Yibin Wang and Yee-Hong Yang},
  doi          = {10.1016/j.asoc.2023.110204},
  journal      = {Applied Soft Computing},
  pages        = {110204},
  shortjournal = {Appl. Soft. Comput.},
  title        = {High-order adams network (HIAN) for image dehazing},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ant colony optimization based on local search for the
vehicle routing problem with simultaneous pickup–delivery and time
window. <em>ASOC</em>, <em>139</em>, 110203. (<a
href="https://doi.org/10.1016/j.asoc.2023.110203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vehicle Routing Problem with Simultaneous Pickup–Delivery and Time Window (VRPSPDTW) is an important logistics distribution problem. Due to the complexity of this problem, there are few researches on it and lack of relevant solutions. To solve this problem, this paper proposes to use the ant colony optimization (ACO) for the first time, which a swarm intelligence optimization algorithm . An ant colony optimization algorithm with destory and repair strategies (ACO–DR) is proposed on the basis of ACO. Firstly, ACO–DR designs a random transition rule with direction to improve the probability of the algorithm to search the target and to enhance the global search ability of the algorithm. Secondly, because the positive feedback property of ACO, it is easy for the algorithm to fall into the local optimum. Therefore, two local operators, the destory operator and the repair operator, are added to avoid this phenomenon. Finally, to verify the performance of the proposed ACO-DR algorithm, it is tested on Solomon benchmark and Gehring–Homberge benchmark and compared with the state-of-the-art algorithms. The experimental results show that the ACO-DR algorithm is feasible and provides a new effective algorithm for solving VRPSPDTW problem. Besides, the proposed algorithm also has practical implications for vehicle routing problem and the results show that it is applicable and effective in practical problems.},
  archive      = {J_ASOC},
  author       = {Hongguang Wu and Yuelin Gao},
  doi          = {10.1016/j.asoc.2023.110203},
  journal      = {Applied Soft Computing},
  pages        = {110203},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ant colony optimization based on local search for the vehicle routing problem with simultaneous pickup–delivery and time window},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cybernetic-parsimonious MCDM modeling with application to
the adoption of circular economy in waste management. <em>ASOC</em>,
<em>139</em>, 110186. (<a
href="https://doi.org/10.1016/j.asoc.2023.110186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Construction and demolition waste (CDW) is the main contributor to economic loss, environmental pollution, and health hazards if the current linear economy model of ‘take-make-consume-dispose’ is not replaced by a Circular Economy (CE) as a solution approach to maximize the use of resources and reduce waste. However, successful CE concept implementation involves strategies that trigger construction and demolition waste management (CDWM) throughout its life cycle, rather than end-of-life strategies, such as reuse and recycling. By meticulously reviewing the literature, nineteen factors that affect CDWM from CE concept implementation throughout six stages of preconstruction, procurement, construction, demolition, transportation, and end-of-life are initially identified. A hybrid fuzzy Multi-Criteria Decision-Making approach is then utilized in two main stages, including an Enhanced Fuzzy Delphi Method in stage one, to refine the identified factors according to Tehran’s construction context, and a Cybernetic Parsimonious Fuzzy Analytic Hierarchy Process in stage two, to prioritize these factors. Three factors were identified during the first stage of the research methodology and added to the factors extracted from the literature. The results indicate that ‘on-site sorting, reusing, and recycling of waste materials’, ‘various procurement models’, and ‘precise implementation of waste management regulations and plans’ are the most important factors, respectively. A comprehensive list of the factors provided as part of the research findings has contributed to the body of knowledge to be used as a snapshot by researchers, while the ranking of the factors gives new insights to stakeholders on ways to manage CDW in projects.},
  archive      = {J_ASOC},
  author       = {Kamyar Kabirifar and Mojtaba Ashour and Maziar Yazdani and Amir Mahdiyar and Morteza Malekjafarian},
  doi          = {10.1016/j.asoc.2023.110186},
  journal      = {Applied Soft Computing},
  pages        = {110186},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cybernetic-parsimonious MCDM modeling with application to the adoption of circular economy in waste management},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCEOMOO: A novel subspace clustering approach using
evolutionary algorithm, off-spring generation and multi-objective
optimization. <em>ASOC</em>, <em>139</em>, 110185. (<a
href="https://doi.org/10.1016/j.asoc.2023.110185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering is an extension of conventional clustering techniques that seeks to find clusters in different subspaces within a data set with higher dimensions. But, recent research results indicate that there are few challenges to be addressed efficiently, such as finding number of subspace clusters automatically and getting optimal subspaces by satisfying multiple objective criteria. This paper focused on these two challenges and proposed a framework called Subspace Clustering using Evolutionary algorithm , Off-Spring generation and Multi-Objective Optimization (SCEOMOO) to find the optimal subspace clusters. The proposed SCEOMOO model is tested on six standard data sets with different validity indices and results are compared with well known existing subspace clustering methods . The comparative performance analysis reveals that the proposed SCEOMOO gives significant improvements in the performance with respect to some of the well known existing models.},
  archive      = {J_ASOC},
  author       = {Runad Khamkar and Pranesh Das and Suyel Namasudra},
  doi          = {10.1016/j.asoc.2023.110185},
  journal      = {Applied Soft Computing},
  pages        = {110185},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SCEOMOO: A novel subspace clustering approach using evolutionary algorithm, off-spring generation and multi-objective optimization},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GA-ENs: A novel drug–target interactions prediction method
by incorporating prior knowledge graph into dual wasserstein generative
adversarial network with gradient penalty. <em>ASOC</em>, <em>139</em>,
110151. (<a href="https://doi.org/10.1016/j.asoc.2023.110151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipartite graph-based drug–target interactions (DTIs) prediction methods are commonly limited by the sparse structure of the graph, resulting in acquiring suboptimal node feature representations. In addition, these defective node features will also interfere with the representation quality of corresponding edge features. To alleviate the sparsity of bipartite graph and get better node representation, according to the prior Knowledge Graph (KG), we developed a novel prediction model based on Variational Graph Auto-Encoder (VGAE) combined with our proposed dual Wasserstein Generative Adversarial Network with gradient penalty strategy (dual-WGAN-GP) for generating edge information and augmenting their representations. Specifically, GA-ENs first utilized dual-WGAN-GP to fill possible edges by a prior KG containing various molecular associations knowledge when constructing a bipartite graph of known DTIs. Moreover, we utilized the KG transfer probability matrix to redefine the drug–drug and target–target similarity matrix , thus constructing the final graph adjacent matrix. Combining graph adjacent matrix with node features, we learn node representations by VGAE and augment them by utilizing dual-WGAN-GP again, thus obtaining final edge representations. Finally, a fully connected network with three layers was designed to predict potential DTIs. Extensive experiment results show that GA-ENs has excellent performance for DTIs prediction and could be a supplement tool for practical DTIs biological screening.},
  archive      = {J_ASOC},
  author       = {Guodong Li and Weicheng Sun and Jinsheng Xu and Lun Hu and Weihan Zhang and Ping Zhang},
  doi          = {10.1016/j.asoc.2023.110151},
  journal      = {Applied Soft Computing},
  pages        = {110151},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GA-ENs: A novel drug–target interactions prediction method by incorporating prior knowledge graph into dual wasserstein generative adversarial network with gradient penalty},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequence-to-sequence based LSTM network modeling and its
application in thermal error control framework. <em>ASOC</em>,
<em>138</em>, 110221. (<a
href="https://doi.org/10.1016/j.asoc.2023.110221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precision machine tool is essential for machining parts with high-precision requirements, and are widely used in the aviation, aerospace, and other fields. The thermal error is an important factor affecting the geometric precision of machined parts, and then its effective control is important. To enhance the control accuracy and execution efficiency, an edge–cloud system is designed to predict and control thermal errors. To obtain the thermal error data, a sensor network, which is composed of the temperature and displacement sensors, is designed for the thermal behavior measurement. Then the temporal and spatial behaviors of thermal errors are revealed from the heat transfer perspective, and a novel sequence-to-sequence model based LSTM network with attention mechanism (SQ-LSTMA) is designed with the full exploration of the long-term (LT) and short-term (ST) memory information of thermal errors. For the designed edge–cloud system framework, the data collection is conducted by the user layer. The edge layer performs the data processing , data storage, and error prediction with the real-time data, and the SQ-LSTMA model training is conducted by the cloud layer with the historical data. The results show that the execution time is shorten effectively and that the geometric precision of the machined part is increased. Additionally, the SQ-LSTMA model is able to reflect the dependence of the current thermal error on the historical thermal errors, and should have the ability to utilize the LT and ST memory information, and the robustness and prediction accuracy of SQ-LSTMA is superior to that of the traditional time-series models.},
  archive      = {J_ASOC},
  author       = {Shuang Zeng and Chi Ma and Jialan Liu and Mengyuan Li and Hongquan Gui},
  doi          = {10.1016/j.asoc.2023.110221},
  journal      = {Applied Soft Computing},
  pages        = {110221},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sequence-to-sequence based LSTM network modeling and its application in thermal error control framework},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Softcomputing in identification of the origin of voynich
manuscript by comparison with ancient dialects. <em>ASOC</em>,
<em>138</em>, 110217. (<a
href="https://doi.org/10.1016/j.asoc.2023.110217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Voynich manuscript is a more than 600-year-old historical manuscript. It is considered one of the most mysterious books in the world. Over the last 100 years, this book has resisted attempts to decipher its content; hence, it is written in unidentified language. Since the discovery of the manuscript, many known and unknown cryptographers have unsuccessfully tried to decipher this book. Also, many mathematical methods have been implemented to determine whether it is a fraudulent historical text or an authentic text containing valuable information. This article aims to show the use of deep learning networks and classical methods to measure the similarity between the individual characters of the alphabet and between other alphabets and Voynich. The first part of the article demonstrates the effectiveness of our method in determining the similarities between individual characters of the Voynich alphabet. In the second part, we find the similarity between the Voynich Manuscript and other individual alphabet sets (languages). In other words, this article shows another possible direction in the research of Voynich manuscript to identify the language dialect family from which Voynich manuscript can theoretically come.},
  archive      = {J_ASOC},
  author       = {Ivan Zelinka and Melvin Lara and Leah C. Windsor and René Lozi},
  doi          = {10.1016/j.asoc.2023.110217},
  journal      = {Applied Soft Computing},
  pages        = {110217},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Softcomputing in identification of the origin of voynich manuscript by comparison with ancient dialects},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GH2_MobileNet: Deep learning approach for predicting green
hydrogen production from organic waste mixtures. <em>ASOC</em>,
<em>138</em>, 110215. (<a
href="https://doi.org/10.1016/j.asoc.2023.110215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Green hydrogen production is fast becoming a key technology in changing the traditional rules of energy production by relying on green energy sources. Due to the high cost of producing green hydrogen based on thermochemical, electrolytic, photolytic, and biological processes renewables, organic waste mixtures can be a good alternative and cost-effective solution for green hydrogen production. In response to this challenge, this paper introduces a proposed deep-learning approach called GH2_MobileNet for predicting the quantity of green hydrogen production (in kilograms) from an organic waste mixture dataset in images. A proposed hybrid model called MobileNet-CNN has been utilized for recognizing and classifying organic waste mixtures types, then, two methodologies called DOWE (Dry organic waste’s weight estimation), and WOWE (Wet organic waste’s weight estimation) are proposed for estimating the weight value of dry and wet organic waste images respectively. Finally, a novel algorithm called GH2PE (Green Hydrogen Production Estimation) is proposed for estimating green hydrogen quantity production (in kilograms) from both of the two types of organic waste mixtures. The proposed approach has been validated on a benchmark dataset of organic waste mixtures images. The validation results clarified that the classification accuracy and average precision, recall, and F1-score reached 98\%. Moreover, the verification results of implementing DOWE and WOWE methods showed that the total estimated weight value of a set of four items of dry organic waste mixtures is 601.92 kg, whereas, the total estimated weight value of other set of four items of wet organic waste mixtures is 3627.77 kg. Finally, the prediction results of green hydrogen production from organic waste mixtures clarified that the total organic waste mixture in the used dataset produces 299.17 kg (27.09 kg from dry-waste mixtures and 272.08 kg from wet-waste mixtures) of green hydrogen. The comparison results also confirmed the outperforming of the proposed deep learning model in recognizing and classifying organic waste items compared to other deep learning models in the literature.},
  archive      = {J_ASOC},
  author       = {Mohamed Torky and Ghada Dahy and Aboul Ella Hassanein},
  doi          = {10.1016/j.asoc.2023.110215},
  journal      = {Applied Soft Computing},
  pages        = {110215},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GH2_MobileNet: Deep learning approach for predicting green hydrogen production from organic waste mixtures},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AMFSA: Adaptive fuzzy neighborhood-based multilabel feature
selection with ant colony optimization. <em>ASOC</em>, <em>138</em>,
110211. (<a href="https://doi.org/10.1016/j.asoc.2023.110211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multilabel classification, the correlations among labels of samples are always ignored by existing feature selection models, which results in inefficient predictions. In addition, the neighborhood radius of samples needs to be manually set, which requires much computation. To effectively conquer these limitations, this article proposes a novel adaptive fuzzy neighborhood-based multilabel feature subset selection approach with ant colony optimization (ACO) for multilabel classification. First, the feature cosine similarity and the label Jaccard similarity between samples are constructed from the feature space and label space. By combining the two abovementioned similarities, the entire similarity between samples is proposed to effectively reflect the similarity between samples in the overall space, and a dynamic adjustment coefficient is developed to control the importance of label similarity. The discriminant relation is proposed to judge the homogeneous or heterogeneous relationship between samples. Second, to address the problem that the fuzzy neighborhood radius is usually chosen artificially, we calculate the average distance between the target sample and all heterogeneous or homogeneous samples. The difference between these two average distances can be used as the adaptive fuzzy neighborhood radius of the target samples. Then, novel adaptive fuzzy neighborhood rough sets are presented, and the fuzzy neighborhood dependency degree is studied to evaluate this distinguishing ability between features and samples under a fuzzy background. Furthermore, neighborhood entropy measures and adaptive fuzzy neighborhood mutual information are investigated. Finally, by integrating fuzzy neighborhood mutual information and dependency degree into the state transition rules of the ACO, a novel ACO-based feature selection algorithm is constructed to achieve this optimal feature set for multilabel classification. Experiments applied to 15 multilabel datasets prove that our developed algorithm is effective in achieving an excellent feature subset with great classification efficiency.},
  archive      = {J_ASOC},
  author       = {Lin Sun and Yusheng Chen and Weiping Ding and Jiucheng Xu and Yuanyuan Ma},
  doi          = {10.1016/j.asoc.2023.110211},
  journal      = {Applied Soft Computing},
  pages        = {110211},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AMFSA: Adaptive fuzzy neighborhood-based multilabel feature selection with ant colony optimization},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density estimation of SARS-CoV2 spike proteins using super
pixels segmentation technique. <em>ASOC</em>, <em>138</em>, 110210. (<a
href="https://doi.org/10.1016/j.asoc.2023.110210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The worldwide outbreak of COVID-19 disease was caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV 2). The existence of spike proteins , which allow these viruses to infect host cells, is one of the distinctive biological traits of various prior viruses. As a result, the process by which these viruses infect people is largely dependent on spike proteins. The density of SARS-CoV-2 spike proteins must be estimated to better understand and develop diagnostics and vaccines against the COVID-19 pandemic. CT scans and X-rays have three issues: frosted glass, consolidation, and strange roadway layouts. Each of these issues can be graded separately or together. Although CT scan is sensitive to COVID-19, it is not very specific. Therefore, patients who obtain these results should have more comprehensive clinical and laboratory tests to rule out other probable reasons. This work collected 586 SARS-CoV 2 transmission electron microscopy (TEM) images from open source for density estimation of virus spike proteins through a segmentation approach based on the superpixel technique. As a result, the spike density means of SARS-CoV2 and SARS-CoV were 21, 97 nm and 22, 45 nm, respectively. Furthermore, in the future, we aim to include this model in an intelligent system to enhance the accuracy of viral detection and classification. Moreover, we can remotely connect hospitals and public sites to conduct environmental hazard assessments and data collection.},
  archive      = {J_ASOC},
  author       = {Bakr Ahmed Taha and Qussay Al-Jubouri and Yousif Al Mashhadany and Mohd Hadri Hafiz Mokhtar and Mohd Saiful Dzulkefly Bin Zan and Ahmad Ashrif A. Bakar and Norhana Arsad},
  doi          = {10.1016/j.asoc.2023.110210},
  journal      = {Applied Soft Computing},
  pages        = {110210},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Density estimation of SARS-CoV2 spike proteins using super pixels segmentation technique},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive differential evolution algorithm with population
size reduction strategy for unconstrained optimization problem.
<em>ASOC</em>, <em>138</em>, 110209. (<a
href="https://doi.org/10.1016/j.asoc.2023.110209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The differential evolution (DE) algorithm is a heuristic random search algorithm that optimizes the problem based on population evolution. It has been widely studied for its advantages of fewer control parameters, fast convergence speed, and strong robustness. Here, an Adaptive Guided Differential Evolution algorithm on Mutation, Parameter and Population (AGDE-MPP) is proposed, which has improved the DE algorithm by adopting a new mutation scheme, a parameter adaptation scheme, and a non-linear population size reduction strategy. The new mutation scheme uses two ordered difference vectors to perturb the mutation direction of the base vector. The parameter adaptation scheme generates the mutation factor by the Cauchy distribution with the mean of the Lehmer mean value from the historical successful mutation factor pool. The new non-linear population size reduction strategy adopts a hyperbolic tangent function curve to control the population size. This adaptive algorithm can balance the exploration and exploitation capabilities at different stages of evolution and obtain solutions with high accuracy and fast convergence speed. The performance of AGDE-MPP and several state-of-the-art algorithms were evaluated on CEC2013 and CEC2017 test suites for 10, 30, and 50 dimensions. The experimental results show that the performance of AGDE-MPP is superior to almost compared algorithm and has strong competitiveness.},
  archive      = {J_ASOC},
  author       = {Xiaoyan Zhang and Qianqian Liu and Yawei Qu},
  doi          = {10.1016/j.asoc.2023.110209},
  journal      = {Applied Soft Computing},
  pages        = {110209},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive differential evolution algorithm with population size reduction strategy for unconstrained optimization problem},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety assessment of excavation system via TOPSIS-based MCDM
modelling in fuzzy environment. <em>ASOC</em>, <em>138</em>, 110206. (<a
href="https://doi.org/10.1016/j.asoc.2023.110206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excavation construction is a high-risk activity, where involves various aspects. Accurate recognition of high-risk factors and implementation of countermeasures can greatly reduce probability occurrence of accidents. Thus, identification for high-risk factors of excavation system is performed through MCDM modelling, which is implemented via technique for order preference by similarity to an ideal solution (TOPSIS). Expert coefficient was developed and applied in relative significant evaluation of experts. In according with engineering practice, risk factors were summarized from geotechnical conditions , surrounding environment, construction measurement, construction and management, which subsequently were applied to build the decision hierarchy. Latest Spherical fuzzy set were utilized to display the judgments of experts on evaluated objects. Apart from that, fuzziness and uncertainties in measured data for risk factors were tackled with triangular fuzzy set. In this regard, expert’s judgments and measured data were both considered in comprehensive weight’s determination on risk factors. The high-risk factors were determined via TOPSIS method. An engineering project about excavation construction was provided to illustrate the potentials of TOPSIS-based MCDM modelling. Risk analysis results indicated that static load, settlement of pipelines, and dynamic load were top three high-risk factors, which were consistent with engineering practice via field inspections. Finally, applicability and robustness of the TOPSIS-based MCDM modelling in high-risk factors’ target was performed through sensitivity and comparative analysis. The identification of high-risk factors via MCDM modelling not only considers the objective measured data and subjective experts’ judgments, but also provides the references for decision making in engineering practice with online survey platform.},
  archive      = {J_ASOC},
  author       = {Song-Shun Lin and Annan Zhou and Shui-Long Shen},
  doi          = {10.1016/j.asoc.2023.110206},
  journal      = {Applied Soft Computing},
  pages        = {110206},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Safety assessment of excavation system via TOPSIS-based MCDM modelling in fuzzy environment},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two phases of metaheuristic techniques for the minimum
conflict weighted spanning tree problem. <em>ASOC</em>, <em>138</em>,
110205. (<a href="https://doi.org/10.1016/j.asoc.2023.110205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum conflict weighted spanning tree (MCWST) problem is a variant of spanning tree problem with conflicting edge pairs whose primary goal is to find a spanning tree ( T T ) that contains the minimum number of conflicting edge-pairs, and the secondary goal is to minimize the weight of T T , iff T T is a conflict free spanning tree. Being NP NP -hard, very few studies have been carried out in the domain of metaheuristic techniques. This paper develops an approach consisting of two phases of metaheuristic techniques (TPMT) in order to tackle the two goals of this problem, where a hybrid artificial bee colony algorithm (hABC) is used as the first phase of TPMT to tackle the first goal of the problem, and as soon as hABC returns a conflict free solution, an iterated local search (ILS) that starts with this conflict free solution is used as the second phase to tackle the second goal. The components of hABC such as neighborhood operator, local search and operator in scout bee phase coordinate effectively in finding a spanning tree with minimum number of conflicts, whereas the components of ILS such as perturbation procedures, local seas and strong perturbation integrated with memory in acceptance criterion coordinate effectively in finding a minimum cost conflict free spanning tree based on the initial conflict free solution returned by hABC. Experimental results on available two types of benchmark instances (type 1 and type 2) show that the proposed TPMT overall is superior to state-of-the-art approaches, not only in terms of solution quality, but also in terms of computational time. Also, TPMT finds some improved results.},
  archive      = {J_ASOC},
  author       = {Punit Kumar Chaubey and Shyam Sundar},
  doi          = {10.1016/j.asoc.2023.110205},
  journal      = {Applied Soft Computing},
  pages        = {110205},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two phases of metaheuristic techniques for the minimum conflict weighted spanning tree problem},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards the sustainable economy through digital technology:
A drone-aided after-sales service scheduling model. <em>ASOC</em>,
<em>138</em>, 110202. (<a
href="https://doi.org/10.1016/j.asoc.2023.110202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many companies are implementing emerging digital technologies to improve service quality. The application of unmanned aerial vehicles (i.e., drones) in electronic products after-sales service operations is a prominent example. With drones, the products that need after-sales services (e.g., repairment) can be delivered between the company store and consumers without requiring consumers to visit the store, which not only enhances consumer satisfaction but also helps improve environmental sustainability as fewer electronic products will be abandoned with the higher after-sales service levels. However, how to optimize drone-aided after-sales operations is still under-explored. An efficient decision support system may leverage the performance of the new service model. This work thus develops a new drone-aided after-sales service optimization model and proposes efficient solution algorithms . In particular, the company provides quick pick-up, repair, and delivery services through online reservation platforms. The store uses drones to perform pick-up and delivery services, and a set of technicians is available to perform the repair tasks. Given a set of service requests, the store must schedule limited resources, i.e., technicians and drones, to maximize the total profit of a workday. We formally describe the scheduling problem under this new model and formulate it as a mixed-integer linear programming model. We then show how the problem can be piece-wisely transformed into a variant of the flexible job shop scheduling problem . We propose several new formulations based on this idea. To handle practical-sized instances, we develop a new fix-and-solve matheuristic that consists of a sorting rule determination process and an approximate model solving process. Numerical experiments are conducted to demonstrate the performance of the proposed models and matheuristic . Sensitivity analyses are also performed to provide useful and practical implications for decision-makers.},
  archive      = {J_ASOC},
  author       = {Yantong Li and Sai-Ho Chung and Xin Wen and Shanshan Zhou},
  doi          = {10.1016/j.asoc.2023.110202},
  journal      = {Applied Soft Computing},
  pages        = {110202},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards the sustainable economy through digital technology: A drone-aided after-sales service scheduling model},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Automatic identification of modal parameters for high arch
dams based on SSI incorporating SSA and k-means algorithm.
<em>ASOC</em>, <em>138</em>, 110201. (<a
href="https://doi.org/10.1016/j.asoc.2023.110201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modal parameters identified by strong seismic observation data of the dam can truly reflect the dynamic characteristics of the structure under working conditions. However, the complex dam–reservoir–foundation structural system under seismic excitation, with dense modalities and strong external disturbances , prevents the accurate identification of the modal parameters of high arch dams on the basis of strong seismic observation data. In this paper, a novel method combining the stochastic subspace identification (SSI), sparrow search algorithm (SSA) and K-means algorithm is proposed to automatically identify the modal parameters of high arch dams . First, based on the strong seismic monitoring data of a high arch dam, SSI is used to identify the modal parameters of the high arch dam, and an improved stabilization diagram is drawn with the natural frequency and modal assurance criterion to avoid the effect of the unstable damping ratio . Second, abnormal poles in the stabilization diagram are identified by combining the local outlier factor and kernel density estimation to eliminate false modes, and the SSA is applied to search for the optimal combination of the K-means clustering algorithm to obtain the initial clustering centers of the stabilization diagram. Finally, the K-means algorithm is used to perform cluster analysis on the effective poles in the stabilization diagram to achieve the automatic identification of the modal parameters. By identifying the modal parameters of a three-degree-of-freedom spring–mass model and a high arch dam under seismic signals, the proposed method is compared with three methods. The results confirm that the novel method is superior to other methods in terms of accuracy and efficiency. The present study can significantly suppress noise, eliminate false modes and automatically identify the real modal parameters without human interference under a low signal-to-noise ratio.},
  archive      = {J_ASOC},
  author       = {Bo Li and Wei Liang and Shengmei Yang and Lixin Zhang},
  doi          = {10.1016/j.asoc.2023.110201},
  journal      = {Applied Soft Computing},
  pages        = {110201},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic identification of modal parameters for high arch dams based on SSI incorporating SSA and K-means algorithm},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-ant colony optimization algorithm based on finite
history archiving and boxed pigs game. <em>ASOC</em>, <em>138</em>,
110193. (<a href="https://doi.org/10.1016/j.asoc.2023.110193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ant colony algorithm has good performance in solving small-scale discrete problems, but still has the shortcomings of poor convergence and tendency to fall into local optimum when dealing with large-scale data. In order to solve these problems, we propose a multi-ant colony algorithm based on finite history archiving and the boxed pigs game model. Firstly, the heterogeneous multi-ant colony system is constructed by ant colony system and max–min ant colony system to enrich the diversity of populations. Second, the limited history archiving strategy is used to enhance the path guidance to enable faster convergence. In addition, the co-evolution of populations is achieved by constructing the boxed pigs game model to increase the performance of the algorithm. Finally, the leveling mechanism is used to help the algorithm escape from the local optimum when it stagnates, thereby improving the accuracy of the solution. The effectiveness of the algorithm is verified by testing benchmark instances of traveling salesman problem . The experimental results show that the improved algorithm has significantly improved in terms of convergence and solution accuracy.},
  archive      = {J_ASOC},
  author       = {Hanke Li and Xiaoming You and Sheng Liu},
  doi          = {10.1016/j.asoc.2023.110193},
  journal      = {Applied Soft Computing},
  pages        = {110193},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-ant colony optimization algorithm based on finite history archiving and boxed pigs game},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sustainability analysis of digital transformation and
circular industrialization with quantum spherical fuzzy modeling and
golden cuts. <em>ASOC</em>, <em>138</em>, 110192. (<a
href="https://doi.org/10.1016/j.asoc.2023.110192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to identify the most essential criteria necessary for cyclical production and digital transformation in countries. First, eight factors are weighted by considering the Quantum spherical fuzzy (QSF) DEMATEL with golden cut method. Furthermore, QSF M-SWARA is also used to weight the criteria so that a comparative evaluation can be performed. Next, emerging seven countries are evaluated according to the performance of cyclical production and digital transformation with QSF TOPSIS with golden cut. A comparative evaluation has also been performed by using QSF ELECTRE methodology. Additionally, the sensitivity analysis is applied with 8 cases by changing the weighting results of the criteria consecutively. It is seen that the ranking results are quite similar for all different cases that indicate the coherency and reliability of the proposed model. The most important novelty of this study is to make a priority analysis by developing an original fuzzy decision-making model to provide cyclical production and digital transformation in enterprises. The findings demonstrate that new generation energy technologies have the greatest importance. Owing to new energy technologies, the use of clean energy can be increased. This issue has an important influence on reaching energy efficiency and reduction of carbon emission problem. Therefore, the purpose of circular production and digital transformation can be achieved much easily.},
  archive      = {J_ASOC},
  author       = {Serhat Yüksel and Hasan Dinçer},
  doi          = {10.1016/j.asoc.2023.110192},
  journal      = {Applied Soft Computing},
  pages        = {110192},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainability analysis of digital transformation and circular industrialization with quantum spherical fuzzy modeling and golden cuts},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exponential loss regularisation for encouraging ordinal
constraint to shotgun stocks quality assessment. <em>ASOC</em>,
<em>138</em>, 110191. (<a
href="https://doi.org/10.1016/j.asoc.2023.110191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal problems are those where the label to be predicted from the input data is selected from a group of categories which are naturally ordered. The underlying order is determined by the implicit characteristics of the real problem. They share some characteristics with nominal or standard classification problems but also with regression ones. In the real world, there are many problems of this type in different knowledge areas, such as medical diagnosis, risk prediction or quality control. The latter has gained an increasing interest in the Industry 4.0 scenario. Some weapons manufacturer follow an aesthetic quality control process to determine the quality of the wood used to produce the stock of the weapons they manufacture. This process is an ordinal classification problem that can be automatised using machine learning techniques . Deep learning methods have been widely used for multiples types of tasks including image aesthetic quality control, where convolutional neural networks are the most common alternative, given that they are focused on solving problems where the input data are images. In this work, we propose a new exponential regularised loss function that is usedto improve the classification performance for ordinal problems when using deep neural networks . The proposed methodology is applied to a real-world aesthetic quality control problem. The results and statistical analysis prove that the proposed methodology outperforms other state-of-the-art methods, obtaining very robust results.},
  archive      = {J_ASOC},
  author       = {Víctor Manuel Vargas and Pedro Antonio Gutiérrez and Riccardo Rosati and Luca Romeo and Emanuele Frontoni and César Hervás-Martínez},
  doi          = {10.1016/j.asoc.2023.110191},
  journal      = {Applied Soft Computing},
  pages        = {110191},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exponential loss regularisation for encouraging ordinal constraint to shotgun stocks quality assessment},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimizing makespan in two-stage assembly additive
manufacturing: A reinforcement learning iterated greedy algorithm.
<em>ASOC</em>, <em>138</em>, 110190. (<a
href="https://doi.org/10.1016/j.asoc.2023.110190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive manufacturing (AM) is becoming increasingly important for producing mass-customized, small-quantity products with relatively low geometric constraints . Although some AM machine scheduling problems have been proposed in recent years, no research has addressed the parallel AM machine scheduling problem with an integrated assembly stage. In this study, a two-stage assembly additive manufacturing scheduling problem is considered, in which multiple parts are produced in job batches using identical parallel AM machines in the first stage and then assembled into the desired products in the second stage. Further, a mixed-integer linear programming model and an innovative reinforcement learning metaheuristic , called the iterated epsilon-greedy algorithm, are proposed to minimize the makespan of this significant scheduling extension. The computational results based on 810 test instances show that the developed approaches are highly effective, efficient, and robust in solving the addressed problem. Notably, the research results can effectively reduce the gap between the theory and practice of AM production planning by integrating the production stage with the assembly stage.},
  archive      = {J_ASOC},
  author       = {Kuo-Ching Ying and Shih-Wei Lin},
  doi          = {10.1016/j.asoc.2023.110190},
  journal      = {Applied Soft Computing},
  pages        = {110190},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimizing makespan in two-stage assembly additive manufacturing: A reinforcement learning iterated greedy algorithm},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent deep reinforcement learning algorithm with
self-adaption division strategy for VNF-SC deployment in SDN/NFV-enabled
networks. <em>ASOC</em>, <em>138</em>, 110189. (<a
href="https://doi.org/10.1016/j.asoc.2023.110189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization can decouple the traditional network function from the dedicated hardware, abstracts the software-based virtual network function from the specialized network equipment, and promotes the fundamental transformation of network service deployment mode. However, the deployment of virtual network function (VNF) service chain is an important and crucial problem and key technology faced and must be rescued. In this paper, the problem of VNF service chain deployment in SDN/NFV-Enabled Networks is investigated. The existing solution strategies based on optimization methods (dynamic programming, linear programming, etc.) and heuristic methods (genetic algorithm, particle swarm optimization, etc.) are only suitable for operation deployment in the case of predictable operations, and it is difficult to meet the real-time support operation scheduling requirements in high dynamic combat scenarios. A new real-time algorithm for VNF service chain deployment based on multi-agent deep reinforcement learning with self-adaption division strategy (MDRL-SaDS) to minimize energy consumption in a period of time is proposed. In proposed algorithm, an oriented self-adaptive strategy to determination the number of agents and the optimal division method of VNF service chain for the markov process modeling is designed. Constructing a new neural network model and design a training strategy of joint supervised and unsupervised learning. The global and long-term benefits are used to optimize the scheduling process, and the decision-making framework of offline learning and online deployment is used to solve the VNF service chain deployment problem. Finally, experimental results indicate that the MDRL-SaDS has more advantages and has higher convergence speed, average reward value and stability than compared algorithms, while decreasing the energy consumption in a period of time.},
  archive      = {J_ASOC},
  author       = {Hejun Xuan and Yi Zhou and Xuelin Zhao and Zhenghui Liu},
  doi          = {10.1016/j.asoc.2023.110189},
  journal      = {Applied Soft Computing},
  pages        = {110189},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-agent deep reinforcement learning algorithm with self-adaption division strategy for VNF-SC deployment in SDN/NFV-enabled networks},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-task twin bounded support vector machine and its safe
screening rule. <em>ASOC</em>, <em>138</em>, 110188. (<a
href="https://doi.org/10.1016/j.asoc.2023.110188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct multi-task twin support vector machine (DMTSVM) obtains great performance in dealing with correlated tasks. However, DMTSVM only considers the empirical risk minimization principle, so it readily causes over-fitting and lowers the prediction accuracy. To enhance the generalization ability of the classifier, we construct a multi-task twin bounded support vector machine (MT-TBSVM), in which a regularization term is introduced into the objective function, thus implementing the structural risk minimization principle. To improve the computational speed of MT-TBSVM, we additionally put forward a safe screening rule (SSR) for it. SSR could identify most inactive samples from the multiple tasks. Therefore, this will result in a significant reduction in the scale of dual problems, meanwhile the testing accuracy keeps unchanged since it does not sacrifice the optimal solution. Moreover, a fast DCDM algorithm is presented for further solving the reduced MT-TBSVM. On fifteen benchmark datasets, numerical experimental results clearly demonstrated the effectiveness of proposed algorithms. Finally, the proposed method is applied to a real Chinese wine dataset to further investigate its validity.},
  archive      = {J_ASOC},
  author       = {Ran An and Yitian Xu and Xuhua Liu},
  doi          = {10.1016/j.asoc.2023.110188},
  journal      = {Applied Soft Computing},
  pages        = {110188},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-task twin bounded support vector machine and its safe screening rule},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time data-driven automatic design of multi-objective
evolutionary algorithm: A case study on production scheduling.
<em>ASOC</em>, <em>138</em>, 110187. (<a
href="https://doi.org/10.1016/j.asoc.2023.110187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithms (MOEAs) have become an important choice for solving multi-objective optimization problems . The performance of MOEAs is highly dependent on the algorithm configuration. Therefore, the algorithm configuration is an essential task in the development and application of MOEAs. In this paper, a real-time data-driven automatic design method for configuring an MOEA with minimal user interference is developed. Real-time data are driven in two ways. One lies in that the learning model is constructed based on the elite configurations selected by the Iterated F-Race (I/F-Race), which is used to bias the sampling toward the best configurations. Another is that the decision tree model is constructed by collecting the evaluated configurations in the process of I/F-Race as the data, and used to help identify the potential configurations to improve the sampling quality. In addition, a configurable MOEA (CMOEA) framework is summarized by integrating three general fitness assignment methods. In the experimental study, a case study on a multi-objective hybrid flowshop scheduling problem is conducted. By comparing with other variants of I/F-Race, the developed method is verified to have the ability of evaluating the promising configurations more fully and conceiving the best MOEA. Compared with the famous frameworks and state-of-the-art MOEAs, the proposed CMOEA framework and the automated algorithm show their superiorities based on different performance metrics.},
  archive      = {J_ASOC},
  author       = {Biao Zhang and Lei-lei Meng and Chao Lu and Jun-qing Li},
  doi          = {10.1016/j.asoc.2023.110187},
  journal      = {Applied Soft Computing},
  pages        = {110187},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time data-driven automatic design of multi-objective evolutionary algorithm: A case study on production scheduling},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolutionary computation-based machine learning for
network attack detection in big data traffic. <em>ASOC</em>,
<em>138</em>, 110184. (<a
href="https://doi.org/10.1016/j.asoc.2023.110184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data scenarios are characterized by multiple devices, massive traffic, and high data dimensionality. In the process of attack identification, the selection of features from massive data directly affects the attack detection effect and has become a key issue that constrains attack identification. Therefore, this paper proposes an evolutionary computation-based machine learning approach for detecting network attacks in big data traffic. First, the RandomSample-SMOTE (Synthetic Minority Over-sampling Technique) method is designed to perform class imbalance processing on network attack traffic collected from big data traffic; second, the feature importance of the attack traffic in different classification layers is calculated and ranked separately using the LightGBM (Light Gradient Boosting Machine) model, and the optimal feature values are selected through retraining; finally, the obtained feature values are used for model training and the most optimal model is obtained by optimizing the hyperparameters with TuneGridSearchCV (Tune’s Grid Search Cross Validation). The results of simulation experiments show that the method in this paper can effectively extract features from big data traffic. It can effectively reduce feature dimensionality, significantly improve detection accuracy and save about 40\% of computation time compared with existing methods.},
  archive      = {J_ASOC},
  author       = {Yan Wang and Haifeng Zhang and Yongjun Wei and Huan Wang and Yong Peng and Zhiyan Bin and Weilong Li},
  doi          = {10.1016/j.asoc.2023.110184},
  journal      = {Applied Soft Computing},
  pages        = {110184},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary computation-based machine learning for network attack detection in big data traffic},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Field features: The impact in learning to rank approaches.
<em>ASOC</em>, <em>138</em>, 110183. (<a
href="https://doi.org/10.1016/j.asoc.2023.110183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to Rank approaches employ Machine Learning techniques for Information Retrieval. Traditionally, the features needed to train a ranking model are naively combined after being extracted from the various fields of the texts. Nevertheless, if not considered carefully, the learning process can make use of strongly correlated features. Moreover, the learned ranking models are not, to date, systematically analyzed in terms of how the field-based features affect their performances. In this work, the impact of using field-based features in Learning to Rank approaches is investigated. Specifically, the Field Learning to Rank technique is proposed to study if the field-based features perform better than the naively combined features. The experiments are conducted employing eight learning to rank approaches on two sizable benchmark datasets: MQ2007 and MQ2008. The models are assessed using three widely adopted Learning to Rank evaluation measures, namely Precision, Mean Average Precision, and Normalized Discounted Cumulative Gain. The results show that the use of field-based features achieve better performance than the naively combined features. Moreover, models aggregated from different fields further improve the ranking results. It is also observed that among the five examined fields, url and title are significantly more effective than wholedoc (full document), body , and anchor to build ranking models. Further, analyses indicate the existence of strong correlations between field features, such as the features from body and wholedoc , title and anchor , or title and url . The proposed Field Learning to Rank technique is shown to have the advantage of avoiding the combination of correlated features. These findings imply that the use of field-based features for training ranking models is valuable for enhancing the effectiveness of Learning to Rank approaches.},
  archive      = {J_ASOC},
  author       = {Hua Yang and Teresa Gonçalves},
  doi          = {10.1016/j.asoc.2023.110183},
  journal      = {Applied Soft Computing},
  pages        = {110183},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Field features: The impact in learning to rank approaches},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge transfer in evolutionary multi-task optimization:
A survey. <em>ASOC</em>, <em>138</em>, 110182. (<a
href="https://doi.org/10.1016/j.asoc.2023.110182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-task optimization (EMTO) is an optimization algorithm designed to optimize multiple tasks simultaneously. In real life, different tasks often correlate to each other, and there exists implicit knowledge or skills common to these tasks. In EMTO, such common knowledge is utilized in an evolutionary optimization process, and EMTO aims to transfer knowledge across different tasks to improve performance in solving each task independently. Therefore, an effective and efficient knowledge transfer mechanism is critical to ensure the success of EMTO. To this end, this survey focuses on the current research progress of knowledge transfer methods in EMTO and proposes a systematic multi-level taxonomy to categorize the existing work. The conduct of categorization is based on two key problems: when and how to perform knowledge transfer. Based on the proposed taxonomy, we also discuss the possibility of integrating various approaches under different categories and applying transfer learning approaches to EMTO. Throughout these discussions, this survey paper aims to identify the potential research directions for improving knowledge transfer performance in EMTO.},
  archive      = {J_ASOC},
  author       = {Ziying Tan and Linbo Luo and Jinghui Zhong},
  doi          = {10.1016/j.asoc.2023.110182},
  journal      = {Applied Soft Computing},
  pages        = {110182},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge transfer in evolutionary multi-task optimization: A survey},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of power quality disturbances using linear
discriminant analysis. <em>ASOC</em>, <em>138</em>, 110181. (<a
href="https://doi.org/10.1016/j.asoc.2023.110181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of renewable energy sources and its integration with the grid, power semiconductor devices applications has been increased rapidly which causes many Power Quality Disturbances (PQDs). These disturbances can cause significant losses in the distribution system, therefore it is essential to recognize and mitigate these disturbances timely. The present work proposes a novel method for the classification of single stage and multiple PQDs based on dimensionality reduction. The main objective is to transform the data set from higher dimensional space to lower dimensional space by eliminating the unnecessary features. This paper proposes a supervised learning dimensionality reduction technique , i.e. Linear Discriminant Analysis (LDA) for the dimensionality reduction of the data set of twenty nine types of PQDs including single stage and multiple PQDs. In this technique, the ratio between class variance and within class variance are maximized for maintaining the maximum class separability , to obtain a lower dimensional space of the features. The performance of LDA is analysed with four type of machine learning classifiers such as k-Nearest Neighbour (KNN), Naive Bayes (NB), Support Vector Machine (SVM) and Random Forest (RF). Classification results show that the higher classification accuracy is achieved for the twenty nine types of PQDs under different noise levels (20 dB to 40 dB).},
  archive      = {J_ASOC},
  author       = {Gurpreet Singh and Yash Pal and Anil Kumar Dahiya},
  doi          = {10.1016/j.asoc.2023.110181},
  journal      = {Applied Soft Computing},
  pages        = {110181},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification of power quality disturbances using linear discriminant analysis},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous stochastic optimization of an open-pit mining
complex with preconcentration using reinforcement learning.
<em>ASOC</em>, <em>138</em>, 110180. (<a
href="https://doi.org/10.1016/j.asoc.2023.110180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A preconcentration facility is a major operational component that is critical for managing capacities and improving process plant efficiency in a mining complex. These facilities have not been considered in previous short-term production scheduling frameworks for mining complexes. Short-term production scheduling is a vital part of planning that helps ensure long-term production targets are meet without compromising value. In this work, a new stochastic mathematical programming formulation for simultaneously optimizing the short-term production schedule with preconcentration considerations is proposed. The optimization formulation considers optimizing the extraction sequence, destination policy, stockpiling and preconcentration decisions jointly to capture potential synergies. In addition, this work investigates a new approach for short-term production scheduling that combines reinforcement learning with stochastic mathematical programming. An actor–critic reinforcement learning agent learns to optimize the short-term production schedule and provides a more flexible framework for adapting heuristics to the scheduling problem. The optimization approach and stochastic formulation are tested in a copper mining complex with multiple mining areas, several material properties, stockpiles, preconcentration facilities, leach pads, process plants and waste dumps. The case study shows the practical aspects of the proposed optimization and the direct benefit of integrating preconcentration decisions in the short-term production schedule; this led to a $140M improvement in annual cashflow. Additionally, the actor–critic​ reinforcement learning algorithm learns a stable policy that provides operational extraction sequences.},
  archive      = {J_ASOC},
  author       = {Zachary Levinson and Roussos Dimitrakopoulos and Julien Keutchayan},
  doi          = {10.1016/j.asoc.2023.110180},
  journal      = {Applied Soft Computing},
  pages        = {110180},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simultaneous stochastic optimization of an open-pit mining complex with preconcentration using reinforcement learning},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilayer time delay reservoir with double feedback loops
for time series forecasting task. <em>ASOC</em>, <em>138</em>, 110179.
(<a href="https://doi.org/10.1016/j.asoc.2023.110179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time Delay Reservoir (TDR) is a special single-node reservoir to process time dependent signals simply and efficiently. It has good performance on time series forecasting tasks. Meanwhile, the single-node structure can be implemented easily in hardware. In fact, its prediction performance is also limited by its own special structure. In this paper, we experimentally verified the low memory capacity of the single feedback loop and the low utilization of virtual nodes in the single layer structure. Therefore, we propose a multilayer time delay reservoir of the double feedback loops to improve the overall efficiency and performance of the reservoir computing . Firstly, we use the double feedback loops structure which has another feedback loop with longer cycle on the basis of the single feedback loop. In this way, the responses generated earlier will be re-injected into the reservoir to improve the memory capacity. Secondly, the single layer is extended to multiple layers. The utilization of virtual nodes in the reservoir was improved during the training process of the output weights. Finally, the nonlinear function of the original TDR is replaced with a nonlinear function of exponential decay to reduce parameters. The experimental results show that the model proposed has better prediction accuracy and anti-noise performance than that of standard TDR on the time series forecasting task and also has better performance compared with other five models.},
  archive      = {J_ASOC},
  author       = {Meiming You and Fei Li and Jiaqi Xi and Guoqiang Wang and Baoxiang Du},
  doi          = {10.1016/j.asoc.2023.110179},
  journal      = {Applied Soft Computing},
  pages        = {110179},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multilayer time delay reservoir with double feedback loops for time series forecasting task},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization assisted bidirectional gated recurrent unit for
healthcare monitoring system in big-data. <em>ASOC</em>, <em>138</em>,
110178. (<a href="https://doi.org/10.1016/j.asoc.2023.110178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A substantial amount of health-related data has been produced as a result of the development of technology and expert systems. Due to noise, formats, size, missing values, and important aspects, it is difficult for the Health Monitoring System (HMS) to obtain trustworthy big data. As a result, the HMS has a harder time getting precise data. Unwanted treatments may result from data with poor quality and noise. Big data analytics are used to translate the data at the third layer of HMS to solve these issues. The WSD and MR data are transformed using the Z score normalization technique in the data analytics layer. The next step in preprocessing social network (SN) content entails character conversion, stop-word elimination, tokenization, lemmatization, and stemming Improved FC-based clustering is also carried out. The best Bi-GRU technique correctly classifies both structured and unstructured health records to forecast patients’ stages of ill health. Using the Arithmetically Updated Coot Optimization Algorithm , the weights of the Bi-GRU are optimized (AUCOA). The doctor’s analytical findings are presented in the last layer. The numerical findings make it evident that the proposed Bi-GRU + AUCOA got the best results in terms of precision (0.8394) and MCC (0.719442), outperforming Bi-GRU and Bi-GRU using existing optimization techniques.},
  archive      = {J_ASOC},
  author       = {Prashant Kumar Shukla Dr, PhD, Associate Professor (Research) and Shalini Stalin Dr, PhD, Assistant Professor and Shubham Joshi Dr, PhD, Assistant Professor and Piyush Kumar Shukla Dr, PhD, Associate Professor and Piyush Kumar Pareek Dr, PhD},
  doi          = {10.1016/j.asoc.2023.110178},
  journal      = {Applied Soft Computing},
  pages        = {110178},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization assisted bidirectional gated recurrent unit for healthcare monitoring system in big-data},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An analytical tool to support public policies and isolation
barriers against SARS-CoV-2 based on mobility patterns and
socio-economic aspects. <em>ASOC</em>, <em>138</em>, 110177. (<a
href="https://doi.org/10.1016/j.asoc.2023.110177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is crucial to develop spatiotemporal analysis tools to mitigate risks during a pandemic. Many dashboards encountered in the literature do not consider how the geolocation characteristics and travel patterns may influence the spread of the virus. This work brings an interactive tool that is capable of crossing information about mobility patterns, geolocation characteristics and epidemiologic variables. To do so, our system uses a mobility network , generated through anonymized mobile location data, which enables the division of a region into representative clusters. The clusters’ aggregated socioeconomic, and epidemiologic indicators can be analyzed through multiple coordinated views. The proposal is to enable users to understand how different locations commute citizens, monitor risk over time, and understand what locations need more assistance, considering different layers of visualization, such as clusters and individual locations. The main novelty is the interactive way to construct the mobility network that defines the social distancing level and the way that risks are managed, since many different geolocation characteristics can be considered and visualized, such as socioeconomic indicators of a location, the economic importance of a set of locations, and the connection of important neighborhoods of a city with other cities. The proposed tool was built and verified by experts assembled to give scientific recommendations to the city administration of Recife, the capital city of Pernambuco. Our analysis shows how a policymaker could use the tool to evaluate different isolation scenarios considering the trade-off between economic activity and contamination risk, where the practical insights can also be used to tighten and relax mitigation measures in other phases of a pandemic.},
  archive      = {J_ASOC},
  author       = {Julio Cezar Soares Silva and Diogo Ferreira de Lima Silva and Nivan Roberto Ferreira Júnior and Adiel Teixeira de Almeida Filho},
  doi          = {10.1016/j.asoc.2023.110177},
  journal      = {Applied Soft Computing},
  pages        = {110177},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An analytical tool to support public policies and isolation barriers against SARS-CoV-2 based on mobility patterns and socio-economic aspects},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey on design and application of
autoencoder in deep learning. <em>ASOC</em>, <em>138</em>, 110176. (<a
href="https://doi.org/10.1016/j.asoc.2023.110176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoencoder is an unsupervised learning model, which can automatically learn data features from a large number of samples and can act as a dimensionality reduction method. With the development of deep learning technology, autoencoder has attracted the attention of many scholars. Researchers have proposed several improved versions of autoencoder based on different application fields. First, this paper explains the principle of a conventional autoencoder and investigates the primary development process of an autoencoder. Second, We proposed a taxonomy of autoencoders according to their structures and principles. The related autoencoder models are comprehensively analyzed and discussed. This paper introduces the application progress of autoencoders in different fields, such as image classification and natural language processing , etc. Finally, the shortcomings of the current autoencoder algorithm are summarized, and prospected for its future development directions are addressed.},
  archive      = {J_ASOC},
  author       = {Pengzhi Li and Yan Pei and Jianqiang Li},
  doi          = {10.1016/j.asoc.2023.110176},
  journal      = {Applied Soft Computing},
  pages        = {110176},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive survey on design and application of autoencoder in deep learning},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedAGCN: A traffic flow prediction framework based on
federated learning and asynchronous graph convolutional network.
<em>ASOC</em>, <em>138</em>, 110175. (<a
href="https://doi.org/10.1016/j.asoc.2023.110175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and real-time traffic flow prediction is an essential component of the Intelligent Transportation System (ITS). Balancing the prediction accuracy and time cost of prediction models is a challenging topic. This paper proposes a deep learning framework (FedAGCN) based on federated learning and asynchronous graph convolutional networks to predict traffic flow accurately in real time. FedAGCN applies asynchronous spatial–temporal graph convolution to model the spatial–temporal dependence in traffic data. In order to reduce the time cost of the deep learning model, we propose a graph federated learning strategy GraphFed to train the model. Experiments were conducted on two public traffic datasets, and the results showed that FedAGCN effectively reduced the training and inference time of the model while maintaining considerable prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Tao Qi and Lingqiang Chen and Guanghui Li and Yijing Li and Chenshu Wang},
  doi          = {10.1016/j.asoc.2023.110175},
  journal      = {Applied Soft Computing},
  pages        = {110175},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FedAGCN: A traffic flow prediction framework based on federated learning and asynchronous graph convolutional network},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of precise forgery detection algorithms in
digital radiography images using convolution neural network.
<em>ASOC</em>, <em>138</em>, 110174. (<a
href="https://doi.org/10.1016/j.asoc.2023.110174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread availability of forged image software necessitates the integrity verification of digital images in industrial and medical applications. Because of image manipulation, detecting small tampering and duplicated forgery from digital radiography (gamma and x-ray) images has become a research challenge, Two essential approaches are proposed for forgery detection from digital radiography images. A precise forgery detection approach with pretrained deep convolution neural networks (CNN) is conducted. Alexnet, Resnet-18 and VGG-19 are three pretrained networks for features extraction. Artificial neural network (ANN) and multiclass support vector machine (MSVM) classifiers are applied for classifying the extracted features into authentic or forged. The second suggested approach depends on Haralick and Zoning extractors. These extracted features are trained and tested using the K-nearest neighbors (KNN) classifier. The suggested approaches are investigated using several manipulated industrial (gamma welding images) and medical (spine images) datasets images. Besides, these approaches are tested with several color benchmark dataset images. The results are verified using a variety of evaluation metrics . The approaches are validated through comparison with published work and high agreements are demonstrated. For digital radiography images, Alexnet pretrained network with MSVM, Resnet-18 pretrained network with ANN and Haralick extractor with KNN achieve the highest accuracy and assessment metrics. It is observed that the performance of pretrained CNN outperforms that of conventional classification algorithms in respect of accuracy with computational time. The developed approaches allow for the precise detection of forgery regions in x-ray and gamma radiographic images as well as digital images.},
  archive      = {J_ASOC},
  author       = {Mohamed S. El_Tokhy},
  doi          = {10.1016/j.asoc.2023.110174},
  journal      = {Applied Soft Computing},
  pages        = {110174},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of precise forgery detection algorithms in digital radiography images using convolution neural network},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive channel attention for rotating component fault
detection with strong noise and limited data. <em>ASOC</em>,
<em>138</em>, 110171. (<a
href="https://doi.org/10.1016/j.asoc.2023.110171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel attention mechanisms have been widely applied to enhance the diagnosis performance of convolutional neural networks (CNN). However, most of the existing channel attention modules take insufficient account of the relationship among different channels, which restricts further improvement of channel attention-based diagnosis methods. Besides, the diagnosis model performance may greatly deteriorate when facing strong noise. To address the above problems, this paper proposed an improved intelligent fault detection method for rotating component based on interactive channel attention (ICA), which contains two submodules to help the CNN model pay attention to channel correlation of both global and local channels. Meanwhile, two more replaceable submodules based on adaptive multi-scale kernel (AMK) and self-attention (SA) are provided to consider the correlation of neighbor channels and long-term channels. Furthermore, a multi-scale convolutional layer with an adaptive selective kernel (ASK) unit is applied to replace the basic convolutional layer of CNN to further enhance the model performance. The proposed method can effectively enhance the feature extraction of the basic model in the scenario with a low signal-noise ratio (SNR) noise. Experimental results with two rotating component datasets demonstrate that the proposed method is superior to the comparison methods, especially in the occasions with strong noise and limited data.},
  archive      = {J_ASOC},
  author       = {Jianguo Miao and Congying Deng and Heng Zhang and Qiang Miao},
  doi          = {10.1016/j.asoc.2023.110171},
  journal      = {Applied Soft Computing},
  pages        = {110171},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interactive channel attention for rotating component fault detection with strong noise and limited data},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of agriculture-food 4.0 supply chain approaches
using fermatean probabilistic hesitant-fuzzy sets based decision making
model. <em>ASOC</em>, <em>138</em>, 110170. (<a
href="https://doi.org/10.1016/j.asoc.2023.110170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The benchmarking of agri-food 4.0 supply chain (Agri4SC) falls under the multiple criteria problem in supply chain visibility (SCV) and supply chain resource integration (SCRI) for improving data analytics capabilities and achieving sustainable performance (SP). It is considered a multiple criteria decision-making (MCDM) problem due to three main concerns, namely, multiple Agri4SC evaluation criteria including the SCV, SCRI and SP criteria. These criteria have relative importance and trade-offs. Despite the tremendous efforts over the last years, none of the developed Agri4SCs have met all of the essential Agri4SC evaluation criteria. Another concern raised in the evaluation and benchmarking of the Agri4SC is the uncertainty of experts. Thus, the main contribution of this research is to propose an Agri4SC benchmarking framework in SCV and SCRI for improving data analytics capabilities and achieving SP based on an extension of the proposed Fermatean probabilistic hesitant fuzzy sets (FPHFSs) and MCDM methods. The methodology process is divided into six main parts. Firstly, an Agri4SC decision matrix is formulated based on the intersection of the Agri4SC alternatives and criteria to cover multiple Agri4SC evaluation criteria issues. Secondly, novel FPHFSs are proposed along with their operational laws, score function, accuracy function, Fermatean probabilistic hesitant fuzzy average mean operator and Fermatean probabilistic hesitant fuzzy weighted average operator. The FPHFS can encompass more sophisticated and uncertain evaluation information. Thirdly, Fermatean probabilistic hesitant fuzzy weighted zero inconsistency is formulated to assign weights to the evaluation criteria. Fourthly, the Fermatean probabilistic hesitant fuzzy decision by opinion score method (FPH-FDOSM) is formulated and used to score the alternatives that were evaluated subjectively based on SCV criteria. Fifthly, the FPH-FDOSM-based multi attributive ideal-real comparative analysis (MAIRCA) scoring method with equal probabilities is proposed to score Agri4SC alternatives that were evaluated subjectively based on weighted economic, environmental and social factors. Lastly, the MAIRCA ranking method with unequal probabilities is introduced to benchmark Agri4SC alternatives that were evaluated objectively based on the weighted subcriteria of SP and the trade-offs amongst the identified criteria. The robustness and reliability of the results are tested via sensitivity analysis and Spearman’s correlation coefficient.},
  archive      = {J_ASOC},
  author       = {Sarah Qahtan and Hassan A. Alsattar and A.A. Zaidan and Muhammet Deveci and Dragan Pamucar and Dursun Delen and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2023.110170},
  journal      = {Applied Soft Computing},
  pages        = {110170},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of agriculture-food 4.0 supply chain approaches using fermatean probabilistic hesitant-fuzzy sets based decision making model},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lithium-ion battery health assessment method based on belief
rule base with interpretability. <em>ASOC</em>, <em>138</em>, 110160.
(<a href="https://doi.org/10.1016/j.asoc.2023.110160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health assessment is important for the safe and reliable operation of lithium-ion battery. However, there exist two typical issues in existing health assessment models for lithium-ion battery: uncertainty of the chemical reaction inside the battery and lack of interpretability of the evaluation results. The belief rule base (BRB) is a rule-based modelling approach and can deal with uncertain information in health state assessment. However, the interpretability of the lithium-ion battery health state assessment model based on BRB needs to be preserved effectively. Thus, a new lithium-ion battery health state assessment model based on belief rule base with interpretability (BRB-I) is proposed in this paper. In the BRB-I model, both structural interpretability and optimization interpretability are constructed through the mechanism analysis of lithium-ion batteries. Moreover, a new interpretability evaluation criterion is defined as the objective function to achieve a balanced optimization between interpretability and accuracy. In addition, an improved optimization method based on the whale optimization algorithm (WOA) is proposed to enhance the interpretability of the model and the optimization process. A case study on the health state assessment of B0006 lithium-ion battery is conducted to illustrate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Peng Han and Wei He and You Cao and YingMei Li and QuanQi Mu and YuHe Wang},
  doi          = {10.1016/j.asoc.2023.110160},
  journal      = {Applied Soft Computing},
  pages        = {110160},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lithium-ion battery health assessment method based on belief rule base with interpretability},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conjoining congestion speed-cycle patterns and deep learning
neural network for short-term traffic speed forecasting. <em>ASOC</em>,
<em>138</em>, 110154. (<a
href="https://doi.org/10.1016/j.asoc.2023.110154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting accurate traffic conditions is essential to regional traffic management. Since congestions are usually caused by regular activities, capturing speed-cycle patterns for congestions is useful for short-term traffic forecasting. In this study, we propose a novel approach, namely Neighbor Subset Deep Neutral Network (NSDNN), to forecast spatio-temporal data; the approach conjoins Deep Neutral Network (DNN) and the subset selection method, in order to extract useful inputs from nearby roads. Appropriate input subsets can be selected for DNN training via congestion cycle patterns, in order to reduce input data dimensions and to avoid artificial high correlations from free-flow traffic in off-peak hours. Furthermore, speed data with time lag is also embedded into the DNN model to generate the multi-timestep forecast model. Experimental results demonstrate that the proposed NSDNN achieves higher accuracy, compared to other conventional methods including the Autoregressive Integrated Moving Average (ARIMA), the correlation method and the k k -Nearest Neighbor ( k k NN). In addition, NSDNN is also comparable to the Long Short Term Memory (LSTM) Neural Network , namely NSLSTM, when the same selected input subset is used. The forecasting system can be used by logistic companies to produce better route planning and fleet management in assigning vehicles.},
  archive      = {J_ASOC},
  author       = {W.M. Tang and K.F.C. Yiu and K.Y. Chan and K. Zhang},
  doi          = {10.1016/j.asoc.2023.110154},
  journal      = {Applied Soft Computing},
  pages        = {110154},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Conjoining congestion speed-cycle patterns and deep learning neural network for short-term traffic speed forecasting},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QE-DAL: A quantum image feature extraction with dense
distribution-aware learning framework for object counting and
localization. <em>ASOC</em>, <em>138</em>, 110149. (<a
href="https://doi.org/10.1016/j.asoc.2023.110149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object counting and localization (OCL) was an essential problem in intelligent transportation fields. The convolutional neural network (CNN)-based models transformed the OCL problems into a regression task . However, the abundant semantic information of the crowd scenes may lead the CNN framework hard to extract adequate features in order to ensure good precision In this work, a Quantum Image Feature Extraction with Dense Distribution-Aware Learning (QE-DAL) framework was proposed to handle this problem. The crowd features were extracted by Quantum layers, which were extracted by encoding, quantum circuits and decode procedures based on the multi-scale architecture. For handling objects, the refined distance compensating operator was adopted to fuse the multi-scale architecture. To relieve the computation burden, a Gaussian distribution estimation mechanism was proposed to initiate and update the bounding sizes of the objects via a point-supervised manner. Finally, the joint loss function, which describes pixel classes, density maps and offset bounding boxes , was built for QE-DAL. The ablation experiment results demonstrated that the effectiveness of the quantum feature extraction architecture and the Gaussian distribution estimation mechanism of QE-DAL was validated to show superior performance than the other state-of-the-art framework. Moreover, the generalization of the QE-DAL was evidenced by the Cross-scene learning evaluation.},
  archive      = {J_ASOC},
  author       = {Ruihan Hu and Zhiri Tang and Rui Yang},
  doi          = {10.1016/j.asoc.2023.110149},
  journal      = {Applied Soft Computing},
  pages        = {110149},
  shortjournal = {Appl. Soft. Comput.},
  title        = {QE-DAL: A quantum image feature extraction with dense distribution-aware learning framework for object counting and localization},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ship weather routing featuring w-MOEA/d and uncertainty
handling. <em>ASOC</em>, <em>138</em>, 110142. (<a
href="https://doi.org/10.1016/j.asoc.2023.110142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a new version of evolutionary multi-objective weather routing (WR) for ships taking into account uncertainties of weather forecasts in route optimization . The method applies authors’ w-MOEA/D algorithm: MOEA/D framework incorporating Decision Maker’s (DM) preferences by means of w-dominance relation. Owing to this, DM preferences are taken into account throughout optimization, allowing the process to focus on the part of vast objective’s space. Only the part of Pareto front being of interest to DM is generated, thus the process converges faster, without sacrificing quality of the final set. All of the above is essential for the WR method, which pursues three objectives while trying to meet multiple constraints and handling uncertainty of weather data. The final method has been implemented as a part of client–server system architecture , whose client part has been installed on board of a m/v Monte da Guia (MdG) vessel navigating between the Portuguese coast and the Azores. The method has then been verified in the course of computer simulations and its results have been compared with real MdG GPS routes. The comparison shows that the presented method is able to find routes that bring progress in terms of the objectives’ while satisfying the constraints.},
  archive      = {J_ASOC},
  author       = {Rafal Szlapczynski and Joanna Szlapczynska and Roberto Vettor},
  doi          = {10.1016/j.asoc.2023.110142},
  journal      = {Applied Soft Computing},
  pages        = {110142},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ship weather routing featuring w-MOEA/D and uncertainty handling},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gradient-based approach for adversarial attack on deep
learning-based network intrusion detection systems. <em>ASOC</em>,
<em>137</em>, 110173. (<a
href="https://doi.org/10.1016/j.asoc.2023.110173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection systems are an essential part of any cybersecurity architecture. These systems are critical in defending networks against a variety of security threats. In recent years, deep neural networks have proved their performance and efficiency in various machine learning tasks, including intrusion detection . However, it is shown that deep learning models are highly vulnerable to adversarial attacks . This paper proposes a new approach for performing an adversarial attack against deep learning-based malicious network activity classification. We use the Jacobian Saliency Map to find the best group of features, with different features and perturbation magnitude, to generate adversarial examples . We evaluate our method on three CIC-IDS2017, CIC-IDS2018, and CIC-DDoS2019 datasets. Our experiments show that our proposed method can achieve better performance while using fewer features in adversarial sample generation than other attacks that depend on a higher number of features. Our technique can generate adversarial samples for more than 18\% of samples in CIC-IDS2017, 15\% of samples in CIC-IDS2018, and 14\% of samples in CIC-DDoS2019, using only three features and 0.1 as the perturbation magnitude. We do a deeper analysis of the attack based on its parameters, distance metrics, and the target model performance. Also, an evaluation model with three criteria, including success rates of the best feature sets, average confidence of the adversarial class, and adversarial samples transferability, is used in our analysis.},
  archive      = {J_ASOC},
  author       = {Hesamodin Mohammadian and Ali A. Ghorbani and Arash Habibi Lashkari},
  doi          = {10.1016/j.asoc.2023.110173},
  journal      = {Applied Soft Computing},
  pages        = {110173},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gradient-based approach for adversarial attack on deep learning-based network intrusion detection systems},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Veg-W2TCN: A parallel hybrid forecasting framework for
non-stationary time series using wavelet and temporal convolution
network model. <em>ASOC</em>, <em>137</em>, 110172. (<a
href="https://doi.org/10.1016/j.asoc.2023.110172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term vegetation time series (TS) forecasting based on climatic data is one of the most challenging topics, capable of assisting in advanced estimation and management for scientific decision-making in different applications. However, these vegetation TS from remote sensing (i.e. normalized difference vegetation index (NDVI) and climatic data (standardized precipitation index (SPI)) are generally non-stationary in the real world. For this reason, traditional TS forecasting algorithms are incapable of extracting sufficient sequence data features , resulting in poor forecasting accuracy . In this paper, a novel parallel hybrid forecasting framework (Veg-W2TCN) is proposed combining multi-resolution analysis wavelet transform (MRA-WT) and temporal convolution network (TCN) model for vegetation change forecasting. In the first step, the NDVI and SPI TSs are divided into multiple groups using land cover information. For effective feature extraction, each group of data is decomposed using MRA-WT into trend and seasonal. Then, the obtained trend/seasonal from NDVI and SPI are concatenated. Multivariate TSs are obtained for each land cover. Then, each component is trained individually using the TCN model. In this step, the grid search algorithm is exploited to select the optimal TCN hyperparameters. Finally, a sum is used to integrate the different results. The proposed method is tested in the Mediterranean area. Veg-W2TCN proved its effectiveness for non-stationary TS forecasting. In fact, around 83\% of pixels have a RMSE lower than 0, 1 using Veg-W2TCN, compared to 74\%, 72\%, 60\% and 59\% of pixels are presented in TCN, long short-term memory (LSTM), multiple linear regression (MLR) and gated recurrent unit (GRU).},
  archive      = {J_ASOC},
  author       = {Manel Rhif and Ali Ben Abbes and Beatriz Martínez and Imed Riadh Farah},
  doi          = {10.1016/j.asoc.2023.110172},
  journal      = {Applied Soft Computing},
  pages        = {110172},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Veg-W2TCN: A parallel hybrid forecasting framework for non-stationary time series using wavelet and temporal convolution network model},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Z-number-valued rule-based classification system.
<em>ASOC</em>, <em>137</em>, 110168. (<a
href="https://doi.org/10.1016/j.asoc.2023.110168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy rule-based classification system (FRBCS) is a popular tool for classification problems due to its interpretability and comprehensibility . As an extension of fuzzy numbers, the concept of Z-number is a more appropriate formal structure to describe uncertain and partially reliable information. A Z-number is an ordered pair of fuzzy numbers, where the second fuzzy number describes the reliability of the first one. As a result of its representation capability, it can receive better classification results . However, there is still a gap in the application of Z-numbers to classification problems due to their high computation complexity. To take advantage of the Z-number, we design a simple way to make Z-numbers apply to classification problems. Use the second fuzzy number to adjust the first fuzzy number to fit the training data. Then we create a kind of Z-number-valued if–then rule by extending the fuzzy if–then rule. In addition, a Z-number-valued rule-based classification system (ZRBCS) is developed, including two main processes: rule generation and new pattern classification. The developed system can cover more information than the classic fuzzy rule-based system, which can improve classification effects. The proposed ZRBCS is compared with classical FRBCS with/without certain degrees and three classical classification algorithms . According to statistical tests, ZRBCS is superior to FRBCS and two other algorithms.},
  archive      = {J_ASOC},
  author       = {Yangxue Li and Enrique Herrera-Viedma and Ignacio Javier Pérez and Mónica Barragán-Guzmán and Juan Antonio Morente-Molinera},
  doi          = {10.1016/j.asoc.2023.110168},
  journal      = {Applied Soft Computing},
  pages        = {110168},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Z-number-valued rule-based classification system},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Route selection in multimodal supply chains: A fuzzy risk
assessment model-BWM-MARCOS framework. <em>ASOC</em>, <em>137</em>,
110167. (<a href="https://doi.org/10.1016/j.asoc.2023.110167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting a route in distribution networks can be considered as a multiple criteria decision-making problem owing to the involvement of conflicting decision criteria, many possible alternatives, different preferences, and uncertainties. Therefore, in this study, we proposed a new hybrid decision-making framework to select an optimal transportation route in multimodal supply chains. The proposed framework innovatively integrates the fuzzy risk assessment model (FRAM), best-worst method (BWM), and measurement of alternatives and ranking according to the compromise solution (MARCOS). FRAM-based fuzzy inference reasoning was applied to calculate the highly reliable risk magnitudes of the qualitative decision criteria. The BWM was used to determine the weights of all decision criteria to reduce the complexity and calculation procedures. MARCOS was employed to rank multimodal routes based on the effects of optimistic (ideal) and pessimistic (anti-ideal) solutions. The proposed framework was validated through an empirical study of route selection within the Greater Mekong sub-region to demonstrate its usefulness and applicability. The findings showed that the hybrid FRAM-BWM-MARCOS methodology employs the predominant features of each method to effectively optimize the route selection problem. It ranks all multimodal transportation routes and identifies the most appropriate alternative as the best compromise solution of the problem. The proposed framework can be viewed as an expert system that aids decision makers and stakeholders in developing new interactive freight distribution plans and transportation policies in fuzzy environments.},
  archive      = {J_ASOC},
  author       = {Nitidetch Koohathongsumrit and Wasana Chankham},
  doi          = {10.1016/j.asoc.2023.110167},
  journal      = {Applied Soft Computing},
  pages        = {110167},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Route selection in multimodal supply chains: A fuzzy risk assessment model-BWM-MARCOS framework},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FPGA-based implementation of deep neural network using
stochastic computing. <em>ASOC</em>, <em>137</em>, 110166. (<a
href="https://doi.org/10.1016/j.asoc.2023.110166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A serious challenge in artificial real-time applications is the hardware implementation of deep neural networks (DNN). Among various methods, stochastic computing (SC)-based implementations received tremendous attention due to the low hardware overhead. However, the slow convergence rate is a major problem in SC-based neural networks’ implementation, and millions of clock cycles are required to generate a relatively accurate output. The reconfigurability and parallel nature of field programmable gate array (FPGA) chips make them a preferable platform for SC-based DNN implementation. A fully or semi-parallel implementation of DNNs requires extensive hardware resources. In this paper, an efficient method for DNN implementation on an FPGA chip is presented to address these problems. The FPGA chip reconfiguration feature allows a DNN with several different neurons and topologies to be implemented on a single chip . Convergence time is significantly reduced by limiting the length of the stochastic bitstreams and establishing synchronization between the processing units based on precise timing. Furthermore, due to the limited number of input–output pins in the FPGA chip, a sequential architecture is proposed wherein the DNN inputs enter through only three 8-bit ports. This makes it possible to implement DNNs for image-processing applications. The proposed method is implemented using the Verilog hardware description language on the Xilinx FPGA Virtex-7 xc7v2000t chip. The results show a more than 82\% reduction in hardware resources and the minimum rate of power consumption compared to state-of-the-art methods. In addition, the average error rate of the implemented DNN is reduced by 2\%.},
  archive      = {J_ASOC},
  author       = {Maedeh Nobari and Hadi Jahanirad},
  doi          = {10.1016/j.asoc.2023.110166},
  journal      = {Applied Soft Computing},
  pages        = {110166},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FPGA-based implementation of deep neural network using stochastic computing},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accurate approach of device-free localization with
attention empowered residual network. <em>ASOC</em>, <em>137</em>,
110164. (<a href="https://doi.org/10.1016/j.asoc.2023.110164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-free localization (DFL) has been recognized as an emerging technology in the Internet of Things (IoT) field. Based on soft computing techniques , e.g., cloud systems , neural networks , wireless sensor networks , etc., DFL can locate the targets that do not carry any equipment. A crucial problem for the DFL is how to extract significant patterns from weak signal variations in order to achieve high accuracy and robustness. To solve this issue, we transform the DFL procedure into a classification task . In this paper, a channel-dependent attention empowered residual network (CA-ResNet) with three residual layers is designed for precise localization by extracting underlying signal features. The CA-ResNet takes the advantages of the residual connection and the attention mechanism in feature extraction, which is expected to have a high level of robustness in the face of extreme noise and abnormal circumstances for DFL. Experiment results based on real-world data demonstrate that our proposed approach provides a high level of localization performance for a suitable grid size. For instance, the CA-ResNet approach achieves high localization accuracy of almost 100\% in general cases. Under the condition of introducing noise to data, e.g., a signal to noise ratio greater than −5 dB, our proposed method maintains high accuracy and strong robustness. Compared with the state-of-the-art DFL methods in several cases, the best localization accuracy obtained by our proposed method is 16.4\% better than the compared AugRF, 15.5\% better than the radio robust image fingerprint indoor localization algorithm (RRIFLoc), and 6.9\% better than the convolutional autoencoder neural network, especially in challenging cases that partial sensor nodes are broken or power off under noisy condition.},
  archive      = {J_ASOC},
  author       = {Lingjun Zhao and Huakun Huang and Weizheng Wang and Zibin Zheng},
  doi          = {10.1016/j.asoc.2023.110164},
  journal      = {Applied Soft Computing},
  pages        = {110164},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An accurate approach of device-free localization with attention empowered residual network},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A case-based reasoning driven ensemble learning paradigm for
financial distress prediction with missing data. <em>ASOC</em>,
<em>137</em>, 110163. (<a
href="https://doi.org/10.1016/j.asoc.2023.110163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial distress prediction is often accompanied by missing sample data. For this purpose, a novel case-based reasoning (CBR) driven ensemble learning paradigm is proposed for financial distress prediction with missing data. In the proposed paradigm, three main stages, CBR-driven missing data imputation, CBR-driven single classifiers prediction, and CBR-driven ensemble result output, are involved. In the first stage, the CBR-driven missing data imputation method is used to fill in missing values in the initial dataset. Second, three different CBR-driven single classification models are constructed using Manhattan distance, Euclidean distance , and cosine distance to predict financial distress, respectively. In the final stage, the weighted majority voting strategy is used to ensemble prediction results of the CBR-driven single classification models to improve prediction accuracy and robustness. For illustration and verification, the experiments on datasets with different missing rates of six Chinese listed companies are performed. And corresponding results show that the proposed CBR-driven ensemble learning paradigm can effectively improve the imputation performance and increase the robustness of classification performance, indicating that the proposed CBR-driven ensemble learning paradigm can be used as a competitive solution to financial distress prediction with missing data.},
  archive      = {J_ASOC},
  author       = {Lean Yu and Mengxin Li},
  doi          = {10.1016/j.asoc.2023.110163},
  journal      = {Applied Soft Computing},
  pages        = {110163},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A case-based reasoning driven ensemble learning paradigm for financial distress prediction with missing data},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A MOEA/d with global and local cooperative optimization for
complicated bi-objective optimization problems. <em>ASOC</em>,
<em>137</em>, 110162. (<a
href="https://doi.org/10.1016/j.asoc.2023.110162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithm based on decomposition has been well-recognized in addressing bi-objective optimization problems . However, it is a serious challenge for MOEA/D to optimize real-world complicated bi-objective optimization problems which have irregular optimal objective spaces, such as long-tailed, peaked, and disconnected types. Hence, an improved MOEA/D with global and local cooperative mechanisms is proposed in this paper for complicated bi-objective optimization problems. The overall optimization is carried out by the coordination of global and local search phases . Specifically, a bidirectional global search strategy based on reference points is first used to broaden the search regions. Aiming at the irregularity of the Pareto fronts , an adaptive neighborhood search strategy is proposed according to the individual sparsity , which can further explore near the obtained global optimal solutions . The cooperation of these two strategies is designed to improve the distribution and uniformity of the population on complicated bi-objective optimization problems. The performance of the proposed algorithm is investigated on a representative set of benchmark test functions and a real-world water resource allocation problem . The simulation results show that the proposed algorithm is very promising for complicated bi-objective optimization problems.},
  archive      = {J_ASOC},
  author       = {Qian Wang and Qinghua Gu and Lu Chen and Yueping Guo and Naixue Xiong},
  doi          = {10.1016/j.asoc.2023.110162},
  journal      = {Applied Soft Computing},
  pages        = {110162},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A MOEA/D with global and local cooperative optimization for complicated bi-objective optimization problems},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel lyapunov-stability-based recurrent-fuzzy system for
the identification and adaptive control of nonlinear systems.
<em>ASOC</em>, <em>137</em>, 110161. (<a
href="https://doi.org/10.1016/j.asoc.2023.110161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The requirement to manage complicated nonlinear systems with high uncertainty is one of the main drivers of progress in the identification and control discipline. Since most of the systems are nonlinear, exact identification and control of them using traditional techniques is a challenging task. This paper proposes a novel Takagi–Sugeno (T-S) type Recurrent Fuzzy System (RFS) for nonlinear system identification and adaptive control . To ensure the system’s overall stability, a novel Lyapunov-stability-based learning algorithm is developed to obtain the parameter update equations for the proposed model. The performance of the proposed model is also evaluated with the well-known gradient-descent-based Back Propagation (BP) learning method and compared with the Lyapunov-stability method. To judge the efficacy of the proposed model and the learning algorithm we have performed an extensive comparative study by considering other popular soft-computing models such as Jordan Recurrent Neural Network (JRNN), Feed-Forward Neural Network (FFNN), Radial Basis Function Network (RBFN) and a conventional recurrent fuzzy system. The results obtained from the proposed method (RFS ＋ Lyapunov-stability-based learning algorithm) are compared with those obtained from other models and are found to be superior.},
  archive      = {J_ASOC},
  author       = {Anuli Dass and Smriti Srivastava and Rajesh Kumar},
  doi          = {10.1016/j.asoc.2023.110161},
  journal      = {Applied Soft Computing},
  pages        = {110161},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel lyapunov-stability-based recurrent-fuzzy system for the identification and adaptive control of nonlinear systems},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ModInterv COVID-19: An online platform to monitor the
evolution of epidemic curves. <em>ASOC</em>, <em>137</em>, 110159. (<a
href="https://doi.org/10.1016/j.asoc.2023.110159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the software ModInterv as an informatics tool to monitor, in an automated and user-friendly manner, the evolution and trend of COVID-19 epidemic curves, both for cases and deaths. The ModInterv software uses parametric generalized growth models, together with LOWESS regression analysis, to fit epidemic curves with multiple waves of infections for countries around the world as well as for states and cities in Brazil and the USA. The software automatically accesses publicly available COVID-19 databases maintained by the Johns Hopkins University (for countries as well as states and cities in the USA) and the Federal University of Viçosa (for states and cities in Brazil). The richness of the implemented models lies in the possibility of quantitatively and reliably detecting the distinct acceleration regimes of the disease. We describe the backend structure of software as well as its practical use. The software helps the user not only to understand the current stage of the epidemic in a chosen location but also to make short term predictions as to how the curves may evolve. The app is freely available on the internet ( http://fisica.ufpr.br/modinterv ), thus making a sophisticated mathematical analysis of epidemic data readily accessible to any interested user.},
  archive      = {J_ASOC},
  author       = {Arthur A. Brum and Giovani L. Vasconcelos and Gerson C. Duarte-Filho and Raydonal Ospina and Francisco A.G. Almeida and Antônio M.S. Macêdo},
  doi          = {10.1016/j.asoc.2023.110159},
  journal      = {Applied Soft Computing},
  pages        = {110159},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ModInterv COVID-19: An online platform to monitor the evolution of epidemic curves},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A complex network-based firefly algorithm for numerical
optimization and time series forecasting. <em>ASOC</em>, <em>137</em>,
110158. (<a href="https://doi.org/10.1016/j.asoc.2023.110158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The firefly algorithm (FA) has gained widespread attention and has been widely applied because of its simple structure, few control parameters and easy implementation. As the traditional FA lacks a mutation mechanism, it tends to fall into local optima, leading to premature convergence, thus affecting the optimization accuracy. To address these limitations, from the perspective of population diversity, a complex network-based FA (CnFA) with scale-free properties is proposed in this paper. The scale-free properties of complex networks effectively ensure the diversity of populations to guide the populations in their search, thus avoiding random interactions of information among populations that could lead to superindividuals controlling the entire population. The property of the power-law distribution of nodes in complex networks is exploited to effectively avoid the premature convergence of the FA and falling into local optima. To verify the search performance of CnFA, we compared the FA and its variants, as well as multiple competitive approaches, on 30 different-dimension benchmark function optimization tasks and two time series prediction tasks. The experimental results and statistical analysis show that CnFA achieves satisfactory performance due to the better balance between exploitation and exploration in the search process. Additionally, we extended the proposed method to two other population-based algorithms, and the experimental results verify that the complex network-based mechanism can enhance the performance of not only the FA but also other population-based evolutionary algorithms .},
  archive      = {J_ASOC},
  author       = {Zhenyu Song and Cheng Tang and Shuangbao Song and Yajiao Tang and Jinhai Li and Junkai Ji},
  doi          = {10.1016/j.asoc.2023.110158},
  journal      = {Applied Soft Computing},
  pages        = {110158},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A complex network-based firefly algorithm for numerical optimization and time series forecasting},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum parallel model predictive control for grid-connected
solid oxide fuel cells. <em>ASOC</em>, <em>137</em>, 110157. (<a
href="https://doi.org/10.1016/j.asoc.2023.110157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuel utilization of the grid-connected solid oxide fuel cells (GSOFCs) controlled by model predictive control (MPC) and proportional–integral–derivative (PID) controller is 80.0\%, which should be improved. MPC obtains the local optimal control strategy online when the power demand change. The predicted accuracy of MPC should be efficiently improved when the system is in a dynamic environment. This work proposes quantum parallel model predictive control (QPMPC) to solve the optimal local problem of MPC. The QPMPC contains one real-life quantum MPC and three virtual parallel quantum MPC controllers. The quantum revolving gate of the QPMPC can achieve more information about the optimal solution, greatly improving the solution efficiency and effectiveness. The MPC of the QPMPC obtains a current optimal strategy determined by online receding-horizon optimization for minimizing control errors. The QPMPC obtains higher prediction accuracy and better ability of power tracking. The QPMPC for GSOFC can save fuel consumption and improve the economy of renewable power generation . The QPMPC for GSOFC achieves higher fuel utilization of hydrogen (82.0\%) than MPC (80.0\%), PID (80.0\%), reinforcement learning (80.0\%), fuzzy control (80.0\%) and sliding mode control (79.7\%).},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Dongduan Liu},
  doi          = {10.1016/j.asoc.2023.110157},
  journal      = {Applied Soft Computing},
  pages        = {110157},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum parallel model predictive control for grid-connected solid oxide fuel cells},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goal distance-based UAV path planning approach, path
optimization and learning-based path estimation: GDRRT*, PSO-GDRRT* and
BiLSTM-PSO-GDRRT*. <em>ASOC</em>, <em>137</em>, 110156. (<a
href="https://doi.org/10.1016/j.asoc.2023.110156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The basic conditions for mobile robots to be autonomous are that the mobile robot localizes itself in the environment and knows the geometric structure of the environment (map). After these conditions are met, this mobile robot is given a specific task, but how the robot will navigate for this task is an important issue. Especially for Unmanned Aerial Vehicles (UAV), whose application has increased recently, path planning in a three-dimensional (3D) environment is a common problem. This study performs three experimental applications to discover the most suitable path for UAV in 3D environments with large and many obstacles. Inspired by Rapidly Random-Exploring Tree Star (RRT*), the first implementation develops the Goal Distance-based RRT* (GDRRT*) approach, which performs intelligent sampling taking into account the goal distance. In the second implementation, the path discovered by GDRRT* is shortened using Particle Swarm Optimization (PSO) (PSO-GDRRT*). In the final application, a network with a Bidirectional Long/Short Term Memory (BiLSTM) layer is designed for fast estimation of optimal paths found by PSO-GDRRT* (BiLSTM-PSO-GDRRT*). As a result of these applications, this study provides important novelties: GDRRT* converges to the goal faster than RRT* in large and obstacle-containing 3D environments. To generate groundtruth paths for training the learning-based network, PSO-GDRRT* finds the shortest paths relatively quickly. Finally, BiLSTM-PSO-GDRRT* provides extremely fast path planning for real-time UAV applications. This work is valuable for real-time autonomous UAV applications in a complex and large environment, as the new methods it offers have fast path planning capability.},
  archive      = {J_ASOC},
  author       = {Muhammet Fatih Aslan and Akif Durdu and Kadir Sabanci},
  doi          = {10.1016/j.asoc.2023.110156},
  journal      = {Applied Soft Computing},
  pages        = {110156},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Goal distance-based UAV path planning approach, path optimization and learning-based path estimation: GDRRT*, PSO-GDRRT* and BiLSTM-PSO-GDRRT*},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A vector visualization of uncertainty complementing the
traditional fuzzy approach with applications in project management.
<em>ASOC</em>, <em>137</em>, 110155. (<a
href="https://doi.org/10.1016/j.asoc.2023.110155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of recent technological developments and new computational and graphical possibilities, scientists and practitioners have become increasingly interested in studying how data and information should be presented. For instance, in project management, it is now recommended to employ dashboards instead of traditional reports. It is also believed that the usage of vectors, resembling the hands of a clock, may increase the efficiency and effectiveness of data presentation and information processing. In light of this, we propose a novel approach to visualization of uncertainty as defined by triangular fuzzy numbers. This new representation is based on vectors whose length represents the range of possible values of an uncertain parameter, while the slope reflects tendencies within possible scenarios. The mathematical foundations and definitions along with the basic properties of this approach are demonstrated in detail. In particular, we show how to transform triangular representations into vector ones and vice versa. The arithmetic operations of addition and multiplication by a crisp number on these vectors are demonstrated as well. Possible applications of the new vector visualization to project uncertainty representation and in project management are described. We also discuss both the advantages and disadvantages of our approach in relation to the traditional visualization as graphs of membership functions. Our proposal is complementary to the traditional one, and they should be used in combination. The new graphical representation of triangular fuzzy numbers expands the available toolbox for visualizing uncertainty not only in project management but in any other area.},
  archive      = {J_ASOC},
  author       = {Jan Schneider and Dorota Kuchta and Rafał Michalski},
  doi          = {10.1016/j.asoc.2023.110155},
  journal      = {Applied Soft Computing},
  pages        = {110155},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A vector visualization of uncertainty complementing the traditional fuzzy approach with applications in project management},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A graph neural network model for deciphering the biological
mechanisms of plant electrical signal classification. <em>ASOC</em>,
<em>137</em>, 110153. (<a
href="https://doi.org/10.1016/j.asoc.2023.110153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification methods of the electrical signal can be effectively applied for the identification of salt tolerance in plants. Biotechnology can help researchers discover genes and proteins related to plant electrical signals, thereby deciphering the biological mechanisms of electrical signal classification, but it is time-consuming and expensive only using biological technologies. Although protein interaction relationships can be predicted using deep learning , the relationships between plant electrical signals and proteins are unclear. This uncertainty can directly lead to protein information incompletion or lack of plant electrical signal data, which is not conducive to prediction performance. To this end, we propose a graph neural network model that integrates plant electrical signal features to discover electrical signal-related proteins (PMESP). In particular, the model constructs protein interaction relationships as a graph structure, where nodes represent proteins. To obtain sufficient feature information of these nodes, we develop an electrical signal feature extraction model based on a bidirectional long short-term memory and utilize a pre-trained neural network to extract the semantic information features of proteins. In the PMESP, the features of the electrical signals and proteins are fused as the input of graph convolutional neural network (GCN), the representations of pairs of nodes are calculated by multilayer GCNs, and then the new protein interaction relationships are predicted by the comparison of the scores between nodes connected by edges to the scores between any pair of nodes. We finally assemble a dataset containing the relationships between electrical signals and protein interactions, employ the method of light induced rhythmic bioelectrogenesis to collect data from Arabidopsis thaliana leaves under salt stress, and use weighted gene co-expression network analysis and crawler methods to obtain Arabidopsis thaliana protein interaction relationships. The results illustrate that the area under the receiver operating characteristic curve and area under precision-recall curve values of the model on the test set are 0.9624 and 0.9684 respectively, which are significantly higher than other link predicting models and previous existing methods.},
  archive      = {J_ASOC},
  author       = {Jiepeng Yao and Yi Ling and Peichen Hou and Zhongyi Wang and Lan Huang},
  doi          = {10.1016/j.asoc.2023.110153},
  journal      = {Applied Soft Computing},
  pages        = {110153},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A graph neural network model for deciphering the biological mechanisms of plant electrical signal classification},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An online learning algorithm for a neuro-fuzzy classifier
with mixed-attribute data. <em>ASOC</em>, <em>137</em>, 110152. (<a
href="https://doi.org/10.1016/j.asoc.2023.110152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {General fuzzy min–max neural network (GFMMNN) is one of the efficient neuro-fuzzy systems for data classification . However, one of the downsides of its original learning algorithms is the inability to handle and learn directly from mixed-attribute data without using encoding techniques. While categorical feature encoding methods can be used with the GFMMNN learning algorithms, they exhibit many shortcomings. Other improved approaches proposed in the literature are not suitable for online learning algorithms working in the dynamically changing environments without ability to retrain or access full historical data, which are usually required for many real world applications . This paper proposes an extended online learning algorithm for the GFMMNN. The proposed method can handle the datasets with both continuous and categorical features. It uses the change in the entropy values of categorical features of the samples contained in a hyperbox to determine if the current hyperbox can be expanded to include the categorical values of a new training instance. An extended architecture of the original GFMMNN and its new membership function are introduced for mixed-attribute data. Important mathematical properties of the proposed learning algorithms are also presented and proved in this paper. The extensive experiments confirmed superior and stable classification performance of the proposed approach in comparison to other relevant learning algorithms for the GFMM model.},
  archive      = {J_ASOC},
  author       = {Thanh Tung Khuat and Bogdan Gabrys},
  doi          = {10.1016/j.asoc.2023.110152},
  journal      = {Applied Soft Computing},
  pages        = {110152},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An online learning algorithm for a neuro-fuzzy classifier with mixed-attribute data},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated interval type-2 fuzzy rough technique for
emergency decision making. <em>ASOC</em>, <em>137</em>, 110150. (<a
href="https://doi.org/10.1016/j.asoc.2023.110150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to develop a hybrid emergency decision-making (EDM) method by combining best–worst method (BWM), multi-attributive border approximation area comparison (MABAC) and prospect theory (PT) in trapezoidal interval type-2 fuzzy rough (TrIT2FR) environment. In this hybrid method , the decision information is represented by trapezoidal interval type-2 fuzzy rough numbers (TrIT2FRNs). Firstly, this paper defines the TrIT2FRN and analyzes its desirable properties . Then, the TrIT2FR-BWM is developed to determine criteria weights. To develop the TrIT2FR-BWM , this paper completes the following three core issues: (i) propose an effective theorem to normalize the TrIT2FR weights; (ii) build a crisp programming model to transform the minmax objective of weight-determining model for the TrIT2FR-BWM ; (iii) design a consistency ratio for the TrIT2FR-BWM to check the reliability of the determined criteria weights. Afterwards, this paper extends the classical MABAC into TrIT2FR environment to calculate the border approximation area (BAA). Subsequently, the PT is used to rank the alternatives, in which the calculated BAA is selected as the reference point. Lastly, the validity of the proposed method is certificated with a real site selection case of makeshift hospitals on COVID-19. Sensitivity analysis and comparative analyses are conducted to illustrate the robustness and superiorities of the proposed method. Some valuable results are summarized as follows: (i) the best alternative determined by the proposed method conforms with the actual selection result, (ii) the proposed models in the TrIT2FR-BWM have strong robustness, (iii) PT is helpful to improve the decision quality of EDM.},
  archive      = {J_ASOC},
  author       = {Ze-hui Chen and Wen Luo},
  doi          = {10.1016/j.asoc.2023.110150},
  journal      = {Applied Soft Computing},
  pages        = {110150},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated interval type-2 fuzzy rough technique for emergency decision making},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Solving DC power flow problems using quantum and hybrid
algorithms. <em>ASOC</em>, <em>137</em>, 110147. (<a
href="https://doi.org/10.1016/j.asoc.2023.110147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power flow calculation plays an important role in planning, operation, and control of the power system . The quantum HHL algorithm can achieve theoretical exponential speedup over classical algorithms on DC power flow calculation. Since the qubit resources in the Noisy Intermediate-scale Quantum (NISQ) era are limited, it is important to discuss the performance considering this limitation. The phase estimation of DC power flow problems is imperfect. This work is carried out under the assumption of imperfect phase estimation. The performance of the HHL algorithm is systematically investigated with different accuracy and redundant qubits. In order to further reduce the required qubit resources, a hybrid quantum–classical algorithm is proposed. By comparing errors of the HHL and hybrid algorithms in the DC power flow calculation of the PJM 5-bus system, it is found that the hybrid algorithm can achieve comparable precision with fewer qubits than HHL by increasing the number of phase estimation modules, which may make the hybrid algorithm a feasible route in the NISQ era.},
  archive      = {J_ASOC},
  author       = {Fang Gao and Guojian Wu and Suhang Guo and Wei Dai and Feng Shuang},
  doi          = {10.1016/j.asoc.2023.110147},
  journal      = {Applied Soft Computing},
  pages        = {110147},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving DC power flow problems using quantum and hybrid algorithms},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of anti-tank guided missiles: An integrated fuzzy
entropy and fuzzy CoCoSo multi criteria methodology using technical and
simulation data. <em>ASOC</em>, <em>137</em>, 110145. (<a
href="https://doi.org/10.1016/j.asoc.2023.110145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Anti-Tank Guided Missiles (ATGMs) selection problem, a sub-problem of the Weapon Systems Selection Problems (WSSP), is an extremely important strategical level decision problem and is the main focus of this study. The study aims to determine a new methodology that represents accurate and reliable results from simulation model. This model decides on the anti-tank weapon system for various combat scenarios with high uncertainty. Data obtained from hundreds of simulation scenarios, with different combat conditions such as friendly and enemy forces, terrain, and environmental conditions, and technical weapon specifications is used to define solutions for the ATGM problem, such a solution proposal was not seen in any of the previous studies. For this aim, we proposed a comprehensive methodology consisting of 4 phases and 15 steps, using simulation and technical data, Fuzzy Shannon’s Entropy (F-Entropy) based on α α -level sets, and Fuzzy CoCoSo with Bonferroni (F-CoCoSo’B) methods for the selection of ATGMs to be used in the close quarter combat conditions. F-Entropy method is utilized for obtaining the criteria weights and F-CoCoSo’B is used for ranking the ATGM alternatives. 3 main and 22 sub-criteria have been determined by considering not only the technical data of the weapon systems but also data obtained from JCATS simulation tool. This combination of data sources results in criteria that reflects the combat environment more precisely, when compared to previous studies where only technical data or the experience of military experts were collected. The sensitivity analysis performed to test the validity and applicability of the proposed methodology approves the stability, robustness, and practicality of the methodology. The purpose of this paper is to present the proposed methodology, findings, and insights.},
  archive      = {J_ASOC},
  author       = {Hamit Erdal and Kemal Gurol Kurtay and Hakan Ayhan Dagistanli and Aygun Altundas},
  doi          = {10.1016/j.asoc.2023.110145},
  journal      = {Applied Soft Computing},
  pages        = {110145},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of anti-tank guided missiles: An integrated fuzzy entropy and fuzzy CoCoSo multi criteria methodology using technical and simulation data},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolving scheduling heuristics with genetic programming for
optimization of quality of service in weakly hard real-time systems.
<em>ASOC</em>, <em>137</em>, 110141. (<a
href="https://doi.org/10.1016/j.asoc.2023.110141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weakly hard real-time system model is used for describing the real-time systems that allow occasional violations of real-time timing constraints. These systems include real-time control systems, multimedia systems, and communication systems . In some approaches that deal with mitigating the system overload in real-time systems with periodic tasks, namely job-skipping algorithms, the constraints defined by the weakly hard real-time model are used as a mechanism for defining the pattern of task instances (jobs) that may be skipped in order to reduce the system load. The performance of these algorithms is usually evaluated with respect to the quality of service metric, which depends on the number of skipped jobs. In this work, we investigate the possibility of using genetic programming in the automated synthesis of scheduling heuristics for optimizing skipping patterns in order to increase the average quality of service in comparison with the conventional job-skipping algorithms. Using genetic programming to automatically synthesize heuristics allows for an easy and quick design of novel heuristics for various problem types and optimization criteria . We present two different approaches for implementing the proposed method. The first approach is to encapsulate the evolved heuristics into job-skipping algorithms known from the literature, namely Red Tasks as Late as Possible (RLP) and Blue When Possible (BWP). The idea of the second approach is to employ the evolved heuristics as standalone job-skipping algorithms. The results show an improvement of up to 15\% in comparison with the state-of-the-art algorithms. The novel methods described in this work present a significant upgrade of the standard job-skipping algorithms as they provide a notable improvement in terms of quality of service while ensuring the fulfillment of weakly hard constraints. Moreover, the presented methods are computationally efficient and are therefore suitable for implementation on real-time operating systems, which is not the case with similar methods based on optimization techniques.},
  archive      = {J_ASOC},
  author       = {Karla Salamun and Ivan Pavić and Hrvoje Džapo and Marko Đurasević},
  doi          = {10.1016/j.asoc.2023.110141},
  journal      = {Applied Soft Computing},
  pages        = {110141},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving scheduling heuristics with genetic programming for optimization of quality of service in weakly hard real-time systems},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An exploratory landscape analysis driven artificial bee
colony algorithm with maximum entropic epistasis. <em>ASOC</em>,
<em>137</em>, 110139. (<a
href="https://doi.org/10.1016/j.asoc.2023.110139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous real-valued optimization problems are existed in various engineering optimization scenarios. The artificial bee colony algorithm (ABC) which is inspired by the foraging behavior of bee colonies is an efficient optimization algorithm to address complex continuous real-valued optimization problems . In this paper, the maximum entropic epistasis (MEE) in the exploratory landscape analysis (ELA) is adopted in the ABC algorithm (MEEABC) to improve the performance of the ABC algorithm. The dimension interaction of the continuous functions computed by the MEE is introduced to guide the search process of the MEEABC algorithm during the employed bee phase and onlooker bee phase. The adaptive mutation methods and the strategy of dynamic population size reduction are implemented to increase the convergence speed and dynamically ameliorate the local exploitation capability. The solutions in the fitness landscape are automatically divided into different clusters to explore the local basin of the fitness landscape via the collaboration between MEE and adaptive mutation methods. The performance of MEEABC is tested on CEC 2017 benchmark test suite. From the experimental results, the MEEABC is superior to ABC variants and state-of-art algorithms regarded with the performance of effectiveness and robustness.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Zhenyu Wang and Ling Wang and Tianpeng Xu and Ningning Zhu and Jonrinaldi},
  doi          = {10.1016/j.asoc.2023.110139},
  journal      = {Applied Soft Computing},
  pages        = {110139},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An exploratory landscape analysis driven artificial bee colony algorithm with maximum entropic epistasis},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MOJI: Character-level convolutional neural networks for
malicious obfuscated JavaScript inspection. <em>ASOC</em>, <em>137</em>,
110138. (<a href="https://doi.org/10.1016/j.asoc.2023.110138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {JavaScript malware is one of the major threats to web security. A big challenge in detecting such malicious JavaScript is obfuscation, which transforms a program code into a harder-to-understand representation while preserving its original functionality. Many malicious JavaScript detection methods perform code abstraction and prior feature extraction to uncover the functionality hidden by obfuscation. However, such preprocessing steps significantly limit the detectors’ efficiency in practical situations. This paper presents Malicious Obfuscated JavaScript Inspector (MOJI), a novel method for malicious JavaScript detection, which requires no code abstraction or prior feature extraction. Instead, our detector directly accepts a sequence of characters in JavaScript code as an input and outputs its maliciousness score. Specifically, we design a character-level convolutional neural network consisting mainly of several 1D convolutional layers and fully connected layers. We evaluate the proposed method on a dataset composed of 24, 000 JavaScript codes and show that our method outperforms existing malicious JavaScript detectors in terms of both detection performance and running time. We also provide an analysis of the effect of additional obfuscation on the same dataset. Our results indicate that MOJI is far more robust to obfuscation than the existing methods and commercial antivirus software.},
  archive      = {J_ASOC},
  author       = {Minato Ishida and Naoshi Kaneko and Kazuhiko Sumi},
  doi          = {10.1016/j.asoc.2023.110138},
  journal      = {Applied Soft Computing},
  pages        = {110138},
  shortjournal = {Appl. Soft. Comput.},
  title        = {MOJI: Character-level convolutional neural networks for malicious obfuscated JavaScript inspection},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Load frequency control of multi-area multi-source system
with nonlinear structures using modified grasshopper optimization
algorithm. <em>ASOC</em>, <em>137</em>, 110135. (<a
href="https://doi.org/10.1016/j.asoc.2023.110135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, power systems are in direct contact with sources such as renewable energy sources , Electric Vehicles (EVs), distributed generation resources, etc. In other words, these sources impose additional imbalances on the system and disturb the balance between generation and consumption. Hence, this study aims to model multi-area multi-source systems with the presence of units such as thermal, hydro, diesel, and gas units, wind farms , EVs, and energy storage sources. Without a properly coordinated controller, they can expose the system to severe stress under unwanted disturbances and cause it to deviate from normal operating conditions. Therefore, after modeling the proposed system, a Proportional–Integral–Derivative (PID) controller based on a low-pass filter is presented as a widely used device in the industry. Since tuning coefficients of this controller based on trial and error were irrational, this issue turned to a frequency-based optimization problem with different operating points in unwanted operating conditions in order to have a robust design. In the previous articles, a fixed working point employed to design the controller. To cover this issue, modeling towards reality, and a wider range of unexpected conditions and events, ± ± 30\% design uncertainty and different loading conditions are considered in this study. Then, the developed Grasshopper Optimization Algorithm (GOA) with decreasing coefficients is applied to solve it. The time-varying coefficients can properly improve the local and global search capabilities. Different scenarios based on unwanted disturbances, uncertainties, and random load changes are considered to evaluate the system and algorithm performance. Numerical results obtained by analyzing comparative criteria on time and frequency domains indicated the proposed method had over 25\% better performance compared to other methods, on average. The results obtained from the frequency and time domains properly show that the settlement and overshoot and undershoot times of the system equipped with the proposed controller are less compared to other designs in the literature, and better stability has been provided in the s-plane. Also, Nyquist and Bode analyses have shown that the system can ensure optimal performance in a wide range of working points.},
  archive      = {J_ASOC},
  author       = {Sina Gouran-Orimi and Ali Ghasemi-Marzbali},
  doi          = {10.1016/j.asoc.2023.110135},
  journal      = {Applied Soft Computing},
  pages        = {110135},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Load frequency control of multi-area multi-source system with nonlinear structures using modified grasshopper optimization algorithm},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weighted error-output recurrent echo kernel state network
for multi-step water level prediction. <em>ASOC</em>, <em>137</em>,
110131. (<a href="https://doi.org/10.1016/j.asoc.2023.110131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With development of information techniques in navigation and shipping, machine learning algorithms are applied in enhancing navigation safety. One of critical areas, which attracts lots of attention from scientists and researchers, is water level prediction. Although randomization-based algorithms obtain good performance in water level prediction, these algorithms still have their own limitations. For example, unstable prediction problem caused by random weights selection, and the error accumulation problem caused by the conventional recurrent algorithm. In this study, we combine three proposed approach with the conventional echo state network . Firstly, the Gaussian kernel method transforms the input features into a high-dimensional features, which in some extent improves the forecasting accuracy . Secondly, kernel reservoir states are proposed. It not only abandons the random selected weights, but it also makes hidden neurons to connect closely. Lastly, a novel weighted error-output recurrent multi-step algorithm is proposed. It uses previous forecasting errors to update the current output weights in order to overcome the error accumulation problem. Based on above three approaches, a multi-step water level prediction model is proposed called Weighted Error-output Recurrent Echo Kernel State Network (WER-EKSN). The experimental results and statistical analysis represent that our proposed model has better forecasting performance than other compared models. It not only has the superior ability in water level prediction, but it also provides the evidences for the management of transportation in water-land, such as flood protection, and management of ship route.},
  archive      = {J_ASOC},
  author       = {Zongying Liu and Xiao Han Xu and Mingyang Pan and Chu Kiong Loo and Shaoxi Li},
  doi          = {10.1016/j.asoc.2023.110131},
  journal      = {Applied Soft Computing},
  pages        = {110131},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted error-output recurrent echo kernel state network for multi-step water level prediction},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A whale optimization algorithm with combined mutation and
removing similarity for global optimization and multilevel thresholding
image segmentation. <em>ASOC</em>, <em>137</em>, 110130. (<a
href="https://doi.org/10.1016/j.asoc.2023.110130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the problem of the whale optimization algorithm (WOA), such as weak local search ability, easy fall into local optimality , unable to better balance the exploration and exploitation abilities of the algorithm, a whale optimization algorithm with combined mutation and removing similarity (CRWOA) is proposed. Firstly, CRWOA proposed an adaptive adjustment method of the parameters A → A→ and C → C→ in the encircling prey stage and an improved bubble net attack update position formula, so that the updated whale is located around the prey, which improves the local search ability of the algorithm. Secondly, a roulette selection operator is proposed to randomly select individuals participating in position updates from the population, which improves the possibility of producing excellent individuals in the random search stage. Thirdly, a combined mutation operator based on individual fitness value is proposed to better balance the exploration and exploitation abilities of the algorithm. Finally, to better maintain the diversity of the population, an improved removing similarity operation is added. In addition, in the first set of experiments, the CEC 2017 benchmark test functions are selected to compare the performance of CRWOA with other group intelligent optimization algorithms . In the second group of experiments, Kapur and Otsu are used as objective functions to perform multilevel threshold segmentation on 10 grayscale images . The experimental results show that CRWOA is better than the five comparison algorithms in terms of convergence and segmentation quality .},
  archive      = {J_ASOC},
  author       = {Jiquan Wang and Jinling Bei and Haohao Song and Hongyu Zhang and Panli Zhang},
  doi          = {10.1016/j.asoc.2023.110130},
  journal      = {Applied Soft Computing},
  pages        = {110130},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A whale optimization algorithm with combined mutation and removing similarity for global optimization and multilevel thresholding image segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust learning algorithm based on agreement among soil
sampling techniques. <em>ASOC</em>, <em>137</em>, 110123. (<a
href="https://doi.org/10.1016/j.asoc.2023.110123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental investigation and modelling require case-based sampling techniques in various domains such as soil, air and water as well as living populations. In most cases, a limited number of sampling techniques can be conducted into a site stemming from the impracticability of geology, time and cost. In addition, if some outliers are recorded due to natural variability and the metrological issues, the modelling process is in need of robust analysis tools. Therefore, a robustness-based sampling agreement and vigorous estimations are needed. The primary purpose of this study is to provide a consensus between different soil sampling methods when a merging is required and to make reliable estimations in case of the existence of any outlier. A machine learning algorithm has been established for reaching targets by considering robustness, transparency, accuracy as well as reproducibility. The algorithm is suited for small data sets and all steps of the algorithm demonstrated that the robust learning algorithm is not severely influenced by the presence of a few outliers. The testing performed based on regression discontinuity analysis and comparative estimations also showed that repeated double robust regression outperforms the conventional multiple least-squares regression. Thus, the learning algorithm can be recommended to the fields of environmental sciences and also may be considered in different disciplines with minor adaptations.},
  archive      = {J_ASOC},
  author       = {Bulent Tutmez},
  doi          = {10.1016/j.asoc.2023.110123},
  journal      = {Applied Soft Computing},
  pages        = {110123},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust learning algorithm based on agreement among soil sampling techniques},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sustainable resilient recycling partner selection for urban
waste management: Consolidating perspectives of decision-makers and
experts. <em>ASOC</em>, <em>137</em>, 110120. (<a
href="https://doi.org/10.1016/j.asoc.2023.110120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sustainable waste supply chains, selecting recycling partners is an important factor in the decision-making process. Waste supply chains have undergone many fundamental modifications because of the rise of concepts such as sustainability, circular economy, and resilience. To overcome the current shortcomings of the literature on recycling partner selection only based on sustainability aspects, an evaluation framework is developed to address recycling partner selection by considering both sustainability and resilience factors. Although developing a sustainable and resilient evaluation framework improves the process of selecting recycling partners, the problem becomes very complex, and multidimensional decision-makers require reliable and accurate tools to make informed decisions. Multi-criteria decision-making (MCDM) methods are useful decision-making tools with high reliability to address problems under uncertainty. Although previous studies have developed several MCDM methods based on various uncertainty sets, the capability to support efficient and accurate group decision-making by decision-makers’ opinions and experts’ judgments has been a major disadvantage. Therefore, this study develops a novel decision-making approach using Z E E -numbers based on the best-worst method (Z E E -BWM) and a combined compromise solution (Z E E -CoCoSo). The proposed novel approach for addressing a sustainability and resilience management problem in an urban setting is demonstrated in a real-life case study using Tabriz, Iran as a case study. According to the results, net profit and the robustness of the waste supply chain are the most important criteria.},
  archive      = {J_ASOC},
  author       = {Gholamreza Haseli and Ali Ebadi Torkayesh and Mostafa Hajiaghaei-Keshteli and Sandra Venghaus},
  doi          = {10.1016/j.asoc.2023.110120},
  journal      = {Applied Soft Computing},
  pages        = {110120},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainable resilient recycling partner selection for urban waste management: Consolidating perspectives of decision-makers and experts},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive learning approach for customer churn prediction
in the telecommunication industry using evolutionary computation and
naïve bayes. <em>ASOC</em>, <em>137</em>, 110103. (<a
href="https://doi.org/10.1016/j.asoc.2023.110103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer churn is a complex challenge for burgeoning competitive organizations, especially in telecommunication. It refers to customers that swiftly leave a company for a competitor. Acquiring new customers has cost the telecommunication industry more than keeping existing customers. Traditionally, customer churn prediction (CCP) models are applied to aid in analyzing customer behavior and achieving prediction accuracy, which allows the telecommunication industry to target prior retention efforts toward them. However, only accurate CCP based on the available data or already trained supervised model is inadequate for efficient churn prediction, as existing approaches have not been shown or designed to learn with the skill of adaptation to respond quickly to changes in the customer behavior or a decision. Therefore, it is essential to design an approach that easily adapts to learn from new decision scenarios and provides instant insights. This study proposes an adaptive learning approach for this perplexing problem of CCP using the Naïve Bayes classifier with a Genetic Algorithm (subclass of an Evolutionary Algorithm) based feature weighting approach. Further, the performance of the proposed approach is evaluated on publicly available datasets (i.e., BigML Telco churn, IBM Telco, and Cell2Cell) which significantly enhances the prediction performance as compared to the baseline classifier (i.e., Naïve Bayes with default setting, Deep-BP-ANN, CNN, Neural Network , Linear Regression, XGBoost , KNN, Logit Boost, SVM, and PCALB methods) in terms of average precision of 0.97, 0.97, 0.98, a recall rate that stands at 0.84, 0.94, 0.97, and F1-score of 0.89, 0.96, 0.97, an MCC of 0.89, 0.96, 0.97, and accuracy 0.95, 0.97, 0.98 on subject datasets, respectively.},
  archive      = {J_ASOC},
  author       = {Adnan Amin and Awais Adnan and Sajid Anwar},
  doi          = {10.1016/j.asoc.2023.110103},
  journal      = {Applied Soft Computing},
  pages        = {110103},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive learning approach for customer churn prediction in the telecommunication industry using evolutionary computation and naïve bayes},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mathematical analysis of generative adversarial networks
based on complex picture fuzzy soft information. <em>ASOC</em>,
<em>137</em>, 110088. (<a
href="https://doi.org/10.1016/j.asoc.2023.110088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) are the models that generate data samples from the statistical distribution of the data. It is one of the most well-known branches of machine learning and deep learning . Different techniques are involved in the processing and production of visual data, which sometimes gives rise to misperception uncertainties. Bearing this issue in mind, we define some solid mathematical concepts to model and resolve the stated problem named complex picture fuzzy soft relations (CPFSRs) which is defined by the Cartesian product (CP) of two complex picture fuzzy soft sets (CPFSSs). The major objective of this study is to develop some innovative and useful notions that may be used to handle difficult and inconsistent information in practical situations. The proposed notion is foremost and superior to the prevailing ideas, where the presented idea is the improved technique of two different theories, named picture fuzzy set (PFS) and soft set (SS). Additionally, it presents the picture fuzzy soft set (PFSS) in professional decision-making by reducing complexions. The evaluated CPFSRs are the improved versions of soft relations, fuzzy relations, complex soft relations, and complex fuzzy relations. Therefore, this paper provides modeling methodologies based on CPFSRs which are used for the analysis of electing the best GAN for effective working. In the process, the score functions are also formulated and analyzed. Finally, a comparative study of existing techniques has been done to show the validity of the proposed work.},
  archive      = {J_ASOC},
  author       = {Naeem Jan and Jeonghwan Gwak and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2023.110088},
  journal      = {Applied Soft Computing},
  pages        = {110088},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mathematical analysis of generative adversarial networks based on complex picture fuzzy soft information},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flipped classroom supported by music combined with deep
learning applied in physical education. <em>ASOC</em>, <em>137</em>,
110039. (<a href="https://doi.org/10.1016/j.asoc.2023.110039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, multimedia teaching has basically become popular in education. As one of the teaching modes produced under the application of educational informatization, the flipped classroom has also attracted great attention from experts in the educational field in China since it was proposed in the United States and practiced overseas. The objective is to explore the application effect of flipped classrooms supported by music integrated with deep learning in physical education to improve the autonomous learning ability of students. Aiming at problems such as less teacher–student interaction, unskilled mastery of skills, and hidden safety hazards in physical education (PE) teaching, the new teaching mode of the flipped classroom is applied to PE teaching. In addition, deep learning and music are integrated to build a model for the application of flipped classrooms to physical education with the support of music integrated with deep learning. Finally, the control group, the experimental group M, and the experimental group N are set up through the control experimental design to compare and analyze the effect of the teaching mode proposed. The results show that the physical quality, mastery of motor skills, deep learning ability, and autonomous learning ability of middle school students in experimental group N with the support of music integrated with deep learning in the flipped classroom improved greatly, showing obvious differences compared with the control group and experimental group M ( P &lt; 0.05). Among them, the average scores of 50 meters running, sit-ups/pull-ups, 800/1000 meters, sitting forward bending, and cross-direction running reach 80.41, 82.35, 81.01, 79.79, and 79.83, respectively. It is found that the constructed system can effectively improve the physical quality, mastery of motor skills, deep learning ability, and autonomous learning ability in physical education teaching of students and can achieve the effect of deep learning. The results can provide an experimental reference for research on later adolescent education.},
  archive      = {J_ASOC},
  author       = {Zhen Zhu and Zhongqiu Xu and Jing Liu},
  doi          = {10.1016/j.asoc.2023.110039},
  journal      = {Applied Soft Computing},
  pages        = {110039},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Flipped classroom supported by music combined with deep learning applied in physical education},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decision-making model for failure modes and effect analysis
based on rough fuzzy integrated clouds. <em>ASOC</em>, <em>136</em>,
110148. (<a href="https://doi.org/10.1016/j.asoc.2023.110148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk assessment is one of the key topical subjects to identify and eradicate potential failures from system, design and structure at the early stage of production process. As different kinds of uncertainties such as diversity, randomness, vagueness coexist, single uncertainty based decision models are not sufficient to evaluate risk priority of failure modes. However, the existing mathematical approaches usually utilize single models ignoring the coexistence of intrapersonal and interpersonal uncertainties in experts’ judgements. The weights computed from relative importance of risk factors are essential information to capture the relation of various factors in decision system. Most of the current references do not consider the objective and subjective weights simultaneously which effects the rationality of results. In the present study, a novel linguistic assessment model , namely rough fuzzy integrated clouds (RFICs), is developed to handle uncertain information by integrating rough fuzzy numbers with cloud model theory. Then a significant decision support model for the analysis of failure modes is constructed based on RFICs, hybrid weighting method and Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) method. The positive and negative ideal solutions are determined using statistical deviations and arithmetic operations of clouds to rank the failure modes. The proposed model is implemented on a real-world application to detect the failures of steam valve system in a power generation plant. The simulation and sensitivity of the proposed model is also analyzed for different variations of control parameters. The proposed approach is compared with four existing approaches to show its effectiveness, out-performance and improved accuracy.},
  archive      = {J_ASOC},
  author       = {Musavarah Sarwar and Ghous Ali and Nauman Riaz Chaudhry},
  doi          = {10.1016/j.asoc.2023.110148},
  journal      = {Applied Soft Computing},
  pages        = {110148},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision-making model for failure modes and effect analysis based on rough fuzzy integrated clouds},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ANP-TOPSIS model for tourist destination choice problems
under temporal neutrosophic environment. <em>ASOC</em>, <em>136</em>,
110146. (<a href="https://doi.org/10.1016/j.asoc.2023.110146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making is a systematic process of solving real-world problems that provide an optimal solution after checking the achievable set of alternatives. As decision-making theory plays an increasingly important role in real life, there is more and more research on decision-making. All possible options are sorted through a multi-criteria decision-making system, allowing users to choose the suitable solution. In the face of many alternatives, the lack of appropriate skills and limited time has led to eventful human judgments. In this framework, the paper introduces the definition of a Temporal Complex neutrosophic set (TCNS) and the operations and properties of TCNS. Then, the TCNS-ANP-TOPSIS model based on TCNS is presented, which extends the ANP model to determine the weights for group multi-criteria and TOPSIS to rank alternatives collected from different time intervals. Further, to prove the feasibility and potentiality of the proposed model, a case study of choosing a tourist destination in Vietnam is described under the TCNS context. A comparative analysis with existing algorithms demonstrates the proposed model’s effectiveness, consistency, and availability.},
  archive      = {J_ASOC},
  author       = {Luong Thi Hong Lan and Do Thi Thu Hien and Nguyen Tho Thong and Florentin Smarandache and Nguyen Long Giang},
  doi          = {10.1016/j.asoc.2023.110146},
  journal      = {Applied Soft Computing},
  pages        = {110146},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ANP-TOPSIS model for tourist destination choice problems under temporal neutrosophic environment},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three-way decisions with fuzzy probabilistic covering-based
rough sets and their applications in credit evaluation. <em>ASOC</em>,
<em>136</em>, 110144. (<a
href="https://doi.org/10.1016/j.asoc.2023.110144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a generalization of equivalent class, fuzzy covering makes descriptions of the objective world more realistic, practical and accurate in some cases. In this paper, we establish four kinds of fuzzy probabilistic covering-based rough set, which combine the fuzzy covering-based neighborhood operator and the fuzzy probabilistic rough set, and then obtain the result of their three-way decision. The differences among the above models lie in the selection of thresholds, which further leads to different classification results . In addition, in order to interpret the thresholds rationally under different models, Bayesian decision procedure is adopted to construct the fuzzy probabilistic covering-based upper and lower approximation operators. Finally, a numerical example of the credit evaluation is applied to illustrate the validity of the proposed model.},
  archive      = {J_ASOC},
  author       = {Wei Li and Bin Yang},
  doi          = {10.1016/j.asoc.2023.110144},
  journal      = {Applied Soft Computing},
  pages        = {110144},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way decisions with fuzzy probabilistic covering-based rough sets and their applications in credit evaluation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A rough-set and AI based approach for hierarchical cognitive
processing of perceptions. <em>ASOC</em>, <em>136</em>, 110143. (<a
href="https://doi.org/10.1016/j.asoc.2023.110143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling the psychological and cognitive processes is an important issue in theoretical and clinical aspects. Most of the modellings concern much on qualitative and philosophical descriptions. This research resorts to quantitative modelling of human cognitive processing based on rough sets, AI , and mathematical approaches. We formalise psychological targets and properties by objects and features. A feature domain is associated with these objects and features. Based on the subjects’ perception of the values in the feature domain, the cognitive processing is proceeded via deterministic procedures and non-deterministic procedures. The non-deterministic procedures involve the setting of rough set approximations . To evaluate the parameters involved in the setting, we design several experiments and algorithm, in particular probabilistic structures and entropy, to evaluate the effects of these parameters. These provide a great degree of flexibility in practical applications. These approaches could be easily implemented by or combined with machine learning and artificial intelligence techniques. Our approach could be applied directly in clinical assessment and mental health probing.},
  archive      = {J_ASOC},
  author       = {Ray-Ming Chen},
  doi          = {10.1016/j.asoc.2023.110143},
  journal      = {Applied Soft Computing},
  pages        = {110143},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A rough-set and AI based approach for hierarchical cognitive processing of perceptions},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight network for portable fry counting devices.
<em>ASOC</em>, <em>136</em>, 110140. (<a
href="https://doi.org/10.1016/j.asoc.2023.110140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the number of fries plays a critical role in the maintenance of fish breeding, transportation, and the preservation of marine resources in aquaculture. Generally speaking, statistics are recorded manually by fishers and government units. Manual recording is time-consuming and increases the workload of fishers. Compared with traditional physical shunt devices, visual-based algorithms have benefits such as non-restriction of labors, minimal equipment installation, and maintenance costs. However, these methods generally come with massive calculations and model parameters, or poor abilities of aggregation handles and counting precision. This paper proposes a fry counting method named MSENet for portable fry counting devices. Firstly, the lightweight network is designed with simpler parameters (Params: 139.46 kB) for portable embedding. The visualized single-channel fry density maps are predicted by feeding the original images and the number of fries is calculated through integration. Then, the Squeeze-and-Excitation block is utilized to strengthen the features of weighty channels. The model training is refined by hyperparameter studies, the shortened preparation stage enhances the portability. What is more, a fry counting dataset NCAUF and an extra set NCAUF-EX are built for verifications of network generalization. The results demonstrate that the lightweight MSENet outperforms in fry counting with higher precision and competently solves the issue of fry aggregation (MAE: 3.33). The source code and pre-trained models are available at: https://github.com/vranlee/MSENet .},
  archive      = {J_ASOC},
  author       = {Weiran Li and Qian Zhu and Hanyu Zhang and Ziyu Xu and Zhenbo Li},
  doi          = {10.1016/j.asoc.2023.110140},
  journal      = {Applied Soft Computing},
  pages        = {110140},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight network for portable fry counting devices},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diversity-driven automated web API recommendation based on
implicit requirements. <em>ASOC</em>, <em>136</em>, 110137. (<a
href="https://doi.org/10.1016/j.asoc.2023.110137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of the web API sharing community, Mashup development has become a popular way for software developers to quickly create Mashup applications . Mashup development allows developers to quickly implement the functionality they want by combining a chosen set of web APIs, which greatly improves development efficiency. However, the existence of numerous web API candidates make it difficult to select the appropriate API quickly. Most existing automated web API recommendation approaches focus on the developer’s description of requirements but typically overestimate the developer’s ability to fully describe their application’s requirements, thus ignoring their implicit and diversified functional requirements. In this paper, we propose a novel automated approach called DI-RAR to retrieve and recommend web API groups for Mashup creation. Specifically, we formulate an automated web API recommendation task as a nondeterministic polynomial problem. First, a self-attention model assigns weights to query to distinguish the core and non-core requirements. then Dynamic planning retrieval generates steiner trees to retrieve API groups and uncovers strongly related implicit requirements to enrich the mashup’s functions. Finally we apply simhash technique to filter similar results and finally provide diverse API groups. experiments based on a real-world dataset are performed to demonstrate the feasibility and efficiency of DI-RAR.},
  archive      = {J_ASOC},
  author       = {Huaizhen Kou and Jian Xu and Lianyong Qi},
  doi          = {10.1016/j.asoc.2023.110137},
  journal      = {Applied Soft Computing},
  pages        = {110137},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diversity-driven automated web API recommendation based on implicit requirements},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end learning of representations for instance-level
document image retrieval. <em>ASOC</em>, <em>136</em>, 110136. (<a
href="https://doi.org/10.1016/j.asoc.2023.110136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance-level document image retrieval plays a vital role in many document image processing systems. An appropriate image representation is of paramount importance for effective retrieval. To this end, we propose an image representation that is well-suited for the instance-level document image retrieval task. A novel end-to-end three-stream Siamese network is presented to learn the image representation, which accepts a triplet: a query image, its matching image and its non-matching image. The network is trained to jointly minimize two types of loss: ranking loss and classification loss. By employing the ranking loss, the distance between the representations of the query image and its matching image can be explicitly forced to be smaller than that between the query image and its non-matching image. Besides, each stream of the network is further extended as a classification model to fully exploit the supervised information of each individual image. The cross-entropy loss is then employed for the classification model. After training, an arbitrary image can be fed to either stream of the network to generate its representation. Extensive comparison and ablation experiments on three datasets have demonstrated the effectiveness of the proposed image representation. The two types of loss have been shown to complement each other.},
  archive      = {J_ASOC},
  author       = {Li Liu and Yue Lu and Ching Y. Suen},
  doi          = {10.1016/j.asoc.2023.110136},
  journal      = {Applied Soft Computing},
  pages        = {110136},
  shortjournal = {Appl. Soft. Comput.},
  title        = {End-to-end learning of representations for instance-level document image retrieval},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding robust influential seeds from networked systems
against structural failures using a niching memetic algorithm.
<em>ASOC</em>, <em>136</em>, 110134. (<a
href="https://doi.org/10.1016/j.asoc.2023.110134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a growing interest in analyzing the dynamics and characteristics of networked systems, and the influence maximization problem is a recent hotspot to determine influential seeds from nodal members. Widespread applicable values can be found in marketing and information digging tasks, which boost more attention on related studies. Several diffusion models and seed determination techniques have been developed. Meanwhile, networked systems are threatened by sabotages and structural perturbances, and recent literature indicated that the robustness of the diffusion process is significant in daily applications, which can be modeled as the robust influence maximization problem. However, existing studies have not touched upon the node-based attack and its impact on the performance of seeds; corresponding robust seeds determination strategies are also lacked. For solving the robust influence maximization problem considering node-based attacks, a numerical measure is designed in this paper to assess the performance of given seeds. A sensitivity analysis is also conducted to test the impairing effect of several nodal importance metrics. Further, guided by the proposed measure, a memetic algorithm with the niching strategy has been devised to search for seeds with robust influential ability, termed NMA-RIM. Experiments on synthetic and real-world networks validate the competitiveness of NMA-RIM over existing algorithms, and the improved efficiency achieved by the niching strategy is also demonstrated. Meanwhile, the difference between selection results guided by the proposed measure and previous ones is shown via empirical analysis. As demonstrated by the obtained results, NMA-RIM outperforms existing approaches, and a better computational efficiency can be achieved. Competitive candidates can be provided for decision makers to solve information diffusion dilemmas.},
  archive      = {J_ASOC},
  author       = {Shuai Wang and Xiaojun Tan},
  doi          = {10.1016/j.asoc.2023.110134},
  journal      = {Applied Soft Computing},
  pages        = {110134},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Finding robust influential seeds from networked systems against structural failures using a niching memetic algorithm},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pruning feedforward small-world neural network by dynamic
sparse regularization with smoothing l1/2 norm for nonlinear system
modeling. <em>ASOC</em>, <em>136</em>, 110133. (<a
href="https://doi.org/10.1016/j.asoc.2023.110133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming to solve the problem of the relatively large architecture for the small-world neural network and improve its generalization ability , we propose a pruning feedforward small-world neural network based on a dynamic regularization method with the smoothing l 1/2 l1/2 norm (PFSWNN-DSRL 1/2 1/2 ) and apply it to nonlinear system modeling. A feedforward small-world neural network is first constructed by the rewiring rule of Watts–Strogatz. By minimizing the modified error function added with a smoothing l 1/2 l1/2 norm, redundant weights are pruned to generate a sparse architecture. A dynamic adjusting strategy is further designed for the regularization strength to balance the tradeoff between the training accuracy and the sparsity . Several experiments are carried out to evaluate the performance of the proposed PFSWNN-DSRL 1/2 1/2 on nonlinear system modeling. The results show that the PFSWNN-DSRL 1/2 1/2 can achieve the satisfactory modeling accuracy with an average of 17\% pruned weights. The comparative results demonstrate that the generalization performance of the proposed model is improved by 8.1\% relative to the baseline method (FSWNN) but with a sparse structure, and the pruning does not degenerate its small-world property.},
  archive      = {J_ASOC},
  author       = {Wenjing Li and Minghui Chu},
  doi          = {10.1016/j.asoc.2023.110133},
  journal      = {Applied Soft Computing},
  pages        = {110133},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A pruning feedforward small-world neural network by dynamic sparse regularization with smoothing l1/2 norm for nonlinear system modeling},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage meta-heuristic for part-packing and
build-scheduling problem in parallel additive manufacturing.
<em>ASOC</em>, <em>136</em>, 110132. (<a
href="https://doi.org/10.1016/j.asoc.2023.110132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the part-packing and build-scheduling problem in parallel additive manufacturing . The multi-part grouping, 2D placement, and 3D rotation should be determined in the part-packing problem. The build allocation and sequencing should be determined in the build-scheduling problem. The objective function is to minimize the makespan. A mixed integer linear programming (MILP) model is developed to find the optimal solution. Then, we propose a two-stage meta-heuristic that can decompose the proposed problem into the part-packing and the build-scheduling stages. In this paper, the two-stage meta-heuristic is applied to using a genetic algorithm (GA) and particle swarm optimization (PSO). To verify the performance of the two-stage meta-heuristic, two types of computational experiments are conducted. In the small-sized instance experiments, the two-stage meta-heuristic compares with the MILP model, and it shows a good performance. In the large-sized instance experiments, we compare two single-stage meta-heuristics applied to using the genetic algorithm and particle swarm optimization . The two-stage meta-heuristic shows the best performance among the proposed meta-heuristics. To analyze the experimental results and extract insights from the two-stage meta-heuristic, we conduct the robustness, impact of part rotation, and sensitivity analyses.},
  archive      = {J_ASOC},
  author       = {Seung Jae Lee and Byung Soo Kim},
  doi          = {10.1016/j.asoc.2023.110132},
  journal      = {Applied Soft Computing},
  pages        = {110132},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage meta-heuristic for part-packing and build-scheduling problem in parallel additive manufacturing},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Union nonparallel support vector machines framework with
consistency. <em>ASOC</em>, <em>136</em>, 110129. (<a
href="https://doi.org/10.1016/j.asoc.2023.110129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though there are several dozens of nonparallel support vector machines (NSVMs), little studies on general forms and characteristics of NSVMs are investigated. To fill in this gap, this paper categorizes the existing NSVMs into two types and reveals the advantages and defects of different types of them. In particular, inconsistency problems for these models are pointed out and discussed. Based on this observation, this paper further proposes and investigates a novel max–min distance-based nonparallel support vector machine (NSVM) with desired consistency. Compared with the existing methods, the proposed NSVM has the consistency of training and test and the consistency of metric. In addition, NSVM also assigns each sample an ascertained loss, which not only is able to identify if a data sample being classified correctly, but also makes NSVM completely in line with its decision rule. NSVM can be easily extended to its nonlinear version, and both of them are effectively solved through a modified proximal difference-of-convex algorithm with extrapolation algorithm. Experimental results on the artificial dataset, benchmark datasets and a real-world dataset support the advantages of the proposed model.},
  archive      = {J_ASOC},
  author       = {Chun-Na Li and Yuan-Hai Shao and Huajun Wang and Ling-Wei Huang and Yu-Ting Zhao and Naihua Xiu and Nai-Yang Deng},
  doi          = {10.1016/j.asoc.2023.110129},
  journal      = {Applied Soft Computing},
  pages        = {110129},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Union nonparallel support vector machines framework with consistency},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big model and small model: Remote modeling and local
information extraction module for medical image segmentation.
<em>ASOC</em>, <em>136</em>, 110128. (<a
href="https://doi.org/10.1016/j.asoc.2023.110128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few years, convolutional neural networks (CNN) and Transformer have achieved great success in medical image segmentation , but they each have inevitable drawbacks. Among them, convolution operation is difficult to calculate the relationship between elements at a certain position and elements far away from the position in the feature map. However, Transformer tends to ignore the importance of local information when exploring the correlation between overall elements. In order to allow the network to acquire both the ability to explore local details and compute the correlation between distant elements, this paper proposes TransUNet++ based on TransUNet. Specifically, this paper proposes two modules, Big Model and Small Model, to explore the element relationship between feature maps. Among them, on the basis of the whole feature map as the basic unit, the Big model can not only calculate the correlation between distant elements in the feature map but also extract the detailed information of the local feature map. On the basis of taking 1/4 of the feature map as the basic unit, the Small model not only explores the correlation between distant elements but also extracts the local details of the feature map. We demonstrate on the Synapse multi-organ segmentation dataset(Synapse) and Automated cardiac diagnosis challenge dataset (ACDC) that using either the Big Model or the Small Model alone can improve the experimental results, and using the Big Model and the Small Model in parallel can achieve more optimal experimental results. Among them, in Synapse dataset, we achieved 80.87\% dice score and 24.79\% HD score, and in ACDC dataset, we achieved 91.41\% dice score and 1.08\% HD score.},
  archive      = {J_ASOC},
  author       = {Lianghui Xu and Liejun Wang and Yongming Li and Anyu Du},
  doi          = {10.1016/j.asoc.2023.110128},
  journal      = {Applied Soft Computing},
  pages        = {110128},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Big model and small model: Remote modeling and local information extraction module for medical image segmentation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-adaptive neuroevolution approach to constructing deep
neural network architectures across different types. <em>ASOC</em>,
<em>136</em>, 110127. (<a
href="https://doi.org/10.1016/j.asoc.2023.110127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroevolution has greatly promoted Deep Neural Network (DNN) architecture design and its applications, while there is a lack of methods available across different DNN types concerning both their scale and performance. In this study, we propose a self-adaptive neuroevolution (SANE) approach to automatically construct various lightweight DNN architectures for different tasks. One of the key settings in SANE is the search space defined by cells and organs self-adapted to different DNN types. Based on this search space, a constructive evolution strategy with uniform evolution settings and operations is designed to grow DNN architectures gradually. SANE is able to self-adaptively adjust evolution exploration and exploitation to improve search efficiency. Moreover, a speciation scheme is developed to protect evolution from early convergence by restricting selection competition within species. To evaluate SANE, we carry out neuroevolution experiments to generate different DNN architectures including convolutional neural network , generative adversarial network and long short-term memory. The results illustrate that the obtained DNN architectures could have smaller scale with similar performance compared to existing DNN architectures. Our proposed SANE provides an efficient approach to self-adaptively search DNN architectures across different types. Our Code is available at https://doi.org/10.24433/CO.2989985.v1 .},
  archive      = {J_ASOC},
  author       = {Zhenhao Shuai and Hongbo Liu and Zhaolin Wan and Wei-Jie Yu and Jun Zhang},
  doi          = {10.1016/j.asoc.2023.110127},
  journal      = {Applied Soft Computing},
  pages        = {110127},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-adaptive neuroevolution approach to constructing deep neural network architectures across different types},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Development and research of triangle-filter convolution
neural network for fuel reloading optimization of block-type HTGRs.
<em>ASOC</em>, <em>136</em>, 110126. (<a
href="https://doi.org/10.1016/j.asoc.2023.110126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of fuel reloading optimization is very demanding, which requires to search for the optimal suitable core configuration within a very huge solution space. To solve this problem, updating of optimization solutions and quantitative evaluation of core configurations are two main steps. With the development of computing science, the meta-heuristic algorithms (MHAs) turn out to be effective for updating optimization solutions; while the artificial neural networks (ANNs) proves to be practical for the other. However, there is few research for the application of convolution neural networks (CNNs). For any given image-like inputs, CNNs could quickly calculate the outputs mainly by convolution operators, using rectangle filters to scan the full inputs. However, the geometry of some reactors is not rectangle so that using rectangle filters fails to traverse. Aiming at solving the problems of fuel reloading optimization for block-type HTGRs , the triangle-filter convolution neural networks (TFCNNs) are developed. At the same time, uniform method and ring-ranked method are also developed for determining the weights of TFCNNs. Combined with genetic algorithm and particle swarm optimization , two wide-applied MHAs, TFCNNs are applied to solve the problems of fuel reloading optimization for block-type HTGRs . The results show that the TFCNNs could greatly predict the optimization parameters of testing data set with accuracy higher than 90\%. In addition, comparing to DRAGON V4 program, the calculation time of TFCNNs is even less than 1.30\% of it, while comparing to BP-ANNs, the ratio of feasible solutions by TFCNNs is evidently higher.},
  archive      = {J_ASOC},
  author       = {Zhan Li and Jincheng Wang and Jie Huang and Ming Ding},
  doi          = {10.1016/j.asoc.2023.110126},
  journal      = {Applied Soft Computing},
  pages        = {110126},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development and research of triangle-filter convolution neural network for fuel reloading optimization of block-type HTGRs},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal fake news detection through data
augmentation-based contrastive learning. <em>ASOC</em>, <em>136</em>,
110125. (<a href="https://doi.org/10.1016/j.asoc.2023.110125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the information exploding era, news can be created or edited purposely for promoting the spreading of social influence. However, unverified or fabricated news can also spread unscrupulously, leading to serious consequences, such as poor decisions or even health risk. Thus, in order to discriminate the fake news, several fake news detection approaches have been proposed and the majority of these methods suffer from low efficacy of detection, due to the lack of multimodal information and the small data size. Hence, we develop a novel machine learning based framework, i.e., BERT-based back- T ranslation T ext and E ntire-image multimodal model with C ontrastive learning (TTEC). In this framework, the text of news is first back-translated encouraging the framework to learn some general characteristics regarding a particular topic. Secondly, both textual and visual features are fed into a BERT-based model in order to produce multimodal features. Thirdly, the contrastive learning is utilized to derive more reasonable multimodal representations through utilizing similar news published in the past. Eventually, to demonstrate the effectiveness of the proposed framework, extensive experiments are conducted and the results show our method outperforms the state of art methods by 3.1\% on Mac. F1 scores.},
  archive      = {J_ASOC},
  author       = {Jiaheng Hua and Xiaodong Cui and Xianghua Li and Keke Tang and Peican Zhu},
  doi          = {10.1016/j.asoc.2023.110125},
  journal      = {Applied Soft Computing},
  pages        = {110125},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multimodal fake news detection through data augmentation-based contrastive learning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AVFakeNet: A unified end-to-end dense swin transformer deep
learning model for audio–visual​ deepfakes detection. <em>ASOC</em>,
<em>136</em>, 110124. (<a
href="https://doi.org/10.1016/j.asoc.2023.110124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the field of machine learning and social media platforms facilitate the creation and rapid dissemination of realistic fake content (i.e., images, videos, audios). Initially, the fake content generation involved the manipulation of either audio or video streams but currently, more realistic deepfakes content is being produced via modifying both audio–visual streams. Researchers in the field of deepfakes detection mostly focus on identifying fake videos exploiting solely visual or audio modality. However, there exist a few approaches for audio–visual deepfakes detection but mostly are not evaluated on a multimodal dataset with deepfakes videos having the manipulations in both streams. The unified approaches evaluated on the audio–visual deepfakes dataset have reported low detection accuracies and failed when the faces are side-posed. Therefore, in this paper, we introduced a novel AVFakeNet framework that focuses on both the audio and visual modalities of a video for deepfakes detection. More specifically, our unified AVFakeNet model is a novel Dense Swin Transformer Net (DST-Net) which consists of an input block, feature extraction block, and output block. The input and output block comprises dense layers while the feature extraction block employs a customized swin transformer module. We have performed extensive experimentation on five different datasets (FakeAVCeleb, Celeb-DF, ASVSpoof-2019 LA, World Leaders dataset, Presidential Deepfakes dataset) comprising audio, visual, and audio–visual deepfakes along with a cross-corpora evaluation to signify the effectiveness and generalizability of our unified framework. Experimental results highlight the effectiveness of the proposed framework in terms of accurately detecting deepfakes videos via scrutinizing both the audio and visual streams.},
  archive      = {J_ASOC},
  author       = {Hafsa Ilyas and Ali Javed and Khalid Mahmood Malik},
  doi          = {10.1016/j.asoc.2023.110124},
  journal      = {Applied Soft Computing},
  pages        = {110124},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AVFakeNet: A unified end-to-end dense swin transformer deep learning model for audio–visual​ deepfakes detection},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-regularization sparse reconstruction based on
multifactorial multiobjective optimization. <em>ASOC</em>, <em>136</em>,
110122. (<a href="https://doi.org/10.1016/j.asoc.2023.110122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent sparse reconstruction, the sparsity and reconstruction error can be considered as two objectives and tackled by multiobjective optimization methods. Since the sparse reconstruction problem can be modeled in multiple regularization forms, it can be addressed in a multifactorial multiobjective optimization paradigm by the evolutionary multitasking approach to transfer useful information across multiple regularization forms to help solve the problem. In this paper, a multi-regularization based on multifactorial multiobjective optimization is proposed to solve the sparse reconstruction problem. First, the sparse reconstruction problem is constructed as a multi-regularization model. Then, this model is optimized by a multifactorial multiobjective optimization method. In the evolutionary process, considering the priority of different regularization in the multi-regularization model, a preference-based selection method is designed. In addition, to accommodate the sparsity characteristic of the sparse reconstruction problem, a sparsity-oriented crossover operator is performed. Finally, an iterative-thresholding-based local search is incorporated into the algorithm to improve the convergence performance. Experiments on multiple datasets and image reconstruction tasks demonstrate the effectiveness and practicality of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Wencheng Han and Hao Li and Maoguo Gong},
  doi          = {10.1016/j.asoc.2023.110122},
  journal      = {Applied Soft Computing},
  pages        = {110122},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-regularization sparse reconstruction based on multifactorial multiobjective optimization},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tools for landscape analysis of optimisation problems in
procedural content generation for games. <em>ASOC</em>, <em>136</em>,
110121. (<a href="https://doi.org/10.1016/j.asoc.2023.110121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The term Procedural Content Generation (PCG) refers to the (semi-)automatic generation of game content by algorithmic means, and its methods are becoming increasingly popular in game-oriented research and industry. A special class of these methods, which is commonly known as search-based PCG, treats the given task as an optimisation problem . Such problems are predominantly tackled by evolutionary algorithms . We will demonstrate in this paper that obtaining more information about the defined optimisation problem can substantially improve our understanding of how to approach the generation of content. To do so, we present and discuss three efficient analysis tools, namely diagonal walks, the estimation of high-level properties, as well as problem similarity measures. We discuss the purpose of each of the considered methods in the context of PCG and provide guidelines for the interpretation of the results received. This way we aim to provide methods for the comparison of PCG approaches and eventually, increase the quality and practicality of generated content in industry.},
  archive      = {J_ASOC},
  author       = {Vanessa Volz and Boris Naujoks and Pascal Kerschke and Tea Tušar},
  doi          = {10.1016/j.asoc.2023.110121},
  journal      = {Applied Soft Computing},
  pages        = {110121},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tools for landscape analysis of optimisation problems in procedural content generation for games},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable AI in big data intelligence of community
detection for digitalization e-healthcare services. <em>ASOC</em>,
<em>136</em>, 110119. (<a
href="https://doi.org/10.1016/j.asoc.2023.110119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender Systems are designed to analysis the available data in the system to predict user’s desires and provide appropriate personalized suggestions to each user that suits their interests. In this paper, we have developed an explainable medical recommender system that uses graph concepts to provide an interpretable approach to medical data. The presented approach is based on community detection algorithms . It forms a graph between the users based on their similarity scores. Individuals with common interests are then grouped using graph community detection algorithms . Two community detection algorithms have been applied on the graphs of users and physicians in our medical recommender system. The results of applying two community detection algorithms are then used to address the cold start problem. We have identified the most influential users using a graph-based technique that finds the overlapping communities. We claim that using the overlapping graph of communities to address cold start problem will enhance the accuracy of the recommendations. Weighting or voting systems are also applied on the selected users to give feedback to potential consumers where there are n different options in a cluster. The similarity score of the users in the overlapping communities has been used to weight the final recommendation. The accuracy of recommended services depends on the proper selection of target populations. The proposed approach outperforms the use of each one of the community detections separately. The accuracy and precision of the proposed method are 93.06 and 88.34, which exceed the highest achieved accuracy in the literature.},
  archive      = {J_ASOC},
  author       = {Arun Kumar Sangaiah and Samira Rezaei and Amir Javadpour and Weizhe Zhang},
  doi          = {10.1016/j.asoc.2023.110119},
  journal      = {Applied Soft Computing},
  pages        = {110119},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable AI in big data intelligence of community detection for digitalization e-healthcare services},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate long-term air temperature prediction with machine
learning models and data reduction techniques. <em>ASOC</em>,
<em>136</em>, 110118. (<a
href="https://doi.org/10.1016/j.asoc.2023.110118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, three customised Artificial Intelligence (AI) frameworks, considering Deep Learning , Machine Learning (ML) algorithms and data reduction techniques, are proposed for a problem of long-term summer air temperature prediction. Specifically, the prediction of the average air temperature in the first and second August fortnights, using input data from previous months, at two different locations (Paris, France) and (Córdoba, Spain), is considered. The target variable, mainly in the first August fortnight, can contain signals of extreme events such as heatwaves , like the heatwave of 2003, which affected France and the Iberian Peninsula. Three different computational frameworks for air temperature prediction are proposed: a Convolutional Neural Network (CNN), with video-to-image translation, several ML approaches including Lasso regression, Decision Trees and Random Forest , and finally a CNN with pre-processing step using Recurrence Plots, which convert time series into images. Using these frameworks, a very good prediction skill has been obtained in both Paris and Córdoba regions, showing that the proposed approaches can be an excellent option for seasonal climate prediction problems.},
  archive      = {J_ASOC},
  author       = {D. Fister and J. Pérez-Aracil and C. Peláez-Rodríguez and J. Del Ser and S. Salcedo-Sanz},
  doi          = {10.1016/j.asoc.2023.110118},
  journal      = {Applied Soft Computing},
  pages        = {110118},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Accurate long-term air temperature prediction with machine learning models and data reduction techniques},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy dynamic MCDM method based on PRSRV for financial risk
evaluation of new energy vehicle industry. <em>ASOC</em>, <em>136</em>,
110115. (<a href="https://doi.org/10.1016/j.asoc.2023.110115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The financial risk evaluation of new energy vehicle (NEV) industry is conducive to the popularization of NEVs, to encourage more investment into the NEV industry, and to promote the improvement of the risk control system. When evaluating the performance of NEV industry in China, it is usually full of uncertainty and dynamic. q-Rung orthopair fuzzy set (q-ROFS) has the characteristics of non-membership and membership with adjustable parameterization q , which is a very effective mathematical model to capture uncertainty. In this paper, the q-rung orthopair fuzzy (q-ROF) distance measure based triangle orthocenter is given. Then, q-ROF score function (SF) based distance measure is proposed for disposing of comparison issue. Moreover, we present nonlinear comprehensive weighting method by integrating subjective weight information and objective weight information (determining by water-filling theory). In order to solve the counter-intuitive phenomena and dynamic trend issue, the dynamic q-ROF aggregation operators are investigated and their properties are proved. Whereafter, q-ROF multi-criteria decision making (MCDM) approach based projection ranking by similarity to referencing vector (PRSRV) is proposed for evaluating financial risk of NEV industry, along with the sensitivity analysis. Finally, a comparison with some existing MCDM methods states that the presented method has strong data adaptability.},
  archive      = {J_ASOC},
  author       = {Xindong Peng and Hai-Hui Huang and Zhigang Luo},
  doi          = {10.1016/j.asoc.2023.110115},
  journal      = {Applied Soft Computing},
  pages        = {110115},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy dynamic MCDM method based on PRSRV for financial risk evaluation of new energy vehicle industry},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary machine learning builds smart education big
data platform: Data-driven higher education. <em>ASOC</em>,
<em>136</em>, 110114. (<a
href="https://doi.org/10.1016/j.asoc.2023.110114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of machine learning has promoted the construction of smart education platforms. It is of great significance to deeply investigate the usage of machine learning techniques in smart education. In this paper, we explore how to utilize evolutionary algorithms and machine learning algorithms to build a smart education big data platform to promote the intelligent development of higher education and better assist the establishment of the smart education system. We combine evolutionary algorithms and machine learning models to build a personalized course recommendation model for the smart education big data platform, which uses deep belief networks and swarm intelligence evolutionary algorithms to recommend relevant content based on the interests of learners. We take advantage of the feature extraction of deep belief networks and combine supervised learning and unsupervised learning to design an intelligent recommendation model for teaching content. At the same time, we utilize the advantages of evolutionary algorithms to tune the model parameters to obtain the best parametric model . We compare our method with other methods on public dataset to show the model performance.},
  archive      = {J_ASOC},
  author       = {Lu Zheng and Cong Wang and Xue Chen and Yihang Song and Zihan Meng and Ru Zhang},
  doi          = {10.1016/j.asoc.2023.110114},
  journal      = {Applied Soft Computing},
  pages        = {110114},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary machine learning builds smart education big data platform: Data-driven higher education},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A learning-based metaheuristic administered positioning
model for 3D IoT networks. <em>ASOC</em>, <em>136</em>, 110113. (<a
href="https://doi.org/10.1016/j.asoc.2023.110113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-dimensional (3D) IoT networks are among the most contemporary and ubiquitous networks in the real world, which consist of a wide variety of components such as smartphones, smart cars, smart watches, wireless sensors , smart flying objects, etc. The objects’ data are typically gathered, transferred, and processed in these networks for particular intentions. Evidently, the data is worthless without knowing the location of its source. The GPS is the simplest way to locate; however, in environments such as deep forests, underwater networks, underground, multi-story buildings, etc., GPS is not applicable. Likewise, recruiting GPS does not cost- and energy-efficient. Hence, a new positioning system is presented in the current paper to locate objects in 3D IoTs. The Slime Mold Algorithm (SMA) in the proposed method is initially modified and then hybridized with the Equilibrium Optimizer (EO). Moreover, a learning strategy is employed to determine and select the optimal algorithm for each iteration. Furthermore, a neighborhood search strategy is included in the proposed algorithm to enhance search efficiency. Next, the Received Signal Strength Indicator (RSSI) is integrated with the proposed algorithm. Eventually, to assess the proposed algorithm’s proficiency and competency of the contributions, the proposed algorithm is applied to fifteen 3D IoTs, and the results are compared with AEO, AO, EO, MRFO, SMA, WOA , PSO , and SSA statistically and visually. The experimental results portend the superiority of the proposed method over competitor algorithms.},
  archive      = {J_ASOC},
  author       = {Saeid Barshandeh and Shima Koulaeizadeh and Mohammad Masdari and Benyamin AbdollahZadeh and Mahsa Ghasembaglou},
  doi          = {10.1016/j.asoc.2023.110113},
  journal      = {Applied Soft Computing},
  pages        = {110113},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A learning-based metaheuristic administered positioning model for 3D IoT networks},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An application of generative adversarial networks to improve
automatic inspection in automotive manufacturing. <em>ASOC</em>,
<em>136</em>, 110105. (<a
href="https://doi.org/10.1016/j.asoc.2023.110105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In manufacturing systems , the quality of inspection is a critical issue. This can be conducted by humans or by employing Computer Vision Systems ( CV S CV S ), which are trained upon representative datasets of images to detect classes of defects that may occur. The construction of such datasets strongly limits the use of CV S CV S methods, as the variety of defects has combinatorial nature. Alternatively, instead of recognizing defects, a system can be trained to detect non-defective standards, becoming appropriate for some application profiles. In automotive manufacturing, for example, parts are assembled within a reduced set of correct combinations, while the amount of possible incorrect assembling is enormous. This paper integrates a Generative Adversarial Network ( GAN GAN ) within the CV S CV S framework used by Renault/Brazil to improve the detection of defective production in its automotive assembly line. By sparing the construction of expensive defect image datasets, our solution has proved to be cost-effective and more efficient in comparison with the current CV S CV S solution to detect defects, besides generalizing better to inspect different components without any modification in the method.},
  archive      = {J_ASOC},
  author       = {Joceleide D.C. Mumbelli and Giovanni A. Guarneri and Yuri K. Lopes and Dalcimar Casanova and Marcelo Teixeira},
  doi          = {10.1016/j.asoc.2023.110105},
  journal      = {Applied Soft Computing},
  pages        = {110105},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An application of generative adversarial networks to improve automatic inspection in automotive manufacturing},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On augmenting topological graph representations for
attributed graphs. <em>ASOC</em>, <em>136</em>, 110104. (<a
href="https://doi.org/10.1016/j.asoc.2023.110104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representations based on embedding methods allow for easier analysis of the network structure and can be used for a variety of tasks, such as link prediction and node classification . These methods have been shown to be effective in a variety of settings and have become an important tool in the field of graph learning. These methods are easy to implement, and their predictions yield interpretable results. However, most graph embedding methods rely solely on graph structural information and do not consider node/edge attributes, limiting their applicability. In this paper, we propose graph-theoretic designs to incorporate node and edge attributes within the topology, enabling graph-embedding methods to seamlessly work on attributed graphs. To find ideal representation for a given attributed graph, we propose augmenting special subgraph structures within original network. We discuss the potential challenges of the proposed approach and prove some of its theoretical limitations. We test the efficacy of our approach by comparing state-of-the-art graph classification models on 15 standard bioinformatics datasets. We observe an encouraging improvement of up to 5\% in classification accuracy on the augmented graphs compared to the results on the original graphs.},
  archive      = {J_ASOC},
  author       = {Anwar Said and Mudassir Shabbir and Saeed-Ul Hassan and Zohair Raza Hassan and Ammar Ahmed and Xenofon Koutsoukos},
  doi          = {10.1016/j.asoc.2023.110104},
  journal      = {Applied Soft Computing},
  pages        = {110104},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On augmenting topological graph representations for attributed graphs},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective evolutionary algorithm with decomposition
and the information feedback for high-dimensional medical data.
<em>ASOC</em>, <em>136</em>, 110102. (<a
href="https://doi.org/10.1016/j.asoc.2023.110102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional medical data often leads to a phenomenon known as the ”curse of dimensionality, ” which causes additional memory and high training costs, as well as degrading the generalization capacity of learning algorithms. To address this issue, a multi-objective evolutionary algorithm that integrates decomposition and the information feedback model (IFMMOEAD) is proposed for high-dimensional medical data. This algorithm not only considers the number of selected features, but also classification accuracy and correlation measures of features when feature dimensionality reduction is executed. The property of IFMMOEAD is first verified by standard benchmarks DTLZ1–DTLZ7. Then, it is used to develop machine learning algorithms for thirty-five high-dimensional cancer gene expression data sets , showing excellent potential for high-dimensional medical machine learning . Finally, the IFMMOEAD is applied to empirical clinical data of multiple myeloma, significantly outperforming existing algorithms in terms of normalized mutual information and adjusted rand index metrics. We suggest that this algorithm could be implemented in medical information systems as a promising technique for high-dimensional medical problems.},
  archive      = {J_ASOC},
  author       = {Mingjing Wang and Ali Asghar Heidari and Huiling Chen},
  doi          = {10.1016/j.asoc.2023.110102},
  journal      = {Applied Soft Computing},
  pages        = {110102},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective evolutionary algorithm with decomposition and the information feedback for high-dimensional medical data},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Superiority combination learning distributed particle swarm
optimization for large-scale optimization. <em>ASOC</em>, <em>136</em>,
110101. (<a href="https://doi.org/10.1016/j.asoc.2023.110101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale optimization problems (LSOPs) have become increasingly significant and challenging in the evolutionary computation (EC) community. This article proposes a superiority combination learning distributed particle swarm optimization (SCLDPSO) for LSOPs. In algorithm design , a master–slave multi-subpopulation distributed model is adopted, which can obtain the full communication and information exchange among different subpopulations, further achieving the diversity enhancement. Moreover, a superiority combination learning (SCL) strategy is proposed, where each worse particle in the poor-performance subpopulation randomly selects two well-performance subpopulations with better particles for learning. In the learning process, each well-performance subpopulation generates a learning particle by merging different dimensions of different particles, which can fully combine the superiorities of all the particles in the current well-performance subpopulation. The worse particle can significantly improve itself by learning these two superiority combination particles from the well-performance subpopulations, leading to a successful search. Experimental results show that SCLDPSO performs better than or at least comparable with other state-of-the-art large-scale optimization algorithms on both CEC2010 and CEC2013 large-scale optimization test suites, including the winner of the competition on large-scale optimization. Besides, the extended experiments with increasing dimensions to 2000 show the scalability of SCLDPSO. At last, an application in large-scale portfolio optimization problems further illustrates the applicability of SCLDPSO.},
  archive      = {J_ASOC},
  author       = {Zi-Jia Wang and Qiang Yang and Yu-Hui Zhang and Shu-Hong Chen and Yuan-Gen Wang},
  doi          = {10.1016/j.asoc.2023.110101},
  journal      = {Applied Soft Computing},
  pages        = {110101},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Superiority combination learning distributed particle swarm optimization for large-scale optimization},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection using information gain and decision
information in neighborhood decision system. <em>ASOC</em>,
<em>136</em>, 110100. (<a
href="https://doi.org/10.1016/j.asoc.2023.110100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a significant preprocessing technique for data mining, which can promote the accuracy of data classification and shrink feature space by eliminating redundant features. Since traditional feature selection algorithms have high time complexity and low classification accuracy , an effective algorithm using Information Gain and decision information is designed. The algorithm introduces Information Gain for performing preliminary dimensionality reduction on high dimensional datasets, and then the decision information is regarded as an evaluation function of features to select features with important information. First, the concept of joint information granule is defined, and neighborhood information entropy measures are proposed based on the joint information granule. In addition, the relationship between these measures is studied, which is helpful to study the uncertainty in data. Second, a nonmonotonic algorithm using the decision information in the neighborhood information entropy measures is proposed to overcome the shortcoming of algorithms based on monotonic evaluation functions, thereby improving the accuracy of data classification . Third, to reduce the time cost of the designed algorithm for high dimensional datasets, Information Gain is introduced to preliminarily eliminate irrelevant features in high dimensional datasets. Finally, the ablation and comparison experiments on twelve public datasets demonstrate the low time cost and high classification accuracy of our algorithm, respectively.},
  archive      = {J_ASOC},
  author       = {Kanglin Qu and Jiucheng Xu and Qincheng Hou and Kangjian Qu and Yuanhao Sun},
  doi          = {10.1016/j.asoc.2023.110100},
  journal      = {Applied Soft Computing},
  pages        = {110100},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection using information gain and decision information in neighborhood decision system},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A shallow hybrid classical–quantum spiking feedforward
neural network for noise-robust image classification. <em>ASOC</em>,
<em>136</em>, 110099. (<a
href="https://doi.org/10.1016/j.asoc.2023.110099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Convolutional Neural Network (CNN)-based image classification systems are often susceptible to noise interruption, i.e. , minor image noise may significantly impact the outcome. On the contrary, classical Spiking Neural Network (SNN) is known for handling noisy data due to the stochastic and temporal behaviour of the spiking neuron signals. However, it is not always feasible to train the weights of the classical SNN due to stochastic and non-differentiable spiking signals. Recent applications of quantum machine learning to stochastic modelling have predicted that quantum computing would prove to be a significant advantage, hence catapulting the study of quantum and neuromorphic computing to a new level of development. Motivated by these observations, this paper introduces a shallow hybrid classical–quantum spiking feedforward neural network referred to as Spiking Quantum Neural Network (SQNN) for dealing with a robust image classification task in the presence of noise and adversarial attacks . The proposed SQNN offers the inherent capabilities of processing unforeseen noisy test images due to its spatial and temporal information. An efficient variational quantum circuit training with the help of a standard back-propagation algorithm obviates the classical Spiking Time-Dependent Plasticity (STDP) and Spike-Prop algorithms, which are often found inefficient in training the feedforward SNN. The proposed SQNN is extensively tested and benchmarked on the PennyLane Quantum Simulator. Experimental results show that the proposed SQNN model supersedes the feedforward SNN (SFNN), Random Quantum Neural Networks (RQNN), AlexNet and ResNet-18 on unseen test images with added noise from the FashionMNIST, MNIST, KMNIST, CIFAR10, and ImageNet datasets.},
  archive      = {J_ASOC},
  author       = {Debanjan Konar and Aditya Das Sarma and Soham Bhandary and Siddhartha Bhattacharyya and Attila Cangi and Vaneet Aggarwal},
  doi          = {10.1016/j.asoc.2023.110099},
  journal      = {Applied Soft Computing},
  pages        = {110099},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A shallow hybrid classical–quantum spiking feedforward neural network for noise-robust image classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Corrigendum to “kernel-based learning and feature selection
analysis for cancer diagnosis” [appl. Soft comput. 51 (2017) 39–48].
<em>ASOC</em>, <em>136</em>, 110098. (<a
href="https://doi.org/10.1016/j.asoc.2023.110098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Seyyid Ahmed Medjahed and Tamazouzt Ait Saadi and Abdelkader Benyettou and Mohammed Ouali},
  doi          = {10.1016/j.asoc.2023.110098},
  journal      = {Applied Soft Computing},
  pages        = {110098},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Kernel-based learning and feature selection analysis for cancer diagnosis” [Appl. soft comput. 51 (2017) 39–48]},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Corrigendum to “gray wolf optimizer for hyperspectral band
selection” [appl. Soft comput. 40 (2016) 178–186]. <em>ASOC</em>,
<em>136</em>, 110097. (<a
href="https://doi.org/10.1016/j.asoc.2023.110097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {S.A. Medjahed and T. Ait Saadi and A. Benyettou and M. Ouali},
  doi          = {10.1016/j.asoc.2023.110097},
  journal      = {Applied Soft Computing},
  pages        = {110097},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Gray wolf optimizer for hyperspectral band selection” [Appl. soft comput. 40 (2016) 178–186]},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “a new hybrid SSA-TA: Salp swarm algorithm
with threshold accepting for band selection in hyperspectral images”
[appl. Soft comput. 95 (2020) 106534]. <em>ASOC</em>, <em>136</em>,
110096. (<a href="https://doi.org/10.1016/j.asoc.2023.110096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Seyyid Ahmed Medjahed and Mohammed Ouali},
  doi          = {10.1016/j.asoc.2023.110096},
  journal      = {Applied Soft Computing},
  pages        = {110096},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “A new hybrid SSA-TA: Salp swarm algorithm with threshold accepting for band selection in hyperspectral images” [Appl. soft comput. 95 (2020) 106534]},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pareto front grid guided multi-objective evolutionary
algorithm. <em>ASOC</em>, <em>136</em>, 110095. (<a
href="https://doi.org/10.1016/j.asoc.2023.110095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multi-objective optimization problems with irregular Pareto Fronts , most widely used decomposition methods in MOEA/D (multi-objective evolutionary algorithms based on decomposition) have shown to be lack of the ability to balance the diversity and the convergence to track the true Pareto Fronts during the search. This research investigates the recently proposed grid-based decomposition methods , which reflect the inherent characteristics of the neighborhood structure in the solution to address the issues of diversity and convergence. The performance of the grid-based decomposition method, however, depends on the size of its grid segmentation, and its time complexity increases with the number of grids. In order to improve the computational efficiency, we propose a new concept of Pareto Front grid to guide the search in MOEA. In order to reduce the computing time, a new nadir point estimation strategy based on statistical analysis has been proposed to estimate the whole population. In addition, based on the idea of knee point, a novel grid-based knee point selection method is proposed in the environmental selection of the next generation. Finally, a grid-based decomposition multi-objective evolutionary algorithm with Pareto Front Grid (PFG-MOEA) is proposed. Extensive experimental analysis demonstrates the effectiveness of the proposed PFG-MOEA against state-of-the-art multi-objective evolution algorithms. As the extension of the CDG-MOEA (Constrained Decomposition approach with Grids MOEA) algorithm in the literature, PFG-MOEA can obtain better performance by consuming much less computing time.},
  archive      = {J_ASOC},
  author       = {Ying Xu and Huan Zhang and Lei Huang and Rong Qu and Yusuke Nojima},
  doi          = {10.1016/j.asoc.2023.110095},
  journal      = {Applied Soft Computing},
  pages        = {110095},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A pareto front grid guided multi-objective evolutionary algorithm},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to stop undesired propagations by using bi-level genetic
algorithms. <em>ASOC</em>, <em>136</em>, 110094. (<a
href="https://doi.org/10.1016/j.asoc.2023.110094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we introduce a general model to analyse any type of propagation system, whether it represents the spread of a fire, fake news, a virus, etc. We study the computational complexity of the problem of minimising the impact of a propagation by cutting some of the connections it can spread through. Even limiting the scope of our cutting strategy to be short-term, we show that the problem is Σ 2 P Σ2P - complete , that is, it is one level above NP - complete in the complexity hierarchy. Intuitively, in Σ 2 P Σ2P - complete problems a hard search for the value fulfilling some property is tackled, but just evaluating that property for each candidate value requires performing another hard, independent search. This complexity suggests that a good method to deal with the problem under consideration is a two-level genetic algorithm , that is, a genetic algorithm that uses another genetic algorithm as fitness function: the former algorithm searches for good candidates for solving the target optimisation, and for each candidate within its population, its fitness is calculated by running the latter algorithm. We apply this implementation to two case studies, compare its results with those of greedy and minimax algorithms, and report experimental results. Our results indicate that bi-level genetic algorithms are good candidates to deal with Σ 2 P Σ2P - complete problems.},
  archive      = {J_ASOC},
  author       = {Javier Galiana and Ismael Rodríguez and Fernando Rubio},
  doi          = {10.1016/j.asoc.2023.110094},
  journal      = {Applied Soft Computing},
  pages        = {110094},
  shortjournal = {Appl. Soft. Comput.},
  title        = {How to stop undesired propagations by using bi-level genetic algorithms},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A differential evolutionary chromosomal gene expression
programming technique for electronic nose applications. <em>ASOC</em>,
<em>136</em>, 110093. (<a
href="https://doi.org/10.1016/j.asoc.2023.110093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intelligent system applications require automated data-driven modeling tools. The performance consistency of modeling tools is very essential to reduce the need for human intervention. Classical Gene Expression Programmings (GEPs) employ predefined genetic rules for the node-based evolution of expression trees in the absence of optimal numerical values of constant terminals, and these shortcomings can limit the search efficiency of expression trees. To alleviate negative impacts of these limitations on the data-driven GEP modeling performance, a Differential Evolutionary Chromosomal GEP (DEC-GEP) algorithm is suggested. The DEC-GEP utilizes the Differential Evolution (DE) algorithm for the optimization of a complete genotype of expression trees. For this purpose, a modifier gene container, which stores numerical values of constant terminals, is appended to the frame of GEP chromosome, and this modified chromosome structure enables simultaneous optimization of expression tree genotypes together with numerical values of constant terminals. Besides, the DEC-GEP algorithm can benefit from exploration and exploitation capabilities of the DE algorithm for more efficient evolution of GEP expression trees. To investigate consistency of the DEC-GEP algorithm in a data-driven modeling application, an experimental study was conducted for soft calibration of the low-cost, solid-state sensor array measurements, and results indicated that the DEC-GEP could yield dependable CO concentration estimation models for electronic nose applications.},
  archive      = {J_ASOC},
  author       = {Davut Ari and Baris Baykant Alagoz},
  doi          = {10.1016/j.asoc.2023.110093},
  journal      = {Applied Soft Computing},
  pages        = {110093},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A differential evolutionary chromosomal gene expression programming technique for electronic nose applications},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An interval type-2 fuzzy ORESTE method for waste-to-energy
plant site selection: A case study in china. <em>ASOC</em>,
<em>136</em>, 110092. (<a
href="https://doi.org/10.1016/j.asoc.2023.110092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serving as a promising solution to manage municipal solid waste (MSW), the waste-to-energy (WTE) system has recently received increasing attention. The first and critical step of constructing a WTE system is selecting a sustainable plant site. To support the site selection of the WTE system, this paper proposes a novel interval type-2 fuzzy decision-making framework. First, a sustainable evaluation index system is established by considering multiple criteria from several dimensions. Second, to deal with the heterogeneous information formats under different criteria, two information transformation mechanisms are developed. Through the proposed information transformation mechanisms , the heterogeneous information formats are unified into interval type-2 fuzzy sets. Then, a new interval type-2 fuzzy distance model is proposed to enhance Besson’s mean ranks in ORESTE method. Based on the proposed distance model, an interval type-2 fuzzy ORESTE method is developed to analyze the conflicts between the alternative sites and select the most desirable one. Finally, an example about the site selection of the WTE system is provided to illustrate the implementation process of our proposal, meanwhile, the comparative analysis and discussions are conducted to verify its superiority and flexibility.},
  archive      = {J_ASOC},
  author       = {Xiao-Hong Pan and Ying-Ming Wang and Shi-Fan He and Álvaro Labella and Luis Martínez},
  doi          = {10.1016/j.asoc.2023.110092},
  journal      = {Applied Soft Computing},
  pages        = {110092},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interval type-2 fuzzy ORESTE method for waste-to-energy plant site selection: A case study in china},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mission planning on preference-based expression trees using
heuristics-assisted evolutionary computation. <em>ASOC</em>,
<em>136</em>, 110090. (<a
href="https://doi.org/10.1016/j.asoc.2023.110090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mission planning problem has so far been solved by using temporal logic and classic planning approaches that have an exponential computational complexity and do not always account for the transition costs. Variants of the Vehicle Routing Problem can use meta-heuristics to give a near-optimal solution; however, have a limited representation capability. In this paper the problem of giving complex instructions (mission) to a robot in the form of an expression tree with AND, OR and THEN operators is studied. This allows users to instruct the robots for everyday tasks like getting a variety of items (modeled by AND) with a few choices (modeled by OR), the picks having temporal constraints (modeled by THEN). Preferences can be added to make the robot solve a sub-mission earlier than another sub-mission. The paper first proposes an algorithm to make a greedy solution by a single parse of the expression tree. Thereafter, the solution is optimized by using a Genetic Algorithm that optimizes the branch to be taken by the OR operator and the ordering of operations. Experimental results are demonstrated on the Pioneer LX robot programmed using the Robot Operating System. The experimental results further show that the proposed approach significantly beats a Genetic Algorithm solution and a Dynamic Programming greedy solution for problem sizes that cannot be solved by using the model verification and exhaustive search techniques.},
  archive      = {J_ASOC},
  author       = {Rahul Kala},
  doi          = {10.1016/j.asoc.2023.110090},
  journal      = {Applied Soft Computing},
  pages        = {110090},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mission planning on preference-based expression trees using heuristics-assisted evolutionary computation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An integrated personalized decision approach with
probabilistic linguistic context for grading restaurants in india.
<em>ASOC</em>, <em>136</em>, 110089. (<a
href="https://doi.org/10.1016/j.asoc.2023.110089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online reviews from the web are rich data sources that promote tourism analytics. Restaurants make a significant contribution to the growth of tourism in India. The literature studies on restaurant selection show that most decision frameworks do not handle uncertainty effectively and pay subtle attention to heterogeneous sources. Additionally, the extant models (i) cannot accept missing entries and its imputation; (ii) reliability of data source agents are not methodically determined; (iii) attributes’ interactions are not properly considered; and (iv) personalized restaurant ranking is unavailable. The research problem considered in this study is the rational selection of restaurants based on online reviews from heterogeneous sources to support travelers in the tourism process. The main objective of this study is to circumvent the challenge in the literatures by proposing a novel integrated decision framework that collects data from heterogeneous rating sources and transforms them into ‘probabilistic linguistic information (PLI)’, which effectively handles uncertainty by relating occurrence probability to each linguistic term . Due to the uncertain nature of online reviews, missing data are inevitable. For rational imputation of data, a case-based method is proposed. Later, the relative significance of each attribute and the reliability of each rating source are determined using ‘criteria importance through intercriteria correlation (CRITIC)’ and Dempster–Shafer-based Bayesian approximation methods. Furthermore, the PLIs from each source are aggregated by using the newly proposed discriminative weighted Muirhead mean operator. Personalized prioritization of restaurants is achieved by using the newly proposed probabilistic linguistic comprehensive (PLC) method that acquires expectation queries from customers. Lastly, the practicality of the developed framework is testified by a real-case example of restaurant selection based on the data collected from online sources via web crawlers . Results infer that (i) the proposed framework is innovative/original, personalized, significant, and mitigates human intervention compared to the extant models, (ii) robust in terms of ranking of restaurants even after adequate weight alterations, and (iii) finally, supports stakeholders to effectively plan their tourism process and attain win-win conditions for effective growth of hospitality sector.},
  archive      = {J_ASOC},
  author       = {Raghunathan Krishankumar and Arunodaya Raj Mishra and K.S. Ravichandran and Samarjit Kar and Amir H. Gandomi and Romualdas Bausys},
  doi          = {10.1016/j.asoc.2023.110089},
  journal      = {Applied Soft Computing},
  pages        = {110089},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated personalized decision approach with probabilistic linguistic context for grading restaurants in india},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Urban hotspot forecasting via automated spatio-temporal
information fusion. <em>ASOC</em>, <em>136</em>, 110087. (<a
href="https://doi.org/10.1016/j.asoc.2023.110087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban hotspot forecasting is one of the most important tasks for resource scheduling and security in future smart cities . Most previous works employed fixed neural architectures based on many complicated spatial and temporal learning modules. However, designing appropriate neural architectures is challenging for urban hotspot forecasting. One reason is that there is currently no adequate support system for how to fuse multi-scale spatio-temporal information rationally by integrating different spatial and temporal learning modules. Another one is that the empirical fixed neural architecture is difficult to adapt to different data scenarios from different domains or cities. To address the above problems, we propose a novel framework based on neural architecture search for urban hotspot forecasting, namely Automated Spatio-Temporal Information Fusion Neural Network (ASTIF-Net). In the search space of our ASTIF-Net, normal convolution and graph convolution operations are adopted to capture spatial geographic neighborhood dependencies and spatial semantic neighborhood dependencies, and different types of temporal convolution operations are adopted to capture short-term and long-term temporal dependencies. In addition to combining spatio-temporal learning operations from different scales, ASTIF-Net can also search appropriate fusion methods for aggregating multi-scale spatio-temporal hidden information. We conduct extensive experiments to evaluate ASTIF-Net on three real-world urban hotspot datasets from different domains to demonstrate that our proposed model can obtain effective neural architectures and achieve superior performance (about 5\% ∼ 10\% 5\%∼10\% improvements) compared with the existing state-of-art baselines.},
  archive      = {J_ASOC},
  author       = {Guangyin Jin and Hengyu Sha and Zhexu Xi and Jincai Huang},
  doi          = {10.1016/j.asoc.2023.110087},
  journal      = {Applied Soft Computing},
  pages        = {110087},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Urban hotspot forecasting via automated spatio-temporal information fusion},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Training generative adversarial networks by auxiliary
adversarial example regulator. <em>ASOC</em>, <em>136</em>, 110086. (<a
href="https://doi.org/10.1016/j.asoc.2023.110086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many variant Generative Adversarial Networks (GANs) have been proposed to address the problem that models are difficult to be trained, such as a network-based model, loss-based method, and training-based technique. However, these models rarely improve training stability by reducing the instability of the generator and discriminator simultaneously. For this purpose, inspired by the idea of network regulation, we design an auxiliary adversarial example regulator and propose a new training framework of GANs. In this method, to reduce the instability of the generator and discriminator simultaneously, we design a penalty to constrain directly and guide the generator to generate images, and gradually adjust the training of the discriminator by the auxiliary adversarial example regulator. With the designed constraint and discriminator, the generated image gets closer to the real image. Finally, experimental results demonstrate that the proposed method outperforms the baseline models . The code is available at https://github.com/AdleyGan/GAN-AE-P .},
  archive      = {J_ASOC},
  author       = {Yan Gan and Mao Ye and Dan Liu and Yiguang Liu},
  doi          = {10.1016/j.asoc.2023.110086},
  journal      = {Applied Soft Computing},
  pages        = {110086},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Training generative adversarial networks by auxiliary adversarial example regulator},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weighted mean of vectors optimization algorithm and its
application in designing the power system stabilizer. <em>ASOC</em>,
<em>136</em>, 110085. (<a
href="https://doi.org/10.1016/j.asoc.2023.110085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate design of the power system stabilizer (PSS) models is a crucial issue due to their significant impact on the stability of power system operation . However, identifying the parameters of a PSS model is a challenging task owing to its nonlinearity and multi-modality characteristics. Due to such characteristics, handling algorithms may be prone to stagnation in local optima. Therefore, this paper proposes a potent integrated optimization algorithm by comprising the weIghted meaN oF vectOrs (INFO) optimizer with chaotic-orthogonal based learning (COBL) and Gaussian bare-bones (GBB) strategies, named INFO-GBB, for achieving the optimal parameters of a PSS model used in a single-machine infinite-bus (SMIB) system. In the INFO-GBB, the COBL aims to enhance the searching capability to explore new regions using the orthogonal design aspect and thus improving the diversity of solutions. Also, the GBB is adopted to assist the algorithm to perform an immediate vicinity of the best solution and thus enhances the exploitation capabilities. The effectiveness and efficacy of the INFO-GBB algorithm is validated on CEC 2020 benchmark suits and the designing task of the PSS model. The achieved results by the INFO-GBB are compared with eighteen well-known algorithms. The statistical verifications along with the Friedman test have ascertained that the INFO-GBB is capable of achieving promising performances compared to the other counterparts. The results obtained based on the Friedman test illustrate that the INFO-GBB offers superior performance over the state-of-the-art algorithms as it outperforms fifteen out of eighteen algorithms by an average rank greater than 61\% for benchmark problems while outperforming O-LSHADE, LSHADE, and TSA algorithms by 25\%, 33\%, and 58\%, respectively. Furthermore, the applicability of the INFO-GBB is realized through designing the PSS model used in a SMIB system. The obtained results indicate that the INFO-GBB algorithm exhibits accurate and superior performance compared to other peers as it provides the lowest value for the integral of time multiplied absolute error (ITAE) performance index which is used as an objective function. For example, the achieved results of the mean ITAE found by INFO-GBB is 1.36E−03 with improvement percentages of 24.93\%, 19.78\%, 13.04\%, 26.64\%, and 24.86\%, over the LSHADE, GWO , EO, RSA, and original INFO algorithms, respectively. Therefore, the INFO-GBB can efficiently affirm its superiority and stability to deal with the function optimization task and parameters’ estimation of the PSS model.},
  archive      = {J_ASOC},
  author       = {Václav Snášel and Rizk M. Rizk-Allah and Davut Izci and Serdar Ekinci},
  doi          = {10.1016/j.asoc.2023.110085},
  journal      = {Applied Soft Computing},
  pages        = {110085},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted mean of vectors optimization algorithm and its application in designing the power system stabilizer},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measures for evaluating IT2FSs constructed from data
intervals. <em>ASOC</em>, <em>136</em>, 110084. (<a
href="https://doi.org/10.1016/j.asoc.2023.110084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval type-2 fuzzy sets (IT2FSs) are increasingly used in fuzzy systems for modeling linguistic words. Various approaches exist to construct such IT2FSs. Although the performance of a fuzzy system is significantly influenced by the quality of the utilized IT2FSs however, currently there are no objective criteria for evaluating IT2FSs. This paper introduces a novel objective measure that enables assessing IT2FSs and helps to select the most appropriate ones. The introduced measure is the aggregation of two sets of sub-measures devised to appraise the sufficiency of support and suitability of membership grades . These measures can be effective on the IT2FSs that are built using a set of data intervals collected from a group of subjects. An application with well-defined experiments is devised and implemented to assess the validity of the proposed measure. Experiments are conducted using IT2FSs constructed by the three well-known approaches, i.e., Interval Approach (IA), Enhanced Interval Approach (EIA), and Hao–Mendel Approach (HMA). It is shown that the application performance with the IT2FSs constructed by the three approaches is in accordance with the calculated measures.},
  archive      = {J_ASOC},
  author       = {Kazem Baratimehr and Mohammad Reza Moosavi and Hooman Tahayori},
  doi          = {10.1016/j.asoc.2023.110084},
  journal      = {Applied Soft Computing},
  pages        = {110084},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Measures for evaluating IT2FSs constructed from data intervals},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An artificial bee colony algorithm with a balance strategy
for wireless sensor network. <em>ASOC</em>, <em>136</em>, 110083. (<a
href="https://doi.org/10.1016/j.asoc.2023.110083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular population based evolutionary computation algorithm, artificial bee colony (ABC) algorithms have attracted many researchers to do further works. In this paper, by utilizing a linkage detection strategy to distinguish nonseparable and separable functions, we present new balance strategies for the updating equations of ABC on the two types of functions respectively. First, for accelerating convergence rate, we propose a historic array to preserve the best individual which the population ever achieved. Second, for the different role of employed and onlooker bees during iterations, compared with the traditional algorithm, we present two updating equations for the two types of bees on the nonseparable and separable functions respectively. Third, a new multi-dimensional updating mechanism for the worst individual and a new updating strategy for scout bees are presented respectively. At last, for verifying its effectiveness, we apply the improved algorithm to optimize the power consumption of sensor nodes in a wireless sensor network . In this paper, verified by the test on the network, the basic benchmark functions and CEC2014, our algorithm shows superior performance with the compared modern algorithms.},
  archive      = {J_ASOC},
  author       = {Shuliang Zhu and Chi-Man Pun and Haipeng Zhu and Shujuan Li and Xiaomei Huang and Hao Gao},
  doi          = {10.1016/j.asoc.2023.110083},
  journal      = {Applied Soft Computing},
  pages        = {110083},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An artificial bee colony algorithm with a balance strategy for wireless sensor network},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Innovative soft computing-enabled cloud optimization for
next-generation IoT in digital twins. <em>ASOC</em>, <em>136</em>,
110082. (<a href="https://doi.org/10.1016/j.asoc.2023.110082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research aims to reduce the network resource pressure on cloud centers (CC) and edge nodes, to improve the service quality and to optimize the network performance. In addition, it studies and designs a kind of edge–cloud collaboration framework based on the Internet of Things (IoT). First, raspberry pi (RP) card working machines are utilized as the working nodes, and a kind of edge–cloud collaboration framework is designed for edge computing . The framework consists mainly of three layers, including edge RP (ERP), monitoring &amp; scheduling RP (MSRP), and CC. Among the three layers, collaborative communication can be realized between RPs and between RPs and CCs. Second, a kind of edge–cloud​ matching algorithm is proposed in the time delay constraint scenario. The research results obtained by actual task assignments demonstrate that the task time delay in face recognition on edge–cloud collaboration mode is the least among the three working modes, including edge only, CC only, and edge–CC collaboration modes, reaching only 12 s. Compared with that of CC running alone, the identification results of the framework rates on edge–cloud collaboration and CC modes are both more fluent than those on edge mode only, and real-time object detection can be realized. The total energy consumption of the unloading execution by system users continuously decreases with the increase in the number of users. It is assumed that the number of pieces of equipment in systems is 150, and the energy-saving rate of systems is affected by the frequency of task generation. The frequency of task generation increases with the corresponding reduction in the energy-saving rate of systems. Based on object detection as an example, the system energy consumption is decreased from 18 W to 16 W after the assignment of algorithms. The included framework improves the resource utility rate and reduces system energy consumption. In addition, it provides theoretical and practical references for the implementation of the edge–cloud collaboration framework.},
  archive      = {J_ASOC},
  author       = {Hailin Feng and Liang Qiao and Zhihan Lv},
  doi          = {10.1016/j.asoc.2023.110082},
  journal      = {Applied Soft Computing},
  pages        = {110082},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Innovative soft computing-enabled cloud optimization for next-generation IoT in digital twins},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A complex network-based vaccination strategy for infectious
diseases. <em>ASOC</em>, <em>136</em>, 110081. (<a
href="https://doi.org/10.1016/j.asoc.2023.110081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the event of an outbreak, it is vital to stop the spread of infectious diseases as quickly as possible, and vaccination is the most important means of stopping the spread of infectious diseases. The research of infectious disease vaccination strategy aims to control the spread of infectious disease as effectively as possible under the circumstance of limited vaccine resources, so as to achieve the result of social group immunity. Most proposed solutions have been based on variations on infectious disease models, or on the structure of networks for vaccination strategies. In this study, the epidemic threshold is used as the optimization objective , which not only transforms the immunization problem into an optimization problem , but also can be applied to different infectious disease models and is more relevant for the study of infectious diseases relative to the network structure. Meanwhile, this work combines machine learning with evolutionary computation, which in turn improves the ability to search for big data problems and reduces computational costs. Community structure is presented to narrow the search based on complex networks. Therefore, we present a novel community-based targeted immunization framework(CTIF) to select nodes for immunization. The proposed CTIF is composed of three phases: community detection, candidate immunization node set generation and target immunizations node set selection. In terms of epidemic threshold optimization, experiments reveal that the suggested algorithm outperforms the baseline strategies.},
  archive      = {J_ASOC},
  author       = {Lihong Sun and Qiang He and Yueyang Teng and Qi Zhao and Xin Yan and Xingwei Wang},
  doi          = {10.1016/j.asoc.2023.110081},
  journal      = {Applied Soft Computing},
  pages        = {110081},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A complex network-based vaccination strategy for infectious diseases},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary computation-based reliability quantification
and its application in big data analysis on semiconductor manufacturing.
<em>ASOC</em>, <em>136</em>, 110080. (<a
href="https://doi.org/10.1016/j.asoc.2023.110080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data analysis of wafer maps in semiconductor manufacturing is essential for process reliability assessment, it is an important means of fault diagnosis in manufacturing. Although many excellent wafer map defect pattern identification (WMDPI) methods have been proposed, such as deep convolutional neural network (DCNN) based methods, there is no method that can make completely correct decisions and avoid false detections. Decision reliability is critical to the training evolution process of DCNN. In this paper, we propose a reliability quantification algorithm for wafer big data analysis. We estimate the parameter evolution uncertainty of DCNN in the WMDPI process by mixed uncertainty and thus quantify the decision reliability. In particular, reliability assessment is achieved by quantifying hybrid uncertainties, including epistemic uncertainty and aleatoric uncertainty . For epistemic uncertainty , this paper uses Monte Carlo dropout and explores the effects of drop rate, action location, and uncertainty criteria on reliability quantification through an empirical study. For aleatoric uncertainty , this paper proposes a calculation method based on Monte Carlo augmentation. The experimental results show that the identification accuracy of unknown defect patterns is 97.04\% when hybrid uncertainty is considered; the identification accuracy of known defect patterns can be improved by up to 7.89\%. The proposed DCNN can effectively avoid false and missed detections in the process of wafer big data analysis.},
  archive      = {J_ASOC},
  author       = {Qiao Xu and Naigong Yu and Mohammad Mehedi Hasan},
  doi          = {10.1016/j.asoc.2023.110080},
  journal      = {Applied Soft Computing},
  pages        = {110080},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary computation-based reliability quantification and its application in big data analysis on semiconductor manufacturing},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary computation-based multitask learning network
for railway passenger comfort evaluation from EEG signals.
<em>ASOC</em>, <em>136</em>, 110079. (<a
href="https://doi.org/10.1016/j.asoc.2023.110079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a core indicator of the railway service industry, the comfort of the railway passengers is an important aspect to measure the advancement of railway technology. The accurate evaluation of railway passenger comfort has received much attention from academia and industry. The EEG-based comfort evaluation method has been hailed as the gold standard due to its rich quantity of information and objectivity. In this study, an evolutionary computation-based multitask learning network named EEG-DEMTL is proposed to evaluate railway passenger comfort from EEG signals. In this network, the multitask learning structure fully exploits the relationship between subtasks such as passenger emotion and the main task, which is passenger overall comfort. Furthermore, the weight definition method based on the differential evolution (DE) algorithm is used to select the best weight setting. To verify the validity of our proposed method, field experiments in a high-speed railway (HSR) are designed, and comfort perceptions and EEG signals of 20 passengers are collected. Compared with the baseline models , which include the support vector machine , K-nearest neighbour and decision trees , the proposed EEG-DEMTL model achieves the best performance. In addition, the comparison results between several weight settings of MTL show that the DE weights can improve the evaluation performance by 6.30\%. This research proposes a novel EEG-based method to meet the requirements of railway passenger comfort evaluation and offers a neurological explanation for railway passenger comfort.},
  archive      = {J_ASOC},
  author       = {Baoquan Cheng and Hanliang Fu and Tao Li and Honghao Zhang and Jianling Huang and Yong Peng and Huihua Chen and Chaojie Fan},
  doi          = {10.1016/j.asoc.2023.110079},
  journal      = {Applied Soft Computing},
  pages        = {110079},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary computation-based multitask learning network for railway passenger comfort evaluation from EEG signals},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tree enhanced deep adaptive network for cancer prediction
with high dimension low sample size microarray data. <em>ASOC</em>,
<em>136</em>, 110078. (<a
href="https://doi.org/10.1016/j.asoc.2023.110078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer prediction based on microarray data can facilitate the molecular exploration of cancers, thus building more accurate cancer prediction models is essential. This study focuses on a deep learning-based cancer prediction model. However, using a deep neural network to predict cancer is a difficult task due to the complexity of the underlying biological patterns and high dimension low sample size (HDLSS) of microarray data, which could bring about over-fitting and large training gradient variance. Therefore, a tree-enhanced deep adaptive network (TEDAN) is proposed to address these issues. Firstly, we employ the idea of the ensemble tree as a feature transformation method to alleviate the over-fitting problem, which generates a feature with a lower dimension and a more discriminative pattern. Secondly, a deep adaptive network (DAN) based on a self-attention mechanism is proposed to model the underlying biological interaction between different genes. Thirdly, a low sample size training (LSST) method is proposed to further reduce the large training gradient variance. Experiment results on six public cancer prediction datasets demonstrate that the TEDAN outperforms other strong baseline models .},
  archive      = {J_ASOC},
  author       = {Yao Wu and Donghua Zhu and Xuefeng Wang},
  doi          = {10.1016/j.asoc.2023.110078},
  journal      = {Applied Soft Computing},
  pages        = {110078},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tree enhanced deep adaptive network for cancer prediction with high dimension low sample size microarray data},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AVQS-NN: Adaptive virtualization for quantum services for
application support using neural networks. <em>ASOC</em>, <em>136</em>,
110075. (<a href="https://doi.org/10.1016/j.asoc.2023.110075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable application support is provided by quantum networks utilizing internal and superposition service network topologies . To provide continuous accessibility, the superposition procedure incorporates service replicas and network virtualization . This study presents a quantum requirement model to be prepared for the new virtualization technology and slicing, notwithstanding the uncertainty of that future. Combinatorial learning paradigms are used in this approach to combat the unpredictability that arises from network allocation and service distribution. This method named Adaptive Virtualization for Quantum Services using Neural Network (AVQS-NN) relies on customer satisfaction based on virtualization’s rising demand and rapid service response. Linear neural networks are used in this technique for both virtualization and verification of quantum states . The quantum state is verified to ensure that virtualization enhancements are balanced between network availability and service needs. States are established and toggled between several responses and processing speeds using the suggested technique. This helps provide fast responses, enhanced processing, and reduced delay. Furthermore, with fewer failures, resource utilization is enhanced.},
  archive      = {J_ASOC},
  author       = {Nakeeb Noor alleema and Christalin Nelson Selvin and Vijayakumar Varadarajan and Anandan Panneerselvam and Ramakrishnan Jothilakshmi and Santhosh kumar perumal},
  doi          = {10.1016/j.asoc.2023.110075},
  journal      = {Applied Soft Computing},
  pages        = {110075},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AVQS-NN: Adaptive virtualization for quantum services for application support using neural networks},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective closed-loop green supply chain model with
disruption risk. <em>ASOC</em>, <em>136</em>, 110074. (<a
href="https://doi.org/10.1016/j.asoc.2023.110074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The benefits of the circular economy are pushing industries towards forming closed-loop supply chains (CLSCs). This transition requires the industries to deal with conventional cost minimization along with various environmental objectives. However, the objectives become difficult to attain if the production process gets disrupted and no suitable recovery mechanism is in place. The extant literature indicates that few researchers have worked to develop a recovery model for CLSC systems that considers both economic and environmental objectives. Thus, this study develops a nonlinear complex mathematical model to minimize the total cost, energy consumption, CO 2 emission, and waste generation of supply chains with a focus on disruption risk. This research contributes to the literature by addressing the model with three existing heuristics — multi-objective genetic algorithm (MOGA), non-dominated sorting genetic algorithm (NSGA-II), and multi-objective bonobo optimizer (MOBO)–and by developing an updated hyper-heuristic algorithm based on a choice function. We employ four performance metrics–algorithm effort (AE), ratio of non-dominated individual (RNI), maximum spread (MS), and average distance (AD)–to compare the efficiency and effectiveness of these algorithms. Our quantitative results show that RSCs can mitigate production shortages stemming from supply chain disruptions. They also demonstrate the benefits of CLSCs with regard to lowering costs, energy consumption, CO 2 emissions, and waste generation.},
  archive      = {J_ASOC},
  author       = {Kazi Wahadul Hasan and Syed Mithun Ali and Sanjoy Kumar Paul and Golam Kabir},
  doi          = {10.1016/j.asoc.2023.110074},
  journal      = {Applied Soft Computing},
  pages        = {110074},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective closed-loop green supply chain model with disruption risk},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sustainability assessment of palm oil industry 4.0
technologies in a circular economy applications based on interval-valued
pythagorean fuzzy rough set-FWZIC and EDAS methods. <em>ASOC</em>,
<em>136</em>, 110073. (<a
href="https://doi.org/10.1016/j.asoc.2023.110073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The palm oil industry is one of the most competitive industries and must comply with industry standards for flexible supply chain operations , productivity, and sustainability. Many researchers have demonstrated that Industry 4.0 technologies in a circular economy and sustainability practices (I4.0-in-a-CE-and-SPs) present promising future research opportunities, particularly for an industry with sustainability challenges. Hence, determining the most sustainable I4.0-in-a-CE-and-SPs application is critical for the palm oil industry. However, this process is deemed as a multiple attributes decision-making (MADM) problem due to the presence of three issues, namely, multiple sustainability performance attributes, uncertainty of the attribute’s importance level, and data variation. Therefore, an MADM solution is necessary to address these issues. This study extended the fuzzy weighted with zero inconsistency (FWZIC) method with an interval-valued Pythagorean fuzzy rough set (IVPFRS) and integrated it with the evaluation based on distance from average solution (EDAS) method to rank I4.0-in-a-CE-and-SP applications. The research method starts with the creation of a decision matrix (DM) based on the intersection of 26 I4.0-in-a-CE-and-SPs applications and 14 sustainability performance attributes. The IVPFRS–FWZIC method is subsequently developed to determine the weights of the sustainability performance attributes. The EDAS method uses these weights and the constructed DM to rank I4.0-in-a-CE-and-SPs applications. The robustness of the proposed methods was evaluated using sensitivity analysis, the Spearman’s correlation coefficient test, and comparison analysis.},
  archive      = {J_ASOC},
  author       = {Hassan Abdulsattar Ibrahim and Aws Alaa Zaidan and Sarah Qahtan and Bilal Bahaa Zaidan},
  doi          = {10.1016/j.asoc.2023.110073},
  journal      = {Applied Soft Computing},
  pages        = {110073},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sustainability assessment of palm oil industry 4.0 technologies in a circular economy applications based on interval-valued pythagorean fuzzy rough set-FWZIC and EDAS methods},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AROA: Adam remora optimization algorithm and deep q network
for energy harvesting in fog-IoV network. <em>ASOC</em>, <em>136</em>,
110072. (<a href="https://doi.org/10.1016/j.asoc.2023.110072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric Vehicles (EV) has gained immense popularity due to the increasing awareness amongst people regarding low carbon emission. Smart vehicles have become a central part of the Internet of vehicles (IoV) infrastructure. Whenever several vehicles distribute its tasks, then the classical centralized model meet various issues, such as security and delay in communication. This paper devises a technique for energy harvesting in the Fog-IoV network. The Fog-IoV network simulation is done for enhanced processing. The three layers, such as fog layer, cloud layer, and IoV layer are adapted for electricity trading. The power prediction is performed with a deep reinforcement learning technique , namely Deep Q network (DQN). The optimal electricity trading is done with the proposed Adam Remora Optimization Algorithm (AROA). The AROA is obtained by the amalgamation of Remora Optimization Algorithm (ROA) and Adam optimization algorithm. The EVs represents buyer of electricity that demands electricity in such a way that Road side unit (RSU) perform bidding. The fitness function is newly modelled using predicted power, price, and distance. The experimentation of the technique is done in terms of fitness, power, and pricing. The proposed AROA-based DQN offered enhanced performance with the highest power of 11.920 and the smallest pricing of 16.949\%.},
  archive      = {J_ASOC},
  author       = {Savita Lohat and Sheilza Jain and Rajender Kumar},
  doi          = {10.1016/j.asoc.2023.110072},
  journal      = {Applied Soft Computing},
  pages        = {110072},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AROA: Adam remora optimization algorithm and deep q network for energy harvesting in fog-IoV network},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A quantum-based approach for offensive security against
cyber attacks in electrical infrastructure. <em>ASOC</em>, <em>136</em>,
110071. (<a href="https://doi.org/10.1016/j.asoc.2023.110071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deciding the correct offensive security strategy for safeguarding the electrical physical infrastructures of smart grids is a challenging task. The offensive security training against various cyber-attacks focuses on a multitude of electrical subsystems and measurement systems like the Load Frequency Control(LFC) system. Primarily, the principal challenge is to categorize and parameterize the various possible cyber-attacks on electrical infrastructures. This is done by specifying and selecting cyber-attacks considering various main and subsystem blocks of the power structural system within each area of major installations. In this research investigation, formal modeling of security strategy is proposed using Lambda calculus with both classical and quantum perspectives. Furthermore, using a Quantum Machine Learning (QML) technique, the procedure for correct vulnerability prediction, exploitation, and execution strategy is presented with an approximated likelihood of attack and its mode. The local and non-local interactions are introduced as quantum threats and entanglement threats similar to False Data Injection (FDI) methods to induce the attack and counter-attack events through quantum causality connections. Finally, the Quirk simulator is used to validate the proposed quantum design of offensive and defensive attack models considering scenarios of exogenous and scaling attacks on the LFC systems that support the feasibility of the present research work to address the issue of cyber-attacks on the power system networks.},
  archive      = {J_ASOC},
  author       = {Lakshmi D. and Neelu Nagpal and S. Chandrasekaran and Jude Hemanth D.},
  doi          = {10.1016/j.asoc.2023.110071},
  journal      = {Applied Soft Computing},
  pages        = {110071},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A quantum-based approach for offensive security against cyber attacks in electrical infrastructure},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aligning heterogeneous optimization problems with optimal
correspondence assisted affine transformation for evolutionary
multi-tasking. <em>ASOC</em>, <em>136</em>, 110070. (<a
href="https://doi.org/10.1016/j.asoc.2023.110070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary multi-tasking optimization (EMTO) aims to boost the overall efficiency of optimizing multiple tasks by triggering knowledge transfer among them. Unfortunately, it may suffer from negative transfer on heterogeneous composite tasks that have low similarity. Some studies try to learn an intertask alignment transformation based on the paired samples from the involved tasks, but risk a failed alignment with improper pairwise methods. To solve this issue, this study proposes an optimal correspondence assisted affine transformation (OCAT) algorithm. OCAT explicitly constructs a mathematical model for the intertask alignment problem and theoretically deduces its optimal solution in an iterative method. As a result, the sample correspondences that enable the learned transformation to achieve the maximum intertask similarity can be located. Besides, a novel approach to deriving the affine transformation formula is also developed for OCAT. The resulting affine alignment transformation will not impair the knowledge contained in the tasks during the alignment process. By integrating OCAT with the estimation of distribution algorithm , this study finally develops a many-tasking optimization algorithm named MaT-EDA, where the solutions from other tasks are explicitly transferred as the samples for estimating the current distribution model. Extensive simulation studies have indicated that OCAT can significantly enhance the performance of EMTO, and MaT-EDA also achieves impressive many-tasking optimization performance .},
  archive      = {J_ASOC},
  author       = {An Chen and Zhigang Ren and Muyi Wang and Shenyu Su and Jiaqi Yun and Yichuang Wang},
  doi          = {10.1016/j.asoc.2023.110070},
  journal      = {Applied Soft Computing},
  pages        = {110070},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Aligning heterogeneous optimization problems with optimal correspondence assisted affine transformation for evolutionary multi-tasking},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The importance of diversity in the variable space in the
design of multi-objective evolutionary algorithms. <em>ASOC</em>,
<em>136</em>, 110069. (<a
href="https://doi.org/10.1016/j.asoc.2023.110069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current Multi-Objective Evolutionary Algorithms ( moea s) do not directly manage the population’s diversity in the variable space. Usually, these kind of mechanisms are only considered in Evolutionary Multimodal Multi-Objective Algorithms ( emma s) which aim to obtain a complete representation of the set of – locally or globally – optimal solutions in variable space. This is a remarkable difference with respect to single-objective optimizers, where maintaining diverse solutions is considered favorable to better explore the search space. The contribution of this research is to show that the quality of current moea s in terms of objective space metrics can be enhanced by integrating mechanisms to explicitly manage the diversity in the variable space. The key is to consider the stopping criterion and elapsed period in order to dynamically alter the importance granted to the diversity in the variable space and to the quality and diversity in the objective space, which is an important difference with respect to emma s. Specifically, more importance is given to the variable space in the initial phases and, decisions are progressively more biased by the information of the objective space as the evolution progresses. This paper presents a novel moea based on decomposition ( avsd-moea/d ) that relies on these principles by means of a novel replacement phase. Extensive experimentation shows the clear benefits provided by the proposed design principle.},
  archive      = {J_ASOC},
  author       = {Carlos Segura and Joel Chacón Castillo and Oliver Schütze},
  doi          = {10.1016/j.asoc.2023.110069},
  journal      = {Applied Soft Computing},
  pages        = {110069},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The importance of diversity in the variable space in the design of multi-objective evolutionary algorithms},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning broad learning system with controllable sparsity
through l0 regularization. <em>ASOC</em>, <em>136</em>, 110068. (<a
href="https://doi.org/10.1016/j.asoc.2023.110068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a novel neural network with efficient learning capacity, broad learning system (BLS) has achieved remarkable success in various regression and classification problems. Due to the broad expansion of nodes, however, BLS is known to have many redundant parameters and nodes, which will increase the memory and computation cost and is adverse to its deployment on equipment with limited resources. To optimize the number of neurons and parameters of BLS and then find the optimal sparse model under a given resource budget, in this paper, we introduce to train BLS through L0 regularization . The regularization constraint term of the BLS objective function is replaced by the L0 regularization method , and the normalized hard threshold iterative method is used to optimize the output weight. More concretely, the size of the model is fixed by controlling the number of output weights under given the resource size, and then parameters and nodes in the network are evaluated and selected from the node set in the training to obtain a BLS with controllable sparsity (CSBLS). Experiments on various data sets demonstrate the effectiveness of our proposed method.},
  archive      = {J_ASOC},
  author       = {Fei Chu and Guanghui Wang and Jun Wang and C.L. Philip Chen and Xuesong Wang},
  doi          = {10.1016/j.asoc.2023.110068},
  journal      = {Applied Soft Computing},
  pages        = {110068},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning broad learning system with controllable sparsity through l0 regularization},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developing GAN-boosted artificial neural networks to model
the rate of drilling bit penetration. <em>ASOC</em>, <em>136</em>,
110067. (<a href="https://doi.org/10.1016/j.asoc.2023.110067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of achieving a single model for estimating the rate of drilling bit penetration (ROP) with high accuracy has been the subject of many efforts. Analytical methods and, later, data-based techniques were utilized for this purpose. However, despite their partial effectiveness, these methods were inadequate for establishing models with sufficient generality. Based on deep learning (DL) concepts, this study has developed an innovative approach that produces general and boosted models capable of more accurately estimating ROPs compared to other techniques. A vital component of this approach is using a deep Artificial Neural Network (ANN) structure known as the Generative Adversarial Network (GAN). The GAN structure is combined with regressor (predictive) ANNs, to boost their performance when estimating the target parameter. The predictive ANNs of this study include Multi-Layer Perceptron Neural Network (MLP-NN) and 1-Dimensional Convolution Neural Network (1D-CNN) structures. More specifically, the key idea of our approach is to utilize GAN’s capability to produce fake samples comparable to true samples in predictive ANNs. Therefore, the proposed approach introduces a two-step predictive model development procedure. As the first step, the GAN structure is trained with the target of the problem as the input feature. GAN’s generator part, which can produce fake ROP samples similar to the true ones after training, is frozen and then replaces the output layer’s neuron of the predictive ANNs. In the second step, final predictive ANNs carrying frozen trained-generator called GAN-Boosted Neural Networks (GB-NNs) are trained to make predictions. Because this approach reduces the computational load of the predictive model training process and increases its quality, the performance of predictive ANN models is improved. An additional innovation of this research is using the residual structure during 1D-CNN network training, which improved the performance of the 1D-CNN by combining the input data with those features extracted from the inputs. This study revealed that the GB-Res 1D-CNN model, a GAN-Boosted 1-Dimensional Convolutional Neural Network with a Residual structure, results in the most accurate prediction. The validity of the GB-Res 1D-CNN model is confirmed by its successful implementation in blind well. As the final step of this study, we conducted a sensitivity analysis to identify the effect of different parameters on the predicted ROP . As expected, the DS and DT parameters significantly affect the model-estimated ROP.},
  archive      = {J_ASOC},
  author       = {Mohammad Hassan Sharifinasab and Mohammad Emami Niri and Milad Masroor},
  doi          = {10.1016/j.asoc.2023.110067},
  journal      = {Applied Soft Computing},
  pages        = {110067},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing GAN-boosted artificial neural networks to model the rate of drilling bit penetration},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comprehensive review of machine learning in geotechnical
reliability analysis: Algorithms, applications and further challenges.
<em>ASOC</em>, <em>136</em>, 110066. (<a
href="https://doi.org/10.1016/j.asoc.2023.110066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geotechnical reliability analysis provides a novel way to rationally take the underlying geotechnical uncertainties into account and evaluate the stability of geotechnical structures by failure probability (or equivalently, reliability index) from a probabilistic perspective, which has gained great attention in the past few decades. With the rapid development of artificial intelligence techniques, various machine learning (ML) algorithms have been successfully applied in geotechnical reliability analysis and the number of relevant papers has been increasing at an accelerating pace. Although significant advances have been made in the past two decades, a systematic summary of this subject is still lacking. To better conclude current achievements and further shed light on future research, this paper aims to provide a state-of-the-art review of ML in geotechnical reliability analysis applications. Through reviewing the papers published in the period from 2002 to 2022 with the topic of applying ML in the reliability analysis of slopes, tunneling, and excavations, the pros and cons of the developed methods are explicitly tabulated. The great achievements that have been made are systematically summarized from two major aspects. In addition, the four potential challenges and prospective research possibilities underlying geotechnical reliability analysis are also outlined, including multisensor data fusion, time-variant reliability analysis, three-dimensional reliability analysis of practical cases, and ML model selection and optimization.},
  archive      = {J_ASOC},
  author       = {Wengang Zhang and Xin Gu and Li Hong and Liang Han and Lin Wang},
  doi          = {10.1016/j.asoc.2023.110066},
  journal      = {Applied Soft Computing},
  pages        = {110066},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comprehensive review of machine learning in geotechnical reliability analysis: Algorithms, applications and further challenges},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel FMEA approach for submarine pipeline risk analysis
based on IVIFRN and ExpTODIM-PROMETHEE-II. <em>ASOC</em>, <em>136</em>,
110065. (<a href="https://doi.org/10.1016/j.asoc.2023.110065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The failure mode and effects analysis (FMEA) approach based on TODIM has been widely applied in risk analysis in several fields. To fill the gap in relevant research in the field of submarine pipeline risk analysis and to address possible problems in the classical TODIM method, a novel FMEA model is proposed in this paper. Firstly, an improvement to the existing interval-valued intuitionistic fuzzy rough number (IVIFRN) theory is made and used to collect expert opinions better to consider individual and group uncertainty. Secondly, ExpTODIM, which is more in line with prospect theory, is combined with the PROMETHEE-II method. And it also incorporates weight calculation methods such as the maximizing deviation method for ranking failure modes. Thirdly, the exponential entropy weight method based on the improved IVIFRN theory is combined with the analytical hierarchical process (AHP) for application in fuzzy comprehensive evaluation to complete the calculation of risk values. Fourthly, the proposed novel FMEA approach is applied to the risk analysis of the submarine pipeline to obtain its failure mode ranking and risk value results. Comparisons with other methods in concept and example analysis show that the proposed method is relatively reliable and has certain advantages over other current FMEA methods based on MCDM . In particular, the proposed method overcomes the theoretical problems of the classical TODIM and is more consistent with logic. It also improves the accuracy and comprehensiveness of analysis results, which can provide a valid and accurate reference for the practical engineering of submarine pipelines.},
  archive      = {J_ASOC},
  author       = {Yang Yu and Jiu Yang and Shibo Wu},
  doi          = {10.1016/j.asoc.2023.110065},
  journal      = {Applied Soft Computing},
  pages        = {110065},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel FMEA approach for submarine pipeline risk analysis based on IVIFRN and ExpTODIM-PROMETHEE-II},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Information entropy-based differential evolution with
extremely randomized trees and LightGBM for protein structural class
prediction. <em>ASOC</em>, <em>136</em>, 110064. (<a
href="https://doi.org/10.1016/j.asoc.2023.110064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of protein tertiary structure is the basis of current genetic engineering, medicinal design, and other biological applications . Protein structural class plays a significant role in the tertiary structure folding and function analysis of protein. However, the growth rate of new amino acid sequence far exceeds the tertiary structure. Existing research methods of confirming protein folding cannot satisfy massive sequences and protein engineering . A high-accuracy prediction result of low-similarity protein dataset is particularly critical to generate the corresponding tertiary structure from the primary structure. In this paper, we construct a novel super-large-scale feature of the primary structure based on secondary structure, evolutionary information, chemical properties, and global descriptors. The diversified and massive features are utilized to predict the protein class based on a novel feature selection algorithm and a gradient boosting decision tree model . To testify the effectiveness and robustness of our proposed method, namely IDEGBM, we choose the 10-fold cross-validation for evaluating four benchmark datasets 25PDB, FC699, D1189 and D640. Experimental results exhibit that our method improves the accuracy in comparison with other state-of-the-art prediction models in terms of both accuracy and efficiency. Furthermore, a representative protein is used to validate that our proposed IDEGBM can be applied to improve the conformation prediction of protein tertiary structure.},
  archive      = {J_ASOC},
  author       = {Yu Zhang and Shangce Gao and Pengxing Cai and Zhenyu Lei and Yirui Wang},
  doi          = {10.1016/j.asoc.2023.110064},
  journal      = {Applied Soft Computing},
  pages        = {110064},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Information entropy-based differential evolution with extremely randomized trees and LightGBM for protein structural class prediction},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A particle swarm optimization and variable neighborhood
search based multipopulation algorithm for inter-domain path computation
problem. <em>ASOC</em>, <em>136</em>, 110063. (<a
href="https://doi.org/10.1016/j.asoc.2023.110063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a common occurrence nowadays for a network to be gigantic in size and complex in architecture. Network navigation thus faces many new problems in terms of routing and resource utilization, one of which arises in multi-domain networks. This paper focuses on the Inter-Domain Path Computation under Node-defined Domain Uniqueness Constraint (IDPC-NDU) problem, whose purpose is to find the shortest path between two nodes in a network under a constraint that such a path is only allowed to traverse each domain at most once. Considering that this is an NP-Hard problem, an approximate approach is more practical than an exact one. Meanwhile, Particles Swarm Optimization Algorithm (PSO) has been long known for its powerful ability to discover near-optimal solutions in a reasonable time; however, its application in discrete search space is limited. Therefore, we decide to use the multi-population framework, a sub-branch of multitasking optimization which allows information exchange not only between problems but also between different optimization heuristics, to improve upon the basic PSO method. Specifically, this paper introduces a hybridization between PSO and Variable Neighborhood Search (VNS). The encoding and decoding method are created specifically for the IDPC-NDU problem and for the PSO algorithm, and VNS serves to enhance further the algorithm’s ability to escape local optima. Experiments are carried out to prove the new algorithm’s efficacy, especially in the context of similar multitasking methods.},
  archive      = {J_ASOC},
  author       = {Do Tuan Anh and Huynh Thi Thanh Binh and Nguyen Duc Thai and Pham Dinh Thanh},
  doi          = {10.1016/j.asoc.2023.110063},
  journal      = {Applied Soft Computing},
  pages        = {110063},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle swarm optimization and variable neighborhood search based multipopulation algorithm for inter-domain path computation problem},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). ABEM: An adaptive agent-based evolutionary approach for
influence maximization in dynamic social networks. <em>ASOC</em>,
<em>136</em>, 110062. (<a
href="https://doi.org/10.1016/j.asoc.2023.110062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization is recognized as a crucial optimization problem , which aims to identify a limited set of influencers to maximize the coverage of influence dissemination in social networks. However, real-world social networks are usually dynamic and large-scale, which leads to difficulty in capturing real-time user and diffusion features to effectively and accurately select the key influencers. In this paper, we propose an adaptive agent-based evolutionary approach to address this challenging issue with agent-based modeling and genetic algorithm . This novel approach identifies the users’ influence capability in a distributed manner and optimizes the influencer set selection in a dynamic environment. An adaptive solution optimizer is proposed as one of the key components, driving the evolutionary process and adapting the candidate solutions dynamically. The proposed approach is also applicable to large-scale networks due to its distributed framework. Evaluation of our approach is performed by using both synthetic networks and real-world datasets. Experimental results demonstrate that the proposed approach outperforms state-of-the-art seeding algorithms in terms of maximizing influence.},
  archive      = {J_ASOC},
  author       = {Weihua Li and Yuxuan Hu and Chenting Jiang and Shiqing Wu and Quan Bai and Edmund Lai},
  doi          = {10.1016/j.asoc.2023.110062},
  journal      = {Applied Soft Computing},
  pages        = {110062},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ABEM: An adaptive agent-based evolutionary approach for influence maximization in dynamic social networks},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Walsh-based surrogate-assisted multi-objective combinatorial
optimization: A fine-grained analysis for pseudo-boolean functions.
<em>ASOC</em>, <em>136</em>, 110061. (<a
href="https://doi.org/10.1016/j.asoc.2023.110061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to study surrogate-assisted algorithms for expensive multiobjective combinatorial optimization problems . Targeting pseudo-boolean domains, we provide a fine-grained analysis of an optimization framework using the Walsh basis as a core surrogate model . The considered framework uses decomposition in the objective space, and integrates three different components, namely, (i) an inner optimizer for searching promising solutions with respect to the so-constructed surrogate, (ii) a selection strategy to decide which solution is to be evaluated by the expensive objectives, and (iii) the strategy used to setup the Walsh order hyper-parameter. Based on extensive experiments using two benchmark problems, namely bi-objective NK-landscapes and unconstrained binary quadratic programming problems (UBQP), we conduct a comprehensive in-depth analysis of the combined effects of the considered components on search performance, and provide evidence on the effectiveness of the proposed search strategies. As a by-product, our work shed more light on the key challenges for designing a successful surrogate-assisted multi-objective combinatorial search process.},
  archive      = {J_ASOC},
  author       = {Bilel Derbel and Geoffrey Pruvost and Arnaud Liefooghe and Sébastien Verel and Qingfu Zhang},
  doi          = {10.1016/j.asoc.2023.110061},
  journal      = {Applied Soft Computing},
  pages        = {110061},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Walsh-based surrogate-assisted multi-objective combinatorial optimization: A fine-grained analysis for pseudo-boolean functions},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Res-DUnet: A small-region attentioned model for cardiac
MRI-based right ventricular segmentation. <em>ASOC</em>, <em>136</em>,
110060. (<a href="https://doi.org/10.1016/j.asoc.2023.110060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Right ventricular function has been associated with a variety of cardiovascular diseases. In the clinical study of right ventricular function, an important step is segmentation of the right ventricle so that functional indicators of the heart can be quantified and evaluated based on the segmented region. Compared to left ventricle, right ventricle (RV) is more difficult to segment due to its irregular shape variations and blurred borders. To improve segmentation accuracy on slices with very small regions of RV, a two-stage deep learning model called Res-DUnet is developed to segment the right ventricle on short-axis slices of cardiac magnetic resonance imaging. The model is divided into two modules, the localization module for extracting the regions of interest (ROIs) and the segmentation module for segmenting the right ventricle. The two-stage model achieves a mean dice score (DSC) of 90.55, a mean Hausdorff distance (HD) of 6.67 and a mean recall of 92.68, outperforming the state-of-the-art models. The clinical indicators derived from the model are analyzed for consistency with the ground truth. The results showed that the model’s performance is comparable to that of the radiologists .},
  archive      = {J_ASOC},
  author       = {Chenkai Su and Jinlian Ma and Yuxiang Zhou and Panpan Li and Zijun Tang},
  doi          = {10.1016/j.asoc.2023.110060},
  journal      = {Applied Soft Computing},
  pages        = {110060},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Res-DUnet: A small-region attentioned model for cardiac MRI-based right ventricular segmentation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A double attention graph network for link prediction on
temporal graph. <em>ASOC</em>, <em>136</em>, 110059. (<a
href="https://doi.org/10.1016/j.asoc.2023.110059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data in the real world can be abstracted into the format of dynamic graph, and predicting the connection relationship between nodes in graph is a problem that is often faced in the researching. With the development of deep learning techniques, Graph Neural Networks(GNNs) have been widely used to solve graph-related problems and achieved great results. However, most of the works focus on static graph, or applied the method of static graph to coarse grained discrete time graph after slice dynamic graph into snapshot. As a kind of data that incorporates all the time process information, temporal graph contains a variety of information that worth to be explored. Several works have been done for temporal graph data, but there are still shortcomings. We believe that when researching the evolution of dynamic graph, the influence of the surrounding environment on each node in local time and space is decisive for the properties of the node, which has not been considered in the previous works. Therefore, we propose a novel general model: Double Attention Temporal Graph Network(DATGN). Through an activity based sampling algorithm , the significant nodes in the local time-structure space that associated with each node are sampled and generated as sequence, then the global information of node is aggregated through a global attention network . After which the information of the sampled sequences is aggregated by a local attention network that does not depend on edge relations, and the final representation vector of the node is obtained to make the prediction. In the experiment, we selected three data sets from different knowledge domains and compared them with the current state-of-art models. In the transductive and inductive link prediction tasks, DATGN both achieved the best results in terms of accuracy and operational efficiency than other baseline models , and we discussed the reasons of this improvement. The experiment results demonstrate that the local spatial–temporal network layer can capture the evolutionary pattern of the time sequence and improve the accuracy of link prediction.},
  archive      = {J_ASOC},
  author       = {Qiao Mi and Xiaoming Wang and Yaguang Lin},
  doi          = {10.1016/j.asoc.2023.110059},
  journal      = {Applied Soft Computing},
  pages        = {110059},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A double attention graph network for link prediction on temporal graph},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spherical search based constrained optimization algorithm
for power flow analysis of islanded microgrids. <em>ASOC</em>,
<em>136</em>, 110057. (<a
href="https://doi.org/10.1016/j.asoc.2023.110057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional power flow (PF) algorithms are ineffective in the droop-regulated islanded microgrids as the slack bus voltage and system operating frequency are presumed constant parameters. Such assumptions are not applicable in the operation of the droop-regulated islanded microgrid. We formulate a novel formulation for islanded microgrids to solve the PF problem as a constrained optimization problem . Non-linear and linear constraints are developed to model the power balance and the various modes of Distributed Generation units (DGs). In islanded microgrids, DGs can be operated in PQ, PV, and droop mode . We propose an optimization algorithm named SS-NR (Spherical Search with Newton–Raphson based repair) for solving the formulated problem. We employ Spherical Search (SS) as a base optimizer to minimize the objective function in this algorithm. Moreover, Newton–Raphson based repair operator is also used within the framework of SS to handle non-linear equality constraints of a PF problem. Then, we compare the performance of the proposed algorithm with the state-of-the-art algorithms of global optimization. The experimental results show that the proposed algorithm performs better than the other contenders in convergence and accuracy. Furthermore, to validate the proposed formulation, we compare SS-NR results with the results of a time-domain simulator and other PF tools. This comparative analysis presents the efficacy of the proposed formulation as well as the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Abhishek Kumar and Bablesh Kumar Jha and Swagatam Das and Rammohan Mallipeddi},
  doi          = {10.1016/j.asoc.2023.110057},
  journal      = {Applied Soft Computing},
  pages        = {110057},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spherical search based constrained optimization algorithm for power flow analysis of islanded microgrids},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum fruit fly algorithm and ResNet50-VGG16 for medical
diagnosis. <em>ASOC</em>, <em>136</em>, 110055. (<a
href="https://doi.org/10.1016/j.asoc.2023.110055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical data are present in large amount and this is difficult to process for the diagnosis and Healthcare organization requires effective technique to handle big data. Existing techniques in medical diagnosis have limitations of imbalance data and overfitting problem. This research applies Quantum Fruit Fly Algorithm (QFFA) technique for feature selection to improve the effectiveness of classification in medical diagnosis. The Min–Max Normalization technique is applied to normalize the images to reduce the difference in pixel values and enhance the images. The ResNet50 and VGG16 deep learning models were applied for the feature extraction. The QFFA technique applies Archimedes spiral to increases the exploitation of the model to select unique features for classification. The Archimedes spiral provides spiral search in the top solutions of the Fruit Fly algorithm that helps to overcome local optima trap and increases exploitation. The QFFA technique selected features were applied to SVM model for the effective classification of medical diseases. The QFFA unique feature selection helps to overcome imbalance and overfitting problem in classification. The QFFA model has achieved better results in terms of various performance metrics such as sensitivity (99.26\%), and accuracy (99.04\%) than existing Deep 1D-CNN and GA-Decision tree models.},
  archive      = {J_ASOC},
  author       = {G.S. Nijaguna and J. Ananda Babu and B.D. Parameshachari and Rocío Pérez de Prado and Jaroslav Frnda},
  doi          = {10.1016/j.asoc.2023.110055},
  journal      = {Applied Soft Computing},
  pages        = {110055},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum fruit fly algorithm and ResNet50-VGG16 for medical diagnosis},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LSTM-markov based efficient anomaly detection algorithm for
IoT environment. <em>ASOC</em>, <em>136</em>, 110054. (<a
href="https://doi.org/10.1016/j.asoc.2023.110054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The seamless integration of wireless and IoT device in normal day to day life and for smart homes has enabled more security and privacy needs. The attack or unauthorized access to these devices may result in issues in decision making inside the smart environment. Attacks and anomalies in open IoT system could provide false alarms and cause delay in processing the information’s. The problem statement considered is, Anomaly detection systems, on the other hand, can be targets of attacks, h/w s/w failures, and thus fall short of their objectives. Because these are power-hungry devices carrying highly sensitive data, an effective attack and anomaly alert system is critical in an IoT-based environment. The present algorithms require high training and additional memory to identify the anomalies in the network, which is not practically feasible to simple edge-based computing devices. The anomalies and false information inside the network are handled with effective anomaly detection algorithm designed in this paper. An efficient anomaly detection method in real-time sensor is identified through markov and LSTM based network and the outliers in the data is clearly removed through the proposed approach. The proposed approach is tested with the real-time DHT sensor monitoring room temperature and room humidity . The proposed methodology provides 96.03\% effective anomalies detection with 92.48\% high training accuracy. The methodology showcase improved with 6.54\% of effective anomaly rejection and 5.13\% of training accuracy when compared with KNN algorithm.},
  archive      = {J_ASOC},
  author       = {Shanmuganathan V. and Suresh A.},
  doi          = {10.1016/j.asoc.2023.110054},
  journal      = {Applied Soft Computing},
  pages        = {110054},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LSTM-markov based efficient anomaly detection algorithm for IoT environment},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-model semi-supervised self-organizing fuzzy inference
system for data stream classification. <em>ASOC</em>, <em>136</em>,
110053. (<a href="https://doi.org/10.1016/j.asoc.2023.110053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning from data streams is widely considered as a highly challenging task to be further researched. In this paper, a novel dual-model self-organizing fuzzy inference system composed of two recently introduced evolving fuzzy systems (EFSs) is proposed for semi-supervised learning from data streams in infinite delay environments. After being primed with a small amount of labelled data during the warm-up period, the proposed model is able to continuously self-learn and self-expand its knowledge base from unlabelled data on a chunk-by-chunk basis with minimal human expert involvement. Thanks to its dual-model structure, the proposed model combines the merits of the two EFS models such that it can continuously identify new prototypes from new pseudo-labelled data to self-improve its knowledge base whilst keeping the impact of pseudo-labelled errors on its decision-making minimized. Numerical examples based on various benchmark problems demonstrate the efficacy of the proposed method, showing its strong potential in real-world applications by offering higher classification accuracy over the state-of-the-art competitors whilst retaining high computational efficiency.},
  archive      = {J_ASOC},
  author       = {Xiaowei Gu},
  doi          = {10.1016/j.asoc.2023.110053},
  journal      = {Applied Soft Computing},
  pages        = {110053},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-model semi-supervised self-organizing fuzzy inference system for data stream classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple information spatial–temporal attention based graph
convolution network for traffic prediction. <em>ASOC</em>, <em>136</em>,
110052. (<a href="https://doi.org/10.1016/j.asoc.2023.110052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction (forecasting) is a key problem in intelligent transportation. It helps engineers to obtain traffic trends in advance so that they can make favorable decisions quickly and accurately, or even improve the working mode of existing systems. However, it is very challenging to design a model for such problem that fully utilize the factors related to traffic. This paper investigates machine learning in traffic prediction and proposes Multiple Information Spatial–Temporal Attention based Graph Convolution Networks (MISTAGCN). The model consists of two parts. The first part utilizes combinations of different inputs and graph structures to compute the corresponding latent variables. In the second part of the model, the latent variables are comprehensively integrated and deeply mined. In particular, as the basic component of the model, a STBlock integrates mechanisms such as temporal attention, spatial attention , graph convolution, ordinary convolution and residual connections to fully explore the potential information contained in the data. Experiments on Haikou online car-hailing dataset and New York yellow taxi trip dataset illustrate that the proposed model outperforms state-of-art baseline models .},
  archive      = {J_ASOC},
  author       = {Shiming Tao and Huyin Zhang and Fei Yang and Yonghao Wu and Cong Li},
  doi          = {10.1016/j.asoc.2023.110052},
  journal      = {Applied Soft Computing},
  pages        = {110052},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple information Spatial–Temporal attention based graph convolution network for traffic prediction},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reliability estimation method based on signal feature
extraction and artificial neural network supported wiener process with
random effects. <em>ASOC</em>, <em>136</em>, 110044. (<a
href="https://doi.org/10.1016/j.asoc.2023.110044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting degradation and reliability is the key to effective condition-based machinery maintenance. This paper presents a reliability estimation method based on an artificial neural network (ANN)-supported Wiener process model with random effects. A number of time-domain features, frequency-domain features, and intrinsic energy features are considered to reflect the health condition of products. These features can be selected by the PROMETHEE II method considering multi-sample and multi-measure conditions. An ANN-supported health index function is defined to describe the relationship between the health condition and the selected signal features. Based on the health index function, an ANN-supported Wiener process is used to model machine degradation. The corresponding health index function training and process parameter inference approaches are presented. An actual testing dataset of a machine bearing is used to demonstrate the proposed method. The proposed method provides accurate service life and reliability predictions.},
  archive      = {J_ASOC},
  author       = {Di Liu and Xiaochuan Duan and Shaoping Wang and Xiaoyu Cui and Xiao Wu and Yu Niu and Jian Shi},
  doi          = {10.1016/j.asoc.2023.110044},
  journal      = {Applied Soft Computing},
  pages        = {110044},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reliability estimation method based on signal feature extraction and artificial neural network supported wiener process with random effects},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Growable genetic algorithm with heuristic-based local search
for multi-dimensional resources scheduling of cloud computing.
<em>ASOC</em>, <em>136</em>, 110027. (<a
href="https://doi.org/10.1016/j.asoc.2023.110027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Dimensional Resources Scheduling Problem (MDRSP, usually a multi-objective optimization problem) has attracted focus in the management of large-scale cloud computing systems as the collaborative operation of various devices in the cloud affects resource utilization and energy consumption. Effective management of the cloud requires a higher performance method to solve MDRSP. Considering the complex coupling between multi-dimensional resources and focusing on virtual machines allocation, we propose GGA-HLSA-RW (GHW, a novel family of genetic algorithms) to optimize the utilization and energy consumption of the cloud. In GGA-HLSA-RW, we add a growth stage to the genetic algorithm and construct a Growable Genetic Algorithm (GGA) using the Heuristic-based Local Search Algorithm (HLSA) with Random multi-Weights (RW) as the growth route. Based on the GHW, we propose GHW-NSGA II and GHW-MOEA/D by applying the sorting strategies and population regeneration mechanism of NSGA II and MOEA/D. To evaluate the performance of GHW, we carry out extensive experiments on the simulation dataset and AzureTraceforPacking2020 for the problems of minimizing the maximum utilization rate of resources for each dimension and minimizing total energy consumption . Experiment results demonstrate the advantages of growth strategy and dimensionality reduction strategy of GHW, as well as validate the applicability and optimality of GHW in realistic cloud computing . The experiments also demonstrate our proposed GHW-NSGA II and GHW-MOEA/D have better convergence rates and optimality than state-of-the-art NSGA II and MOEA/D.},
  archive      = {J_ASOC},
  author       = {Guangyao Zhou and WenHong Tian and Rajkumar Buyya and Kui Wu},
  doi          = {10.1016/j.asoc.2023.110027},
  journal      = {Applied Soft Computing},
  pages        = {110027},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Growable genetic algorithm with heuristic-based local search for multi-dimensional resources scheduling of cloud computing},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Short-term electricity price forecasting based on similarity
day screening, two-layer decomposition technique and bi-LSTM neural
network. <em>ASOC</em>, <em>136</em>, 110018. (<a
href="https://doi.org/10.1016/j.asoc.2023.110018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity price forecasting (EPF) has been challenged by the widespread grid integration of renewable energy (RE), so it is critical to develop a highly accurate and reliable EPF model. In this study, novel considerations of RE generation factors are made, and a quantitative model for the impact of RE on electricity prices is built using random forests (RF) and improved Mahalanobis Distance (IMD). To reduce data duplication, similar days of EPF are first selected. Then it is suggested to decompose the electricity price series into multiple intrinsic mode functions (IMF) and residuals with different frequencies using a two-layer decomposition model based on improved comprehensive ensemble empirical mode decomposition (ICEEMD) and variational mode decomposition (VMD), in order to reduce data noise and volatility. Finally, EPF model based on Bi-directional long short-term memory (Bi-LSTM) is established to forecast multiple subsequences, and the final price forecasting result is obtained after integrated processing. Experimental results show that the RF-IMD-ICEEMD-VMD-Bi-LSTM hybrid model can significantly improve the forecasting performance, reduce the prediction error of EPF, and has the best performance among the comparison models.},
  archive      = {J_ASOC},
  author       = {Keke Wang and Min Yu and Dongxiao Niu and Yi Liang and Sha Peng and Xiaomin Xu},
  doi          = {10.1016/j.asoc.2023.110018},
  journal      = {Applied Soft Computing},
  pages        = {110018},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term electricity price forecasting based on similarity day screening, two-layer decomposition technique and bi-LSTM neural network},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A combined mixed integer programming and deep neural
network-assisted heuristics algorithm for the nurse rostering problem.
<em>ASOC</em>, <em>136</em>, 109919. (<a
href="https://doi.org/10.1016/j.asoc.2022.109919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of the nurse rostering problem (NRP) is to obtain a scheduling plan that optimizes the allocation of human resources, effectively reducing work pressure on nurses and improving work efficiency and quality. Because various constraints must be considered during scheduling, the NRP is complicated and known to be NP-hard. Existing research has not combined learning mechanisms with NRP. This study constructively explores the possibility of combining an optimization method and a learning mechanism to automatically produce feasible solutions and proposes a feature vector and a reconstruction mechanism to assist in this exploration. We aim to learn a policy that is generalizable for NRPs of various sizes and design a hybrid algorithm with learning and optimization methods to solve the general NRP. The algorithm has two main parts: a deep neural network (DNN) improvement part and a reconstruction part. In the DNN improvement part, a feature vector is used to describe heterogeneous NRP solutions and normalizes these solutions to the same dimension. Then, the DNN model determines the best heuristic for approximating the local optimal solution . The method reconstructs the structure of the current solution with embedded mixed integer programming (MIP), quickly escaping the local optimum and enhancing the diversity of the search process, increasing the likelihood of determining an optimal solution. Different experiments and statistical tests were conducted by comparing various configurations and approaches. The detailed computational and statistical results demonstrate the competitive performance of the proposed method.},
  archive      = {J_ASOC},
  author       = {Ziyi Chen and Patrick De Causmaecker and Yajie Dou},
  doi          = {10.1016/j.asoc.2022.109919},
  journal      = {Applied Soft Computing},
  pages        = {109919},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combined mixed integer programming and deep neural network-assisted heuristics algorithm for the nurse rostering problem},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective genetic model for co-clustering ensemble.
<em>ASOC</em>, <em>135</em>, 110058. (<a
href="https://doi.org/10.1016/j.asoc.2023.110058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-clustering ensemble establishes a consensus co-clustering over the data, and the ensemble process can be described as an optimization problem that can be solved by genetic algorithms . However, co-clustering ensemble methods based on genetic models are very few, in which fuzzy clustering and hard clustering is not combined. In this paper, a multi-objective genetic model for co-clustering ensemble (GMCCE) is proposed, and the corresponding objective function is designed. First, to process fuzzy samples and general samples more appropriately, bilateral fuzzy clustering and hard co-clustering are combined organically. Then, chromosomes are encoded as the membership of rows and columns, and after evolution process, the best chromosome is the consensus result. Finally, the proposed model is used to design a GMCCE algorithm. To evaluate the potential of GMCCE, extensive experiments are carried out, including comparison with base co-clustering algorithms and state-of-the-art algorithms. The results demonstrate that the GMCCE algorithm outperforms other algorithms.},
  archive      = {J_ASOC},
  author       = {Yuxin Zhong and Hongjun Wang and Wenlu Yang and Luqing Wang and Tianrui Li},
  doi          = {10.1016/j.asoc.2023.110058},
  journal      = {Applied Soft Computing},
  pages        = {110058},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective genetic model for co-clustering ensemble},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolutionary intelligent control system for a flexible
joints robot. <em>ASOC</em>, <em>135</em>, 110043. (<a
href="https://doi.org/10.1016/j.asoc.2023.110043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a model for a serial robotic system with flexible joints (RFJ) using Euler–Lagrange equations, which integrates the oscillatory dynamics generated by the flexible joints at specific operating points, using a pseudo-Ornstein-Uhlenbeck process with reversion to the mean. We also propose a Stochastic Flexible - Adaptive Neural Integrated System (SF-ANFIS) to identify and control the RFJ with two degrees of freedom . For the configuration of the model, we use two adaptive strategies. One strategy is based on the Generalised Delta Rule (GDR). In contrast, a second strategy is based on the EDA-MAGO algorithm (Estimation Distribution Algorithms - Multi-dynamics Algorithm for Global Optimisation), improving online learning. We considered three stages for analysing and validating the proposed SF-ANFIS model: a first identification stage, a second stage defined by the adaptive control process, and a final stage or cancellation of oscillations. Results show that, for the identification stage, the SF-ANFIS model showed better statistical indices than the MADALINE model in control for the second joint , which presents the greatest oscillations; among those that stand out, the IOA (0.9955), VG (1.0012) and UAPC2 (-0.0003). For the control stage, The SF-ANFIS model showed, in a general way, the best behaviour in the system’s control for both joints, thanks to the capacity to identify and cancel oscillations based on the advanced sampling that defines the EDA algorithm. For the cancellation of the oscillations stage, the SF-ANFIS achieved the best behaviour, followed by the MADALINE model, where it is highlighted the UAPC2 (0.9525) value.},
  archive      = {J_ASOC},
  author       = {Alejandro Pena and Juan C. Tejada and Juan David Gonzalez-Ruiz and Lina María Sepúlveda-Cano and Francisco Chiclana and Fabio Caraffini and Mario Gongora},
  doi          = {10.1016/j.asoc.2023.110043},
  journal      = {Applied Soft Computing},
  pages        = {110043},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary intelligent control system for a flexible joints robot},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-source fuzzy comprehensive evaluation. <em>ASOC</em>,
<em>135</em>, 110042. (<a
href="https://doi.org/10.1016/j.asoc.2023.110042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-layer fuzzy comprehensive evaluation (FCE) is a common evaluation method, but it is not applicable to the situation when factor sets of data sources have intersections. Due to this situation being common in today’s data deluge era, the two-layer FCE needs to be improved urgently. Therefore, this study proposes the multi-source FCE which does not limit whether factor sets have intersections or not. The objective of the multi-source FCE is to develop a comprehensive evaluation of an object (individual, product quality, customer credibility, etc.) on the basis of the evaluation data of each data source, whether factor sets of data sources have intersections or not. The underlying idea of the multi-source FCE is to fuse a multi-source FCE problem to an FCE problem. In the fusion process, this study forms an objective function to obtain optimal weights, and this makes the theoretical rationality of the multi-source FCE guaranteed. Finally, an example is given to illustrate the proposed method.},
  archive      = {J_ASOC},
  author       = {Xueyan Xu and Fusheng Yu and Witold Pedrycz and Xubo Du},
  doi          = {10.1016/j.asoc.2023.110042},
  journal      = {Applied Soft Computing},
  pages        = {110042},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-source fuzzy comprehensive evaluation},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A health state-related ensemble deep learning method for
aircraft engine remaining useful life prediction. <em>ASOC</em>,
<em>135</em>, 110041. (<a
href="https://doi.org/10.1016/j.asoc.2023.110041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining useful life (RUL) prediction for aircraft engines is crucial to enabling predictive maintenance . Current RUL predictions for aircraft engines mainly focus on model-based and data-driven methods that employ a single model or algorithm. Few studies on RUL prediction have been conducted by using an ensemble method that combines prediction results from multiple algorithms. As an emerging frontier technology, ensemble learning has become a topic of interest in the field of RUL prediction because it can achieve better prediction performance than single model. In this study, a health-state-related (HSR) ensemble deep learning method that considers different degradation laws of the aircraft engine is proposed for RUL prediction. First, a health baseline is constructed and lifetime degradation is divided into several health states to represent different degradation laws. The Mahalanobis distance to the health baseline is utilized to recognize the current health state of the aircraft engine. Second, three deep learning methods, namely stacked autoencoder , convolutional neural network and long short-term memory, are selected as member algorithms and trained on different health states. Thus, different member algorithm sets are constructed for different health states, learning different degradation laws in different health states. Third, self-adaptive ensemble weight sets for different health states are calculated by applying ridge regression, which can comprehensively utilize the prediction results of each algorithm model in different health states. A case study is conducted by using a dataset of the PHM data challenge to demonstrate the effectiveness of the proposed method. The experiment result shows that the proposed HSR ensemble deep learning method can considerably improve prediction performance compared with methods that are based on a single prediction algorithm and ensemble learning method that does not consider the health state.},
  archive      = {J_ASOC},
  author       = {Yujie Cheng and Jiyan Zeng and Zili Wang and Dengwei Song},
  doi          = {10.1016/j.asoc.2023.110041},
  journal      = {Applied Soft Computing},
  pages        = {110041},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A health state-related ensemble deep learning method for aircraft engine remaining useful life prediction},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EGNN: Graph structure learning based on evolutionary
computation helps more in graph neural networks. <em>ASOC</em>,
<em>135</em>, 110040. (<a
href="https://doi.org/10.1016/j.asoc.2023.110040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph neural networks (GNNs) have been successfully applied in many fields due to their characteristics of neighborhood aggregation and have achieved state-of-the-art performance. While most GNNs process graph data, the original graph data is frequently noisy or incomplete, resulting in suboptimal GNN performance. In order to solve this problem, a Graph Structure Learning (GSL) method has recently emerged to improve the performance of graph neural networks by learning a graph structure that conforms to the ground truth. However, the current strategy of GSL is to iteratively optimize the optimal graph structure and a single GNN, which will encounter several problems in training, namely vulnerability and overfitting. A novel GSL approach called evolutionary graph neural network (EGNN) has been introduced in this work in order to improve defense against adversarial attacks and enhance GNN performance. Unlike the existing GSL method, which optimizes the graph structure and enhances the parameters of a single GNN model through alternating training methods, evolutionary theory has been applied to graph structure learning for the first time in this work. Specifically, different graph structures generated by mutation operations are used to evolve a set of model parameters in order to adapt to the environment (i.e., to improve the classification performance of unlabeled nodes). An evaluation mechanism is then used to measure the quality of the generated samples in order to retain only the model parameters (progeny) with good performance. Finally, the progeny that adapt to the environment are retained and used for further optimization. Through this process, EGNN overcomes the instability of graph structure learning and always evolves the best progeny, providing new solutions for the advancement and development of GSL. Extensive experiments on various benchmark datasets demonstrate the effectiveness of EGNN and the benefits of evolutionary computation-based graph structure learning.},
  archive      = {J_ASOC},
  author       = {Zhaowei Liu and Dong Yang and Yingjie Wang and Mingjie Lu and Ranran Li},
  doi          = {10.1016/j.asoc.2023.110040},
  journal      = {Applied Soft Computing},
  pages        = {110040},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EGNN: Graph structure learning based on evolutionary computation helps more in graph neural networks},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Applying particle swarm optimization algorithm-based
collaborative filtering recommender system considering rating and
review. <em>ASOC</em>, <em>135</em>, 110038. (<a
href="https://doi.org/10.1016/j.asoc.2023.110038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of electronic commerce , the availability of a large amount of information on the products, as well as from other users, make the customers’ decision-making processes more time-consuming. The recommender system has emerged to assist the users choosing suitable products more easily, while companies can precision marketing more effectively. To solve the above-mentioned problems, this study adopted the particle swarm optimization algorithm (PSO) to determine the most suitable similarity of consumer ratings to avoid the problem of data distortion due to data sparsity . Moreover, bidirectional encoder representations from transformers (BERT) were applied to extract the characteristics of consumer feedbacks. Finally, the PSO was employed to determine the appropriate weight matrix and combine the characteristics of different data types . The combination of rating and review data could improve the recommendation performance. In addition, the proposed method was applied on six datasets of Amazon, and it outperformed several existing methods in terms of mean absolute error and mean squared error .},
  archive      = {J_ASOC},
  author       = {R.J. Kuo and Shu-Syun Li},
  doi          = {10.1016/j.asoc.2023.110038},
  journal      = {Applied Soft Computing},
  pages        = {110038},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Applying particle swarm optimization algorithm-based collaborative filtering recommender system considering rating and review},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Short-term PV power forecasting based on time series
expansion and high-order fuzzy cognitive maps. <em>ASOC</em>,
<em>135</em>, 110037. (<a
href="https://doi.org/10.1016/j.asoc.2023.110037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving short-term accurate photovoltaic power forecasting is of great significance to improve the efficiency of grid operation, especially in power stations where historical values of meteorological parameters are lagged or not recorded. Therefore, numerous studies have been presented to predict the intricate stochastic properties of photovoltaic power sequence in recent years. However, due to the limitation of algorithm performance and the effect of noise in the time series, previously proposed methods may not extract effective features from power data. Furthermore, most studies have only focused on point prediction, which ignores the uncertain information and unavoidable forecast bias. In this study, a hybrid framework based on fuzzy information granulation algorithm, improved variational mode decomposition technique, and high-order fuzzy cognitive maps is proposed to fill these gaps. Comparison experiments were set up using 5-minute photovoltaic power data from Alice Springs, Australia. The computational results not only demonstrate that the proposed framework significantly improves forecast accuracy of short-term photovoltaic power, but also achieves effective interval prediction by fuzzy information.},
  archive      = {J_ASOC},
  author       = {Yurui Xia and Jianzhou Wang and Ziyuan Zhang and Danxiang Wei and Li Yin},
  doi          = {10.1016/j.asoc.2023.110037},
  journal      = {Applied Soft Computing},
  pages        = {110037},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term PV power forecasting based on time series expansion and high-order fuzzy cognitive maps},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Integrating intuitionistic preferences into the graph model
for conflict resolution with applications to an ecological compensation
conflict in taihu lake basin. <em>ASOC</em>, <em>135</em>, 110036. (<a
href="https://doi.org/10.1016/j.asoc.2023.110036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological compensation is an important means to solve the problem of river basin water pollution, which has been widely used at home and abroad. However, the study of ecological compensation problems from the perspective of the management by using the graph model for conflict resolution (GMCR) is still a challenge. Thus, the aim of this article is to construct the graph model with intuitionistic preferences to model and analyze real life ecological compensation conflicts. Specifically, due to the lack of information and the inherent fuzziness of human cognition in ecological compensation conflicts, the intuitionistic preference structure is incorporated into the framework of the GMCR to represent the decision makers’ (DMs’) uncertain preference. Based on this, two sets of intuitionistic stability definitions are defined to analyze and solve the possible resolution for conflict problems with two DMs and multiple DMs, respectively. Note that a state is called intuitionistic stability for DM in a graph model based on the given intuitionistic stability definitions if and only if there is no reward for a given state to leave the initial state, that is to say, the evaluated value of the focus DM is less than an intuitionistic satisficing threshold of the same DM. Furthermore, the intuitionistic equilibrium is considered as the possible solution to the conflict problems and is intuitionistic stable for all DMs in the graph model. Finally, the ecological compensation conflicts of the Taihu Lake basin are used to demonstrate the usefulness and correctness of the graph model with intuitionistic preferences.},
  archive      = {J_ASOC},
  author       = {Dayong Wang and Jing Huang and Yejun Xu},
  doi          = {10.1016/j.asoc.2023.110036},
  journal      = {Applied Soft Computing},
  pages        = {110036},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating intuitionistic preferences into the graph model for conflict resolution with applications to an ecological compensation conflict in taihu lake basin},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-dimensional VGGNet for high-dimensional data.
<em>ASOC</em>, <em>135</em>, 110035. (<a
href="https://doi.org/10.1016/j.asoc.2023.110035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a deep learning model for classifying high-dimensional data and seek to achieve optimal evaluation accuracy and robustness based on multicriteria decision-making (MCDM) for high-dimensional data analysis applications during comprehensive evaluation (CE) activities. We propose a novel one-dimensional visual geometry group network (1D_VGGNet) to overcome the problem that high-dimensional data are too complicated and unstable to be feasibly applied. Then, to effectively handle one-dimensional MCDM, we present a 1D_VGGNet classifier to replace the two-dimensional convolution operation applied to image data with a one-dimensional convolution operation applied to one-dimensional MCDM. Furthermore, to solve the invariance problem of the generated feature maps, the maxpooling kernel size can be flexibly adjusted to effectively meet the requirements of reducing the feature map dimension and speeding up training and prediction on different datasets. The improvement is reasonable for various high-dimensional data application scenarios. Moreover, we propose a novel objective function to accurately evaluate network performance since the objective function includes a variety of representative performance evaluation metrics , and the average value is calculated as one of the CE metrics. The experimental results illustrate that the proposed framework outperforms a one-dimensional convolutional neural network (1D_CNN) for comprehensive classification on the Shaoxing University student achievement dataset and the MIT-BIH Arrhythmia database and achieves average gains of 36.3\% and 12.1\% in terms of the designated evaluation metric.},
  archive      = {J_ASOC},
  author       = {Sheng Feng and Liping Zhao and Haiyan Shi and Mengfei Wang and Shigen Shen and Weixing Wang},
  doi          = {10.1016/j.asoc.2023.110035},
  journal      = {Applied Soft Computing},
  pages        = {110035},
  shortjournal = {Appl. Soft. Comput.},
  title        = {One-dimensional VGGNet for high-dimensional data},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gene selection of microarray data using heatmap analysis and
graph neural network. <em>ASOC</em>, <em>135</em>, 110034. (<a
href="https://doi.org/10.1016/j.asoc.2023.110034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is not feasible to investigate the whole genes at a microscopic level for disease classification in Genomics. It might take substantial time to execute any meaningful analysis and the computational resources will be misused as not all the genes are responsible for the disease linked to a cell. Currently, it is quite challenging to select the most significant genes from high-dimensional microarray data for disease classification. In search of a better process, a novel gene subset selection technique has been developed based on Heatmap Analysis and Graph Neural Network ( HAGNN ). In the proposed method, a heatmap analysis has been performed for the different classes of microarray data to obtain the Region of Interest ( ROIs ). These ROIs are extracted from the original dataset and undergo a node reduction technique followed by an edge reduction technique in Graph Neural Network ( GNN ). This paper is concluded with an optimal subset of the most significant genes that cause cancer. The popular base classifiers have been used to evidence the importance of the selected genes as compared to the original data with the help of several metrics. The obtained results clearly show that the proposed methodology outperformed the other existing methods and make a greater impact on the advancement of the GNN -based gene selection method.},
  archive      = {J_ASOC},
  author       = {Soumen Kumar Pati and Ayan Banerjee and Sweta Manna},
  doi          = {10.1016/j.asoc.2023.110034},
  journal      = {Applied Soft Computing},
  pages        = {110034},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gene selection of microarray data using heatmap analysis and graph neural network},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-dimensional feature selection method based on
modified gray wolf optimization. <em>ASOC</em>, <em>135</em>, 110031.
(<a href="https://doi.org/10.1016/j.asoc.2023.110031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For data mining tasks on high-dimensional data, feature selection is a necessary pre-processing stage that plays an important role in removing redundant or irrelevant features and improving classifier performance. The Gray Wolf optimization algorithm is a global search mechanism with promising applications in feature selection, but tends to stagnate in high-dimensional problems with locally optimal solutions. In this paper, a modified gray wolf optimization algorithm is proposed for feature selection of high-dimensional data. The algorithm introduces ReliefF algorithm and Coupla entropy in the initialization process , which effectively improves the quality of the initial population. In addition, modified gray wolf optimization includes two new search strategies: first, a competitive guidance strategy is proposed to update individual positions, which make the algorithm’s search more flexible; second, a differential evolution-based leader wolf enhancement strategy is proposed to find a better position where the leader wolf may exist and replace it, which can prevent the algorithm from falling into local optimum. The results on 10 high-dimensional small-sample gene expression datasets demonstrate that the proposed algorithm selects less than 0.67\% of the features, improves the classification accuracy while further reducing the number of features, and obtains very competitive results compared with some advanced feature selection methods. The comprehensive study analysis shows that proposed algorithm better balances the exploration and exploration balance, and the two search strategies are conducive to the improvement of gray wolf optimization search capability.},
  archive      = {J_ASOC},
  author       = {Hongyu Pan and Shanxiong Chen and Hailing Xiong},
  doi          = {10.1016/j.asoc.2023.110031},
  journal      = {Applied Soft Computing},
  pages        = {110031},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A high-dimensional feature selection method based on modified gray wolf optimization},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient regular expression inference approach for
relevant image extraction. <em>ASOC</em>, <em>135</em>, 110030. (<a
href="https://doi.org/10.1016/j.asoc.2023.110030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional approaches for extracting relevant images automatically from web pages are error-prone and time-consuming. To improve this task, operations such as preparing a larger dataset and finding new features are used in the web data extraction approaches. However, these operations are difficult and laborious. In this study, we propose a fully-automated approach based on alignment of regular expressions to automatically extract the relevant images from web pages. The automatically constructed regular expressions has been applied to a classification task for the first time. In this respect, a multi-stage inference approach is developed for generating regular expressions from the attribute values of relevant and irrelevant image elements in web pages. The proposed approach reduces the complexity of the alignment of two regular expressions by applying a constraint on a version of the Levenshtein distance algorithm. The classification accuracy of regular expression approaches is compared with the naive Bayes, logistic regression , J48, and multilayer perceptron classifiers on a balanced relevant image retrieval dataset consisting of 360 image element samples for 10 shopping websites. According to the cross-validation results, the regular expression inference-based classification achieved a 0.98 f-measure with only 5 frequent n-grams, and it outperformed other classifiers on the same set of features. The classification efficiency of the proposed approach is measured at 0.108 ms, which is very competitive with other classifiers.},
  archive      = {J_ASOC},
  author       = {Hayri Volkan Agun and Erdinç Uzun},
  doi          = {10.1016/j.asoc.2023.110030},
  journal      = {Applied Soft Computing},
  pages        = {110030},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient regular expression inference approach for relevant image extraction},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Acceleration-based artificial bee colony optimizer for a
distributed permutation flowshop scheduling problem with
sequence-dependent setup times. <em>ASOC</em>, <em>135</em>, 110029. (<a
href="https://doi.org/10.1016/j.asoc.2023.110029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed permutation flowshop scheduling problem (DPFSP) has attracted a lot of attentions in recent years. Furthermore, sequence-dependent setup time (SDST) often occurs in the job’s production, and total flowtime becomes a more important and meaningful objective for the modern dynamic productive environment. Therefore, the DPFSP with SDST under minimization of total flowtime is investigated in this paper, and the mathematical model is established. To solve the problem, this paper presents three constructive heuristics and an effective discrete artificial bee colony (EDABC) method based on the acceleration. In the EDABC, the constructive heuristics are used to initialize population. Moreover, based on the problem-specific of sequence-dependent, the employed bees stage, onlooker bees stage and scout bees stage are improved by exploring the acceleration methods to evaluate the neighboring solutions. Further, to strengthen the performance of the proposed algorithm, three effective problem-oriented local search methods are developed, including referenced local search (RLS), improved local search 1 (ILS1) and improved local search 2 (ILS2). RLS is used to the best solution after the initialization, and ILS1 and ILS2 are designed in the framework of improved variable neighborhood descent. The extensive computational experiences are carried out, and the results show that the proposed methods, including three heuristics and EDABC, have outstanding performance for the SDST/DPFSP to minimize the total flowtime.},
  archive      = {J_ASOC},
  author       = {Yang Yu and Feng-Qi Zhang and Jiang-Ping Huang},
  doi          = {10.1016/j.asoc.2023.110029},
  journal      = {Applied Soft Computing},
  pages        = {110029},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Acceleration-based artificial bee colony optimizer for a distributed permutation flowshop scheduling problem with sequence-dependent setup times},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Future video frame prediction based on generative
motion-assistant discriminative network. <em>ASOC</em>, <em>135</em>,
110028. (<a href="https://doi.org/10.1016/j.asoc.2023.110028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of deep learning , video frame prediction has become a hotspot in the field of computer vision due to its wide range of applications in anomaly detection , robot decision-making, weather forecasting, and autonomous driving . Although current video frame prediction methods have made remarkable progress, the majority of them directly generate prediction frames by extracting potential spatial distribution patterns from the video data. They lack spatiotemporal information modeling, which leads to high latency, ambiguity, and unrealistic results. In this work, we propose an end-to-end video prediction network model (Generative Differential-Assisted Discriminative Network, abbreviated as GDDNet). It combines the advantages of the difference generation method to extract short-term variations from the image and attention mechanisms to recall global contextual motion information. Furthermore, the differential attention mechanism (DAM) module can guide the model to allocate attention resources more efficiently. These strategies considerably improve the model’s ability to represent motion features in video frames. To further optimize the prediction effect, we introduce adversarial training to enhance the clarity and authenticity of the video frames. In order to ensure the consistency of spatiotemporal distribution between predicted and real frames, we introduce a sequential frame discriminator . Experimental results on the KITTI, UCF-101, and Caltech pedestrian datasets demonstrate the effectiveness of the GDDNet and compare it to the state-of-the-art model. Multi-frame prediction and ablation experiments show that our proposed model not only improves the quality of predictions, but also provides a more flexible prediction framework.},
  archive      = {J_ASOC},
  author       = {Chenming Li and Xiuhong Chen},
  doi          = {10.1016/j.asoc.2023.110028},
  journal      = {Applied Soft Computing},
  pages        = {110028},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Future video frame prediction based on generative motion-assistant discriminative network},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage coevolution method for deep CNN: A case study in
smart manufacturing. <em>ASOC</em>, <em>135</em>, 110026. (<a
href="https://doi.org/10.1016/j.asoc.2023.110026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart manufacturing system is very complex and there are a lot of different types of data to deal with, which lead to the difficulty of usage. Frequently manually tuning hyperparameters and modifying the architecture of the network have become a major problem for participants, and it has seriously affected the application and promotion of Deep learning (DL) in industry. In order to solve this problem, a novel self-evolving deep CNN method: two-stage coevolution method (TSC) is proposed in this paper to automatically optimize the hyperparameters and effectively evolve the most suitable network by summarizing the characteristics of the excellent artificial architectures. The first stage is mainly to optimize the hyperparameters with Orthogonal experimental algorithm. The second stage is to produce the best deep CNN with the optimized hyperparameters through self-evolving computation. In the second stage, three well-known deep-CNN architectures are used as the initialization seeds and each seed is presented by a particle and a gene to coevolve the necessary factors for a deep CNN driven by particle swarm optimization (PSO) and genetic algorithm (GA). At last, a case study for smart manufacturing systems was carried out to demonstrate the effectiveness and convenience of the proposed method. And the TSC method was also compared with other two self-evolving methods. The experiment results show that TSC method is superior over other well-known algorithms.},
  archive      = {J_ASOC},
  author       = {Yuanju Qu and Yue Ma and Xinguo Ming and Yangpeng Wang and Shenghui Cheng and Xianghua Chu},
  doi          = {10.1016/j.asoc.2023.110026},
  journal      = {Applied Soft Computing},
  pages        = {110026},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage coevolution method for deep CNN: A case study in smart manufacturing},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An enhanced vortex search algorithm based on fluid particle
density transfer for global and engineering optimization. <em>ASOC</em>,
<em>135</em>, 110024. (<a
href="https://doi.org/10.1016/j.asoc.2023.110024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vortex search algorithm is a new meta-heuristic algorithm for solving optimization problems . However, it has some disadvantages such as weak stability, tendency to trap into local minima, and high iterations. To overcome these disadvantages, this paper proposes a density transfer vortex search algorithm based on Cauchy distribution and Archimedean spiral surrounding for global and engineering optimization. The algorithm is divided into two populations. In the first population, the mutual action of the Cauchy random step and Gaussian step is introduced to improve the searchability. To improve the exploitation ability of the algorithm in the current optimal solution, an Archimedean spiral surrounding mechanism is introduced in the second population. Moreover, we introduce the vortex flow mechanism, which mimics the physical characteristics of the vortex flow . With the number of iterations increasing, candidate solutions of the first population are gradually affected by the vortex, and their density continuously shifts to the second population. The relationship between the exploration and exploitation in different iteration periods is reasonably adjusted through the density transfer of two populations. In the experiment, the performance of the proposed algorithm and six competitive algorithms are tested on the IEEE CEC2021 global optimization benchmark suite, eight real-world engineering optimizations and a scheduling problem for automated storage and retrieval systems (AS/RS). The experimental results show that the proposed algorithm has significant improvements in convergence precision, speed and stability. The proposed algorithm not only solves complex global optimizations and real-world engineering optimizations with good convergence performance, but also improves efficiency and reduces energy consumption when solving the scheduling problem of AS/RS.},
  archive      = {J_ASOC},
  author       = {Jingsen Liu and Haoran Li and Yu Li and Huan Zhou},
  doi          = {10.1016/j.asoc.2023.110024},
  journal      = {Applied Soft Computing},
  pages        = {110024},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced vortex search algorithm based on fluid particle density transfer for global and engineering optimization},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary algorithm for vehicle routing for shared
e-bicycle battery replacement and recycling. <em>ASOC</em>,
<em>135</em>, 110023. (<a
href="https://doi.org/10.1016/j.asoc.2023.110023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared electric bicycles (e-bicycles) are becoming an increasingly popular model of sharing economy that provides the public with a convenient and labor-saving transport alternative. Efficient battery replacement is critical to maintain the operational capability of e-bicycles, but is quite difficult as there are often a large number of e-bicycles that are widely dispersed. In this paper, we study a problem of scheduling multiple vehicles to deliver fully-charged batteries to and take back low-power batteries from shared e-bicycles scattered in numerous parking spots, while the low-power batteries are recharged in battery depots for potential re-utilization. The aim of this problem is to minimize the completion time of all battery replacement tasks so as to keep a high operational efficiency of the system. The problem differs from existing vehicle routing problems in two main special features: (1) equivalent exchange of fully-charged batteries and low-power batteries; (2) recycling of batteries. To efficiently solve this special problem, we propose an evolutionary algorithm using variable local-search-based mutation to balance global exploration and local exploitation and employing enhanced local search around each newly found best known solution to improve accuracy. Experiments on test instances and real-world instances demonstrate the competitive performance of the proposed method.},
  archive      = {J_ASOC},
  author       = {Yu-Jun Zheng and Xin Chen and Hong-Fang Yan and Min-Xia Zhang},
  doi          = {10.1016/j.asoc.2023.110023},
  journal      = {Applied Soft Computing},
  pages        = {110023},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary algorithm for vehicle routing for shared e-bicycle battery replacement and recycling},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective hyper heuristic-based memetic algorithm for the
distributed assembly permutation flow-shop scheduling problem.
<em>ASOC</em>, <em>135</em>, 110022. (<a
href="https://doi.org/10.1016/j.asoc.2023.110022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an effective Hyper Heuristic-based Memetic Algorithm (HHMA) is proposed to solve the Distributed Assembly Permutation Flow-shop Scheduling Problem (DAPFSP) with the objective of minimizing the maximum completion time. A novel searching-stage-based solution representation scheme is presented for both improving the search efficiency and maintaining potential solutions. In the global search stage, Estimation of Distribution Algorithm (EDA) is employed as the high level strategy of EDA-based Hyper Heuristic (EDAHH) to find promising product sequences for further exploitation. Based on the newly found knowledge of critical-products, several efficient Low-Level Heuristics (LLHs) are well designed to construct the LLH set so that the powerful exploration ability of the EDAHH can be guaranteed. A simulated-annealing-like type of acceptance criterion is also embedded into each LLH to avoid premature convergence. Then a Critical-Products-based Referenced Local Search (CP-RLS) method is proposed to improve the quality of superior sub-population by operating on the sub-job-sequences derived from the critical products. The benefit of the presented CP-RLS lies in the excellent exploitation ability with substantially reduced computational cost. Finally, performance evaluation and comparison are both carried out on a benchmark set and the results demonstrate the superiority of HHMA over the state-of-the-art algorithms for the DAPFSP.},
  archive      = {J_ASOC},
  author       = {Hong-Bo Song and You-Hong Yang and Jian Lin and Jing-Xuan Ye},
  doi          = {10.1016/j.asoc.2023.110022},
  journal      = {Applied Soft Computing},
  pages        = {110022},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective hyper heuristic-based memetic algorithm for the distributed assembly permutation flow-shop scheduling problem},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-reservoir echo state networks with hodrick–prescott
filter for nonlinear time-series prediction. <em>ASOC</em>,
<em>135</em>, 110021. (<a
href="https://doi.org/10.1016/j.asoc.2023.110021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Echo State Network (ESN) is a representative model for reservoir computing , which is capable of high-speed model training for machine learning tasks with time series data . Extended models of the ESN, such as Multi-Reservoir ESNs (MRESNs), have been intensively studied for performance improvement in recent years. In this study, we propose a new model called an HP-MRESN by combining an MRESN with the Hodrick–Prescott (HP) filter for nonlinear time series prediction. The proposed HP-MRESN comprises three basic components: a time series decomposer, a reservoir state extractor, and an ensemble decoder. In the time series decomposer, we recursively leverage the HP filter to decompose original time-series data into multiple trend and cycle components. In the reservoir state extractor, each time series component is fed into a corresponding reservoir-state encoder for generating a reservoir state which is extracted as it is or through the principal component analysis. In the ensemble decoder, the states of multiple reservoirs are collected and processed to produce model outputs. Moreover, we propose a greedy algorithm to automatically find the best model architectures under designated hyperparameters for different prediction tasks. Experimental results on a total of 24 nonlinear time-series prediction tasks with 6 real-world datasets demonstrate that our proposed HP-MRESN not only can outperform some existing representative MRESN models and fully-trained RNN models but also can have relatively low training time. In addition, performance comparisons between the HP-MRESN and related MRESN models with other prepossessing methods show the benefit of time series decompositions using the HP filter. The codes of the proposed method are publicly available on https://github.com/Ziqiang-IRCN/HP-MRESN .},
  archive      = {J_ASOC},
  author       = {Ziqiang Li and Yun Liu and Gouhei Tanaka},
  doi          = {10.1016/j.asoc.2023.110021},
  journal      = {Applied Soft Computing},
  pages        = {110021},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-reservoir echo state networks with Hodrick–Prescott filter for nonlinear time-series prediction},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Redefining teaching-and-learning-process in TLBO and its
application in cloud. <em>ASOC</em>, <em>135</em>, 110017. (<a
href="https://doi.org/10.1016/j.asoc.2023.110017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teaching-Learning-Based Optimization (TLBO) algorithm was developed to solve single-objective optimization problems . TLBO is inspired by the theory of teaching-learning mechanism. Basic TLBO works better for unimodal problems but poorly for multi-model problems because of its poor exploration. To provide a fair exploration for solving complex optimization problems, we redefined the learning strategy to the basic TLBO. This newly redefined variant is called Multi-Teacher Teaching-Learning Based Optimization (MT-TLBO). The performance of MT-TLBO was tested on the latest optimization benchmark functions , CEC-C06, 2019. It is observed that the performance of MT-TLBO is superb comparatively. After that, it is simulated to solve workflow scheduling problems by minimizing the execution cost of the standard workflow and maximizing the workload on computing resources in a cloud environment. Finally, it is found that MT-TLBO produces minimal execution cost and a fair workload distribution on resources on standard benchmark workflow, comparatively.},
  archive      = {J_ASOC},
  author       = {Satya Deo Kumar Ram and Shashank Srivastava and K.K. Mishra},
  doi          = {10.1016/j.asoc.2023.110017},
  journal      = {Applied Soft Computing},
  pages        = {110017},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Redefining teaching-and-learning-process in TLBO and its application in cloud},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Creating deep neural networks for text classification tasks
using grammar genetic programming. <em>ASOC</em>, <em>135</em>, 110009.
(<a href="https://doi.org/10.1016/j.asoc.2023.110009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification is one of the Natural Language Processing (NLP) tasks. Its objective is to label textual elements, such as phrases, queries, paragraphs, and documents. In NLP, several approaches have achieved promising results regarding this task. Deep Learning-based approaches have been widely used in this context, with deep neural networks (DNNs) adding the ability to generate a representation for the data and a learning model. The increasing scale and complexity of DNN architectures was expected, creating new challenges to design and configure the models. In this paper, we present a study on the application of a grammar-based evolutionary approach to the design of DNNs, using models based on Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM), and Graph Neural Networks (GNNs). We propose different grammars, which were defined to capture the features of each type of network, also proposing some combinations, verifying their impact on the produced designs and performance of the generated models. We create a grammar that is able to generate different networks specialized on text classification, by modification of Grammatical Evolution (GE), and it is composed of three main components: the grammar, mapping, and search engine. Our results offer promising future research directions as they show that the projected architectures have a performance comparable to that of their counterparts but can still be further improved. We were able to improve the results of a manually structured neural network in 8, 18\% in the best case.},
  archive      = {J_ASOC},
  author       = {Dimmy Magalhães and Ricardo H.R. Lima and Aurora Pozo},
  doi          = {10.1016/j.asoc.2023.110009},
  journal      = {Applied Soft Computing},
  pages        = {110009},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Creating deep neural networks for text classification tasks using grammar genetic programming},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Genetic algorithm based probabilistic model for agile
project success in global software development. <em>ASOC</em>,
<em>135</em>, 109998. (<a
href="https://doi.org/10.1016/j.asoc.2023.109998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development organizations implement agile methods in global software development (GSD) to leverage benefits in terms of low development cost, continuous project delivery, and high-quality product. Despite the benefits, using the agile process in the GSD organizations is not a straightforward task due to inhibit additional risk factors that could lead agile projects to failure. GSD organizations are continuously improving their process management activities to improve the rate of agile project success. Project success could be improved if the project manager or organization management has preliminary information about the derailment of the project features. This work aims to investigate the most influential agile project features in determining the project outcomes and develop a cost-effective, and effort-based prediction model to improve the probability of successful completion in the globally distributed environment. To do so, a nature-inspired optimization algorithm i.e., genetic algorithm (GA), has been employed to achieve these goals. In the proposed model, GA considers the probability of success with respect to cost to determine probable project outcomes. An efficacy measure is formulated as a fitness function in GA which maximizes the success of agile project outcomes relative to cost. The optimization model has been tested with two different prediction models i.e., Naive Bayes classifier (NBC) and logistic regression (LR). We performed the experiment on data gathered through the survey administered from the globally distributed agile projects. The results demonstrate that prediction models calculate the efficacy for best solutions as 0.531 and 0.5850 for NBC and LR, respectively. Moreover, the ranking of each project feature based on their relative cost identified using NBC and LR have more similarities. The t test results are significant, i.e., t = = 6.068, p = = 0.001 &amp;lt; 0.005, which indicates that no significant differences have been observed between the ranking assigned by two different methods ( NBC and LR). The results reveal that the developed prediction model based on identified eight agile project features that the GSD organization management and agile team need to focus more on to facilitate cost-effective successful implementation of agile projects.},
  archive      = {J_ASOC},
  author       = {Mohammad Shameem and Mohammad Nadeem and Abu Taha Zamani},
  doi          = {10.1016/j.asoc.2023.109998},
  journal      = {Applied Soft Computing},
  pages        = {109998},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Genetic algorithm based probabilistic model for agile project success in global software development},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Population-based discrete state transition algorithm with
decomposition and knowledge guidance applied to electrolytic cell
maintenance decision. <em>ASOC</em>, <em>135</em>, 109996. (<a
href="https://doi.org/10.1016/j.asoc.2023.109996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With continuous scale expansion and higher safety requirements in modern aluminum electrolysis , it is more and more necessary to realize an intelligent decision-making for shutting-down and running of aluminum reduction cells (SRARC). However, this realization needs to solve a special single-objective constrained integer optimization problem (SCIOP), where the key challenge is a high requirement for the constraint-handling ability. In this paper, based on a high-scalability intelligent optimization algorithm called discrete state transition algorithm (DSTA), a population-based DSTA with decomposition and knowledge guidance (PDSTA/D-S) is proposed. This PDSTA/D-S improves the constraint-handling ability of DSTA from three aspects. Firstly, a hybrid framework combining DSTA with genetic algorithm is proposed. Secondly, a decomposition-based multi-objective optimization for constrained problems with uniformly-angled weighted sum vector is proposed. Thirdly, the manual decision-making is transferred to a knowledge-based transformation operator of DSTA. Therefore, a high-level performance of decision-making for SRARC can be obtained. The related experiments on a SRARC which is built from the practical production have demonstrated that the proposed PDSTA/D-S not only makes an effective improvement of DSTA from three aspects, but also has a more advanced performance compared with other existing high-performance intelligent optimization algorithms.},
  archive      = {J_ASOC},
  author       = {Jue Shi and Xiaofang Chen and Yongfang Xie and Hongliang Zhang and Yubo Sun},
  doi          = {10.1016/j.asoc.2023.109996},
  journal      = {Applied Soft Computing},
  pages        = {109996},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Population-based discrete state transition algorithm with decomposition and knowledge guidance applied to electrolytic cell maintenance decision},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using empirical wavelet transform and high-order fuzzy
cognitive maps for time series forecasting. <em>ASOC</em>, <em>135</em>,
109990. (<a href="https://doi.org/10.1016/j.asoc.2023.109990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies on time series forecasting have employed fuzzy cognitive maps (FCMs). However, it is required to develop techniques capable of effective responses and great accuracy for large-scale non-stationary time series, such as sharp-fluctuated datasets. For forecasting such data, the present study introduces an accurate, precise, and efficient combined forecasting framework with ridge regression, high-order FCM (HFCM), and empirical wavelet transform (EWT). The proposed learning method named as EWT HFCM EWTHFCM . This model employs EWT for non-stationary time series transformation into a multivariate sequence. The HFCM then models each multivariate time series through a node. For the optimized learning part, ridge regression is employed to perform the process for the HFCM representation of large-scale time series. The main reasons to choose the ridge regression are its circle-like phenomenon which makes it a perfect fit for being combined with EWT. This fitness is due to the optimal filter bank selection behavior and adaptivity feature of EWT, eventually making the ridge regression fast, efficient, and accurate. Subsequently, HFCM models and forecasts the multivariate time series and helps find the trend pattern. Then, the multivariate time series is reconstructed by inverse EWT to predict time series at all time steps. The root mean square error (RMSE) is measured on 15 real-life standard datasets, of which 8 are utilized to compare the proposed framework to 11 state-of-art algorithms. According to the experimental results and comparison with other research papers, we recommend using EWT HFCM EWTHFCM as a new, superior, and accurate method for many forecasting purposes and circumstances.},
  archive      = {J_ASOC},
  author       = {Hossein Abbasian Mohammadi and Sedigheh Ghofrani and Ali Nikseresht},
  doi          = {10.1016/j.asoc.2023.109990},
  journal      = {Applied Soft Computing},
  pages        = {109990},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using empirical wavelet transform and high-order fuzzy cognitive maps for time series forecasting},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel evolutionary algorithm for water distribution
network design, using the masters–students model in distributed
environment. <em>ASOC</em>, <em>135</em>, 109986. (<a
href="https://doi.org/10.1016/j.asoc.2023.109986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Water Distribution Networks Design Problem study is widespread in the scientific community due to its practical applicability. In this work, a sequential evolutionary algorithm has been designed, developed, and successfully applied to solve small and medium instances; it has been called the Evolutionary Algorithm for Water Distribution Network Design Problem. This algorithm is executed in centralized environments and can perfectly solve these instances in less than minutes. For large-scale real-world instances, the Evolutionary Algorithm has been adapted to work in distributed environments by using a novel parallel model, also proposed in this work, called the Masters–Students​ model. This model has been used for designing, developing, and implementing the resulting parallel evolutionary algorithm , named PEA-WDND. The Evolutionary Algorithm would last for days in the case of real-world instances, but the parallel algorithm solves them in seconds or, at maximum, in minutes. This study shows that the parallel algorithm yields an execution time lower than the execution time obtained from the evolutionary algorithm for different theoretical and practical instances.},
  archive      = {J_ASOC},
  author       = {Erika Yesenia Avila-Melgar and Marco Antonio Cruz-Chávez and Beatriz Martínez-Bahena and Marta Lilia Eraña-Díaz and Martín H. Cruz-Rosales},
  doi          = {10.1016/j.asoc.2023.109986},
  journal      = {Applied Soft Computing},
  pages        = {109986},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel evolutionary algorithm for water distribution network design, using the Masters–Students model in distributed environment},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy decision support system for sustainable operational
performance optimization for boiler unit in milk process industry.
<em>ASOC</em>, <em>135</em>, 109983. (<a
href="https://doi.org/10.1016/j.asoc.2023.109983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a new sustainable operational performance optimization-based decision support system for assessing the failure behaviour of the Boiler Unit (BU) in the milk process industry. A novel fuzzy Jaya Lambda–Tau Optimization (JLTO) approach-based modelling was established for tabulating various reliability indices of BU. Fuzzy Set (FS) theory-based concepts were applied to consider the vagueness/uncertainty involved in the collected data. JLTO approach-based results show decrease in the boiler unit’s availability with the increase in uncertainty level. Further, to improve the boiler unit’s availability using qualitative data obtained from expert’s opinion, Failure Mode and Effect Analysis (FMEA) was performed. To address the shortcomings of conventional FMEA approach-based results and to pinpoint the riskier failure causes contributing to decrease in system’s availability, Fuzzy-Complex Proportional Assessment (FCOPRAS) decision-making approach was integrated with FMEA approach. Fuzzy Combinative Distance-based Assessment (FCODAS) approach was also applied to compare the ranking results for accurate decision-making. The proposed integrated framework predicts the failure behaviour with high accuracy by considering the uncertainty and finding the riskier failure causes of the considered system which is one of the main drawbacks of existing FS theory-based integrated models. The proposed model-based results will help in developing an optimal digitalized maintenance schedule, which will be beneficial for improving the overall performance of the considered unit. The digitalized maintenance schedule will also be useful for BU of various other industries across the globe. Sensitivity analysis was also done for assessing the robustness of the proposed sustainable operational performance optimization-based decision support system.},
  archive      = {J_ASOC},
  author       = {Nand Gopal and Dilbagh Panchal},
  doi          = {10.1016/j.asoc.2023.109983},
  journal      = {Applied Soft Computing},
  pages        = {109983},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy decision support system for sustainable operational performance optimization for boiler unit in milk process industry},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Multi-criteria fuzzy portfolio selection based on three-way
decisions and cumulative prospect theory. <em>ASOC</em>, <em>134</em>,
110033. (<a href="https://doi.org/10.1016/j.asoc.2023.110033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portfolio selection is one of the hottest issue in decision-making and management engineering. But due to the capital market natural complexity and investors’ irrational behaviors, it is not easy for investors to achieve their predefined goals . In this study, we propose a novel three-way decisions model based on cumulative prospect theory and outranking relations. Compared with traditional two-way decisions, the introduction of a boundary region into the three-way decisions theory makes it possible to reduce decision risk. By constructing an outranked set for each alternative and a hybrid multi-criteria decision-making matrix, three strategies are proposed by us to design the three-way decisions model. In order to test the effectiveness of the proposed model, we introduce it into a fuzzy multi-period portfolio selection case and design an improved particle swarm optimization as the solution algorithm . Finally, the effectiveness of the algorithm is validated by some test functions. And an experiment based on real market data validates the proposed multi-period portfolio selection model outperforms other compared models in terms of return, risk and risk-adjusted criteria.},
  archive      = {J_ASOC},
  author       = {Xianhe Wang and Bo Wang and Tiantian Li and Huaxiong Li and Junzo Watada},
  doi          = {10.1016/j.asoc.2023.110033},
  journal      = {Applied Soft Computing},
  pages        = {110033},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria fuzzy portfolio selection based on three-way decisions and cumulative prospect theory},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter extraction of solar photovoltaic models using
queuing search optimization and differential evolution. <em>ASOC</em>,
<em>134</em>, 110032. (<a
href="https://doi.org/10.1016/j.asoc.2023.110032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the photovoltaic (PV) model’s multi-model and nonlinear properties, extracting its parameters is a difficult problem to solve. Furthermore, because of the features of the problem, the algorithms that are used to solve it are subject to becoming stuck in local optima. Nonetheless, proper estimation of the parameters is essential due to the large impact they have on the performance of the PV system in terms of current and energy production. Moreover, the majority of the previously proposed algorithms have satisfactory results for determining PV model parameters. However, for precision and robustness, they generally use a lot of computational resources, such as the quantity of fitness assessments. For alleviating the previous problems, in this paper, an improved queuing search optimization (QSO) algorithm dependent on the differential evolution (DE) technique and bound-constraint amendment procedure, which is called IQSODE, has been presented to efficiently extract the PV parameter values for various PV models. The DE algorithm is applied to each solution generated by the QSO algorithm in order to increase population diversity. IQSODE is tested against other state-of-the-art algorithms. The practical and statistical findings show that IQSODE outperforms other methods in extracting parameters from PV models such as single diode, double diode, and photovoltaic module models. Also, the performance of the proposed algorithm is assessed utilizing two practical manufacturer’s datasheets (TFST40 and MCSM55). Statistically, the IQSODE outperforms other state-of-the-art algorithms in terms of convergence speed , reliability, and accuracy. Thus, the presented method is deemed to be a viable solution for PV model parameter extraction.},
  archive      = {J_ASOC},
  author       = {Amr A. Abd El-Mageed and Amr A. Abohany and Hatem M.H. Saad and Karam M. Sallam},
  doi          = {10.1016/j.asoc.2023.110032},
  journal      = {Applied Soft Computing},
  pages        = {110032},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter extraction of solar photovoltaic models using queuing search optimization and differential evolution},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A diversity-enhanced memetic algorithm for solving electric
vehicle routing problems with time windows and mixed backhauls.
<em>ASOC</em>, <em>134</em>, 110025. (<a
href="https://doi.org/10.1016/j.asoc.2023.110025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electric vehicle routing problem (EVRP) has been studied increasingly because of environmental concerns. However, existing studies on the EVRP mainly focus on time windows and sole linehaul customers, which might not be practical as backhaul customers are also ubiquitous in reality. In this study, we investigate an EVRP with time windows and mixed backhauls (EVRPTWMB), where both linehaul and backhaul customers exist and can be served in any order. To address this challenging problem, we propose a diversity-enhanced memetic algorithm (DEMA) that integrates three types of novel operators, including genetic operators based on adaptive selection mechanism, a selection operator based on similarity degree, and modification operators for tabu search . Experimental results on 54 new instances and two classical benchmarks show that the proposed DEMA can effectively solve the EVRPTWMB as well as other related problems. Furthermore, a case study on a realistic instance with up to 200 customers and 40 charging stations in China also confirms the desirable performance of the DEMA.},
  archive      = {J_ASOC},
  author       = {Jianhua Xiao and Jingguo Du and Zhiguang Cao and Xingyi Zhang and Yunyun Niu},
  doi          = {10.1016/j.asoc.2023.110025},
  journal      = {Applied Soft Computing},
  pages        = {110025},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A diversity-enhanced memetic algorithm for solving electric vehicle routing problems with time windows and mixed backhauls},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weighted kappa measures for ordinal multi-class
classification performance. <em>ASOC</em>, <em>134</em>, 110020. (<a
href="https://doi.org/10.1016/j.asoc.2023.110020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing the classification performance of ordinal classifiers is a challenging problem under imbalanced data compositions. Considering the critical impact of the metrics on the choice of classifiers, employing a metric with the highest performance is crucial. Although Cohen’s kappa measure is used for performance assessment, there are better-performing agreement measures under different formations of ordinal confusion matrices . This research implements weighted agreement measures as evaluation metrics for ordinal classifiers. The applicability of agreement and mainstream performance metrics to various practice fields under challenging data compositions is assessed. The sensitivity of the metrics in detecting subtle distinctions between ordinal classifiers is analyzed. Five kappa-like agreement measures with six weighting schemes are employed as evaluation metrics. Their reliability/usefulness is compared to the mainstream and recently proposed metrics, including F1, Matthews correlation coefficient, and informational agreement. The performance of 37 metrics is analyzed in two extensive numerical studies, including synthetic confusion matrices and real datasets. Promising metrics under practical circumstances are identified, and recommendations about the best metric to evaluate ordinal classifiers under different conditions are made. Overall, the weighted Scott’s pi-measure is found useful, sensitive to small differences in the classification performance, and reliable under general conditions.},
  archive      = {J_ASOC},
  author       = {Ayfer Ezgi Yilmaz and Haydar Demirhan},
  doi          = {10.1016/j.asoc.2023.110020},
  journal      = {Applied Soft Computing},
  pages        = {110020},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted kappa measures for ordinal multi-class classification performance},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel hybrid model combining βSARMA and LSTM for time
series forecasting. <em>ASOC</em>, <em>134</em>, 110019. (<a
href="https://doi.org/10.1016/j.asoc.2023.110019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is an important and active research area due to the significance of prediction and decision-making in several applications. Most commonly used models for time series forecasting are based on Gaussianity assumption, e.g., AR (Autoregressive), ARMA (Autoregressive moving average), ARIMA (autoregressive integrated moving average), etc. But for many applications, the gaussianity presumption is too restrictive, hence non-gaussian based time series models are becoming more and more popular. Many of the hybrid models that are currently used in the literature combine ARIMA and artificial neural network (ANN) while taking various time series data into account with different approaches. Although the accuracy of the predictions made by these models is higher than that of the individual models, there is room for further accuracy improvement if the dynamics of the provided time series is taken into consideration while applying the models. In this study, a new hybrid β S A R M A − L S T M βSARMA−LSTM model for time series forecasting is proposed. It combines a non-gaussian based time series model called Beta seasonal autoregressive moving average β S A R M A βSARMA with a recurrent neural network model called Long Short Term Memory Network (LSTM). The advantage of the proposed model is that β S A R M A βSARMA is based on the beta distribution , which contains stochastic seasonal dynamics, and LSTM is a recurrent neural network which can be used to a variety of sequential data with high levels of accuracy. In this work, a β S A R M A βSARMA model is applied on a given time series data in order to identify the linear structure in the data and the error between the original and β S A R M A βSARMA predicted data is considered as a nonlinear model , which is then modelled using LSTM. The asymptotic stability of the proposed approach is analysed to ensure that the proposed model may not show increasing variance over time. The proposed hybrid β S A R M A − L S T M βSARMA−LSTM model along with individual ARIMA, β S A R M A βSARMA , LSTM, Multilayer perceptron (MLP), and some existing hybrid model ARIMA-ANN was applied on some real, simulated, and experimental datasets such as: relative humidity data, Air passengers, Bitcoin , Sunspots and Mackey Glass series. The results obtained using proposed model for all these data sets show higher prediction accuracy for both one-step and multi-step ahead forecasts.},
  archive      = {J_ASOC},
  author       = {Bhupendra Kumar and Sunil and Neha Yadav},
  doi          = {10.1016/j.asoc.2023.110019},
  journal      = {Applied Soft Computing},
  pages        = {110019},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel hybrid model combining βSARMA and LSTM for time series forecasting},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective SHADE with manta ray foraging optimizer for
structural design problems. <em>ASOC</em>, <em>134</em>, 110016. (<a
href="https://doi.org/10.1016/j.asoc.2023.110016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a hybrid multi-objective success history-based parameter adaptive differential evolution (SHADE) with manta ray foraging optimizer (MRFO) for structural design problems, called MO-SHADE-MRFO. In the proposed algorithm, the updating rules of SHADE, a variant of differential evolution with great performance, are combined with the operators from MRFO, a recent swarm-based metaheuristic algorithm inspired from the manta ray with cyclone, chain and somersault foraging behaviors, which can balance the exploration and exploitation of the algorithm for structural design problems. Furthermore, MO-SHADE-MRFO utilizes the external archive to save and update the obtained Pareto fronts during the optimization process. The proposed algorithm is verified by multi-objective truss optimization problems with two objectives of minimizing the structural weight and the compliance, including 10-bar, 25-bar, 37-bar, 120-bar, 200-bar and 942-bar truss problems. Moreover, 9 different multi-objective metaheuristic algorithms are implemented to compare with the proposed algorithm, where three metrics are used to measure the performance of the algorithms, including hypervolume (HV), inverted generational distance (IGD), and spacing-to-extent (STE). According to the experimental results, MO-SHADE-MRFO can provide the best statistical values of HV, IGD and STE in most cases, ranking the first among the compared algorithms. Besides, the proposed algorithm also gives well-distributed Pareto solutions for the tested problems, illustrating the effectiveness of the hybrid updating rules of SHADE and MRFO.},
  archive      = {J_ASOC},
  author       = {Changting Zhong and Gang Li and Zeng Meng and Haijiang Li and Wanxin He},
  doi          = {10.1016/j.asoc.2023.110016},
  journal      = {Applied Soft Computing},
  pages        = {110016},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective SHADE with manta ray foraging optimizer for structural design problems},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TSO-GCN: A graph convolutional network approach for
real-time and generalizable truss structural optimization.
<em>ASOC</em>, <em>134</em>, 110015. (<a
href="https://doi.org/10.1016/j.asoc.2023.110015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The truss structural optimization is a major research topic in the field of structural, civil, aerospace engineering , etc. Conventionally, the truss structural optimization methods are often inefficient because they run in iterations and are computationally intensive during each iteration. In this paper, we propose a generative design framework based on deep learning networks to predict three dimensional structural topologies without iterative computations while achieving acceptable accuracy. Different from most commonly used deep learning driven structural optimization approaches that transform structural geometries into images, our innovation lies in solving optimization problems based on geometric analysis from the perspective of graphs, and efficiently predict the near optimal truss structure with a negligible computational time. Therefore, it shows potential significance in design scenarios when the structure is described by connections and massive number of optimization computations are required. The proposed generative design framework is called TSO-GCN (Truss Structural Optimization - Graph Convolutional Network), which is an encoder–decoder based graph convolution network designed to map the problem definition and the desired truss layout. Once trained, it is expected to directly predict truss layouts by feeding into the encoded optimization problem definitions. To train the TSO-GCN, a dataset consisting of different number of problem definitions and their corresponding minimum volume results generated by conventional methods is constructed and fed into the network. The experiments show that TSO-GCN can predict results with the near optimal accuracy compared with conventional approaches while costing an average time of only 1 s. Besides, extra experiments with totally unseen dataset are performed to demonstrate the generalizability of the proposed method.},
  archive      = {J_ASOC},
  author       = {Shuai Zheng and Lingjie Qiu and Fengxin Lan},
  doi          = {10.1016/j.asoc.2023.110015},
  journal      = {Applied Soft Computing},
  pages        = {110015},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TSO-GCN: A graph convolutional network approach for real-time and generalizable truss structural optimization},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An artificial intelligence-as-a-service architecture for
deep learning model embodiment on low-cost devices: A case study of
COVID-19 diagnosis. <em>ASOC</em>, <em>134</em>, 110014. (<a
href="https://doi.org/10.1016/j.asoc.2023.110014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus Disease-2019 (COVID-19) causes Severe Acute Respiratory Syndrome-Corona Virus-2 (SARS-CoV-2) and has opened several challenges for research concerning diagnosis and treatment. Chest X-rays and computed tomography (CT) scans are effective and fast alternatives to detect and assess the damage that COVID causes to the lungs at different stages of the disease. Although the CT scan is an accurate exam, the chest X-ray is still helpful due to the cheaper, faster, lower radiation exposure, and is available in low-incoming countries. Computer-aided diagnostic systems based on Artificial Intelligence (AI) and computer vision are an alternative to extract features from X-ray images, providing an accurate COVID-19 diagnosis. However, specialized and expensive computational resources come across as challenging. Also, it needs to be better understood how low-cost devices and smartphones can hold AI models to predict diseases timely. Even using deep learning to support image-based medical diagnosis, challenges still need to be addressed once the known techniques use centralized intelligence on high-performance servers, making it difficult to embed these models in low-cost devices. This paper sheds light on these questions by proposing the Artificial Intelligence as a Service Architecture (AIaaS), a hybrid AI support operation, both centralized and distributed, with the purpose of enabling the embedding of already-trained models on low-cost devices or smartphones. We demonstrated the suitability of our architecture through a case study of COVID-19 diagnosis using a low-cost device. Among the main findings of this paper, we point out the performance evaluation of low-cost devices to handle COVID-19 predicting tasks timely and accurately and the quantitative performance evaluation of CNN models embodiment on low-cost devices.},
  archive      = {J_ASOC},
  author       = {Larissa Ferreira Rodrigues Moreira and Rodrigo Moreira and Bruno Augusto Nassif Travençolo and André Ricardo Backes},
  doi          = {10.1016/j.asoc.2023.110014},
  journal      = {Applied Soft Computing},
  pages        = {110014},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An artificial intelligence-as-a-service architecture for deep learning model embodiment on low-cost devices: A case study of COVID-19 diagnosis},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DNN surrogates for turbulence closure in CFD-based shape
optimization. <em>ASOC</em>, <em>134</em>, 110013. (<a
href="https://doi.org/10.1016/j.asoc.2023.110013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A DNN-based surrogate for turbulence (and transition) closure of the Reynolds-Averaged Navier–Stokes (RANS) equations is presented. The DNN configuration, namely the hyperparameters (number of layers, number of neurons, type of activation functions) and the DNN input data result from a Metamodel-Assisted Evolutionary Algorithm (MAEA)-based optimization. The trained DNN replaces the numerical solution of the turbulence and/or transition model PDEs during the solution of the RANS equations, by estimating the necessary turbulent viscosity field in each pseudo-time step iteration . The gain from using such a computational tool becomes pronounced in cases in which many calls to the CFD tools are necessary; a typical example is CFD-based (shape) optimization. Thus, the new model (abbreviated to RANS-DNN) is used as the evaluation software in MAEA-based shape optimization problems. Three aerodynamic cases covering a wide gamut of applications from 2D to 3D, internal to external and compressible to incompressible flows are selected to demonstrate the capabilities of the RANS-DNN model. Using the RANS-DNN instead of the standard RANS solver a significant reduction in the optimization turnaround time is achieved in cases dealing with an isolated airfoil , a turbomachinery cascade and a car.},
  archive      = {J_ASOC},
  author       = {Marina G. Kontou and Varvara G. Asouti and Kyriakos C. Giannakoglou},
  doi          = {10.1016/j.asoc.2023.110013},
  journal      = {Applied Soft Computing},
  pages        = {110013},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DNN surrogates for turbulence closure in CFD-based shape optimization},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing high-risk glucose forecasting errors by evolving
interpretable models for type 1 diabetes. <em>ASOC</em>, <em>134</em>,
110012. (<a href="https://doi.org/10.1016/j.asoc.2023.110012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes mellitus is a metabolic disease involving high blood glucose levels that can lead to serious medical consequences. Hence, for diabetic patients the prediction of future glucose levels is essential in the management of the disease. Most of the forecasting approaches in the literature evaluate the effectiveness of glucose predictors only with numerical metrics. These approaches are limited because they evenly treat all the errors without considering their different clinical impact that could involve lethal effects in dangerous situations such as hypo- or hyperglycemia. To overcome such a limitation, this paper aims to devise models for reducing high-risk glucose forecasting errors for Type 1 diabetic patients. For this purpose, we exploit a Grammatical Evolution algorithm to induce personalized and interpretable forecasting glucose models assessed with a novel, composite metric to satisfy both clinical and numerical requirements of the estimated predictions. To assess the effectiveness of the proposed approach, a real-world data set widely used in literature, consisting of data from several patients suffering from Type 1 diabetes, has been adopted. The experimental findings show that the induced models are interpretable and capable of assuring predictions with a good tradeoff between medical quality and numerical accuracy and with remarkable performance in reducing high-risk glucose forecasting errors. Furthermore, their performance is better than or comparable to that of other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {A. Della Cioppa and I. De Falco and T. Koutny and U. Scafuri and M. Ubl and E. Tarantino},
  doi          = {10.1016/j.asoc.2023.110012},
  journal      = {Applied Soft Computing},
  pages        = {110012},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reducing high-risk glucose forecasting errors by evolving interpretable models for type 1 diabetes},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fermatean fuzzy based quality function deployment
methodology for designing sustainable mobility hub center.
<em>ASOC</em>, <em>134</em>, 110001. (<a
href="https://doi.org/10.1016/j.asoc.2023.110001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility Centers can provide integrated different options of mobility and intermodal services, and their associated facilities and all supporting technologies. Mobility hub centers are centrally located that enable people to go where they want to go without a private vehicle by combining many options such as walking, public transportation, bike sharing and car sharing. A well-designed mobility hub reduces GHG emissions by reducing vehicle miles within the city and provides accessible space for transferring of passengers from one mode of transport to another. To fulfill these aims, mobility hubs must be in the center and technologically well designed so that all individuals in the society can easily access it. Mobility hubs should also be able to increase or improve the mobility of passengers, and in turn ensure the satisfaction of all citizens. Due to the importance of passenger requirements (PRs) and sustainable design requirements (SDRs), to satisfy PRs while designing a mobility hub, in this study, a Fermatean Fuzzy based Quality Function Deployment (QFD) methodology is developed and applied to determine PRs and SDRs to satisfy PRs while designing a mobility hub in Kadikoy/Istanbul. As a relatively new extension of fuzzy sets, Fermatean Fuzzy Set (FFS) is used, in this study, for judgements to model fuzziness and uncertainty more extensively. According to results, the most important SDR to design mobility hub is “Mobility as a service (MaaS)” while the second and third most important SDRs are “Increase smartphone connectivity to create awareness of multi-modal options” and “Increase green spaces in the terminal”. This study’s results can support authorities in making strategic decisions in favor of future mobility hub designs.},
  archive      = {J_ASOC},
  author       = {Sukran Seker and Nezir Aydin},
  doi          = {10.1016/j.asoc.2023.110001},
  journal      = {Applied Soft Computing},
  pages        = {110001},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fermatean fuzzy based quality function deployment methodology for designing sustainable mobility hub center},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consideration of reciprocal judgments through decomposed
fuzzy analytical hierarchy process: A case study in the pharmaceutical
industry. <em>ASOC</em>, <em>134</em>, 110000. (<a
href="https://doi.org/10.1016/j.asoc.2023.110000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of academic research considering improvements/applications in the multiple criteria decision-making (MCDM) field has been increasing in the literature day by day. Among these works, the most used MCDM method is undoubtedly the analytic hierarchy process (AHP). Decomposed Fuzzy Sets (DFS) have been recently proposed to the literature in order to measure the inconsistency in expert judgments by providing optimistic and pessimistic point of view. The main objective of this paper is to extend AHP method by using DFS. DFS makes pairwise comparisons in AHP more reliable by considering the individual answers given by the decision makers to the reciprocal questions under vagueness and impreciseness. The proposed method, Decomposed Fuzzy Analytic Hierarchy Process (DF-AHP), is applied to determine the importance degrees of evaluation criteria in the pharmaceutical industry in order to illustrate the applicability of the approach. Based on the results, the “quality” is determined to be the most significant criterion in this industry. The results obtained from DF-AHP are compared with the results obtained from both traditional AHP and Pythagorean fuzzy AHP. The comparative analysis has also shown the most significant criterion is the same with the proposed method but slightly differences in the rank of the sub-criteria. The research can help businesses better understand the critical risks in the pharmaceutical industry.},
  archive      = {J_ASOC},
  author       = {Selcuk Cebi and Fatma Kutlu Gündoğdu and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2023.110000},
  journal      = {Applied Soft Computing},
  pages        = {110000},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consideration of reciprocal judgments through decomposed fuzzy analytical hierarchy process: A case study in the pharmaceutical industry},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-efficient decentralized elastic-net broad
learning system based on quantized and censored communications.
<em>ASOC</em>, <em>134</em>, 109999. (<a
href="https://doi.org/10.1016/j.asoc.2023.109999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an efficient learning model, the broad learning system (BLS) has achieved great success in machine learning and various applications due to its outstanding performance. Differing from typical deep learning based models, the BLS has a more straightforward structure and few parameters. In this study, we focus on developing a decentralized version of the elastic-net BLS (D-ENBLS) with communication efficiency. The scenario we considered is that the training data is distributed throughout a network of interconnected agents, which prohibits sharing the raw data owing to resource limitations or privacy concerns. In such a distributed paradigm, the communication between agents is an important issue to investigate. From the perspective of saving communication resources, we introduce quantization and communication censoring strategies on the D-ENBLS algorithm to improve the communication procedure and minimize the communication cost with minimal performance degradation . The novel algorithm refers to as DQC-ENBLS, in which quantization reduces the number of bits for each transmission, whereas communication censoring reduces the total number of transmissions. By formulating the training problem as a finite-sum minimization, the alternating multiplication method (ADMM) is employed to solve the optimization problem in a decentralized manner. The experimental results verify that the DQC-ENBLS algorithm can reduce communication cost while maintaining similar performance on the test datasets .},
  archive      = {J_ASOC},
  author       = {Jinyan Liang and Wu Ai and Huazhou Chen and Guoqiang Tang},
  doi          = {10.1016/j.asoc.2023.109999},
  journal      = {Applied Soft Computing},
  pages        = {109999},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Communication-efficient decentralized elastic-net broad learning system based on quantized and censored communications},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained image analysis for facial expression
recognition using deep convolutional neural networks with bilinear
pooling. <em>ASOC</em>, <em>134</em>, 109997. (<a
href="https://doi.org/10.1016/j.asoc.2023.109997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions reflect people’s feelings, emotions, and motives, attracting researchers to develop a self-acting automatic facial expression recognition system. With the advances of deep learning frameworks for automatic facial expression recognition, the model complexity, limited training samples, and subtle micro facial muscle movements make the facial emotion expression system challenging. This research proposed a deep learning framework using fine-grained facial action unit detection to identify facial activity, behavior, and mood and recognize a person’s emotions based on these individual patterns. The proposed facial expression recognition system involves pre-processing, feature representation and normalization, hyper-parameter tuning, and classification. Here, two different convolutional neural network models have been introduced because of feature learning and representation, followed by classification. Various advanced feature representation methods, such as image augmentation, matrix normalization, fine-tuning, and transfer learning methods, have been applied to improve the performance of the proposed work. The proposed work’s performance and efficiency are evaluated under different approaches. The proposed work has been tested on standard Static Facial Expressions in the Wild, short name SFEW 1.0, SFEW 2.0, and Indian Movie Face (IMFDB) benchmark databases. The performances of the proposed system due to these databases are 48.15\%, 80.34\%, and 64.17\%, respectively. The quantitative analysis of these results is compared with the standard existing state-of-the-art methods that show the proposed model outperforms the other competing methods.},
  archive      = {J_ASOC},
  author       = {Sanoar Hossain and Saiyed Umer and Ranjeet Kr. Rout and M. Tanveer},
  doi          = {10.1016/j.asoc.2023.109997},
  journal      = {Applied Soft Computing},
  pages        = {109997},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fine-grained image analysis for facial expression recognition using deep convolutional neural networks with bilinear pooling},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly detection based on weighted fuzzy-rough density.
<em>ASOC</em>, <em>134</em>, 109995. (<a
href="https://doi.org/10.1016/j.asoc.2023.109995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density-based method is a more widely used anomaly detection . However, most of the existing density-based methods mainly focus on dealing with certainty data and do not consider the problem of uncertainty and fuzziness of the data. Fuzzy rough set theory , as an important mathematical model of granular computing , provides an effective method for information processing of uncertain data. For this reason, this paper proposes an anomaly detection based on fuzzy-rough density. First, the fuzzy-rough density is defined to describe the degree of aggregation of objects. Then, fuzzy entropy is introduced to compute the weights of each attribute. Further, an anomaly score is constructed to characterize the anomaly degree of the samples, which takes into account both the density and fuzziness of the samples. Finally, extensive experiments are conducted on publicly available data with nine popular detection methods. The experimental results show that the proposed method achieves better performance on three types of datasets.},
  archive      = {J_ASOC},
  author       = {Zhong Yuan and Baiyang Chen and Jia Liu and Hongmei Chen and Dezhong Peng and Peilin Li},
  doi          = {10.1016/j.asoc.2023.109995},
  journal      = {Applied Soft Computing},
  pages        = {109995},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Anomaly detection based on weighted fuzzy-rough density},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute reductions of quantitative dominance-based
neighborhood rough sets with a-stochastic transitivity of fuzzy
preference relations. <em>ASOC</em>, <em>134</em>, 109994. (<a
href="https://doi.org/10.1016/j.asoc.2023.109994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reductions based on approximate operations have never been proposed in quantitative dominance-based neighborhood rough sets. In this paper, we mainly discuss these problems and present an accelerated process by constructing a particular transitivity of fuzzy preference relations with aggregation operators called A-stochastic transitivity. Firstly, definitions of approximating qualities are given by considering the ordered consistence between condition and decision attributes . Secondly, theories of attribute reductions based on approximate operations are analyzed. Thirdly, the accelerated process of attribute reductions is investigated with A-stochastic transitivity and the algorithm is designed. Moreover, the efficiency of the proposed method is stressed by execution time of attribute reductions, which is evaluated by statistical hypothesis testing on some public data sets. Finally, the effectiveness of our algorithm is verified by comparing results with classical methods in rough set theory and machine learning .},
  archive      = {J_ASOC},
  author       = {Shuyun Yang and Hongying Zhang and Guang Shi and Yingjian Zhang},
  doi          = {10.1016/j.asoc.2023.109994},
  journal      = {Applied Soft Computing},
  pages        = {109994},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attribute reductions of quantitative dominance-based neighborhood rough sets with A-stochastic transitivity of fuzzy preference relations},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Simultaneous detection for multiple anomaly data in
internet of energy based on random forest. <em>ASOC</em>, <em>134</em>,
109993. (<a href="https://doi.org/10.1016/j.asoc.2023.109993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly Data Detection plays a core role in defensing cyber-attacks, ensuring reliability, protecting security and improving robustness in smart grid. This study constructs a fast method to detect multi-node anomaly data including outlier and implicit characteristics. The outliers are determined according historical datasets, while implicit features invoked are summed up from satisfying with operation requirements of smart grid. For accomplishment of simultaneous detection for multiple abnormal data, Improved Random Forest Algorithm (IRFA) is proposed through modifying the following aspects: data set reconstruction, bootstrap sampling, decision tree generation and majority voting. In addition, parallel strategies are designed to improve algorithmic efficiency . Plenty of simulations are used to evaluate performance of the developed method through plenty of loads information from various transformers. Here not only explore the impact of various features and decision trees on algorithm’s performances, but also investigate the correspondence between node number and decision tree. And the accuracy in this study is compared with that of BP Neural Network and Support Vector Machine . Simulations demonstrate that here the algorithm has excellent detection accuracy and efficiency, i.e., the detection accuracy for single-node is 97\%, while that for multiple-node are higher than 95\%. It is versatile and universal, and very suitable for abnormal data identification from multiple-node in smart grid with the cyber–physical systems interactions.},
  archive      = {J_ASOC},
  author       = {Qiang Li and Limei Zhang and Guanghui Zhang and Hanyi Ouyang and Muke Bai},
  doi          = {10.1016/j.asoc.2023.109993},
  journal      = {Applied Soft Computing},
  pages        = {109993},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simultaneous detection for multiple anomaly data in internet of energy based on random forest},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrated outranking techniques based on spherical fuzzy
information for the digitalization of transportation system.
<em>ASOC</em>, <em>134</em>, 109992. (<a
href="https://doi.org/10.1016/j.asoc.2023.109992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digitalization of the traditional technologies have facilitated humans globally as these technologies are not only time saving, modern, environment friendly but are economic as well. This study is concerned with the digitalization of the public transportation system in Istanbul to reduce the environmental pollution that is a big cause of different harmful diseases and climate changes. This article aims to manifest the decision-making aptitude of the elimination and choice translating reality methods working within the boundaries of modern, well structured and highly adaptable model of spherical fuzzy sets keenly focusing on group decision-making by presenting two significant techniques. Thus, the proposed methodologies operate by considering the dominance and subordination between alternatives relying on the prime concepts of concordance, discordance and indifferent sets. Further, these sets are exploited to determine the concordance and discordance indices in accordance with the weights of the decision criteria. Finally, the decision is made by observing the outranking relations among alternatives using the threshold values. The proposed strategies seek help from the technique of spherical fuzzy analytical hierarchy process for the evaluation of criteria weights. Noticing the widespread scope and appealing benefits of digital systems, this article targets the digitalization of the public transportation system in Istanbul. The potential of proposed approaches are highlighted via comparative studies.},
  archive      = {J_ASOC},
  author       = {Muhammad Akram and Kiran Zahid and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2023.109992},
  journal      = {Applied Soft Computing},
  pages        = {109992},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated outranking techniques based on spherical fuzzy information for the digitalization of transportation system},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensembling shallow siamese architectures to assess
functional asymmetry in alzheimer’s disease progression. <em>ASOC</em>,
<em>134</em>, 109991. (<a
href="https://doi.org/10.1016/j.asoc.2023.109991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of methods based on artificial intelligence for the classification of medical imaging is widespread. Given the high dimensionality of this type of images, it is imperative to use the information contained in relevant regions for further classification. This information can be derived from the morphology of the region of interest, in terms of measurements such as area, perimeter, etc. However, the performance of the classification system strongly depends on the correct selection of the type of information employed. We propose in this work an alternative for evaluating differences between brain regions that relies on the basis of Siamese neural networks . Initially, brain scans are delimited by an anatomical atlas. Next, each pair of regions of interest is then entered into a Siamese network, which is formed by relating the distance between the two individual outputs and the corresponding label. Features are extracted from the embeddings of the final linear layer. Finally, the classification is performed by combining the characteristics of each pair of regions into an ensemble architecture. Performance was assessed by determining how asymmetry between the right and left hemispheres changes during progressive brain degeneration, from mild cognitive impairment to severe atrophy associated with Alzheimer’s disease (AD). Our method discriminates with an accuracy of 98.95\% between controls and AD patients, and most important, it predicts the cognitive decline in patients suffering from mild cognitive impairment that will develop AD before it occurs with an accuracy of 78.41\%. These results demonstrate the applicability of our proposal in the study of a wide range of pathologies.},
  archive      = {J_ASOC},
  author       = {Juan E. Arco and Andrés Ortiz and Diego Castillo-Barnes and Juan M. Górriz and Javier Ramírez and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.asoc.2023.109991},
  journal      = {Applied Soft Computing},
  pages        = {109991},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensembling shallow siamese architectures to assess functional asymmetry in alzheimer’s disease progression},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse deep neural networks for modeling aluminum
electrolysis dynamics. <em>ASOC</em>, <em>134</em>, 109989. (<a
href="https://doi.org/10.1016/j.asoc.2023.109989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have become very popular in modeling complex nonlinear processes due to their extraordinary ability to fit arbitrary nonlinear functions from data with minimal expert intervention. However, they are almost always overparameterized and challenging to interpret due to their internal complexity. Furthermore, the optimization process to find the learned model parameters can be unstable due to the process getting stuck in local minima. In this work, we demonstrate the value of sparse regularization techniques to significantly reduce the model complexity. We demonstrate this for the case of an aluminum extraction process, which is highly nonlinear system with many interrelated subprocesses . We trained a densely connected deep neural network to model the process and then compared the effects of sparsity promoting ℓ 1 ℓ1 regularization on generalizability , interpretability , and training stability. We found that the regularization significantly reduces model complexity compared to a corresponding dense neural network. We argue that this makes the model more interpretable, and show that training an ensemble of sparse neural networks with different parameter initializations often converges to similar model structures with similar learned input features. Furthermore, the empirical study shows that the resulting sparse models generalize better from small training sets than their dense counterparts.},
  archive      = {J_ASOC},
  author       = {Erlend Torje Berg Lundby and Adil Rasheed and Jan Tommy Gravdahl and Ivar Johan Halvorsen},
  doi          = {10.1016/j.asoc.2023.109989},
  journal      = {Applied Soft Computing},
  pages        = {109989},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sparse deep neural networks for modeling aluminum electrolysis dynamics},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Importance analysis of decision making factors based on
fuzzy decision trees. <em>ASOC</em>, <em>134</em>, 109988. (<a
href="https://doi.org/10.1016/j.asoc.2023.109988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification is one important and commonly used Machine Learning technique . The classification effectiveness can be influenced by different factors and initial data used for the classifier induction and in the process of classification, including. The quality of measurements of the input attributes of new samples for the classification can affect the accuracy and reliability of the classification result . At the same time, the degree of influence of different attributes is not the same. There are some attributes that are most important for the classification because have a greater influence on the classification result than others. A new method for the determination of the most important attributes is proposed. This method is developed based on the approach of Importance Analysis, which is widely used in reliability engineering. The proposed method investigates the sensitivity of the classification from the input attributes and indicates the attributes for which changes have the most impact on the classification result. The attribute’s importance is evaluated by the special index which is known as structural importance in reliability engineering. This method application is illustrated in the paper by Fuzzy Decision Tree , but it can be used for any other classifiers induction. The use of a fuzzy classifier allows for taking into consideration of possible uncertainty of initial data.},
  archive      = {J_ASOC},
  author       = {Elena Zaitseva and Jan Rabcan and Vitaly Levashenko and Miroslav Kvassay},
  doi          = {10.1016/j.asoc.2023.109988},
  journal      = {Applied Soft Computing},
  pages        = {109988},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Importance analysis of decision making factors based on fuzzy decision trees},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A feature selection approach based on NSGA-II with ReliefF.
<em>ASOC</em>, <em>134</em>, 109987. (<a
href="https://doi.org/10.1016/j.asoc.2023.109987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a hybrid feature selection algorithm based on a multi-objective algorithm with ReliefF (MOFS-RFGA) is proposed. Combining the advantages of filter and wrapper methods, the two types of algorithms are hybridized to improve the capability when solving feature selection problems. First, the ReliefF algorithm is used to score the features according to their importance to the instance class. Then, the feature scoring information is used to initialize the population. Also, a new crossover and mutation operator are designed in this paper to guide the crossover and mutation process based on feature scoring information to improve the search direction of MOFS-RFGA in search space and enhance the convergence performance. In the experiments, MOFS-RFGA is compared with seven advanced multi-objective feature selection algorithms on 20 datasets, and the results show that MOFS-RFGA can fully utilize the advantages of filter and wrapper methods, beating the excellent performance of the comparison algorithms on a large number of datasets, and ensuring good classification performance while cutting a large number of features.},
  archive      = {J_ASOC},
  author       = {Yu Xue and Haokai Zhu and Ferrante Neri},
  doi          = {10.1016/j.asoc.2023.109987},
  journal      = {Applied Soft Computing},
  pages        = {109987},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature selection approach based on NSGA-II with ReliefF},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-agent virtual market model for generalization in
reinforcement learning based trading strategies. <em>ASOC</em>,
<em>134</em>, 109985. (<a
href="https://doi.org/10.1016/j.asoc.2023.109985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have successfully used reinforcement learning (RL) to train an intelligent agent that learns profitable trading strategies from financial market data. Most of RL trading studies have simplified the effect of the actions of the trading agent on the market state. The trading agent is trained to maximize long-term profit by optimizing fixed historical data. However, such approach frequently results in the trading performance during out-of-sample validation being considerably different from that during training. In this paper, we propose a multi-agent virtual market model (MVMM) comprised of multiple generative adversarial networks (GANs) which cooperate with each other to reproduce market price changes. In addition, the action of the trading agent can be superimposed on the current state as the input of the MVMM to generate an action-dependent next state. In this research, real historical data were replaced with the simulated market data generated by the MVMM. The experimental results indicated that the trading strategy of the trained RL agent achieved a 12\% higher profit and exhibited low risk of loss in the 2019 China Shanghai Shenzhen 300 stock index futures backtest.},
  archive      = {J_ASOC},
  author       = {Fei-Fan He and Chiao-Ting Chen and Szu-Hao Huang},
  doi          = {10.1016/j.asoc.2023.109985},
  journal      = {Applied Soft Computing},
  pages        = {109985},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-agent virtual market model for generalization in reinforcement learning based trading strategies},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 5G/5G+ network management employing AI-based continuous
deployment. <em>ASOC</em>, <em>134</em>, 109984. (<a
href="https://doi.org/10.1016/j.asoc.2023.109984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a novel concept of deployment process dedicated to continuous development, integration, testing, and upgrading the software-defined AI/ML-based functionalities in 5G/5G+ cellular networks , reliable and minimally invasive for network infrastructure, users, and service quality, automating DevOps and performance management towards the zero-touch networks. The idea is demonstrated by the continuous deployment framework, where a virtual cluster of representative cellular network resources coexists synchronously with the operator network serving the thousands-to-millions of end-users. Performance KPIs measured in this twinning subnetwork is the time series exhibiting the complex interrelations governing a temporal state of the whole system. One of the crucial problems is the detection when the changes of configuration of individual base station impacted its performance, which was realized by change points identification using an enhanced-PELT (e-PELT) algorithm. This yielded a sequence of complex data patterns (between successive change points) into sections of unique waveforms corresponding to stable network conditions. The e-PELT scheme includes a peak removal and an ACPO module for automatic penalty search. e-PELT algorithm, validated against experimental data in comparative studies, outperforms the AMOC, Binary Segmentation, and BOCPD schemes, increasing the F 1 F1 -score by more than 12\% and the A c c u r a c y Accuracy by 10\% over the PELT baseline. The results obtained in a CD cluster with fixed and automated penalty search show that e-PELT provides outstanding results, proving that the proposed approach can be successfully applied to analyze data sequences from different 5G network configurations. The concept of AI/ML-featured Continuous Deployment Framework dedicated to cellular networks can impact the development and integration of new telecommunication features, including possible extension for the role of standardized NWDAF module in customer networks, enhanced with the application of methodology outlined in the paper.},
  archive      = {J_ASOC},
  author       = {Michał Panek and Adam Pomykała and Ireneusz Jabłoński and Michał Woźniak},
  doi          = {10.1016/j.asoc.2023.109984},
  journal      = {Applied Soft Computing},
  pages        = {109984},
  shortjournal = {Appl. Soft. Comput.},
  title        = {5G/5G+ network management employing AI-based continuous deployment},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning accelerating complex parameters
optimizations based on quantum-inspired parallel multi-layer monte carlo
algorithm: Theory, application, implementation. <em>ASOC</em>,
<em>134</em>, 109982. (<a
href="https://doi.org/10.1016/j.asoc.2022.109982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In doubly-fed induction generator-based wind turbines (DFIG-WTs), the rotor-side controller (RSC) with optimized parameters improves wind energy utilization efficiency. With long optimization times and inadequate exploration and development capabilities, conventional intelligent optimization algorithms are hard to find the controller parameters quickly in the complexity and nonlinearity of DFIG-WTs. A quantum-inspired parallel multi-layer Monte Carlo algorithm accelerated by transfer learning (QPMMCOA-TL) is proposed to shorten the optimization time of parameters and obtain the controller parameters more satisfactorily simultaneously. The QPMMCOA-TL possesses strong optimization capabilities through an accelerated search method based on transfer learning , a diversified population coding way, a parallel multi-layer structure, way of searching in the narrowing feasible region. In the optimization process, the fitness function replaced by trained deep neural networks is transferred to the search process of the QPMMCOA-TL for shorting the optimization time. The QPMMCOA-TL is applied to test two benchmark functions and compared with seven metaheuristic algorithms for completing the validity verification. The optimization time of the QPMMCOA-TL when searching the parameters of the RSC is 1188 s, which is one-tenth or less than other algorithms. Furthermore, the reliability and stability of the optimized controller are comprehensively enhanced.},
  archive      = {J_ASOC},
  author       = {Kunlun Han and Tianwei Huang and Linfei Yin},
  doi          = {10.1016/j.asoc.2022.109982},
  journal      = {Applied Soft Computing},
  pages        = {109982},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transfer learning accelerating complex parameters optimizations based on quantum-inspired parallel multi-layer monte carlo algorithm: Theory, application, implementation},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Microgrid energy management using metaheuristic optimization
algorithms. <em>ASOC</em>, <em>134</em>, 109981. (<a
href="https://doi.org/10.1016/j.asoc.2022.109981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the economic dispatch problem of microgrids . Firstly, it presents the application of both traditional and newly introduced metaheuristic optimization algorithms to solve for the optimal power flow problem for the IEEE 30 bus system after which the best performing algorithm is chosen for cost-effective economic dispatch in a microgrid designed upon the microgrid facility present at Wroclaw University of Science and Technology. All algorithms investigated have been combined with the academic power analysis tool, MATPOWER. The idea behind the approach is to find a compromise between the solution search capabilities of the metaheuristics and the optimized performance of MATPOWER. The algorithms explored include 3 traditional algorithms which are the genetic algorithm , particle swarm optimization and mixed integer distributed ant colony optimization and 2 recently developed algorithms which are the political optimizer and the Lichtenberg algorithm. Hyperparameter tuning was carried out for all investigated algorithms. The results have shown that the ant-colony based algorithm is the most suitable of all the choices in terms of having the best convergence time of 19.17 s, a final solution value of 801.57 ($/h) and reliability in terms of reproducing the best solution for the test system. It is then used for economic dispatch which is guided by an objective function that minimizes the levelized cost of energy in the microgrid.},
  archive      = {J_ASOC},
  author       = {Vishnu Suresh and Przemyslaw Janik and Michal Jasinski and Josep M. Guerrero and Zbigniew Leonowicz},
  doi          = {10.1016/j.asoc.2022.109981},
  journal      = {Applied Soft Computing},
  pages        = {109981},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Microgrid energy management using metaheuristic optimization algorithms},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A discrete teaching–learning based optimization algorithm
with local search for rescue task allocation and scheduling.
<em>ASOC</em>, <em>134</em>, 109980. (<a
href="https://doi.org/10.1016/j.asoc.2022.109980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The allocation and scheduling of the emergency rescue forces is a fundamental task in emergency management. This paper aims to address the allocation and scheduling problem to minimize the average completion time of all rescue teams by using a discrete teaching–learning based optimization algorithm with local search (DTOLS). First, an improved k k -means clustering algorithm with constraints is proposed to assign tasks to rescue teams based on the location of rescue tasks. Second, a hybrid discrete optimization algorithm based on a teaching–learning mechanism is designed to generate the task scheduling sequence for each rescue team as an initial solution. Next, an efficient two-phase local search strategy is presented to improve the current solution. For three neighborhood task moves based on problem characteristics, which contains insert task within a team, swap tasks within a team, insert task between teams, the speed-up techniques are introduced to reduce the computational complexity of calculating completion time of a rescue team. Finally, the parameters of DTOLS are calibrated by Taguchi method to determine appropriate values. DTOLS is compared with the state-of-the-art algorithms, and the experimental results demonstrate the effectiveness of DTOLS in solving a set of test instances.},
  archive      = {J_ASOC},
  author       = {Ying Xu and Xiaobo Li and Qian Li},
  doi          = {10.1016/j.asoc.2022.109980},
  journal      = {Applied Soft Computing},
  pages        = {109980},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete teaching–learning based optimization algorithm with local search for rescue task allocation and scheduling},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EXplainable ensemble strategy using distinct and restrict
learning biases: A case study on the brazilian forest. <em>ASOC</em>,
<em>134</em>, 109976. (<a
href="https://doi.org/10.1016/j.asoc.2022.109976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised learning algorithms consider different learning biases from the universe of all admissible functions to induce classifiers. When using ensembles, one takes advantage of different biases typically built from the same algorithm to combine complementary classifiers into a single model, such as Random Forest , that builds up several trees from different attributes and examples. This paper innovates ensemble strategies by explaining and exploring distinct, restrict, and complementary biases from different algorithms. Multi-bias classifiers are combined using Fuzzy rules to execute symbolic reasoning and explain how each learning bias contributes to the final classification results . The contributions of our work are twofold: first, the proposed approach looks for the most suitable learner by individually analyzing the attributes of each new instance, and second, the process used to perform such a search is based on inferences run on fuzzy rules, that uses IF–THEN structures, which are interpretable, thus allowing to explain the process used to select the best learner. Finally, it is worth emphasizing that our approach was applied to the Brazilian biodiversity dataset to corroborate that, even working on hundreds of examples, results are promising, thus stimulating studies on biodiversity and the design of sustainable economic solutions.},
  archive      = {J_ASOC},
  author       = {Tatiane Nogueira Rios and Ricardo Rios and Rodrigo Mello},
  doi          = {10.1016/j.asoc.2022.109976},
  journal      = {Applied Soft Computing},
  pages        = {109976},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EXplainable ensemble strategy using distinct and restrict learning biases: A case study on the brazilian forest},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of intelligent computing solver with morlet wavelet
neural networks for nonlinear predator–prey model. <em>ASOC</em>,
<em>134</em>, 109975. (<a
href="https://doi.org/10.1016/j.asoc.2022.109975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of integrated intelligent computing solver with Morlet wavelet neural networks (MW-NNs) is presented for solving the mathematical predator–prey model by exploiting the strength of MW-NNs modeling, optimization ability of global search with genetic algorithms (GAs) and rapid local search eminence of sequential quadratic programming (SQP), i.e., MW-NNs-GA-SQP. The proposed MW-NNs-GA-SQP scheme is used to analyze the predator–prey dynamics for six different variables coefficient values . The validation, correctness and reliability of the presented MW-NNs-GA-SQP technique is attained through the consistent matched outcomes with the reference Adams numerical results. Moreover, statistics investigations have been accomplished to verify the precision and accuracy of the outcomes with proposed MW-NNs-GA-SQP solver via the performances of Theil’s inequality coefficient, Nash Sutcliffe efficiency and mean absolute error .},
  archive      = {J_ASOC},
  author       = {Muhammad Umar and Zulqurnain Sabir and Muhammad Asif Zahoor Raja and Fazli Amin and Tareq Saeed and Yolanda Guerrero Sanchez},
  doi          = {10.1016/j.asoc.2022.109975},
  journal      = {Applied Soft Computing},
  pages        = {109975},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of intelligent computing solver with morlet wavelet neural networks for nonlinear predator–prey model},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge detection and graph neural networks to classify
mammograms: A case study with a dataset from vietnamese patients.
<em>ASOC</em>, <em>134</em>, 109974. (<a
href="https://doi.org/10.1016/j.asoc.2022.109974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammograms are breast X-ray images and they are used by doctors, among other purposes, as an effective means of detecting breast cancer. Screening mammography is crucial since it allows doctors to understand better the situation and have suitable intervention. The classification of medical modalities is a prerequisite for development of computer-aided diagnosis tools in healthcare, and various techniques have been proposed to automatically classify from mammography images. Though there have been several tools developed, they have been mostly validated with data collected from Western women. Based on our initial investigations, breast anatomy in Vietnamese women differs from that of Western women, due to denser breast tissue. In this paper, we propose MammoGNN – a practical solution to the classification of mammograms using the synergy between image processing techniques and graph neural networks . First, a well-founded edge detection algorithm was applied to provide input for the recommendation engine. Afterward, we empirically experimented to select suitable graph neural networks to manage the training and prediction. A mammogram dataset was curated from 2, 351 Vietnamese women to validate the conceived tool. By several testing instances, MammoGNN obtains a maximum accuracy of 100\%, precision and recall of 1.0 on independent and shuffle test sets for both classification of BI-RADS scores and breast density types. The experimental results also demonstrate that our proposed approach obtains an optimal prediction performance on the considered datasets, outperforming different baselines. We anticipate that the proposed approach can be deployed as a non-invasive pre-screening tool to assist doctors in performing their diagnosis activities.},
  archive      = {J_ASOC},
  author       = {Linh T. Duong and Cong Q. Chu and Phuong T. Nguyen and Son T. Nguyen and Binh Q. Tran},
  doi          = {10.1016/j.asoc.2022.109974},
  journal      = {Applied Soft Computing},
  pages        = {109974},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Edge detection and graph neural networks to classify mammograms: A case study with a dataset from vietnamese patients},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ECML driven geographical location of utility poles in smart
grid: Data analysis and high-definition recognition. <em>ASOC</em>,
<em>134</em>, 109973. (<a
href="https://doi.org/10.1016/j.asoc.2022.109973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of smart grid is inseparable from the appropriate deployment of utility poles. China has many poles; if they are fully utilized, can they bring more power to the smart grid? Fortunately, thanks to advances in technologies such as artificial intelligence and the Internet of Things , can we use Google Earth to recognize utility poles and optimize the deployment of power facilities? Fundamentally, Google Earth generates static images. Faced with many static images, we need further image processing for high-definition recognition of utility poles. In addition to the current chaotic distribution of poles, we also need to optimize the deployment of power facilities. However, due to the resolution of Google Earth and the aerial photography angle, many backlight phenomena are not conducive to the recognition of utility poles. Therefore, this paper proposes a backlight image enhancement algorithm based on convolutional neural networks (CNN) and constructs a novel network architecture that integrates decomposition, restoration, and adjustment to recognize poles in high-definition under backlight. Furthermore, to solve the problems of the overflow of CNN parameters and unclear training effect, particle swarm optimization (PSO), the evolutionary computing-based machine learning (ECML) is used to search CNN parameters automatically and seek the optimal solution to achieve the optimization of the overall model. The experiments prove that the CNN-based image enhancement algorithm effectively recognizes the utility poles under different illumination and backlight. At the same time, the experimental results show that the PSO-based optimization method can optimize CNN parameters obviously, and the classification accuracy is increased.},
  archive      = {J_ASOC},
  author       = {Tao Chen and Muhammad Murtadha Othman and Xianju Wang and Yong Zhu and Zelei Zhu and Jiakai Xiao},
  doi          = {10.1016/j.asoc.2022.109973},
  journal      = {Applied Soft Computing},
  pages        = {109973},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ECML driven geographical location of utility poles in smart grid: Data analysis and high-definition recognition},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Financial trading decisions based on deep fuzzy
self-organizing map. <em>ASOC</em>, <em>134</em>, 109972. (<a
href="https://doi.org/10.1016/j.asoc.2022.109972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The volatility features of financial data would considerably change in different periods, that is one of the main factors affecting the applications of machine learning in quantitative trading. Therefore, to effectively distinguish fluctuation patterns of financial markets can provide meaningful information for the trading decision. In this article, a novel intelligent trading system based on deep fuzzy self-organizing map (DFSOM) companied with GRU networks is proposed, where DFSOM is utilized for the clustering of financial data to acquire multiple fluctuation patterns in an unsupervised way. Firstly, in order to capture the trend features and evade the effect of high noises in financial data, the images of extended candlestick charts instead of raw data are processed and the obtained features are applied for the following unsupervised learning , where candlestick charts are produced with both price and volume information. Secondly, by using the candlestick features, a two-layer deep fuzzy self-organizing map is constructed to carry out the clustering, where two-layer models carry out the clustering in multiple time scales to improve the processing of time-dependent information. Thirdly, GRU networks are used to implement the prediction task, based on which an intelligent trading model is constructed. The feasibility and effectiveness of the proposed method are verified by using various real financial datasets.},
  archive      = {J_ASOC},
  author       = {Dehao Pei and Chao Luo and Xiaomei Liu},
  doi          = {10.1016/j.asoc.2022.109972},
  journal      = {Applied Soft Computing},
  pages        = {109972},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Financial trading decisions based on deep fuzzy self-organizing map},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recognition of human emotion transition from video sequence
using triangulation induced various centre pairs distance signatures.
<em>ASOC</em>, <em>134</em>, 109971. (<a
href="https://doi.org/10.1016/j.asoc.2022.109971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotion recognition manifests the concept of contextual understanding of human communication in nonverbal form. This fascinates us to monitor the dynamic information of an emotion. In this article, we present an automatic recognition system for observing gradual changes in human emotion available in the form of a video sequence. In our proposed system, we introduce a triangulation mechanism for geometric feature generation from salient landmarks on the face image. Here, well known active appearance model (AAM) is applied to consecutive image frames in a sequence to track all locations of landmark points. In this work, we explore four centre points (Incentre, Circumcentre, Barycentre, and Orthocentre) from all the triangles formed by salient facial points selected in a triplet. Six different geometric distance signatures viz. Incentre–Circumcentre, Incentre–Barycentre, Incentre–Orthocentre, Circumcentre–Barycentre, Circumcentre–Orthocentre, and Barycentre–Orthocentre are extracted accordingly by taking all possible pairs of those four centres into account to signify several ways of the geometric representation of an emotional image sequence. The discriminative merit of each distance signature is analysed independently by employing them separately as input feature sets into a multilayer perceptron (MLP) classifier for recognition of six basic emotions(anger, disgust, fear, happiness, sadness, and surprise). We verify the performance of our proposed system through the experimentation on several publically available benchmark video datasets such as Extended Cohn–Kanade (CK＋), M&amp;M Initiative (MMI), and Multimedia Understanding Group (MUG). Experimental results indicate impressive accuracy of 98.47\% for CK＋, 92.57\% for MMI, and 98.62\% for the MUG dataset on Incentre–Circumcentre pair distance signature. Moreover, to justify the effectiveness of our proposed method we compare our experimental results with other existing states of the art available in the literature.},
  archive      = {J_ASOC},
  author       = {Md Nasir and Paramartha Dutta and Avishek Nandi},
  doi          = {10.1016/j.asoc.2022.109971},
  journal      = {Applied Soft Computing},
  pages        = {109971},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recognition of human emotion transition from video sequence using triangulation induced various centre pairs distance signatures},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Broad learning system with takagi–sugeno fuzzy subsystem for
tobacco origin identification based on near infrared spectroscopy.
<em>ASOC</em>, <em>134</em>, 109970. (<a
href="https://doi.org/10.1016/j.asoc.2022.109970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tobacco origin identification is significantly important in tobacco industry. Modeling analysis for sensor data with near infrared spectroscopy has become a popular method for rapid detection of internal features. However, for sensor data analysis using traditional artificial neural network or deep network models, the training process is extremely time-consuming. In this paper, a novel broad learning system with Takagi–Sugeno (TS) fuzzy subsystem is proposed for rapid identification of tobacco origin. Incremental learning is employed in the proposed method, which obtains the weight matrix of the network after a very small amount of computation, resulting in much shorter training time for the model, with only about 3 s for the extra step training. The experimental results show that the TS fuzzy subsystem can extract features from the near infrared data and effectively improve the recognition performance. The proposed method can achieve the highest prediction accuracy (95.59\%) in comparison to the traditional classification algorithms , artificial neural network , and deep convolutional neural network , and has a great advantage in the training time with only about 128 s.},
  archive      = {J_ASOC},
  author       = {Di Wang and Simon X. Yang},
  doi          = {10.1016/j.asoc.2022.109970},
  journal      = {Applied Soft Computing},
  pages        = {109970},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Broad learning system with Takagi–Sugeno fuzzy subsystem for tobacco origin identification based on near infrared spectroscopy},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyper-parameter initialization of classification algorithms
using dynamic time warping: A perspective on PCA meta-features.
<em>ASOC</em>, <em>134</em>, 109969. (<a
href="https://doi.org/10.1016/j.asoc.2022.109969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning, a concept from the area of automated machine learning, aims at providing decision support for data scientists by recommending a suitable setting (a machine learning algorithm or its hyper-parameters) to be used for a given dataset. Such a recommendation is based the assumption that an optimal setting for a certain dataset would also be suitable for other, similar datasets. Similarity of datasets is computed from their characteristics, named meta-features, several types of which have been developed thus far. This paper introduces a novel perspective on PCA meta-features which, despite their good descriptive characteristics and easy computation, are rarely used in meta-learning. A novel meta-learning approach utilizing DTW, a well-known similarity measure for time-series, is proposed for computing dataset similarities based on the series of cumulative variances explained by their respective principal components. The results from a large-scale experiment, comparing the proposed approach to multiple baselines on 50 real-world datasets, show the potential of combining PCA and DTW in meta-learning and encourage further investigation in this direction.},
  archive      = {J_ASOC},
  author       = {Tomáš Horváth and Rafael G. Mantovani and André C.P.L.F. de Carvalho},
  doi          = {10.1016/j.asoc.2022.109969},
  journal      = {Applied Soft Computing},
  pages        = {109969},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyper-parameter initialization of classification algorithms using dynamic time warping: A perspective on PCA meta-features},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A policy optimization algorithm based on sample adaptive
reuse and dual-clipping for robotic action control. <em>ASOC</em>,
<em>134</em>, 109967. (<a
href="https://doi.org/10.1016/j.asoc.2022.109967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When applying deep reinforcement learning in the real physical environment for decision-making, how to improve the sample efficiency while ensuring training stability is an urgent problem that needs to be solved. In order to solve this problem, some of on-policy algorithms are proposed and have achieved state-of-the-art performance. However, these on-policy algorithms, such as proximal policy optimization (PPO) algorithm, have the drawback of extremely low sample efficiency. In this study, we proposed a novel policy optimization method named improved proximal policy optimization algorithm based on sample adaptive reuse and dual-clipping (SARD-PPO) for robotic action control, which combines the advantage of the on-policy methods in training stability with the advantage of the off-policy methods in sample efficiency. First, we analyzed the clipping mechanism of the PPO algorithm, devised a more constrained clipping mechanism based on the analysis of the relationship between the clipping mechanism and the objective constraints, and developed a policy updating method that reuses the old samples of the prior policy in a more principle-based way. Second, we ensured the training stability of the algorithm through element-level dual-clipping, as well as adaptive adjustment and reuse of the entire policy trajectory. The experimental results on six tasks in the MuJoCo benchmark indicate that SARD-PPO can significantly improve policy performance while balancing policy training stability and sample efficiency, outperforming the baseline PPO algorithm and other SOTA policy gradient methods using on- and off-policy samples in terms of overall performance.},
  archive      = {J_ASOC},
  author       = {Li-yang Zhao and Tian-qing Chang and Jie Zhang and Lei Zhang and Kai-xuan Chu and Li-bin Guo and De-peng Kong},
  doi          = {10.1016/j.asoc.2022.109967},
  journal      = {Applied Soft Computing},
  pages        = {109967},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A policy optimization algorithm based on sample adaptive reuse and dual-clipping for robotic action control},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing deep-network based novelty assessment model in
design education. <em>ASOC</em>, <em>134</em>, 109966. (<a
href="https://doi.org/10.1016/j.asoc.2022.109966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novelty is a significant factor in evaluating creative aptitude of mass examination in Design education. Novelty is a measure of uniqueness which is computed based on relative comparison among solutions. Presently, novelty assessment of solutions illustrating creative aptitude in mass examination is conducted by domain-specific experts. During the process of evaluation, examiners drill their thought processes and assesses based on their frame of reference. Moreover due to the ever-increasing number of students in these examinations where students compete for admission to Design schools, examiners are confronted with multiple challenges viz., evaluation in stipulated time, evaluation to be conducted in large scale, etc. These difficulties might frustrate examiners and might lead to errors in evaluation. The investigation in this paper is exactly geared towards this issue and we explored whether technology can support examiners in situations like this. Features are extracted for evaluating novelty by human-centred design approach. A model is proposed to evaluate novelty in academic settings specifically in mass examinations. This model is validated by utilizing it in different case studies based on the type of Design solutions in mass examinations. This model is implemented using Deep Learning (DL)-based architectures. Findings emphasize that there is negligible difference in outcome of these architectures and human expert-based evaluation, which confirms the capacity of the devised model. This study would support pedagogues in the evaluation process that are conducted on a large scale. It would also help reducing logistics, time, and man-power in evaluation process. Consistency in subjective assessment would ensure selection of right candidate, thereby increases trust in the assessment system.},
  archive      = {J_ASOC},
  author       = {Nandita Bhanja Chaudhuri ( PhD, Assistant Professor ) and Debayan Dhar ( PhD, Assistant Professor )},
  doi          = {10.1016/j.asoc.2022.109966},
  journal      = {Applied Soft Computing},
  pages        = {109966},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing deep-network based novelty assessment model in design education},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex matrix and multi-feature collaborative learning for
polarimetric SAR image classification. <em>ASOC</em>, <em>134</em>,
109965. (<a href="https://doi.org/10.1016/j.asoc.2022.109965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest-regularized subspace (NRS) algorithm is an effective tool to obtain both accuracy and speed for PolSAR image classification. However, existing NRS-based methods can only learn the polarimetric feature vector well, but ignore the PolSAR complex matrix structure and channel correlation . So, how to collaboratively learn the original complex matrix and multiple features to improve the classification accuracy is a key problem. Besides, speckle noises are also the main factor of causing misclassification . To address these limitations, two novel methods are proposed. Firstly, a superpixel-based Riemannian NRS(SRNRS) method is proposed, which can not only learn complex matrix structure by Riemannian metric, but also reduce the speckle noises and computing time by superpixels. Then, a superpixel-based collaborative learning method(CM_SJNRS) is proposed, which integrates the complex matrix and multiple features into the NRS classification framework for the first time. Coupled dictionaries and different metrics are designed for two kinds of feature spaces respectively, and then a collaborative learning model is developed to fuse them. Experimental results demonstrate the proposed SRNRS method can reduce both the speckle and computing time, and the proposed CM_SJNRS method can improve classification performance by fusing two types of features.},
  archive      = {J_ASOC},
  author       = {Junfei Shi and Wei Wang and Haiyan Jin and Tiansheng He},
  doi          = {10.1016/j.asoc.2022.109965},
  journal      = {Applied Soft Computing},
  pages        = {109965},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Complex matrix and multi-feature collaborative learning for polarimetric SAR image classification},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two evolutionary approaches with objective-specific
variation operators for vehicle routing problem with time windows and
quality of service objectives. <em>ASOC</em>, <em>134</em>, 109964. (<a
href="https://doi.org/10.1016/j.asoc.2022.109964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a variant of the vehicle routing problem with time windows where the goal is to maximize the quality of service delivered to the customer. In the literature, this problem contains three objectives targeted at improving the quality of service. In this paper, we have proposed two evolutionary approaches, viz., a steady-state grouping genetic algorithm and a discrete differential evolution algorithm , to address this problem. The crossover and mutation operators are designed by considering the characteristics of each objective. The proposed approaches are incorporated with various heuristics that provide a set of better initial solutions in comparison to purely random initial solutions. We have also proposed two bounds for each objective. The approaches presented in this paper are tested on the Solomon instances which are considered as the standard benchmark instances for the vehicle routing problem with time windows in the literature. The proposed approaches are compared with the state-of-the-art approach available in the literature. The computational results demonstrate that our approaches are better in terms of solution quality and execution time than the state-of-the-art approach.},
  archive      = {J_ASOC},
  author       = {Gaurav Srivastava and Alok Singh},
  doi          = {10.1016/j.asoc.2022.109964},
  journal      = {Applied Soft Computing},
  pages        = {109964},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two evolutionary approaches with objective-specific variation operators for vehicle routing problem with time windows and quality of service objectives},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrusion detection model using gene expression programming
to optimize parameters of convolutional neural network for energy
internet. <em>ASOC</em>, <em>134</em>, 109960. (<a
href="https://doi.org/10.1016/j.asoc.2022.109960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The open, interconnected, and shared operational characteristics of the energy Internet introduce more sophisticated cybersecurity attacks. How to accurately detect these cyber attacks is crucial for energy Internet security protection. Existing machine learning-based intrusion detection algorithms cannot cope with the continuous increase of network traffic and features in the energy Internet. And convolutional neural networks (CNN) can be a good solution for the descending and optimal selection of high-dimensional intrusion features. Unfortunately, traditional convolutional neural networks have complex structures with many parameters and are prone to fall into local optimality . To fill the gap of CNN, in this paper, we use a gene expression programming (GEP) to optimize the parameters of CNN and propose an intrusion detection algorithm based on GEP-CNN (GCNN-IDS). Our key idea is to avoid the convolutional neural network from falling into local optimum by designing a new code on GEP and fitness function to optimize the parameters of the CNN using the global search capability of GEP. The experimental results on two benchmark datasets and a real dataset substantiate that the detection accuracy of the optimized CNN-based intrusion detection algorithm (ICNN-IDS) reaches up to 0.9143 under different parameter combinations; meanwhile, compared with other algorithms, the detection accuracy, precision, recall, F 1 F1 and false detection rate of the intrusion detection model proposed in this paper reach 0.9897, 0.99, 0.98, 0.97 and 0.0126, respectively.},
  archive      = {J_ASOC},
  author       = {Deng Song and Xinya Yuan and Qianliang Li and Jie Zhang and Mengfei Sun and Xiong Fu and Lechan Yang},
  doi          = {10.1016/j.asoc.2022.109960},
  journal      = {Applied Soft Computing},
  pages        = {109960},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intrusion detection model using gene expression programming to optimize parameters of convolutional neural network for energy internet},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multivariable fuzzy rule-based models and their granular
generalization: A visual interpretable framework. <em>ASOC</em>,
<em>134</em>, 109958. (<a
href="https://doi.org/10.1016/j.asoc.2022.109958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy rule-based models have been widely used due to their interpretability and effectiveness. However, they still encounter challenges when dealing with multivariable and large-scale data. In this study, we first propose a novel approach to establish a selective sampling and mapping data reduction method. The method focuses on reducing data variables while decreasing the number of samples, and an appropriate scaling size can be chosen for different situations. Then, a multivariable data-driven fuzzy rule-based model is developed based on the processed data. Moreover, the data projection approach using the distance metric helps to preserve the structural characteristics of the original data. The results are visually presented to facilitate an interpretable description of the subsequent rule-based modeling. Furthermore, due to the inevitable inaccuracy in the projection process of numeric modeling, we introduce the allocation of information granularity to extend the model to a granular form at a more abstract level. Experimental studies on both synthetic and publicly available datasets demonstrate that the proposed method has superior effectiveness and efficiency compared to the existing state-of-the-art regression algorithms.},
  archive      = {J_ASOC},
  author       = {Yan Li and Xingchen Hu and Witold Pedrycz and Fangjie Yang and Zhong Liu},
  doi          = {10.1016/j.asoc.2022.109958},
  journal      = {Applied Soft Computing},
  pages        = {109958},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multivariable fuzzy rule-based models and their granular generalization: A visual interpretable framework},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum-inspired algorithm for direct multi-class
classification. <em>ASOC</em>, <em>134</em>, 109956. (<a
href="https://doi.org/10.1016/j.asoc.2022.109956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few decades, quantum machine learning has emerged as a groundbreaking discipline. Harnessing the peculiarities of quantum computation for machine learning tasks offers promising advantages. Quantum-inspired machine learning has revealed how relevant benefits for machine learning problems can be obtained using the quantum information theory even without employing quantum computers . In the recent past, experiments have demonstrated how to design an algorithm for binary classification inspired by the method of quantum state discrimination, which exhibits high performance with respect to several standard classifiers. However, a generalization of this quantum-inspired binary classifier to a multi-class scenario remains nontrivial. Typically, a simple solution in machine learning decomposes multi-class classification into a combinatorial number of binary classifications, with a concomitant increase in computational resources. In this study, we introduce a quantum-inspired classifier that avoids this problem. Inspired by quantum state discrimination, our classifier performs multi-class classification directly without using binary classifiers. We first compared the performance of the quantum-inspired multi-class classifier with eleven standard classifiers. The comparison revealed an excellent performance of the quantum-inspired classifier. Comparing these results with those obtained using the decomposition in binary classifiers shows that our method improves the accuracy and reduces the time complexity. Therefore, the quantum-inspired machine learning algorithm proposed in this work is an effective and efficient framework for multi-class classification. Finally, although these advantages can be attained without employing any quantum component in the hardware, we discuss how it is possible to implement the model in quantum hardware.},
  archive      = {J_ASOC},
  author       = {Roberto Giuntini and Federico Holik and Daniel K. Park and Hector Freytes and Carsten Blank and Giuseppe Sergioli},
  doi          = {10.1016/j.asoc.2022.109956},
  journal      = {Applied Soft Computing},
  pages        = {109956},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum-inspired algorithm for direct multi-class classification},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust fuzzy stochastic multi-objective model for stone
paper closed-loop supply chain design considering the flexibility of
soft constraints based on me measure. <em>ASOC</em>, <em>134</em>,
109944. (<a href="https://doi.org/10.1016/j.asoc.2022.109944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Closed-Loop Supply Chain Network Design (CLSCND) encourages attention to the economic and environmental issues concerning returned products. CLSCND is a significant decision issue and it is accompanied by uncertainty. Hybrid uncertainties and the flexibility of soft constraints are some of the key issues in CLSCND. There is a research gap surrounding the utilization of the uncertainties of randomness, epistemic, and soft constraints simultaneously with a robust procedure. This study focuses on the Me measure and proposes a novel flexible, probabilistic, and stochastic programming based on robust optimization . In this approach, a convex mixture of the pessimistic–optimistic views based on the Me measure is implemented. The least level of satisfaction with soft constraints is determined Optimally that the need for repetitive reviews by Decision-Maker (DM) is obviated in the proposed model. Additionally, the model controls violations of soft constraints as well as stochastic deviations, probabilistic deviations, unfulfilled demand, and capacity. A case study is conducted on the development of a stone paper considering economic, environmental, and responsive objectives by minimizing total cost and carbon emissions as well as transportation and processing time. In addition, to estimate the weight of the objectives and handle the multi-objective model, the Best–Worst method (BWM) and the interactive fuzzy programming approach are employed, respectively. Robustness analysis and sensitivity analysis for the case study are then performed and the results are simulated and evaluated using the realization model. The findings revealed that the suggested model performed well.},
  archive      = {J_ASOC},
  author       = {Seyyed Jalaladdin Hosseini Dehshiri and Maghsoud Amiri and Laya Olfat and Mir Saman Pishvaee},
  doi          = {10.1016/j.asoc.2022.109944},
  journal      = {Applied Soft Computing},
  pages        = {109944},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust fuzzy stochastic multi-objective model for stone paper closed-loop supply chain design considering the flexibility of soft constraints based on me measure},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized face detector-based intelligent face mask
detection model in IoT using deep learning approach. <em>ASOC</em>,
<em>134</em>, 109933. (<a
href="https://doi.org/10.1016/j.asoc.2022.109933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of the “Internet of Medical Things (IoMT)” allows for the collection and processing of data in healthcare systems. At the same time, it is challenging to study the requirements of public health prevention. Here, mask-wearing is considered an efficient preventive measure for avoiding virus transfer. Hence, it is necessary to implement an automated mask identification model to prevent public epidemics. The main scope of the proposed method is to design a face mask detection model with IoT using a “Single Shot Multi-box Detector (SSD)” and a hybrid deep learning method. The novelty of the proposed model is that the enhancement made in the face detection and face classification with the developed ASMFO by optimizing the parameters like the threshold in SSD, steps per execution in ResNet , and learning rate in MobileNet, which makes it more efficient and to perform better the conventional models. Here, the parameter optimization is carried out using a hybrid optimization algorithm named Adaptive Sailfish Moth Flame Optimization (ASMFO). Then, the detected face images are given to the hybrid approach named Hybrid ResMobileNet (HResMobileNet)-based classification, where the parameters are tuned using the same ASMFO algorithm for achieving accurate mask detection results. However, the suggested mask identification model with IoT based on three standard datasets is compared with the conventional meta-heuristic algorithms and existing classifiers with various measures. Thus, the experimental analysis is conducted to analyze the effectiveness of the proposed framework over different meta-heuristic algorithms and existing classifiers. The implemented ASMFO-HResMobileNet provides 18.57\%, 15.67\%, 17.56\%, 16.24\%, and 19.2\% elevated accuracy than SVM, CNN , VGG16-LSTM, ResNet 50, MobileNetv2, and ResNet 50-MobileNetv2.},
  archive      = {J_ASOC},
  author       = {Raghda Awad Shaban Naseri ( Ph.D. Research ) and Ayça Kurnaz ( Assistant professor ) and Hameed Mutlag Farhan ( Ph.D. Research )},
  doi          = {10.1016/j.asoc.2022.109933},
  journal      = {Applied Soft Computing},
  pages        = {109933},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized face detector-based intelligent face mask detection model in IoT using deep learning approach},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A metaheuristic-based efficient strategy for multi-unit
production planning with unique process constraints. <em>ASOC</em>,
<em>134</em>, 109871. (<a
href="https://doi.org/10.1016/j.asoc.2022.109871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Production planning provides optimal strategic planning to attain economic benefits in the petrochemical industries and helps to maintain competitiveness in the global market. This work proposes an efficient modeling strategy for combinatorial production planning involving unique process constraints in the development stage of the petrochemical industry. The decisions involved are the choice of products to be produced, the product quantity, the suitable process for the selected product, and the number of processing units to be implemented at the operating capacity. The objective is to determine an optimal production plan by maximizing the profit while satisfying all the resource and bound constraints . This work analyzed the strategy in the literature to identify the factors affecting metaheuristic techniques in determining better solutions for production planning involving unique process constraints. The compact and precise strategy proposed in this work reduces the number of decision variables and constraints up to 90\% without compromising the rigor of the model. The efficacy of the proposed strategy is demonstrated on four case studies and is solved using eight different metaheuristic techniques, namely (i) Multi-Population based Ensemble of Mutation Strategies Differential Evolution, (ii) Sanitized Teaching Learning Based Optimization, (iii) Artificial Bee Colony , (iv) Dynamic Neighborhood Learning Particle Swarm Optimizer, (v) Sine Cosine Algorithm, (vi) Symbiotic Organisms Search, (vii) Single Phase Multi-Group Teaching Learning Based Optimization with Lévy Flight, and (viii) Harris Hawk Optimization. The results determined by these techniques in solving the case studies using the proposed strategy are better than those provided in the literature. It indicates the suitability of the proposed strategy with metaheuristic techniques and emphasizes the importance of efficient modeling of a problem. The proposed strategy provided up to 5\% improvement in profit.},
  archive      = {J_ASOC},
  author       = {Remya Kommadath and Debasis Maharana and Prakash Kotecha},
  doi          = {10.1016/j.asoc.2022.109871},
  journal      = {Applied Soft Computing},
  pages        = {109871},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A metaheuristic-based efficient strategy for multi-unit production planning with unique process constraints},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel optimization approach for stock price forecasting
using multi-layered sequential LSTM. <em>ASOC</em>, <em>134</em>,
109830. (<a href="https://doi.org/10.1016/j.asoc.2022.109830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock markets can often be one of the most volatile places to invest. Statistical analysis of past stock performance and external factors play a major role in the decision to buy or sell stocks. These factors are all used to maximize profits. Stock price index forecasting has been a subject of great research for many years, and several machine learning and deep learning algorithms have been proposed to simplify this complex task, but little success has been found so far. In order to forecast stocks accurately, it is crucial to understand the context-specific dependence of stock prices on their past values. The use of Long Short Term Memory (LSTM), which is capable of understanding long-term data dependencies , can help overcome this obstacle. In this context, this paper proposes a novel optimization approach for stock price prediction that is based on a Multi-Layer Sequential Long Short Term Memory (MLS LSTM) model which makes use of the adam optimizer. Additionally, the MLS LSTM algorithm uses normalized time series data divided into time steps to determine the relationship between past values and future values in order to make accurate predictions. Furthermore, it eliminates the vanishing gradient problem associated with simple recurrent neural networks . The stock price index is forecasted by taking into account past performance information along with past trends and patterns. The results illustrate that a 95.9\% prediction accuracy is achieved on the training data set and a 98.1\% accuracy on the testing data set with the MLS LSTM algorithm, which dramatically exceeds the performance of other machine learning and deep learning algorithms . The mean absolute percentage error was observed to be 1.79\% on the training set and 2.18\% on the testing set, respectively. Moreover, the proposed model is able to estimate the stock price with a normalized root mean squared error of 0.019, thus giving an accurate forecast and making it a feasible real-world solution.},
  archive      = {J_ASOC},
  author       = {Abdul Quadir Md and Sanjit Kapoor and Chris Junni A.V. and Arun Kumar Sivaraman and Kong Fah Tee and Sabireen H. and Janakiraman N.},
  doi          = {10.1016/j.asoc.2022.109830},
  journal      = {Applied Soft Computing},
  pages        = {109830},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Novel optimization approach for stock price forecasting using multi-layered sequential LSTM},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fuzzy AHP-based approach for prioritization of cost
overhead factors in agile software development. <em>ASOC</em>,
<em>133</em>, 109977. (<a
href="https://doi.org/10.1016/j.asoc.2022.109977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agile software development (ASD) is a widely-used practical approach for effective project management that intends to satisfy the client’s needs through continuous testing and frequent delivery. Accurate cost estimation plays a significant role in the success of ASD since the project’s success and failure greatly depend on the hidden cost factors. However, to the best of our knowledge, the current state-of-the-art lacks in considering, validating and prioritizing the critical cost factors helpful in improving the cost estimation accuracy during the ASD process. Thus, there is a need to provide a recent view of the critical factors impacting the cost of agile-based projects. This article provides an up-to-date view of the targeted research context. To accomplish this, we propose a quantitative framework that effectively prioritizes the identified cost factors grounded on the 4Ps (i.e., people, project, process, and product) standard. For identification and validation of the cost overhead factors, we perform a systematic literature review and empirical study in the ASD context. Likewise, the current study classifies and prioritizes the validated factors using a multi-criterion decision making Fuzzy-Analytic Hierarchy Process technique, which effectively rectifies the subjectivity and handles the uncertainty among the identified factors. The implementation results provide a list of prioritized cost overhead factors that would assist agile practitioners during the cost estimation process in the ASD context.},
  archive      = {J_ASOC},
  author       = {Syed Abusaeed and Saif Ur Rehman Khan and Atif Mashkoor},
  doi          = {10.1016/j.asoc.2022.109977},
  journal      = {Applied Soft Computing},
  pages        = {109977},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy AHP-based approach for prioritization of cost overhead factors in agile software development},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive bezier curve-based membership function formulation
scheme for interpretable edge detection. <em>ASOC</em>, <em>133</em>,
109968. (<a href="https://doi.org/10.1016/j.asoc.2022.109968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quest for the design of interpretable models is expanding as there is a far-reaching reliance on Black box models , owing to their lesser interpretability in predictions. This paper presents an interpretable fuzzy logic model for edge detection based on a novel generalized domain independent parametric Adaptive Bezier Curve-based Membership Function (ABCMF) constructed for image fuzzification . To bring out a robust fuzzy framework using the developed novel membership function, the fuzzified image is convolved with the oversimplified fuzzy-based edge detector to determine the direction of intensity changes. Finally, using the λ − λ− cut technique, the edge detected image is transformed back to crisp form with adaptively varying λ λ . The efficacy of the proposal is exhaustively tested on BSDS500, BIPED, and MDBD, and the attained simulation results are compared with traditional and current methods. From the evaluated metrics, it can be inferred that the proposed method offers a consistent accuracy greater than 91\% in comparison with its counterparts. Also, when analyzing in terms of interpretability , fairness, F 1 − F1− score, and computational efficiency, the approach offered an increment of 6\% when compared with recent models.},
  archive      = {J_ASOC},
  author       = {Cherukula Madhu and Sudhakar M.S.},
  doi          = {10.1016/j.asoc.2022.109968},
  journal      = {Applied Soft Computing},
  pages        = {109968},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive bezier curve-based membership function formulation scheme for interpretable edge detection},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial of the special issue intelligent solutions for
efficient logistics and sustainable transportation. <em>ASOC</em>,
<em>133</em>, 109961. (<a
href="https://doi.org/10.1016/j.asoc.2022.109961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Daniel Urda ( Guest Editors ) and Patricia Ruiz and El Ghazali Talbi and Pascal Bouvry and Jamal Toutouh},
  doi          = {10.1016/j.asoc.2022.109961},
  journal      = {Applied Soft Computing},
  pages        = {109961},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Editorial of the special issue intelligent solutions for efficient logistics and sustainable transportation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An open-set fault diagnosis framework for MMCs based on
optimized temporal convolutional network. <em>ASOC</em>, <em>133</em>,
109959. (<a href="https://doi.org/10.1016/j.asoc.2022.109959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability of the modular multilevel converters (MMCs) provides a vital guarantee for the uninterrupted operation of a system. Insulated Gate Bipolar Transistors (IGBTs) open circuit fault diagnosis is a common challenge in MMCs applications. In this paper, a novel open-set fault diagnosis framework called Multiscale-AAM-OTCN is proposed to solve both the known and unknown fault diagnosis problems of MMCs by outputting current signals. First, batch normalization and layer normalization are introduced into the original Temporal Convolutional Network (TCN) model to accelerate convergence and promote the generalization ability of the model for different tasks. Second, to strengthen the feature extraction ability of the model, the multiscale coordinate residual attention (MCRA) mechanism is designed for the one-dimensional (1D) current signal to improve the performance and stability of the method. Compared with recently developed attention mechanisms such as convolutional block attention module (CBAM), efficient channel attention (ECA), simple, parameter-free attention module (SimAM) and coordinate attention (CA), the proposed MCRA exhibits better performance in MMCs fault diagnosis tasks. Finally, the additive angular margin (AAM) loss and local outlier factor (LOF) algorithm are integrated into the Multiscale-OTCN framework to identify the density difference between known and unknown fault clusters by controlling the intra-class similarity and inter-class variance of the samples. The experimental results demonstrate the feasibility of the proposed fault diagnosis framework for known and unknown fault diagnoses.},
  archive      = {J_ASOC},
  author       = {Qun Guo and Jing Li and Fengdao Zhou and Gang Li and Jun Lin},
  doi          = {10.1016/j.asoc.2022.109959},
  journal      = {Applied Soft Computing},
  pages        = {109959},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An open-set fault diagnosis framework for MMCs based on optimized temporal convolutional network},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surrogate-assisted hybrid evolutionary algorithm with local
estimation of distribution for expensive mixed-variable optimization
problems. <em>ASOC</em>, <em>133</em>, 109957. (<a
href="https://doi.org/10.1016/j.asoc.2022.109957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some real-world design optimization problems can be formulated as expensive mixed-variable optimization problems (EMVOPs), which involve both continuous and discrete decision variables and expensive function evaluations. The main challenges for solving EMVOPs are the handling of mixed variables, limited number of function evaluations and multiple disconnected regions in the search space. In this work, we propose a novel algorithm with global and local search strategies for improving the search ability on disconnected regions, and it only consumes hundreds of function evaluations. The global module employs hybrid evolutionary operators and a Gower distance based surrogate model for handling mixed variables. The local estimation of distributions in different local regions performs in a competitive switching way to combine their advantages, and local surrogate models trained with selected samples improve the accuracy of approximated evaluations for the locally generated solutions. In the late stage, a local continuous search module is executed for refining the continuous decision variables. Verification results on the artificial benchmarks demonstrate that our proposed algorithm is competitive and works effectively. To verify the practicability of the algorithm, it is applied on a convolutional neural network hyperparameter optimization problems and obtains satisfactory results.},
  archive      = {J_ASOC},
  author       = {Yongcun Liu and Handing Wang},
  doi          = {10.1016/j.asoc.2022.109957},
  journal      = {Applied Soft Computing},
  pages        = {109957},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-assisted hybrid evolutionary algorithm with local estimation of distribution for expensive mixed-variable optimization problems},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Evolutionary computation-based machine learning for smart
city high-dimensional big data analytics. <em>ASOC</em>, <em>133</em>,
109955. (<a href="https://doi.org/10.1016/j.asoc.2022.109955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Science and technology development promotes Smart City Construction (SCC) as a most imminent problem. This work aims to improve the comprehensive performance of the Smart City-oriented high-dimensional Big Data Management (BDM) platform and promote the far-reaching development of SCC. It comprehensively optimizes the calculation process of the BDM platform through Machine Learning (ML), reduces the dimension of the data, and improves the calculation effect. To this end, this work first introduces the concept of SCC and the BDM platform application design. Then, it discusses the design concept of using ML technology to optimize the calculation effect of the BDM platform. Finally, the Tensor Train Support Vector Machine (TT-SVM) model is designed based on dimension reduction data processing . The proposed model can comprehensively optimize the BDM platform, and the model is compared with other models and evaluated. The research results show that the accuracy of the reduced dimension classification of the TT-SVM model is more than 95. The lowest average processing time for the model’s reduced dimension classification is about 1ms. The model’s highest data processing accuracy is about 98\%, and the average processing time is between 1.0–1.5ms. Compared with traditional models and BDM platforms, the proposed model has a breakthrough performance improvement, so it plays an important role in future SCC . This work has achieved a great breakthrough in big data processing , and innovatively improved the application mode of high-dimensional big data technology by integrating multiple technologies. Therefore, the finding provides targeted technical reference for algorithms in BDM platform and contributes to the construction and improvement of Smart City.},
  archive      = {J_ASOC},
  author       = {Xiaoming Li and Dan Zhang and Ye Zheng and Wuyang Hong and Weixi Wang and Jizhe Xia and Zhihan Lv},
  doi          = {10.1016/j.asoc.2022.109955},
  journal      = {Applied Soft Computing},
  pages        = {109955},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary computation-based machine learning for smart city high-dimensional big data analytics},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Terminal voltage prediction of li-ion batteries using
combined neural network and teaching learning based optimization
algorithm. <em>ASOC</em>, <em>133</em>, 109954. (<a
href="https://doi.org/10.1016/j.asoc.2022.109954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The static and dynamic model parameters are critical parameters for the accurate estimation of open-circuit voltage and the terminal voltage of a Lithium-Ion (Li-Ion) battery. This work mainly focuses on the investigation of an accurate method for predicting the open-circuit voltage and terminal voltage of Li-Ion batteries. This work proposes an Enhanced Neural Network and Teaching Learning-Based Optimization (ENN-TLBO) algorithm to predict the terminal voltage of a Li-Ion battery used in electric vehicle applications. In this work, the static model is obtained by using a neural network (NN) while the dynamic model parameters are predicted by using the Teaching Learning Based Optimization technique (TLBO). In the NN method, different network parameters are varied to predict the accurate State of Charge (SoC)-Open Circuit Voltage (OCV) relation of the battery. The proposed static models are validated by the performance metrics such as RMSE and R 2 , the best static model is identified by low Root Mean Square Error (RMSE) and high R 2 value of the model. In recent times many optimization algorithms are reported in the literature to predict the dynamic model, which requires certain algorithm-specific tuning parameters that affects the performance of the algorithm. The proposed TLBO algorithm is implemented with lesser tuning parameters to predict the hysteresis constant of the dynamic model. The RMSE value of the predicted terminal voltage at different temperature profiles is calculated to validate the proposed method. The proposed method has several advantages such as fewer tuning parameters, simple to implement, and high accuracy.},
  archive      = {J_ASOC},
  author       = {S. Siva Suriya Narayanan and S. Thangavel},
  doi          = {10.1016/j.asoc.2022.109954},
  journal      = {Applied Soft Computing},
  pages        = {109954},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Terminal voltage prediction of li-ion batteries using combined neural network and teaching learning based optimization algorithm},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Post-pandemic healthcare for COVID-19 vaccine: Tissue-aware
diagnosis of cervical lymphadenopathy via multi-modal ultrasound
semantic segmentation. <em>ASOC</em>, <em>133</em>, 109947. (<a
href="https://doi.org/10.1016/j.asoc.2022.109947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of COVID-19 vaccines all around the world, billions of people have benefited from the vaccination and thereby avoiding infection. However, huge amount of clinical cases revealed diverse side effects of COVID-19 vaccines, among which cervical lymphadenopathy is one of the most frequent local reactions. Therefore, rapid detection of cervical lymph node (LN) is essential in terms of vaccine recipients’ healthcare and avoidance of misdiagnosis in the post-pandemic era. This paper focuses on a novel deep learning-based framework for the rapid diagnosis of cervical lymphadenopathy towards COVID-19 vaccine recipients. Existing deep learning-based computer-aided diagnosis (CAD) methods for cervical LN enlargement mostly only depend on single modal images, e.g., grayscale ultrasound (US), color Doppler ultrasound, and CT, while failing to effectively integrate information from the multi-source medical images. Meanwhile, both the surrounding tissue objects of the cervical LNs and different regions inside the cervical LNs may imply valuable diagnostic knowledge which is pending for mining. In this paper, we propose an Tissue-Aware Cervical Lymph Node Diagnosis method (TACLND) via multi-modal ultrasound semantic segmentation. The method effectively integrates grayscale and color Doppler US images and realizes a pixel-level localization of different tissue objects, i.e., lymph, muscle, and blood vessels. With inter-tissue and intra-tissue attention mechanisms applied, our proposed method can enhance the implicit tissue-level diagnostic knowledge in both spatial and channel dimension, and realize diagnosis of cervical LN with normal, benign or malignant state. Extensive experiments conducted on our collected cervical LN US dataset demonstrate the effectiveness of our methods on both tissue detection and cervical lymphadenopathy diagnosis. Therefore, our proposed framework can guarantee efficient diagnosis for the vaccine recipients’ cervical LN, and assist doctors to discriminate between COVID-related reactive lymphadenopathy and metastatic lymphadenopathy.},
  archive      = {J_ASOC},
  author       = {Yue Gao and Xiangling Fu and Yuepeng Chen and Chenyi Guo and Ji Wu},
  doi          = {10.1016/j.asoc.2022.109947},
  journal      = {Applied Soft Computing},
  pages        = {109947},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Post-pandemic healthcare for COVID-19 vaccine: Tissue-aware diagnosis of cervical lymphadenopathy via multi-modal ultrasound semantic segmentation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-objective discrete differential evolution algorithm
for energy-efficient two-stage flow shop scheduling under time-of-use
electricity tariffs. <em>ASOC</em>, <em>133</em>, 109946. (<a
href="https://doi.org/10.1016/j.asoc.2022.109946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the appeal for environmental protection and the efficient utilization of energy, local governments in China actively promote time-of-use (TOU) electricity tariffs for manufacturing enterprises. Motivated by a real-life industrial scenario of large special-purpose pressure vessel production, this paper addresses an energy-efficient two-stage flow shop scheduling problem under TOU tariffs to minimize the total electricity cost and the mean tardiness. Since the problem is computationally intractable, we focus on developing a multi-objective discrete differential evolution (MDDE) algorithm. Specifically, based on the optimal properties of the problem, we tailor an encoding scheme that consists of two job sequences and an idle time vector. The novel mutation and crossover operators are designed to generate the trail individuals, and the hypervolume contribution indicator is incorporated into the bi-criteria selection operator to measure the quality of the solution. Furthermore, two neighborhood structures are designed to iteratively improve the non-dominated solutions in the external archive set. We evaluate the performance of the MDDE algorithm via extensive computational experiments. The experimental results indicate that the MDDE algorithm can obtain the good approximate Pareto front , and it outperforms the well-known NSGA-II, SPEA2 and MOEA/D algorithms in solution quality.},
  archive      = {J_ASOC},
  author       = {Ling Xue and Xiuli Wang},
  doi          = {10.1016/j.asoc.2022.109946},
  journal      = {Applied Soft Computing},
  pages        = {109946},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective discrete differential evolution algorithm for energy-efficient two-stage flow shop scheduling under time-of-use electricity tariffs},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal convolutional networks with RNN approach for
chaotic time series prediction. <em>ASOC</em>, <em>133</em>, 109945. (<a
href="https://doi.org/10.1016/j.asoc.2022.109945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of chaotic time series, which constitutes many systems in the field of science and engineering, has recently become the focus of attention of researchers. Chaotic time series prediction is making future predictions about these systems using previously observed data for a nonlinear chaotic system with a known initial condition. Chaotic time series prediction can be applied in many fields such as weather forecasting, finance and stock markets. Many disciplines work on solving time series prediction problem, ranging from forecasting weather events days in advance to traders predicting the future of stocks. In recent studies, it has been observed that hybrid deep neural network methods give better performance in solving time series prediction problems and have gained popularity in order to benefit from the advantages of more than one method in solving such problems. In this study, a hybrid deep neural network architecture is proposed for chaotic time series prediction. The used hybrid approach includes both temporal convolutional network to extract low level features from input and recurrent neural network layers such as long short-term memory and gated recurrent units to capture temporal information. Simulations were carried out on nine different chaotic time series dataset which are obtained from Lorenz, Rössler and a Lorenz-like chaotic equation sets, and twenty-one electrocardiogram (ECG) recordings of patients with arrhythmias. In the benchmark study, in which twelve different methods, including classical machine learning , deep neural network and hybrid models were used, the proposed model achieved the best prediction performance with an average root-mean-square error (RMSE) value of 0.0022 for chaotic dataset and 0.0082 for ECG arrhythmia dataset. Performance evaluation metrics show that the proposed hybrid architecture can compete with the models in state-of-the-art studies in chaotic time series prediction.},
  archive      = {J_ASOC},
  author       = {Hatice Vildan Dudukcu and Murat Taskiran and Zehra Gulru Cam Taskiran and Tulay Yildirim},
  doi          = {10.1016/j.asoc.2022.109945},
  journal      = {Applied Soft Computing},
  pages        = {109945},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Temporal convolutional networks with RNN approach for chaotic time series prediction},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Heuristic smoothing ant colony optimization with
differential information for the traveling salesman problem.
<em>ASOC</em>, <em>133</em>, 109943. (<a
href="https://doi.org/10.1016/j.asoc.2022.109943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traveling salesman problem (TSP) is an NP complete problem with potential applications. Thus far, numerous approaches have been proposed to solve the TSP including exact, heuristic and metaheuristic methods. Among them, the ant colony optimization (ACO) algorithm, belonging to metaheuristic methods, has proven to be an efficient method for solving the TSP. However, the phenomena of rapid convergence to local optima and unsatisfactory computational accuracy distinctly limit the performance of ACO. This study therefore proposes a novel ACO algorithm (HSDACO) to compensate for these shortcomings. In the HSDACO algorithm, the involvement of heterogeneous population automation at each iteration generates better candidate solutions with maintaining parameter diversity. Then, the implementation of three smoothing techniques with the 2- Opt method guarantees the effective assessment of candidate solutions in favor of higher quality. Next, the introduction of a differential information updating mechanism, using differential edge information obtained from the candidate solutions, enables the evaporation operation of the pheromone trail to obtain more reasonable guidance. Subsequently, the design of evolutionary state estimation and adjustment is used to monitor the population under correct guidance and regulate the evolutionary state effectively and efficiently. Finally, the HSDACO algorithm is evaluated on public TSP benchmark instances and compared with the other state-of-the-art algorithms. Experimental results show that the proposed HSDACO achieves substantial improvement for mid-scale TSP instances and overwhelming advantages for small-scale TSP instances, in terms of solution accuracy and convergence speed.},
  archive      = {J_ASOC},
  author       = {Wei Li and Cancan Wang and Ying Huang and Yiu-ming Cheung},
  doi          = {10.1016/j.asoc.2022.109943},
  journal      = {Applied Soft Computing},
  pages        = {109943},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heuristic smoothing ant colony optimization with differential information for the traveling salesman problem},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoML technologies for the identification of sparse
classification and outlier detection models. <em>ASOC</em>,
<em>133</em>, 109942. (<a
href="https://doi.org/10.1016/j.asoc.2022.109942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated machine learning (AutoML) technologies offer powerful methods to automate the choice of meta-parameters and the instantiations of components of the machine learning training pipelines, such as an optimum form of data preprocessing or a suitable strength of model regularization. Besides given training data, AutoML relies on a suitable learning objective or scoring function and a search space in which to optimize the choices. Currently, most AutoML technologies focus on a single objective, which is related to the expected accuracy of the found model as evaluated according to the chosen learning objective. Additional desired characteristics such as model sparsity for an increased model efficiency and interpretability can be integrated as additional penalty terms in the objective function. Yet, this leads to one solution only, and does not mediate in between accuracy and sparsity as two usually contradicting objectives. In this contribution, we are interested in AutoML technologies which explore the full Pareto-front of sparse versus accurate models rather than a single average only. Since it is not guaranteed that architectural and meta-parameter choices stay constant along the full Pareto-front, averaging the two objectives is not necessarily optimal in this realm. Hence we propose how to treat this challenge by a novel iterative pipeline, which combines an AutoML method with feature selection technologies. We compare this technology to alternatives including baselines in two relevant modeling tasks, classification and outlier detection. We demonstrate the performance of these strategies for a couple of benchmark tasks. 1},
  archive      = {J_ASOC},
  author       = {Aleksei Liuliakov and Luca Hermes and Barbara Hammer},
  doi          = {10.1016/j.asoc.2022.109942},
  journal      = {Applied Soft Computing},
  pages        = {109942},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AutoML technologies for the identification of sparse classification and outlier detection models},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Airport flight ground service time prediction with missing
data using graph convolutional neural network imputation and
bidirectional sliding mechanism. <em>ASOC</em>, <em>133</em>, 109941.
(<a href="https://doi.org/10.1016/j.asoc.2022.109941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate flight ground service time prediction plays a key role in the efficient operation of airports , which can help managers make better resource scheduling optimization decisions , reduce flight delays and improve airport operation capabilities. However, since flight ground service data is typically acquired by sensor equipment or manually recorded, data missing and abnormalities caused by sensor failure or manual recording errors are an unavoidable occurrence, making the prediction problem challenging. Although missing values can be imputed, the traditional methods usually fill in missing values and then make predictions separately, which leads to a two-step process and increases the prediction error. Besides, the existing one-step methods still suffer some limitations and shortcomings: failure to integrate bidirectional spatial–temporal dependencies leads to unsatisfying model prediction accuracy and poor robustness under high data missing rates, which cannot be directly used in the study of flight ground service time prediction with missing values. To solve this problem, we presented a sliding bidirectional graph convention network model framework that capable of dealing predicting with missing values. First, we proposed a new adjacency matrix construction method to capture more comprehensive spatial–temporal dependencies and developed a graph convention network based missing value imputation unity (GCNM) to inference and filling missing features. Then, multiple GCNM units are stacked to develop a bidirectional module BDGCNM to realize the filling and prediction into one step. Finally, we introduced a variable sliding window mechanism to improve the model’s robustness. Extensive experiments on real airport data sets and PEMS traffic data sets show that our method is more accurate and robust than the current state-of-the-art.},
  archive      = {J_ASOC},
  author       = {Chang Liu and YanRu Chen and Hao Wang and YuanYuan Zhang and Xuewu Dai and Qian Luo and LiangYin Chen},
  doi          = {10.1016/j.asoc.2022.109941},
  journal      = {Applied Soft Computing},
  pages        = {109941},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Airport flight ground service time prediction with missing data using graph convolutional neural network imputation and bidirectional sliding mechanism},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing an irrigation treatment using an evolutionary
algorithm and a knowledge discovery framework based on deep models.
<em>ASOC</em>, <em>133</em>, 109940. (<a
href="https://doi.org/10.1016/j.asoc.2022.109940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, learning models have good accuracy and learn complex patterns which are hidden in the data. Occasionally, patterns which are discovered using these models cannot be discovered by a human. So representing the discovered patterns and knowledge as an explainable knowledge for human is important. In this paper, a framework is proposed for knowledge discovery from real agricultural datasets. In the proposed framework, Deep Learning (DL) models are used for learning the patterns and effective features from tabular datasets; The integrated gradient method is used as an indirect metric for representing the black-box models as an interpretable model; A fuzzy expert system is used for interpreting the discovered knowledge as an expert system; An especial evolutionary algorithm is used for extracting the knowledge as a fuzzy expert system . In order to evaluate the proposed framework, some experiments are performed on the real tabular dataset on the agricultural field for extracting knowledge about productivity. For validating the discovered knowledge, an optimization algorithm is proposed which uses the discovered expert system for designing optimal irrigation treatments on the selected dataset. DSSAT, which is a farm simulator, is used for validating discovered knowledge. The obtained results show that the discovered knowledge can improve the productivity by 30\% and decrease the used water by 30\%. In addition, yields of the designed treatment and discovered knowledge correlate more than 0.8.},
  archive      = {J_ASOC},
  author       = {Ehsan Pazouki},
  doi          = {10.1016/j.asoc.2022.109940},
  journal      = {Applied Soft Computing},
  pages        = {109940},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing an irrigation treatment using an evolutionary algorithm and a knowledge discovery framework based on deep models},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and robust spatial fuzzy bounded k-plane clustering
method for human brain MRI image segmentation. <em>ASOC</em>,
<em>133</em>, 109939. (<a
href="https://doi.org/10.1016/j.asoc.2022.109939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy k-plane clustering (FkPC) is a soft plane-based clustering that efficiently clusters non-spherically distributed data. However, the FkPC method is sensitive to noise and provides unbounded cluster planes. To overcome these two limitations, we propose two modifications in the conventional FkPC method , referred as fuzzy bounded k-plane clustering method with local spatial information (FBkPC_S1). We introduce FCM objective function to bound the cluster planes and local spatial information in the objective function of FkPC to handle noise. The proposed FBkPC_S1 clustering method is fast and robust as it produces bounded cluster planes and can provide accurate segmentation in presence of noise. To show the effectiveness of the proposed FBkPC_S1 method, extensive experiments are performed on one synthetic image dataset and three publicly available human brain MRI datasets. The performance of the proposed FBkPC_S1 method is compared with 19 related methods in terms of average segmentation accuracy and Dice score. The proposed method achieves 91\%, 65\% and 75\% average segmentation accuracy in the presence of noise artifacts on BrainWeb, IBSR and MRBrainS18 MRI datasets, respectively. Experimental results and statistical test demonstrate superior performance of the proposed FBkPC_S1 method in comparison to related methods.},
  archive      = {J_ASOC},
  author       = {Puneet Kumar and R.K. Agrawal and Dhirendra Kumar},
  doi          = {10.1016/j.asoc.2022.109939},
  journal      = {Applied Soft Computing},
  pages        = {109939},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fast and robust spatial fuzzy bounded k-plane clustering method for human brain MRI image segmentation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A class of general type-2 fuzzy controller based on adaptive
alpha-plane for nonlinear systems. <em>ASOC</em>, <em>133</em>, 109938.
(<a href="https://doi.org/10.1016/j.asoc.2022.109938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a general type-2 Takagi–Sugeno–Kang fuzzy controller (GT2-TSKFC) for controlling uncertain systems. The proposed GT2-TSKFC uses equidistant type-2 triangular membership functions (MFs) for the antecedents, Larsen’s implication method, type-1 fuzzy sets for the consequent parameters, and a direct defuzzification method. The analytical structure of the proposed controller indicates that the alpha-plane and apex of the secondary MFs have a noticeable effect on calculating the control signal. Adaptation of the alpha-plane and apex of the secondary MFs is performed using the Lyapunov function to achieve the stability of the controlled system. The proposed controller is applied to an uncertain nonlinear inverted pendulum system. The results of the proposed control algorithm are compared with those of a general type-2 fuzzy controller with a specific number of alpha-planes, a quasi type-2 fuzzy controller, an interval type-2 fuzzy controller, and a type-1 fuzzy controller to demonstrate the robustness and effectiveness of the proposed scheme.},
  archive      = {J_ASOC},
  author       = {Ahmad M. El-Nagar and Mohammad El-Bardini and A. Aziz Khater},
  doi          = {10.1016/j.asoc.2022.109938},
  journal      = {Applied Soft Computing},
  pages        = {109938},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A class of general type-2 fuzzy controller based on adaptive alpha-plane for nonlinear systems},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust energy management technique for plug-in hybrid
electric vehicle with traffic condition identification. <em>ASOC</em>,
<em>133</em>, 109937. (<a
href="https://doi.org/10.1016/j.asoc.2022.109937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper proposes an efficient hybrid approach for optimal energy management of plug in hybrid electric vehicle (PHEV) with traffic conditions. The proposed hybrid system is joint operation of Atomic Orbital Search (AOS) and Recalling Enhanced Recurrent Neural Network (RERNN) and normally named as AOS-RERNN approach. The main purpose of the proposed approach is to control the energy through Internet of Vehicles (IoVs), which provides significant fuel economy of PHEV. Based on certain traffic condition the derived driving cycle-based parameter of the energy management is optimized by the AOS approach in online. The controlling thresholds are optimized by AOS to provide a set of optimal parameters. At last, the performance of the proposed system is executed on MATLAB/Simulink working platform compared with various existing methods. The proposed approach provides improved fuel economy than the existing approaches.},
  archive      = {J_ASOC},
  author       = {C.N. Gnanaprakasam and S. Meena and M. Nivethitha Devi and N. Shanmugasundaram and S. Sridharan},
  doi          = {10.1016/j.asoc.2022.109937},
  journal      = {Applied Soft Computing},
  pages        = {109937},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust energy management technique for plug-in hybrid electric vehicle with traffic condition identification},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter estimation in unbalanced three-phase distribution
lines using general regression neural networks with inconsistent data
handling capacity. <em>ASOC</em>, <em>133</em>, 109936. (<a
href="https://doi.org/10.1016/j.asoc.2022.109936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The feeder parameters of a power system play an important role in its operation and analysis. However, in many situations, the values used in the calculations differ from the actual parameters, resulting in large errors. Therefore, in this study, a highly accurate feeder parameter estimation method for a three-phase distribution line was proposed. The proposed method uses the measured node voltages and power quantities from the two ends of the distribution line. A precise electrical model of the three-phase distribution line was used. An improved type of neural network , namely general regression neural networks (GRNN), is used to solve the complex non-linear equation and then estimate the actual parameters. However, the proposed estimation process does not require synchronised phasors or phasor-measurement units installed at the point of measurement. Nonetheless, the estimation process is also capable of handling incorrect data information, noisy datasets, and measurement errors while maintaining accuracy at the desired level. The estimator process is examined and analysed using four different IEEE unbalanced three-phase systems. In addition, the proposed methodology is compared with standard algorithms of radial basis function networks , quasi-Newton, and multi-run optimisation methods on the basis of maximum absolute percentage error and other standard error functions. The hyperparameter-tuned GRNN model was also tested using several statistical significance tests. In all scenarios, the performance and accuracy of the proposed methodology and model exceeded those of the other methods.},
  archive      = {J_ASOC},
  author       = {Nien-Che Yang and Abhilash Sen},
  doi          = {10.1016/j.asoc.2022.109936},
  journal      = {Applied Soft Computing},
  pages        = {109936},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter estimation in unbalanced three-phase distribution lines using general regression neural networks with inconsistent data handling capacity},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DHAN: Encrypted JPEG image retrieval via DCT
histograms-based attention networks. <em>ASOC</em>, <em>133</em>,
109935. (<a href="https://doi.org/10.1016/j.asoc.2022.109935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image retrieval , the images to be retrieved are stored on remote servers. Since the images contain amounts of privacy information and the server cannot be fully trusted, people usually encrypt their images before uploading them to the server, which raises the demand for encrypted image retrieval (EIR). Current EIR techniques extract ruled hand-craft features from cipher images first and then build retrieval models (e.g., support vector machine , SVM) by these features, or use deep learning models (e.g., Convolutional Neural Network , CNN) to learn cipher-image representations in an end-to-end manner. However, SVM is not skilled at learning non-linear embedding in complex image database, and end-to-end EIR leads to low image security or retrieval performance because CNN is sensitive to extreme chaotic cipher images. Not-end-to-end EIR offers excellent encryption performance, and deep learning can further mine non-linear embedding from ruled hand-craft features. To this end, we propose a novel EIR scheme, named discrete cosine transform ( D CT) H istograms-based A ttention N etworks (DHAN), which is based on deep learning to enhance expression ability of cipher-image in a not-end-to-end manner. Specifically, the DCT coefficients of images are encrypted by value replacement and block permutation encryption, and then the effective histogram features of DCT coefficients are extracted from the cipher images since the sets of DCT frequency in encrypted images are similar to that of plain images. After that, to dynamically learn the salient features of cipher images, we propose a new module named ResAttention and design deep attention networks to provide retrieval. Extensive experiments on two datasets demonstrate that DHAN not only provides high image security but also greatly improves retrieval performance than that of existing schemes.},
  archive      = {J_ASOC},
  author       = {Qihua Feng and Peiya Li and Zhixun Lu and Zhibo Zhou and Yongdong Wu and Jian Weng and Feiran Huang},
  doi          = {10.1016/j.asoc.2022.109935},
  journal      = {Applied Soft Computing},
  pages        = {109935},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DHAN: Encrypted JPEG image retrieval via DCT histograms-based attention networks},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A digital decision approach for scheduling process planning
of shared bikes under internet of things environment. <em>ASOC</em>,
<em>133</em>, 109934. (<a
href="https://doi.org/10.1016/j.asoc.2022.109934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital technology plays an important role in the construction of intelligent transportation systems . The digitization of travel and traffic information contributes to the efficiency, equality, and safety of mobility for urban residents. This research aims at improving the imbalance between the supply and demand of bike-sharing system. A scheduling process planning algorithm for shared bikes towards Internet of Things (IoT) environment was proposed in this research. Firstly, the algorithm combined the massive shared bikes data of IoT, Long Short-Term Memory and Gated Recurrent Unit neural networks (LSTM-GRU) hybrid model was employed to predict the shared bike demand towards electronic fence(e-fence). Then, based on the division of scheduling sub-areas, the mathematical model of scheduling path optimization for shared bikes was constructed, in addition to the constraints and costs, the minimum carbon emission was also considered as the objective function. Finally, the algorithm was applied to the bike-sharing system in Yanqing District (Beijing China) as the case study. The results show that: the LSTM-GRU hybrid model proposed in this research has a high prediction accuracy of 6.16 mean square error and 0.86 goodness of fit, the sub-area partition model can reduce the average scheduling demand imbalance degree by 58.90\%. Under the same scheduling task, electric vehicles can reduce carbon emissions by about 32\% compared with fuel vehicles. The proposed shared bike scheduling planning algorithm can provide a decision-making guidance for related operation departments, as well as realize low-carbon and sustainable development for urban transportation system .},
  archive      = {J_ASOC},
  author       = {Xianyu Wu and Jie Lin and Yang Yang and Jingxue Guo},
  doi          = {10.1016/j.asoc.2022.109934},
  journal      = {Applied Soft Computing},
  pages        = {109934},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A digital decision approach for scheduling process planning of shared bikes under internet of things environment},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute community detection based on latent representation
learning and graph regularized non-negative matrix factorization.
<em>ASOC</em>, <em>133</em>, 109932. (<a
href="https://doi.org/10.1016/j.asoc.2022.109932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, complex networks have become increasingly difficult to detect effectively with the rapid development of networks. On the one hand, the scale of the network has increased sharply, and on the other hand, the nodes in the network contain rich content. The existing algorithms do not take the different richness of attribute information of nodes in the attribute network into account. And the topology information of the node is inconsistent with the attribute information. Thus, this paper proposes an attribute community detection algorithm based on latent representation learning and graph-regularized non-negative matrix factorization (LRL-GNMF). First, the topological information and attribute information in the attribute network is decomposed based on non-negative matrix factorization respectively. Thereby, the member distribution matrix and the attribute distribution matrix are obtained. Secondly, an affinity matrix is constructed for the attribute matrix, and the latent representation of the attribute information is obtained using the latent representation learning method. In addition, a transition matrix is constructed according to the Markov transition probability. The node membership distribution matrix and the attribute distribution matrix are linked. Finally, since the attribute information of nodes in the network is different, a topology-dominated model and an attribute-dominated model are respectively constructed to solve this problem. At the same time, a graph regularization term is introduced to guide the model to obtain more accurate community detection. This paper conducts experimental analysis on 8 real networks. The experimental results show that the proposed algorithm in this paper outperforms the other 11 compared algorithms in the evaluation indicators of community detection accuracy and standard mutual information.},
  archive      = {J_ASOC},
  author       = {Ronghua Shang and Weitong Zhang and Zhiyuan Li and Chao Wang and Licheng Jiao},
  doi          = {10.1016/j.asoc.2022.109932},
  journal      = {Applied Soft Computing},
  pages        = {109932},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attribute community detection based on latent representation learning and graph regularized non-negative matrix factorization},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A kernel-free laplacian quadratic surface optimal margin
distribution machine with application to credit risk assessment.
<em>ASOC</em>, <em>133</em>, 109931. (<a
href="https://doi.org/10.1016/j.asoc.2022.109931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel binary classification approach named Laplacian quadratic surface optimal margin distribution machine (LapQSODM), for semisupervised learning . This new model exploits the geometric information embedded in unlabeled samples through a manifold regularizer to overcome the problem of insufficient labeled samples. Different from the traditional support vector machines (SVMs) based on the largest minimum margin idea and using the kernel technique to address the nonlinearity in the datasets, our proposed model optimizes the margin distribution and directly generates a quadratic surface to perform the classification. This new model not only improves the generalization performance but also avoids the difficulty of searching for an appropriate kernel function and tuning its parameters. For the regular-scale datasets, we extend the classical conjugate gradient algorithm for the proposed method and design an easy iterative method to calculate its exact step size; for the large-scale datasets, we develop an unbiased estimation for the LapQSODM gradient by utilizing one or several samples and design a stochastic gradient descent with variance reduction (SVRG) algorithm that is efficient and effective. Then, we conduct a comprehensive numerical experiment on both artificial and public benchmark datasets. The experimental results show LapQSODM has a better generalization performance than most of the well-known benchmark methods and is more robust due to the optimization of the margin distribution. Finally, we apply LapQSODM to three realistic credit risk assessment problems. The promising numerical results demonstrate the great potential and ability of our model in the real practice.},
  archive      = {J_ASOC},
  author       = {Jingyue Zhou and Ye Tian and Jian Luo and Qianru Zhai},
  doi          = {10.1016/j.asoc.2022.109931},
  journal      = {Applied Soft Computing},
  pages        = {109931},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A kernel-free laplacian quadratic surface optimal margin distribution machine with application to credit risk assessment},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pareto optimal prediction intervals with hypernetworks.
<em>ASOC</em>, <em>133</em>, 109930. (<a
href="https://doi.org/10.1016/j.asoc.2022.109930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the relevance of probabilistic forecasting grows, the need of estimating multiple high-quality prediction intervals (PI) also increases. In the current state of the art, most deep neural network gradient descent-based methods take into account interval width and coverage into a single loss function, focusing on a unique nominal coverage target, and adding additional parameters to control the coverage–width trade-off. The Pareto Optimal Prediction Interval Hypernetwork (POPI-HN) approach developed in this work has been derived to treat this coverage–width trade-off as a multi-objective problem, obtaining a complete set of Pareto Optimal solutions (Pareto front). POPI-HN are able to be trained through gradient descent with no need to add extra parameters to control the width–coverage trade-off of PIs. Once the Pareto set has been obtained, users can extract the PI with the required coverage. Comparative results with recently introduced Quality-Driven loss show similar behavior in coverage while improving interval width for the majority of the studied domains, making POPI-HN a competing alternative for estimating uncertainty in regression tasks where PIs with multiple coverages are needed.},
  archive      = {J_ASOC},
  author       = {Antonio Alcántara and Inés M. Galván and Ricardo Aler},
  doi          = {10.1016/j.asoc.2022.109930},
  journal      = {Applied Soft Computing},
  pages        = {109930},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pareto optimal prediction intervals with hypernetworks},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ANN-entropy-FA model for prediction and optimization of
biodiesel-based engine performance. <em>ASOC</em>, <em>133</em>, 109929.
(<a href="https://doi.org/10.1016/j.asoc.2022.109929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work incorporated full factorial experimentation for performance analysis of Nahar oil-based biodiesel in a four-stroke diesel engine . The controllable input parameters are engine revolution per minute (N), engine load (EL), and blend mixture (BM) respectively. The measured engine performance characteristics are brake thermal efficiency (BTE), brake-specific fuel consumption (BSFC), and exhaust gas temperature (ET). Prediction and optimization of engine output characteristics have been computed through the ANN-entropy-FA hybrid model. In this model, a trained Artificial Neural Network (ANN) is used to compute the objective function value during optimization using the Firefly algorithm (FA). The maximum absolute percentage error of trained ANN during the prediction of output parameters is found as less than 3\%. The entropy method is employed to determine the weight of output parameters in the combined objective function. ANN-FA optimization determined minimum BSFC = 0.386 kg/kW-hr, maximum BTE = 24.60\%, and minimum ET = 163.24 °C with corresponding operational input parameter setting of low engine rpm (1250 rpm), medium engine load (15.20 kg), and a high percentage of blend mixture ( ≈ ≈ 25\%) respectively. A significantly low absolute\% error of around 1.75\% during experimental validation of ANN-FA optimized output indicates the efficacy of the proposed model.},
  archive      = {J_ASOC},
  author       = {Sudipto Chaki and Tapas Kumar Biswas},
  doi          = {10.1016/j.asoc.2022.109929},
  journal      = {Applied Soft Computing},
  pages        = {109929},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ANN-entropy-FA model for prediction and optimization of biodiesel-based engine performance},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute reduction in inconsistent grey decision systems
based on variable precision grey multigranulation rough set model.
<em>ASOC</em>, <em>133</em>, 109928. (<a
href="https://doi.org/10.1016/j.asoc.2022.109928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly deals with attribute reduction of inconsistent grey decision systems (IGDSs) based on the variable precision grey multigranulation rough set (VP-GMGRS). Firstly, we present two transformation models to transform IGDS into consistent decision confidence system. One is the consistent decision system transformation model, based on which, an IGDS can be transformed into a VP-GMGRS approximate distribution consistent decision system. The other is the decision confidence system transformation model, which can be degenerated to a classical group decision system. Meanwhile, we educe related judgement theorems of approximation distribution consistent set in IGDS. Following that, a theoretical attribute reduction approach is presented by employing discernibility attribute sets and function based on VP-GMGRS approximate distributions. In addition, algorithms and illustrative examples with IGDS are employed and assisted to understand and explain the mechanism of attribute reduction theoretical approaches. Finally, comparison experiments are organized to verify the validity and feasibility of the proposed reduction method.},
  archive      = {J_ASOC},
  author       = {Yun Kang and Jianhua Dai},
  doi          = {10.1016/j.asoc.2022.109928},
  journal      = {Applied Soft Computing},
  pages        = {109928},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attribute reduction in inconsistent grey decision systems based on variable precision grey multigranulation rough set model},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model reference control by recurrent neural network built
with paraconsistent neurons for trajectory tracking of a rotary inverted
pendulum. <em>ASOC</em>, <em>133</em>, 109927. (<a
href="https://doi.org/10.1016/j.asoc.2022.109927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This investigation presents a recurrent paraconsistent neural network (RPNN), as the main element of the model reference control (MRC) strategy for the rotary inverted pendulum (RIP). The RIP characteristics, such as nonlinearity , two-degree-of-freedom (2DoF) motion, and under-actuated system, make it an ideal device to apply and test the RPNN. The designed paraconsistent neural model reference controller (PNMRC) uses three RPNNs: two of them to model the arm and pendulum angles and the third one to control the system while tracking a reference trajectory . The hidden neurons of the RPNN use the paraconsistent annotated logic by 2-value annotations (PAL2v) rules as an activation function . PAL2v, as a member of the paraconsistent logics family, deals with uncertain and contradictory data, representing a potentially robust alternative to applications of artificial neural networks in control. The PAL2v neuron is detailed and compared with other activation functions in recurrent neural networks (RNN). With real-time experiments, the PNMRC strategy is compared with classical control methodology, presenting excellent performance.},
  archive      = {J_ASOC},
  author       = {Arnaldo de Carvalho Junior and Bruno Augusto Angelico and João Francisco Justo and Alexandre Maniçoba de Oliveira and João Inacio da Silva Filho},
  doi          = {10.1016/j.asoc.2022.109927},
  journal      = {Applied Soft Computing},
  pages        = {109927},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Model reference control by recurrent neural network built with paraconsistent neurons for trajectory tracking of a rotary inverted pendulum},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective automatic analysis of lung ultrasound data
from COVID-19 patients by means of deep learning and decision trees.
<em>ASOC</em>, <em>133</em>, 109926. (<a
href="https://doi.org/10.1016/j.asoc.2022.109926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 raised the need for automatic medical diagnosis, to increase the physicians’ efficiency in managing the pandemic. Among all the techniques for evaluating the status of the lungs of a patient with COVID-19, lung ultrasound (LUS) offers several advantages: portability, cost-effectiveness, safety. Several works approached the automatic detection of LUS imaging patterns related COVID-19 by using deep neural networks (DNNs). However, the decision processes based on DNNs are not fully explainable, which generally results in a lack of trust from physicians. This, in turn, slows down the adoption of such systems. In this work, we use two previously built DNNs as feature extractors at the frame level, and automatically synthesize, by means of an evolutionary algorithm , a decision tree (DT) that aggregates in an interpretable way the predictions made by the DNNs, returning the severity of the patients’ conditions according to a LUS score of prognostic value. Our results show that our approach performs comparably or better than previously reported aggregation techniques based on an empiric combination of frame-level predictions made by DNNs. Furthermore, when we analyze the evolved DTs, we discover properties about the DNNs used as feature extractors. We make our data publicly available for further development and reproducibility.},
  archive      = {J_ASOC},
  author       = {Leonardo Lucio Custode and Federico Mento and Francesco Tursi and Andrea Smargiassi and Riccardo Inchingolo and Tiziano Perrone and Libertario Demi and Giovanni Iacca},
  doi          = {10.1016/j.asoc.2022.109926},
  journal      = {Applied Soft Computing},
  pages        = {109926},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective automatic analysis of lung ultrasound data from COVID-19 patients by means of deep learning and decision trees},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of multimodal hub-and-spoke transportation network
for emergency relief under COVID-19 pandemic: A meta-heuristic approach.
<em>ASOC</em>, <em>133</em>, 109925. (<a
href="https://doi.org/10.1016/j.asoc.2022.109925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When COVID-19 suddenly broke out, the epidemic areas are short of basic emergency relief which need to be transported from surrounding areas. To make transportation both time-efficient and cost-effective, we consider a multimodal hub-and-spoke transportation network for emergency relief schedules. Firstly, we establish a mixed integer nonlinear programming (MINLP) model considering multi-type emergency relief and multimodal transportation. The model is a bi-objective one that aims at minimizing both transportation time consumption and transportation costs. Due to its NP-hardness, devising an efficient algorithm to cope with such a problem is challenging. This study thus employs and redesigns Grey Wolf Optimizer (GWO) to tackle it. To benchmark our algorithm, a real-world case is tested with three solution methods which include other two state-of-the-art meta-heuristics. Results indicate that the customized GWO can solve such a problem in a reasonable time with higher accuracy. The research could provide significant practical management insights for related government departments and transportation companies on designing an effective transportation network for emergency relief schedules when faced with the unexpected COVID-19 pandemic.},
  archive      = {J_ASOC},
  author       = {Chi Li and Peixiu Han and Min Zhou and Ming Gu},
  doi          = {10.1016/j.asoc.2022.109925},
  journal      = {Applied Soft Computing},
  pages        = {109925},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of multimodal hub-and-spoke transportation network for emergency relief under COVID-19 pandemic: A meta-heuristic approach},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The choice of scaling technique matters for classification
performance. <em>ASOC</em>, <em>133</em>, 109924. (<a
href="https://doi.org/10.1016/j.asoc.2022.109924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataset scaling, also known as normalization, is an essential preprocessing step in a machine learning pipeline. It is aimed at adjusting attributes scales in a way that they all vary within the same range. This transformation is known to improve the performance of classification models , but there are several scaling techniques to choose from, and this choice is not generally done carefully. In this paper, we execute a broad experiment comparing the impact of 5 scaling techniques on the performances of 20 classification algorithms among monolithic and ensemble models, applying them to 82 publicly available datasets with varying imbalance ratios . Results show that the choice of scaling technique matters for classification performance, and the performance difference between the best and the worst scaling technique is relevant and statistically significant in most cases. They also indicate that choosing an inadequate technique can be more detrimental to classification performance than not scaling the data at all. We also show how the performance variation of an ensemble model, considering different scaling techniques, tends to be dictated by that of its base model. Finally, we discuss the relationship between a model’s sensitivity to the choice of scaling technique and its performance and provide insights into its applicability on different model deployment scenarios . Full results and source code for the experiments in this paper are available in a GitHub repository. 1},
  archive      = {J_ASOC},
  author       = {Lucas B.V. de Amorim and George D.C. Cavalcanti and Rafael M.O. Cruz},
  doi          = {10.1016/j.asoc.2022.109924},
  journal      = {Applied Soft Computing},
  pages        = {109924},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The choice of scaling technique matters for classification performance},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive niching particle swarm optimization with local
search for multimodal optimization. <em>ASOC</em>, <em>133</em>, 109923.
(<a href="https://doi.org/10.1016/j.asoc.2022.109923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal optimization problems (MMOPs), in which multiple optimal solutions need to be found for decision-makers, are common in real-world applications. Finding as many peaks as feasible and enhancing the accuracy of solutions on already identified peaks are the objectives of solving MMOPs. An adaptive particle swarm optimization (PSO) based on speciation, species regulation, and local search, termed SR-PSO-MAES, is proposed to achieve these objectives. First, an adaptive speciation strategy is introduced to divide the population, which forms species according to the similarity of particles and does not need to set the radius of species in advance. Second, a species regulation strategy is proposed to avoid a few species occupying most of the population during the iterative process. Third, a matrix adaptation evolution strategy (MAES) with a restart scheme as a local search strategy is used to evaluate the species for further improving the accuracy of the solutions. By comparing with 15 state-of-the-art multimodal optimization algorithms , the experimental results through a benchmark test problem and a real wet spinning process validate the superiority of the proposed SR-PSO-MAES algorithm.},
  archive      = {J_ASOC},
  author       = {Rui Wang and Kuangrong Hao and Biao Huang and Xiuli Zhu},
  doi          = {10.1016/j.asoc.2022.109923},
  journal      = {Applied Soft Computing},
  pages        = {109923},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive niching particle swarm optimization with local search for multimodal optimization},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A clustering-based extended genetic algorithm for the
multidepot vehicle routing problem with time windows and
three-dimensional loading constraints. <em>ASOC</em>, <em>133</em>,
109922. (<a href="https://doi.org/10.1016/j.asoc.2022.109922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the multidepot vehicle routing problem with time windows and three-dimensional loading constraints (MDVRPTW-TDLC) is a multiconstraint and combinatorial optimization problem , this study proposes a resource sharing scheme on the basis of customer clustering to achieve a balance of spatial resource spatial distributions, thereby reducing the operating costs in a collaborative multidepot logistics network. Moreover, this study proposes a vehicle compartment partition strategy based on cargo characteristics such as type and sizes to increase vehicle loading rates, after which the proposed MDVRPTW-TDLC is formulated as a bi-objective mixed-integer programming model to minimize the total operating costs while maximizing the average loading rate of vehicles. Subsequently, a two-stage hybrid algorithm combining a three-dimensional (3D) k -harmonic means clustering algorithm and extended nondominated sorting genetic algorithm-II (ENSGA-II) is developed to find the Pareto optimal solutions and solve the MDVRPTW-TDLC optimization model, followed by an introduction of the 3D k -harmonic means clustering algorithm to cluster customers, thereby reducing computational complexity . The proposed ENSGA-II, integrating the improved Clarke–Wright savings algorithm and NSGA-II is then used to obtain the final optimal solutions, followed by the application of a selective exchange mechanism between the clustering algorithm and ENSGA-II to enhance the global and local optimization capability, enabling fast and efficient computation of optimal solutions. Based on our investigations revealed through an algorithm comparison using the multiobjective harmony search algorithm, multiobjective evolution algorithm, multiobjective particle swarm optimization , monarch butterfly optimization, Runge Kutta optimizer, and Harris hawks optimization, ENSGA-II demonstrates better performance in reaching high-quality objective function values. Finally, a real-world case study of Chongqing City, China is employed to verify the efficiency of the proposed model and optimization algorithm , after which a comparison of the optimization results among different vehicle compartment partition strategies demonstrates the efficiency of the proposed method. We also discuss 16 scenarios with and without the vehicle compartment partition strategy with computational results showing an improved vehicle loading rate and reduced logistics operating costs with the adopted vehicle compartment partition strategy. Therefore, our results confirm that the proposed MDVRPTW-TDLC can facilitate the sustainable development of logistics operations , and providing decision support for constructing intelligent logistics systems smart cities.},
  archive      = {J_ASOC},
  author       = {Yong Wang and Yuanhan Wei and Xiuwen Wang and Zheng Wang and Haizhong Wang},
  doi          = {10.1016/j.asoc.2022.109922},
  journal      = {Applied Soft Computing},
  pages        = {109922},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A clustering-based extended genetic algorithm for the multidepot vehicle routing problem with time windows and three-dimensional loading constraints},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based integrated framework for stock price
movement prediction. <em>ASOC</em>, <em>133</em>, 109921. (<a
href="https://doi.org/10.1016/j.asoc.2022.109921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock market prediction is a very important problem in the economics field. With the development of machine learning , more and more algorithms are applied in the stock market to predict the stock price movement. However, stock market prediction is regarded as a challenging task for the noise and volatility of stock market data. Therefore, in this paper, a novel hybrid model SA-DLSTM is proposed to predict stock market and simulation trading by combine a emotion enhanced convolutional neural network (ECNN), the denoising autoencoder (DAE) models, and long short-term memory model (LSTM). Firstly, user-generated comments on Internet were used as a complement to stock market data, and ECNN was applied to extract the sentiment representation. Secondly, we extract the key features of stock market data by DAE, which can improve the prediction accuracy. Thirdly, we take the timeliness of emotion on stock market into consideration and construct more reliable and realistic sentiment indexes. Finally, the key features of stock data and sentiment indexes are fed into LSTM to make stock market prediction. Experiment results show that the prediction accuracy of SA-DLSTM are superior to other compared models. Meanwhile, SA-DLSTM has a good performance both in return and risk. It can help investors make wise decisions.},
  archive      = {J_ASOC},
  author       = {Yanli Zhao and Guang Yang},
  doi          = {10.1016/j.asoc.2022.109921},
  journal      = {Applied Soft Computing},
  pages        = {109921},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning-based integrated framework for stock price movement prediction},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). A multi-objective chicken swarm optimization algorithm
based on dual external archive with various elites. <em>ASOC</em>,
<em>133</em>, 109920. (<a
href="https://doi.org/10.1016/j.asoc.2022.109920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimization problems (MOPs) that widely exist in real world concern all optimal solutions compromised among multiple objectives. Chicken swarm optimization algorithm derived from emergent behaviors of organisms provides an effective way for handling MOPs. To speed up convergence and improve uniformity of Pareto-optimal solutions, a multi-objective chicken swarm optimization algorithm based on dual external archives and boundary learning strategy (MOCSO-DABL) is proposed in this paper. Dual external archives are employed to distinguish and choose two types of elite solutions, with the purpose of more effectively guiding individual evolution. A boundary learning strategy guides the chickens to learn from boundary individuals in the later stage of evolution. Moreover, fast non-dominated sorting is adopted to establish the hierarchical social structure of a chicken population, and learning strategies of roosters, hens and chicks are improved to meet the requirements of MOPs. Experimental results on 14 benchmark functions show that the proposed MOCSO-DABL outperforms other five state-of-the-art algorithms significantly.},
  archive      = {J_ASOC},
  author       = {Zhenwu Wang and Wenteng Zhang and Yinan Guo and Mengjie Han and Benting Wan and Shangchao Liang},
  doi          = {10.1016/j.asoc.2022.109920},
  journal      = {Applied Soft Computing},
  pages        = {109920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective chicken swarm optimization algorithm based on dual external archive with various elites},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring deep feature-blending capabilities to assist
glaucoma screening. <em>ASOC</em>, <em>133</em>, 109918. (<a
href="https://doi.org/10.1016/j.asoc.2022.109918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last three decades, computer vision has had a vital role in the healthcare sector by providing soft computing-based robust and intelligent diagnostic solutions. Glaucoma is a critical ophthalmic disease that can trigger irreversible loss of vision. The number of patients with glaucoma is increasing dramatically worldwide. Manual ophthalmic assessment of glaucoma detection is a tedious, error-prone, time-consuming, and subjective task. Therefore, computer-assisted automatic glaucoma diagnosis methods are required to strengthen existing diagnostic methods with their robust performance. Optic disc (OD) and optic cup (OC) segmentation have a key role in glaucoma detection. Accurate segmentation of the OD and OC provides valuable computational and clinical details that can substantially assist in the glaucoma screening process. Retinal fundus images have extensive variations in terms of size, shape, pixel intensity values, and background effects that make segmentation challenging. To mitigate these challenges, we developed two novel networks for accurate OD and OC segmentation. An efficient shallow segmentation network (ESS-Net) is the base network whereas a feature-blending-based shallow segmentation network (FBSS-Net) is the final network of this work. ESS-Net is a shallow architecture with a maximum-depth semantic preservation block for accurate segmentation, while FBSS-Net uses internal and external feature blending to improve overall segmentation performance . To confirm their effectiveness, we evaluated both networks using four publicly available datasets; REFUGE, Drions-DB, Drishti-GS, and Rim-One-r3. The proposed methods exhibited excellent segmentation performance, requiring a small number of trainable parameters (3.02 million parameters).},
  archive      = {J_ASOC},
  author       = {Adnan Haider and Muhammad Arsalan and Chanhum Park and Haseeb Sultan and Kang Ryoung Park},
  doi          = {10.1016/j.asoc.2022.109918},
  journal      = {Applied Soft Computing},
  pages        = {109918},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exploring deep feature-blending capabilities to assist glaucoma screening},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic chaotic gold-panning optimizer and its typical
engineering applications. <em>ASOC</em>, <em>133</em>, 109917. (<a
href="https://doi.org/10.1016/j.asoc.2022.109917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm intelligence algorithms are one of the key technologies in solving optimization problems for practical engineering applications , such as mechanical structure design, image analysis, process flow design, etc. Higher accuracy and efficiency mean better comprehensive performance in the practical engineering system based on optimization methods. Gold-Panning Algorithm is one of the swarm intelligence algorithms proposed in 2021, for solving image segmentation problems. However, the self decision-making mechanism based on multi-agent information interaction introduced in it weakens its convergence ability, resulting in its ability to exploit potential solutions being limited. Hence, chaotic maps were introduced to improve the optimization capacity and efficiency, which can provide a more reliable and effective ability to explore and exploit potential optimal solutions in different iteration stages. Moreover, a dynamic selection strategy is utilized to choose the better step-size iteration scheme between the Gaussian distribution and Levy flight. It can further strengthen the exploitation capability of the Gold-Panning Algorithm, reducing the possibility of premature convergence. Based on CEC’2020 benchmark functions , Dynamic Chaotic Gold-Panning Optimizer is compared with the other meta-heuristic algorithms to evaluate its performance and the results shows strong competitiveness in both robustness and accuracy for solving optimization problems . Then, based on the proposed optimizer, its binary variant is applied in feature selection. Besides, combined with the bilateral filtering and threshold segmentation model , the image blind denoising and image segmentation optimization schemes are proposed respectively. Simulation results indicate it presents an excellent comprehensive performance in solving the corresponding engineering task.},
  archive      = {J_ASOC},
  author       = {Dong Wei and Houzhe Wang and Jianbo Dai and Jinheng Gu and Chao Tan and Haifeng Yan and Lei Si},
  doi          = {10.1016/j.asoc.2022.109917},
  journal      = {Applied Soft Computing},
  pages        = {109917},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic chaotic gold-panning optimizer and its typical engineering applications},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective search for gender-fair and semantically
correct word embeddings. <em>ASOC</em>, <em>133</em>, 109916. (<a
href="https://doi.org/10.1016/j.asoc.2022.109916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness is a crucial non-functional requirement of modern software systems that rely on the use of Artificial Intelligence (AI) to make decisions regarding our daily lives in application domains such as justice, healthcare and education. In fact, these algorithms can exhibit unwanted discriminatory behaviours that create unfair outcomes when the software is used, such as giving privilege to one group of users over another (e.g., males vs. females). Mitigating algorithmic bias during the development life cycle of AI-enabled software is crucial given that any bias in these algorithms is inherited by the software systems using them. However, previous work has shown that mitigating bias can impact the performance of such systems. Therefore, we propose herein a novel use of soft computing for improving AI-enabled software fairness. Specifically, we exploit multi-objective search, as opposed to previous work optimising fairness only, to strike an optimal balance between reducing gender bias and improving semantic correctness of word embedding models, which are at the core of many AI-enabled systems. To assess the effectiveness of our proposal, we carry out a thorough empirical study based on the most recent best practice for the evaluation of search-based approaches and AI-enabled software. We explore seven different search-based approaches, and benchmark them against both baseline and state-of-the-art approaches applied to a popular and widely used word embedding model, namely Word2Vec . Our results show that multi-objective search outperforms single-objective search, and generates word embeddings that are strictly better than the original ones in both objectives, bias and semantic correctness, for all investigated cases. Additionally, our approach generates word embeddings of higher semantic correctness than those generated by using state-of-the-art techniques in all cases, while also achieving a higher degree of fairness in 67\% of the cases. These findings show the feasibility and effectiveness of multi-objective search as a tool for engineers to incorporate fair and accurate word embedding models in their AI-enabled systems.},
  archive      = {J_ASOC},
  author       = {Max Hort and Rebecca Moussa and Federica Sarro},
  doi          = {10.1016/j.asoc.2022.109916},
  journal      = {Applied Soft Computing},
  pages        = {109916},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective search for gender-fair and semantically correct word embeddings},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-class classification model with parametrized target
outputs for randomized-based feedforward neural networks. <em>ASOC</em>,
<em>133</em>, 109914. (<a
href="https://doi.org/10.1016/j.asoc.2022.109914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized-based Feedforward Neural Networks approach regression and classification (binary and multi-class) problems by minimizing the same optimization problem . Specifically, the model parameters are determined through the ridge regression estimator of the patterns projected in the hidden layer space (randomly generated in its neural network version) for models without direct links and the patterns projected in the hidden layer space along with the original input data for models with direct links. The targets are encoded for the multi-class classification problem according to the 1-of- J J encoding ( J J the number of classes), which implies that the model parameters are estimated to project all the patterns belonging to its corresponding class to one and the remaining to zero. This approach has several drawbacks, which motivated us to propose an alternative optimization model for the framework. In the proposed optimization model, model parameters are estimated for each class so that their patterns are projected to a reference point (also optimized during the process), whereas the remaining patterns (not belonging to that class) are projected as far away as possible from the reference point. The final problem is finally presented as a generalized eigenvalue problem . Four models are then presented: the neural network version of the algorithm and its corresponding kernel version for the neural networks models with and without direct links. In addition, the optimization model has also been implemented in randomization-based multi-layer or deep neural networks . The empirical results obtained by the proposed models were compared to those reported by state-of-the-art models in the correct classification rate and a separability index (which measures the degree of separability in projection terms per class of the patterns belonging to the class of the others). The proposed methods show very competitive performance in the separability index and prediction accuracy compared to the neural networks version of the comparison methods (with and without direct links). Remarkably, the model provides significantly superior performance in deep models with direct links compared to its deep model counterpart.},
  archive      = {J_ASOC},
  author       = {Antonio Manuel Durán-Rosal and Aggeo Durán-Fernández and Francisco Fernández-Navarro and Mariano Carbonero-Ruz},
  doi          = {10.1016/j.asoc.2022.109914},
  journal      = {Applied Soft Computing},
  pages        = {109914},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-class classification model with parametrized target outputs for randomized-based feedforward neural networks},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine-based mosquito taxonomy with a lightweight
network-fused efficient dual ConvNet with residual learning and
knowledge distillation. <em>ASOC</em>, <em>133</em>, 109913. (<a
href="https://doi.org/10.1016/j.asoc.2022.109913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taxonomy plays a vital role in identifying different mosquito species. Studies show that though not all mosquitoes threaten humanity, specific species exist in less fortunate areas that immensely disrupt people’s lives. As identified, researchers discovered that deficiency in identifying between a vector mosquito that carries a lethal disease apart from non-vectors led to people becoming susceptible. Recently, studies proposed automating these mosquitoes’ classification so that people who lack awareness can soon obtain assistance from an intelligent system. However, most solutions still require expensive computations and specialized resources to operate and even reproduce, making the most vulnerable areas or groups of people unable to benefit from them. Therefore, this work solves this problem with a lightweight model built by compressing, duplicating, and fusing a Deep Convolutional Neural Network model (DCNN), adding a modified residual block, and training it through Knowledge Distillation (KD). Upon assessment, results yielded significant performance improvements, as the proposed model reached 99.22\% overall accuracy that only requires 0.33 GFLOPs to operate and consumes only 437 KB of disk space. In addition, results also showed the benefits of KD in saliency. Compared to most studies, previous and current state-of-the-art DCNNs, this work shows promising viability to solve the problem pragmatically.},
  archive      = {J_ASOC},
  author       = {Francis Jesmar P. Montalbo},
  doi          = {10.1016/j.asoc.2022.109913},
  journal      = {Applied Soft Computing},
  pages        = {109913},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine-based mosquito taxonomy with a lightweight network-fused efficient dual ConvNet with residual learning and knowledge distillation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A space variable-scale scheduling method for digital
vehicle-to-grid platform under distributed electric energy storage.
<em>ASOC</em>, <em>133</em>, 109911. (<a
href="https://doi.org/10.1016/j.asoc.2022.109911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of digital technology in the intelligent transportation field, the vehicle-to-grid (V2G) pattern, which utilizes electric vehicles (EVs) as the distributed storage resources of electric energy, has won increasingly more attention from authorities and decision-makers to promote the sustainable development of regenerative clean energy. This paper aims to study the interaction and integration problem between EVs and grid for the digital V2G platform, so as to support load aggregators making scheduling decisions. According to the variable-scale data analysis theory, a space–time scale space model is established to describe the demand response behavioral feature of EV users, and the lightning-Scale Transformation method (LST) is also proposed. Compared to the original unidirectional scale transformation modes, the LST is capable of adjusting the space and time observation scale collaboratively. A vehicle-to-grid scheduling method based on the lightning-Scale Transformation (V2G-LST) is put forward, in order to make scheduling plans by considering EV users’ behavioral feature and charger resource constraints. We collect the real dataset from 863 Beijing charging stations using the APIs of Amap, as well as the investigation data of the State Grid Corporation of China to verify the efficacy of our proposed method. Comparative experimental results verify that the V2G-LST could obtain the lower dispatch cost and higher EV charger usage rate than the maximum dispatchable quantity first method EVC-MDQ under both charging and discharging scenarios.},
  archive      = {J_ASOC},
  author       = {Ai Wang and Xuedong Gao and Mincong Tang},
  doi          = {10.1016/j.asoc.2022.109911},
  journal      = {Applied Soft Computing},
  pages        = {109911},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A space variable-scale scheduling method for digital vehicle-to-grid platform under distributed electric energy storage},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing the barriers of digitally sustainable
transportation system for persons with disabilities using fermatean
fuzzy double normalization-based multiple aggregation method.
<em>ASOC</em>, <em>133</em>, 109910. (<a
href="https://doi.org/10.1016/j.asoc.2022.109910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital technology can guarantee safety and security improvement in public transport together with increasing affordability for disabled travelers. This study aims to propose a hybrid multi-criteria decision-making method for prioritizing the digital technologies under sustainable transportation of disabled travelers under Fermatean fuzzy set context. For this purpose, main barriers are identified to implement the digital transportation under sustainable transportation for person with disabilities (PWDs). In the proposed method, firstly a novel weighting model is proposed to find the weights of the decision makers . Second, some Fermatean fuzzy interaction aggregation operators (AOs) are introduced to aggregate the decision information. Third, a combined criteria weight-determining model is developed based on Fermatean fuzzy criteria Importance through inter-criteria correlation and stepwise weight assessment ratio analysis models, which can overcome the insufficiencies of individual objective or subjective model. Based on the proposed AOs and weighting models, a hybrid double normalization-based multi-aggregation approach is proposed from Fermatean fuzzy perspective. Finally, a case study of digital technologies assessment for PWDs with sensitivity and comparative analyses are executed to illustrate the potentiality and practicality of the present approach. According to the findings, the most significant barriers to implement digital technology under sustainable transportation for PWDs are administrative restrictions (0.0819), inappropriate driver attitude (0.0791), and unable to secure wheelchair on vehicle (0.776), respectively. The evaluation results also show that the option artificial intelligence and machine learning has the highest overall utility degree (0.6701) among a set of five digital technologies. Hence, the findings of the study indicate that the present method can be used by public or private organizations to improve the transportation services for PWDs.},
  archive      = {J_ASOC},
  author       = {Ibrahim M. Hezam and Arunodaya Raj Mishra and Pratibha Rani and Ahmad Alshamrani},
  doi          = {10.1016/j.asoc.2022.109910},
  journal      = {Applied Soft Computing},
  pages        = {109910},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessing the barriers of digitally sustainable transportation system for persons with disabilities using fermatean fuzzy double normalization-based multiple aggregation method},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting domestic waste generation during successive
COVID-19 lockdowns by bidirectional LSTM super learner neural network.
<em>ASOC</em>, <em>133</em>, 109908. (<a
href="https://doi.org/10.1016/j.asoc.2022.109908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of domestic waste generation is a challenging task for municipalities to implement sustainable waste management strategies. In the present study, domestic waste generation in the Kingdom of Bahrain, representing a Small Island Developing State (SIDS) case study, has been investigated during successive COVID-19 lockdowns due to the pandemic in 2020. Temporal trends of daily domestic waste generation between 2019 and 2020 and their statistical analyses exhibited remarkable variations highlighting the impact of consecutive COVID-19 lockdowns on domestic waste generation. Machine learning has great potential for predicting solid waste generation rates, but only a few studies utilized deep learning approaches. The state-of-the-art Bidirectional Long Short-Term Memory (BiLSTM) network model as a deep learning method is applied to forecast daily domestic waste data in 2020. Bayesian optimization algorithm (BOA) was hybridized with BiLSTM to generate a super learner approach. The performance of the BOA-BiLSTM super learner model was further compared with the statistical ARIMA model. Performance indicators of the developed models using ARIMA and BiLSTM showed that the latter yielded superior performance for short-term forecasts of domestic waste generation. The MAE , RMSE , MAPE, and R 2 were 47.38, 60.73, 256.43, and 0.46, respectively, for the ARIMA model, compared to 3.67, 12.57, 0.24, and 0.96, respectively, for the BiLSTM model. Additionally, the relative errors for the BiLSTM model were lower than those of the ARIMA model. This study highlights that the BiLSTM can be a reliable forecasting tool for solid waste management policymakers during public health emergencies .},
  archive      = {J_ASOC},
  author       = {Majeed S. Jassim and Gulnur Coskuner and Nahid Sultana and S.M. Zakir Hossain},
  doi          = {10.1016/j.asoc.2022.109908},
  journal      = {Applied Soft Computing},
  pages        = {109908},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting domestic waste generation during successive COVID-19 lockdowns by bidirectional LSTM super learner neural network},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of covid-19 and other pneumonia cases from CT and
x-ray chest images using deep learning based on feature reuse residual
block and depthwise dilated convolutions neural network. <em>ASOC</em>,
<em>133</em>, 109906. (<a
href="https://doi.org/10.1016/j.asoc.2022.109906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covid-19 has become a worldwide epidemic which has caused the death of millions in a very short time. This disease, which is transmitted rapidly, has mutated and different variations have emerged. Early diagnosis is important to prevent the spread of this disease. In this study, a new deep learning-based architecture is proposed for rapid detection of Covid-19 and other symptoms using CT and X-ray chest images. This method, called CovidDWNet, is based on a structure based on feature reuse residual block (FRB) and depthwise dilated convolutions (DDC) units. The FRB and DDC units efficiently acquired various features in the chest scan images and it was seen that the proposed architecture significantly improved its performance. In addition, the feature maps obtained with the CovidDWNet architecture were estimated with the Gradient boosting (GB) algorithm. With the CovidDWNet+GB architecture, which is a combination of CovidDWNet and GB, a performance increase of approximately 7\% in CT images and between 3\% and 4\% in X-ray images has been achieved. The CovidDWNet+GB architecture achieved the highest success compared to other architectures, with 99.84\% and 100\% accuracy rates, respectively, on different datasets containing binary class (Covid-19 and Normal) CT images. Similarly, the proposed architecture showed the highest success with 96.81\% accuracy in multi-class (Covid-19, Lung Opacity, Normal and Viral Pneumonia) X-ray images and 96.32\% accuracy in the dataset containing X-ray and CT images. When the time to predict the disease in CT or X-ray images is examined, it is possible to say that it has a high speed because the CovidDWNet+GB method predicts thousands of images within seconds.},
  archive      = {J_ASOC},
  author       = {Gaffari Celik},
  doi          = {10.1016/j.asoc.2022.109906},
  journal      = {Applied Soft Computing},
  pages        = {109906},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detection of covid-19 and other pneumonia cases from CT and X-ray chest images using deep learning based on feature reuse residual block and depthwise dilated convolutions neural network},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale random walk driven adaptive graph neural network
with dual-head neighboring node attention for CT segmentation.
<em>ASOC</em>, <em>133</em>, 109905. (<a
href="https://doi.org/10.1016/j.asoc.2022.109905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting objects with indistinct boundaries and large variations from CT volumes is a challenging issue due to overlapping intensity distributions from neighboring tissues or long-distance semantic relations . We propose a multi-scale random walk (RW) driven graph neural network (GNN) to address this issue. A graph is first initialized to represent image regions and deep semantic features from the segmentation encoder by graph nodes and attributes. We then propose a multi-scale graph reasoning model where for each scale, graph node attribute embedding is obtained by an adaptive GNN with dual-head neighboring node attention, while graph topology is evolved by RW. The neighboring-node attention mechanism is designed to learn and incorporate the importance and influence of neighboring nodes on their connected nodes. Random walking to multi-order neighbors enhance the contextual information formulation and diffusion along graph edges. Finally, multi-scale knowledge learnt from graphs is adaptively fused by a new graph-wise attention fusion module before reshaping and feeding to the segmentation decoder. We evaluate the contributions of major innovations by ablation studies, comparison with other state-of-the-art models on public kidney and tumor segmentation dataset. The generalization ability of our model is validated by different segmentation backbones. Experimental results show that the novel multi-scale adaptive graph reasoning architecture and RW-enhanced GNN model improved the segmentation of objects from adjacent tissues.},
  archive      = {J_ASOC},
  author       = {Ping Xuan and Xixi Wu and Hui Cui and Qiangguo Jin and Linlin Wang and Tiangang Zhang and Toshiya Nakaguchi and Henry B.L. Duh},
  doi          = {10.1016/j.asoc.2022.109905},
  journal      = {Applied Soft Computing},
  pages        = {109905},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale random walk driven adaptive graph neural network with dual-head neighboring node attention for CT segmentation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). DQN-enabled content caching and quantum ant colony-based
computation offloading in MEC. <em>ASOC</em>, <em>133</em>, 109900. (<a
href="https://doi.org/10.1016/j.asoc.2022.109900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation-intensive and time-delay-sensitive tasks show explosive growth, which makes mobile user devices put forward higher requirements for such tasks. Mobile user devices are difficult to meet the task requirements due to limited computing capacity. To handle this situation, a mobile edge computing system requires to be constructed. Nevertheless, in the mobile edge computing system, if no popular content is cached in the edge server, users will repeatedly send requests to a remote cloud data center when requesting content, which will cause delay and backhaul link load. To handle this situation, this article plans to put forward a content caching policy. In this policy, the caching benefit in cache replacement, delay in transmission and backhaul link load is taken into account in the content cache. Then DQN is used to handle the problem to obtain the optimal content caching scheme. In addition, in the mobile edge computing environment, the computing capacity of mobile user devices is limited, so task data needs to be unloaded to the edge server for local or remote execution to improve the computing capacity. To handle the above problems, this article plans to put forward a quantum ant colony based computing offloading strategy. This strategy takes into account three aspects: delay, energy consumption and server cost. Quantum ant colony algorithm is used to handle the situation. Experimental results indicate that the content caching strategy based on DQN can effectively improve cache hit percentage, and cut down transmission delay and return link load. Quantum ant colony based computational offloading strategies can increase task completion rates , and decrease task completion delays, equipment energy consumption and total system cost.},
  archive      = {J_ASOC},
  author       = {Chunlin Li and Yong Zhang and Youlong Luo},
  doi          = {10.1016/j.asoc.2022.109900},
  journal      = {Applied Soft Computing},
  pages        = {109900},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DQN-enabled content caching and quantum ant colony-based computation offloading in MEC},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revised solution technique for a bi-level
location-inventory-routing problem under uncertainty of demand and
perishability of products. <em>ASOC</em>, <em>133</em>, 109899. (<a
href="https://doi.org/10.1016/j.asoc.2022.109899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bi-level programming is an efficient tool to tackle decentralized decision-making processes in supply chains with upper level (i.e., leader) and lower level (i.e., follower). The leader makes the first decision while the follower makes the second decision. In this research, a bi-level programming formulation for the problem of location-inventory-routing in a two-echelon supply chain, including a number of central warehouses in the first echelon and retailers in the second echelon with perishable products under uncertain demand, is proposed. The total operational costs at both levels are minimized considering capacity constraints. Due to the uncertain nature of the problem, a scenario-based programming is utilized. The economic condition or unforeseen events such as COVID-19 or Russia-Ukraine war can be good examples for uncertainty sources in today’s world. The model determines the optimal locations of warehouses, the routes between warehouses and retailers, number of received shipments and the amount of inventory held at each retailer. A revised solution method is designed by using multi-choice goal programming for solving the problem. The given revised method attempts to minimize the deviations of each decision maker’s solution from its ideal value assuming that the upper level is satisfied higher than the lower level. Base on some numerical analysis, the proposed solution technique is more sensitive to the upper bounds of the goals rather than the lower bounds.},
  archive      = {J_ASOC},
  author       = {Fezzeh Partovi and Mehdi Seifbarghy and Maryam Esmaeili},
  doi          = {10.1016/j.asoc.2022.109899},
  journal      = {Applied Soft Computing},
  pages        = {109899},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Revised solution technique for a bi-level location-inventory-routing problem under uncertainty of demand and perishability of products},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TomFusioNet: A tomato crop analysis framework for mobile
applications using the multi-objective optimization based late fusion of
deep models and background elimination. <em>ASOC</em>, <em>133</em>,
109898. (<a href="https://doi.org/10.1016/j.asoc.2022.109898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop disease is a critical concern for the farmers which needs to be addressed to mitigate production loss. Multifarious frameworks have been proposed for the rapid recognition of tomato crop diseases, however, their practical deployment is a daunting task. The existing models suffer from performance issues such as overfitting and gradient vanishing. To alleviate these issues, this research proposes an end-to-end tomato crop analysis framework, TomFusioNet , for mobile applications. For feature extraction, the late-fusion technique is leveraged by aggregating the results of modified cross-domain transfer learning models using the maximum likelihood prediction strategy. Multi-layer Perceptron models are utilized as meta-learners. TomFusioNet’s pipeline comprises two modules, namely, DeepRec and DeepPred. DeepRec provides preliminary disease recognition results while DeepPred further identifies the type of illness in crop. Logic gate mapping is employed to reduce unnecessary wastage of mobile computation. The hyperparameters of DeepPred are tuned using the multi-objective optimization based non-dominated sorting genetic algorithm II for performance enhancement. We highlighted the significance of feature relevancy ; therefore, a hue, saturation and value (HSV) color model-based background elimination technique is also proposed. TomFusioNet can be incorporated in a smartphone app , conceptualized for remote crop monitoring. The proposed DeepRec and DeepPred models achieved an average accuracy of 99.93\% and 98.32\%, respectively. According to experimental results, TomFusioNet, which is built on NSGA-II, beats state-of-the-art models with a convergence loss of 0.021 and an AUC value of 99.10\%. The proposed app framework’s functioning does not require any human in loop. Furthermore, the latency of framework in real-time is less than 2 s, hence, it is effective for rapid tomato crop analysis.},
  archive      = {J_ASOC},
  author       = {Harshit Kaushik and Anvi Khanna and Dilbag Singh and Manjit Kaur and Heung-No Lee},
  doi          = {10.1016/j.asoc.2022.109898},
  journal      = {Applied Soft Computing},
  pages        = {109898},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TomFusioNet: A tomato crop analysis framework for mobile applications using the multi-objective optimization based late fusion of deep models and background elimination},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid decision making approach for new service
development process of renewable energy investment. <em>ASOC</em>,
<em>133</em>, 109897. (<a
href="https://doi.org/10.1016/j.asoc.2022.109897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims at evaluating new service development (NSD) process of renewable energy investments. A novel fuzzy hybrid multi-criteria decision-making model is then introduced. Firstly, selected criteria are weighted with a multi stepwise weight assessment ratio analysis (M-SWARA) methodology based on bipolar q-rung orthopair fuzzy sets (q-ROFSs) and golden cut. Secondly, balanced scorecard (BSC)-based project network for the NSD process of renewable energy investments is constructed. These alternatives are ranked by using the elimination and choice translating reality (ELECTRE) approach with bipolar q-ROFSs and golden cut. Afterwards, the Project Evaluation and Review Technique (PERT) diagram is developed by defining immediate predecessors based on the ranking results. Consequently, different paths are created to understand effective ways to generate NSD process. Our findings will show the crucial processes for the NSD process of renewable energy investments, and the PERT analysis will provide optimal paths to increase the performance of NSD process of the renewable energies. It is concluded that analysis is the most critical NSD process for clean energy investment projects. The findings indicate that the shortest path is defined as Path 2 with 48.6\% of the total importance degree. On the other hand, the longest path is determined as Path 1 with 100\%. Path 4 has the weakest importance with 65.2\% in comparison to Path 3 with 83.4\% with respect to the comparison same activity numbers. It is strongly recommended that both technical and financial aspects should be evaluated with necessary research while generating new products. This issue can have a positive contribution to the early solution of any potential problems.},
  archive      = {J_ASOC},
  author       = {Luis Martínez and Hasan Dinçer and Serhat Yüksel},
  doi          = {10.1016/j.asoc.2022.109897},
  journal      = {Applied Soft Computing},
  pages        = {109897},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid decision making approach for new service development process of renewable energy investment},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A time-aware self-attention based neural network model for
sequential recommendation. <em>ASOC</em>, <em>133</em>, 109894. (<a
href="https://doi.org/10.1016/j.asoc.2022.109894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation is one of the hot research topics in recent years. Various sequential recommendation models have been proposed, of which Self-Attention (SA)-based models are shown to have state-of-the-art performance. However, most of the existing SA-based sequential recommendation models do not make use of temporal information, i.e., timestamps of user–item interactions, except for an initial attempt (Li et al., 2020). In this paper, we propose a Time-Aware Transformer for Sequential Recommendation (TAT4SRec), an SA-based neural network model which utilizes the temporal information and captures users’ preferences more precisely. TAT4SRec has two salient features : (1) TAT4SRec utilizes an encoder–decoder structure to model timestamps and interacted items separately and this structure appears to be a better way of making use of the temporal information. (2) in the proposed TAT4SRec, two different embedding modules are designed to transform continuous data (timestamps) and discrete data (item IDs) into embedding matrices respectively. Specifically, we propose a window function-based embedding module to preserve the continuous dependency contained in similar timestamps. Finally, extensive experiments demonstrate the effectiveness of the proposed TAT4SRec over various state-of-the-art MC/RNN/SA-based sequential recommendation models under several widely-used metrics. Furthermore, experiments are also performed to show the rationality of the different proposed structures and demonstrate the computation efficiency of TAT4SRec. The promising experimental results make it possible to apply TAT4SRec in various online applications.},
  archive      = {J_ASOC},
  author       = {Yihu Zhang and Bo Yang and Haodong Liu and Dongsheng Li},
  doi          = {10.1016/j.asoc.2022.109894},
  journal      = {Applied Soft Computing},
  pages        = {109894},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A time-aware self-attention based neural network model for sequential recommendation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured prediction of sparse dependent variables for
traffic state estimation in large-scale networks. <em>ASOC</em>,
<em>133</em>, 109893. (<a
href="https://doi.org/10.1016/j.asoc.2022.109893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, one of the biggest challenges in modern traffic engineering is related to traffic state estimation (TSE). Although many machine learning and domain models can be used for TSE, they do not consider the sparsity and spatial dependence of traffic state variables. In this paper, we propose a hybrid soft computing model of two Gaussian conditional random field (GCRF) models for the inference of traffic speed, which is a relevant variable for TSE and travel information systems. The proposed model can infer the traffic state variables in large-scale networks whose nodes are geographically dispersed. Moreover, by combining a Gaussian conditional random field binary classification model (GCRFBC), which classifies traffic regimes as free-flow or potentially congested, and a regression GCRF model for the prediction of traffic speed in potentially congested traffic regimes, the model addresses two specifics of the problem: sparsity in traffic data, and the fact that observations are not independent. The proposed model was tested on two large-scale real-world networks in Serbia, namely an arterial E70-E75 335 km long highway stretch and the major ski resort Kopaonik with 55 km of ski slopes. In addition, the proposed model showed better prediction performance than several other unstructured and structured models.},
  archive      = {J_ASOC},
  author       = {Andrija Petrović and Sandro Radovanović and Mladen Nikolić and Boris Delibašić and Miloš Jovanović},
  doi          = {10.1016/j.asoc.2022.109893},
  journal      = {Applied Soft Computing},
  pages        = {109893},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Structured prediction of sparse dependent variables for traffic state estimation in large-scale networks},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantile-guided multi-strategy algorithm for dynamic
multiobjective optimization. <em>ASOC</em>, <em>133</em>, 109892. (<a
href="https://doi.org/10.1016/j.asoc.2022.109892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multiobjective optimization problems (DMOPs) require that the algorithm is capable of tracking the position of Pareto-optimal solution (PS) in a rapidly changing environment. Prediction-based and memory-based methods currently gain much attention for their good tracking ability. However, an elaborate predictor may not be suitable for different problems and the improper reuse of the memory may lower the quality of solution. To overcome these limitations, a quantile-guided multi-strategy algorithm (QMA) for dynamic multiobjective optimization is proposed in this paper. Quantile is always employed to represent characteristics of data because of its robustness to the outliers. In QMA, historical quantile information of decision space is used to find the new position of quantile . Then, the solution set is expanded based on the new quantile. Moreover, historical search information is used to assist in the evolution of population, which is achieved through selecting the mapping matrix as a judgment of retrieval. Meanwhile, the quantile-guided environmental change degree detector is employed to determine the number of retained and the randomly generated individuals when there is no similarity. Experimental results carried out on various DMOPs demonstrate that QMA is highly competitive compared with some state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Anran Cao and Hao Sun and Ziyu Hu and Pengfei Chen and Zhiwei Zhao},
  doi          = {10.1016/j.asoc.2022.109892},
  journal      = {Applied Soft Computing},
  pages        = {109892},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantile-guided multi-strategy algorithm for dynamic multiobjective optimization},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale group classification decision making method and
its application with trust–interest dual factors in social network.
<em>ASOC</em>, <em>133</em>, 109890. (<a
href="https://doi.org/10.1016/j.asoc.2022.109890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide application of computer technology promotes the development of large-scale group decision making. However, with the increase of the number of people involved in decision-making, the complexity of group decision making process is also expanded. In this paper, we propose a large-scale group classification decision making method with trust–interest dual factors in social network, which reduce the dimension of large group based on the deep mining of social network between decision makers , and aggregate the fuzzy decision information in the form of cloud model to get the decision results. Specifically, we define the elastic modularity and the utility function to describe the trust-characteristic and interest-characteristic of subgroups, divide the experts into high-trust communities and high-interest coalitions with the corresponding algorithm respectively. We set the preference of expert in subgroup as cloud droplet , and the preference of the subgroup is gathered in the form of cloud model. The merged subgroup of isolated decision maker is determined according to the preference membership matrix between the preference of isolated decision maker and each subgroup. The feasibility of the method is verified by the combinatorial technology selection for ultra-low emission transformation of large coal-fired power plants.},
  archive      = {J_ASOC},
  author       = {Xinlei Sun and Jianjun Zhu},
  doi          = {10.1016/j.asoc.2022.109890},
  journal      = {Applied Soft Computing},
  pages        = {109890},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Large-scale group classification decision making method and its application with trust–interest dual factors in social network},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Direct derivation scheme of DT-RNN algorithm for discrete
time-variant matrix pseudo-inversion with application to robotic
manipulator. <em>ASOC</em>, <em>133</em>, 109861. (<a
href="https://doi.org/10.1016/j.asoc.2022.109861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The improvement of recurrent neural network (RNN) algorithms is one of targets of many researchers, and these algorithms are widely used to solve time-variant problems in a variety of domains. A novel direct derivation scheme of discrete time-variant RNN (DT-RNN) algorithm for addressing discrete time-variant matrix pseudo-inversion is discussed in this paper. To be more specific, firstly, a DT-RNN algorithm mathematically founded on the second-order Taylor expansion is proposed for dealing with discrete time-variant matrix pseudo-inversion, and it does not require the theoretical support of continuous time-variant RNN (CT-RNN) algorithm. Secondly, the results of theoretical analyses of the proposed DT-RNN algorithm are also presented in this paper. These results demonstrate that the novel DT-RNN algorithm has remarkable computing performance. The efficiency and applicability of the DT-RNN algorithm have been verified through one numerical experiment and two robotic manipulator experiments.},
  archive      = {J_ASOC},
  author       = {Yang Shi and Wenhan Zhao and Shuai Li and Bin Li and Xiaobing Sun},
  doi          = {10.1016/j.asoc.2022.109861},
  journal      = {Applied Soft Computing},
  pages        = {109861},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Direct derivation scheme of DT-RNN algorithm for discrete time-variant matrix pseudo-inversion with application to robotic manipulator},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive scenario subset selection for worst-case
optimization and its application to well placement optimization.
<em>ASOC</em>, <em>133</em>, 109842. (<a
href="https://doi.org/10.1016/j.asoc.2022.109842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we consider simulation-based worst-case optimization problems with continuous design variables and a finite scenario set. To reduce the number of simulations required and increase the number of restarts for better local optimum solutions, we propose a new approach referred to as adaptive scenario subset selection ( AS3 ). The proposed approach subsamples a scenario subset as a support to construct the worst-case objective function in a given neighborhood, and we introduce such a scenario subset. Moreover, we develop a new optimization algorithm by combining AS3 and the covariance matrix adaptation evolution strategy (CMA-ES), denoted AS3-CMA-ES . At each algorithmic iteration, a subset of support scenarios is selected, and CMA-ES attempts to optimize the worst-case objective computed only through a subset of the scenarios. The proposed algorithm reduces the number of simulations required by executing simulations on only a scenario subset, rather than on all scenarios. In numerical experiments, we verified that AS3-CMA-ES is more efficient in terms of the number of simulations than the brute-force approach and a surrogate-assisted approach lq-CMA-ES when the ratio of the number of support scenarios to the total number of scenarios is relatively small. In addition, the usefulness of AS3-CMA-ES was evaluated for well placement optimization for carbon dioxide capture and storage (CCS). In comparison with the brute-force approach and lq-CMA-ES , AS3-CMA-ES was able to find better solutions because of more frequent restarts.},
  archive      = {J_ASOC},
  author       = {Atsuhiro Miyagi and Kazuto Fukuchi and Jun Sakuma and Youhei Akimoto},
  doi          = {10.1016/j.asoc.2022.109842},
  journal      = {Applied Soft Computing},
  pages        = {109842},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive scenario subset selection for worst-case optimization and its application to well placement optimization},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deterministic ship roll forecasting model based on
multi-objective data fusion and multi-layer error correction.
<em>ASOC</em>, <em>132</em>, 109915. (<a
href="https://doi.org/10.1016/j.asoc.2022.109915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing frequency of international shipping and marine resources development activities, ship motion prediction plays an increasingly important role in ensuring the safety of offshore operations . However, in the field of ship motion prediction, the deep feature information of raw high-resolution ship motion data, as well as the predictable components in the initial prediction residuals is usually neglected. In this paper, a deterministic ship roll forecasting model based on multi-objective data fusion and multi-layer error correction is proposed. The proposed model consists of three stages, which are data pre-processing stage, multi-objective data fusion forecasting stage, and multi-layer error correction stage. To verify the stability and validity of the proposed model, an experimental study was conducted using three sets of measured ship roll motion data collected in the South China Sea from 2018 to 2020. Taking the 1-step, 5-step, and 10-step predictions of dataset #1 as an example, the RMSE values of the proposed model are 0.0130°, 0.0612°, and 0.0791°, respectively. Through three analytical experiments and four comparison experiments, it is proved that the proposed model is able to obtain accurate deterministic point forecasts, which can better assist the sailor in decision making.},
  archive      = {J_ASOC},
  author       = {Yunyu Wei and Zezong Chen and Chen Zhao and Xi Chen},
  doi          = {10.1016/j.asoc.2022.109915},
  journal      = {Applied Soft Computing},
  pages        = {109915},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deterministic ship roll forecasting model based on multi-objective data fusion and multi-layer error correction},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature distillation siamese networks for object tracking.
<em>ASOC</em>, <em>132</em>, 109912. (<a
href="https://doi.org/10.1016/j.asoc.2022.109912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Siamese-based trackers have shown remarkable improvement in visual tracking. The general trends are interested in making deeper and more complicated networks to pursue higher accuracy. However, these advances result in cumbersome trackers with respect to size and speed, which hinders the deployment of deep trackers on edge devices. Due to the tracking scenario complexity and temporal coherence, the backbone network of deep trackers emphasizes the target appearance information more. However, the appearance feature is sensitive to parameter variation, making the traditional deep model compression methods hard to compress a deep tracker. To bridge the gap between deep Siamese trackers and practical use, we propose a new feature distillation algorithm suitable for deep trackers in this paper. Firstly, motivated by the concept of divide-and-conquer, we formulate the feature distillation into a stepwise distillation problem and perform distillation on each minimum unit to relieve the hard-to-distill problem of appearance feature. Secondly, we reconstruct the student model into a combination of convolution kernels and a point-wise convolutional layer , which enables the student model to inherit all the parameters of the teacher model during initialization. Finally, we propose a 3-step warm-up training strategy to address the student model’s degradation and structural adaptation problems during training. Extensive experiments on eight benchmarks demonstrate that our proposed method compresses the fully convolutional Siamese Networks (SiamFC) and its variant Siamd and achieves leading tracking performance with only 2.1 MB model size while running at 225 fps.},
  archive      = {J_ASOC},
  author       = {Hanlin Huang and Guixi Liu and Yi Zhang and Ruke Xiong},
  doi          = {10.1016/j.asoc.2022.109912},
  journal      = {Applied Soft Computing},
  pages        = {109912},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature distillation siamese networks for object tracking},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi population-based chaotic differential evolution for
multi-modal and multi-objective optimization problems. <em>ASOC</em>,
<em>132</em>, 109909. (<a
href="https://doi.org/10.1016/j.asoc.2022.109909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) is a simple but powerful evolutionary algorithm used in multiple sciences and engineering disciplines to tackle optimization problems . DE has some disadvantages, such as premature convergence and the low convergence rate that prompts the worst DE execution structure in the constrained environment. The occurrence of these constraints split up the exploration area into viable and un-viable intervals. To overcome the abovementioned issues, we chose to take advantage of the vital characteristics of two mutation strategies: DE/rand/1 and DE/best/2. This research proposes a novel DE variant called Multi-population-based chaotic DE (MPC-DE) to solve multi-model and multi-objective optimization problems . The proposed MPC-DE is divided into two sub-populations with chaotic-based enhanced population initialization approaches, Sinusoidal and Tent map chaotic population initialization. Each sub-population follows the proposed improved mutation strategies based on two-dimensional chaotic maps, i.e., Baker’s map and Arnold’s Cat Map for DE/rand/1 in the first sub-population, and Zaslavskii Map for DE/best/2 in the second sub-population. Finally, the selection criteria are proposed to select the best offspring produced by each sub-population following the mutant vectors generated by the proposed mutation strategies. MPC-DE is evaluated on the dynamic multi-model and multi-objective optimization problems, i.e., benchmark problems for CEC 2017 and CEC 2020, respectively. To verify MPC-DE’s performance, we compare it with the latest DE variants, namely, EFADE, MPEDE, SHADE, EPSDE, L-SHADE, ESMDE, CoDE, and JADE. The proposed MPC-DE is also employed to solve the Economic Load Dispatch Problem (EDP) and reduce fuel costs. We used a 60-unit bus system and a 180-unit bus system to solve EDP and compared it to recent EDP solvers such as DPADE, JADE, EPSDE, SaDE, DE/BBO, DE, MIMO , TLBO, BPSO, CSO, ORCSA, CSA, ORCCRO, BBO, and ED-DE. The empirical results confirmed that MPC-DE outperformed other recent variants for multi-objective optimization problems and EDP.},
  archive      = {J_ASOC},
  author       = {Hafiz Tayyab Rauf and Jiechao Gao and Ahmad Almadhor and Ali Haider and Yu-Dong Zhang and Fadi Al-Turjman},
  doi          = {10.1016/j.asoc.2022.109909},
  journal      = {Applied Soft Computing},
  pages        = {109909},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi population-based chaotic differential evolution for multi-modal and multi-objective optimization problems},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A single-loop reliability-based design optimization using
adaptive differential evolution. <em>ASOC</em>, <em>132</em>, 109907.
(<a href="https://doi.org/10.1016/j.asoc.2022.109907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability-based design optimization (RBDO) is an efficient tool for generating reliable and optimal solution under uncertainty of design variables. The major challenges in solving RBDO problems are generating optimal or near optimal reliable solution and higher computational cost. In this paper, a single-loop RBDO formulation is developed for addressing these challenges that uses shifting vector approach for achieving feasibility for violated constraints or performance functions. The formulation also incorporates target and trial vectors of differential evolution (DE) for guiding the algorithm. DE is also made adaptive by designing a heuristic parameter that controls two mutation operators for both exploration and exploitation of search space. The proposed RBDO method is tested on three mathematical and four engineering examples. The reliability of obtained solutions from RBDO methods are verified using Monte Carlo simulations with sample size of one million. The results of the proposed method are compared with various RBDO methods from the literature and a double-loop based DE method. It is found that the proposed method generates the best reliable solution for all examples.},
  archive      = {J_ASOC},
  author       = {Raktim Biswas and Deepak Sharma},
  doi          = {10.1016/j.asoc.2022.109907},
  journal      = {Applied Soft Computing},
  pages        = {109907},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A single-loop reliability-based design optimization using adaptive differential evolution},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A constrained multi-objective evolutionary algorithm
assisted by an additional objective function. <em>ASOC</em>,
<em>132</em>, 109904. (<a
href="https://doi.org/10.1016/j.asoc.2022.109904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In constrained multi-objective optimization, the degree of constraint violation as an additional objective function has been optimized together with the original M M objective functions for better diversity. However, it still faces the challenge of deeply exploring feasible regions while maintaining the diversity of the population. To this end, this paper proposes a novel constrained multi-objective evolutionary algorithm assisted by an additional objective function, called CMAOO. First, the main population is constructed to optimize an ( M M +1)-objective optimization problem consisting of the original M M objective functions and the degree of constraint violation. Additionally, all the feasible solutions are saved in an external archive . Then, the main population and the external archive are evolved to search the whole space and the feasible regions, respectively. After that, their offspring are combined to separately update the external archive and the main population. Experimental studies are conducted to test the performance of CMAOO with four state-of-the-art algorithms on 34 test problems and a real-world problem. The results demonstrate that CMAOO is competitive to solve constrained multi-objective optimization problems.},
  archive      = {J_ASOC},
  author       = {Yongkuan Yang and Pei-Qiu Huang and Xiangsong Kong and Jing Zhao},
  doi          = {10.1016/j.asoc.2022.109904},
  journal      = {Applied Soft Computing},
  pages        = {109904},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A constrained multi-objective evolutionary algorithm assisted by an additional objective function},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly detection of power battery pack using gated
recurrent units based variational autoencoder. <em>ASOC</em>,
<em>132</em>, 109903. (<a
href="https://doi.org/10.1016/j.asoc.2022.109903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid and accurate detection of battery pack anomalies and timely fault-tolerant control is of great importance to the safe operation of electric vehicles (EVs). The occurrence of battery system fire accidents shows that if an anomaly cannot be detected early in the potential stage before thermal runaway, the anomaly may develop into a serious fault at any time and lead to irreparable damage. This paper presents such a semi-supervised anomaly detection model, by using a gated recurrent unit (GRU) based variational autoencoder (VAE) (GRU-VAE) framework that detects the early potential anomalies of EV power battery packs. Specifically, employing a GRU-based inference network enables the model to learn the robust latent feature representations of the input multivariate time series (MVTS), which are then used by the generator network to reconstruct the input data. In the training phase, minimizing the reconstruction error helps to learn the data distribution of normal samples. In the testing phase, a large reconstruction error of an input sample indicates that it contains potential abnormal events. In particular, the GRU is used to capture the complex time dependencies in MVTS, and the VAE is used to reconstruct the input sample with probability. The Peaks over Threshold (POT) model in the classical extreme value theory (EVT) is used to properly set the anomaly detection threshold. Experimentation over the real EV operation datasets shows that the model can effectively detect the potential anomalies in MVTS, which is expected to provide a reference for intelligent power battery pack anomaly detection technology.},
  archive      = {J_ASOC},
  author       = {Changcheng Sun and Zhiwei He and Huipin Lin and Linhui Cai and Hui Cai and Mingyu Gao},
  doi          = {10.1016/j.asoc.2022.109903},
  journal      = {Applied Soft Computing},
  pages        = {109903},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Anomaly detection of power battery pack using gated recurrent units based variational autoencoder},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Search and tracking strategy of autonomous surface
underwater vehicle in oceanic eddies based on deep reinforcement
learning. <em>ASOC</em>, <em>132</em>, 109902. (<a
href="https://doi.org/10.1016/j.asoc.2022.109902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to dynamic changes and instability of oceanic eddies, continuous tracking and sampling using mobile platforms is a challenging field. Aiming at the requirements of accurate observation of mesoscale eddies, this paper studies the problem of searching and tracking the eddy center in a mapless environment with an underactuated autonomous surface underwater vehicle (ASUV) and proposes a path planning method based on the deep reinforcement learning (DRL). Firstly, the existing observation methods are summarized, and the dynamic tracking framework of mesoscale eddies is established. Then, the DRL and long short-term memory (LSTM) are combined to train end-to-end and real-time planning strategies in an eddy environment. Finally, a high-fidelity simulation platform in the eddy environment is built, and actual data from the Kuroshio Extension region is used to verify the effectiveness and feasibility of the strategies. The experimental results show that the ASUV with the proposed strategies can realize the autonomous search and tracking of the eddy center, have good stability and real-time performance, and the tracking error is always within an acceptable range.},
  archive      = {J_ASOC},
  author       = {Dalei Song and Wenhao Gan and Peng Yao},
  doi          = {10.1016/j.asoc.2022.109902},
  journal      = {Applied Soft Computing},
  pages        = {109902},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Search and tracking strategy of autonomous surface underwater vehicle in oceanic eddies based on deep reinforcement learning},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fractional sampling operators of multivariate fuzzy
functions and applications to image processing. <em>ASOC</em>,
<em>132</em>, 109901. (<a
href="https://doi.org/10.1016/j.asoc.2022.109901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce a novel family of sampling-type operators of multivariate fuzzy-valued functions based on the fractional mean values of the approximated function. We discuss some convergence properties of these operators concerning an L p Lp -type fuzzy metric and examine the rate of convergence of the operators in Lipschitz classes of multivariate fuzzy valued functions. Some illustrative examples and graphs are given to demonstrate the convergence behavior of the operators in both one and two-dimensional cases. Finally, we present an algorithm for fuzzy image processing with numerical findings using the rule-based contrast enhancement method including a multidimensional fuzzy image matrix.},
  archive      = {J_ASOC},
  author       = {Uğur Kadak},
  doi          = {10.1016/j.asoc.2022.109901},
  journal      = {Applied Soft Computing},
  pages        = {109901},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fractional sampling operators of multivariate fuzzy functions and applications to image processing},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Digital transformation in the defense industry: A maturity
model combining SF-AHP and SF-TODIM approaches. <em>ASOC</em>,
<em>132</em>, 109896. (<a
href="https://doi.org/10.1016/j.asoc.2022.109896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an inevitable process, digitalization has become a priority for many companies. The measurement of digital maturity is the first step toward adequately executing this. Although digital maturity models (DMM) have been developed for different sectors in the literature, such studies in the defense industry are lacking due to sector-specific dynamics. This study aims to close this gap and proposes a digital maturity model specific to the defense industry. In this study, a novel model was developed that combines the SF-AHP and SF-TODIM methods due to the uncertainty and hesitancy contained in the evaluation. The validity of the presented novel model has been demonstrated in a prominent defense company in Turkey. According to the results, the most notable digital maturity dimensions are the evaluation of opportunities and alignment with stakeholders. In addition, the model indicates that the company owns the required soft skills, such as leadership, organizational culture, and strategic determination for digital transformation (DT). On the other hand, essential hard skills such as technology and operational competencies are yet to be improved. Lastly, sensitivity and comparison analyses are conducted to validate and verify the obtained results’ stability and robustness.},
  archive      = {J_ASOC},
  author       = {Emine Elif Nebati and Berk Ayvaz and Ali Osman Kusakci},
  doi          = {10.1016/j.asoc.2022.109896},
  journal      = {Applied Soft Computing},
  pages        = {109896},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Digital transformation in the defense industry: A maturity model combining SF-AHP and SF-TODIM approaches},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive learning for single-output complex systems via data
augmentation and data type identification. <em>ASOC</em>, <em>132</em>,
109895. (<a href="https://doi.org/10.1016/j.asoc.2022.109895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive learning approach is proposed for single-output complex systems (SOCS) with two features: data augmentation (A) and data type identification (T). Data augmentation is used to handle small-sized data gathered from multiple phases. Three data types, that is, normal, critical, and noisy data, are identified based on the causal logic relation between the input and single output of complex systems. Moreover, the proposed approach is implemented in an adaptive manner because the testing dataset in the current phase is combined with the training dataset to form a more comprehensive training dataset to predict the output of the next phase. Therefore, the new approach is called adaSOCS-A-T, which first implements data augmentation followed by data type identification. A practical case of building tilt rate (BTR) prediction in metro tunnel construction is studied. The case study results show that the proposed approach can produce more accurate prediction results compared with the other approaches. The proposed adaSOCS-A-T approach is further validated by (1) the comparative superiority of adaSOCS-A-T over adaSOCS with no features, adaSOCS-A with only data augmentation, adaSOCS-T with only data type identification, and adaSOCS-T-A with data type identification followed by data augmentation, (2) comparative results using MIXUP, SMOTE, and Gaussian as the data augmentation methods, and (3) further statistical t -test results.},
  archive      = {J_ASOC},
  author       = {Leilei Chang and Hao Liu and Limao Zhang and Xiaobin Xu and Jiang Jiang},
  doi          = {10.1016/j.asoc.2022.109895},
  journal      = {Applied Soft Computing},
  pages        = {109895},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive learning for single-output complex systems via data augmentation and data type identification},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk assessment approach for analyzing risk factors to
overcome pandemic using interval-valued q-rung orthopair fuzzy decision
making method. <em>ASOC</em>, <em>132</em>, 109891. (<a
href="https://doi.org/10.1016/j.asoc.2022.109891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of developing and implementing sustainable strategies to prevent spread of COVID-19 for society typically requires integrating all social, technological, economic, governmental aspects in a systematic way. Since the clear understanding of risk factors contribute to the success of the strategies applied against COVID-19, a risk assessment procedure is applied in this study to properly evaluate risk factors cause to spread of pandemic as a multi-complex decision problem. Therefore, due to the evaluation of risk factors, which often involves uncertain information, the model is constructed based on interval-valued q-rung orthopair fuzzy-COmplex PRoportional ASsessment (IVq-ROF-COPRAS) method. While the developed framework is efficient to enhance the quality of decisions by implementing more realistic, precise, and effective application procedure under uncertain environment, it has capability to help governments for developing comprehensive strategies and responses. According to the results of the proposed risk analysis model, the top three risk factors are “The Approach that Prioritizes the Economy in Policies”, “Insufficient Process Control in Normalization” and “Lack of Epidemic Management Culture in Individuals and Businesses”. Lastly, to show applicability and efficiency of the model sensitivity and comparative analysis were conducted at the end of the study.},
  archive      = {J_ASOC},
  author       = {Sukran Seker and Fatma Betül Bağlan and Nezir Aydin and Muhammet Deveci and Weiping Ding},
  doi          = {10.1016/j.asoc.2022.109891},
  journal      = {Applied Soft Computing},
  pages        = {109891},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Risk assessment approach for analyzing risk factors to overcome pandemic using interval-valued q-rung orthopair fuzzy decision making method},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive operating performance assessment framework
based on distributed siamese gated recurrent unit for hot strip mill
process. <em>ASOC</em>, <em>132</em>, 109889. (<a
href="https://doi.org/10.1016/j.asoc.2022.109889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For modern industrial processes, the operating performance assessment has a significant role in ensuring high quality and efficiency in production under normal condition. However, with the switching of working conditions, aging of the equipments, and uncertain disturbance, the inherent nonlinear and dynamic characteristics make it difficult to make an accurate assessment. Meanwhile, industrial processes usually include multiple subsystems, and traditional distributed assessment methods seldom consider practical communications between different subsystems. In addition, normal operating performance assessment and fault level assessment are rarely investigated simultaneously in the existing methods. In this paper, a comprehensive operating performance assessment framework is proposed to deal with the above issues. First, a Siamese gated recurrent unit network is developed by Siamese neural network and gated recurrent unit. Then, based on the process knowledge, a distributed Siamese gated recurrent unit (DSGRU) framework with partial communication is proposed. After that, the assessment strategies are designed for normal and abnormal conditions, respectively. Finally, the effectiveness of the proposed framework is demonstrated on a real hot strip mill process (HSMP).},
  archive      = {J_ASOC},
  author       = {Chuanfang Zhang and Kaixiang Peng and Jie Dong and Long Miao},
  doi          = {10.1016/j.asoc.2022.109889},
  journal      = {Applied Soft Computing},
  pages        = {109889},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive operating performance assessment framework based on distributed siamese gated recurrent unit for hot strip mill process},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial and temporal saliency based four-stream network with
multi-task learning for action recognition. <em>ASOC</em>, <em>132</em>,
109884. (<a href="https://doi.org/10.1016/j.asoc.2022.109884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition is a challenging video understanding task for the following two reasons: (i) the complex video background impairs the recognition of desirable actions, and (ii) the fusion of spatial information and temporal information. In this paper, we proposed a novel spatial and temporal saliency based four-stream network with multi-task learning. The proposed model comprises four streams: an appearance stream ( i.e . a spatial stream), a motion stream ( i.e . a temporal stream), a novel spatial saliency stream and a novel temporal saliency stream. The spatial stream captures the global spatial information from videos using the sampled RGB video frames as the input. The temporal stream captures the global motion information of each pixel using the sampled stacked optical flow frames as the input. The novel spatial saliency stream is used to acquire spatial saliency information from spatial saliency frames, and the novel temporal saliency stream is used to acquire temporal saliency information from temporal saliency frames. In addition, based on the four streams, multi-task learning based LSTM is adopted, which can share the complementary knowledge between different CNN features extracted from different stacked frames. The multi-task learning based LSTM can capture long-term dependency relationships between the consecutive frames over temporal evolution , which take full advantage of CNNs and LSTMs. We conduct experiments on three popular video action recognition datasets, including the UCF101 action dataset, the HMDB51 action dataset and the large-scale Kinetics action dataset, to verify the effectiveness of the proposed network, and the results demonstrate that the proposed network achieves better performance than the state-of-the-art methods on these action recognition datasets.},
  archive      = {J_ASOC},
  author       = {Ming Zong and Ruili Wang and Yujun Ma and Wanting Ji},
  doi          = {10.1016/j.asoc.2022.109884},
  journal      = {Applied Soft Computing},
  pages        = {109884},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spatial and temporal saliency based four-stream network with multi-task learning for action recognition},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extending machine learning prediction capabilities by
explainable AI in financial time series prediction. <em>ASOC</em>,
<em>132</em>, 109876. (<a
href="https://doi.org/10.1016/j.asoc.2022.109876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction with higher accuracy is vital for stock market prediction. Recently, considerable amount of effort has been poured into employing machine learning (ML) techniques for successfully predicting stock market price direction. No matter how successful the proposed prediction model is, it can be argued that there occur two major drawbacks for further increasing the prediction accuracy. The first one can be referred as the black box nature of ML techniques, in other words inference from the predictions cannot be explained. Furthermore, due to the complex characteristics of the predicted time series, no matter how sophisticated techniques are employed, it would be very difficult to achieve a marginal increase in accuracy that would meaningfully offset the additional computational burden it brings in. For these two reasons, instead of chasing incremental improvements in accuracy, we propose utilizing an “e X plainable A rtificial I ntelligence” (XAI) approach which can be employed for assessing the reliability of the predictions hence allowing decision maker to abstain from poor decisions which are responsible for declining overall prediction performance. If there would be a measure of how sure the prediction model is on any prediction, the predictions with a relatively higher reliability could be used to make a decision while lower quality decisions could be avoided. In this study, a novel two-stage stacking ensemble model for stock market direction prediction based on ML , empirical mode decomposition (EMD) and XAI is proposed. Our experiments have shown that, proposed prediction model supported with local interpretable model-agnostic explanations (LIME) achieved the highest accuracy of 0.9913 when only the most trusted predictions have been considered on KOSPI dataset and analogous successful results have been obtained from five other major stock market indices.},
  archive      = {J_ASOC},
  author       = {Taha Buğra Çeli̇k and Özgür İcan and Elif Bulut},
  doi          = {10.1016/j.asoc.2022.109876},
  journal      = {Applied Soft Computing},
  pages        = {109876},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Extending machine learning prediction capabilities by explainable AI in financial time series prediction},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An application of fuzzy soft sets to a real-life problem:
Classification of wood materials to prevent fire-related injuries and
deaths. <em>ASOC</em>, <em>132</em>, 109875. (<a
href="https://doi.org/10.1016/j.asoc.2022.109875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of methods are used to determine the health and life-threatening consequences of wood combustion. However, these methods suffer from some limitations. They analyze the data obtained during combustion individually rather than evaluating them together. Furthermore, they do not take into account the fact that some of the parameters that influence the process can be positive, while others can be negative, or that they may not be of equal importance. In this study, we present a fuzzy soft set-based algorithm to handle such issues. The proposed algorithm can also deal with problems arising when the membership values of the objects are close together. It also contributes a novel approach to the literature regarding the selection of wood materials that are suitable for use in fire-prone areas. In order to demonstrate the applicability of our approach we performed an experiment and analyzed wood materials, which are frequently used in wooden structures, according to parameters that may affect human life and health. We also highlight how the present work contributes to the state-of-the-art by interpreting our results using a method that already exists in the literature.},
  archive      = {J_ASOC},
  author       = {Esra Korkmaz and Cemal Özcan and Mustafa Korkmaz},
  doi          = {10.1016/j.asoc.2022.109875},
  journal      = {Applied Soft Computing},
  pages        = {109875},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An application of fuzzy soft sets to a real-life problem: Classification of wood materials to prevent fire-related injuries and deaths},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Censored deep reinforcement patrolling with information
criterion for monitoring large water resources using autonomous surface
vehicles. <em>ASOC</em>, <em>132</em>, 109874. (<a
href="https://doi.org/10.1016/j.asoc.2022.109874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring and patrolling large water resources is a major challenge for nature conservation. The problem of acquiring data of an underlying environment that usually changes within time involves a proper formulation of the information. The use of Autonomous Surface Vehicles equipped with water quality sensor modules can serve as an early-warning system for contamination peak-detection, algae blooms monitoring, or oil-spill scenarios. In addition to information gathering, the vehicle must plan routes that are free of obstacles on non-convex static and dynamics maps. This work proposes a novel framework to obtain a collision-free policy using deterministic knowledge of the environment by means of a censoring operator and noisy networks that addresses the informative path planning with emphasis in temporal patrolling. Using information gain as a measure of the uncertainty reduction over data, it is proposed a Deep Q-Learning algorithm improved by a Q-Censoring mechanism for model-based obstacle avoidance. The obtained results demonstrate the effectiveness of the proposed algorithm for both cases in the Ypacaraí monitorization task. Simulations showed that the use of noisy-networks are a good choice for enhanced exploration, with 3 times less redundancy in the paths with respect to — greedy policy. Previous coverage strategies are also outperformed both in the accuracy of the obtained contamination model by a 13\% on average and by a 37\% in the detection of dangerous contamination peaks. Finally, the achieved results indicate the appropriateness of the proposed framework for monitoring scenarios with autonomous vehicles.},
  archive      = {J_ASOC},
  author       = {Samuel Yanes Luis and Daniel Gutiérrez-Reina and Sergio Toral Marín},
  doi          = {10.1016/j.asoc.2022.109874},
  journal      = {Applied Soft Computing},
  pages        = {109874},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Censored deep reinforcement patrolling with information criterion for monitoring large water resources using autonomous surface vehicles},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semantic relation-aware deep neural network model for
end-to-end conversational recommendation. <em>ASOC</em>, <em>132</em>,
109873. (<a href="https://doi.org/10.1016/j.asoc.2022.109873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommendation system (CRS) aims at recommending appropriate items to the user through a multi-turn conversation. The end-to-end CRS is a type of CRS that models the recommendation task and the conversation task simultaneously which has attracted more and more attention in recent years. At the same time, knowledge graph and Transformer are incorporated into the end-to-end CRS to generate better recommendations and better responses to the user, which makes the CRS have state-of-the-art performance. It is known that there exist semantic relations in a conversation. However, we observe that existing end-to-end CRSs in general ignore the semantic relations in the conversation and therefore would likely hinder the performance of CRSs. Motivated by this, we propose a gated cross- and self-attention based CRS utilizing semantic relation information (ASR) model, which can explicitly model and utilize the semantic relations in a conversation. To the best of our knowledge, we are the first to advocate for modelling and utilizing the semantic relations in the end-to-end CRS, which could help to improve the performance of the CRS. Furthermore, to mitigate the class-imbalance problem that most end-to-end CRSs face, we propose a new negative sampling method which could make the proposed CRS learn better. Moreover, we design a Transformer-based dialogue module integrating the semantic relations in a conversation to generate more diversified and precise responses. Extensive experiments on widely used benchmark datasets demonstrate that the proposed ASR model achieves state-of-the-art results in both recommendation and conversation tasks.},
  archive      = {J_ASOC},
  author       = {Jiajin Wu and Bo Yang and Dongsheng Li and Lihui Deng},
  doi          = {10.1016/j.asoc.2022.109873},
  journal      = {Applied Soft Computing},
  pages        = {109873},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semantic relation-aware deep neural network model for end-to-end conversational recommendation},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CJT-DEO: Condorcet’s jury theorem and differential evolution
optimization based ensemble of deep neural networks for pulmonary and
colorectal cancer classification. <em>ASOC</em>, <em>132</em>, 109872.
(<a href="https://doi.org/10.1016/j.asoc.2022.109872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is one of the most dangerous diseases globally, causing adverse effects on human life, with early detection and treatment planning being crucial for patients. Amongst different malignancies, Lung and Colorectal cancer cause the first and second most cancer deaths in the world, respectively. In this study, the authors aim to analyze LC25000 histopathological image dataset for lung and colon cancer detection. The fundamental goal of the proposed research is to leverage the ensemble learning approach to improve the classification performance of deep learning models. Many previous studies have proposed several ensemble methods and weighting schemes. However, none of them optimized the assigned weights using a meta-heuristic-based approach as per our best knowledge. The authors have applied Differential Evolution optimization to optimize and find the optimal assigned weights to the classifiers while training the ensemble model. In addition, a novel approach to ensemble base learners with majority voting based on Condorcet’s Jury Theorem has also been proposed. This proposed method has been shown to save a lot of computational efforts by eliminating the training procedure of meta-learners. Besides this, the authors also demonstrated that Condorcet’s Jury Theorem holds while ensembling the N N number of classifiers in Neural Networks . Our proposed method and experimental results outperformed compared to the state-of-the-art with the optimized ensemble model showing an accuracy of 99.78\% and Condorcet’s Jury Theorem-based ensemble model 99.88\% on 5-class classification.},
  archive      = {J_ASOC},
  author       = {Gaurav Srivastava and Aninditaa Chauhan and Nitesh Pradhan},
  doi          = {10.1016/j.asoc.2022.109872},
  journal      = {Applied Soft Computing},
  pages        = {109872},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CJT-DEO: Condorcet’s jury theorem and differential evolution optimization based ensemble of deep neural networks for pulmonary and colorectal cancer classification},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selection of IoT service provider for sustainable transport
using q-rung orthopair fuzzy CRADIS and unknown weights. <em>ASOC</em>,
<em>132</em>, 109870. (<a
href="https://doi.org/10.1016/j.asoc.2022.109870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-of-things (IoT) is a fast-growing technology and sustainable transportation is a crucial application area of IoTs. Selecting an appropriate IoT service provider (IoTSP) is cumbersome and viewed as a multi-criteria decision problem. Recent studies on IoTSP selection inferred that uncertainty and subjectivity during preference elicitation are not flexibly handled. Moreover, they mentioned hesitation and interrelationship are crucial behaviors to be modeled. Motivated by the inferences, this research aims to develop an integrated framework with q-rung orthopair fuzzy sets (q-ROFSs) to evaluate IoTSPs. In this line, the CRITIC method is presented for experts’ importance determination under the q-ROF context. Later, Cronbach’s measure is used for criteria weight calculation with q-ROFSs. Also, an algorithm for prioritization is developed by extending the CRADIS formulation. Lastly, the proposed framework is testified through a case example of IoTSP selection for smart city utilities in India. Results infer that availability, total cost, and security/privacy are crucial criteria for evaluating IoTSPs. Sensitivity and comparison analysis is further conducted to determine the framework’s robustness. This study can assist urban planners, politicians, and other stakeholders in selecting proper IoTSPs when forming sustainable smart cities.},
  archive      = {J_ASOC},
  author       = {Raghunathan Krishankumar and Fatih Ecer},
  doi          = {10.1016/j.asoc.2022.109870},
  journal      = {Applied Soft Computing},
  pages        = {109870},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Selection of IoT service provider for sustainable transport using q-rung orthopair fuzzy CRADIS and unknown weights},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolving chimp optimization algorithm by weighted
opposition-based technique and greedy search for multimodal engineering
problems. <em>ASOC</em>, <em>132</em>, 109869. (<a
href="https://doi.org/10.1016/j.asoc.2022.109869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an evolved chimp optimization algorithm (ChOA) that uses greedy search (GS) and opposition-based learning (OBL) to respectively increase the ChOA’s capabilities for exploration and exploitation in addressing real practical engineering-constrained problems. In order to investigate the efficiency of the GSOBL-ChOA, its performance is evaluated by twenty-three standard benchmark functions , 10 benchmark functions from CEC06-2019, a randomly generated landscape, and 12 real practical Constrained Optimization Problems (COPs-2020) from a wide variety of engineering fields, including power system design, synthesis and process design, industrial chemical producer, power-electronic design, mechanical design, and animal feed ratio. The findings are compared to those obtained using benchmark optimizers such as CMA-ES and SHADE as state-of-the-art optimization techniques and CEC competition winners; standard ChOA; OBL-GWO, OBL-SSA, and OBL-CSA as the best benchmark OBL-based algorithms. In order to perform a comprehensive assessment, three non-parametric statistical tests, including the Wilcoxon rank-sum, Bonferroni–Dunn and Holm, and Friedman average rank tests, are utilized. The top two algorithms are GSOBL-ChOA and CMA-ES, with scores of forty and eleven, respectively, among 27 mathematical functions. jDE100 obtained the highest score of 100 in the 100-digit challenge, followed closely by DISHchain1e+12, which achieved the highest possible score of 97, and GSOBL-ChOA obtained the fourth-highest score of 93. Finally, GSOBL-ChOA and CMA-ES outperform other benchmarks in five and four real practical COPs, respectively. The source code of the paper can be downloaded using the following link: https://se.mathworks.com/matlabcentral/fileexchange/119108-evolving-chimp-optimization-algorithm-by-weighted-opposition .},
  archive      = {J_ASOC},
  author       = {Qiuyu Bo and Wuqun Cheng and Mohammad Khishe},
  doi          = {10.1016/j.asoc.2022.109869},
  journal      = {Applied Soft Computing},
  pages        = {109869},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving chimp optimization algorithm by weighted opposition-based technique and greedy search for multimodal engineering problems},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Introducing disruption on stagnated group decision making
processes using fuzzy ontologies. <em>ASOC</em>, <em>132</em>, 109868.
(<a href="https://doi.org/10.1016/j.asoc.2022.109868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Group Decision Making processes, experts debate about how to rank a set of alternatives. It is usual that, at a certain point of the discussion, the debate gets stuck. In this paper, a novel Group Decision Making method for environments with a high number of alternatives is presented. Fuzzy Ontologies are used in order to represent the alternatives and their characteristics. Moreover, a novel stagnation analysis is used in order to determine if the debate gets stuck. If it does, the method modifies the alternatives set in order to introduce new options and remove the least popular ones. This way, the debate can revive since that the new alternatives provide different points of view. The presented method helps experts to conduct long and thorough debates in order for them to be able to make effective and reliable decisions.},
  archive      = {J_ASOC},
  author       = {J.A. Morente-Molinera and A. Morfeq and R. Al-Hmouz and E.B. Ashary and J.F. Su and E. Herrera-Viedma},
  doi          = {10.1016/j.asoc.2022.109868},
  journal      = {Applied Soft Computing},
  pages        = {109868},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Introducing disruption on stagnated group decision making processes using fuzzy ontologies},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological and dimensional constraints based optimal
placement of layout entities using clustering and genetic algorithm.
<em>ASOC</em>, <em>132</em>, 109867. (<a
href="https://doi.org/10.1016/j.asoc.2022.109867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layout planning involves the placement of layout entities, such as kitchen, living-room, office-space, in a house or building in appropriate positions conforming to topological and dimensional constraints. This paper harnesses varied approaches at different phases for optimal layout planning. These include Vaastu Shastra, an ancient Indian system of architecture that optimizes the layout design in buildings and houses. Vaastu Shastra is used to provide the topological structure of layout entities (LE) and for finding appropriate positions for them in the layout. Using the topological constraints, the entities are first divided into four groups: North-East (NE), South-East (SE), North-West (NW) and South-West (SW). Multi-Population Genetic Algorithm (MPGA) is next employed to find topological relations between layout entities. Finally, Entity Planning Genetic Algorithm (EPGA) is used to optimally place the groups in the layout incorporating these topological relations and preserving the various dimensional constraints. The dimensional constraints include the length and width of the layout entities, aspect ratios of the length and width of the layout entities, and the length and width of the layout itself. The system is implemented in AutoCAD as a tool and Auto Lisp as a programming language .},
  archive      = {J_ASOC},
  author       = {Arun Kumar and Kamlesh Dutta and Abhishek Srivastava},
  doi          = {10.1016/j.asoc.2022.109867},
  journal      = {Applied Soft Computing},
  pages        = {109867},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Topological and dimensional constraints based optimal placement of layout entities using clustering and genetic algorithm},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive artificial neural network for reliability
analyses of complex engineering systems. <em>ASOC</em>, <em>132</em>,
109866. (<a href="https://doi.org/10.1016/j.asoc.2022.109866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliability analyses of engineering systems require methods capable of reducing the number of function calls and response variances to calculate the failure probability. This paper is aimed to employ a combination of the Monte Carlo simulation (MCS), artificial neural network (ANN) and control variate technique (CVT) to improve the efficiency and accuracy of the reliability analyses of various engineering systems. The ANN used in this paper is of the multilayer perceptron type. The paper uses the design of experiments (DoE), importance sampling and uniform sampling methods to generate the samples. The combination of the MCS, ANN model, DoEs, and CVT is used for the reliability analyses and significantly improved the results in terms of efficiency. Various numerical examples have shown the superiority of the improved ANN model using CVT in reducing the number of calls of the limit state function and revealed that the obtained results are not sensitive to the number of hidden layer neurons . The reduced number of required simulations and computation costs has enabled the proposed method to be applicable to reliability analyses in complex engineering systems such as structures made by functionally graded materials .},
  archive      = {J_ASOC},
  author       = {Naser Cheraghi and Mahmoud Miri and Mohsen Rashki},
  doi          = {10.1016/j.asoc.2022.109866},
  journal      = {Applied Soft Computing},
  pages        = {109866},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive artificial neural network for reliability analyses of complex engineering systems},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-world-events data sifting through ultra-small labeled
datasets and graph fusion. <em>ASOC</em>, <em>132</em>, 109865. (<a
href="https://doi.org/10.1016/j.asoc.2022.109865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information on social media is vital, especially for events such as natural disasters or terrorist attacks, that might cause rapid growth of data sharing through social media networks. However, collecting and processing data of an event is a challenging task and essentially requires a great deal of data cleaning and filtering out what is relevant/irrelevant to the event. Data sifting task endeavors to identifying the related content to the depicted event data. We propose a learning strategy to dynamically learn complementary contributions from different data-driven features through a semi-supervised graph-fusion technique. Our proposed method relies upon minimal training labeled data samples — ultra-small data learning. Learning through a small labeled set is also of particular interest to forensic investigators and medical researchers — concerning massive data labeling and minimizing energy-efficient computing to reduce redundancy and repetitions. We assess the effectiveness of the proposed semi-supervised method on five datasets from real-world events. Compared with prior-art (supervised and semi-supervised ones), experimental results show the proposed method achieves the best classification results and most efficient computational footprint.},
  archive      = {J_ASOC},
  author       = {Didier A. Vega-Oliveros and José Nascimento and Bahram Lavi and Anderson Rocha},
  doi          = {10.1016/j.asoc.2022.109865},
  journal      = {Applied Soft Computing},
  pages        = {109865},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-world-events data sifting through ultra-small labeled datasets and graph fusion},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Zero-carbon measure prioritization for sustainable freight
transport using interval 2 tuple linguistic decision approaches.
<em>ASOC</em>, <em>132</em>, 109864. (<a
href="https://doi.org/10.1016/j.asoc.2022.109864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freight transport is the main source of carbon emissions. Implementing freight transport zero-carbon measures in an orderly manner has a very important practical value for sustainable development. Thus, the interval 2-tuple linguistic multi-attribute decision-making (I2TL-MADM) approach of zero-carbon measure prioritization for sustainable freight transport is presented. Considering the advantages of interval 2-tuple variables (I2TLVs) expressing the uncertainty evaluation information of zero-carbon measures, the decision model is developed based on I2TLVs. Firstly, the shortcomings of the inconsistent fuzzy preference relations (FPRs) constructed by comparing attribute values expressed by I2TLVs are revealed. Then, additive and multiplicative dominance degrees of I2TLVs are proposed, and their properties are discussed to ensure the consistency of decision information. Two I2TL-MADM methods based on the novel dominance degrees are developed to rank the zero-carbon measures of sustainable freight transport. Finally, the rationality of the developed I2TL-MADM decision framework is illustrated by numerical examples of Guangzhou. The results show that the developed decision methods can effectively construct the consistent FPRs and rank the zero-carbon measures.},
  archive      = {J_ASOC},
  author       = {Zhenyu Zhang and Huirong Zhang and Lixin Zhou},
  doi          = {10.1016/j.asoc.2022.109864},
  journal      = {Applied Soft Computing},
  pages        = {109864},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Zero-carbon measure prioritization for sustainable freight transport using interval 2 tuple linguistic decision approaches},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive minimum adjustment consensus model for large-scale
group decision making under social networks and its application in
integrated care of older people. <em>ASOC</em>, <em>132</em>, 109863.
(<a href="https://doi.org/10.1016/j.asoc.2022.109863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elderly care has become a vital livelihood issue as the aging population continues to grow. Generally, integrated care of older people (ICOPE) involves doctors, nurses, and family members. The service quality evaluation of integrated care can be modeled by social network large-scale group decision making (SN-LSGDM), and linguistic distribution assessment (LDA) is a powerful means to express evaluation information. However, it lacks the associated research. This paper proposes an adaptive minimum adjustment consensus model for SN-LSGDM with LDA and then applies it to evaluate elderly care modes. Firstly, trust relationships are updated according to the opinion similarity and the trust deviation. Then, a new clustering method based on trust density and opinion similarity is proposed to cluster DMs. Further, the expert and subgroup consensus levels are measured by considering the distance from the collective opinion. Moreover, adaptive consensus models are constructed in terms of the minimum adjustment of subgroups and experts. Finally, a case study of the service quality of ICOPE demonstrates the effectiveness of the new method, and comparison and sensitivity analysis reflect the advantages of the new approach.},
  archive      = {J_ASOC},
  author       = {Fanyong Meng and Bicong Chen and Chunqiao Tan},
  doi          = {10.1016/j.asoc.2022.109863},
  journal      = {Applied Soft Computing},
  pages        = {109863},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive minimum adjustment consensus model for large-scale group decision making under social networks and its application in integrated care of older people},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A relax-and-fix pareto-based algorithm for a bi-objective
vaccine distribution network considering a mix-and-match strategy in
pandemics. <em>ASOC</em>, <em>132</em>, 109862. (<a
href="https://doi.org/10.1016/j.asoc.2022.109862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mass vaccination in pandemics is one of the most effective strategies to halt disease transmission. A well-organized vaccination plan is essential for guaranteeing its success in cases with limited vaccines. On the other hand, most vaccines need more than one dose for appropriate immunity, intensifying the vaccine shortage probability and complicating decision-making. This study presents a bi-objective mixed-integer linear model for a vaccine distribution chain problem, simultaneously considering economic and social objectives, with multi-dose vaccination adaptable even for booster doses. Moreover, the compatible vaccines in each vaccination process are considered a mix-and-match strategy to diversify vaccination alternatives and alleviate the vaccine shortage risk. It is shown that the problem is NP-hard, and we develop two heuristic and meta-heuristic algorithms to solve real-size instances in a reasonable time. Both algorithms use problem size reduction and problem decomposition techniques to produce an approximate Pareto front for the problem. The heuristic algorithm employs relax-and-fix and fix-and-optimize techniques, and an epsilon-constraint method to solve the multi-objective problem. The proposed meta-heuristic algorithm is based on the hybrid of two well-known evolutionary algorithms , particle swarm optimization , and genetic algorithm . It also uses a multi-objective framework called PESA-II to generate the Pareto front . Both proposed heuristic and meta-heuristic algorithms can employ parallel computing , which significantly reduces computational time. Finally, we investigate the proposed algorithms’ performance using 30 random problem instances and a case study from Iran. The results demonstrate that heuristic and meta-heuristic algorithms have obtained a reasonable solution with a maximum of 6.9\% and 16.3\% cost objective function gap respectively. The heuristic algorithm is superior to the meta-heuristic in terms of Pareto front quality metrics.},
  archive      = {J_ASOC},
  author       = {Alireza Nikoubin and Mehdi Mahnam and Ghasem Moslehi},
  doi          = {10.1016/j.asoc.2022.109862},
  journal      = {Applied Soft Computing},
  pages        = {109862},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A relax-and-fix pareto-based algorithm for a bi-objective vaccine distribution network considering a mix-and-match strategy in pandemics},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). LayerLog: Log sequence anomaly detection based on
hierarchical semantics. <em>ASOC</em>, <em>132</em>, 109860. (<a
href="https://doi.org/10.1016/j.asoc.2022.109860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {System logs record the running status of systems, and log anomaly detection can help locate anomalies timely to reduce error time and ensure normal operation. Logs in text format contain abundant semantic information . Still, the methods based on log count vector and log key sequence do not take into account the semantics of log data, causing a high missing detection rate. And existing log semantics-based methods fail to fully consider the semantic information among words, logs, and log sequences. Besides, current methods require the usage of Log Parser in preprocessing, which could negatively impact detection accuracy and cause semantic loss. In this paper, we discover the three-layered structure of log data, named the “Word Log-Log Sequence” hierarchy, and propose LayerLog, a novel framework for log sequence anomaly detection based on the hierarchical semantics of log data. Without Log Parser in preprocessing phase , LayerLog can effectively extract semantic features from each layer and is the first framework to consider the semantics of words, logs, and log sequence. What is more, LayerLog can detect execution order anomaly, operation anomaly, and incomplete anomaly of log sequence simultaneously in an end-to-end way. We have evaluated sufficient experiments on two commonly-used public datasets and the experimental results confirm the effectiveness of LayerLog.},
  archive      = {J_ASOC},
  author       = {Chunkai Zhang and Xinyu Wang and Hongye Zhang and Jiahua Zhang and Hanyu Zhang and Chuanyi Liu and Peiyi Han},
  doi          = {10.1016/j.asoc.2022.109860},
  journal      = {Applied Soft Computing},
  pages        = {109860},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LayerLog: Log sequence anomaly detection based on hierarchical semantics},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Soft sensor for non-invasive detection of process events
based on eigenresponse fuzzy clustering. <em>ASOC</em>, <em>132</em>,
109859. (<a href="https://doi.org/10.1016/j.asoc.2022.109859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Changes in process states and properties can be observed through measured variables. In this way, by classifying time series segments of measured data, changes in model parameters can be detected and the system state can be inferred. Time series classification methods are used in many fields, but the work presented here focuses mainly on the field of manufacturing. In the category of whole-series time series classifiers, the Nearest Neighbor classifier is often used. The aim of this work is to introduce an alternative supervised method for time series classification — Eigenresponse Fuzzy Clustering (EFC). We introduce class eigenresponses, which are time series prototypes of a class. We propose the learning eigenresponses for each class using a fuzzy clustering technique . Unlike some existing methods, we propose the use of multiple prototypes per class to better describe a wider range of values for each class. Moreover, the presented method is evaluated on several datasets. Using a dataset obtained on an industrial test bench on an e-bike drive assembly line, the method correctly classifies all time series. To further validate the performance, a set of publicly available datasets (UCR Archive) is used. For the category of datasets most similar to the target industrial application, an improvement over the benchmark approach is obtained.},
  archive      = {J_ASOC},
  author       = {Žiga Stržinar and Boštjan Pregelj and Igor Škrjanc},
  doi          = {10.1016/j.asoc.2022.109859},
  journal      = {Applied Soft Computing},
  pages        = {109859},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft sensor for non-invasive detection of process events based on eigenresponse fuzzy clustering},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A probabilistic forecasting approach for air quality
spatio-temporal data based on kernel learning method. <em>ASOC</em>,
<em>132</em>, 109858. (<a
href="https://doi.org/10.1016/j.asoc.2022.109858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable air quality index forecasting is an important guarantee for government to take early emergency measures to protect the ecological environment and public health. Nevertheless, providing accurate and reliable forecasting is still a difficult and challenging task, owing to the high nonlinearity and noise in the AQI (air quality index). This study develops a novel framework based on feature selection, the Error Correction Model, and the Residual estimation method with new structure fast kernels, to conveniently improve the prediction performance and efficiently quantify the uncertainty of the point prediction generated by any pre-trained model. This framework takes some representative ML (machine learning) tools as examples, with the significant input variables extracted by the two-stage feature selection method for accurate AQI point prediction, and models their residuals by the multi-output Gaussian process and the multi-output S t u d e n t − t Student−t process, which rely on the new multi-output I/O kernels with chronological memory to embed the relevant information of the point prediction and the residuals as well as of different output for making predictions on residual distributions, thereby enhancing prediction performance and enriching uncertainty information. Besides, an acceleration method based on the Orthogonal Random Features method is also proposed to enable the framework to handle large-scale data at an acceptable time cost. The framework is validated by the AQI Spatio-temporal data of four observation points in Shijiazhuang, China, and the result reveals that the proposed framework provides valid prediction intervals and enhances the accuracy of the ML models’ point predictions. Specifically, the framework reduces the prediction error of almost all basic models, and provides shorter prediction intervals than other common methods. Especially, the multi-output GP model or multi-output TP model obtains the best improvement in point prediction and the shortest prediction intervals.},
  archive      = {J_ASOC},
  author       = {Haolin Zhan and Xin Zhu and Jianming Hu},
  doi          = {10.1016/j.asoc.2022.109858},
  journal      = {Applied Soft Computing},
  pages        = {109858},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A probabilistic forecasting approach for air quality spatio-temporal data based on kernel learning method},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AEFusion: A multi-scale fusion network combining axial
attention and entropy feature aggregation for infrared and visible
images. <em>ASOC</em>, <em>132</em>, 109857. (<a
href="https://doi.org/10.1016/j.asoc.2022.109857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of image fusion is to generate an image that contains more complementary information. Existing image fusion methods suffer from loss of detail information, artifacts and/or inconsistencies. To alleviate these problems, we propose a feature extraction network combining with Axial-attention, which can capture long-range semantic information while extracting multi-scale features and thus has stronger feature representation capabilities. Likewise, existing fusion strategies also suffer from loss of details. To solve this problem, a new fusion strategy is proposed, where a novel attention mechanism is constructed by applying entropy features to aggregate edge and detail features. At the same time, a new loss function is designed to constrain the network. To validate the efficiency of the proposed method, validation experiments are performed on public datasets. Compared with other fusion methods, the experimental results of the proposed method demonstrate state-of-the-art advantages in both subjective and objective evaluations. Furthermore, ablation studies illustrate the superiority of the proposed method.},
  archive      = {J_ASOC},
  author       = {Bicao Li and Jiaxi Lu and Zhoufeng Liu and Zhuhong Shao and Chunlei Li and Yifan Du and Jie Huang},
  doi          = {10.1016/j.asoc.2022.109857},
  journal      = {Applied Soft Computing},
  pages        = {109857},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AEFusion: A multi-scale fusion network combining axial attention and entropy feature aggregation for infrared and visible images},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compressing convolutional neural networks with hierarchical
tucker-2 decomposition. <em>ASOC</em>, <em>132</em>, 109856. (<a
href="https://doi.org/10.1016/j.asoc.2022.109856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) play a crucial role and achieve top results in computer vision tasks but at the cost of high computational cost and storage complexity. One way to solve this problem is the approximation of the convolution kernel using tensor decomposition methods . In this way, the original kernel is replaced with a sequence of kernels in a lower-dimensional space. This study proposes a novel CNN compression technique based on the hierarchical Tucker-2 (HT-2) tensor decomposition and makes an important contribution to the field of neural network compression based on low-rank approximations. We demonstrate the effectiveness of our approach on many CNN architectures on CIFAR-10 and ImageNet datasets. The obtained results show a significant reduction in parameters and FLOPS with a minor drop in classification accuracy . Compared to different state-of-the-art compression methods , including pruning and matrix/tensor decomposition, the HT-2, as a new alternative, outperforms most of the cited methods. The implementation of the proposed approach is very straightforward and can be easily coded in every deep learning library.},
  archive      = {J_ASOC},
  author       = {Mateusz Gabor and Rafał Zdunek},
  doi          = {10.1016/j.asoc.2022.109856},
  journal      = {Applied Soft Computing},
  pages        = {109856},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Compressing convolutional neural networks with hierarchical tucker-2 decomposition},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape-constrained multi-objective genetic programming for
symbolic regression. <em>ASOC</em>, <em>132</em>, 109855. (<a
href="https://doi.org/10.1016/j.asoc.2022.109855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe and analyze algorithms for shape-constrained symbolic regression, which allow the inclusion of prior knowledge about the shape of the regression function. This is relevant in many areas of engineering — in particular, when data-driven models, which are based on data of measurements must exhibit certain properties (e.g. positivity, monotonicity, or convexity/concavity). To satisfy these properties, we have extended multi-objective algorithms with shape constraints. A soft-penalty approach is used to minimize both the constraint violations and the prediction error. We use the non-dominated sorting genetic algorithm (NSGA-II) as well as the multi-objective evolutionary algorithm based on decomposition (MOEA/D). The algorithms are tested on a set of models from physics textbooks and compared against previous results achieved with single objective algorithms. Further, we generated out-of-domain samples to test the extrapolation behavior using shape constraints and added a different level of noise on the training data to verify if shape constraints can still help maintain the prediction errors to a minimum and generate valid models. The results showed that the multi-objective algorithms were capable of finding mostly valid models, also when using a soft-penalty approach. Further, we investigated that NSGA-II achieved the best overall ranks on high noise instances.},
  archive      = {J_ASOC},
  author       = {C. Haider and F.O. de Franca and B. Burlacu and G. Kronberger},
  doi          = {10.1016/j.asoc.2022.109855},
  journal      = {Applied Soft Computing},
  pages        = {109855},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Shape-constrained multi-objective genetic programming for symbolic regression},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building interpretable predictive models with context-aware
evolutionary learning. <em>ASOC</em>, <em>132</em>, 109854. (<a
href="https://doi.org/10.1016/j.asoc.2022.109854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building prediction models with the right balance between performance and interpretability is currently a great challenge in machine learning . A large number of recent studies have focused on either building intrinsically interpretable models or developing general explainers for blackbox models. Although these methods have been widely adopted, their interpretability or explanations are not always useful because of the lack of contexts considered in training machine learning models and producing explanations. This paper aims to tackle this significant challenge by developing a context-aware evolutionary learning algorithm (CELA) for building interpretable prediction models. A new context extraction method based on unsupervised self-structuring learning algorithms is developed to treat data in contexts. The proposed algorithm overcomes the limitations of existing evolutionary learning methods in handling a large number of features and large datasets by training specialised interpretable models based on the automatically extracted contexts. The new algorithm has been tested on complex regression datasets and a real-world building energy prediction task. The results suggest CELA can outperform well-known interpretable machine learning (IML) algorithms, the state-of-the-art evolutionary algorithm , and can produce predictions much closer to the results of blackbox algorithms such as XGBoost and artificial neural networks than the compared IML methods. Further analyses also demonstrate that the CELA’s prediction models are smaller and easier to interpret than those obtained by the evolutionary learning algorithm without context awareness .},
  archive      = {J_ASOC},
  author       = {Binh Tran and Chamika Sudusinghe and Su Nguyen and Damminda Alahakoon},
  doi          = {10.1016/j.asoc.2022.109854},
  journal      = {Applied Soft Computing},
  pages        = {109854},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Building interpretable predictive models with context-aware evolutionary learning},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The effects of data balancing approaches: A case study.
<em>ASOC</em>, <em>132</em>, 109853. (<a
href="https://doi.org/10.1016/j.asoc.2022.109853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced datasets affect the performance of machine learning algorithms adversely. To cope with this problem, several resampling methods have been developed recently. In this article, we present a case study approach for investigating the effects of data balancing approaches. The case study concerns the discrimination between growth hormone treated and non-treated animals using Liquid Chromatography-High Resolution Mass Spectrometry (LC-HRMS) data. Our LC-HRMS dataset contains 1241 bovine urine samples, of which only 65 specimens were from animal studies and guaranteed to contain growth-stimulating hormones while the rest has been reported to be untreated, making it a ∼ ∼ 5\% imbalanced dataset. In this research, classification algorithms , combined with resampling strategies and dimensionality reduction methods, were investigated to find a prediction model to correctly identify the samples of treated animals. Furthermore, to cope with a large number of missing data points in the given dataset, a replacement with random low values strategy was applied. Our results showed that the replacement method was effective, and LogisticRegression combined with the oversampling algorithms SMOTE or ADASYN, GaussianProcessClassifier with the oversampling algorithm SMOTE, and LinearDiscriminantAnalysis were the best performing models after log transformation of the dataset was followed by Recursive Feature Elimination.},
  archive      = {J_ASOC},
  author       = {Paul Mooijman and Cagatay Catal and Bedir Tekinerdogan and Arjen Lommen and Marco Blokland},
  doi          = {10.1016/j.asoc.2022.109853},
  journal      = {Applied Soft Computing},
  pages        = {109853},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The effects of data balancing approaches: A case study},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A particle swarm optimizer with dynamic balance of
convergence and diversity for large-scale optimization. <em>ASOC</em>,
<em>132</em>, 109852. (<a
href="https://doi.org/10.1016/j.asoc.2022.109852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle swarm optimization is found ineffective in large-scale optimization. The main reason is that particle swarlarge-scalem optimization cannot effectively balance convergence and diversity. This paper proposes a particle swarm optimizer with a dynamic balance of convergence and diversity (PSO-DBCD). In the proposed algorithm, a competitive multi-swarm mechanism is put forward, based on which a convergence-guiding learning strategy is proposed for the management of convergence pressure . Furthermore, an entropy-based local diversity measurement is proposed to measure the local diversity of particles. Afterwards, a diversity-guiding learning strategy is proposed based on the local diversity information to further improve the diversity preservation ability of the algorithm. Theoretical analyses are presented to investigate the characteristics of PSO-DBCD. Comprehensive experiments are conducted based on the benchmarks posted on CEC 2013 and several state-of-the-art algorithms to test the performance and scalability of the proposed algorithm. The PSO-DBCD exhibits evident advantages over the compared algorithms in the optimization results with respect to the statistical test results. The proposed strategies are demonstrated to be effective in managing the convergence speed and the swarm diversity. Lastly, a case study of centralized electric vehicle charging optimization shows that PSO-DBCD can reduce the cost of charging for people who use electric vehicles.},
  archive      = {J_ASOC},
  author       = {Dongyang Li and Lei Wang and Weian Guo and Maoqing Zhang and Bo Hu and Qidi Wu},
  doi          = {10.1016/j.asoc.2022.109852},
  journal      = {Applied Soft Computing},
  pages        = {109852},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle swarm optimizer with dynamic balance of convergence and diversity for large-scale optimization},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic detection of covid-19 from chest x-ray and lung
computed tomography images using deep neural networks and transfer
learning. <em>ASOC</em>, <em>132</em>, 109851. (<a
href="https://doi.org/10.1016/j.asoc.2022.109851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world has been undergoing the most ever unprecedented circumstances caused by the coronavirus pandemic, which is having a devastating global effect in different aspects of life. Since there are not effective antiviral treatments for Covid-19 yet, it is crucial to early detect and monitor the progression of the disease, thereby helping to reduce mortality. While different measures are being used to combat the virus, medical imaging techniques have been examined to support doctors in diagnosing the disease. In this paper, we present a practical solution for the detection of Covid-19 from chest X-ray (CXR) and lung computed tomography (LCT) images, exploiting cutting-edge Machine Learning techniques . As the main classification engine, we make use of EfficientNet and MixNet, two recently developed families of deep neural networks . Furthermore, to make the training more effective and efficient, we apply three transfer learning algorithms. The ultimate aim is to build a reliable expert system to detect Covid-19 from different sources of images, making it be a multi-purpose AI diagnosing system. We validated our proposed approach using four real-world datasets. The first two are CXR datasets consist of 15, 000 and 17, 905 images, respectively. The other two are LCT datasets with 2, 482 and 411, 528 images, respectively. The five-fold cross-validation methodology was used to evaluate the approach, where the dataset is split into five parts, and accordingly the evaluation is conducted in five rounds. By each evaluation, four parts are combined to form the training data, and the remaining one is used for testing. We obtained an encouraging prediction performance for all the considered datasets. In all the configurations, the obtained accuracy is always larger than 95.0\%. Compared to various existing studies, our approach yields a substantial performance gain. Moreover, such an improvement is statistically significant.},
  archive      = {J_ASOC},
  author       = {Linh T. Duong and Phuong T. Nguyen and Ludovico Iovino and Michele Flammini},
  doi          = {10.1016/j.asoc.2022.109851},
  journal      = {Applied Soft Computing},
  pages        = {109851},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic detection of covid-19 from chest X-ray and lung computed tomography images using deep neural networks and transfer learning},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLP-GCN: Confidence and label propagation applied to graph
convolutional networks. <em>ASOC</em>, <em>132</em>, 109850. (<a
href="https://doi.org/10.1016/j.asoc.2022.109850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node classification is an important task in many graph-related applications. Recently, graph neural networks like Graph Convolutional Networks (GCNs) have provided low-dimensional informative representations of the nodes of the graphs. However, there are some issues with the application of GCNs in real-world problems. First, information sharing among nodes is limited to directly connected nodes at each layer of GCNs and the contribution of all neighbors of each node is the same in feature aggregation. Second, learning a new representation and training a classifier over this representation require a considerable amount of labeled nodes while there are few labeled nodes in many real world node classification tasks. To compensate for the lack of enough labeled data, in this paper, we propose a novel method for label propagation for GCNs. The main idea of this method, named CLP-GCN, is to propagate the confidence scores of the labels along with the pseudo-labels assigned to the nodes. Two closeness criteria are used to estimate the similarity of each pair of nodes and this similarity measure governs the label propagation from one node of the graph to its neighbors. The confidence of each pseudo-label is determined based on the confidences of the adjacent nodes with the same label. Moreover, the adjacency matrix of GCN is replaced with a more informative matrix representing the structural closeness of the nodes resulting in a more selective feature smoothing in this network. Comprehensive experiments on popular datasets like Citation, Co-authorship, Co-purchase, and knowledge graph datasets demonstrate the superiority of CLP-GCN over state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Mohadeseh Ghayekhloo and Ahmad Nickabadi},
  doi          = {10.1016/j.asoc.2022.109850},
  journal      = {Applied Soft Computing},
  pages        = {109850},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CLP-GCN: Confidence and label propagation applied to graph convolutional networks},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Industry classification based on supply chain network
information using graph neural networks. <em>ASOC</em>, <em>132</em>,
109849. (<a href="https://doi.org/10.1016/j.asoc.2022.109849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number and trade volume of Chinese firms are increasing year by year. The resulting variety of complex transactions have made risk control and government supervision difficult. China’s listed companies have specific classifications, but most non-listed companies do not have comparable classifications, making it difficult to analyze all companies on the same basis. Supply chain networks have proved to contain rich information, which can more completely reflect transaction relationships. This study mines hidden information obtained from the supply chain network to classify participating companies. We construct the supply chain network data set of listed companies, and use the graph neural network (GNN) algorithm to classify these companies. Experiments show that this method is effective and can produce better results than the commonly used machine learning methods. On average the accuracy of industry classification for listed companies is improved by over 2\%, and time required is greatly reduced. In addition, we use economic variables derived from supply chain concepts to try to explain the effectiveness and economic significance of GNN, and find that GNN can also be used to classify companies into multiple industries. Our findings provide new insights, as well as a potential method to label a private company’s industry using only public text information, which can be used for the study of smart industry classification and mining implicit information from the perspective of supply chain networks.},
  archive      = {J_ASOC},
  author       = {Desheng Wu and Quanbin Wang and David L. Olson},
  doi          = {10.1016/j.asoc.2022.109849},
  journal      = {Applied Soft Computing},
  pages        = {109849},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Industry classification based on supply chain network information using graph neural networks},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessment of groundwater potential modeling using support
vector machine optimization based on bayesian multi-objective
hyperparameter algorithm. <em>ASOC</em>, <em>132</em>, 109848. (<a
href="https://doi.org/10.1016/j.asoc.2022.109848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, water supply in order to achieve sustainable development goals is one of the most important concerns and challenges in most countries. For this reason, accurate identification of areas with groundwater potential is one of the important tools in the protection, management and exploitation of water resources. Accordingly, the present study was conducted with the aim of modeling and predicting groundwater potential in Markazi province, Iran using Multivariate adaptive regression spline (MARS) and Support vector machine (SVM) machine learning models and using two random search (RS) and Bayesian optimization hyperparameter algorithms to optimize the parameters of the SVM model. For this purpose, 18 variables affecting the groundwater potential and 3482 spring locations were used to model the groundwater potential. Data for modeling were divided into two categories of training (70\%) and validation (30\%). The receiver operating characteristics (ROC) were used to evaluate the performance of the models. The results of evaluation models showed that using hyperparameters random search and Bayesian optimization were improved SVM accuracy in training and validation stages. Bayesian optimization methods are very efficient because they are consciously choosing the parameters of the model that this strategy improves the performance of the model. Evaluating accuracy in the validation stage showed that the AUC value is for MARS, SVM, RS-SVM and B-SVM models 87.40\%, 88.25\%, 90.73\% and 91.73\%, respectively. The results of assessment variables importance showed elevation, precipitation in the coldest month, soil and slope variables have the most importance in modeling groundwater potential, while aspect, profile curvature and TWI variables, have the least importance in predicting groundwater potential in Markazi province.},
  archive      = {J_ASOC},
  author       = {Duong Tran Anh and Manish Pandey and Varun Narayan Mishra and Kiran Kumari Singh and Kourosh Ahmadi and Saeid Janizadeh and Thanh Thai Tran and Nguyen Thi Thuy Linh and Nguyen Mai Dang},
  doi          = {10.1016/j.asoc.2022.109848},
  journal      = {Applied Soft Computing},
  pages        = {109848},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessment of groundwater potential modeling using support vector machine optimization based on bayesian multi-objective hyperparameter algorithm},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparative study of metaheuristic optimization algorithms
for image steganography based on discrete fourier transform domain.
<em>ASOC</em>, <em>132</em>, 109847. (<a
href="https://doi.org/10.1016/j.asoc.2022.109847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image steganography and watermarking have demonstrated their efficiency in creating a covert communication channel and in digital image authentication respectively. The importance of cybersecurity tasks sets high efficiency requirements for information embedding methods. Many studies in this area focus on finding new ways to improve the embedding quality. One of them is to use metaheuristic optimization algorithms . The use of classical metaheuristics is common enough for a data hiding area. However, a lot of new metaheuristics have been proposed over the past few years. Their applicability has not yet been evaluated in the field of steganography and digital watermarking . In this paper, we present a comparative study on the efficiency of seven metaheuristic optimization algorithms for finding the best embedding options in the phase spectrum of the Discrete Fourier Transform (DFT). One of the contributions of our research is an improved DFT-based data hiding algorithm, which is a development of the error-free embedding algorithm we obtained earlier. The new algorithm also provides error-free extraction of embedded data, and it is distinguished by the high invisibility of embedding through the use of metaheuristic optimization. The use of metaheuristic optimization led to an increase in the PSNR value by an average of 2\%–6\% and an increase in the capacity value by an average of 16\%–25\%. Another important contribution of our research is the original formulation of the optimization problem for different classes of metaheuristics. The experimental results demonstrate the efficiency of metaheuristic optimization for solving data hiding problem. The Differential Evolution and the Particle Swarm Optimization showed the best values of embedding indicators among the classical metaheuristic optimization algorithms . The Gradient-Based Optimizer showed the highest efficiency among modern algorithms. Thus, our research indicates the relevance of further studies of other modern optimization algorithms for a data hiding area.},
  archive      = {J_ASOC},
  author       = {Anna Melman and Oleg Evsutin},
  doi          = {10.1016/j.asoc.2022.109847},
  journal      = {Applied Soft Computing},
  pages        = {109847},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Comparative study of metaheuristic optimization algorithms for image steganography based on discrete fourier transform domain},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential stackelberg games with bounded rationality.
<em>ASOC</em>, <em>132</em>, 109846. (<a
href="https://doi.org/10.1016/j.asoc.2022.109846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stackelberg Games (SGs) assume the perfect rationality of players. However, in real-life situations modeled by SGs, the followers may act not perfectly rationally, as their decisions may be affected/bounded by biases of various kinds, reflecting human behavior in the real world. Anchoring Theory (AT) is one of the popular bounded rationality (BR) models. It postulates that humans have a tendency to flatten the probabilities of the available options, i.e. their probability distribution is perceived as more uniform than is actually the case. This paper proposes a formulation of AT in sequential extensive-form SGs ( ATSG ) and its linearized approximate version (ATSGL) suitable for Mixed-Integer Linear Program (MILP) solution methods. ATSGL is implemented in three MILP/LP state-of-the-art methods for solving sequential SGs and compared with two recent non-MILP metaheuristic approaches based on the original non-simplified ATSG formulation, which rely on Monte Carlo sampling ( O2UCT ) and Evolutionary Algorithms ( EASG ), respectively. Experimental evaluation indicates that non-MILP heuristic approaches provide better solutions and scale better in time than MILPs in the AT setting. The efficacy of ATSG is further evaluated in experiments involving humans as followers, which show that it is more advantageous to use the ATSG leader’s strategy than the Stackelberg Equilibrium strategy , which assumes the perfect rationality of the follower. The results confirm the existence of the human follower’s AT-bias and the possibility to exploit it by the leader. An additional advantage of heuristic methods is the flexibility of the potential BR formulation they are able to incorporate.},
  archive      = {J_ASOC},
  author       = {Jan Karwowski and Jacek Mańdziuk and Adam Żychowski},
  doi          = {10.1016/j.asoc.2022.109846},
  journal      = {Applied Soft Computing},
  pages        = {109846},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sequential stackelberg games with bounded rationality},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Spherical search algorithm with adaptive population control
for global continuous optimization problems. <em>ASOC</em>,
<em>132</em>, 109845. (<a
href="https://doi.org/10.1016/j.asoc.2022.109845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spherical search algorithm (SSA) calculates the spherical boundary and generates new solutions on it by two sub-populations jointly. Many researches have shown that SSA is a promising algorithm, but in some cases, the fixed sub-population size causes it to be prone to search inadequately and easily falling into local optimum. In this paper, a new improved algorithm, named SSAP, is proposed to alleviate these problems. In SSAP, we propose a novel population control strategy to efficiently balance exploration and exploitation. This strategy adaptively adjusts the number of individuals in both sub-populations to improve the search performance of the algorithm. It is realized by adjusting the frequency of search patterns through a cumulative index. Comparative experiments conducted on a large number of benchmark functions show that SSAP significantly outperforms other state-of-the-art algorithms. Additionally, SSAP is used to solve real-world problems to further verify its validity. Finally, the search characteristics and population diversity of SSAP are analyzed.},
  archive      = {J_ASOC},
  author       = {Kaiyu Wang and Yirui Wang and Sichen Tao and Zonghui Cai and Zhenyu Lei and Shangce Gao},
  doi          = {10.1016/j.asoc.2022.109845},
  journal      = {Applied Soft Computing},
  pages        = {109845},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spherical search algorithm with adaptive population control for global continuous optimization problems},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rapid solution of logical equivalence problems by quantum
computation algorithm. <em>ASOC</em>, <em>132</em>, 109844. (<a
href="https://doi.org/10.1016/j.asoc.2022.109844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a quantum computation algorithm that enables solving the problem of logical equivalence verification in exponentially less time than the classical deterministic computation. In this novel quantum algorithm , the oracles of the two evaluated functions are executed in series to yield a common target qubit which then interacts with an ancillary qubit . We found that the degree of entanglement (measured by the concurrence) of the target and ancillary qubits is a reliable witness for the logical equivalence property of the two functions. The steps number of the quantum algorithm is inversely proportional to the square of the standard error ϵ 2 ϵ2 of the measured concurrence value, with no dependence on the input size ′ n ′ ′n′ of each function. This corresponds to a number of evaluations of the two functions: O ( ϵ − 2 ) O(ϵ−2) for the quantum algorithm compared with O ( 2 n ) O(2n) for the classical approach. To assess the algorithm performance, two sets of experiments are conducted using the IBM Q Experience simulator for input sizes: 2 and 12 variables per function. While the former verifies that the results of the experiment are in a good match with the theory, the latter showcases the quantum supremacy of the presented algorithm. In particular, The latter shows that the quantum algorithm requires only 200 oracles queries compared with 2 13 213 queries for the classical algorithm.},
  archive      = {J_ASOC},
  author       = {Mohammed Zidan and Salem F. Hegazy and Mahmoud Abdel-Aty and Salah S.A. Obayya},
  doi          = {10.1016/j.asoc.2022.109844},
  journal      = {Applied Soft Computing},
  pages        = {109844},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rapid solution of logical equivalence problems by quantum computation algorithm},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and development of a forecasting tool for the
identification of new target markets by open time-series data and deep
learning methods. <em>ASOC</em>, <em>132</em>, 109843. (<a
href="https://doi.org/10.1016/j.asoc.2022.109843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globalization and rapid advancements in the IT sector brought new challenges and intensified competition between companies, strongly highlighting the demand for Business Intelligence and Analytics in decision-making and strategy development planning. Motivated by the analysis and forecasting capabilities offered by time-series data and its limited exploitation in marketing literature, this paper introduces a multidisciplinary framework, called MULTIFOR, which aims to synthesize and deliver profound knowledge about the macroeconomic environment and the attractiveness of new target markets. MULTIFOR main objective is to fully support interested parties (i.e., industries, SMEs, scholars, local and national authorities, etc.) in their decision-making and strategy planning tasks, by providing accurate forecasts and recommendations. The proposed framework along with its Web service offers a unique solution since it encapsulates: marketing fundamentals (PESTEL analysis), open data (time-series data from open Web databases) and deep learning methods (LSTM networks) for time-series analysis and forecasting. MULTIFOR has been tested and validated through a use case on elevator and escalator (E&amp;E) industry, which is a strong industrial sector which has important impact worldwide, with studying its scope in European markets. The research findings revealed that LSTM networks , which according to the existing literature are superior to other forecasting models, when combined with macroeconomic theory can achieve greater forecasting accuracy in identifying new international markets. In the case of MULTIFOR, the improvement of LSTM networks performance was achieved through the selection of appropriate indicators and the pre-processing of time-series data exploiting PESTEL analysis. In particular, MULTIFOR achieved approximately 70\% of the 900 time-series reduction of errors in the forecasting process in time-series derived from the PESTEL analysis for European countries . Moreover, the exploitation of MULTIFOR in the E&amp;E industry, revealed the countries of northern Europe as the most attractive markets, of which Sweden and The Netherlands stand out.},
  archive      = {J_ASOC},
  author       = {Odysseas Tsilingeridis and Vaia Moustaka and Athena Vakali},
  doi          = {10.1016/j.asoc.2022.109843},
  journal      = {Applied Soft Computing},
  pages        = {109843},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design and development of a forecasting tool for the identification of new target markets by open time-series data and deep learning methods},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Failure mode and effects analysis based on rough cloud model
and MULTIMOORA method: Application to single-point mooring system.
<em>ASOC</em>, <em>132</em>, 109841. (<a
href="https://doi.org/10.1016/j.asoc.2022.109841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The failure mode and effects analysis (FMEA) is an important tool for risk assessment, which can identify and eliminate potential failure modes in a system. However, due to the inherent drawbacks of the traditional FMEA method, the risk assessment results may be inaccurate. In this study, an FMEA approach based on the rough cloud model and MULTIMOORA (Multi-Objective Optimization on the basis of Ratio Analysis plus full multiplicative form) method is developed to evaluate the risk of the single-point mooring system. The main contributions of this study are as follows. First, an interval asymmetric rough cloud (IARC) model is proposed by combining cloud model and rough set theory , which can deal with the randomness of expert individual evaluation and the uncertainty of expert group evaluation. Second, an IARC power weighted average operator is proposed to eliminate the influence of extreme expert opinions when aggregating expert opinions. Third, according to the feature of single-point mooring system failure, a two-level risk factor hierarchical structure is established, and the subjective and objective weights are integrated to determine the risk factor weights. Fourth, an extended MULTIMOORA method based on the rough cloud model is developed to determine the risk priority of failure modes. To illustrate the implementation process of the approach, we have applied it to a single-point mooring system of an FPSO (Floating Production Storage and Offloading) in the South China Sea. The comparison analysis with the existing methods is also presented to validate the feasibility and effectiveness of this approach.},
  archive      = {J_ASOC},
  author       = {Jianxing Yu and Qingze Zeng and Yang Yu and Shibo Wu and Hongyu Ding and Wentao Ma and Hantao Gao and Jiu Yang},
  doi          = {10.1016/j.asoc.2022.109841},
  journal      = {Applied Soft Computing},
  pages        = {109841},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Failure mode and effects analysis based on rough cloud model and MULTIMOORA method: Application to single-point mooring system},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A balanced-quantum inspired evolutionary algorithm for
solving disassembly line balancing problem. <em>ASOC</em>, <em>132</em>,
109840. (<a href="https://doi.org/10.1016/j.asoc.2022.109840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disassembly lines are the most efficient choice for recovering parts and components on a large scale from old products. The efficient utilization of disassembly line resources requires optimal selection and sequencing of disassembly tasks which constitutes the disassembly line balancing problem (DLBP). In this work, a novel Balanced Quantum-inspired evolutionary algorithm (Balanced-QEA) is proposed to optimize profit and workload balance for the DLBP. Quantum evolutionary algorithms (QEA) utilize stochastic solution representation in the form of q-bits to explore the solution space. The proposed approach differs from traditional QEA by strategically making multiple observations for a single quantum individual. This modification aims to address the weakness of traditional QEA by utilizing stochastic information in quantum solutions more effectively. The application of the proposed approach is illustrated numerically using an example of radio set to maximize profit and workload balance. For validating the utility of proposed modification, the performance of Balanced-QEA is compared with traditional QEA and five other popular evolutionary algorithms in the field of DLBP. The comparisons are done using benchmark disassembly instances of different scales. Results show that the proposed Balanced QEA is superior to QEA and other algorithms in terms of best solutions.},
  archive      = {J_ASOC},
  author       = {Rakshit Kumar Singh and Amit Raj Singh and Ravindra Kumar Yadav},
  doi          = {10.1016/j.asoc.2022.109840},
  journal      = {Applied Soft Computing},
  pages        = {109840},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A balanced-quantum inspired evolutionary algorithm for solving disassembly line balancing problem},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimensional synthesis of four-bar mechanisms using rao
algorithms and their variants. <em>ASOC</em>, <em>132</em>, 109839. (<a
href="https://doi.org/10.1016/j.asoc.2022.109839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the dimensional synthesis of four-bar mechanisms for path generation. The synthesis of the four-bar mechanisms is proposed using nine advanced optimization algorithms, namely the Rao-1 algorithm, Rao-2 algorithm, Rao-3 algorithm, self-adaptive multi-population (SAMP) Rao-1 algorithm, SAMP Rao-2 algorithm, SAMP Rao-3 algorithm, quasi-oppositional-based (QO) Rao-1 algorithm, QO Rao-2 algorithm, and QO Rao-3 algorithm. Six different cases of synthesis of a four-bar mechanism are considered. The brief literature review based on the synthesis of mechanisms is also presented. The designs of mechanisms obtained using Rao algorithms and their variants are compared with the designs acquired using other advanced optimization algorithms, such as genetic algorithm (GA), particle swarm optimization (PSO) algorithm, differential evolution (DE), imperialist competitive algorithm (ICA), hybrid GA-DE algorithm, simulated annealing (SA), Malaga university mechanism synthesis algorithm (MUMSA), IMMa optimization algorithm (IOA), hybrid Taguchi random coordinate search algorithm (HTRCA), hybrid GA and fuzzy logic (GA–FL) algorithm, ant-gradient method (AGM), teaching–learning-based optimization algorithm (TLBO), and hybrid Lagrange interpolation differential evolution algorithm (HLIDE). The comparison of results reveals the efficacy of Rao algorithms and their variants for synthesizing the four-bar mechanism.},
  archive      = {J_ASOC},
  author       = {R.V. Rao and R.B. Pawar},
  doi          = {10.1016/j.asoc.2022.109839},
  journal      = {Applied Soft Computing},
  pages        = {109839},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dimensional synthesis of four-bar mechanisms using rao algorithms and their variants},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data clustering using leaders and followers optimization and
differential evolution. <em>ASOC</em>, <em>132</em>, 109838. (<a
href="https://doi.org/10.1016/j.asoc.2022.109838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data clustering is an important research topic in data mining. Although cluster analysis based on optimization algorithms has attracted great attention, optimization-based techniques face difficulties due to the non-linear objective function and complicated search space. Leaders and followers optimization (LaF) introduced in the 2015 IEEE Congress on Evolutionary Computation , and differential evolution algorithm (DE) are two efficient evolutionary computation methods, and they own some special advantages. The key power of LaF is the exploration in multi-modal search spaces, but it has a poor performance in the exploitation. On the other hand, DE based on the DE/best/1 mutation operator significantly promotes the exploitation process. In this study, the strong properties of LaF and DE are combined to balance exploration and exploitation in the search space to discover the cluster centroids . Besides, the proposed clustering method , i.e., LaF-DE, does not need parameter settings, unlike the existing optimization-based partitional clustering methods. Hence, this study proposes a straightforward, parameter-free, and efficient novel hybrid algorithm for the optimization-based partitional data clustering problem. Many experiments on the functions from CEC2017 test suite show that LaF-DE has better optimization performance and higher stability than the state-of-the-art metaheuristic algorithms . LaF-DE has been compared with well-known clustering techniques on the UCI and Shape datasets. The experimental results and statistical tests indicate that LaF-DE outperforms the well-known partitional clustering methods on 8 of 12 datasets in terms of internal performance metrics. Besides, LaF-DE performs better than density peaks clustering on 9 of 12 datasets in terms of external performance metrics.},
  archive      = {J_ASOC},
  author       = {Ezgi Zorarpacı},
  doi          = {10.1016/j.asoc.2022.109838},
  journal      = {Applied Soft Computing},
  pages        = {109838},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data clustering using leaders and followers optimization and differential evolution},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultrasmall fully-convolution GVA-net for point cloud
processing. <em>ASOC</em>, <em>132</em>, 109837. (<a
href="https://doi.org/10.1016/j.asoc.2022.109837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose GVA-net, a fully-convolutional architecture for point cloud classification and part-segmentation. We prove that the hierarchical of the receptive field among the permutation-constant neighborhood leads to better mean accuracy on the benchmark ModelNet40 dataset by 0.7pp with respect to the best method relying on local context aggregation — PointVGG. We proved that substituting a fully-connected MLP-based classifier with a convolution classifying module, followed by average pooling significantly reduces the complexity of the model without deterioration results. The code used in our study is open-source and publicly available in a repository under the MIT license at https://github.com/jamesWalczak/gva-net .},
  archive      = {J_ASOC},
  author       = {Jakub Walczak and Patryk Najgebauer and Adam Wojciechowski and Rafał Scherer},
  doi          = {10.1016/j.asoc.2022.109837},
  journal      = {Applied Soft Computing},
  pages        = {109837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ultrasmall fully-convolution GVA-net for point cloud processing},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “automated design of heuristics for the
container relocation problem using genetic programming,” [appl. Soft
comput. 130 (2022) 109696]. <em>ASOC</em>, <em>132</em>, 109836. (<a
href="https://doi.org/10.1016/j.asoc.2022.109836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Marko Đurasević and Mateja Đumić},
  doi          = {10.1016/j.asoc.2022.109836},
  journal      = {Applied Soft Computing},
  pages        = {109836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Automated design of heuristics for the container relocation problem using genetic programming”, [Appl. soft comput. 130 (2022) 109696]},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tire normal force estimation using artificial neural
networks and fuzzy classifiers: Experimental validation. <em>ASOC</em>,
<em>132</em>, 109835. (<a
href="https://doi.org/10.1016/j.asoc.2022.109835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tire normal forces play a crucial role in vehicle dynamic control systems (VDCs), and an accurate estimation of them could substantially improve vehicle handling and safety. Total normal forces on tires are the combination of the static and dynamic values. These values change by varying the vehicle static parameters (vehicle mass and CG position), the road grade, and vehicle dynamic states. All of the mentioned forces and parameters are used in the design of vehicle dynamics controllers. Therefore, to have an efficient estimation solution to access all different target controllers, the proposed estimation algorithm is divided into two parts. The first part is an ANN-based vehicle mass and CG position estimation algorithm and the second one is a deep-learning-based algorithm and an integrated hardware–software method to estimate dynamic normal forces. The first part includes two neural network (NN) blocks related to vehicle roll and pitch dynamics. The outputs of the NN blocks are load distributions for the axes associated with each dynamics. Based on the measurement unit data, the vehicle’s maneuvers are classified into one of many categories used for downstream tasks. The rules of the fuzzy classification logic determine which of the mentioned blocks to be activated. At the same time, an experimentally validated 9-DOF vehicle model instantaneously observes the estimated values. In the second part, long short-term memory (LSTM), gated recurrent unit (GRU), and an integrated hardware–software method were developed to assess the tire’s dynamic normal forces during maneuvers. The performed simulations and the results of field tests show that the proposed algorithm can accurately estimate the considered parameters in the presence of noise and disturbances. It is shown that the proposed algorithm is more accurate and faster than the extended Kalman filter and recursive least square estimation methods to estimate static values.},
  archive      = {J_ASOC},
  author       = {Ali Hosseini Salari and Hossein Mirzaeinejad and Majid Fooladi Mahani},
  doi          = {10.1016/j.asoc.2022.109835},
  journal      = {Applied Soft Computing},
  pages        = {109835},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tire normal force estimation using artificial neural networks and fuzzy classifiers: Experimental validation},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on fuzzy systems for biomedical science in
healthcare. <em>ASOC</em>, <em>132</em>, 109834. (<a
href="https://doi.org/10.1016/j.asoc.2022.109834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Weiping Ding and Javier Andreu Perez and Yiu-ming Cheung and Swagatam Das and Xiaodong Yue and Dariusz Mrozek},
  doi          = {10.1016/j.asoc.2022.109834},
  journal      = {Applied Soft Computing},
  pages        = {109834},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Special issue on fuzzy systems for biomedical science in healthcare},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal forecast combination based on PSO-CS approach for
daily agricultural future prices forecasting. <em>ASOC</em>,
<em>132</em>, 109833. (<a
href="https://doi.org/10.1016/j.asoc.2022.109833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting agricultural commodity prices accurately is a challenging task due to the complexity of the trading market and the variability of influencing factors. Many studies have proven that forecast combination is an effective strategy for improving forecast performance relative to individual forecasting. In the field of forecast combination, how to determine the reasonable weights for combination is still an open question. This study proposed an optimal forecast combination framework for agricultural commodity prices forecasting, which integrates the decomposition–reconstruction–ensemble methodology with an improved nature-inspired global optimization algorithm . The update mechanism of particle swarm optimization (PSO) is introduced to improve cuckoo search (CS), in order to reduce the searching blindness in the huge exploration space. The framework consists of four steps. First, data decomposition using empirical wavelet transform (EWT), singular spectral analysis (SSA), and variational mode decomposition (VMD); Second, component reconstruction via a modified reconstruction approach based on the largest comprehensive grey correlation degree clustering (CGCD); Third, individual forecasting using autoregressive integrated moving average regression (ARIMA), exponential smoothing (ETS), back propagation neural network (BPNN) and extreme learning machine (ELM); Fourth, forecast combination via PSO-CS weight assignment method. Using corn and wheat future prices as research samples, the experimental results demonstrated that: (a) the PSO-CS weight assignment approach is superior to other combination approaches in most cases; (b) the CGCD approach can effectively reduce the computational cost of forecasting and improve the prediction performance; (c) the Full-PSO-CS model provides the most accurate forecast due to the diversity of individual forecasts, it reduces MAPE by 43.66\% and improves directional accuracy by 30.80\% on average compared with the best single model.},
  archive      = {J_ASOC},
  author       = {Liling Zeng and Liwen Ling and Dabin Zhang and Wentao Jiang},
  doi          = {10.1016/j.asoc.2022.109833},
  journal      = {Applied Soft Computing},
  pages        = {109833},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal forecast combination based on PSO-CS approach for daily agricultural future prices forecasting},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AntNetAlign: Ant colony optimization for network alignment.
<em>ASOC</em>, <em>132</em>, 109832. (<a
href="https://doi.org/10.1016/j.asoc.2022.109832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Alignment (NA) is a hard optimization problem with important applications such as, for example, the identification of orthologous relationships between different proteins and of phylogenetic relationships between species. Given two (or more) networks, the goal is to find an alignment between them, that is, a mapping between their respective nodes such that the topological and functional structure is well preserved. Although the problem has received great interest in recent years, there is still a need to unify the different trends that have emerged from diverse research areas. In this paper, we introduce AntNetAlign , an Ant Colony Optimization (ACO) approach for solving the problem. The proposed approach makes use of similarity information extracted from the input networks to guide the construction process . Combined with an improvement measure that depends on the current construction state, it is able to optimize any of the three main topological quality measures. We provide an extensive experimental evaluation using real-world instances that range from Protein–Protein Interaction (PPI) networks to Social Networks. Results show that our method outperforms other state-of-the-art approaches in two out of three of the tested scores within a reasonable amount of time, specially in the important S 3 S3 score. Moreover, it is able to obtain near-optimal results when aligning networks with themselves. Furthermore, in larger instances, our algorithm was still able to compete with the best performing method in this regard.},
  archive      = {J_ASOC},
  author       = {Guillem Rodríguez Corominas and Maria J. Blesa and Christian Blum},
  doi          = {10.1016/j.asoc.2022.109832},
  journal      = {Applied Soft Computing},
  pages        = {109832},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AntNetAlign: Ant colony optimization for network alignment},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-attn BLS: Multi-head attention mechanism with broad
learning system for chaotic time series prediction. <em>ASOC</em>,
<em>132</em>, 109831. (<a
href="https://doi.org/10.1016/j.asoc.2022.109831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The observational 1-D signals available for realizing the highly accurate intrinsic attractor fitting of deep learning network approaches are often insufficient because of the complexity and nonlinearity of chaotic time series. Unlike deep models, a broad learning system (BLS) with the attention mechanism exhibits a unique and preeminent pattern prediction ability. Thus, this system has been applied as a practical trend in many fields. However, the application of multi-head attention fused manifold broad learning architecture to chaotic time series prediction remains inadequate. Thus, a multi-head attentional BLS (Multi-Attn BLS) for chaotic time series prediction is proposed in this study to improve the prediction accuracy of chaotic time series further. Our model develops a novel framework that combines the high computational efficiency of broad learning with the multi-head attention mechanism . First, the received data are reconstructed into fixed-size tuples. The multidimensional arrays with embedding dimensions and time delay are used as the input to a broad learning network. Subsequently, a robust BLS with a spatiotemporal multi-head attention mechanism is developed to depict the internal dynamic evolution. The Multi-Attn BLS model can capture key spatiotemporal feature information and achieve high predictive performance . It also has a good generalization ability in practical nonlinear complex systems. Comparative experiments with the traditional long short-term memory (LSTM) network and the primitive BLS show that its computing speed and generalization ability are improved. Furthermore, the network is good at capturing the spatiotemporal features of the sequence because of the multi-head attention mechanism. The experimental results show that our model outperforms BLS, ridge regression, and LSTM on the four main evaluation indicators (root mean square error, root mean square percentage error, mean absolute error , and mean absolute percentage error) in predicting classical systems (Lorenz and Rossler systems). Moreover, the model has an excellent prediction effect in the real-world chaotic system of sea clutter .},
  archive      = {J_ASOC},
  author       = {Liyun Su and Lang Xiong and Jialing Yang},
  doi          = {10.1016/j.asoc.2022.109831},
  journal      = {Applied Soft Computing},
  pages        = {109831},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-attn BLS: Multi-head attention mechanism with broad learning system for chaotic time series prediction},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Allocation of renewable resources with radial distribution
network reconfiguration using improved salp swarm algorithm.
<em>ASOC</em>, <em>132</em>, 109828. (<a
href="https://doi.org/10.1016/j.asoc.2022.109828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an optimal and simultaneous allocation of the photovoltaic panel (PV) and wind turbine (WT) with the reconfiguration of radial distribution networks to reduce power losses cost and improve reliability. Determining the optimal optimization variables are very important to obtain the maximum benefits of renewable resource allocation and network reconfiguration, i.e. achieving the lowest losses and reliability cost. Problem optimization variables are the location, size and power factor of PV and WT as well as network tie-line switches states. Operational constraints include renewable sources size and radiality constraints. The optimization is conducted using an improved salp swarm algorithm (ISSA). Operators of the differential evolutionary (DE) algorithm are used to enhance the performance of conventional salp swarm algorithm (SSA) to prevent trapping in local optima and to increase the convergence speed in achieving the global optimal solution . The methodology is implemented on IEEE 33 and 69 bus distribution networks in various scenarios of renewable sources allocation and reconfiguration and simulation results including the cost of losses, cost of reliability improvement and net saving is presented before and after the optimization. The simulation results show that the ISSA is easily able to find the location and size of the renewable units optimally as well as the best network configuration in different scenarios. The results clear that optimal and simultaneous allocation of WTs and reconfiguration using ISSA has the lowest losses and reliability improvement cost, more net saving and better network performance than the other scenarios. The superiority of the ISSA is also confirmed in solving the problem in comparison with conventional SSA and also the previous studies.},
  archive      = {J_ASOC},
  author       = {Rahim Fathi and Behrouz Tousi and Sadjad Galvani},
  doi          = {10.1016/j.asoc.2022.109828},
  journal      = {Applied Soft Computing},
  pages        = {109828},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Allocation of renewable resources with radial distribution network reconfiguration using improved salp swarm algorithm},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locality sensitive hashing-driven multifactorial
evolutionary algorithms for multitask optimization. <em>ASOC</em>,
<em>132</em>, 109827. (<a
href="https://doi.org/10.1016/j.asoc.2022.109827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, evolutionary multitasking optimization (EMTO) has attracted considerable attention. Different from the traditional algorithms that solve one optimization problem at a time, EMTO aims at solving multiple problems simultaneously, through transferring useful information among individuals. The most well-known algorithm in this area is the multifactorial evolutionary algorithm (MFEA). However, when designing the information sharing mechanism, MFEA and its variants focus solely on the properties of individuals in the objective space, while neglecting the valuable information in the decision space. To make full use of the inherent knowledge of the evolving population, this paper presents a novel EMTO framework by using the locality sensitive hashing (LSH) method to analyze the population distribution in the decision space and then guide the information sharing. Specifically, LSH uses hash functions to map the individuals in the decision space into hash codes, ensuring that similar individuals have a higher probability to be mapped into the same code. Then, based on the matching relationships of the hash codes and skill factors between individuals, different reproduction and evaluation strategies are exerted to satisfy the specific requirements under different conditions. This way, the knowledge from both the decision space and the objective space are utilized for the purpose of better manipulating the individuals. Additionally, an opposition-based learning method is integrated in this algorithm to enhance exploration. The comprehensive experimental studies confirm the efficacy and efficiency of the proposed method on both single-objective multitask benchmarks and multi-objective multitask benchmarks. According to the statistic tests, the proposed single-objective and multi-objective algorithms significantly outperform their best competitors on 14/18 and 12/18 tasks, respectively.},
  archive      = {J_ASOC},
  author       = {Tuo-Bin Yu and Yu-Hui Zhang and Yue-Jiao Gong and Yuan Li},
  doi          = {10.1016/j.asoc.2022.109827},
  journal      = {Applied Soft Computing},
  pages        = {109827},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Locality sensitive hashing-driven multifactorial evolutionary algorithms for multitask optimization},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Invariance measures for neural networks. <em>ASOC</em>,
<em>132</em>, 109817. (<a
href="https://doi.org/10.1016/j.asoc.2022.109817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability . Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.},
  archive      = {J_ASOC},
  author       = {Facundo Manuel Quiroga and Jordina Torrents-Barrena and Laura Cristina Lanzarini and Domenec Puig-Valls},
  doi          = {10.1016/j.asoc.2022.109817},
  journal      = {Applied Soft Computing},
  pages        = {109817},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Invariance measures for neural networks},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep manifold regularized semi-nonnegative matrix
factorization for multi-view clustering. <em>ASOC</em>, <em>132</em>,
109806. (<a href="https://doi.org/10.1016/j.asoc.2022.109806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development and application of modern technology, multi-view data exists widely. The deep matrix factorization model is widely used because it mines the hierarchical information of the samples, but it lacks a geometric structure protection mechanism. Moreover, in multi-view representation learning , strong representations have consensus from multi-view data and are more likely to exist in the structural relationships of data objects. To address these issues, we propose Deep Manifold Regularized Semi-Nonnegative Matrix Factorization for Multi-view Clustering (DMRMF_MVC), which preserves the geometric structure of data objects through multi-layer embedding graph regularization . The graph regularizer combines the nearest neighbor relationship with the exclusive relationship of data objects to express the internal manifold structure of the data fully. Furthermore, to distinguish the value of each view data, the adaptive weight is associated with representation learning to maximize the complementarity between the views. Extensive experimental results show that DMRMF_MVC outperforms other state-of-the-art algorithms based on multiple indicators on various datasets.},
  archive      = {J_ASOC},
  author       = {Xiangnan Liu and Shifei Ding and Xiao Xu and Lijuan Wang},
  doi          = {10.1016/j.asoc.2022.109806},
  journal      = {Applied Soft Computing},
  pages        = {109806},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep manifold regularized semi-nonnegative matrix factorization for multi-view clustering},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data augmentation techniques in natural language processing.
<em>ASOC</em>, <em>132</em>, 109803. (<a
href="https://doi.org/10.1016/j.asoc.2022.109803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Augmentation (DA) methods – a family of techniques designed for synthetic generation of training data – have shown remarkable results in various Deep Learning and Machine Learning tasks. Despite its widespread and successful adoption within the computer vision community, DA techniques designed for natural language processing (NLP) tasks have exhibited much slower advances and limited success in achieving performance gains. As a consequence, with the exception of applications of back-translation to machine translation tasks, these techniques have not been as thoroughly explored by the wider NLP community. Recent research on the subject still lacks a proper practical understanding of the relationship between the various existing DA methods. The connection between DA methods and several important aspects of its outputs, such as lexical diversity and semantic fidelity, is also still poorly understood. In this work, we perform a comprehensive study of NLP DA techniques, comparing their relative performance under different settings. We analyze the quality of the synthetic data generated, evaluate its performance gains and compare all of these aspects to previous existing DA procedures.},
  archive      = {J_ASOC},
  author       = {Lucas Francisco Amaral Orosco Pellicer and Taynan Maier Ferreira and Anna Helena Reali Costa},
  doi          = {10.1016/j.asoc.2022.109803},
  journal      = {Applied Soft Computing},
  pages        = {109803},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data augmentation techniques in natural language processing},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust observer-based output-feedback control for epidemic
models: Positive fuzzy model and separation principle approach.
<em>ASOC</em>, <em>132</em>, 109802. (<a
href="https://doi.org/10.1016/j.asoc.2022.109802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A feedback-type vaccination and treatment policy for a nonlinear susceptible–infected–recovered (SIR) epidemic model involving uncertainties and disturbances is designed by considering only the number of infected individuals. Specifically, an observer-based output-feedback (OBOF) controller design methodology in the Takagi–Sugeno fuzzy model framework is established. The designed controller exhibits L ∞ L∞ – L ∞ L∞ disturbance attenuation performance, is robust against norm-bounded parametric uncertainties , and can preserve the closed-loop positivity. The primary contributions of the proposed design condition can be characterized as follows: (i) a novel structure of a fuzzy OBOF controller to ensure the closed-loop positivity; (ii) separate disturbance attenuation performances for the independent design of the controller and observer; (iii) The independent design conditions formulated as convex optimization problems in the presence of parametric uncertainties ; (iv) the demonstration of the separation principle in the positive L ∞ L∞ – L ∞ L∞ disturbance attenuation problem. Results of the case study pertaining to the SIR model demonstrate the efficacy of the proposed methodology.},
  archive      = {J_ASOC},
  author       = {Ho Jae Lee},
  doi          = {10.1016/j.asoc.2022.109802},
  journal      = {Applied Soft Computing},
  pages        = {109802},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust observer-based output-feedback control for epidemic models: Positive fuzzy model and separation principle approach},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimization framework for COVID-19 vaccine allocation
and inventory management: A case study. <em>ASOC</em>, <em>132</em>,
109801. (<a href="https://doi.org/10.1016/j.asoc.2022.109801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the novel coronavirus pandemic wreaked havoc globally, governments have implemented massive vaccination programs to tackle it. However, since the pandemic’s emergence moves beyond the second year, some issues have stymied vaccination programs, including vaccine hesitancy, vaccine distribution inequality, new strains of the virus, and a possibility that the virus enters a stage of a requirement for cyclical vaccination. These challenges highlight the need for an appropriate mass COVID-19 vaccination program. Therefore, we attempt to address this problem by developing a bi-objective integrated vaccine allocation and inventory management framework. The goal is to minimize the system costs while maximizing the vaccination service level. Several important factors, such as multiple types of vaccines, the vaccines’ perishability concept, demand uncertainty, and motivational strategy, have been addressed using dynamic planning. Besides that, the model development mechanism is carried out to be compatible and applicable to the current general vaccination program policies, forcing few strategic changes. Then, a case study concerning the vaccination program of the city of Mashhad in Iran is applied to the model. The results demonstrated significant advantages in total cost, vaccine shortage, and wastage compared to the current policy. Finally, the Lagrangian relaxation method is implemented on the model to strengthen further its capacity to handle larger-scale problems.},
  archive      = {J_ASOC},
  author       = {Jamal Nahofti Kohneh and Masoud Amirdadi and Ebrahim Teimoury},
  doi          = {10.1016/j.asoc.2022.109801},
  journal      = {Applied Soft Computing},
  pages        = {109801},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimization framework for COVID-19 vaccine allocation and inventory management: A case study},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An interpretable convolutional neural network for nuclear
power plant abnormal events. <em>ASOC</em>, <em>132</em>, 109792. (<a
href="https://doi.org/10.1016/j.asoc.2022.109792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When an abnormal situation occurs in a nuclear power plant (NPP), operators must properly diagnose the event among hundreds of possible abnormal events. To do so, they monitor changes in plant parameters and confirm the correct abnormal operating procedure when the parameters match the entry conditions described in that procedure. In this process, operators are burdened with a lot of information. The purpose of this study is to optimize the number of main parameters to be monitored for abnormal state diagnosis in NPPs by clarifying the classification process with a deep learning model. To increase the transparency of the trained convolutional neural network model in the diagnosis of 10 different NPP states, we applied three explanation techniques: saliency mapping, guided gradient-weighted class activation mapping, and deep learning important features + Shapley values. These techniques can highlight the particular input parameters that are the most influential to the classification. Each transparency result confirmed that the parameters selected by these techniques can be a key rationale in NPP abnormal state diagnosis. By averaging the results of the two methods with the highest transparency performance, it was possible to intuitively classify all 10 NPP states with only 6 optimized monitoring parameters.},
  archive      = {J_ASOC},
  author       = {Ji Hyeon Shin and Junyong Bae and Jae Min Kim and Seung Jun Lee},
  doi          = {10.1016/j.asoc.2022.109792},
  journal      = {Applied Soft Computing},
  pages        = {109792},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interpretable convolutional neural network for nuclear power plant abnormal events},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced elman spike neural network based sentiment analysis
of online product recommendation. <em>ASOC</em>, <em>132</em>, 109789.
(<a href="https://doi.org/10.1016/j.asoc.2022.109789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The major task of Natural Language Processing (NLP) is Sentiments Analysis (SA) or Opinion Mining (OM). It captures the opinion, trust, and feelings of every user’s about the related products to find whether the attitude of users is positive, negative, or neutral. By this, companies can make necessary changes in the product for customer satisfaction. The majority of the existing approaches on sentiment analysis have inaccurate and takes more time. Therefore, in this manuscript, an Enhanced Elman Spike Neural Network based Sentiment Analysis of Online Product recommendation is proposed (EESNN-SA-OPR). Here, filtering Collaborative (FC) and product to product (P–P) similarity are used as new recommendation systems. The aim of Collaborative Filtering is “to predict the best shops and product–product​ similarity is to predict the best products”. Initially, the datas are taken from Amazon reviews database. Then the input data is pre-processed. The trilateral smoothing filtering (TRSF) is used to remove the content which is no more needed and texts related filtering. After that, the features like manufacturing price (MRP), Manufacturing date (MFD), discounts, offers, ratings in qualities are extracted by using Dominant Gradient Local Ternary Pattern descriptor (DGLTPD) technique. At last, enhanced Elman spike neural network classifies the product recommendation as excellent, good, very good, bad and very bad. The proposed method is executed in MATLAB and its performance is analyzed under some performance metrics, like mean absolute error (MAE), mean squared error (MSE), mean absolute percentage error (MAPE), accuracy, F-Score, recall and precision. The proposed EESNN-SA-OPR method provides 23.14\%, 15.96\%, 31.54\% higher accuracy and 12.33\%, 21.31\%, 41.09\% lower mean absolute error compared with the existing techniques, like Sentiment analysis of online product reviews utilizing DLMNN and future prediction of online product utilizing IANFIS (DLMNN-SA-OPR), a machine learning-based sentiment analysis of online product reviews with new term weighting along feature selection method (LSIBA-ENN-SA-OPR) and Sentiment Analysis on the Reviews of Online Product utilizing Optimized RNN-LSTM along Support Vector Machine (RNN-LSTM-SA-OPR) respectively.},
  archive      = {J_ASOC},
  author       = {Solairaj A. and Sugitha G. and Kavitha G.},
  doi          = {10.1016/j.asoc.2022.109789},
  journal      = {Applied Soft Computing},
  pages        = {109789},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced elman spike neural network based sentiment analysis of online product recommendation},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A three-way classification with fuzzy decision trees.
<em>ASOC</em>, <em>132</em>, 109788. (<a
href="https://doi.org/10.1016/j.asoc.2022.109788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is concerned with the design of a three-way classification mechanism realized through combing fuzzy decision trees and expressing uncertainty associated with classification results . The fuzzy decision tree used in this study is constructed through the generalization of the commonly used decision trees . The notion of three-way decision model first proposed for the interpretation of rules generated in rough set approximation has been widely used in many fields. This study proposes an efficient way to flag data with high level of uncertainty with the classification realized by fuzzy decision trees, which is the capability that commonly used fuzzy decision trees do not have. The data identified in this way are left to users’ judgments or more advanced classification techniques . The developed mechanism is formed as a two-stage construct where a fuzzy decision tree is built by generalizing the Boolean classification boundaries of a pre-constructed decision tree using fuzzy sets and then determining the level of uncertainty to identify instances to be rejected due to a lack of sufficient confidence in their belongingness. The rejected instances that are difficult to process are classified as non-commitment cases and left to some further analyses. The rejection quality of the developed three-way classifier is quantified in terms of the classification accuracy and rejection coefficient. We also elaborate on striking a sound tradeoff between these two performance indicators. Experimental studies demonstrate that the developed mechanism could effectively improve the classification accuracy at the cost of a small proportion of the rejected instances and achieve better performance in comparison with other three-way decision models when generating a three-way decision output.},
  archive      = {J_ASOC},
  author       = {Xiaoyu Han and Xiubin Zhu and Witold Pedrycz and Zhiwu Li},
  doi          = {10.1016/j.asoc.2022.109788},
  journal      = {Applied Soft Computing},
  pages        = {109788},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way classification with fuzzy decision trees},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A surrogate assisted evolutionary multitasking optimization
algorithm. <em>ASOC</em>, <em>132</em>, 109775. (<a
href="https://doi.org/10.1016/j.asoc.2022.109775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary algorithms (EAs) have been applied with strong abilities to solve a wide range of applications, but it can solve one problem at a time. To improve efficiency, an emerging research paradigm in the field of evolutionary computation, Evolutionary multi-tasking (EMT) was proposed. EMT solves multiple optimization tasks simultaneously. The effectiveness of EMT is to improve the solutions for each task via inter-task knowledge transfer. Multifactorial evolutionary algorithms (MFEAs) is the first algorithm proposed to solve multi-task optimization problems . However, it tends to suffer from the issue of negative knowledge transfer. To address this issue and improve the performance of MFEA, we propose to construct a surrogate model as a helper task is optimized and target task simultaneously in MFEA. According to the proposed method, the surrogate model is a related task for each corresponding target task to enhance positive inter-task knowledge transfer. Besides, the surrogate model can reduce the number of local optima and has a simple structure. Experiments are conducted on benchmarks and real-world reservoir flood generation power problems to examine the performance of the proposed algorithm. Comparative experiments on several widely used test problems demonstrated that surrogate models as helper tasks enable significantly improve the performance of MFEA.},
  archive      = {J_ASOC},
  author       = {Shangqi Yang and Yutao Qi and Rui Yang and Xiaoliang Ma and Haibin Zhang},
  doi          = {10.1016/j.asoc.2022.109775},
  journal      = {Applied Soft Computing},
  pages        = {109775},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate assisted evolutionary multitasking optimization algorithm},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A PCA-based fuzzy tensor evaluation model for
multiple-criteria group decision making. <em>ASOC</em>, <em>132</em>,
109753. (<a href="https://doi.org/10.1016/j.asoc.2022.109753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-criteria group decision-making (MCGDM) problems mainly consist of multiple factors and multiple Decision Makers (DMs) or Users, for which dimension extension is necessary when considering all the entries of DMs together. Tensor, a generalized form of a matrix, displays a multi-way array item, which is the most suitable and practical way to represent high-dimensional data without losing any information. In this paper, we first reduce the dimension through Principal Component Analysis (PCA), which helps consider the most-informative criteria. Then, we reintroduced the tensor as a fuzzy-form tensor for the MCGDM problem because of user information uncertainty. We choose Interval-valued Neutrosophic Fuzzy numbers (IVNFNs) as the basis for the tensor form because of their ability to distinguish between truth, indeterminacy , and falsity in the data. Lastly, a Generalized Interval-valued Neutrosophic Fuzzy Weighted Geometric (GIVNFWG) operator is defined. Moreover, a generalized framework for fuzzy-form tensors for high-dimensional MCGDM problems is proposed. The feasibility and efficiency of this proposed process is illustrated for a real-world MCGDM problem of ranking the most efficient Third Party Reverse Logistics Partners (3PRLPs), i.e., recycled fiber-based paper mills for the packaging industry. The obtained results are according to the experts and are validated using sensitivity analysis. This analysis facilitates in assessing the impact on the overall ranking performance of 3PRLPs by considering various combinations of environment and technological sub-criteria.},
  archive      = {J_ASOC},
  author       = {Meenu Singh and Millie Pant and Lingping Kong and Zahra Alijani and Václav Snášel},
  doi          = {10.1016/j.asoc.2022.109753},
  journal      = {Applied Soft Computing},
  pages        = {109753},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A PCA-based fuzzy tensor evaluation model for multiple-criteria group decision making},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two effective heuristic methods of determining the numbers
of fuzzy clustering centers based on bilevel programming. <em>ASOC</em>,
<em>132</em>, 109718. (<a
href="https://doi.org/10.1016/j.asoc.2022.109718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy clustering method plays an increasingly important role in data analysis, image segmentation and many other fields. How to determine the number of clustering centers is a technical problem faced by most clustering methods . Starting from the index-based clustering method and the idea based on the fusion of multiple clustering techniques , this paper designs two methods of determining the numbers of fuzzy clustering centers based on bilevel programming respectively. Firstly, based on the bilevel fuzzy clustering model, an evolutionary algorithm is designed based on the absorptive criterion of fuzzy clustering method(EA-Ac-FCM), and its effectiveness in solving bilevel fuzzy clustering problem is verified by empirical analysis. Secondly, we propose an alternate minimization clustering by a collaborative strategy(AM-CC), the mean shift and fuzzy clustering complement each other and guide the clustering together. Compared with the traditional clustering methods , all of the numerical tests show that the two proposed methods could identify the number of cluster centers of the objective to be clustered correctly and handle the data analysis and image segmentation tasks better.},
  archive      = {J_ASOC},
  author       = {Kaikai Qiao and Junrong Zhang and Jiawei Chen},
  doi          = {10.1016/j.asoc.2022.109718},
  journal      = {Applied Soft Computing},
  pages        = {109718},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two effective heuristic methods of determining the numbers of fuzzy clustering centers based on bilevel programming},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced safety prediction of vault settlement in urban
tunnels using the pair-copula and bayesian network. <em>ASOC</em>,
<em>132</em>, 109711. (<a
href="https://doi.org/10.1016/j.asoc.2022.109711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To analyze and study the key control factors of the risk of vault deformation in tunnels excavated at small clear distances, a risk assessment method based on a pair-copula Bayesian network (PCBN) model is proposed. Based on an analysis of the factors affecting the settlement of the vault of an excavated tunnel, an index system for the risk assessment of the vault settlement is established. A PCBN model based on a pair-copula function and BN can effectively deal with the complex risk system and the correlation problems within the risk system. Using the constructed PCBN model, a risk analysis of the dome settlement caused by the excavation in the Donghu Tuanshan tunnel in Wuhan is carried out, and the risk status of the dome settlement of the tunnel is determined to be basically safe. It can be determined from this research that a pair-copula correlation concept combined with a BN retains the strength of both concepts, and a safety risk analysis method for the settlement of tunnel vaults based on the PCBN model is proposed to perform real-time and effective safety risk assessment and provide decision support for the shield construction stage. Through a correlation analysis of the risk indicators of the risk system, it is determined that the risk indicators that have the most significant impact on the deformation of the vault are the state of the groundwater, the complexity of the construction environment, the soil quality of the tunnel arch bottom, and the soil quality of the tunnel vault. These risk indicators are used as key risk indicators for decision-making. Based on this method, decision-making suggestions for reducing the risk of vault deformation are proposed. Good results are achieved in project implementation, and safety management of the vault deformation of a small clear excavated tunnel is provided.},
  archive      = {J_ASOC},
  author       = {Xianguo Wu and Zongbao Feng and Yang Liu and Yawei Qin and Tingyou Yang and Junchao Duan},
  doi          = {10.1016/j.asoc.2022.109711},
  journal      = {Applied Soft Computing},
  pages        = {109711},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced safety prediction of vault settlement in urban tunnels using the pair-copula and bayesian network},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
