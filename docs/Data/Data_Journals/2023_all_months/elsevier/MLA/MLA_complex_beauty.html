<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mla---57">MLA - 57</h2>
<ul>
<li><details>
<summary>
(2023). Use prompt to differentiate text generated by ChatGPT and
humans. <em>MLA</em>, <em>14</em>, 100497. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Chat Generative Pre-trained Transformer (ChatGPT) achieves increased proficiency in diverse language tasks, its potential implications for academic integrity and plagiarism risks have become concerning. Traditional plagiarism detection tools primarily analyze text passages, which may fall short when identifying machine-generated text. This study aims to introduce a method that uses both prompts and essays to differentiate between machine-generated and human-written text, with the goal of enhancing classification accuracy and addressing concerns of academic integrity. Leveraging a dataset of student-written essays responding to eight distinct prompts, we generated comparable essays with ChatGPT. Similarity scores within machine-generated essays (“within” scores) and between human-written and machine-generated essays (“between” scores) were computed. Subsequently, we used the percentile scores of the “between” scores within the “within” scores distribution to gauge the probability of an essay being machine-generated. Our proposed method achieved high classification accuracy, with an AUC score of 0.991, a false positive rate of 0.01, and a false negative rate of 0.037 in the test set. This validates its effectiveness in distinguishing between machine-generated and human-written essays and shows that it outperforms existing approaches based solely on text passages. This research presents a straightforward and effective method to detect machine-generated essays using prompts, providing a reliable solution to maintain academic integrity in the era of advanced language models like ChatGPT. Nevertheless, the method is not without its limitations, warranting further research to investigate its performance across diverse educational contexts, various prompts, and different model hyperparameters.},
  archive      = {J_MLA},
  author       = {Ruopeng An and Yuyi Yang and Fan Yang and Shanshan Wang},
  doi          = {10.1016/j.mlwa.2023.100497},
  journal      = {Machine Learning with Applications},
  pages        = {100497},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Use prompt to differentiate text generated by ChatGPT and humans},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing deep reinforcement learning architectures for
autonomous racing. <em>MLA</em>, <em>14</em>, 100496. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classical autonomous racing, a perception, planning, and control pipeline is employed to navigate vehicles around a track as quickly as possible. In contrast, neural network controllers have been used to replace either part of or the entire pipeline. This paper compares three deep learning architectures for F1Tenth autonomous racing: full planning, which replaces the global and local planner, trajectory tracking, which replaces the local planner and end-to-end, which replaces the entire pipeline. The evaluation contrasts two reward signals, compares the DDPG, TD3 and SAC algorithms and investigates the generality of the learned policies to different test maps. Training the agents in simulation shows that the full planning agent has the most robust training and testing performance. The trajectory tracking agents achieve fast lap times on the training map but low completion rates on different test maps. Transferring the trained agents to a physical F1Tenth car reveals that the trajectory tracking and full planning agents transfer poorly, displaying rapid side-to-side swerving (slaloming). In contrast, the end-to-end agent, the worst performer in simulation, transfers the best to the physical vehicle and can complete the test track with a maximum speed of 5 m/s. These results show that planning methods outperform end-to-end approaches in simulation performance, but end-to-end approaches transfer better to physical robots.},
  archive      = {J_MLA},
  author       = {Benjamin David Evans and Hendrik Willem Jordaan and Herman Arnold Engelbrecht},
  doi          = {10.1016/j.mlwa.2023.100496},
  journal      = {Machine Learning with Applications},
  pages        = {100496},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Comparing deep reinforcement learning architectures for autonomous racing},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A collaborative filtering recommendation framework utilizing
social networks. <em>MLA</em>, <em>14</em>, 100495. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering is a widely used technique for providing personalized recommendations to users. However, traditional collaborative filtering methods fail to consider the social connections between users. The current study proposes a collaborative filtering recommendation framework that employs social networks to generate more precise and pertinent recommendations. The framework is based on a modified version of the user-based collaborative filtering algorithm, which computes user similarity based on their ratings and social connections. The similarity measure is determined by a weighted combination of these two factors, with the weights learned through an optimization process. The framework is evaluated using a dataset of movie ratings and social connections between users. The findings reveal that the proposed approach outperforms traditional collaborative filtering methods regarding recommendation accuracy and relevance. Moreover, the framework can offer more diverse recommendations compared to traditional methods. In summary, the proposed framework integrates social networks to enhance the accuracy and relevance of collaborative filtering recommendations. The approach has various applications, including e-commerce, music, and movie recommendation, and can potentially address the issues of cold-start and sparsity in collaborative filtering.},
  archive      = {J_MLA},
  author       = {Aamir Fareed and Saima Hassan and Samir Brahim Belhaouari and Zahid Halim},
  doi          = {10.1016/j.mlwa.2023.100495},
  journal      = {Machine Learning with Applications},
  pages        = {100495},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A collaborative filtering recommendation framework utilizing social networks},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying the top determinants of psychological resilience
among community older adults during COVID-19 in taiwan: A random forest
approach. <em>MLA</em>, <em>14</em>, 100494. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resilience in the context of the ongoing COVID-19 pandemic has emerged as a critical public health concern for the elderly population. However, the extent to which a structured model can effectively determine resilience among older adults remains uncertain. In this study, we sought to uncover the most influential factors of resilience using the Promotive and Protective Factors and Processes (PPFP) model. Our sample comprised 936 community-dwelling older adults aged 50 years and above. Through the implementation of random forest analysis and traditional logistic regression, we investigated potential determinants of resilience. Our results demonstrated the efficacy of random forest (RF) analysis, with the area under the receiver operating characteristic curve (AUC) ranging from 0.806 to 0.890 for different resilience score thresholds. Notably, the determinants of psychological resilience that emerged as most significant included stress, depression, self-rated socioeconomic status, exercise habits, and cognition. The application of the PPFP framework in our study offered substantial benefits to healthcare practitioners, enabling them to identify both internal and external variables that warrant intervention in bolstering resilience. Furthermore, the utilization of the random forest algorithm proved invaluable in machine learning, particularly for ranking the importance of resilience-related factors.},
  archive      = {J_MLA},
  author       = {Jia-Jen Chen and Li-Fan Liu and Sheng-Mao Chang and Chi-Pang Lu},
  doi          = {10.1016/j.mlwa.2023.100494},
  journal      = {Machine Learning with Applications},
  pages        = {100494},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Identifying the top determinants of psychological resilience among community older adults during COVID-19 in taiwan: A random forest approach},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Slow feature subspace: A video representation based on slow
feature analysis for action recognition. <em>MLA</em>, <em>14</em>,
100493. (<a href="https://doi.org/10.1016/j.mlwa.2023.100493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new video representation for subspace-based action recognition. Traditional subspace-based methods represent a video as a subspace by applying principal component analysis (PCA) to its frames. However, this subspace might lead to an imprecise representation of actions, as PCA loses the temporal information of frames. Therefore, we introduce the slow feature subspace based on the slow feature analysis (SFA). SFA extracts a set of slow features of an input video by projecting the video frames onto the weight vectors that minimize the data variance over time. Motivated by these properties, several methods based on SFA were proposed for action recognition. However, they do not explicitly consider the distribution of the slow features, which represents essential action information. Therefore, our key idea is to capture this distribution through a low-dimensional subspace called slow feature subspace. Our subspace is generated by applying PCA to several weight vectors corresponding to the slowest components obtained by SFA. Our framework replaces the subspaces of several traditional mutual subspace methods with our slow feature subspace to improve their results in action recognition. This approach transforms the comparison between two videos into the comparison between two slow feature subspaces using the canonical angles between them, avoiding vector concatenation and data aggregation. The effectiveness of our framework is demonstrated through extensive experiments with various datasets. Our results show that our slow feature subspace can improve the traditional subspace-based methods and achieve competitive performance compared to different methods, including state-of-the-art neural networks .},
  archive      = {J_MLA},
  author       = {Suzana Rita Alves Beleza and Erica K. Shimomoto and Lincon S. Souza and Kazuhiro Fukui},
  doi          = {10.1016/j.mlwa.2023.100493},
  journal      = {Machine Learning with Applications},
  pages        = {100493},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Slow feature subspace: A video representation based on slow feature analysis for action recognition},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing lung abnormalities detection and classification
using a deep convolutional neural network and GRU with explainable AI: A
promising approach for accurate diagnosis. <em>MLA</em>, <em>14</em>,
100492. (<a href="https://doi.org/10.1016/j.mlwa.2023.100492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and timely detection and classification of lung abnormalities are crucial for effective diagnosis and treatment planning. In recent years, Deep Learning (DL) techniques have shown remarkable performance in medical image analysis. This paper presents a novel and promising approach, namely DCNN-GRU, for improving the detection and classification of lung abnormalities. Our proposed model combines the capabilities of a Deep Convolutional Neural Network (DCNN) with a Gated Recurrent Unit (GRU) while incorporating Explainable AI techniques. Specifically, the DCNN-GRU model leverages the power of CNNs to automatically extract meaningful features from lung images, capturing both local and global patterns. The extracted features are fed into a GRU, which effectively models temporal dependencies and captures sequential information inherent in lung images. This integration allows the model to understand complex lung abnormalities accurately. Additionally, we emphasize the integration of Explainable Artificial Intelligence (XAI) techniques like LIME, SHAP, and Grad-CAM to enhance the interpretability and transparency of our model. To evaluate the proposed approach, we conducted experiments on COVID-19 and Lung cancer using two different datasets. The model achieved a promising accuracy of 99.30\% and 98.97\% for COVID-19, and lung cancer, respectively. Furthermore, the model significantly reduces training time compared to existing approaches. The results demonstrate that our model outperforms existing approaches, achieving a high accuracy rate in detection and classification tasks . Furthermore, the XAI provides valuable insights into the model’s decision-making process, aiding clinicians in understanding and validating the predictions.},
  archive      = {J_MLA},
  author       = {Md Khairul Islam and Md Mahbubur Rahman and Md Shahin Ali and S.M. Mahim and Md Sipon Miah},
  doi          = {10.1016/j.mlwa.2023.100492},
  journal      = {Machine Learning with Applications},
  pages        = {100492},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing lung abnormalities detection and classification using a deep convolutional neural network and GRU with explainable AI: A promising approach for accurate diagnosis},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accounting for diverse feature-types improves patient
stratification on tabular clinical datasets. <em>MLA</em>, <em>14</em>,
100490. (<a href="https://doi.org/10.1016/j.mlwa.2023.100490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular Clinical and Biomedical Routine Data (CBRD) contains diverse feature types. Recent research shows that the conventional application of Uniform Manifold Projection and Approximation (UMAP) to extract clusters from the low dimensional embedding can prove ineffective due to the diverse feature types in such datasets. Feature-type Distributed Clustering (FDC) workflow accounts for these diverse feature types resulting in a more informative low-dimensional embedding. However, a rigorous assessment of the FDC algorithm is missing so far. In this work, we conducted comprehensive benchmarking experiments to compare the quality of the cluster distributions and low dimensional embeddings generated by the FDC against that of the ones generated by UMAP using standard objective measures: Silhouette score, Dunn index, and ANOVA. Our results confirm that FDC can indeed be the better choice to embed tabular data with diverse feature types in low dimensions and thereby extract clusters from such an embedding. In addition, we provide a rationale behind the choice of metrics proposed in the FDC workflow. Moreover, we also point out some problems with the original Canberra metric used to reduce ordinal features in the FDC workflow and provide a solution in the form of a modified version of the Canberra metric. Using seven datasets from the medical domain for benchmarking, we demonstrate that FDC leads to improved patient stratification.},
  archive      = {J_MLA},
  author       = {Saptarshi Bej and Chaithra Umesh and Manjunath Mahendra and Kristian Schultz and Jit Sarkar and Olaf Wolkenhauer},
  doi          = {10.1016/j.mlwa.2023.100490},
  journal      = {Machine Learning with Applications},
  pages        = {100490},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Accounting for diverse feature-types improves patient stratification on tabular clinical datasets},
  volume       = {14},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel application of XAI in squinting models: A position
paper. <em>MLA</em>, <em>13</em>, 100491. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence , and Machine Learning especially, are becoming increasingly foundational to our collective future. Recent developments around generative models such as ChatGPT, and DALL-E represent just the tip of the iceberg in new gadgets that will change the way we live our lives. Convolutional Neural Networks (CNNs) and Transformer models are at the heart of advancements in the autonomous vehicles and health care industries as well. Yet these models, as impressive as they are, still make plenty of mistakes without justifying or explaining what aspects of the input or internal state, was responsible for the error. Often, the goal of automation is to increase throughput, processing as many tasks as possible in a short a period of time. For some use cases the cost of mistakes might be acceptable as long as production is increased above some set margin. However, in health care, autonomous vehicles, and financial applications, the cost of a mistake might have catastrophic consequences. For this reason, industries where single mistakes can be costly are less enthusiastic about early AI adoption. The field of eXplainable AI (XAI) has attracted significant attention in recent years with the goal of producing algorithms that shed light into the decision-making process of neural networks . In this paper we show how robust vision pipelines can be built using XAI algorithms with the goal of producing automated watchdogs that actively monitor the decision-making process of neural networks for signs of mistakes or ambiguous data. We call these robust vision pipelines, squinting pipelines.},
  archive      = {J_MLA},
  author       = {Kenneth Wenger and Katayoun Hossein Abadi and Damian Fozard and Kayvan Tirdad and Alex Dela Cruz and Alireza Sadeghian},
  doi          = {10.1016/j.mlwa.2023.100491},
  journal      = {Machine Learning with Applications},
  pages        = {100491},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A novel application of XAI in squinting models: A position paper},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crossing language identification: Multilingual ASR framework
based on semantic dataset creation &amp; Wav2Vec 2.0. <em>MLA</em>,
<em>13</em>, 100489. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an innovative methodology to enhance the performance of multilingual Automatic Speech Recognition (ASR) systems by capitalizing on the high semantic similarity between sentences across different languages and eliminating the requirement for Language Identification (LID). To achieve this, special bilingual datasets were created from the Mozzila Common Voices datasets in Spanish, Russian, and Portuguese. The process involves computing sentence embeddings using Language-agnostic BERT and selecting sentence pairs based on high and low cosine similarity . Subsequently, we train the Wav2vec 2.0 XLSR53 model on these datasets and assess its performance utilizing Character Error Rate (CER) and Word Error Rate (WER) metrics. The experimental results indicate that models trained on high-similarity samples consistently surpass their low-similarity counterparts, emphasizing the significance of high semantic similarity data selection for precise and dependable ASR performance. Furthermore, the elimination of LID contributes to a simplified system with reduced computational costs and the capacity for real-time text output. The findings of this research offer valuable insights for the development of more efficient and accurate multilingual ASR systems, particularly in real-time and on-device applications.},
  archive      = {J_MLA},
  author       = {Or Haim Anidjar and Roi Yozevitch and Nerya Bigon and Najeeb Abdalla and Benjamin Myara and Revital Marbel},
  doi          = {10.1016/j.mlwa.2023.100489},
  journal      = {Machine Learning with Applications},
  pages        = {100489},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Crossing language identification: Multilingual ASR framework based on semantic dataset creation &amp; Wav2Vec 2.0},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation metrics for video captioning: A survey.
<em>MLA</em>, <em>13</em>, 100488. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic evaluation metrics play an important role in assessing video captioning systems. Popular metrics used for assessing such approaches are based on word matching and may fail to evaluate the quality of automatically generated captions due to inherent natural language ambiguity. Moreover, they require many reference sentences for effective scoring. With the fast development of image and video captioning methodologies using deep learning in recent years, many metrics have been proposed for evaluating such approaches. In this study, we present a survey of automatic evaluation metrics for the video captioning task. Moreover, we highlight the challenges in evaluating video captioning and propose a taxonomy to organize the existing evaluation metrics. We also briefly describe and identify the advantages and shortcomings of those metrics and identify applications or contexts in which these metrics can be better used. To identify the advantages and limitations of the evaluation metrics, we quantitatively compare them using videos from different datasets employed for the video description task. Finally, we discuss the advantages and limitations of the metrics and propose some promising future research directions, such as semantic measurement, explainability, adaptability, extension to other languages, dataset limitations, and multimodal free-reference metrics.},
  archive      = {J_MLA},
  author       = {Andrei de Souza Inácio and Heitor Silvério Lopes},
  doi          = {10.1016/j.mlwa.2023.100488},
  journal      = {Machine Learning with Applications},
  pages        = {100488},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Evaluation metrics for video captioning: A survey},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rectangularization of gaussian process regression for
optimization of hyperparameters. <em>MLA</em>, <em>13</em>, 100487. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process regression (GPR) is a powerful machine learning method which has recently enjoyed wider use, in particular in physical sciences. In its original formulation, GPR uses a square matrix of covariances among training data and can be viewed as linear regression problem with equal numbers of training data and basis functions. When data are sparse, avoidance of overfitting and optimization of hyperparameters of GPR are difficult, in particular in high-dimensional spaces where the data sparsity issue cannot practically be resolved by adding more data. Optimal choice of hyperparameters, however, determines success or failure of the application of the GPR method. We show that parameter optimization is facilitated by rectangularization of the defining equation of GPR. On the example of a 15-dimensional molecular potential energy surface we demonstrate that this approach allows effective hyperparameter tuning even with very sparse data.},
  archive      = {J_MLA},
  author       = {Sergei Manzhos and Manabu Ihara},
  doi          = {10.1016/j.mlwa.2023.100487},
  journal      = {Machine Learning with Applications},
  pages        = {100487},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Rectangularization of gaussian process regression for optimization of hyperparameters},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A BCI system for imagined bengali speech recognition.
<em>MLA</em>, <em>13</em>, 100486. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-invasive brain-computer interface (BCI) can be used in imagined speech recognition to aid people with speech disorders and has been investigated in different languages. However, this technology is in a very early stage for Bengali speech recognition. Therefore, the aim of this research is to develop a BCI system that can recognize imagined Bengali speech. In this study, a non-invasive 14-channel electroencephalography (EEG) headset was employed to record the EEG signals from 30 subjects. The subjects were instructed to imagine 11 Bengali vowels and 10 digits . Then, we extracted four statistical features from the recorded EEG signals. A coarse-to-fine level classification was performed to categorize the imagined EEG data by exploiting these features. The data was classified at coarse level to distinguish between a vowel and a digit. Each vowel and digit were then classified at the fine level. In our classification, random forest achieved the highest recognition accuracy of 84.28\% at the coarse level and 76.13\% at the fine level. In order to comprehend the role played by each brain lobe in recognizing imagined speech, we additionally examined each of the brain lobes. The frontal lobe demonstrated the highest accuracy, with a coarse level accuracy of 80.65\% and a fine level accuracy of 66.31\%. Moreover, the proposed model outperformed the reported literature in this field.},
  archive      = {J_MLA},
  author       = {Arman Hossain and Kathak Das and Protima Khan and Md. Fazlul Kader},
  doi          = {10.1016/j.mlwa.2023.100486},
  journal      = {Machine Learning with Applications},
  pages        = {100486},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A BCI system for imagined bengali speech recognition},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reinforcement learning algorithm for scheduling parallel
processors with identical speedup functions. <em>MLA</em>, <em>13</em>,
100485. (<a href="https://doi.org/10.1016/j.mlwa.2023.100485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we investigate a real-time system where computationally intensive tasks are executed using cloud computing platforms in data centers . These data centers are designed to provide prompt responses to incoming demands. To achieve this objective, an efficient scheduling system is crucial for determining the assignment of jobs to processors and the optimal starting times for each job’s execution. In this paper, we propose a novel reinforcement learning algorithm that introduces a new state variable and utilizes a set of virtual bins to classify jobs based on their remaining processing times. Our objective is to minimize the total slowdown, which is defined as the sum of completion time ratios to job demand sizes. We conduct a performance evaluation by comparing our developed algorithm with commonly used and highly efficient dispatching rules found in existing literature. The computational results demonstrate that our proposed reinforcement learning approach outperforms other solution approaches available in the literature. Furthermore, we illustrate the generalization capability of our algorithm and its ability to achieve superior results compared to dispatching rules when applied to new test instances.},
  archive      = {J_MLA},
  author       = {Farid Ziaei and Mohammad Ranjbar},
  doi          = {10.1016/j.mlwa.2023.100485},
  journal      = {Machine Learning with Applications},
  pages        = {100485},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A reinforcement learning algorithm for scheduling parallel processors with identical speedup functions},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-module-based CVAE to predict HVCM faults in the SNS
accelerator. <em>MLA</em>, <em>13</em>, 100484. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multi-module framework based on Conditional Variational Autoencoder (CVAE) to detect anomalies in the power signals coming from multiple High Voltage Converter Modulators (HVCMs). We condition the model with the specific modulator type to capture different representations of the n o r m a l normal waveforms and to improve the sensitivity of the model to identify a specific type of fault when we have limited samples for a given module type. We studied several Artificial Neural Network (ANN) architectures for our CVAE model and evaluated the model performance by looking at their loss landscape for stability and generalization. Our results for the Spallation Neutron Source (SNS) experimental data show that the trained model generalizes well to detecting multiple fault types for several HVCM module types. The results of this study can be used to improve the HVCM reliability and overall SNS uptime.},
  archive      = {J_MLA},
  author       = {Yasir Alanazi and Malachi Schram and Kishansingh Rajput and Steven Goldenberg and Lasitha Vidyaratne and Chris Pappas and Majdi I. Radaideh and Dan Lu and Pradeep Ramuhalli and Sarah Cousineau},
  doi          = {10.1016/j.mlwa.2023.100484},
  journal      = {Machine Learning with Applications},
  pages        = {100484},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multi-module-based CVAE to predict HVCM faults in the SNS accelerator},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimization of high computational cost in data
preprocessing and modeling using MPI4Py. <em>MLA</em>, <em>13</em>,
100483. (<a href="https://doi.org/10.1016/j.mlwa.2023.100483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data preprocessing is a fundamental stage in deep learning modeling and serves as the cornerstone of reliable data analytics . These deep learning models require significant amounts of training data to be effective, with small datasets often resulting in overfitting and poor performance on large datasets. One solution to this problem is parallelization in data modeling , which allows the model to fit the training data more effectively, leading to higher accuracy on large data sets and higher performance overall. In this research, we developed a novel approach that effectively deployed tools such as MPI and MPI4Py from parallel computing to handle data preprocessing and deep learning modeling processes. As a case study, the technique is applied to COVID-19 data from state of Tennessee, USA. Finally, the effectiveness of our approach is demonstrated by comparing it with existing methods without parallel computing concepts like MPI4Py. Our results demonstrate promising outcome for the deployment of parallel computing in modeling to minimize high computational cost.},
  archive      = {J_MLA},
  author       = {E. Oluwasakin and T. Torku and S. Tingting and A. Yinusa and S. Hamdan and S. Poudel and N. Hasan and J. Vargas and K. Poudel},
  doi          = {10.1016/j.mlwa.2023.100483},
  journal      = {Machine Learning with Applications},
  pages        = {100483},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Minimization of high computational cost in data preprocessing and modeling using MPI4Py},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-phased cluster-based approach towards ranked
forecast-model selection. <em>MLA</em>, <em>13</em>, 100482. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sales forecasting processes are usually automated to some extent in the retail sector and practitioners often have limited knowledge pertaining to the selection of appropriate forecasting methods. In this paper, we propose a generic two-phased, cluster-based framework capable of assisting retail forecasting practitioners in the selection of appropriate forecasting methods for time series of their retail sales data. One phase of the framework, called the benchmarking phase, involves establishing a benchmark data set (or updating it if it already exists) which can be leveraged to inform feature-based forecast model identification and ranking for different clusters of time series. The computationally efficient identification of a tailored shortlist of forecast models is thus facilitated during the other framework phase , called the implementation phase, for each sales time series presented to it by a retail organisation, based on the features of the time series presented. The two phases of the framework may be applied repeatedly in alternating fashion, enlarging the benchmark data set and improving its representativeness each time after having applied the implementation phase to the sales time series of a new retail organisation by re-applying the processes of the benchmarking phase. One iteration of this alternating application of the two framework phases is demonstrated and validated in respect of the M5 forecasting competition data (employed during the benchmarking phase) and a data set of the retail chain Corporacion Favorita (subsequently employed during the implementation phase).},
  archive      = {J_MLA},
  author       = {Reinard C. Ganzevoort and Jan H. van Vuuren},
  doi          = {10.1016/j.mlwa.2023.100482},
  journal      = {Machine Learning with Applications},
  pages        = {100482},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A two-phased cluster-based approach towards ranked forecast-model selection},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient generation of text feedback in object-oriented
programming education using cached performer revision. <em>MLA</em>,
<em>13</em>, 100481. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer, a deep learning technology, has achieved great success in various applications such as natural language processing . However, its space–time consumption remains a concern, leading to the issue of AI technology monopoly. Therefore, downsizing this technology is critical to mitigating the adverse effects of technological monopoly on innovation and competition. This article proposes a revised version of the Performer, an efficient variant of Transformer. Specifically, the proposed revision aims to resolve three issues associated with Performer. First, generating a target token necessitates padding tokens that require O ( L ) space and time for sequences of maximum length L . Second, Performer redundantly calculate attentions between previously generated tokens. Third, inconsistencies arise between the training and inference phases due to normalizer calculation in the stable FAVOR+ masked attention operation. The proposed revision introduces a cached version of the FAVOR+ operation, resulting in fast text generation with O ( L ) time and O (1) space complexity. To examine the effectiveness of the proposed revision, a Performer-based encoder/decoder model that generates text feedback for code correction in object-oriented programming education is developed and evaluated. The results show that the revised Performer achieves high accuracy and a more than 120-fold increase in inference speed. In addition, user evaluation shows that the feedback generated by this model is more beneficial for programming education than that generated by ChatGPT.},
  archive      = {J_MLA},
  author       = {Feng Hsu Wang},
  doi          = {10.1016/j.mlwa.2023.100481},
  journal      = {Machine Learning with Applications},
  pages        = {100481},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Efficient generation of text feedback in object-oriented programming education using cached performer revision},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New interpretation of GNRB® knee arthrometer results for ACL
injury diagnosis support using machine learning. <em>MLA</em>,
<em>13</em>, 100480. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GNRB is an arthrometer and alternative diagnostic method less expensive than MRI and more accurate than KT-1000 in Anterior Cruciate Ligament (ACL) tears detection. Dynamic knee laxity tests are more complex to analyze and will require a new solution of universal interpretation. The hypothesis is that using a solution based on Artificial Intelligence (AI) will allow us to obtain a more accurate and robust non-invasive diagnostic method than the current solution with three laxity thresholds. AI can enhance the reliability of this analysis by utilizing advanced algorithms and incorporating a wide range of additional parameters, leading to more precise diagnostics. The existing process solely rely on laxity differences obtained from the device, overlooking influential factors like clamping force. By considering a broader set of parameters and employing well-calibrated models a comparative study was performed between different Machine Learning (ML) models and Ensemble Learning to get the best compromise. The correction process will leverage statistical analysis of the current solutions. Association of Voting, Stacking and threshold laxity methods results report a 6\% increase in accuracy and approximately 13\% improvement in tear detection compared to the current solution with 1384 GNRB® measurements. Predicted diagnoses are also more prone to new data from patients unknown to the model and confirmed using a validation database. A first ML model was introduced in ACL tears detection using GNRB device. GNRB coupled with ML was encouraging with better results than the current static diagnostic method. It could be integrated and recommended as a complementary solution to MRI.},
  archive      = {J_MLA},
  author       = {Jean Mouchotte and Matthieu LeBerre and Théo Cojean and Henri Robert},
  doi          = {10.1016/j.mlwa.2023.100480},
  journal      = {Machine Learning with Applications},
  pages        = {100480},
  shortjournal = {Mach. Learn. Appl.},
  title        = {New interpretation of GNRB® knee arthrometer results for ACL injury diagnosis support using machine learning},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilinear multitask learning by transformed tensor
singular value decomposition. <em>MLA</em>, <em>13</em>, 100479. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of multilinear multitask learning (MLMTL), in which all tasks are stacked into a third-order tensor for consideration. In contrast to conventional multitask learning, MLMTL can explore inherent correlations among multiple tasks in a better manner by utilizing multilinear low rank structure. Existing approaches about MLMTL are mainly based on the sum of singular values for approximating low rank matrices obtained by matricizing the third-order tensor. However, these methods are suboptimal in the Tucker rank approximation. In order to elucidate intrinsic correlations among multiple tasks, we present a new approach by the use of transformed tensor nuclear norm (TTNN) constraint in the objective function. The main advantage of the proposed approach is that it can acquire a low transformed multi-rank structure in a transformed tensor by applying suitable unitary transformations which is helpful to determine principal components in grouping multiple tasks for describing their intrinsic correlations more precisely. Furthermore, we establish an excess risk bound of the minimizer of the proposed TTNN approach. Experimental results including synthetic problems and real-world images, show that the mean-square errors of the proposed method is lower than those of the existing methods for different number of tasks and training samples in MLMTL.},
  archive      = {J_MLA},
  author       = {Xiongjun Zhang and Jin Wu and Michael K. Ng},
  doi          = {10.1016/j.mlwa.2023.100479},
  journal      = {Machine Learning with Applications},
  pages        = {100479},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multilinear multitask learning by transformed tensor singular value decomposition},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analogical inference from distributional structure: What
recurrent neural networks can tell us about word learning. <em>MLA</em>,
<em>13</em>, 100478. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One proposal that can explain the remarkable pace of word learning in young children is that they leverage the language-internal distributional similarity of familiar and novel words to make analogical inferences about possible meanings of novel words (Lany and Gómez, 2008; Lany and Saffran, 2011; Savic et al., 2022b; Unger and Fisher, 2021; Wojcik and Saffran, 2015). However, a cognitively and developmentally plausible computational account of how language-internal lexical representations are acquired to enable this kind of analogical inference has not been previously investigated. In this work, we tested the feasibility of using the SRN (Elman, 1990) as the supplier of language-internal representations for use in analogical inference. While the SRN is in many ways well suited to this task, we discuss several theoretical challenges that might limit its success. In a series of simulations with controlled artificial languages and the CHILDES corpus, we show that Recurrent Neural Networks (RNNs) are prone to acquiring ‘entangled’ lexical semantic representations , where some features of a word are partially encoded in the representations of other frequently co-occurring words. However, we also show that this problem is mitigated when RNNs are first trained on language input to young children, due to the fact that its distributional structure more reliably predicts semantic category membership of individual words. Overall, our work sheds light on the conditions under which RNNs organize their learned knowledge so that word-level information can be more easily extracted and used in downstream processes, such as word learning.},
  archive      = {J_MLA},
  author       = {Philip A. Huebner and Jon A. Willits},
  doi          = {10.1016/j.mlwa.2023.100478},
  journal      = {Machine Learning with Applications},
  pages        = {100478},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Analogical inference from distributional structure: What recurrent neural networks can tell us about word learning},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of deep neural networks for inferring pressure
in polymeric acoustic transponders/sensors. <em>MLA</em>, <em>13</em>,
100477. (<a href="https://doi.org/10.1016/j.mlwa.2023.100477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive sensor-transponders have raised interest for the last few decades, due to their capability of low-cost remote monitoring without the need for energy storage. Their operating principle includes receiving a signal from a source and then reflecting the signal. While well-established transponders operate through electromagnetic antennas, those with a fully acoustic design have advantages such as lower cost and simplicity. Therefore, detection of pressures using the ultrasound signal that is backscattered from an acoustic resonator has been of interest recently. In order to infer the pressure from the backscattered signal , the established approach has been based upon the principle of detection of the shift to the frequency of resonance. Nevertheless, regression of the pressure from the signal with a small error is challenging and has been subject to research. Here in this paper, we explore an approach that employs deep learning for inferring pressure from the ultrasound reflections of polymeric resonators. We assess if neural network regressors can efficiently infer pressure reflected from a fully acoustic transponder. For this purpose, we compare the performance of several regressors such as a convolutional neural network , a network inspired by the ResNet , and a fully connected neural network. We observe that deep neural networks are advantageous in inferring pressure information with a minimal need for analyzing the signal. Our work suggests that a deep learning approach has the potential to be integrated with or replace other traditional approaches for inferring pressure from an ultrasound signal reflected from fully acoustic transponders or passive sensors.},
  archive      = {J_MLA},
  author       = {Seyedhamidreza Alaie and Subhi J. Al’Aref},
  doi          = {10.1016/j.mlwa.2023.100477},
  journal      = {Machine Learning with Applications},
  pages        = {100477},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Application of deep neural networks for inferring pressure in polymeric acoustic transponders/sensors},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven acoustic measurement of moisture content in
flowing biomass. <em>MLA</em>, <em>13</em>, 100476. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring the moisture content in flowing biomass is critical to processes such as liquid biofuel conversion, such as biogasoline, biodiesel, bio jet kerosene, etc. However, biomass tends to flow in aggregates, which results in significant inhomogeneities in the amount of biomass flowing in front of a sensor at a given time, and there can be significant overlap in the material properties of dry vs wet biomass, leading to poor signal-to-noise ratio. We present a technique for identifying biomass moisture content using a series of acoustic pitch-catch measurements to quantify the sound speed and acoustic amplitude through the biomass, in conjunction with classical machine learning techniques , including Naive Bayes, Random Forest , and K-Nearest Neighbors classification. We amplify the differences between the acoustic measurements in different moisture levels by collecting a series of pulse-echo measurements, which we sort in order of ascending sound speed. We test the accuracy of the technique on experimentally-prepared batches of corn stover biomass with specified moisture levels, and measure the average error in the estimated moisture level as a function of the number of pitch-catch measurements used. We observe average estimation errors as low as 6.7\% by increasing the number of measurements and optimizing the hyperparameters. This work presents a novel method determining moisture content in flowing biomass with inhomogeneous flow. Additionally, this technique has application in optimizing biomass conversion processes, as well as other fields including, paper production, natural fiber processing, and mineral extraction.},
  archive      = {J_MLA},
  author       = {J. Greenhall and C. Pantea and P. Vakhlamov and E.S. Davis and T. Semelsberger},
  doi          = {10.1016/j.mlwa.2023.100476},
  journal      = {Machine Learning with Applications},
  pages        = {100476},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Data-driven acoustic measurement of moisture content in flowing biomass},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DS-GAU: Dual-sequences gated attention unit architecture for
text-independent speaker verification. <em>MLA</em>, <em>13</em>,
100469. (<a href="https://doi.org/10.1016/j.mlwa.2023.100469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-independent speaker verification provides people identified from their voice characteristics. In this paper, we propose a new method, Dual-Sequences Gate Attention Unit to improve the accuracy of a massive speaker verification system . Dual-Sequences Gate Attention Unit is based on the Gated Dual Attention Unit and the Gated Recurrent Unit. Two different inputs from the same source are the state pooling layer in the x-vector and the frame layer information in the x-vector. It is developed by applying the attention mechanism to the traditional Gated Recurrent Unit to enhance the learning ability of the x-vector system. The whole system follows the statistics pooling from each time-delay neural network layer of the x-vector baseline. It passes through the Dual-Sequences Gate Attention Unit layer to aggregate more information from the variant temporal context of input features while training at the frame level. We train our model on the Voxceleb2 and then evaluate the accuracy of Voxceleb1 and the Speakers in the Wild dataset for simulation. Finally, the system is compared with the x-vector, L-vector, and ETDNN-OPGRUs x-vector. There is an obvious improvement to our proposed method. Compared with the x-vector system, it shows that at least 17.5\% on Voxceleb1 and 0.5\% on Speakers in the Wild equal error rate improvement is achieved in the fusion system.},
  archive      = {J_MLA},
  author       = {Tsung-Han Tsai and Tran Dang Khoa},
  doi          = {10.1016/j.mlwa.2023.100469},
  journal      = {Machine Learning with Applications},
  pages        = {100469},
  shortjournal = {Mach. Learn. Appl.},
  title        = {DS-GAU: Dual-sequences gated attention unit architecture for text-independent speaker verification},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent framework for coagulant dosing optimization in
an industrial-scale seawater reverse osmosis desalination plant.
<em>MLA</em>, <em>12</em>, 100475. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and reliable desalination through seawater reverse osmosis (SWRO) mandates optimized pre-treatment strategies to minimize organic and inorganic fouling. Coagulation, the process of agglomerating colloidal particles using chemical coagulants, in combination with media filtration to reduce colloidal fouling on reverse osmosis membranes is commonly used in seawater pretreatment. Due to its inherent complexity and the absence of physical models to quantify the efficiency of coagulation, overdosing of coagulants is ubiquitously observed to maintain filtered water quality. To address this problem, we use Artificial neural networks (ANNs) to optimize coagulant dosing by predicting the SDI after chemical dosing. The model is developed by using large-scale plant data comprising of different seawater physical parameters and plant operational data including pH, SDI, turbidity, coagulant dosing rate, and flocculant dosing rate. By using feature engineering, selection, and our domain knowledge, new input parameters are derived, irrelevant parameters are eliminated, and these are used as inputs to train the model. The developed ANNs model achieved a prediction accuracy of 95\% also outperforms other machine learning methods, and upon industrialization it reduced annual coagulant consumption by 11.7\% when implemented in a commercial SWRO plant producing 216,000 m 3 /day of desalinated water.},
  archive      = {J_MLA},
  author       = {Muhammad Ghifari Ridwan and Thomas Altmann and Ahmed Yousry and Ratul Das},
  doi          = {10.1016/j.mlwa.2023.100475},
  journal      = {Machine Learning with Applications},
  pages        = {100475},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Intelligent framework for coagulant dosing optimization in an industrial-scale seawater reverse osmosis desalination plant},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based multi-target regression for
traffic-related air pollution forecasting. <em>MLA</em>, <em>12</em>,
100474. (<a href="https://doi.org/10.1016/j.mlwa.2023.100474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic-related air pollution (TRAP) remains one of the main contributors to urban pollution and its impact on climate change cannot be overemphasised. Experts in developed countries strive to make optimal use of traffic and air quality data to gain valuable insights into its effect on public health. Over the years, the research community has developed advanced methods of forecasting traffic-related pollution using several machine learning methods albeit with persistent accuracy and insufficient data challenges. Despite the potentials of emerging techniques such as multi-target deep neural network to achieve optimal solutions, they are yet to be fully exploited in the air quality space due to their complexity and unavailability of the right training data. It is to this end that this study investigates the impact of integrating an updated data set including road elevation, vehicle emissions factor and background maps with traffic flow, weather and pollution data on TRAP forecasting. To explore the robustness and adaptability of our methodology, the study was carried out in one major city (London), one smaller city (Newport) and one large town (Chepstow) in the United Kingdom. The forecasting task was modelled as a multi-target regression problem and experiments were carried out to predict N O 2 , P M 2 . 5 NO2,PM2.5 and P M 10 PM10 concentrations over multiple timesteps. Fastai’s tabular model was used alongside prophet’s time-series model and scikit-learn’s multioutputregressor for experimentation with fastai recording the overall best performance. Statistical tests run using Friedman and Wilcoxon test also revealed the significance of the fastai model with a p-values &amp;lt; 0.05. Finally, a model explanation tool was then used to reveal the most and least influential features from the newly curated data set. Results showed traffic count and speed were part of the most contributing features. This result demonstrates the impact of these and other introduced features on TRAP forecasting and will serve as a foundation for related studies.},
  archive      = {J_MLA},
  author       = {Taofeek Dolapo Akinosho and Muhammad Bilal and Enda Thomas Hayes and Anuoluwapo Ajayi and Ashraf Ahmed and Zaheer Khan},
  doi          = {10.1016/j.mlwa.2023.100474},
  journal      = {Machine Learning with Applications},
  pages        = {100474},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep learning-based multi-target regression for traffic-related air pollution forecasting},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Encoder–decoder-based image transformation approach for
integrating multiple spatial forecasts. <em>MLA</em>, <em>12</em>,
100473. (<a href="https://doi.org/10.1016/j.mlwa.2023.100473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the damage caused by heavy rainfall worsens, there is a growing demand for improved forecasts. One practical approach to address this issue is the linear integration of multiple existing forecasts, which allows for visualizing the contribution of each forecast at different locations. However, current methods such as arithmetic and Bayesian averages utilize a single weight shared across the entire space, making it difficult to account for local variations in importance. Additionally, while U-Net-based spatial forecasts have been proposed, they are limited to short-term predictions and do not facilitate the visualization of individual forecast contributions due to their non-linear processes. To overcome these challenges, we propose a new integration framework based on U-Net image transformation. This framework generates weight images that dynamically integrate forecasts based on both time and location. To effectively handle large and imbalanced precipitation data, we introduce novel extensions to the U-Net model. These extensions address heavily imbalanced precipitation data and enable position and time-dependent integration. Experimental results using real precipitation forecast data in Japan demonstrate that our proposed method outperforms existing integration methods.},
  archive      = {J_MLA},
  author       = {Hirotaka Hachiya and Yusuke Masumoto and Atsushi Kudo and Naonori Ueda},
  doi          = {10.1016/j.mlwa.2023.100473},
  journal      = {Machine Learning with Applications},
  pages        = {100473},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Encoder–decoder-based image transformation approach for integrating multiple spatial forecasts},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing atrial fibrillation detection accuracy: A wavelet
transform filtered single lead ECG signal analysis with artificial
neural networks and novel feature extraction. <em>MLA</em>, <em>12</em>,
100472. (<a href="https://doi.org/10.1016/j.mlwa.2023.100472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atrial Fibrillation (AF) is a common cardiac arrhythmia that can lead to fatal outcomes. Detecting AF early is crucial for improving patient outcomes and reducing complications. However, AF is difficult to detect due to asymptomatic cases, false-positive test results, and patient non-adherence. Therefore, it is essential to develop effective methods for early AF detection that can prolong patient life. This study proposes a novel approach for AF detection using a wavelet transform-based filtered ECG signal and a neural network with a novel feature extractor. Firstly, the wavelet transform is applied to filter the ECG signal; secondly, the novel feature extractor is designed to extract important features from the filtered ECG signal. The extracted features are then used as input to a neural network for AF classification. The proposed method is evaluated on two publicly available ECG datasets, and the results show a prominent accuracy of 96\% with a ROC is 0.95 and 86\% with a ROC is 0.84 for the two datasets, respectively. In addition, to enhance the model’s performance, we have utilized 5-fold cross-validation which achieved an accuracy of 0.99 for Dataset 1 and 0.90 for Dataset 2, respectively. Compared to existing AF detection methods, our approach provides a promising solution that can improve the early diagnosis and treatment of AF. This study presents a new avenue for AF detection and has the potential to advance the field of cardiac arrhythmia detection.},
  archive      = {J_MLA},
  author       = {D.U.S. Duranta and Md Shahin Ali and Abhilash Arjan Das and Md Mahbubur Rahman and Md Manjurul Ahsan and Md Sipon Miah and Md Khairul Islam},
  doi          = {10.1016/j.mlwa.2023.100472},
  journal      = {Machine Learning with Applications},
  pages        = {100472},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing atrial fibrillation detection accuracy: A wavelet transform filtered single lead ECG signal analysis with artificial neural networks and novel feature extraction},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Marine predators algorithm: A comprehensive review.
<em>MLA</em>, <em>12</em>, 100471. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marine predators algorithm (MPA) is a recently proposed metaheuristic algorithm that mimics the marine predators behavior when attacking their preys. Recently, the MPA has been broadly employed to tackle numerous optimization problems in various research areas and has confirmed its supremacy over a large number of the metaheuristic algorithms regard to convergence speed and accuracy thanks to its simplicity, flexible implementation and few adjustable parameters requirements. A comprehensive review of the MPA is presented in this paper along with its variants such as binary, discrete, modifications, hybridizations, chaotic, quantum and multi-objective versions. This paper also reviews various applications of MPA in electrical engineering, computer science, medicine, etc. Moreover, further research directions for MPA are suggested. The source code of the MPA can be found at: http://www.alimirjalili.com/MPA.html .},
  archive      = {J_MLA},
  author       = {Sylvère Mugemanyi and Zhaoyang Qu and François Xavier Rugema and Yunchang Dong and Lei Wang and Christophe Bananeza and Arcade Nshimiyimana and Emmanuel Mutabazi},
  doi          = {10.1016/j.mlwa.2023.100471},
  journal      = {Machine Learning with Applications},
  pages        = {100471},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Marine predators algorithm: A comprehensive review},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning for anomaly detection in log data: A survey.
<em>MLA</em>, <em>12</em>, 100470. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic log file analysis enables early detection of relevant incidents such as system failures. In particular, self-learning anomaly detection techniques capture patterns in log data and subsequently report unexpected log event occurrences to system operators without the need to provide or manually model anomalous scenarios in advance. Recently, an increasing number of approaches leveraging deep learning neural networks for this purpose have been presented. These approaches have demonstrated superior detection performance in comparison to conventional machine learning techniques and simultaneously resolve issues with unstable data formats . However, there exist many different architectures for deep learning and it is non-trivial to encode raw and unstructured log data to be analyzed by neural networks. We therefore carry out a systematic literature review that provides an overview of deployed models, data pre-processing mechanisms, anomaly detection techniques, and evaluations. The survey does not quantitatively compare existing approaches but instead aims to help readers understand relevant aspects of different model architectures and emphasizes open issues for future work.},
  archive      = {J_MLA},
  author       = {Max Landauer and Sebastian Onder and Florian Skopik and Markus Wurzenberger},
  doi          = {10.1016/j.mlwa.2023.100470},
  journal      = {Machine Learning with Applications},
  pages        = {100470},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep learning for anomaly detection in log data: A survey},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-cancer classification; an analysis of neural network
models. <em>MLA</em>, <em>12</em>, 100468. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer identification is generally framed as binary classification , normally discrimination of a control group from a single cancer group. However, such models lack any cancer-specific information, as they are only trained on one cancer type. The models fail to account for competing cancer risks. Pan-cancer evaluation requires a model trained on multiple cancer types, and controls, simultaneously, so that a physician can be directed to the correct area of the body for further testing. We investigate neural network models to address multi-cancer classification problems across several data types commonly applied in cancer prediction, including circulating miRNA expression, protein, and mRNA. In particular, we present an analysis of neural network depth and type, and investigate how this relates to classification performance. In our comparisons, we include several state-of-the-art neural networks from the literature. We provide details on the optimal network depth and type, the activation functions and layer sizes. : Our analysis evidences that shallow (i.e., 1 or 2 layer), feed-forward neural network architectures offer greater performance in terms of mean sensitivity and precision when compared to deeper (i.e., &gt; 2 &amp;gt;2 layer) feed-forward models, Convolutional Neural Network (CNN), and Graph CNN (GCNN) architectures, across a range of measurement technologies in cancer prediction (e.g., miRNA, mRNA and protein). We also discover that hyperbolic tangent activation functions offer the most consistent performance, and the optimal feed-forward models have descending layer size structure.},
  archive      = {J_MLA},
  author       = {James W. Webber and Kevin Elias},
  doi          = {10.1016/j.mlwa.2023.100468},
  journal      = {Machine Learning with Applications},
  pages        = {100468},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multi-cancer classification; an analysis of neural network models},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ForeTiS: A comprehensive time series forecasting framework
in python. <em>MLA</em>, <em>12</em>, 100467. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Summary: Time series forecasting is a research area with applications in various domains, nevertheless without yielding a predominant method so far. We present ForeTiS , a comprehensive and open source Python framework that allows rigorous training, comparison, and analysis of state-of-the-art time series forecasting approaches. Our framework includes fully automated yet configurable data preprocessing and feature engineering. In addition, we use advanced Bayesian optimization for automatic hyperparameter search. ForeTiS is easy to use, even for non-programmers, requiring only a single line of code to apply state-of-the-art time series forecasting. Various prediction models, ranging from classical forecasting approaches to machine learning techniques and deep learning architectures, are already integrated. More importantly, as a key benefit for researchers aiming to develop new forecasting models, ForeTiS is designed to allow for rapid integration and fair benchmarking in a reliable framework. Thus, we provide a powerful framework for both end users and forecasting experts. Availability: ForeTiS is available at https://github.com/grimmlab/ForeTiS . We provide a setup using Docker , as well as a Python package at https://pypi.org/project/ForeTiS/ . Extensive online documentation with hands-on tutorials and videos can be found at https://foretis.readthedocs.io .},
  archive      = {J_MLA},
  author       = {Josef Eiglsperger and Florian Haselbeck and Dominik G. Grimm},
  doi          = {10.1016/j.mlwa.2023.100467},
  journal      = {Machine Learning with Applications},
  pages        = {100467},
  shortjournal = {Mach. Learn. Appl.},
  title        = {ForeTiS: A comprehensive time series forecasting framework in python},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time to die 2: Improved in-game death prediction in dota 2.
<em>MLA</em>, <em>12</em>, 100466. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitive video game playing, an activity called esports, is increasingly popular to the point that there are now many professional competitions held for a variety of games. These competitions are broadcast in a professional manner similar to traditional sports broadcasting. Esports games are generally fast paced, and due to the virtual nature of these games, camera positioning can be limited. Therefore, knowing ahead of time where to position cameras, and what to focus a broadcast and associated commentary on, is a key challenge in esports reporting. This gives rise to moment-to-moment prediction within esports matches which can empower broadcasters to better observe and process esports matches. In this work we focus on this moment-to-moment prediction and in particular present techniques for predicting if a player will die within a set number of seconds for the esports title Dota 2 . A player death is one of the most consequential events in Dota 2 . We train our model on ‘telemetry’ data gathered directly from the game itself, and position this work as a novel extension of our previous work on the challenge. We use an enhanced dataset covering 9,822 Dota 2 matches. Since the publication of our previous work, new dataset parsing techniques developed by the WEAVR project enable the model to track more features, namely player status effects, and more importantly, to operate in real time. Additionally, we explore two new enhancements to the original model: one data-based extension and one architectural. Firstly we employ learnt embeddings for categorical features, e.g. which in game character a player has selected, and secondly we explicitly model the temporal element of our telemetry data using recurrent neural networks . We find that these extensions and additional features all aid the predictive power of the model achieving an F1 score of 0.54 compared to 0.17 for our previous model (on the new data). We improve this further by experimenting with the length of the time-series in the input data and find using 15 time steps further improves the F1 score to 0.62. This compares to F1 of 0.1 for a standard RNN on the same task. Additionally a deeper analysis of the Time to Die model is carried out to assess its suitability as a broadcast aid.},
  archive      = {J_MLA},
  author       = {Charles Ringer and Sondess Missaoui and Victoria J. Hodge and Alan Pedrassoli Chitayat and Athanasios Kokkinakis and Sagarika Patra and Simon Demediuk and Alvaro Caceres Munoz and Oluseji Olarewaju and Marian Ursu and Ben Kirman and Jonathan Hook and Florian Block and Anders Drachen and James Alfred Walker},
  doi          = {10.1016/j.mlwa.2023.100466},
  journal      = {Machine Learning with Applications},
  pages        = {100466},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Time to die 2: Improved in-game death prediction in dota 2},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid deep learning and GARCH-family models for forecasting
volatility of cryptocurrencies. <em>MLA</em>, <em>12</em>, 100465. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of Deep Learning and GARCH-type models has been proved to be superior to the single models in forecasting of volatility in various markets such as energy, main metals, and especially stock markets. To verify this hypothesis for cryptocurrencies market, we constructed various Deep Learning models based on Feed Forward Neural Networks (DFFNNs) and Long Short-Term Memory (LSTM) networks and evaluated their performance in forecasting the volatility of 27 cryptocurrencies. Then, different hybrid models were built in which the outputs of three GARCH-type models, namely GARCH, EGARCH, and APGARCH, with three different assumptions for the residuals’ distribution were fed into the DFFNN and LSTM networks. In other words, GARCH-type models were utilized as feature extractors and the deep learning models leveraged a sequence of extracted features as their inputs to produce the volatility of the next day. Our findings revealed that not only the deep learning models improve the forecasts of GARCH-type models with any distribution assumption, the forecasts of GARCH-type models as informative features can significantly increase the predictive power of the studied deep learning models; namely, the DFFNN and LSTM models.},
  archive      = {J_MLA},
  author       = {Bahareh Amirshahi and Salim Lahmiri},
  doi          = {10.1016/j.mlwa.2023.100465},
  journal      = {Machine Learning with Applications},
  pages        = {100465},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Hybrid deep learning and GARCH-family models for forecasting volatility of cryptocurrencies},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loss landscape engineering via data regulation on PINNs.
<em>MLA</em>, <em>12</em>, 100464. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-Informed Neural Networks have shown unique utility in parameterising the solution of a well-defined partial differential equation using automatic differentiation and residual losses. Though they provide theoretical guarantees of convergence, in practice the required training regimes tend to be exacting and demanding. Through the course of this paper, we take a deep dive into understanding the loss landscapes associated with a PINN and how that offers some insight as to why PINNs are fundamentally hard to optimise for. We demonstrate how PINNs can be forced to converge better towards the solution, by way of feeding in sparse or coarse data as a regulator. The data regulates and morphs the topology of the loss landscape associated with the PINN to make it easily traversable for the minimiser. Data regulation of PINNs helps ease the optimisation required for convergence by invoking a hybrid unsupervised–supervised training approach, where the labelled data pushes the network towards the vicinity of the solution, and the unlabelled regime fine-tunes it to the solution.},
  archive      = {J_MLA},
  author       = {Vignesh Gopakumar and Stanislas Pamela and Debasmita Samaddar},
  doi          = {10.1016/j.mlwa.2023.100464},
  journal      = {Machine Learning with Applications},
  pages        = {100464},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Loss landscape engineering via data regulation on PINNs},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient-free kernel conditional stein discrepancy goodness
of fit testing. <em>MLA</em>, <em>12</em>, 100463. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a gradient-free statistical goodness-of-fit test for determining if a joint sample ( x i , y i ) (xi,yi) is drawn from p ( y | x ) π x p(y|x)πx for some density π x πx given a conditional distribution. This test is an alternative to Kernel Conditional Stein Discrepancy, which require the computation of model derivatives and are therefore impractical for complex statistical models. Our method, known as Gradient-Free Kernel Conditional Stein Discrepancy, does not require the calculation of derivatives, this makes it a great tool for tackling difficult problems such as evaluating the performance of generative models . It is able to detect convergence and divergence with the same level of accuracy as the gradient-based method. We also discuss the application of this test in importance sampling and compare its performance with two other conventional methods.},
  archive      = {J_MLA},
  author       = {Elham Afzali and Saman Muthukumarana},
  doi          = {10.1016/j.mlwa.2023.100463},
  journal      = {Machine Learning with Applications},
  pages        = {100463},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Gradient-free kernel conditional stein discrepancy goodness of fit testing},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing MEG interference using machine learning.
<em>MLA</em>, <em>12</em>, 100462. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetoencephalography (MEG) is a non-invasive imaging technique that measures the naturally occurring electrical activity of the brain. A MEG signal contains important information about the health of the brain and can be used to detect any abnormalities that could point to a neurological disease. MEG sensors are very sensitive, and so they are very susceptible to noise. Denoising these signals efficiently will make analyzing the data much easier. In this paper, we have utilized several components in order to obtain, denoise, and then store MEG data. First, data is submitted into a React application which then stores the raw data, along with user information into a MYSQL database. Then, the data passes through a 9-layer Denoising Autoencoder (DAE). Afterwards the output is then stored in a separate MYSQL database and its noisy version. The SNR of a signal after passing it through the model was able to be increased by a maximum of 88\%. On average, the model was able to increase the SNR by 45.63\%. Besides providing neurologists valuable information regarding the brain, it also serves as an easily accessible tool for viewing and cleaning MEG data.},
  archive      = {J_MLA},
  author       = {Sammi Hamdan and Kyle DuBray and Jordan Treutel and Rajendra Paudyal and Khem Poudel},
  doi          = {10.1016/j.mlwa.2023.100462},
  journal      = {Machine Learning with Applications},
  pages        = {100462},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Reducing MEG interference using machine learning},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human activity classification using deep learning based on
3D motion feature. <em>MLA</em>, <em>12</em>, 100461. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity classification is needed to support various fields. The health sector, for example, requires the ability to monitor the activities of patients, the elderly, or people with special needs to provide services with fast response as needed. In the traditional classification model , the steps taken to start from the input of data and then proceed with feature extraction, representation, classifier and end with semantic labels . The classification stage uses Convolutional Neural Network (CNN) deep learning to data input, CNN, and semantic labels. This paper proposes a novel method of classifying nine activities based on the movement features of changes in joint distance using Euclidean on the order of frames in each activity segment as input to the CNN model. This study’s motion feature extraction technique was tested using various window sizes to obtain the best classification accuracy . The experimental results show that the selection of window size 16 on the motion feature setting will produce an optimal model accuracy of 94.08\% in classifying human activities.},
  archive      = {J_MLA},
  author       = {Endang Sri Rahayu and Eko Mulyanto Yuniarno and I. Ketut Eddy Purnama and Mauridhi Hery Purnomo},
  doi          = {10.1016/j.mlwa.2023.100461},
  journal      = {Machine Learning with Applications},
  pages        = {100461},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Human activity classification using deep learning based on 3D motion feature},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ViCTer: A semi-supervised video character tracker.
<em>MLA</em>, <em>12</em>, 100460. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video character tracking problem refers to tracking certain characters of interest in the video and returning the appearing time slots for those characters. Solutions to this problem can be applied in various video-analysis-related areas, such as movie analysis and automatic video clipping. However, there are very few researches investigating this problem and there are no existing relevant benchmark datasets available. In this paper, we design a novel model 1 to solve this problem by combining a semi-supervised face recognition network and a multi-human tracker. For the face recognition network, we propose a semi-supervised learning method to fully leverage the unlabeled images in the video, thus reducing the required number of labeled face images. Triplet loss is also used during the training to better distinguish among inter-class samples. However, a single face recognition network is insufficient for video character tracking since people do not always show their frontal faces, or sometimes their faces are blocked by some obstacles. Therefore, a multi-human tracker is integrated into the model to address those problems. Additionally, we collect a dataset for the video character tracking problem, Character Face in Video, which can support various experiments for evaluating video character tracker performance. Experiments show that the proposed semi-supervised face recognition model can achieve more than 98.5\% recognition accuracy, and our video character tracker can track in near-real-time and achieve 70\% ∼ ∼ 80\% average intersection-over-union tracking accuracy on the dataset.},
  archive      = {J_MLA},
  author       = {Zilinghan Li and Xiwei Wang and Zhenning Zhang and Volodymyr Kindratenko},
  doi          = {10.1016/j.mlwa.2023.100460},
  journal      = {Machine Learning with Applications},
  pages        = {100460},
  shortjournal = {Mach. Learn. Appl.},
  title        = {ViCTer: A semi-supervised video character tracker},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shrinkage heteroscedastic discriminant algorithms for
classifying multi-class high-dimensional data: Insights from a national
health survey. <em>MLA</em>, <em>12</em>, 100459. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical data applications, there are often a large number of pre-processed heteroscedastic features. Discriminant analysis is a standard statistical learning method that is useful for classifying such multivariate features. It is well known in literature that the Linear Discriminant Analysis (LDA) is quite sub-optimal for the analysis of high-dimensional heteroscedastic data because of the inherent singularity and instability of the within-class variance. However, shrinkage discriminant analysis (SDA) and its variants often perform better due to its robustness against inherent multicollinearity and heteroscedasticity. In this article, we propose some newly modified discriminant classification algorithms based on the SDA and compare their sensitivities with those of other competing algorithms. The empirical application show that the proposed algorithms perform moderately well for datasets with high dimensions and unequal co-variance structures when applied to simulated and nutrition data with inherent heteroscedasticity and outliers. The sensitivity and precision of the algorithms for the target classes ranges from 70\%–100\%. The balanced accuracy of all the algorithms ranges from 50 to 75\% for the three-class problem considered. Heteroscedastic discriminant algorithm performs moderately well with high sensitivity for classifying health data with high and low dimensions.},
  archive      = {J_MLA},
  author       = {Olushina Olawale Awe and Natisha Dukhi and Ronaldo Dias},
  doi          = {10.1016/j.mlwa.2023.100459},
  journal      = {Machine Learning with Applications},
  pages        = {100459},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Shrinkage heteroscedastic discriminant algorithms for classifying multi-class high-dimensional data: Insights from a national health survey},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extremely randomised trees machine learning model for
electricity theft detection. <em>MLA</em>, <em>12</em>, 100458. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity ranks among the world’s most plundered commodities. The fraudulent act of acquiring electrical power without paying for it is termed electricity theft . Electricity theft is captured in power distribution systems as non-technical losses (NTL), representing a major loss in revenue for power utility companies. Electricity theft has far-reaching financial consequences owing to unrealised revenue, and this has a knock-on effect on both developed and developing countries because electricity represents a major part of a country’s GDP and facilitates other industries. AMI-based smart energy meters (SM) gather large amounts of electricity consumption (EC) data that power utilities can utilise to monitor and detect fraudulent customers. This EC data is fed to a machine learning (ML) based electricity theft detection model to learn the behaviour of fraudulent customers. However, existing ML-based electricity theft detection (ETD) models do not produce the best outcomes because of; consecutive missing values in EC datasets, data class imbalance problems , inappropriate hyperparameter tuning of ML models, etc. This research introduces an ETD model using an extremely randomised trees classifier to detect electricity theft in smart grids efficiently. SMOTE Tomek sampling is used to deal with the data class imbalance, and the grid search optimisation technique is employed to optimise the hyperparameters of the proposed model. The proposed model shows its capacity to detect electricity theft by obtaining 98\%, 95.06\%, 98\%, 97\%, 98\%, and 99.65\% accuracy, Matthew’s correlation coefficient, detection rate, Precision, F1-score, and area under the curve receiver operating characteristic, respectively.},
  archive      = {J_MLA},
  author       = {Stanley Yaw Appiah and Emmanuel Kofi Akowuah and Valentine Chibueze Ikpo and Albert Dede},
  doi          = {10.1016/j.mlwa.2023.100458},
  journal      = {Machine Learning with Applications},
  pages        = {100458},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Extremely randomised trees machine learning model for electricity theft detection},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A machine learning approach for hierarchical classification
of software requirements. <em>MLA</em>, <em>12</em>, 100457. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of software requirements into different categories is a critically important task in requirements engineering (RE). Developing machine learning (ML) approaches for requirements classification has attracted great interest in the RE community since the 2000s. This paper aims to address two related problems that have been challenging real-world applications of ML approaches: the problems of class imbalance and high dimensionality with low sample size data (HDLSS). These problems can greatly degrade the classification performance of ML methods. The paper proposes HC4RC , a novel ML approach for multiclass classification of requirements. HC4RC solves the aforementioned problems through semantic-role based feature selection, dataset decomposition and hierarchical classification. We experimentally compare the effectiveness of HC4RC with three closely related approaches — two of which are based on a traditional statistical classification model whereas one using an advanced deep learning model . Our experiment shows: (1) The class imbalance and HDLSS problems present a challenge to both traditional and advanced ML approaches. (2) The HC4RC approach is simple to use and can effectively address the class imbalance and HDLSS problems compared to similar approaches. This paper makes an important practical contribution to addressing the class imbalance and HDLSS problems in multiclass classification of software requirements.},
  archive      = {J_MLA},
  author       = {Manal Binkhonain and Liping Zhao},
  doi          = {10.1016/j.mlwa.2023.100457},
  journal      = {Machine Learning with Applications},
  pages        = {100457},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A machine learning approach for hierarchical classification of software requirements},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep echo state networks in data marketplaces. <em>MLA</em>,
<em>12</em>, 100456. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Marketplaces are the digital platform for data buyers and data sellers to trade information as valuable products or items. The expectation taken for granted from the users of a data marketplace is the truth of the exchanged information. However, the trade of factual data also means the marketable product is no longer unique but a series of replicas. If every user within the data marketplace owns the same information, this data eventually becomes valueless. There are specific instances where the traded products are sought to be always unique, for instance predictions or digital art. This article applies Deep Echo State Networks (ESNs) in data marketplaces that map tradeable data into a larger dimensional space via the dynamics of reservoirs with fixed and non-linear properties. These reservoirs generate unique tradeable data products that cannot be replicated, therefore ensuring its exclusivity and commercial value. The validation results show that ESNs can also be applied to generate random tradeable products in different dimensional spaces. Specifically, the reservoir with its associated neural perturbation emulates a digital creator that generates unique and exclusive content based on 1D functions and 2D images.},
  archive      = {J_MLA},
  author       = {Will Serrano},
  doi          = {10.1016/j.mlwa.2023.100456},
  journal      = {Machine Learning with Applications},
  pages        = {100456},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep echo state networks in data marketplaces},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling on a budget: Avoiding stale recommendations with
timely updates. <em>MLA</em>, <em>11</em>, 100455. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems usually create static models from historical data. Due to concept drift and changes in the environment, such models are doomed to become stale, which causes their performance to degrade. In live production environments, models are therefore typically retrained at fixed time-intervals. Of course, every retraining comes at a significant computational cost, making very frequent model updates unrealistic in practice. In some cases, the cost is worth it, but in other cases an update could be redundant and the cost an unnecessary loss. The research question then consists of finding an acceptable update schedule for your recommendation system, given a limited budget. This work provides a pragmatic analysis of model staleness for a variety of collaborative filtering algorithms in news and retail domains, where concept drift is a known impediment. We highlight that the rate at which models become stale is highly dependent on the environment they perform in and that this property can be derived from data. These findings are corroborated by empirical observations from four large-scale online experiments. Instead of retraining at regular intervals, we propose an adaptive scheduling method that aims to maximise the accuracy of the recommendations within a fixed resource budget. Offline experiments show that our proposed approach improves recommendation performance while keeping the cost constant. Our findings can guide practitioners to spend their available resources more efficiently.},
  archive      = {J_MLA},
  author       = {Robin Verachtert and Olivier Jeunen and Bart Goethals},
  doi          = {10.1016/j.mlwa.2023.100455},
  journal      = {Machine Learning with Applications},
  pages        = {100455},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Scheduling on a budget: Avoiding stale recommendations with timely updates},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of deep convolutional networks for improved risk
assessments of post-wildfire drinking water contamination. <em>MLA</em>,
<em>11</em>, 100454. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change continues to increase the frequency of wildfires in the western United States, driven by land use change and prolonged and intensified droughts. Simultaneously, an ongoing extension of communities into wildland areas has been observed over the last decades, increasing the wildfire threat to populations in the wildland–urban interface. In recent years, the problem of increased levels of volatile organic compounds in drinking water distribution systems after wildfires has gained attention as it poses a significant health threat to communities in those wildfire-prone areas. No adequate deterministic process models to predict the risk of high levels of volatile organic compounds in drinking water distribution systems after wildfires are available at this point, leaving data-driven machine learning approaches as solutions for addressing this problem. Here we build on a preceding study and enhance the assessment of maximum contamination exceedance probability predictions by applying deep convolutional neural networks trained with water samples from communities affected by wildfires in California and Oregon. We used satellite surface reflectance imagery in combination with gridded information of topography, fuel load, meteorology, and infrastructure as model inputs. The results show that the ensemble of deep convolutional neural networks increases the accuracy of predictions of post-wildfire water contamination by 4.1\% compared to preceding models based on Bayesian regularized shallow feedforward networks , reaching an overall accuracy of 92\% for the test dataset. The method provides a practical approach for emergency planning and pre-fire allocation of resources to support the provision of safe drinking water for wildfire-prone communities.},
  archive      = {J_MLA},
  author       = {Andres Schmidt and Lisa M. Ellsworth and Jenna H. Tilt and Mike Gough},
  doi          = {10.1016/j.mlwa.2023.100454},
  journal      = {Machine Learning with Applications},
  pages        = {100454},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Application of deep convolutional networks for improved risk assessments of post-wildfire drinking water contamination},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting firm performance and size using machine learning
with a bayesian perspective. <em>MLA</em>, <em>11</em>, 100453. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the issue of predicting the financial performance of firms in registered manufacturing in developing countries using machine learning methods along with economic theory to explain the findings. While literature suggests that predictability of top line measures related to sales is lower compared to bottom line measures such as net profits for small informal establishments in developing countries, we find the opposite holds true for firms in registered manufacturing in the food processing industry in India based on the results from machine learning techniques of Bayesian additive regression trees (BART), boosted trees, bootstrap forests, and regression tree algorithms. BART models in validation outperformed the other algorithms in predictability of the dependent variables. Across ten validation studies, BART had an average R 2 ranging from 0.922 to 0.934 and boosted tree models had an average R 2 ranging from 0.873 to 0.905 for predicting sales. A key significant independent variable for predicting sales across all categories and algorithms was real raw material expenses explaining approximately 83\% to 88\% of the total sums of squares in all validations. This is in line with the realities of the food processing industry, which is intensive in its raw material usage. The dependent variable ‘profits’ (as measured by real profits before depreciation, interest, taxes and amortization, ‘real_pbdita’) was more difficult to predict relative to sales. BART models again outperformed the other algorithms in validation with an average R 2 ranging from 0.745 to 0.818. Key significant variables in the models were more diverse where raw material expenses, compensation to employees, and net- or total-fixed assets explained the largest proportions of the total sums of squares. The results from a machine learning approach with a Bayesian perspective can enhance the understanding of the mechanisms that translate sales into profits for registered manufacturing, thereby aiding policy-making for small businesses in the formal sector in developing countries.},
  archive      = {J_MLA},
  author       = {Debdatta Saha and Timothy M. Young and Jessica Thacker},
  doi          = {10.1016/j.mlwa.2023.100453},
  journal      = {Machine Learning with Applications},
  pages        = {100453},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Predicting firm performance and size using machine learning with a bayesian perspective},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Text augmentation using a graph-based approach and clonal
selection algorithm. <em>MLA</em>, <em>11</em>, 100452. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotated data is critical for machine learning models, but producing large amounts of data with high-quality labeling is a time-consuming and labor-intensive process. Natural language processing (NLP) and machine learning models have traditionally relied on the labels given by human annotators with varying degrees of competency, training, and experience. These kinds of labels are incredibly problematic because they are defined and enforced by arbitrary and ambiguous standards. In order to solve these issues of insufficient high-quality labels, researchers are now investigating automated methods for enhancing training and testing data sets . In this paper, we demonstrate how our proposed method improves the quality and quantity of data in two cybersecurity problems (fake news identification &amp; sensitive data leak) by employing the clonal selection algorithm (CLONALG) and abstract meaning representation (AMR) graphs, and how it improves the performance of a classifier by at least 5\% on two datasets.},
  archive      = {J_MLA},
  author       = {Hadeer Ahmed and Issa Traore and Mohammad Mamun and Sherif Saad},
  doi          = {10.1016/j.mlwa.2023.100452},
  journal      = {Machine Learning with Applications},
  pages        = {100452},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Text augmentation using a graph-based approach and clonal selection algorithm},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised visual anomaly detection based on
convolutional autoencoder and transfer learning. <em>MLA</em>,
<em>11</em>, 100451. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep neural networks have shown that reconstruction-based methods using autoencoders have potential for anomaly detection in visual inspection tasks. However, there are challenges when applying these methods to high-resolution images, such as the need for large network training and computation of anomaly scores. Autoencoder-based methods detect anomalies by comparing an input image to its reconstruction in pixel space, which can result in poor performance due to imperfect reconstruction. In this paper, we propose a method to address these challenges by using a conditional patch-based convolutional autoencoder and one-class deep feature classification. We train an autoencoder using only normal images and compute anomaly maps as the difference between the input and output of the autoencoder. We then embed these anomaly maps using a pretrained convolutional neural network feature extractor. Using the deep feature embeddings from the anomaly maps of training samples, we train a one-class classifier to compute an anomaly score for an unseen sample. A simple threshold-based criterion is used to determine if the unseen sample is anomalous or not. We compare our proposed algorithm to state-of-the-art methods on multiple challenging datasets, including a dataset of zipper cursors and eight datasets from the MVTec dataset collection. We find that our approach outperforms alternatives in all cases, achieving an average precision score of 94.77\% for zipper cursors and 96.51\% for MVTec datasets.},
  archive      = {J_MLA},
  author       = {Jamal Saeedi and Alessandro Giusti},
  doi          = {10.1016/j.mlwa.2023.100451},
  journal      = {Machine Learning with Applications},
  pages        = {100451},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Semi-supervised visual anomaly detection based on convolutional autoencoder and transfer learning},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated recognition of individual performers from
de-identified video sequences. <em>MLA</em>, <em>11</em>, 100450. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of individual humans from RGB image data is well-established. However, in many domains, such as in healthcare or applications involving children, ethical issues have been raised around using traditional RGB image data because individuals can be identified from these data. The widespread availability of reliable depth data, and the associated human skeleton data derived from these data, presents an opportunity to differentiate between individuals while potentially avoiding individually identifiable features. Using skeleton data only, we developed a unique 20-dimensional bone segment length feature vector for 1,761 trials (1,759,980 image frames) of data, captured from 14 participants who engaged in a one-hour group intervention playing Xbox One Kinect Bowling twice-weekly for 24 weeks. We then evaluated our novel feature using representative batch processing (k-nearest neighbour) and real-time (multi-layer perceptron) models, validated against manually-labelled ground-truth data. Our results suggest that our skeleton feature can differentiate between instances (i.e., individuals) with an accuracy over all participants of 100\% for batch processing and 96.57\% in real-time, and deals well with class imbalances. Our results suggest that we can reliably differentiate between individual persons using only skeleton data derived from depth image data in medical research.},
  archive      = {J_MLA},
  author       = {Zizui Chen and Stephen Czarnuch and Erica Dove and Arlene Astell},
  doi          = {10.1016/j.mlwa.2023.100450},
  journal      = {Machine Learning with Applications},
  pages        = {100450},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Automated recognition of individual performers from de-identified video sequences},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implementing associative memories by echo state network for
the applications of natural language processing. <em>MLA</em>,
<em>11</em>, 100449. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Echo State Network (ESN) based associative memories and their applications to English words. Among the papers describing ESN applications in general, some papers deal with natural language processing (NLP). Those NLP papers utilize an ESN’s property of temporal signal learning to translate an English sentence into its corresponding predicate logic formula. In practical NLP applications, input sentences often include misspelled or undefined words. To cope with such problems, we constructed a training algorithm to realize an ESN-based associative memory. This algorithm can be used in two ways: auto-associative one when the input and output patterns are the same in each pair of the training data, and hetero-associative one otherwise. In this paper, we firstly show the basic performance of an ESN-based auto-associative memory by applying it to two-dimensional images. We made sure that the performance is improved by averaging the result of multiple runs. Secondly, we describe the performances of ESN-based auto- and hetero-associative memories when being applied to English words. The former is to recall correct English words from incomplete spelling ones, and the latter is to chain two different English words of an input and its corresponding output. The former performance in recalling 26 incomplete words can be improved by averaging the outputs of multiple runs, and the improved success rate of correct words is around 65\%. The latter performance in chaining 26 words attains the success rate of 83.4\%.},
  archive      = {J_MLA},
  author       = {Hiroshi Kage},
  doi          = {10.1016/j.mlwa.2023.100449},
  journal      = {Machine Learning with Applications},
  pages        = {100449},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Implementing associative memories by echo state network for the applications of natural language processing},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable trading pattern designed for machine learning
applications. <em>MLA</em>, <em>11</em>, 100448. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial markets are a source of non-stationary multidimensional time series which has been drawing attention for decades. Each financial instrument has its specific changing-over-time properties, making its analysis a complex task. Hence, improvement of understanding and development of more informative, generalisable market representations are essential for the successful operation in financial markets, including risk assessment, diversification, trading, and order execution . In this study, we propose a volume-price-based market representation for making financial time series more suitable for machine learning pipelines. We use a statistical approach for evaluating the representation. Through the research questions, we investigate, i) whether the proposed representation allows any improvement over the baseline (always-positive) performance; ii) whether the proposed representation leads to increased performance over the price levels market pattern; iii) whether the proposed representation performs better on the liquid markets, and iv) whether SHAP feature interactions are reliable to be used in the considered setting. Our analysis shows that the proposed volume-based method allows successful classification of the financial time series patterns, and also leads to better classification performance than the price levels-based method, excelling specifically on more liquid financial instruments. Finally, we propose an approach for obtaining feature interactions directly from tree-based models and compare the outcomes to those of the SHAP method. This results in the significant similarity between the two methods, hence we claim that SHAP feature interactions are reliable to be used in the setting of financial markets.},
  archive      = {J_MLA},
  author       = {Artur Sokolovsky and Luca Arnaboldi and Jaume Bacardit and Thomas Gross},
  doi          = {10.1016/j.mlwa.2023.100448},
  journal      = {Machine Learning with Applications},
  pages        = {100448},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Interpretable trading pattern designed for machine learning applications},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Portfolio optimization based on neural networks
sensitivities from assets dynamics respect common drivers. <em>MLA</em>,
<em>11</em>, 100447. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework for modeling asset and portfolio dynamics, incorporating this information into portfolio optimization. We define drivers for asset and portfolio dynamics and their optimal selection. For this framework, we introduce the Commonality Principle, providing a solution for the optimal selection of portfolio drivers as the common drivers. Portfolio constituent dynamics are modeled by Partial Differential Equations , and solutions approximated with neural networks . Sensitivities with respect to the common drivers are obtained via Automatic Adjoint Differentiation. Information on asset dynamics is incorporated via sensitivities into portfolio optimization. Portfolio constituents are embedded into the space of sensitivities with respect to their common drivers, and a distance matrix in this space called the Sensitivity matrix is used to solve the convex optimization for diversification. The sensitivity matrix measures the similarity of the projections of portfolio constituents on a vector space formed by common drivers’ returns and is used to optimize for diversification on both idiosyncratic and systematic risks while adding directionality and future behavior information via returns dynamics. For portfolio optimization, we perform hierarchical clustering on the sensitivity matrix. The clustering tree is used for recursive bisection to obtain the weights. To the best of the author’s knowledge, this is the first time that sensitivities’ dynamics approximated with neural networks have been used for portfolio optimization. Secondly, that hierarchical clustering on a matrix of sensitivities is used to solve the convex optimization problem and incorporate the hierarchical information of these sensitivities. Thirdly, public and listed variables can be used to obtain maximum idiosyncratic and systematic diversification by means of the sensitivity space with respect to optimal portfolio drivers. We reach over-performance in many experiments with respect to all other out-of-sample methods for different markets and real datasets. We also include a recipe for the methodology to increase performance even further, and tackle the main issues in portfolio management such as regimes, non-stationarity, overfitting, and selection bias.},
  archive      = {J_MLA},
  author       = {Alejandro Rodriguez Dominguez},
  doi          = {10.1016/j.mlwa.2022.100447},
  journal      = {Machine Learning with Applications},
  pages        = {100447},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Portfolio optimization based on neural networks sensitivities from assets dynamics respect common drivers},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep generative modelling of aircraft trajectories in
terminal maneuvering areas. <em>MLA</em>, <em>11</em>, 100446. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airspace design is subject to a multitude of constraints, which are mainly driven by the concern to keep the risk of mid-air collision below a target level of safety. For that purpose, Monte Carlo simulation methods can be applied to estimate aircraft conflict probability but require the accurate generation of artificial trajectories. Generative models allow to generate an infinite number of trajectories for air traffic procedures where only few observations are available. The generated trajectories must not only resemble observed trajectories in terms of statistical distributions but they should stay flyable and consider uncertainty due to weather, air traffic control, aircraft performances, or human factors. This paper focuses on the generation problem, and its main contribution lies in the adaptation of the Variational Autoencoder structure to the problem of 4-dimensional aircraft trajectories modelling using Temporal Convolutional Networks and a prior distribution composed of a Variational Mixture of Posteriors (VampPrior). The proposed model has been trained on trajectories in the Terminal Manoeuvre Area of Zurich airport, which have a particularly high degree of variability as air traffic controllers often take actions that deviate aircraft from the nominal approach procedure. The model has demonstrated great abilities to take into account such amount of uncertainty. Regarding metrics that evaluate the estimation of the statistical distribution of the observed trajectories, and the flyability of the generated ones, the proposed method outperforms traditional statistical methods by being able to generate more complex and realistic trajectories.},
  archive      = {J_MLA},
  author       = {Timothé Krauth and Adrien Lafage and Jérôme Morio and Xavier Olive and Manuel Waltert},
  doi          = {10.1016/j.mlwa.2022.100446},
  journal      = {Machine Learning with Applications},
  pages        = {100446},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep generative modelling of aircraft trajectories in terminal maneuvering areas},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bi-objective hybrid vibration damping optimization model
for synchronous flow shop scheduling problems. <em>MLA</em>,
<em>11</em>, 100445. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow shop scheduling deals with the determination of the optimal sequence of jobs processing on machines in a fixed order with the main objective consisting of minimizing the completion time of all jobs (makespan). This type of scheduling problem appears in many industrial and production planning applications. This study proposes a new bi-objective mixed-integer programming model for solving the synchronous flow shop scheduling problems with completion time. The objective functions are the total makespan and the sum of tardiness and earliness cost of blocks. At the same time, jobs are moved among machines through a synchronous transportation system with synchronized processing cycles. In each cycle, the existing jobs begin simultaneously, each on one of the machines, and after completion, wait until the last job is completed. Subsequently, all the jobs are moved concurrently to the next machine. Four algorithms, including non-dominated sorting genetic algorithm (NSGA II), multi-objective simulated annealing (MOSA), multi-objective particle swarm optimization (MOPSO), and multi-objective hybrid vibration-damping optimization (MOHVDO), are used to find a near-optimal solution for this NP-hard problem. In particular, the proposed hybrid VDO algorithm is based on the imperialist competitive algorithm (ICA) and the integration of a neighborhood creation technique. MOHVDO and MOSA show the best performance among the other algorithms regarding objective functions and CPU Time, respectively. Thus, the results from running small-scale and medium-scale problems in MOHVDO and MOSA are compared with the solutions obtained from the epsilon-constraint method. In particular, the error percentage of MOHVDO’s objective functions is less than 2\% compared to the epsilon-constraint method for all solved problems. Besides the specific results obtained in terms of performance and, hence, practical applicability, the proposed approach fills a considerable gap in the literature. Indeed, even though variants of the aforementioned meta-heuristic algorithms have been largely introduced in multi-objective environments, a simultaneous implementation of these algorithms as well as a compared study of their performance when solving flow shop scheduling problems has been so far overlooked.},
  archive      = {J_MLA},
  author       = {Madjid Tavana and Vahid Hajipour and Mohammad Alaghebandha and Debora Di Caprio},
  doi          = {10.1016/j.mlwa.2022.100445},
  journal      = {Machine Learning with Applications},
  pages        = {100445},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A bi-objective hybrid vibration damping optimization model for synchronous flow shop scheduling problems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid attention mechanism for multi-target entity
relation extraction using graph neural networks. <em>MLA</em>,
<em>11</em>, 100444. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction is a cardinal natural language processing task employed to process a given corpus and infer hidden connections and relations between real-world objects. Most contemporary research works employ state-of-the-art language models and techniques for determining the type of relationship existing between a pair of entities in a given sentence but are computationally expensive and fail to identify or match the entities present in the sentence as a single task, rather they breakdown the problem into subtasks or rely on a multi-module framework requiring multiple propagations through the network. The paper presents a novel methodology for extracting relations between multiple pairs of entities present in the sentence and performs the relation classification task . The proposed methodology employs a graph neural network , and in contrast to the existing research, the proposed mechanism makes use of a hybrid attention mechanism to dynamically optimize the graph edges to capture the relevant details in the network graph to aid as an attention mechanism to aid in faster computation, compared to the more typical transformer-based networks that overwhelm CPU-based systems. The paper also studies the effect of the number of hop transformations on the graph and other hyper-parameters controlling the input sentence representation. The proposed model architecture achieves a macro avg. F1 score of 86.2 on the SemEval 2010 relation extraction dataset, with further room for improvement.},
  archive      = {J_MLA},
  author       = {Arshad Javeed},
  doi          = {10.1016/j.mlwa.2022.100444},
  journal      = {Machine Learning with Applications},
  pages        = {100444},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A hybrid attention mechanism for multi-target entity relation extraction using graph neural networks},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparative performance analysis of intelligence-based
algorithms for optimizing competitive facility location problems.
<em>MLA</em>, <em>11</em>, 100443. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most companies operate to maximize profits and increase their market shares in competitive environments. Since the proper location of the facilities conditions their market shares and profits, the competitive facility location problem (CFLP) has been extensively applied in the literature. This problem generally falls within the class of NP-hard problems, which are difficult to solve. Therefore, choosing a proper solution method to optimize the problem is a key factor. Even though CFLPs have been consistently solved and investigated, an important question that keeps being neglected is how to choose an appropriate solution technique. Since there are no specific criteria for choosing a solution method, the reasons behind the selection approach are mostly unclear. These models are generally solved using several optimization techniques. As harder-to-solve problems are usually solved using meta-heuristics, we apply different meta-heuristic techniques to optimize a new version of the CFLP that incorporates reliability and congestion. We divide the algorithms into four categories based on the nature of the meta-heuristics: evolution-based, swarm intelligence-based, physics-based, and human-based. GAMS software is also applied to solve smaller-size CFLPs. The genetic algorithm and differential evolution of the first category, particle swarm optimization and artificial bee colony optimization of the second, Tabu search and harmony search of the third, and simulated annealing and vibration damping optimization of the fourth are applied to solve our CFLP model. Statistical analyses are implemented to evaluate and compare their relative performances. The results show the algorithms of the first and third categories perform better than the others.},
  archive      = {J_MLA},
  author       = {Vahid Hajipour and Seyed Taghi Akhavan Niaki and Madjid Tavana and Francisco J. Santos-Arteaga and Sanaz Hosseinzadeh},
  doi          = {10.1016/j.mlwa.2022.100443},
  journal      = {Machine Learning with Applications},
  pages        = {100443},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comparative performance analysis of intelligence-based algorithms for optimizing competitive facility location problems},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Behavioral recommendation engine driven by only
non-identifiable user data. <em>MLA</em>, <em>11</em>, 100442. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recommendation systems utilize personal data to device personalized recommendations for users. Even though it seems favorable, security risks like data breaches are inevitable. This research proposes a novel reinforcement learning ‘approach’ to recommend users without collecting identifiable data. With only user activity on a session, our proposed method can model and track user behavior and formulate a recommendation system. We conclude that our algorithms demonstrate positive results in capturing user behavior without collecting private data of any kind from the user. The research is two folds. On one hand, we experiment using traditional reinforcement learning techniques (MDP, Q-learning), and on the other hand, we use deep reinforcement learning algorithms (DQN, DDQN, and D3QN) on a movie recommendation scenario. Interestingly, we observe that MDP and D3QN works comparatively better on movie recommendations.},
  archive      = {J_MLA},
  author       = {Kishor Datta Gupta and Nafiz Sadman and Akib Sadmanee and Md. Kamruzzaman Sarker and Roy George},
  doi          = {10.1016/j.mlwa.2022.100442},
  journal      = {Machine Learning with Applications},
  pages        = {100442},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Behavioral recommendation engine driven by only non-identifiable user data},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-powered learning methods in the internet of things: A
survey. <em>MLA</em>, <em>11</em>, 100441. (<a
href="https://doi.org/10.1016/j.mlwa.2022.100441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The trend of the era of the Internet of Everything has promoted the integration of various industries and the Internet of Things (IoT) technology, and the scope of influence of the IoT is developing in a wider and deeper level. With the extension of the fields involved, the in-depth progress of the IoT is facing a bottleneck. For example, the security of IoT network and software have problems that are difficult to reconcile. Graph-powered learning methods such as graph embedding and graph neural network (GNN) are expected. How to use the graph learning method in IoT is a question that has to be discussed in relation to the future of the Internet of Things. This paper comprehensively discusses related research and summarizes the progress of using graph-powered learning to promote the network anomaly detection , malware detection , IoT device management, service recommendation and other aspects of IoT. And discuss the results of using graph theory and graph-powered learning methods according to the IoT fields such as smart transportation, Industrial Internet of Things (IIoT), Social Internet of Things (SIoT), smart medical care, smart home, smart grid, and smart city. Finally, in view of the existing issues and trends, this paper proposes future research directions including city various predictions, dynamics and heterogeneity, semantic analysis, resource consumption, point cloud, digital twins , and remote sensing.},
  archive      = {J_MLA},
  author       = {Yuxi Li and Shuxuan Xie and Zhibo Wan and Haibin Lv and Houbing Song and Zhihan Lv},
  doi          = {10.1016/j.mlwa.2022.100441},
  journal      = {Machine Learning with Applications},
  pages        = {100441},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Graph-powered learning methods in the internet of things: A survey},
  volume       = {11},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
