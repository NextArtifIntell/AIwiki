<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMCOM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comcom---334">COMCOM - 334</h2>
<ul>
<li><details>
<summary>
(2023). Analysis of feasible region in network slicing: Markov chain
approach. <em>COMCOM</em>, <em>212</em>, 420–431. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is a key feature introduced for 5G and Beyond 5G (B5G) to provide complete connectivity between people and things by overcoming limited resources. This study considers a theoretical approach for resource allocation when network slicing is employed. Specifically, an algorithm finding a feasible region of users for a given environment is proposed, and a continuous time Markov chain is adopted for its analysis. Some performance measures , such as average throughput and expected total revenue of the infrastructure provider, are expressed by the stationary probabilities of the Markov chain . A simple example and three different environments are presented in the experiment to illustrate the analysis procedure and investigate the effects of environments on performance, respectively. This analytical approach can simplify and speed up the evaluation of the performance of the network operator&#39;s strategy and increases the reliability of the evaluation, which is beneficial in practical applications. Moreover, this can provide theory based-guidance for infrastructure providers in determining the consistent network slice allocation policies, considering the users&#39; needs and the network operator&#39;s decision strategy.},
  archive      = {J_COMCOM},
  author       = {Meejoung Kim},
  doi          = {10.1016/j.comcom.2023.08.014},
  journal      = {Computer Communications},
  pages        = {420-431},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of feasible region in network slicing: Markov chain approach},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Driving a key generation strategy with training-based
optimization to provide safe and effective authentication using data
sharing approach in IoT healthcare. <em>COMCOM</em>, <em>212</em>,
407–419. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology offers a solution to the interoperability issues that currently plague health information systems (BT). Additionally, blockchain technology provides the cutting-edge standard that ensures the safe exchange of computerized health data among healthcare providers , institutions, people, and medical specialists. Recently, both businesses and academics recognized the Internet of Things (IoT) as one of the top study fields. Nevertheless, a significant issue in IoT healthcare across several product domains is the secrecy of patient health data transmission. For secure and effective authentication with data sharing strategy in IoT healthcare, a method called Driving Training Based Optimization (DTBO) Key generation is presented to address these issues. The entities involved in the proposed DTBO_Key generation technique are the user, owner, authenticating minor, blockchain, and IoT server. The suggested technique consists of seven stages: initialization, registration phase, key creation, login phase, data protection, authentication process , and verification phase . A minimum memory consumption of 2.983 MB, a computation cost of 28.008\%, and an accuracy of 0.9 were attained using the proposed method.},
  archive      = {J_COMCOM},
  author       = {Anand Muni Mishra and Yogesh Ramdas Shahare and Piyush Kumar Shukla and Akhtar Husain and Santar Pal Singh and Sultan Alyami and Abdullah Alghamdi and Tariq Ahamed Ahanger},
  doi          = {10.1016/j.comcom.2023.09.016},
  journal      = {Computer Communications},
  pages        = {407-419},
  shortjournal = {Comput. Commun.},
  title        = {Driving a key generation strategy with training-based optimization to provide safe and effective authentication using data sharing approach in IoT healthcare},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Verification of a multi-connectivity protocol for tactile
internet applications. <em>COMCOM</em>, <em>212</em>, 390–406. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tactile Internet refers to a network that enables real-time, high-reliability haptic communication and control between humans, machines, and objects over the Internet. Tactile Internet applications include the remote control of drones, cars or industrial tele-operation. In this context, the Multi-connection Tactile Internet Protocol (MTIP) is a novel multipath transport protocol designed to support the requirements of Tactile Internet applications in large, private mobile networks. The objective of this paper is to analyze and verify the correctness and performance of the MTIP protocol to ensure that the protocol functions correctly under different network scenarios and it is ready to meet the performance requirements of Tactile Internet applications. For that purpose, a two-step approach is employed to analyze and verify MTIP. In the first step, a formal model of MTIP is developed using timed automata and the uppaal tool is utilized to verify correctness properties represented as temporal formulas. In the second step, the performance of the protocol is analyzed using the statistical model checking features of uppaal ( uppaal smc ) in scenarios that are difficult and expensive to reproduce in a real network. The results indicate that MTIP’s model meets the specified temporal properties , and the performance evaluation showcases the potential and trade-off of using multiple paths to enhance the communication. Based on the analysis and verification results, the paper emphasizes the readiness of MTIP for real-world deployment and highlights its potential benefits for enhancing the performance of Tactile Internet applications.},
  archive      = {J_COMCOM},
  author       = {Delia Rico and María-del-Mar Gallardo and Pedro Merino},
  doi          = {10.1016/j.comcom.2023.10.013},
  journal      = {Computer Communications},
  pages        = {390-406},
  shortjournal = {Comput. Commun.},
  title        = {Verification of a multi-connectivity protocol for tactile internet applications},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal path selection using reinforcement learning based
ant colony optimization algorithm in IoT-based wireless sensor networks
with 5G technology. <em>COMCOM</em>, <em>212</em>, 377–389. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoTs) expanded quickly, giving rise to numerous services, apps, electronic devices with integrated sensors, and associated protocols, which are still being developed today. By enabling physical objects to communicate with each other and share important information while making decisions and carrying out their essential jobs, the IoTs enable them to see, hear, think, and execute crucial tasks. Wireless sensor networks (WSN), which act as the IoT&#39;s permanent layer, are essential for fifth-generation (5G) communications, which need the IoT to be considerably helped. A WSN comprises many sensor nodes that track and transmit data to the sink. Every round&#39;s data transmission ends at the sink (or base station). This work presents a Proximal Policy Optimization based Ant Colony Optimization (PPO-ACO) algorithm for optimal path selection in WSN. The proposed algorithm combines the strengths of both PPO with a reinforcement learning (RL) method, and ACO, a swarm intelligence method, to address the stochastic nature of the network and the complex trade-off between energy efficiency and security. The PPO component learns the policy for path selection based on the sampled rewards, while the ACO component updates the pheromone levels to guide the search toward the optimal path. Compared to the state-of-the-art, our simulation findings show that the suggested PPO-ACO algorithm performs better in terms of the number of active nodes. The average residual energy of the suggested algorithm decreases later than existing algorithms, indicating its higher efficiency.},
  archive      = {J_COMCOM},
  author       = {Ghanshyam Prasad Dubey and Shalini Stalin and Omar Alqahtani and Areej Alasiry and Madhu Sharma and Aliya Aleryani and Piyush Kumar Shukla and M. Turki-Hadj Alouane},
  doi          = {10.1016/j.comcom.2023.09.015},
  journal      = {Computer Communications},
  pages        = {377-389},
  shortjournal = {Comput. Commun.},
  title        = {Optimal path selection using reinforcement learning based ant colony optimization algorithm in IoT-based wireless sensor networks with 5G technology},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based intrusion detection for high-dimensional
imbalanced traffic. <em>COMCOM</em>, <em>212</em>, 366–376. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Industry 4.0 , industrial big data has become a hot topic in the field of smart manufacturing. However, the large-scale data flow generated by industrial IoT also has serious security challenges. This paper proposes a new multi-module intrusion detection system : DWGF-IDS, which consists of three modules: feature extraction, imbalance processing and traffic anomaly detection . Firstly, a deep denoising autoencoder is used to extract the deep feature representation of the data and improve the generalization performance of the detection model by adding noise to the autoencoder . Secondly, a Wasserstein Generative Adversarial Network - Gradient Penalty optimized based on the self-attention mechanism is used to generate a few classes in the anomalous traffic. Finally, the weights and bias values in the deep denoising autoencoder are transferred to the deep neural network structure, and a DNN improved based on focal loss is used to implement multi-classification detection on the reduced dimensional balanced traffic data. The system performance was evaluated using two datasets, namely NSL-KDD and CSE-CIC-IDS-2018. The multi-classification accuracy achieved on these datasets were 85.05\% and 99.57\%, respectively. The experimental results show that DWGF-IDS effectively copes with the high dimensionality and imbalance of IoT data, improves the detection rate of unknown attacks, and improves the misclassification of rare classes of attack traffic.},
  archive      = {J_COMCOM},
  author       = {Yuheng Gu and Yu Yang and Yu Yan and Fang Shen and Minna Gao},
  doi          = {10.1016/j.comcom.2023.10.018},
  journal      = {Computer Communications},
  pages        = {366-376},
  shortjournal = {Comput. Commun.},
  title        = {Learning-based intrusion detection for high-dimensional imbalanced traffic},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning for clustering of wireless spectrum
activity. <em>COMCOM</em>, <em>212</em>, 353–365. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, much work has been done on processing of wireless spectrum data involving machine learning techniques in domain-related problems for cognitive radio networks , such as anomaly detection , modulation classification, technology classification and device fingerprinting. Most of the solutions are based on labeled data, created in a controlled manner and processed with supervised learning approaches. However, spectrum data measured in real-world environment is highly nondeterministic, making its labeling a laborious and expensive process, requiring domain expertise, thus being one of the main drawbacks of using supervised learning approaches in this domain. In this paper, we investigate the utilization of self-supervised learning (SSL) for exploring spectrum activities in a real-world unlabeled data . In particular, we assess the performance of SSL models, based on the reference DeepCluster architecture. We carefully consider the current state-of-the-art feature extractors, taking into account the performance and complexity trade-offs. Our findings demonstrate that SSL models achieve superior performance regarding the feature quality and clustering performance compared to baseline feature learning approaches. With SSL models we achieve significant reduction of the feature vectors size by two orders of magnitude, while improving the performance by a factor ranging from 2 to 2.5 across the evaluation metrics , supported by visual assessment. Furthermore, we showcase how adapting the reference SSL architecture to domain-specific data is followed by a substantial reduction in model complexity up to one order of magnitude, without compromising, and in some cases, even improving the clustering performance.},
  archive      = {J_COMCOM},
  author       = {Ljupcho Milosheski and Gregor Cerar and Blaž Bertalanič and Carolina Fortuna and Mihael Mohorčič},
  doi          = {10.1016/j.comcom.2023.10.009},
  journal      = {Computer Communications},
  pages        = {353-365},
  shortjournal = {Comput. Commun.},
  title        = {Self-supervised learning for clustering of wireless spectrum activity},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revving up VNDN: Efficient caching and forwarding by
expanding content popularity perspective and mobility. <em>COMCOM</em>,
<em>212</em>, 342–352. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network caching in Vehicular Named Data Networks (VNDN) has the potential to support latency-sensitive services, though given the massive amount of content generated by vehicles in a VNDN, it is challenging to cache sufficiently diverse content across the network for an acceptable hit ratio. Redundant interest transmissions can negatively impact network overhead, wasting network resources when retrieving duplicate content. In the literature, a caching decision is typically based on ongoing popularity, computed from a vehicle’s experience with requests received for individual content. This leads to each vehicle caching the most popular content. Instead, we group content into different categories. All content in a popular category of vehicular service (e.g., tourist information category) is cached equally, not just the most popular content within a category (e.g., information relating to a particular museum). This increases content diversity in the cache. Further, we posit that a service requester from a particular location at a specific time would be interested in the same category of service/application as other users from the same location/time. Leveraging this observation, we consider both forthcoming spatial/temporal and ongoing popularity of vehicular services when ranking their popularity. We also address network overhead with a mobility model that captures the communication duration between vehicles. This parameter, which has been overlooked in existing literature, enables the selection of an appropriate forwarder for content requests. We use ndnSIM VNDN with a mobility trace of Luxembourg city for simulation. Compared to an approach where individual content-wise popularity is computed, our scheme shows more than 100\% improvement in cache hit ratio where the number of contents in the network varied between 1 0 4 104 and 1 0 5 105 . Simultaneously, we also enhanced the diversity of content caching, while considering the trade-off between cache hit ratio and network overhead.},
  archive      = {J_COMCOM},
  author       = {Sangita Dhara and Akbar Majidi and Siobhan Clarke},
  doi          = {10.1016/j.comcom.2023.10.004},
  journal      = {Computer Communications},
  pages        = {342-352},
  shortjournal = {Comput. Commun.},
  title        = {Revving up VNDN: Efficient caching and forwarding by expanding content popularity perspective and mobility},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A three-level slicing algorithm in a multi-slice
multi-numerology context. <em>COMCOM</em>, <em>212</em>, 324–341. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fifth generation (5G) mobile networks feature a new concept called Bandwidth Part (BWP) that permits applying various multiple Orthogonal Frequency-Division Multiplexing (OFDM) subcarrier spacing, also coined numerologies, in the same band. With flexible numerology, the sought-after purpose is achieving lower latency for the Ultra Reliable Low Latency Communications (URLLC) service. However, the lucrative multiple numerology gains might not stand because of the ensuing Inter-Numerology Interference (INI) problems. In this paper, we assess the feasibility, as well as the profitability, of multi-numerology in a multi-slice setting. In such a context, it is paramount to study the radio resource allocation problem at the Radio Access Network (RAN) level. To that aim, we propose a three-level slicing algorithm that efficiently selects the BWP serving the URLLC users from a set of BWPs using different numerologies, designs a guard band among these BWPs to reduce INI without wasting radio resources, and attributes a bandwidth to each slice depending on multiple parameters such as the users’ Key Performance Indicators (KPIs), channel conditions, INI, and the incurred monetary cost. This work considers the three 5G services, namely the enhanced Mobile BroadBand (eMBB) service, the URLLC service and the massive Machine-Type Communications (mMTC) service. The proposed algorithm combines various tools from Game Theory, Deep-Q learning Networks as well as savvy heuristics. The performance evaluation demonstrates the high efficiency of our solution in terms of latency and throughput compared to well-known approaches in the State-of-the-Art.},
  archive      = {J_COMCOM},
  author       = {Joe Saad and Kinda Khawam and Mohamad Yassin and Salvatore Costanzo},
  doi          = {10.1016/j.comcom.2023.10.012},
  journal      = {Computer Communications},
  pages        = {324-341},
  shortjournal = {Comput. Commun.},
  title        = {A three-level slicing algorithm in a multi-slice multi-numerology context},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing intelligent IoT services development by integrated
multi-token code completion. <em>COMCOM</em>, <em>212</em>, 313–323. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is a revolutionary network of interconnected devices embedded with sensors and software that enables seamless communication, data sharing, and intelligent decision-making in the form of IoT services. To facilitate the efficient development of IoT services, code completion technique provides a promising solution by providing suggestions for missing code snippets. The development trend of IoT services is to support more mobile device terminals. Mobile devices are portable and easy to use, allowing IoT device operation and management anytime and anywhere. However, the current multi-token completion methods struggle to guarantee code generation quality under the constraints of low resources and low latency, making it difficult to fully support IoT service development. We propose a multi-token code completion framework, S2RCC, which completes code from skeleton to refinement with dual encoder and dual decoder. The framework consists of two phases: first, the code skeleton, which is the simplification of code containing structure-sensitive tokens, is predicted based on the semantics of the code context; second, the broken context is repaired with the predicted skeleton, and then parsed into the code structure so that the specific tokens can be generated combining the semantics and structure of context. Furthermore, we then provide an implementation of the framework, representing the repaired code as an improved Heterogeneous code graph and fusing the semantics and structure of code context by the three-layer stacked attention. We conducted experiments on multi-token completion datasets, showing that our model has achieved the state-of-the-art with the smallest possible scale and the fastest generation speed.},
  archive      = {J_COMCOM},
  author       = {Yu Xia and Tian Liang and WeiHuan Min and Li Kuang and Honghao Gao},
  doi          = {10.1016/j.comcom.2023.10.014},
  journal      = {Computer Communications},
  pages        = {313-323},
  shortjournal = {Comput. Commun.},
  title        = {Enhancing intelligent IoT services development by integrated multi-token code completion},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential game theory based multi criterion network
partitioning for controller placement in software defined wide area
networks. <em>COMCOM</em>, <em>212</em>, 298–312. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of determining the number and location of controllers to be deployed in software defined network (SDN) affects various network performance metrics. Large-scale networks are typically divided into multiple domains of smaller size and then deploy one or more controller(s) in each domain. In this paper, we propose a sequential game theory based multi-criteria partitioning scheme for controller placement in large-scale SDN. The proposed scheme automatically identifies the optimal number of domains to be created in the network while considering multiple conflicting objectives. The proposed strategy comprises of three stages. The first stage generates the initial composition of domains. The second stage identifies the actual number of domains to be created in the network and the temporary composition of these domains by formulating several finite multi action two player sequential games. The third stage generates homogeneous domains by formulating several single action two player sequential games. The second and third stages are repeated until there is no improvement in the system objectives. We utilized a game tree for representing the finite sequential game . We also presented a backward induction algorithm for computing the player’s best response strategies, also known as Nash equilibrium . The proposed strategy is evaluated on medium and large-scale networks from Internet Topology Zoo. Further, the performance of the proposed approach was compared with three existing approaches from the literature. Although the complexity of the proposed SGTCP approach is asymptotically higher than existing approaches, the proposed approach converges rapidly in practice and the actual time it takes is quite low. The proposed approach outperforms existing state-of-the-art network partitioning approaches for controller placement in SDN.},
  archive      = {J_COMCOM},
  author       = {Balaprakasa Rao Killi and Rakesh Tripathi and Venkatarami Reddy Chintapalli},
  doi          = {10.1016/j.comcom.2023.10.007},
  journal      = {Computer Communications},
  pages        = {298-312},
  shortjournal = {Comput. Commun.},
  title        = {Sequential game theory based multi criterion network partitioning for controller placement in software defined wide area networks},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A provably secure,privacy-preserving lightweight
authentication scheme for peer-to-peer communication in healthcare
systems based on internet of medical things. <em>COMCOM</em>,
<em>212</em>, 284–297. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Medical Things (IoMT) is a novel paradigm that plays a significant role in healthcare applications. The Patient’s healthcare information is gathered, and remotely monitored through these IoMT systems. Hence, patients will get proper medical care and other emergency services to enhance survival and control morbidity. However, the patient’s data which is transferred between the devices through an insecure channel should be prevented against security threats and attacks. Therefore, secure authentication and key management schemes play an important part in IoMT communications. On the other hand, while considering the resource-constrained devices used in IoMT systems it is necessary to propose lightweight authentication schemes . In this paper, we have proposed a secure lightweight authentication scheme (LAS) for IoMT based healthcare systems. According to LAS, all the devices have to be registered and should approved by a central authority but in the authentication and communication phases, the devices can establish peer-to-peer communication without the intervention of a central authority. Furthermore, the privacy of the communicating parties is preserved by implementing hash based pseudo-identification method. In this paper, we have done the formal security analysis using BAN logic, the ROR model, and the Scyther tool. The informal security analysis is also shown proof of the resilience capabilities of LAS on various other well-known threats and attacks. Moreover, the proposed LAS is implemented using NS3 simulator to analyze its network performance such as End-to-End delay, Packet Loss Rate, and throughput. Lastly, the performance of the proposed scheme is also evaluated in terms of computational and communication costs. As per the result, the proposed LAS outperformed well when compared with other lightweight authentication schemes .},
  archive      = {J_COMCOM},
  author       = {Panchami V. and Rubell Marion Lincy G. and Mahima Mary Mathews and Sharon Justine},
  doi          = {10.1016/j.comcom.2023.07.042},
  journal      = {Computer Communications},
  pages        = {284-297},
  shortjournal = {Comput. Commun.},
  title        = {A provably Secure,Privacy-preserving lightweight authentication scheme for peer-to-peer communication in healthcare systems based on internet of medical things},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the design and performance of scheduling policies
exploiting spatial diversity for URLLC. <em>COMCOM</em>, <em>212</em>,
275–283. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the performance of packet scheduling schemes for Ultra-Reliable Low-Latency Communications (URLLC) services. We exploit spatial diversity (i.e., redundant coverage of users) guaranteed in numerous 5G radio access network scenarios to examine the impact of multi-connectivity. Thus, we consider a set of URLLC users connected to two frequency layers or Radio Access Technologies (RATs) to ensure minimal queuing time. We review four packet scheduling and redundancy schemes, namely Join-the-Shortest-Queue (JSQ), the shortest expected delay (SED), systematic Redundancy (RED), and redundancy with Cancellation upon completion (CAN). We choose the outage probability as a metric, defined as the packet’s probability of arriving after some given target delay. We show that RED performs well at low load, whereas JSQ and SED are better when the load rises. Besides, CAN outperforms all other schemes. We then discuss the trade-off between performance and implementation complexity.},
  archive      = {J_COMCOM},
  author       = {A. Chagdali and S.E. Elayoubi and A.M. Masucci and A. Simonian},
  doi          = {10.1016/j.comcom.2023.10.002},
  journal      = {Computer Communications},
  pages        = {275-283},
  shortjournal = {Comput. Commun.},
  title        = {On the design and performance of scheduling policies exploiting spatial diversity for URLLC},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unequal error protection-based rate-splitting precoding for
VLC systems: BER analysis. <em>COMCOM</em>, <em>212</em>, 262–274. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate-Splitting Multiple Access (RSMA), as a promising next-generation multiple access technique, can be integrated with Visible Light Communication (VLC) to address the challenge of accommodating multiple users under limited modulation bandwidth of Light Emitting Diodes (LEDs). However, most of the current works about RSMA assume perfect Successive Interference Cancellation (SIC) during the decoding procedure and neglect the error propagation problem due to SIC errors, which will greatly affect the performance in real applications. In view of this, we consider the design of Rate-Splitting (RS) precoder for downlink Multi-User Multiple-Input Single-Output (MU-MISO) VLC systems in the presence of SIC errors. For the first time in the literature, we derive analytic Bit Error Rate (BER) expressions in this case and analyze the error performance of a conventional RS precoder , which unveils the error performance degradation due to SIC errors. To mitigate this effect and improve the robustness of the system, we propose a novel RS precoding scheme by providing Unequal Error Protection (UEP) for common and private streams, which is achieved by changing an error protection factor. By this means, the transmission power is flexibly allocated to the common and private symbol stream, which can reduce the error probability as a whole. Simulation results indicate that the proposed RS precoder can provide significant error performance gains (up to 82.9\%) over the baseline.},
  archive      = {J_COMCOM},
  author       = {Chenxi Lai and Shaoshuai Gao and Guofang Tu},
  doi          = {10.1016/j.comcom.2023.10.008},
  journal      = {Computer Communications},
  pages        = {262-274},
  shortjournal = {Comput. Commun.},
  title        = {Unequal error protection-based rate-splitting precoding for VLC systems: BER analysis},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance trade-offs of auto scaling schemes for NFV with
reliability requirements. <em>COMCOM</em>, <em>212</em>, 251–261. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A major concern for future mobile networks is to meet extremely stringent reliability requirements without incurring very high energy consumption. To this aim, we need auto-scaling techniques to switch on/off servers on demand, ensuring that they are active when required. One key challenge is to keep the right number of active servers while accounting for both the fallibility of servers and their non-zero boot up time: if this number is too small we risk service disruption , but if it is too large resources will be wasted. In this paper, we analyze this challenge by assessing the performance of different strategies to support an energy-efficient highly-reliable service, which range from deploying few blade (i.e., reliable) servers to deploying many nano (i.e., unreliable) servers. Our main take-away message is two-fold: (1) a server farm of nano servers can provide a more energy efficient operation than a farm of blade servers, but (2) it involves a very dynamic operation both in terms of task migrations and (de)activations, which challenges the endurance of the hardware.},
  archive      = {J_COMCOM},
  author       = {Jesus Perez-Valero and Jaime Garcia-Reinoso and Albert Banchs and Pablo Serrano and Jorge Ortin and Xavier Costa-Perez},
  doi          = {10.1016/j.comcom.2023.10.001},
  journal      = {Computer Communications},
  pages        = {251-261},
  shortjournal = {Comput. Commun.},
  title        = {Performance trade-offs of auto scaling schemes for NFV with reliability requirements},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stop &amp; offload: Periodic data offloading in UAV
networks. <em>COMCOM</em>, <em>212</em>, 239–250. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarms of Unmanned Aerial Vehicles (UAVs) are a key technology to support communication in many harsh environments where fixed infrastructures (e.g., 5G) are disrupted or not available. However, the fast mobility and highly dynamic network topology pose unique challenges and require the development of novel multi-hop routing protocols. Previous work in this direction extends geographical protocols or adapts approaches designed for Mobile Ad-hoc NETworks (MANETs), rarely taking full advantage of UAV capabilities. In this paper, we introduce a novel data offloading approach, namely Stop &amp; Offload , that exploits the device controllable mobility to facilitate network routing. The swarm of UAVs performs data offloading synchronously and recurrently. At fixed intervals of time, the swarm interrupts the sensing mission ( Stop ) and moves, as little as possible, to build a connected formation to the base station and offload the data ( Offload ). We provide both centralized solutions — assuming a long-range control channel — and a distributed solution — working in the absence of a control channel. By means of extensive simulations we show that our proposals outperform state-of-the-art solutions, decreasing the time taken to build a connected formation of about 45\% and increasing the time spent on sensing of 10\%. Additionally, we compared our protocol with various routing strategies and observe remarkable improvements, including a 50\% reduction in average packet delay .},
  archive      = {J_COMCOM},
  author       = {Novella Bartolini and Andrea Coletta and Flavio Giorgi and Gaia Maselli and Matteo Prata and Domenicomichele Silvestri},
  doi          = {10.1016/j.comcom.2023.10.003},
  journal      = {Computer Communications},
  pages        = {239-250},
  shortjournal = {Comput. Commun.},
  title        = {Stop &amp; offload: Periodic data offloading in UAV networks},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Choquet integral based deep learning model for COVID-19
diagnosis using eXplainable AI for NG-IoT models. <em>COMCOM</em>,
<em>212</em>, 227–238. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 outbreak has caused a global threat to the world healthcare system. The virus has mutated into different variants and mutations which spread more rapidly, are more deadly and have the lesser effect of vaccines. The number of cases is way more than the number of cases doctors can handle. Widely used methods of RT-PCR are tedious and time-consuming, Instant methods like antigen are very less effective and give incorrect results. Hence Artificial Intelligence (AI)-based Computer-aided Diagnosis (CAD) methods that help doctors in the correct and efficient diagnosis are the need of the hour. In this study, we propose a Deep Learning based XAI model for aiding clinical interpretation on Chest X-ray (CXR) images. The XAI diagnostic model can be integrated into NG-IoT devices using CAD models by provide accurate explainable diagnostics. We used deep-learning models with integrated Choquet integral as an aggregation function for more precise classification. Integrating it with the XAI model Grad CAM ++ increased its explainability. In our proposed work we used four different deep learning models ResNet-50, Inception V3, Densenet-121 and DCNN with integrated Choquet Integral function which increased the accuracy by 2\% for Densenet-121 and DCNN and 1\% for ResNet-50 and Inception V3. Using Grad CAM++ on the images increases the acceptance by medical professionals by clearly depicting how the models took the decision. We can depict that our proposed model Densenet 121 integrated with ChI outperformed all the other methods.},
  archive      = {J_COMCOM},
  author       = {Deepanshi and Ishan Budhiraja and Deepak Garg and Neeraj Kumar},
  doi          = {10.1016/j.comcom.2023.09.032},
  journal      = {Computer Communications},
  pages        = {227-238},
  shortjournal = {Comput. Commun.},
  title        = {Choquet integral based deep learning model for COVID-19 diagnosis using eXplainable AI for NG-IoT models},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AP-HBSG: Authentication protocol for heterogeneous
blockchain-based smart grid environment. <em>COMCOM</em>, <em>212</em>,
212–226. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smart grid (SG) is one of the largest Internet of Things (IoT) applications. Therefore, it encompasses a variety of smart objects, including smart appliances , smart meters, and sensors, among others. All of these devices are scattered throughout the SG to serve a variety of objectives. As a result, the SG network architecture is exposed to various security threats. Furthermore, numerous authentication and key agreement techniques have been developed throughout the years to protect the communication between entities. However, several of them are homogeneous protocols, which means that only entities running similar cryptographic schemes can communicate. In addition, the SG communication system is centralized and susceptible to a single point of failure and management. Moreover, the existing protocols involve extensive cryptographic elements that cannot be processed by a smart meter’s (SM) computing power. To address the aforementioned issues, an AP-HBSG: authentication protocol for heterogeneous blockchain-based smart grid environment is designed. The designed protocol operates in a decentralized environment, thus eliminating a single point of failure and management. Furthermore, a blind signature is applied to the blockchain network to add authentication security among blockchain nodes. In addition, AP-HBSG is lightweight and it fits the SM computing capability. Moreover, AP-HBSG can protect the communication of parties interacting in a heterogeneous environment. On the other hand, the security of the presented protocol is analyzed in the random oracle model (ROM) and proved under the elliptic curve discrete logarithm (ECDL) problem and the computational Diffie–Hellman (CDH) problem. The protocol’s performance analysis shows that, compared to the most recent protocols, ours has lower communication and computation costs. The computation cost (ms) is 122,68, 216.86, 391.34, and 92.01 for WHZS, KKN, WLCTA, and ours, respectively. On the other hand, the communication cost (bytes) is 344, 184, 568, and 182 for WHZS, KKN, WLCTA, and ours, respectively.},
  archive      = {J_COMCOM},
  author       = {Egide Nkurunziza and Tandoh Lawrence and Elfadul Issameldeen and Gervais Mwitende},
  doi          = {10.1016/j.comcom.2023.09.034},
  journal      = {Computer Communications},
  pages        = {212-226},
  shortjournal = {Comput. Commun.},
  title        = {AP-HBSG: Authentication protocol for heterogeneous blockchain-based smart grid environment},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-UAV task allocation based on GCN-inspired binary
stochastic l-BFGS. <em>COMCOM</em>, <em>212</em>, 198–211. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task allocation has been one of the key issues for cooperative control of multiple unmanned aerial vehicles (Multi-UAVs), which has attracted a large number of researchers to conduct research in recent years. As the number of tasks and resource types increase, the solution time of most of the existing methods increases sharply, and are difficult to be deployed in other scenarios. To deal with task allocation problems with large-scale tasks and multiple types of resources, this paper proposed a multi-UAV task allocation method based on graph convolutional network (GCN)-inspired binary stochastic L-BFGS (GBSL-BFGS) with strong generalization. First, the objectives and constraints of the task allocation problem are analyzed, while a flexible and easily scalable method for describing the task allocation problem is proposed. Then, the GBSL-BFGS task allocation method is proposed for large-scale multi-UAV cluster. By introducing GCN as a graph mapper, the L-BFGS algorithm is able to optimize the binary decision matrix in the task allocation problem. Simulation experiments demonstrated that the GBSL-BFGS optimization method has a better performance and computational efficiency compared with other methods, especially for large-scale multi-UAV task allocation problems.},
  archive      = {J_COMCOM},
  author       = {An Zhang and Baichuan Zhang and Wenhao Bi and Zhanjun Huang and Mi Yang},
  doi          = {10.1016/j.comcom.2023.09.033},
  journal      = {Computer Communications},
  pages        = {198-211},
  shortjournal = {Comput. Commun.},
  title        = {Multi-UAV task allocation based on GCN-inspired binary stochastic L-BFGS},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimizing cost for influencing target groups in social
network: A model and algorithmic approach. <em>COMCOM</em>,
<em>212</em>, 182–197. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stimulated by practical applications arising from economics, viral marketing, and elections, this paper studies the problem of Groups Influence with Minimum cost ( GIM GIM ), which aims to find a seed set with the smallest cost that can influence all target groups in a social network, where each user is assigned a cost and a score and a group of users is influenced if the total score of influenced users in the group is at least a certain threshold. As the group influence function, defined as the number of influenced groups or users, is neither submodular nor supermodular, theoretical bounds on the quality of solutions returned by the well-known greedy approach may not be guaranteed. In this work, two efficient algorithms with theoretical guarantees for tackling the GIM GIM problem, named Groups Influence Approximation ( GIA GIA ) and Exact Groups Influence ( EGI EGI ), are proposed. GIA GIA is a bi-criteria polynomial-time approximation algorithm and EGI EGI is an (almost) exact algorithm; both can return good approximate solutions with high probability. The novelty of our approach lies in two aspects. Firstly, a novel group reachable reverse sample concept is proposed to estimate the group influence function within an error bound. Secondly, a framework algorithmic is designed to find serial candidate solutions with checking theoretical guarantees at the same time. Besides theoretical results, extensive experiments conducted on real social networks show our algorithms’ performance. In particular, both EGI EGI and GIA GIA provide the solution quality several times better, while GIA GIA is up to 800 times faster than the state-of-the-art algorithms.},
  archive      = {J_COMCOM},
  author       = {Phuong N.H. Pham and Canh V. Pham and Hieu V. Duong and Václav Snášel and Nguyen Trung Thanh},
  doi          = {10.1016/j.comcom.2023.09.022},
  journal      = {Computer Communications},
  pages        = {182-197},
  shortjournal = {Comput. Commun.},
  title        = {Minimizing cost for influencing target groups in social network: A model and algorithmic approach},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 5G-RCOLAB: A system level simulator for 5G and beyond in
rural areas. <em>COMCOM</em>, <em>212</em>, 161–181. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes 5G-RCOLAB, an open source simulation tool for performance evaluation of 5G/6G wireless networks in rural areas. The tool is designed with a layered architecture, based on three modules: (i) a link-level simulator of a communication channel for remote areas, with a mapping Link to System (L2S) interface; (ii) a Long-Term Evolution (LTE)-based physical layer adapted for remote areas, together with a Medium Access Control (MAC) layer that includes a novel Collaborative Spectrum Sensing (CSS) technique with security features for Byzantine attacks and spectrum usage efficiency; and (iii) an application oriented network layer that emulates the behavior of typical users in rural areas. The tool has the goal to reduce simulation complexity of large scale 5G rural networks and at the same time, to represent coherently the features of the physical and MAC layers, that include new channel models that cover long distances with the use of TV White Spaces (TVWS) in the Ultra High Frequency (UHF) and Very High Frequency (VHF) bands, as well as novel CSS techniques to improve spectrum efficiency. The 5G-RCOLAB was validated both by field test measurements from a Proof of Concept (PoC) prototype and by the simulation and evaluation of three 5G common core use cases in rural areas. The results show that the 5G-RCOLAB features allow a better evaluation of novel technologies implemented in the physical and MAC layers and provide paths for improvements and deployments of cost effective solutions of 5G in rural or remote areas.},
  archive      = {J_COMCOM},
  author       = {Gabriel C. Ferreira and Priscila Solís Barreto and Marcos F. Caetano and Eduardo Alchieri and Daniel Costa Araujo and Rodrigo P. Cavalcanti and Diego Aguiar Sousa},
  doi          = {10.1016/j.comcom.2023.09.026},
  journal      = {Computer Communications},
  pages        = {161-181},
  shortjournal = {Comput. Commun.},
  title        = {5G-RCOLAB: A system level simulator for 5G and beyond in rural areas},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A futuristic green service computing approach for smart
city: A fog layered intelligent service management model for smart
transport system. <em>COMCOM</em>, <em>212</em>, 151–160. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of technologies like the Internet of Things (IoT), context awareness , and decentralized computing at the edge of the network, service delivery in smart city parlance has been rapidly evolved. Management of Information and Communication Technologies (ICT) infrastructure for such dynamic environment at scale brings about new challenges. Existing fog layer resource management involves context-sharing and migration for real-time vertical and cross-vertical services. However, improper context migration might affect performance negatively. In this paper, the authors have envisioned improving the Quality of Service (QoS) of smart transportation while employing context-aware computing and Artificial Intelligence , which helps alleviate massive data transfers among Fog nodes and intelligent vehicles in real-time. The proposed Context-Aware Intelligent Transportation System (CAITS) manages the services of intelligent vehicles and manages the road traffic of traditional vehicles in an effective manner, with a three-layered learning model that accounts for on-vehicle, on-co-vehicle, and on-fog-and-vehicle learning by means of a platoon control algorithm as well as federated learning at the Fog level. Simulations are carried out on CloudSim simulator under different scenarios and the results demonstrate that the proposed scheme improves prediction efficacy of contexts at the Fog layer by approximately 8\%–24\% than existing models which in turn reflects in reduced service time and energy consumption of EVs and reduces the CO2 of environment.},
  archive      = {J_COMCOM},
  author       = {K. Hemant Kumar Reddy and Rajat Subhra Goswami and Diptendu Sinha Roy and Senior Member, IEEE},
  doi          = {10.1016/j.comcom.2023.08.001},
  journal      = {Computer Communications},
  pages        = {151-160},
  shortjournal = {Comput. Commun.},
  title        = {A futuristic green service computing approach for smart city: A fog layered intelligent service management model for smart transport system},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network approaches in blockchain-based systems:
Applications, challenges, and future directions. <em>COMCOM</em>,
<em>212</em>, 141–150. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain technology represents a transformative innovation that facilitates the storage, dissemination, and access of vital information within a distributed database framework. While not an inherent part of blockchain technology, peer-to-peer networks are frequently employed in its implementation due to their ability to enhance connection throughput and minimize connection overhead. These networks, known for their high bandwidth, low connection overhead, and robust security standards, are increasingly viewed as influential elements in many contemporary blockchain systems. The core of blockchain technology, however, lies in its distributed ledger that records and verifies transactions. Over recent years, a variety of network approaches incorporating blockchain-based systems have been proposed across numerous domains, including the Internet of Things (IoT), intelligent vehicle networks, and medical data access control, among others. This paper aims to explore and evaluate a selection of these network approaches, providing a comprehensive overview of current applications, along with the inherent advantages and disadvantages of such systems. Furthermore, we present a systematic introduction to future developments in network approaches incorporating blockchain-based systems, setting the stage for a deeper understanding of this rapidly evolving field.},
  archive      = {J_COMCOM},
  author       = {Chen Wang and Jin Zhao},
  doi          = {10.1016/j.comcom.2023.09.018},
  journal      = {Computer Communications},
  pages        = {141-150},
  shortjournal = {Comput. Commun.},
  title        = {Network approaches in blockchain-based systems: Applications, challenges, and future directions},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attacks and vulnerabilities of wi-fi enterprise networks:
User security awareness assessment through credential stealing attack
experiments. <em>COMCOM</em>, <em>212</em>, 129–140. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enterprise Wi-Fi networks are essential for businesses and public administrations as they provide a perfectly scalable and secure system. In the university environment, they are often deployed to offer services to students. One of the most famous university Wi-Fi Enterprise networks is Eduroam, which stands for education roaming; it is a worldwide Wi-Fi access and roaming service widely adopted by the international research and education community. It is based on 802.1x mechanisms that use TLS tunnels for achieving mutual authentication goals, and, as such, it requires careful configuration of mobile devices and responsible users’ behaviors to avoid trivial attacks carried out with rogue Access Points (APs). Differently than employees in a corporate network whose devices are properly configured by ICT teams, the user base of Eduroam consists of (likely) millions of students and professors around the world, with a myriad of different and uncontrolled devices. To assess the security of 802.1x in general, and more specifically that of Eduroam, we ran attacks against two communities of students of increasing size in order to test how users (and their devices) react when rogue 802.1x APs appear in the list of available networks. We then focused our attention on devices, and investigated their detailed dependence on different WPA-Enterprise configurations and certificate settings. The aftermath is that, even with a completely passive attack (users are keeping devices in their pockets), it is possible to steal credentials from more than one-third of the students. While most of the 802.1x vulnerabilities employed in this work should be considered somewhat known (being disclosed in former technical papers), our work appears to raise a threefold concern: (i) most pragmatic 802.1x configurations appear to be grossly insecure; (ii) no Apple’s iPhone felt in our attack unless explicitly forced by the user, owing to its reduced possibility for a user to misconfigure the terminal; and (iii) the awareness of Wi-Fi authentication threats even in relatively skilled end users is close to zero.},
  archive      = {J_COMCOM},
  author       = {Ivan Palamà and Alessandro Amici and Gabriele Bellicini and Francesco Gringoli and Fabio Pedretti and Giuseppe Bianchi},
  doi          = {10.1016/j.comcom.2023.09.031},
  journal      = {Computer Communications},
  pages        = {129-140},
  shortjournal = {Comput. Commun.},
  title        = {Attacks and vulnerabilities of wi-fi enterprise networks: User security awareness assessment through credential stealing attack experiments},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beam adaptation using out-of-band signals for robust
millimeter-wave communications. <em>COMCOM</em>, <em>212</em>, 116–128.
(<a href="https://doi.org/10.1016/j.comcom.2023.08.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave technology has proved to be one of the enabling technologies of the fifth generation of wireless networks. Despite the merits of the millimeter-wave spectrum, its directionality imposes more overhead on communication systems than the non-millimeter wave spectrum. In this paper, we reduce the overhead on communication system by using omnidirectional non-millimeter wave signal to find millimeter wave signal paths. We optimize the beam shape based on the paths that are discovered with the non-millimeter wave signal. The optimized beam is then used for communication in millimeter wave communication. We present experiments to show the conformity of the non-millimeter wave signal with millimeter wave signal in terms of discovered paths in various environments. Using the commercial off-the-shelf hardware, we validate the method of beam alignment over non-millimeter wave paths.},
  archive      = {J_COMCOM},
  author       = {Masoud Zarifneshat and Li Xiao},
  doi          = {10.1016/j.comcom.2023.08.024},
  journal      = {Computer Communications},
  pages        = {116-128},
  shortjournal = {Comput. Commun.},
  title        = {Beam adaptation using out-of-band signals for robust millimeter-wave communications},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An UAV and EV based mobile edge computing system for total
delay minimization. <em>COMCOM</em>, <em>212</em>, 104–115. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we studied a collaborative mobile edge computing(MEC) system based on unmanned aerial vehicle(UAV) and electric vehicle(EV), in which the EV moves to the target area carrying the UAV to provide computing and offloading services to the user equipment(UE). We aimed at minimizing all the UE tasks delay by optimizing the task offloading ratio, the UAV hovering position, and the computing resource allocation of EV and UAV, respectively. The problem was formulated as a Nonlinear Programming(NLP) problem, and we decomposed it into EV related subproblem and UAV related subproblem by the Block-Coordinate Descent(BCD) method. For EV related subproblem, we obtained the optimal offloading ratio by making the computing time on the UAV equal to its offloading time, and proved the method is feasible. For the UAV related subproblem, we introduced the non-orthogonal multiple access(NOMA) and successive interference cancellation(SIC) techniques to improve the communication efficiency, and we obtained the optimal hovering position by using the successive convex approximation(SCA) technique twice to transfer this subproblem into a convex problem . Finally, an overall iterative algorithm was proposed. To verify the effectiveness of our algorithm, we compared our scheme with ‘PSO’, ‘GT’, ‘MC’ and benchmark algorithm.},
  archive      = {J_COMCOM},
  author       = {Qiang Tang and Chen Dai and Zhiqiang Yu and Dun Cao and Jin Wang},
  doi          = {10.1016/j.comcom.2023.09.025},
  journal      = {Computer Communications},
  pages        = {104-115},
  shortjournal = {Comput. Commun.},
  title        = {An UAV and EV based mobile edge computing system for total delay minimization},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mobility in pedestrian communication simulations: Impact of
microscopic models and solutions for integration. <em>COMCOM</em>,
<em>212</em>, 90–103. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling Pedestrian-to-X (P2X) communication plays an important role for use-cases such as crowd networking, wireless contact tracing or the evaluation of Intelligent Transportation Systems (ITS) for the protection of Vulnerable Road Users (VRUs). Since in general the mobile communication depends on the position of the pedestrians, their mobility needs to be modeled. Often simplified mobility models such as the random-waypoint or cellular automata based models are used. However, for ad hoc networks and Inter-Vehicular Communication (IVC), it is well-known that a detailed model for the microscopic mobility has a strong influence — which is why state-of-the-art simulation frameworks for IVC often combine vehicular mobility and network simulators . Therefore, this paper investigates to what extent a detailed modeling of the pedestrian mobility on an operational level influences the results of P2X communication and its applications. We model P2X scenarios within the open-source coupled simulation environment CrowNet. It enables us to simulate the identical P2X scenario while varying the pedestrian mobility simulator as well as the used model. In a first step, two communication scenarios (pedestrian to server via 5G New Radio, pedestrian to pedestrian via PC5 Sidelink) are investigated in different mobility scenarios. Initial results demonstrate that time- and location-dependent factors represented by detailed microscopic mobility models can have a significant influence on the results of wireless communication simulations, indicating a need for detailed pedestrian mobility models in particular for scenarios with pedestrian crowds. While for scenarios with a focus on pedestrian communications this can be achieved by applying locomotion models available in pedestrian dynamics simulators, modeling vehicle-to-pedestrian communication is challenging: Commonly used vehicular mobility simulators apply very simplified pedestrian mobility models; pedestrian dynamics simulators often have no or very limited support for modeling vehicular traffic . Therefore, in a second step, we present a simple approach to simultaneously couple two microscopic mobility simulators (one for modeling pedestrian and one for modeling vehicular mobility) with a pedestrian communication simulation. We implement the concept and apply it in an example scenario modeling vehicle-to-pedestrian communication in a part of the city center of Munich, where we evaluate the influence on application metrics such as the accuracy of the position information received for a pedestrian.},
  archive      = {J_COMCOM},
  author       = {Lars Wischhof and Maximilian Kilian and Stefan Schuhbäck and Matthias Rupp and Gerta Köster},
  doi          = {10.1016/j.comcom.2023.09.029},
  journal      = {Computer Communications},
  pages        = {90-103},
  shortjournal = {Comput. Commun.},
  title        = {Mobility in pedestrian communication simulations: Impact of microscopic models and solutions for integration},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An IoT and machine learning enhanced framework for real-time
digital human modeling and motion simulation. <em>COMCOM</em>,
<em>212</em>, 78–89. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Digital Human Model (DHM) is a computational technique employed for simulating human motion and behavior, finding applications across sectors such as manufacturing, healthcare, and education. Despite its potential, current DHM simulations grapple with challenges like high computational costs, significant latency, low precision, and limited realism. Addressing these challenges, this paper introduces a novel DHM simulation approach that synergizes the Kinect motion capture system, Internet of Things (IoT) devices, and advanced machine learning techniques . Distinctively, our method captures real-time human motion data through IoT devices and the Kinect system, followed by data preprocessing and feature extraction. In contrast to traditional methodologies, we harness deep learning to establish time-series models, integrating convolutional neural networks and dilated convolutional kernels to amplify processing capabilities and cater to multi-scale data. Furthermore, we employ virtual reality technology to craft a digital human structural model, streamlining motion simulation and optimization. Experimental evaluations underscore that our approach consistently outperforms conventional methods across all metrics, demonstrating robust real-time simulation capabilities. In essence, this paper pioneers an innovative technological paradigm for DHM simulation, heralding new avenues for research and applications in related domains.},
  archive      = {J_COMCOM},
  author       = {Haiping Huang and Lingjun Zhao and Yisheng Wu},
  doi          = {10.1016/j.comcom.2023.09.024},
  journal      = {Computer Communications},
  pages        = {78-89},
  shortjournal = {Comput. Commun.},
  title        = {An IoT and machine learning enhanced framework for real-time digital human modeling and motion simulation},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On multi-path bandwidth scheduling for multiple fixed-slot
reservations in high-performance networks. <em>COMCOM</em>,
<em>212</em>, 63–77. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale applications in various fields generate big data at a high speed, which needs to be transferred through High-Performance Networks (HPNs). Bandwidth scheduling in HPNs plays a pivotal role in providing Quality of Service (QoS) to such applications. In this paper, we formulate two bandwidth scheduling problems: (i) maximize the amount of bandwidths to be allocated for multiple fixed-slot bandwidth reservation requests (FBRRs), referred to as maxAB-MultF, and (ii) maximize the number of requests to be satisfied as well as the degree of user’s satisfaction for multiple prioritized fixed-slot bandwidth reservation requests (PFBRRs), referred to as maxNR-MultPF. We prove both of these problems to be NP-complete, and design two heuristic algorithms , namely, Maximal Reserved Bandwidth Resources (MaxRBR) and Dynamic Resource Occupation (DynRO), for them, respectively. For each problem, we also design two algorithms based on greedy strategy for performance comparison. Extensive simulation results illustrate that MaxRBR for maxAB-MultF improves Reserved Bandwidth Resource Ratio over two other algorithms in comparison by 18\% and 15\%, respectively, and DynRO for maxNR-MultPF improves over another two algorithms in comparison scheduling success ratio by 10\% and 5\%, respectively, and user satisfaction by 6.5\% and 5.5\%, respectively. Considering the rapid expansion of HPNs in both speed and scale, the proposed scheduling algorithms have great potentials to improve the network performance of big data applications that require these two types of services.},
  archive      = {J_COMCOM},
  author       = {Kaitao Huo and Yongqiang Wang and Chen Ji and Chen Yue and Chase Q. Wu and Michelle M. Zhu},
  doi          = {10.1016/j.comcom.2023.09.023},
  journal      = {Computer Communications},
  pages        = {63-77},
  shortjournal = {Comput. Commun.},
  title        = {On multi-path bandwidth scheduling for multiple fixed-slot reservations in high-performance networks},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BCSoM: Blockchain-based certificateless aggregate
signcryption scheme for internet of medical things. <em>COMCOM</em>,
<em>212</em>, 48–62. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of fog computing and blockchain in the Internet of Medical Things (IoMT) domain has wholly transformed the healthcare industry and e-health services. In IoMT, the patient’s personal health data is shared on an open channel that makes the IoMT system an easy target for attackers. Therefore, confidentiality and authentication are the key security requirements for an IoMT system. Although few works have been proposed to provide secure communication in conventional healthcare infrastructures, limited work has been done to achieve data confidentiality and authentication in blockchain-based IoMT under fog environment. Therefore, this work proposes a blockchain-assisted certificateless aggregate signcryption called BCSoM scheme that achieves data confidentiality and device/patient authentication. In the proposed work, first, IoMT devices generate a signcrypted text and transmit it to an aggregator that generates an aggregated signcrypted text and sends it to a receiver fog server. The receiver fog server verifies the signcrypted text through the blockchain and also performs unsigncryption to retrieve the original message. The proposed scheme is proven secure under the Discrete Logarithm (DL) and Computational Diffe–Hellman (CDH) assumptions. Finally, a thorough performance analysis using the Hyperledger Fabric platform and cryptographic libraries shows that the proposed scheme is computationally more efficient when compared to existing works.},
  archive      = {J_COMCOM},
  author       = {Ashish Tomar and Sachin Tripathi},
  doi          = {10.1016/j.comcom.2023.09.027},
  journal      = {Computer Communications},
  pages        = {48-62},
  shortjournal = {Comput. Commun.},
  title        = {BCSoM: Blockchain-based certificateless aggregate signcryption scheme for internet of medical things},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-simulated digital twin on the network edge: A vehicle
platoon. <em>COMCOM</em>, <em>212</em>, 35–47. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an approach to create high-fidelity models suited for digital twin application of distributed multi-agent cyber–physical systems (CPSs) exploiting the combination of simulation units through co-simulation. This approach allows for managing the complexity of cyber–physical systems by decomposing them into multiple intertwined components tailored to specific domains. The native modular design simplifies the building, testing, prototyping, and extending CPSs compared to monolithic simulator approaches. A system of platoon of vehicles is used as a case study to show the advantages achieved with the proposed approach. Multiple components model the physical dynamics, the communication network and protocol, as well as different control software and external environmental situations. The model of the platooning system is used to compare the performance of Vehicle-to-Vehicle communication against a centralized multi-access edge computing paradigm. Moreover, exploiting the detailed model of vehicle dynamics, different road surface conditions are considered to evaluate the performance of the platooning system. Finally, taking advantage of the co-simulation approach, a solution to drive a platoon in critical road conditions has been proposed. The paper shows how co-simulation and design space exploration can be used for parameter calibration and the design of countermeasures to unsafe situations.},
  archive      = {J_COMCOM},
  author       = {Maurizio Palmieri and Christian Quadri and Adriano Fagiolini and Cinzia Bernardeschi},
  doi          = {10.1016/j.comcom.2023.09.019},
  journal      = {Computer Communications},
  pages        = {35-47},
  shortjournal = {Comput. Commun.},
  title        = {Co-simulated digital twin on the network edge: A vehicle platoon},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random spanning trees for expanders, sparsifiers, and
virtual network security. <em>COMCOM</em>, <em>212</em>, 21–34. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes probabilistic methods for utilizing random spanning trees generated via a random walk process . We generalize a method by Goyal et al. for weighted graphs and show that it is possible to approximate the expansion of every cut in a weighted graph with the union of random spanning trees generated by a random walk on a weighted graph. Particularly, we show that our union of random spanning trees is a spectral sparsifier of the graph, and we show that for 1 n 1n&amp;lt;ϵ≤1 , O ( log n / ϵ 2 ) O(logn/ϵ2) random spanning trees are required in order to spectrally approximate a bounded degree expander graph. In another part of our research work, we show that our random spanning trees based construction provides security features for virtual networks, in context of Software-Defined Networking. Namely, we demonstrate that our construction allows on-demand efficient monitoring or anonymity services.},
  archive      = {J_COMCOM},
  author       = {Shlomi Dolev and Daniel Khankin},
  doi          = {10.1016/j.comcom.2023.09.028},
  journal      = {Computer Communications},
  pages        = {21-34},
  shortjournal = {Comput. Commun.},
  title        = {Random spanning trees for expanders, sparsifiers, and virtual network security},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy-rule-decided small cell offloading for rate-adaptive
SVC-DASH video streaming over the vehicle environment. <em>COMCOM</em>,
<em>212</em>, 1–20. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed an SVC-DASH video streaming method over the vehicular environment based on the Multi-access Edge Computing (MEC) architecture. For the streaming concern, a control scheme using the Segment-Set-based buffer-aware bitrate adaptation with the backward quality&#39;s increment control was proposed; for the mobility concern, a Small Cell Video Streaming Offloading (SC–VSO) control scheme adopting the multiple attribute fuzzy-based decision for deciding whether it can offload the video streaming traffic from a macro cell to a small cell or not was proposed. To have the suitable bitrate adaptation and stable quality variation, three factors that the proposed streaming control scheme considers are (i) estimated bandwidth, (ii) buffer occupancy and (iii) quality variation. Since the vehicle is moving across many types of Base Stations (BSs), the proposed mobility control scheme can decide whether to have SC-VSO or not before vehicle X entering into the ahead small cell&#39;s signal coverage based on (i) the network situations of the corresponding macro and small cell and (ii) the reported context from vehicle X using the proposed fuzzy logic mechanism. The performance evaluation results shown that the proposed method has the higher video quality and better quality stability for video streaming over the vehicle environment.},
  archive      = {J_COMCOM},
  author       = {Chung-Ming Huang and Han-I Wang},
  doi          = {10.1016/j.comcom.2023.09.014},
  journal      = {Computer Communications},
  pages        = {1-20},
  shortjournal = {Comput. Commun.},
  title        = {Fuzzy-rule-decided small cell offloading for rate-adaptive SVC-DASH video streaming over the vehicle environment},
  volume       = {212},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-friendly statistical counting for pedestrian
dynamics. <em>COMCOM</em>, <em>211</em>, 178–192. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relying on Wi-Fi signals broadcasted by smartphones became the de-facto standard in the domain of pedestrian crowd monitoring. This method got the edge over other traditional means owing to the fact that insights are built upon data which uniquely identifies individuals and, thus, allows highly accurate crowd profiling over time. On the other hand, handling such uniquely identifying data in such a way that it does not expose the sensed individuals to potential privacy infringements proves to be a difficult task. Although several protection techniques were proposed, they yield data which, combined with other external knowledge, can still be used for tracing back to specific individuals. To address this issue, we propose a construction which protects the short-term storage and processing of privacy-sensitive Wi-Fi detections under strong cryptographic guarantees and makes available in the clear, as end results, only statistical counts of crowds. To produce these statistical counts, we make use of homomorphically encrypted Bloom filters as facilitators for oblivious set membership testing under encryption. We implement the system and perform evaluation on both simulated data and a real-world crowd-monitoring dataset, demonstrating that it is feasible to achieve highly accurate statistical counts in a privacy-friendly way.},
  archive      = {J_COMCOM},
  author       = {Valeriu-Daniel Stanciu and Maarten van Steen and Ciprian Dobre and Andreas Peter},
  doi          = {10.1016/j.comcom.2023.09.009},
  journal      = {Computer Communications},
  pages        = {178-192},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-friendly statistical counting for pedestrian dynamics},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A certificateless authenticated searchable encryption with
dynamic multi-receiver for cloud storage. <em>COMCOM</em>, <em>211</em>,
157–177. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The searchable encryption schemes (PEKS, IBEKS, and CLPEKS) based on public-key cryptography (i.e., PKI-based, Identity-based, and Certificateless) enable the data users to accomplish searches on encrypted data stored in cloud storage by a cloud server. However, these schemes have three fundamental challenges: (1) Impervious to Keyword Guessing Attacks (KGAs) and File Injection Attacks (FIA), (2) Keyword privacy in query-trapdoor and encryption-keywords algorithms, and (3) Support for conjunctive keyword search in dynamic multi-user environments. Consequently, the authors produced Certificateless Authenticated Searchable Encryption with Dynamic Multi-receiver (CLASEDM). It also provides two security models that enable keyword privacy in encryption-keyword and query-trapdoor: Chosen keyword to Encryption-keyword attack (CKE) and Chosen keyword to Query-trapdoor attack (CKQ). And the authentication in the encryption of the keywords method also defends against KGA and FIA attacks. Afterward, the authors perform a conjunctive keyword search and add or remove the data recipient from the document’s access list. Finally, the performance analysis shows the CLASEDM scheme outperforming the related systems in comparative and experimental studies.},
  archive      = {J_COMCOM},
  author       = {Venkata Bhikshapathi Chenam and Syed Taqi Ali},
  doi          = {10.1016/j.comcom.2023.09.007},
  journal      = {Computer Communications},
  pages        = {157-177},
  shortjournal = {Comput. Commun.},
  title        = {A certificateless authenticated searchable encryption with dynamic multi-receiver for cloud storage},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ComAvg: Robust decentralized federated learning with random
committees. <em>COMCOM</em>, <em>211</em>, 147–156. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been widely used in IoT applications. However, FL is vulnerable to various attacks in its each phase. Existing defense in federated learning mainly focus on the centralized setting. And centralized parameter-server settings require a trusted third party to collect and distribute model parameters. However, the requirement of a trusted third party cannot always be satisfied in many cases. Meanwhile, the centralized settings suffer from the inherent vulnerability of single-point-of-failure (SPOF), in which the whole system cease to function once the parameter server is broken. Therefore, decentralized federated learning has gain great attention recently. Existing conventional defense strategies are mostly designed for the centralized parameter-server architecture. The problem is that these conventional defense strategies cannot cope with new challenges occurred in highly decentralized settings of FL. Firstly, in a trustless setting, malicious participants can cause breakdown to the whole system on the communication level by disrupting model exchanges. Secondly, current defensive methods cannot effectively identify and rule out malicious participants. In either case, a harmful bias hurts the performance even if malicious participants do not perform model-level attacks. Therefore, defensive strategies for the decentralized-manner FL are in urgent need. To this end, we propose a committee-based FL system, named ComAvg , under a trustless setting. ComAvg provides a general coordination scheme for robust aggregation of distributed learning . With reliability assessment scheme to expel abnormal participants and fortified classic model exchange methods, the conventional centralized methods of FL can be easily modified into decentralized versions to cope with the two challenges aforementioned. Finally, we implement a prototype of ComAvg and perform various groups of evaluations on its robustness. The prototype-based evaluation results and theoretical analysis show that the proposed ComAvg is effective against model attacks such as sign-flipping and communication-level isolating attacks.},
  archive      = {J_COMCOM},
  author       = {Sicong Zhou and Huawei Huang and Ruixin Li and Jialiang Liu and Zibin Zheng},
  doi          = {10.1016/j.comcom.2023.09.004},
  journal      = {Computer Communications},
  pages        = {147-156},
  shortjournal = {Comput. Commun.},
  title        = {ComAvg: Robust decentralized federated learning with random committees},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AOR: Adaptive opportunistic routing based on reinforcement
learning for planetary surface exploration. <em>COMCOM</em>,
<em>211</em>, 134–146. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planetary surface exploration mobile ad-hoc networks (PSEMANET) show the characteristics of routing void caused by craters and electromagnetic interference , relatively high dynamics caused by node heterogeneity, and small node capacity caused by load constraints, which is the reason for the increase of communication delay and packet loss . Adaptive opportunistic routing based on reinforcement learning (AOR) is an opportunistic routing protocol that determines the forwarding node based on the current coordinates, queue length, and the number of neighbors. The full forwarding areas are used to avoid routing void, and the dual competition mechanism is designed to avoid the node hidden problem . The Q-value obtained by reinforcement learning is used to design the dynamic delay cost (DDC) so that nodes in the forwarding areas can compete to achieve adaptive path switching to the environment. Compared with representative proactive routing OLSR , reactive routing AODV , geographic routing GPSR and opportunistic routing BLR, AOR and AOR with memory(AOR-M) have high packet delivery ratio (PDR) and low end-to-end delay in planetary surface exploration scenarios implemented by our simulation system. And the AOR-M protocol shows the lowest expected end-to-end delay and highest channel utilization in both high and low speed scenarios. The result shows that the AOR-M provides efficient and robust routing in planetary surface exploration mobile ad-hoc networks with a complex environment, highly dynamic nodes, and performance constraints.},
  archive      = {J_COMCOM},
  author       = {Yijie Wang and Ziping Yu and Zhongliang Zhao and Xianbin Cao},
  doi          = {10.1016/j.comcom.2023.09.008},
  journal      = {Computer Communications},
  pages        = {134-146},
  shortjournal = {Comput. Commun.},
  title        = {AOR: Adaptive opportunistic routing based on reinforcement learning for planetary surface exploration},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-efficient security-aware scheduling for dependent tasks
with endpoint contention in edge computing. <em>COMCOM</em>,
<em>211</em>, 119–133. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing empowers latency-sensitive and computation-intensive applications composed of multiple dependent tasks of mobile devices to execute on nearby edge servers. Despite the advantages, intermediate data across edge servers faces external security threats. Moreover, most existing works on task scheduling are designed on the idealized system model where communications are performed in parallel, they neglect the endpoint contention problem during data transmission, and thereby can hardly meet the user-specified deadline. To tackle these issues, this paper studies the endpoint contention-aware scheduling problem for dependent tasks to minimize the execution cost under the security and deadline constraints in the edge computing environment. We propose a heuristic algorithm named CSSDE that first distributes the deadline to each task and sorts tasks according to probabilistic endpoint contention-aware upward rank, and then allocates bandwidth resources and computation resources to intermediate data and tasks, respectively, finally performs rescheduling to improve the initial scheduling scheme. In addition, based on CSSDE and genetic algorithm , we present a metaheuristic algorithm called C-GA that considers the impact of task ordering and incorporates a multi-population co-evolution mechanism to enhance population diversity and avoid falling into local optimum for a better scheduling scheme. Simulation experiments are conducted on synthetic and real-world applications. The results demonstrate that our proposed algorithms achieve a higher success rate, and C-GA reduces the execution cost by 43.92, 40.52, and 8.56 percent compared with SCAS-E, ELSH, and CSSDE, respectively.},
  archive      = {J_COMCOM},
  author       = {Zengpeng Li and Huiqun Yu and Guisheng Fan and Qifeng Tang and Jiayin Zhang and Liqiong Chen},
  doi          = {10.1016/j.comcom.2023.08.023},
  journal      = {Computer Communications},
  pages        = {119-133},
  shortjournal = {Comput. Commun.},
  title        = {Cost-efficient security-aware scheduling for dependent tasks with endpoint contention in edge computing},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The low latency networking method for task-driven
MEC-enabled UAV swarm. <em>COMCOM</em>, <em>211</em>, 104–118. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are increasingly being used for special missions as Mobile Edge Computing (MEC) devices, and the utilization of multiple UAVs who are integrated into formations for collaboratively executing tasks that cannot be performed by single aircraft has also been a recent research hit. In this paper, we present a low latency networking method aims to shorten the preparation time so that MEC-enabled swarm can as quick as possible concentrate on the task execution. For reducing the time of swarm initialization, we consider applying the Leader-Follower (L-F) topology model in networking and formation control . Firstly, by combining the nature of wireless communication and correlation analysis, we improve the traditional L-F model to avoid too many nodes accessing the same node and raise the communication efficiency. Secondly, in order to choose the topology that can provide better performance in network, we adopt an improved Particle Swarm Optimization (PSO) algorithm to customize the appropriate factor coefficients during leader selection for swarms with different densities and communication initial conditions. Experiments and evaluations demonstrate that the proposed algorithm has significant effect in reducing network latency .},
  archive      = {J_COMCOM},
  author       = {Huimin Gao and Bo Jiang and Hong Xu and Siji Chen},
  doi          = {10.1016/j.comcom.2023.08.019},
  journal      = {Computer Communications},
  pages        = {104-118},
  shortjournal = {Comput. Commun.},
  title        = {The low latency networking method for task-driven MEC-enabled UAV swarm},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing mobile operator’s tariff package pricing scheme
based on user’s internet behavior. <em>COMCOM</em>, <em>211</em>,
93–103. (<a href="https://doi.org/10.1016/j.comcom.2023.07.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the rapid growth of the mobile internet market, high-priced telecom packages are limiting the growth of mobile internet use. Telecom operators and internet companies are working together to offer free-flow packages. This paper proposes a complete content selection and package pricing scheme (CSPPS) based on data provided by a telecom provider. This paper first conducts a feasibility analysis on the free traffic package scheme. By analyzing users’ usage detailed records (UDRs), this paper identified a model for user internet behavior. A scheme for content selection was developed based on this model. To discover users’ online timing characteristics, we developed a tensor decomposition factor model. To complete the package pricing process, a two-sided market model and SoftMax algorithm were combined. Performance evaluations were conducted using real data from China Unicom. According to our experimental results, our package selection scheme is effective for different scenarios.},
  archive      = {J_COMCOM},
  author       = {Juanjuan Wang and Jun Zeng and Hao Wen and Zhiyi Hu},
  doi          = {10.1016/j.comcom.2023.07.025},
  journal      = {Computer Communications},
  pages        = {93-103},
  shortjournal = {Comput. Commun.},
  title        = {Designing mobile operator’s tariff package pricing scheme based on user’s internet behavior},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A coordinates-based hierarchical computing framework towards
spatial data processing. <em>COMCOM</em>, <em>211</em>, 83–92. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle-based mobile base station (VMBS) has been developed by NTT a few years ago, which plays an important role in reconstructing an emergency communication network by integrating with mobile edge computing to support the cloud service for data pre-processing tasks. Spatial data processing refers to characteristics, such as fast-changing, massive, and potentially infinite. Considering the capacities of network bandwidth and storage of VMBS, new challenging issues have emerged such as how to quantify information loss and how to optimize data processing in the paradigm of mobile edge computing . To address these two challenges, we design a coordinates-based hierarchical computing framework for spatial data processing, named HierCom . Using HierCom, the optimal position of VMBS can be selected to decrease the transmission delay, thereby reducing information loss in the data processing in an edge computing environment. Furthermore, we also propose a near least neighbor -based algorithm to optimize the information loss. Finally, simulations on the real-world dataset show that the information loss of our method is reduced by 27\% compared with the baseline method .},
  archive      = {J_COMCOM},
  author       = {Chen Qiu and Haoda Wang and Qinglin Yang and Chunhua Su and Huawei Huang},
  doi          = {10.1016/j.comcom.2023.09.005},
  journal      = {Computer Communications},
  pages        = {83-92},
  shortjournal = {Comput. Commun.},
  title        = {A coordinates-based hierarchical computing framework towards spatial data processing},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSEC-d based energy consumption optimization strategy in
satellite communication systems. <em>COMCOM</em>, <em>211</em>, 73–82.
(<a href="https://doi.org/10.1016/j.comcom.2023.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a satellite communication system that consists of a source low earth orbit (SLEO) satellite, several relay low earth orbit (RLEO) satellites, and a destination ground station (GS). Under the assumption that the data can be split into different sizes of data flow while transmitted through the system in different time slots, a joint route selection and time-slot allocation problem are formulated. Considering the constraints on the availability of transmission links, conservation of data flow, and system resources, an optimization problem that minimizes the total system energy consumption is formulated. As the original optimization problem is NP-hard, which cannot be solved conveniently, we transform the problem to the maximum transmission rate route selection subproblem and energy-efficient resource allocation subproblem . To solve these subproblems, we propose two algorithms, namely the multi-time-expanding graph-based maximum transmission rate route selection (MTEG-R) algorithm and the minimum system energy consumption - Dijkstra (MSEC-D) algorithm. The proposed algorithms enable the determination of routes selection and time slots allocation for data transmission. Simulation results demonstrate the effectiveness of the proposed scheme.},
  archive      = {J_COMCOM},
  author       = {Yin Wang and Kang’An Gui},
  doi          = {10.1016/j.comcom.2023.09.006},
  journal      = {Computer Communications},
  pages        = {73-82},
  shortjournal = {Comput. Commun.},
  title        = {MSEC-D based energy consumption optimization strategy in satellite communication systems},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SFC-based multi-domain service customization and deployment.
<em>COMCOM</em>, <em>211</em>, 59–72. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to satisfy diversified service demands, future networks are expected to provide higher quality network services , especially in 5G scenarios. Network Function Virtualization (NFV) is a promising architecture to achieve flexible service provisioning by introducing Service Function Chain (SFC) technology. With explosive expansion of network devices and end users, the complex multi-domain network environment and personalized service demands of different users pose serious challenges to efficient service provisioning. To this end, this paper presents a novel SFC-based Multi-domain Service Customization and Deployment framework, MSCD, by blending merits of Software Defined Networking (SDN) and NFV technologies. MSCD framework can automatically customize multi-domain network services based on users’ personalized service preferences, and deploy them in multi-domain networks, to provide customizable multi-domain services for different users. Taking into account user personalized service preferences, resource allocation and VNF dependency constraints, the multi-domain service customization and deployment problem is mathematically formulated as an optimization model with the target of service satisfaction degree maximization. Furthermore, a Greedy strategy based Heuristic Service Customization and dePloyment (GHSCP) algorithm is proposed to customize and deploy the multi-domain services efficiently. Simulation results demonstrate that the proposed GHSCP algorithm is efficient and outperforms comparison algorithms in terms of service acceptance ratio and service satisfaction degree.},
  archive      = {J_COMCOM},
  author       = {Chuangchuang Zhang and Yanming Liu and Shuning Zhang and Hongyong Yang and Fuliang Li and Xingwei Wang},
  doi          = {10.1016/j.comcom.2023.08.025},
  journal      = {Computer Communications},
  pages        = {59-72},
  shortjournal = {Comput. Commun.},
  title        = {SFC-based multi-domain service customization and deployment},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of edge computing-based information-centric
networking in smart cities. <em>COMCOM</em>, <em>211</em>, 46–58. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many data resources and network availability are needed for smart city applications to execute at their highest efficiency level. Demand for these objects is driving up data traffic, which in turn is placing strain on the network. The 5G-enabled Internet of Things applications address these difficulties in smart city applications . This article proposes an Information-centric Networking System using Multiaccess Edge Computing (ICNMEC) to reduce computation offloading and optimize data traffic. This system&#39;s 5G network slicing approaches combine edge computing and software characterization. Internet of Things applications have been used to store and analyze the information gathered. In addition, an algorithm known as OMNM (Optimized Memory Network Management) is created to control network traffic better and better use of storage. With minimal delays, network traffic, and storage ratio, the system&#39;s modelling tests demonstrate that it is very efficient. This method can progressively enhance the pace at which one can access and use the system. The performance assessment shows that the proposed method can improve the efficiency ratio of 95.141\%, storage utilization ratio of 60.1\% and access rate by 0.9, reducing network traffic and delay by 0.6.},
  archive      = {J_COMCOM},
  author       = {Hayder sabah salih and Mustafa Musa Jaber and Mohammed Hasan Ali and Sura Khalil Abd and Ahmed Alkhayyat and R. Q Malik},
  doi          = {10.1016/j.comcom.2023.09.003},
  journal      = {Computer Communications},
  pages        = {46-58},
  shortjournal = {Comput. Commun.},
  title        = {Application of edge computing-based information-centric networking in smart cities},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An IoT based forest fire detection system using integration
of cat swarm with LSTM model. <em>COMCOM</em>, <em>211</em>, 37–45. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The destruction of millions of acres of forest each year by forest fires is a global environmental crisis that has real-world consequences for people&#39;s livelihoods and the health of our planet. The ability to foresee the onset of such a natural disaster is, thus, of paramount importance in reducing this risk. There have been numerous proposed technologies and novel approaches for detecting and preventing forest fires. Integrating AI to automate fire prediction and detection is becoming increasingly common. To provide effective forest fire detection, people make use of several technological expansions, with the IoT for data collecting and Artificial Intelligence (AI) for the forecast process. Artificial intelligence (AI) is a key study technique that has been proven to be the best in enhancing the presentation of detecting fire threats in important locations by several researchers. Due to the importance of object detection in this investigation, EfficientDet was chosen for implementation. It is suggested that fire breakouts be detected using a Recurrent LSTM Neural Network (RLSTM-NN). Here, we propose a Cat Swarm Fractional Calculus Optimization (CSFCO) algorithm for deep learning that combines the best features of Cat Swarm Optimization (CSO) with fractional calculus for optimal training results (FC). Terms of the simulation results reveal that the suggested process outdoes the state-of-the-art approaches. The suggested typical can identify the onset of a fire with a precision of 98.6\% and an error rate of only 0.14\%.},
  archive      = {J_COMCOM},
  author       = {Mahaveerakannan R and Cuddapah Anitha and Aby K Thomas and Sanju Rajan and T. Muthukumar and G. Govinda Rajulu},
  doi          = {10.1016/j.comcom.2023.08.020},
  journal      = {Computer Communications},
  pages        = {37-45},
  shortjournal = {Comput. Commun.},
  title        = {An IoT based forest fire detection system using integration of cat swarm with LSTM model},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generation of irregular grid maps for fingerprinting-based
mobile radio localization using farthest-first traversal and
low-discrepancy sequences. <em>COMCOM</em>, <em>211</em>, 24–36. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Localization techniques play a fundamental role in various applications like in the development of smart cities, making the improvement of such technologies indispensable. In this context, radio frequency (RF) fingerprinting (FP)-based localization methods are attractive due to their lower implementation cost, lower energy consumption , and better performance in non-line-of-sight conditions compared to GPS . With this in mind, we propose an RF FP-based localization method using an irregular grid map generation strategy. The grid map segmentation procedure is based on the assumption that the RF measurements collected in a region carry information on how users are often spatially distributed. The algorithms employed in the irregular grid map generation are the farthest-first traversal and low-discrepancy R R -sequence. Different irregular grid map generators are compared with each other and with the regular grid map. We evaluate the generators in two different scenarios: with measurements obtained through simulation with ns-3 and via a driving-test procedure in an urban area. Numerical results indicate that our proposed irregular generator presents a lower localization error in both scenarios and is the only one capable of meeting one of the accuracy requirements stated by the Federal Communications Commission that demands a localization error of up to 50 m in 80\% of the emergency calls.},
  archive      = {J_COMCOM},
  author       = {Gustavo P. Bittencourt and João Paulo P.G. Marques and Daniel C. Cunha},
  doi          = {10.1016/j.comcom.2023.08.021},
  journal      = {Computer Communications},
  pages        = {24-36},
  shortjournal = {Comput. Commun.},
  title        = {Generation of irregular grid maps for fingerprinting-based mobile radio localization using farthest-first traversal and low-discrepancy sequences},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). POLSTM: Poplar optimization-based long short term memory
model for resource allocation in cloud environment. <em>COMCOM</em>,
<em>211</em>, 11–23. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the evolution of Internet of Things (IoT) paradigm, the number of devices is growing day by day which arises the stringent requirements for various communications. The standard cloud computing model is not capable of efficiently hosting IoT tasks due to the high latency associated with it. Hence, this work utilizes the mobile edge computing and fog concept to process the input tasks independent of the cloud layer. Generally, the high computation costs and energy consumption resulted in the applications such as power-hungry and computation-intensive, which become massive challenges for an IoT device. Motivated by the previous research, we mainly plan to investigate the resource allocation issues that arise in the MEC-enabled IoT–Fog–cloud architecture. We propose a Poplar Optimization algorithm (POA) based on Attention 1DCNN-LSTM architecture (POA-A1DCNN-LSTM) for improving the total utility of the MEC servers by optimizing the energy consumption and task delay. Initially, the POA algorithm is implemented for cluster head (optimal fog node) selection using the node degree, node distance, and residual energy . Next, the A1DCNN-LSTM architecture is employed for task offloading by selecting the fog node with minimal task length and processing delay. The performance of the proposed method is validated by average latency , user satisfaction, network lifetime, energy consumption, average time delay , and normalized system utility. The experimentation results revealed that the proposed method attained better effectiveness in different metrics by achieving 1.5 ms, 0.65, 60 s, 0.45 j, 385 ms, and 0.98 compared to state-of-the-art methods.The experimentation outcomes demonstrate the effectiveness of the proposed POA-A1DCNN-LSTM architecture to decrease task completion delay and offer effective task scheduling as well as resource allocation performances in the MEC-enabled IoT–Fog–cloud network.},
  archive      = {J_COMCOM},
  author       = {Prithi Samuel and Arumugham Vinothini and Jayashree Kanniappan},
  doi          = {10.1016/j.comcom.2023.08.008},
  journal      = {Computer Communications},
  pages        = {11-23},
  shortjournal = {Comput. Commun.},
  title        = {POLSTM: Poplar optimization-based long short term memory model for resource allocation in cloud environment},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information centric wireless communication for variation
detection and mitigation model in industrial internet of things.
<em>COMCOM</em>, <em>211</em>, 1–10. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industrial Internet of Things (IIoT) harmonizes smart devices, machines, and intelligent technologies to improve production efficiency through the information-centric network. Information-Centric Networking (ICN) is a potentially useful method in control sequences and intelligent operations due to adversary impacts. Here, the sharing of content chunks between different machines&#39; varying control spans and operation cycles has been partially connected to the internet. This article introduces a Variant Loop Detection and Mitigation Model (VLDMM) for preventing illegitimate entries in ICN. The proposed model is backboned by recurrent learning for training the loop closures and time achievements in ICN. The harmonization is utilized under different production outputs observed from the previous cycles. The interruptions and process halts are identified using recurrent training by correlating the previous operation logs in ICN. Therefore, ICN may solve the challenges based on the available control slots and variations. This learning retains the production efficiency and prevents halts under controlled time and loop closures.},
  archive      = {J_COMCOM},
  author       = {Donald J.P and Linda Joseph},
  doi          = {10.1016/j.comcom.2023.09.001},
  journal      = {Computer Communications},
  pages        = {1-10},
  shortjournal = {Comput. Commun.},
  title        = {Information centric wireless communication for variation detection and mitigation model in industrial internet of things},
  volume       = {211},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling federated learning of explainable AI models within
beyond-5G/6G networks. <em>COMCOM</em>, <em>210</em>, 356–375. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quest for trustworthiness in Artificial Intelligence (AI) is increasingly urgent, especially in the field of next-generation wireless networks. Future Beyond 5G (B5G)/6G networks will connect a huge amount of devices and will offer innovative services empowered with AI and Machine Learning tools. Nevertheless, private user data, which are essential for training such services, are not an asset that can be unrestrictedly shared over the network, mainly because of privacy concerns. To overcome this issue, Federated Learning (FL) has recently been proposed as a paradigm to enable collaborative model training among multiple parties, without any disclosure of private raw data. However, the initiative to natively integrate FL services into mobile networks is still far from being accomplished. In this paper we propose a novel FL-as-a-Service framework that provides the B5G/6G network with flexible mechanisms to allow end users to exploit FL services, and we describe its applicability to a Quality of Experience (QoE) forecasting service based on a vehicular networking use case. Specifically, we show how FL of eXplainable AI (XAI) models can be leveraged for the QoE forecasting task, and induces a benefit in terms of both accuracy, compared to local learning, and trustworthiness, thanks to the adoption of inherently interpretable models. Such considerations are supported by an extensive experimental analysis on a publicly available simulated dataset. Finally, we assessed how the learning process is affected by the system deployment and the performance of the underlying communication and computation infrastructure, through system-level simulations, which show the benefits of deploying the proposed framework in edge-based environments.},
  archive      = {J_COMCOM},
  author       = {José Luis Corcuera Bárcena and Pietro Ducange and Francesco Marcelloni and Giovanni Nardini and Alessandro Noferi and Alessandro Renda and Fabrizio Ruffini and Alessio Schiavo and Giovanni Stea and Antonio Virdis},
  doi          = {10.1016/j.comcom.2023.07.039},
  journal      = {Computer Communications},
  pages        = {356-375},
  shortjournal = {Comput. Commun.},
  title        = {Enabling federated learning of explainable AI models within beyond-5G/6G networks},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeXT: Architecture, prototyping and measurement of a
software-defined testing framework for integrated RF network simulation,
experimentation and optimization. <em>COMCOM</em>, <em>210</em>,
342–355. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support rigorous and repeatable experimental evaluation of wireless networked systems, the community has made significant efforts to develop experimentation platforms. However, existing platforms primarily focus on the data plane, i.e., the forwarding infrastructure, without explicitly considering the control plane. To fill this gap, in this work we develop NeXT , a software-defined playground with integrated wireless network simulation, experimentation and optimization capabilities. We first design the data plane, which integrates an event-driven broadband wireless network simulator called UBSim and a software-defined wireless network testing facility called RoboNet . We then design NeXT’s control plane, where a software toolchain is developed and deployed to support both traditional model-based optimization and new data-driven control techniques. We showcase the experimentation capability of NeXT considering a series of optimization and control problems in different wireless networks.},
  archive      = {J_COMCOM},
  author       = {Jiangqi Hu and Zhiyuan Zhao and Maxwell McManus and Sabarish Krishna Moorthy and Yuqing Cui and Nicholas Mastronarde and Elizabeth Serena Bentley and Michael Medley and Zhangyu Guan},
  doi          = {10.1016/j.comcom.2023.08.018},
  journal      = {Computer Communications},
  pages        = {342-355},
  shortjournal = {Comput. Commun.},
  title        = {NeXT: Architecture, prototyping and measurement of a software-defined testing framework for integrated RF network simulation, experimentation and optimization},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic multiple access based on deep reinforcement learning
for internet of things. <em>COMCOM</em>, <em>210</em>, 331–341. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of wireless communication technology, the rational use of spectrum resources utilizing efficient multiple access technology has become a research hotspot. The channel access strategy in dynamic spectrum access technology will directly affect the system’s spectrum utilization. Deep reinforcement learning is proposed to train users in the network to learn optimal channel access strategy to achieve reasonable allocation of spectrum resources on the channel. At the same time, the multiple access scheme adopted by users who have access to the channel will also affect the throughput of the system. Traditional multiple access schemes include Orthogonal Multiple Access scheme and Non-orthogonal Multiple Access (NOMA) scheme. In a dynamic environment, the multiple access scheme suitable for users on the channel is also dynamic. Therefore, DRL is introduced to help users learn the multiple access scheme selection strategy to maximize user throughput . Finally, we propose a dynamic multiple access algorithm based on deep reinforcement learning, which optimizes the channel access strategy and scheme selection strategy respectively, and improves the performance. The simulation results show that the proposed scheme is superior to other schemes in various scenarios, especially, the average throughput of the proposed scheme is about 0.8 Mbps and 0.5 Mbps higher than that of the FDMA scheme and NOMA scheme, respectively.},
  archive      = {J_COMCOM},
  author       = {Xin Liu and Zengqi Li},
  doi          = {10.1016/j.comcom.2023.08.012},
  journal      = {Computer Communications},
  pages        = {331-341},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic multiple access based on deep reinforcement learning for internet of things},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IoT based real-time traffic monitoring system using images
sensors by sparse deep learning algorithm. <em>COMCOM</em>,
<em>210</em>, 321–330. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent traffic monitoring systems are necessary and useful tools due to the emerging technologies related to the Internet of Things (IoT) and Artificial Intelligence (AI). The integration of both technologies can facilitate better urban traffic management for fast networks, such as 5G , 5G + + and 6G environments. However, existing studies focus on complex and expensive solutions or present a latency for generating new training models with accuracy not superior to 98\%. Thus, this paper proposes a vehicle identification using a sparse and soft variant of the CNN-based approach, the LightSpaN, which offers a fast training model, without using a complex solution. The effectiveness of the proposed solution is evaluated using the Simulation of Urban MObility (SUMO) tool and a real vehicle traffic implementation using IoT devices. The proposal identified the majority kind of vehicles in a short period of time, faster and more accurately than the related works. The results validated the proposed solution for a real-time traffic monitoring system, presenting an average accuracy of around 99.9\% for emergency vehicles. Furthermore, a reduction of both total waiting time and total traveling time was reached by our proposal.},
  archive      = {J_COMCOM},
  author       = {Rodrigo Barbosa and Okey Daniel Ogobuchi and Omole Oluwatoyin Joy and Muhammad Saadi and Renata Lopes Rosa and Sattam Al Otaibi and Demóstenes Zegarra Rodríguez},
  doi          = {10.1016/j.comcom.2023.08.007},
  journal      = {Computer Communications},
  pages        = {321-330},
  shortjournal = {Comput. Commun.},
  title        = {IoT based real-time traffic monitoring system using images sensors by sparse deep learning algorithm},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedCSR: A new cluster sampling based on rotation mechanism
in horizontal federated learning. <em>COMCOM</em>, <em>210</em>,
312–320. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Horizontal federated learning (HFL) is a distributed framework that can be used when datasets from various participants have similar feature spaces but different sample spaces. HFL employs clustering algorithms to facilitate information sharing among cluster clients and model aggregation. However, this approach is not suitable for data with heterogeneity, and its stability cannot be guaranteed. Differing from all the current methods to address these issues, we propose a new HFL framework by employing a cluster sampling method based on a rotation mechanism. In our method, we calculate the magnitude of differences in client data by using Gaussian Model Mixture (GMM), and adaptively adjust the probability of intra-cluster clients being selected by the server as well as the number of clusters. Our numerical experiments show that the new framework can handle different data scenarios and outperforms other baselines in terms of convergence speed and accuracy.},
  archive      = {J_COMCOM},
  author       = {Qiang Dai and Tongjiang Yan and Pengcheng Ren},
  doi          = {10.1016/j.comcom.2023.08.016},
  journal      = {Computer Communications},
  pages        = {312-320},
  shortjournal = {Comput. Commun.},
  title        = {FedCSR: A new cluster sampling based on rotation mechanism in horizontal federated learning},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). APAP: An adaptive packet-reproduction and active packet-loss
data collection protocol for WSNs. <em>COMCOM</em>, <em>210</em>,
294–311. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In lossy, large-scale wireless sensor networks (WSNs), data collection protocols face the challenge of balancing expected data transmission reliability and limited resources of sensors. Packet redundancy strategies have been widely employed to guarantee transmission reliability. However, these schemes often result in energy waste, especially in sink-neared hotspot districts. We propose the APAP to balance the two competing interests in energy-constrained WSNs. APAP mainly consists of two parts: adaptive packet-reproduction routing and active packet-loss mechanism. In the first part, we design a compensation function to compensate for packet losses and employ it in the packet-reproduction routing. Through available network parameters, such a function offers an optimized number of redundant packets. In the second part, we devise a novel distributed packet-loss mechanism to detect and intercept redundant packets in the hotspot area . Hotspot nodes record the simple traffic information in their vicinity on a local micro-table, these nodes transform their mode based on the records to reach a basic consensus with one-hop neighbors, they then collaboratively intercept excess packets from outside, thus the injection traffic can be mitigated. Such an interception area has the potential to be expanded and prolonged as the redundancy becomes more severe. Our mechanism imposes low-complexity requirements on general devices. Moreover, thorough mathematical analyses are offered for the performance of APAP. Simulation results indicate that with high reliability, the maximum node energy consumption is reduced by 44.7\% to 66.3\% compared to conventional protocols in one round. Besides, the network lifetime is prolonged by 91.1\% to 200\%.},
  archive      = {J_COMCOM},
  author       = {Xing Gao and An He and Guangwei Wu and Jinhuan Zhang},
  doi          = {10.1016/j.comcom.2023.08.015},
  journal      = {Computer Communications},
  pages        = {294-311},
  shortjournal = {Comput. Commun.},
  title        = {APAP: An adaptive packet-reproduction and active packet-loss data collection protocol for WSNs},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DA-DMPF: Delay-aware differential multi-path forwarding of
industrial time-triggered flows in deterministic network.
<em>COMCOM</em>, <em>210</em>, 285–293. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-sensitive network (TSN) utilizes gating technology to achieve mixed traffic transmission across the common network, replacing dedicated networks with time division multiplexing , simplifying the interconnection architecture . TSN fundamentally changes the uncertainty of traditional Ethernet through clock synchronization , traffic scheduling, and network configuration . TSN can meet the requirements of industrial communication for low delay, low jitter, and high-reliable data transmission. It is also fully compatible with the existing Ethernet system and is an important communication platform supporting the development of the industrial Internet. Time-triggered flow is the key flow responsible for transmitting real-time data in time-sensitive networks. Its transmission should strictly follow the gating schedule. Selecting different transmission paths will affect the success rate of scheduling. To solve the delay optimization problem of industrial time-triggered (TT) flow path selection, this paper proposes a novel forwarding mechanism for industrial TT flow, namely delay-aware differential multi-path forwarding (DA-DMPF). This mechanism first designs a multipath forwarding index based on sending delay and processing delay by analyzing the end-to-end delay, then converts the multipath forwarding problem into a 0-1 integer linear planning problem, and finally solves the forwarding path set of the stream through differential multipath forwarding algorithm to achieve low delay parallel transmission of the stream. The simulation results show that compared with load-balanced multi-path forwarding and Hop-Load forwarding mechanisms, this scheme reduces the end-to-end average delay of industrial TT flows by 22.55\% and 17.32\% respectively, meeting the low delay transmission requirements of TT flows.},
  archive      = {J_COMCOM},
  author       = {Jin Wang and Chang Liu and Liang Zhou and Jie Wang and Xiao Yu},
  doi          = {10.1016/j.comcom.2023.08.013},
  journal      = {Computer Communications},
  pages        = {285-293},
  shortjournal = {Comput. Commun.},
  title        = {DA-DMPF: Delay-aware differential multi-path forwarding of industrial time-triggered flows in deterministic network},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Early detection of new web tracking methods across 1.5
million sites. <em>COMCOM</em>, <em>210</em>, 273–284. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current web tracking practices pose a constant threat to the privacy of Internet users. As a result, the research community has recently proposed different tools to combat well-known tracking methods. However, the early detection of new, previously unseen tracking systems is still an open research problem. In this paper, we present TrackSign+ , a novel approach to discovering new web tracking methods. The main idea behind TrackSign+ is the use of code fingerprinting to identify common pieces of code shared across multiple domains. To detect tracking fingerprints, TrackSign+ builds a novel 4-mode network graph that captures the relationship between domains, URLs, online resources, and code fingerprints. We evaluated TrackSign+ with the 1.5M most popular Internet domains, including more than 45M web resources from almost 77M HTTP requests. Our results show that our method can detect new web tracking resources with high precision (over 92\%). TrackSign+ was able to detect more than 300k new trackers, 800k new tracking resources, and 4.5M new tracking URLs, not yet detected by most popular pattern lists at the time. Finally, we also validated the effectiveness of TrackSign+ with more than 20 years of historical data from the Internet Archive.},
  archive      = {J_COMCOM},
  author       = {Ismael Castell-Uroz and Óscar Sánchez-de-Mingo and Pere Barlet-Ros},
  doi          = {10.1016/j.comcom.2023.08.017},
  journal      = {Computer Communications},
  pages        = {273-284},
  shortjournal = {Comput. Commun.},
  title        = {Early detection of new web tracking methods across 1.5 million sites},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking methodology for stateful NAT64 gateways.
<em>COMCOM</em>, <em>210</em>, 256–272. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The benchmarking of Network Address and Protocol Translation from IPv6 clients to IPv4 servers (stateful NAT64) gateways is challenging from a methodological point of view because the state of the art benchmarking standards have some requirements that are conflicting when applied to stateful NAT64 gateways. In this paper, several methodological gaps are pointed out and a benchmarking methodology is proposed, which is applicable for any stateful NATxy gateways, where x and y are in {4, 6}. It bridges all the gaps by reconciling the conflicting requirements and facilitating the execution of the industry standard benchmarking measurement procedures (throughput, latency, frame loss rate, packet delay variation) with stateful NATxy gateways. New performance metrics specific to stateful testing are also defined: maximum connection establishment rate, connection tear down rate, and connection tracking table capacity. The proposed methodology is suitable for examining the scalability of the stateful NATxy gateways, too. The methodology is validated by applying it to the benchmarking of three radically different stateful NAT64 implementations: Jool, tayga plus iptables, and OpenBSD Packet Filter (PF). The details of the measurements and their results are fully disclosed.},
  archive      = {J_COMCOM},
  author       = {Gábor Lencse and Keiichi Shima and Kenjiro Cho},
  doi          = {10.1016/j.comcom.2023.08.009},
  journal      = {Computer Communications},
  pages        = {256-272},
  shortjournal = {Comput. Commun.},
  title        = {Benchmarking methodology for stateful NAT64 gateways},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theoretical modeling and analysis of uplink performance for
three-dimension spatial RISs-aided wireless communication systems.
<em>COMCOM</em>, <em>210</em>, 243–255. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable intelligent surface (RIS) has emerged as a crucial technology capable of improving the performance of future wireless communication (WC) systems. Although a significant body of studies has investigated into the performance analysis of RIS-aided WC systems, most of them fail to consider the impact of multiple RISs on WC systems. Particularly, the influence of RISs randomly distributed in 3-D (three dimension) space is still an open issue. Furthermore, how phase shift error and ARQ (automatic repeat request) scheme affect the transmission behavior of RIS-aided WC systems should also be taken into account. In light of the above limitations, we propose a novel theoretical model to analyze the uplink transmission performance of 3-D spatial RISs-aided WC systems. In the modeling process, we firstly provide an end-to-end (E2E) channel model using a single RIS, where the RIS enables optimal phase shift control. Next we construct a 3-D spatial uplink transmission model, where the multiple RISs are spatially distributed as a homogeneous 3-D PPP (Poisson point process). The impacts of multiple factors including the selection of RISs, buffer size of user, traffic rate and ARQ scheme are comprehensively considered. With this, we derive the closed-form expressions of uplink transmission metrics. Moreover, we further extend the proposed theoretical model under imperfect phase shift control. Finally, we evaluate the uplink transmission performance of 3-D spatial RIS-aided WC systems, and validate the proposed theoretical model.},
  archive      = {J_COMCOM},
  author       = {Sheng Hao and Yu He and Huyin Zhang and Jianqun Cui},
  doi          = {10.1016/j.comcom.2023.07.035},
  journal      = {Computer Communications},
  pages        = {243-255},
  shortjournal = {Comput. Commun.},
  title        = {Theoretical modeling and analysis of uplink performance for three-dimension spatial RISs-aided wireless communication systems},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximizing the stable throughput of heterogeneous nodes
under airtime fairness in a CSMA environment. <em>COMCOM</em>,
<em>210</em>, 229–242. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability region of non-persistent CSMA is analyzed in a general heterogeneous network , where stations have different mean packet arrival rates , packet transmission times probability distributions and transmission probabilities. The considered model of CSMA captures the behavior of the well known CSMA/CA, at least as far as stability and throughput evaluation are concerned. The analysis is done both with and without collision detection . Given the characterization of the stability region, throughput-optimal transmission probabilities are identified under airtime fairness, establishing asymptotic upper and lower bounds of the maximum achievable stable throughput. The bounds turn out to be insensitive to the probability distribution of packet transmission times. Numerical results highlight that the obtained bounds are tight not only asymptotically, but also for essentially all values of the number of stations. The insight gained leads to the definition of a distributed adaptive algorithm to adjust the transmission probabilities of stations so as to attain the maximum stable throughput.},
  archive      = {J_COMCOM},
  author       = {Andrea Baiocchi},
  doi          = {10.1016/j.comcom.2023.08.010},
  journal      = {Computer Communications},
  pages        = {229-242},
  shortjournal = {Comput. Commun.},
  title        = {Maximizing the stable throughput of heterogeneous nodes under airtime fairness in a CSMA environment},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-UAV-assisted computation offloading in DT-based
networks: A distributed deep reinforcement learning approach.
<em>COMCOM</em>, <em>210</em>, 217–228. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the industrial Internet, Mobile Edge Computing (MEC) can provide the ability to transfer a large number of delay-sensitive and compute-intensive tasks to MEC servers, thus improving Quality of Service (QoS). Considering Unmanned Aerial Vehicles (UAVs) have the advantages of wide communication coverage and low deployment cost, UAVs have great potential to be employed as aerial base stations to provide computation resources for Intelligent Mobile Devices (IMDs). Due to the limited computation resources and energy of IMDs, we designed a multi-UAV-assisted MEC system in ground cells. To minimize the weighted sum of task completion delay and energy consumption, and ensure the QoS requirements of IMDs, we jointly consider the dynamic channel state, renewable energy utilization, UAVs trajectory, and tasks offloading ratio. To solve the non-convexity problem of complex high-dimensional states, we propose a model-free Deep Reinforcement Learning (DRL) offloading scheme based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. Moreover, we adopt Federated Learning (FL) to train DRL models to enhance the robustness of the model and the security of IMDs data. Meanwhile, the real environment is modeled as Digital Twin (DT) to monitor network changes and train the local DRL model, and the central cloud server can obtain the local model in real-time to aggregate the global model. Extensive experimental numerical results show that the proposed algorithm improves the system energy efficiency and reduces task completion delay.},
  archive      = {J_COMCOM},
  author       = {Junling Shi and Chunyu Li and Yunchong Guan and Peiyu Cong and Jie Li},
  doi          = {10.1016/j.comcom.2023.07.041},
  journal      = {Computer Communications},
  pages        = {217-228},
  shortjournal = {Comput. Commun.},
  title        = {Multi-UAV-assisted computation offloading in DT-based networks: A distributed deep reinforcement learning approach},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CAMELAMA: Cooperative awareness and spaceborne monitoring
enabled by location-assisted medium access. <em>COMCOM</em>,
<em>210</em>, 205–216. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In beaconing systems such as AIS or ADS-B, used by ships and aircraft, each node periodically broadcasts its navigational state to nearby nodes to increase traffic safety. Nowadays, these beacons are also used as a source for satellite-based global traffic monitoring. This dual use imposes competing needs on the medium access control protocol as the size of the collision domains varies by a large factor between the use cases. Even the subproblem of solely avoiding terrestrial nodes’ packets to collide from the perspective of a receiving satellite is not trivial to solve if the satellite’s collision domain spans multiple hops in the terrestrial network . Based on ideas of the LAMA protocol, we propose CAMELAMA, a novel contention-free medium access control protocol for position awareness beaconing. Our low-overhead approach uses neither forwarding of node state nor handshakes but only the navigational data that is shared between the terrestrial nodes anyway. CAMELAMA provides local cooperative awareness while at the same time desynchronizing transmissions within a satellite’s collision domain even in terrestrially disconnected topologies. In a simulation-based performance evaluation we find that CAMELAMA outperforms SO-TDMA (the MAC protocol of AIS) and also scales better with respect to high node densities.},
  archive      = {J_COMCOM},
  author       = {Holger Döbler and Björn Scheuermann},
  doi          = {10.1016/j.comcom.2023.07.015},
  journal      = {Computer Communications},
  pages        = {205-216},
  shortjournal = {Comput. Commun.},
  title        = {CAMELAMA: Cooperative awareness and spaceborne monitoring enabled by location-assisted medium access},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-graph fusion based graph convolutional networks for
traffic prediction. <em>COMCOM</em>, <em>210</em>, 194–204. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is significant for transportation management and travel route planning, and it is challenging as the spatial dependencies are complex and temporal patterns are dynamic. Local spatial dependencies exist between nodes nearby, and global spatial dependencies are between distant nodes with similar traffic patterns. One direct method to capture multiple spatial dependencies is to design a prediction model with multiple graph convolutional networks . However, it introduces high memory and parameter costs. The same with the learnable adjacency matrix-based approaches. Furthermore, existing methods are inefficient for temporal dependency modeling. To overcome such limitations, we propose a multi-Graph Fusion-based Graph Convolutional Network (GFGCN) for traffic prediction, where a multi-graph fused graph convolutional module is proposed without building multiple graph convolutional networks . The adjacency matrix in one graph convolutional network can reflect multiple spatial relationships through subspace merging on the Grassmann manifold. Moreover, a temporal module combined with the attention mechanism and a dilated convolutional network to model the temporal dynamic efficiently is designed. For validation and analysis, extensive experiments on three real-world datasets are performed. Experimental results show that the proposed GFGCN outperforms the baselines with better prediction accuracy.},
  archive      = {J_COMCOM},
  author       = {Na Hu and Dafang Zhang and Kun Xie and Wei Liang and Kuanching Li and Albert Zomaya},
  doi          = {10.1016/j.comcom.2023.08.004},
  journal      = {Computer Communications},
  pages        = {194-204},
  shortjournal = {Comput. Commun.},
  title        = {Multi-graph fusion based graph convolutional networks for traffic prediction},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A task-oriented hybrid routing approach based on deep
deterministic policy gradient. <em>COMCOM</em>, <em>210</em>, 183–193.
(<a href="https://doi.org/10.1016/j.comcom.2023.07.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of communication and transmission technologies, the applications of Internet of Things (IoT) and telemedicine are increasingly sensitive to network latency . To meet this urgent demand, learning-based routing approaches emerge, with better performance and higher flexibility. These routing approaches can be divided into single-path and multipath, which have lower transmission delay and better load balancing respectively. However, the traffic scheduling policy is not only affected by the network states but also needs to consider the requirements for traffic tasks. We propose a Task-Oriented Hybrid Routing Approach (TOHRA) based on Deep Deterministic Policy Gradient (DDPG) in Software-Defined Networking (SDN). First, the hybrid routing optimization algorithm comprehensively considers the network states and task requirements, and outputs a task-oriented hybrid routing that combines the advantages of single-path and multipath. Moreover, we design a hop-by-hop traffic segmentation model based on DDPG to output the traffic segmentation ratio on hybrid routing to adapt to the changing transmission path. Experimental results show that TOHRA achieves the best performance of load balancing and network throughput. Especially in the case of Type C tasks in Germany topology, the average network throughput of THORA is increased by 32.86\%, and the average variance of link load rate is reduced by 46.62\%.},
  archive      = {J_COMCOM},
  author       = {Zongxuan Sha and Ru Huo and Chuang Sun and Shuo Wang and Tao Huang},
  doi          = {10.1016/j.comcom.2023.07.040},
  journal      = {Computer Communications},
  pages        = {183-193},
  shortjournal = {Comput. Commun.},
  title        = {A task-oriented hybrid routing approach based on deep deterministic policy gradient},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Edge computing-based generative adversarial network for
photo design style transfer using conditional entropy distance.
<em>COMCOM</em>, <em>210</em>, 174–182. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cell phone photography systems have made significant advancements due to their widespread adoption. However, processing the large amount of data generated by these systems on the cloud often leads to high network latency and bandwidth requirements , limiting computational efficiency. To address this challenge, this study introduces a novel design style transfer method for generating high-quality stylized photos in cell phone photography systems using Generative Adversarial Networks (GANs) with an edge computing-based approach. The proposed method aims to enhance the visual appeal of cell phone images by mimicking the artistic style of paintings through abstraction and emphasis on key features. To achieve this objective, a conditional GAN is trained on meticulously labeled data pairs that identify crucial regions in images. This approach offers an efficient solution for computationally intensive tasks by utilizing inference on edge computing nodes. This effectively reduces network latency and bandwidth requirements while improving computational efficiency. Additionally, a penalty is introduced using conditional entropy distance to ensure consistent entropy levels across different photo samples, resulting in higher quality generated photos. Experimental results demonstrate that the proposed CEDGAN model achieves a detection accuracy ranging from 95\% to 98\%, with consistent results, thus laying a solid foundation for effective photo enhancement. When collecting important portions of materials, CEDGAN maintains a detection accuracy of around 90\% to 95\% without any significant difference in overall performance. As a result, this method opens up new possibilities for cell phone photography enthusiasts who desire to effortlessly create artistic and stylized photos using edge computing-based GANs. It offers a practical solution for combining technological innovation with creativity in photography.},
  archive      = {J_COMCOM},
  author       = {Shan Liu and Qi Zhang and Lingling Huang},
  doi          = {10.1016/j.comcom.2023.07.027},
  journal      = {Computer Communications},
  pages        = {174-182},
  shortjournal = {Comput. Commun.},
  title        = {Edge computing-based generative adversarial network for photo design style transfer using conditional entropy distance},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NFV-empowered digital twin cyber platform: Architecture,
prototype, and a use case. <em>COMCOM</em>, <em>210</em>, 163–173. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building, managing, upgrading, and diagnosing networks becomes increasingly difficult due to the increased complexity in protocol design, enlarged network scale, rapidly-growing types of network applications, and the uncertainty of network faults and cyber attacks . It is not satisfactory to rely on network simulators/emulators for conducting these tasks because of different defects including poor fidelity, long implementing time, and weak interconnection with physical networks. To enable the interplay of networks and their virtual counterparts, this paper aims to build a Network Functions Virtualization (NFV)-empowered digital twin cyber platform (DTCP) for enabling network innovation and test with high fidelity, low complexity, and high timeliness. First, after regulating the paradigm of DTCP, a six-dimensional model is defined to profile a DTCP. Second, the architecture of the DTCP is detailed along with the key enabling technologies. Third, a prototype implementation of the DTCP is shortly introduced. Lastly, a use case based on fault diagnosis for video conference system is tested on the prototype. Results show that our DTCP prototype is a good platform for conducting different types of network operations without explicitly providing the mapping model between physical and virtual networks.},
  archive      = {J_COMCOM},
  author       = {Li Deng and Xianglin Wei and Yan Gao and Guang Cheng and Liang Liu and Ming Chen},
  doi          = {10.1016/j.comcom.2023.07.030},
  journal      = {Computer Communications},
  pages        = {163-173},
  shortjournal = {Comput. Commun.},
  title        = {NFV-empowered digital twin cyber platform: Architecture, prototype, and a use case},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel routing protocol for underwater wireless sensor
networks based on shifted energy efficiency and priority.
<em>COMCOM</em>, <em>210</em>, 147–162. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Wireless Sensor Networks (UWSNs) are among the most promising research areas these days due to their unique characteristics and diverse underwater applications. Though a number of routing protocols have been designed and implemented for UWSNs over the past few years, the researchers face several challenges, e.g., low speed of propagation, small bandwidth, limited battery power, etc., while designing routing protocols for communication in UWSNs. Acoustic sensor nodes are equipped with batteries with limited power and it is quite costly to replace or recharge them. The network will not survive for the desired period of time if the power of node batteries is not efficiently used. To effectively resolve this issue, this paper proposes a Shifted Energy Efficiency and Priority (SHEEP) routing protocol for UWSNs. The proposed protocol aims to enhance the efficiency of the state-of-the-art Energy Balanced Efficient and Reliable Routing ( E B E R 2 EBER2 ) protocol for UWSNs. SHEEP is built upon the depth and energy of the current forwarding node, the depth of the expected next forwarding node, and the average energy difference among the expected forwarders. Simulation results demonstrate that SHEEP improves the energy efficiency and packet delivery ratio in comparison to E B E R 2 EBER2 by 7.4\% and 13\% respectively.},
  archive      = {J_COMCOM},
  author       = {Muhammad Ismail and Hamza Qadir and Farrukh Aslam Khan and Sadeeq Jan and Zahid Wadud and Ali Kashif Bashir},
  doi          = {10.1016/j.comcom.2023.07.014},
  journal      = {Computer Communications},
  pages        = {147-162},
  shortjournal = {Comput. Commun.},
  title        = {A novel routing protocol for underwater wireless sensor networks based on shifted energy efficiency and priority},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-UAV-assisted covert communications for secure content
delivery in internet of things. <em>COMCOM</em>, <em>210</em>, 138–146.
(<a href="https://doi.org/10.1016/j.comcom.2023.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have found extensive applications in fast deploying Internet of Things in non-infrastructure areas due to their high maneuverability and robust networking. However, the open nature of wireless channels makes air-to-ground content delivery face serious security risks. A multi-UAV-assisted covert communication system model is proposed in this paper, which focuses on the secure content delivery from multiple UAVs to multiple legitimate ground users (LGUs) under the detection of an illegal interceptor. Our objective is to maximize the average covert rate (ACR) for all LGUs by synergistically optimizing UAV-LGU scheduling, multi-UAV transmit power, and multi-UAV trajectory, while complying with the covertness constraint. To cope with the formulated non-convex problem, we divide it into three sub-problems, and then propose an iterative algorithm to alternately solve them. Simulation results demonstrate that the proposed algorithm can significantly improve the ACR of the system by adaptively adjusting the Multi-UAV trajectory, flight speed and transmit power, while meeting the covertness constraint. Moreover, compared with the two benchmark schemes, the proposed joint optimization scheme can enhance the ACR by 5\% and 31\%, respectively, when the flight period is 100 s.},
  archive      = {J_COMCOM},
  author       = {Ye Liu and Zhenyu Na and Yue Zhang and Xiaofei Qin and Bin Lin},
  doi          = {10.1016/j.comcom.2023.08.006},
  journal      = {Computer Communications},
  pages        = {138-146},
  shortjournal = {Comput. Commun.},
  title        = {Multi-UAV-assisted covert communications for secure content delivery in internet of things},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robustness of satellite constellation networks.
<em>COMCOM</em>, <em>210</em>, 130–137. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss how resilient satellite constellation networks are against attacks. Two types of robustness are focused in this paper. One is the topology-related network robustness, which mainly assesses the effect of attacks or faults on satellites and links. The other is the network function robustness related to routing mechanisms, which mainly assesses how resource allocation mechanisms affect network robustness. To this purpose, two satellite constellation network models based on physical network topology and traffic are proposed, along with a new satellite importance index and the robustness metrics. Based on the newly proposed satellite importance index, three different types of attacking strategies are implemented in this paper, i.e., random attacks, selective attacks based on the initial state of the network, and selective attacks based on the current state of the network. The routing methods involved in function robustness mainly include different local state routing algorithms , which we summarize into a brand-new routing model with tunable parameters. Simulation results show that the removals by the selective attacks based on the current state of the network are often more harmful than the other attack strategies. Constellation networks almost collapse for the above attack strategies as the attack ratio are nearly 0.6, 0.4, and 0.2 respectively. Meanwhile, the larger constellation is more robust than the smaller one against the attacks before collapses. However, the larger constellation collapses earlier for the selective attack strategies. In addition, local-state routing algorithms with a certain level of state awareness capability can be used to improve the network function robustness.},
  archive      = {J_COMCOM},
  author       = {Xin Xu and Zhixiang Gao and Aijun Liu},
  doi          = {10.1016/j.comcom.2023.07.036},
  journal      = {Computer Communications},
  pages        = {130-137},
  shortjournal = {Comput. Commun.},
  title        = {Robustness of satellite constellation networks},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple UAVs collaborative traffic monitoring with
intention-based communication. <em>COMCOM</em>, <em>210</em>, 116–129.
(<a href="https://doi.org/10.1016/j.comcom.2023.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an innovative ground traffic management approach that utilizes the mobility, flexibility, and collaborative capabilities of multiple Unmanned Aerial Vehicles (UAVs). The objective is to enhance the navigation and driving experience of self-driving vehicles by employing UAVs to cover blind areas that cannot be observed by ground monitoring equipment, thereby facilitating the avoidance of congested routes. This study focuses on determining the optimal number of UAVs and enhancing the scheduling strategy and task assignment method by introducing novel UAV communication collaboration techniques. The research faces challenges such as a limited quantity of UAVs, rapid response and feedback, variations in traffic conditions and tasks, and system complexity. Furthermore, the difficulty of communication collaboration is exacerbated by the incomplete connectivity of UAV networks. To address these challenges, this article introduces a method that utilizes intention information as the message for communication between UAVs, thereby enhancing their collaborative capabilities. Using the Multi-Agent Reinforcement Learning (MARL) neural network , this method generates intention information and utilizes the intention information generated by other UAVs to optimize the flight control and traffic monitoring task allocation strategy. This method effectively addresses the challenge of UAV collaboration in efficiently managing a large volume of stochastic traffic monitoring tasks. Experimental results demonstrate that the proposed approach significantly improves vehicle navigation accuracy and achieves a more balanced distribution of workload among multiple UAVs. Moreover, by sharing road conditions detected in the cloud, it reduces operational costs and opens up possibilities for the practical implementation of UAVs in traffic management.},
  archive      = {J_COMCOM},
  author       = {Shuai Liu and Yuebin Bai},
  doi          = {10.1016/j.comcom.2023.08.005},
  journal      = {Computer Communications},
  pages        = {116-129},
  shortjournal = {Comput. Commun.},
  title        = {Multiple UAVs collaborative traffic monitoring with intention-based communication},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-efficient power control strategy of the delay
tolerable service based on the reinforcement learning. <em>COMCOM</em>,
<em>210</em>, 102–115. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the rapid development of Internet technology and its applications has led to an exponential growth in the number of Internet users and wireless terminal devices, resulting in a corresponding increase in energy consumption. This has necessitated the need to reduce energy consumption while maintaining the quality of communication services. To this end, we investigate the possibility of improving energy efficiency (EE) of delay tolerable (DT) services by allocating resources based on the time-domain water-filling algorithm. We first transform the non-convex problem of maximizing EE into a convex problem of minimizing transmission power to obtain the optimal solution, and then use the greedy algorithm to obtain an upper bound. Furthermore, to capture a more realistic scenario, an Approximate Statistical Dynamic Programming (ASDP) algorithm is introduced, but its effect on enhancing EE is limited. To overcome this limitation, three Deep Reinforcement Learning (DRL) algorithms are implemented. The simulations results show that the settings of maximum transmit power and SNR during agent training have an impact on the performance of the agent. Finally, by comparing the mean values of transmission power, outage probability , equilibrium power and performance improvement percentage of several algorithms, we conclude that the Deep Deterministic Policy Gradient (DDPG) algorithm produces the best agent performance in the environment with a fixed SNR of 2 (dB).},
  archive      = {J_COMCOM},
  author       = {Mengmeng Bai and Rui Zhu and Jianxin Guo and Feng Wang and Hangjie Zhu and Yushuai Zhang},
  doi          = {10.1016/j.comcom.2023.07.034},
  journal      = {Computer Communications},
  pages        = {102-115},
  shortjournal = {Comput. Commun.},
  title        = {Energy-efficient power control strategy of the delay tolerable service based on the reinforcement learning},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid classical relay and advanced RISs for performance
enhancement of IoT sensor networks with impaired hardware.
<em>COMCOM</em>, <em>210</em>, 90–101. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a combination of traditional relay and advanced reconfigurable intelligent surfaces (RISs) in an Internet of things (IoT) sensor network. In particular, due to the long-range transmission from IoT sensor to receiver, a traditional relay is deployed between them. To greatly enhance the reliabilities of IoT-relay and relay-receiver communications, two RIS groups are exploited where each RIS group has multiple RISs distributed in different areas. Since the IoT, relay, and receiver are often low-cost devices, the hardware impairments (HI) are taken into account in the proposed system. We derive the closed-form expressions of average symbol error rate (SER) and capacity of the proposed hybrid RISs and relay aided IoT sensor system affected by HI and with direct IoT-relay and relay-receiver links (called as the hybrid RIS-R-HI-Wi system). Then, the performance of the hybrid RIS-R-HI-Wi system is compared with that of the relevant systems such as the hybrid RIS-R-HI system without direct links (the RIS-R-HI-Wo system), the RIS-R-ideal hardware (ID) systems with and without direct links (the RIS-R-ID-Wi and RIS-R-ID-Wo systems), and the traditional relay systems with HI and ID (the R-HI and R-ID systems). The theoretical expressions are validated through Monte-Carlo simulations. Numerical illustrations show the huge benefits of utilizing multi-RIS and relay in the hybrid RIS-R-HI-Wi system. Therefore, the performance of the RIS-R-HI-Wi system is dramatically higher than that of the RIS-R-HI-Wo and R-HI systems. Moreover, a significant performance loss caused by HI and high carrier frequencies is determined. Thus, the SER and capacity of the RIS-R-HI-Wi, RIS-R-HI-Wo, and R-HI systems are saturated in the high transmit power regime. In this circumstance, various effective methods such as reducing the distortion noises caused by the HI, using the suitable carrier frequencies, and choosing the appropriate locations of the RISs should be applied in the hybrid RIS-R-HI-Wi system for improving the performance significantly.},
  archive      = {J_COMCOM},
  author       = {Phuong T. Tran and Ba Cao Nguyen and Tran Manh Hoang and Huu Minh Nguyen and Nguyen Van Vinh},
  doi          = {10.1016/j.comcom.2023.07.038},
  journal      = {Computer Communications},
  pages        = {90-101},
  shortjournal = {Comput. Commun.},
  title        = {Hybrid classical relay and advanced RISs for performance enhancement of IoT sensor networks with impaired hardware},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bandwidth abstraction with the end-to-end latency-bounded
reliability provisioning based on martingale theory. <em>COMCOM</em>,
<em>210</em>, 79–89. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The provisioning of reliability regard to end-to-end (E2E) latency is crux for Ultra-Reliable Low Latency Communications (URLLCs). Based on martingale theory and stochastic network calculus , an E2E latency-bounded reliability analysis framework is proposed for the multi-hop system, where the arrival traffic possesses burstiness and the services provided by nodes are heterogeneous. The Wald martingales of arrival and service processes are constructed respectively, which contribute to reveal the influence of multiple random processes entangled with each other on the E2E latency. Leveraging the Doob maximum inequality of martingales and the features of moment generation functions, a tight upper bound of the unreliability regard to E2E latency is captured. A martingale parameter, named as tandem service descriptor, is proposed to embody the features of the tandem service mode provided for the specifically bursty traffic . The bandwidth abstraction algorithm is designed, which decouples the E2E latency-bounded reliability requirement as the demanded bandwidth of each node. An instantiation scheme of service rates is proposed to conduct the dynamical transmission power allocation according to the access states and channel characteristics in the wireless access network .},
  archive      = {J_COMCOM},
  author       = {Baozhu Yu and Wei Liu and Xuefen Chi},
  doi          = {10.1016/j.comcom.2023.08.003},
  journal      = {Computer Communications},
  pages        = {79-89},
  shortjournal = {Comput. Commun.},
  title        = {Bandwidth abstraction with the end-to-end latency-bounded reliability provisioning based on martingale theory},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the optimal resistance against mafia and distance fraud
in distance-bounding protocols. <em>COMCOM</em>, <em>210</em>, 69–78.
(<a href="https://doi.org/10.1016/j.comcom.2023.07.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance-bounding protocols are security protocols with a time measurement phase used to detect relay attacks, whose security is typically measured against mafia-fraud and distance-fraud attacks. A prominent subclass of distance-bounding protocols, known as lookup-based protocols , use simple lookup operations to diminish the impact of the computation time in the distance calculation. Independent results have found theoretical lower bounds 1 2 n n 2 + 1 12nn2+1 and 1 2 n 12n , where n n is the number of time measurement rounds, on the security of lookup-based protocols against mafia and distance-fraud attacks, respectively. However, it is still an open question whether there exists a protocol achieving both security bounds. This article closes this question in two ways. First, we prove that the two lower bounds are mutually exclusive, meaning that there does not exist a lookup-based protocol that provides optimal protection against both types of attacks. Second, we provide a lookup-based protocol that approximates those bounds by a small constant factor. Our experiments show that, restricted to a memory size that linearly grows with n n , our protocol offers strictly better security than previous lookup-based protocols against both types of fraud.},
  archive      = {J_COMCOM},
  author       = {Reynaldo Gil-Pons and Sjouke Mauw and Rolando Trujillo-Rasua},
  doi          = {10.1016/j.comcom.2023.07.033},
  journal      = {Computer Communications},
  pages        = {69-78},
  shortjournal = {Comput. Commun.},
  title        = {On the optimal resistance against mafia and distance fraud in distance-bounding protocols},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An auction-based distributed network slicing scheme for
resource allocation in satellite-UAV integrated networks.
<em>COMCOM</em>, <em>210</em>, 58–68. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is a promising solution to satisfy different service quality requirements in the satellite-UAV integrated network of B5G and 6G. To relieve the significant transmission delay and system overhead during network slicing, we propose an auction-based distributed network slicing scheme (ADNSS), in which the resource allocation is collaboratively performed by satellite heads using an auction algorithm without transmitting the messages to the ground station in a centralized way. The convergence and service delay performance of ADNSS is analyzed in this paper. Moreover, considering that the deviation of theoretical slice analysis models from actual ones may lead to the degradation of service delay performance of ADNSS, we design an online optimization method driven by the multi-arm bandit algorithm (ORLOM) to enhance the accuracy of slice models of ADNSS in the actual environment. Numerical results show that we can achieve optimal service delay performance by ADNSS applying ORLOM after finite slicing windows and realize 8\% performance improvement compared with ADNSS without ORLOM when the service number is 9. In addition, ADNSS can reduce the system overhead by 8.5\%–76\% compared with the centralized method in the scenario where the hops count from satellite heads to the ground station exceeds that between satellite heads by 3. As the hops count difference and service number increase, ADNSS shows more significant advantages in reducing service delay and system overhead.},
  archive      = {J_COMCOM},
  author       = {Xin Tong and Xu Li and Ying Liu},
  doi          = {10.1016/j.comcom.2023.08.002},
  journal      = {Computer Communications},
  pages        = {58-68},
  shortjournal = {Comput. Commun.},
  title        = {An auction-based distributed network slicing scheme for resource allocation in satellite-UAV integrated networks},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RUMP: Resource usage multi-step prediction in extreme edge
computing. <em>COMCOM</em>, <em>210</em>, 45–57. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme Edge Computing that leverages the copious yet underutilized computational resources of Extreme Edge Devices (EEDs) has gained significant momentum lately. Estimating the computational capabilities of EEDs can be strenuously challenging since EEDs are user-owned devices, and are thus subject to a highly dynamic user access behavior (i.e., dynamic resource usage). In this paper, we propose the Resource Usage Multi-step Prediction (RUMP) scheme. RUMP is the first scheme that strives to enable multistep-ahead prediction of the dynamic resource usage of EEDs (i.e., workers) in a computationally efficient way, while providing a relatively high prediction accuracy. Towards that end, RUMP exploits the use of the Hierarchical Dirichlet Process-Hidden Semi-Markov Model (HDP-HSMM). In addition, RUMP uses the Simple and Exponential Moving Average (SMA&amp;EMA) and Savitzky-Golay (SG) filters to improve the prediction accuracy. We scrupulously study the trade-off between computational efficiency, prediction accuracy, practicality, and adaptability of the underlying prediction model by conducting complexity analysis, performing various experiments on a testbed of heterogeneous workers, and comparing the HDP-HSMM model used in RUMP to three other prominent prediction models in different dynamic resource usage scenarios . Extensive evaluations show that RUMP achieves a 91\% categorical multi-step prediction accuracy and renders a small performance gap of 6\% on average in terms of the Mean Absolute Error (MAE) compared to representatives of state-of-the-art prediction models, while yielding a low computational complexity .},
  archive      = {J_COMCOM},
  author       = {Ruslan Kain and Sara A. Elsayed and Yuanzhu Chen and Hossam S. Hassanein},
  doi          = {10.1016/j.comcom.2023.07.029},
  journal      = {Computer Communications},
  pages        = {45-57},
  shortjournal = {Comput. Commun.},
  title        = {RUMP: Resource usage multi-step prediction in extreme edge computing},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward the simulation of WiFi fine time measurements in NS3
network simulator. <em>COMCOM</em>, <em>210</em>, 35–44. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi has become the most widely used indoor positioning technology. The Fine Time Measurement (FTM) protocol introduced in the IEEE 802.11-2016 standard uses radio frequency based two-way time-of-flight (ToF) estimation, which promises precise indoor ranging and positioning. However, even with an ToF approach an exact indoor positioning is challenging due to the peculiarities of the propagation of the wireless signal such as signal attenuation , multipath propagation and signal fading. Moreover, the used WiFi hardware and its configuration like channel bandwidth also plays a major role. We present FTM-ns3 , a software module which implements the 802.11 FTM protocol so that it can be used within the widely used ns3 network simulator . Moreover, we conducted experiments using commodity WiFi-FTM hardware, Intel 8260 and ESP32, and derived empirical error models which can be used in simulations to study the performance of novel FTM-based localization schemes under real channel propagation conditions while taking into account the specifics of the used WiFi hardware and configuration of FTM. Finally, we present results from simulations of a simple localization scheme based on FTM and multilateration which show the great influence of ranging inaccuracy introduced due to multipath propagation in typical indoor environments with line-of-sight (LoS) but strong multipath. Our module is provided to the community as open source and can be easily customized and extended.},
  archive      = {J_COMCOM},
  author       = {Anatolij Zubow and Christos Laskos and Falko Dressler},
  doi          = {10.1016/j.comcom.2023.07.028},
  journal      = {Computer Communications},
  pages        = {35-44},
  shortjournal = {Comput. Commun.},
  title        = {Toward the simulation of WiFi fine time measurements in NS3 network simulator},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic modelling of multi-layer HAP-LEO systems in 6G
for energy saving: An analytical approach. <em>COMCOM</em>,
<em>210</em>, 22–34. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current era of communication systems , the next-generation technology known as sixth generation (6G) is anticipated to extensively utilize space-air-terrestrial networks (SAT). High altitude platforms (HAPs) and low earth orbit (LEO) satellites are essential components of SAT network. The presented study proposes an architecture which concentrates on energy saving of HAP in the SAT network. Further, a stochastic model has been proposed which considers three energy saving states of HAP; power saving, standby, and sleep states. It has been assumed that in these states, the energy consumption of HAP is very low or zero. Whenever, there is an arrival of a packet in the system, HAP will start providing service immediately and the whole system will be in active state. The analysis of the stochastic model has been carried out using Markov process (MP) and semi-Markov process (SMP). The system has been analysed in steady-state for both the MP and SMP to obtain the steady-state probabilities. Through the numerical results, the effective contribution of energy saving states in enhancing the energy saving of HAP system has been provided and a comparison for the MP and SMP case has been described in detail. To verify the accuracy of the presented model, a discrete-event simulation approach is employed.},
  archive      = {J_COMCOM},
  author       = {Raina Raj and S. Dharmaraja},
  doi          = {10.1016/j.comcom.2023.07.037},
  journal      = {Computer Communications},
  pages        = {22-34},
  shortjournal = {Comput. Commun.},
  title        = {Stochastic modelling of multi-layer HAP-LEO systems in 6G for energy saving: An analytical approach},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed routing-aware power control scheme for
underwater wireless sensor networks. <em>COMCOM</em>, <em>210</em>,
10–21. (<a href="https://doi.org/10.1016/j.comcom.2023.07.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the data routing process in underwater wireless sensor networks , the communication ranges of nodes profoundly affect the network performances, such as energy utilization and delay overhead. Thus, in this paper, we design a routing-aware power control scheme for underwater wireless sensor networks , whose primary purpose is to assign a differentiated optimal communication radius for each sensor node . Compared with the existing studies, this scheme pays more attention to the combination of power control and existing opportunity routing mechanisms, the distributed implementation of algorithms, and the specific characteristics of underwater sensor network topology . The scheme consists of two main algorithms, namely, the rough adjustment algorithm and the fine adjustment algorithm. These two algorithms employ the game theory and the flooding mechanism to achieve the trade-off among network connectivity maintenance, routing performance promotion, and energy utilization optimization during the power control process. Based on the theoretical analysis, with the proposed algorithms, each node can adjust its transmitted power in a distributed, convergent, and low-complexity manner. According to our numerical simulation experiment, the designed power control scheme could improve the diverse networking performances, including the network lifetime, the packet delivery ratio , the energy efficiency, the load balance, and the end-to-end delay.},
  archive      = {J_COMCOM},
  author       = {Zhongwei Shen and Hongxi Yin and Fangyuan Xing and Xiuyang Ji and An Huang},
  doi          = {10.1016/j.comcom.2023.07.024},
  journal      = {Computer Communications},
  pages        = {10-21},
  shortjournal = {Comput. Commun.},
  title        = {A distributed routing-aware power control scheme for underwater wireless sensor networks},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multimodal dual-fusion entity extraction model for large
and complex devices. <em>COMCOM</em>, <em>210</em>, 1–9. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of large and complex devices with a multi-source heterogeneous data environment, the extraction of network device configuration related entity information from diverse modalities of the Internet of Things data is a crucial and fundamental step towards establishing a domain knowledge graph for global Zero Touch Provisioning of network devices. In this paper, we present a novel multimodal dual-fusion entity extraction model that serves as a foundation for an intelligent and efficient network device configuration process . Firstly, the multimodal data is encoded, followed by the ViLBERT pre-training model to obtain more feature information of entities for multimodal front-end fusion. Next, the attention weights of each modal feature are learned through a multilayer neural network classifier and probabilistic graphical model , facilitating multimodal back-end fusion and reducing information redundancy . Finally, entity recognition is accomplished by employing a cohesive memory module that extracts the essential parameters for device configuration. The simulation results demonstrate that the proposed model performs exceptionally well on the MSCOCO2017 public dataset and the SFZK-Dev data network device dataset, with F 1 values of the comprehensive evaluation index of model quality at 94.65\% and 96.94\%, respectively, indicating high stability. Additionally, the link prediction metrics HITS@1 achieved accuracy levels of 58.21\% and 68.04\%.},
  archive      = {J_COMCOM},
  author       = {Weiming Tong and Xu Chu and Wenqi Jiang and Zhongwei Li},
  doi          = {10.1016/j.comcom.2023.07.026},
  journal      = {Computer Communications},
  pages        = {1-9},
  shortjournal = {Comput. Commun.},
  title        = {A multimodal dual-fusion entity extraction model for large and complex devices},
  volume       = {210},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A joint cluster-based RRM and low-latency framework using
the full-duplex mechanism for NR-V2X networks. <em>COMCOM</em>,
<em>209</em>, 513–525. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The radio resource management (RRM) problem in new radio vehicle-to-everything (NR-V2X) communication systems is a combinatorial optimization problem , and it is hard to achieve an optimum result in polynomial time . To reduce the complexity, linear algorithms or meta-heuristics can solve the problem. In this study, we proposed a joint cluster-based resource management and low-latency framework using a full-duplex mechanism (JCRRM-FD) for NR-V2X networks, which is used for vehicle-to-vehicle (V2V)/ device-to-device (D2D) and vehicle-to-infrastructure (V2I) transmission in NR-V2X networks. The delay, resource management, and system throughput analysis of the full-duplex mechanism in this framework was performed to deal with resource utilization and latency constraints. Moreover, joint cluster-based radio resource management and ant colony optimization (ACO) algorithms were proposed to manage and optimize resources efficiently to achieve user separation. The swarm intelligence algorithm is a standard meta-heuristic algorithm employed to deal with the optimization concern by exploiting the V2X communication network’s unlimited speed (maximizing the data transfer capacity and overall network performance) while considering the quality-of-service (QoS) requirements. A comprehensive experiment analysis was enacted to evaluate the efficiency of the developed JCRRM-FD framework with baseline approaches. Based on the simulation results, the proposed JCRRM-FD framework enhances the fairness index , average delay, packet drop rate , best cost value, CDF, and throughput compared to the benchmark approaches.},
  archive      = {J_COMCOM},
  author       = {Syed Muhammad Waqas and Yazhe Tang and Lisu Yu and Fakhar Abbas},
  doi          = {10.1016/j.comcom.2023.07.032},
  journal      = {Computer Communications},
  pages        = {513-525},
  shortjournal = {Comput. Commun.},
  title        = {A joint cluster-based RRM and low-latency framework using the full-duplex mechanism for NR-V2X networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A study on 5G performance and fast conditional handover for
public transit systems. <em>COMCOM</em>, <em>209</em>, 499–512. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fifth-generation (5G) networks are now in a stable phase in terms of commercial release. 5G design is flexible to support a diverse range of radio bands (i.e., low-, mid-, and high-band) and application requirements. Since its initial roll-out in 2019, extensive measurements studies have revealed key aspects of commercial 5G deployments (e.g., coverage, signal strength, throughput, latency, handover , and power consumption among others) for several scenarios (e.g., pedestrian and car mobility, mid-, and high-bands, etc.). In this paper, we make a twofold contribution. First, we carry out an in-depth measurement study of 5G in a large public bus transit system in a major European city. Second, based on the insights observed with the measurement study, we propose a new target cell selection criteria applicable to Fast Conditional Handover (FCHO), a 3GPP-specific 5G technique to foster reliable mobility. Our results are based on an extensive measurement campaign performed with several mobile phones connected to several mobile network operators totaling more than 1500 km over three months. The measurements reveal how flexible the network deployment is by analyzing Radio Resource Control (RRC) messages, mobility management and the suitability of our FCHO solution, and application performance.},
  archive      = {J_COMCOM},
  author       = {Claudio Fiandrino and David Juárez Martínez-Villanueva and Joerg Widmer},
  doi          = {10.1016/j.comcom.2023.07.020},
  journal      = {Computer Communications},
  pages        = {499-512},
  shortjournal = {Comput. Commun.},
  title        = {A study on 5G performance and fast conditional handover for public transit systems},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain based resource allocation in cloud and
distributed edge computing: A survey. <em>COMCOM</em>, <em>209</em>,
469–498. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing and distributed edge computing provide computing resources to meet the surging demands for computing caused by developments in technologies such as the Internet of Things (IoT) and Mobile communication (5G). Centralized resource allocation approaches in both computing paradigms suffer from single-point failure, tampering, modification in allocation results, and biased actions. Recently, blockchain has become popular in designing decentralized systems because of its features, including transparency, decentralization, and anti-tamper. In this paper, we provide a comprehensive survey of the research works applying blockchain in resource allocation in both computing paradigms , addressing the issues in centralized resource allocation approaches. Firstly, we identify several key research questions acting as motivation. To provide background knowledge, first, we discuss the centralized resource allocation in both computing paradigms and associated challenges. Then we discuss blockchain, its structure, working, characteristics and types, followed by its benefits to resource allocation. We identify several metrics to provide a comparative study of the works. We present a depth overview of blockchain-based resource allocation works in three domains: cloud computing, distributed edge computing and integrated edge and cloud computing. In each domain, works are summarized from three aspects: works using blockchain platforms, works providing blockchain frameworks and works advocating blockchain. We discuss consensus mechanisms in the works related to blockchain-based resource allocation, as the consensus mechanism is a fundamental part of the blockchain. Further, we provide key challenges requiring our attention. Finally, we conclude the survey.},
  archive      = {J_COMCOM},
  author       = {Gaurav Baranwal and Dinesh Kumar and Deo Prakash Vidyarthi},
  doi          = {10.1016/j.comcom.2023.07.023},
  journal      = {Computer Communications},
  pages        = {469-498},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain based resource allocation in cloud and distributed edge computing: A survey},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OWL: A data sharing scheme with controllable anonymity and
integrity for group users. <em>COMCOM</em>, <em>209</em>, 455–468. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As our society becomes digital and communication technology develops, global collaboration is an inevitable trend, where data-sharing is a critical component of cooperation across organizations. Identity privacy and data integrity are vital issues in data-sharing. Existing works struggle to address these problems simultaneously, either privacy leaking or privacy abuse. In this work, we proposed OWL , a data-sharing scheme that (1) provides users on-demand anonymity and (2) allows users to verify data integrity while preserving anonymity. To achieve (1) OWL enables controllable anonymity that allows de-anonymity for the malicious while keeping anonymity to the honest providers based on traceable ring signature technology. To achieve (2), OWL designs a data integrity auditing scheme that uses vector commitment to verify data integrity without privacy leakage . Furthermore, OWL employs the blockchain to store immutable auxiliary information for the integrity and controllable anonymity. We also employ the state channel to resolve the performance bottleneck of blockchain and design methods to improve the usage of the state channel for group users. We prove that OWL achieves controllable anonymity and integrity. Finally, we implement the experiment to evaluate the efficiency of OWL .},
  archive      = {J_COMCOM},
  author       = {Yongxin Zhang and Zijian Bao and Qinghao Wang and Ning Lu and Wenbo Shi and Bangdao Chen and Hong Lei},
  doi          = {10.1016/j.comcom.2023.07.022},
  journal      = {Computer Communications},
  pages        = {455-468},
  shortjournal = {Comput. Commun.},
  title        = {OWL: A data sharing scheme with controllable anonymity and integrity for group users},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A secure physical health test data sharing scheme based on
token distribution and programmable blockchains. <em>COMCOM</em>,
<em>209</em>, 444–454. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computers and networks, more and more convenient services are provided relying on the Internet. In this paper, we focus on the data sharing in students’ physical health tests. To reduce the burden of filling in information manually, physical health test service providers have designed a mode of obtaining students’ information, including personal information and test results, from a trusted third-party tester for gathering all physical health items and automatically filling. This mode is called one network unified management. However, if the trusted third-party organization sends too much unnecessary information to service requester , dishonest requester will maliciously use the students’ redundant information, which causes great security risk to students’ information. To solve the problem, this paper designs a token distribution scheme adapted to that mode based on programmable blockchains . Secure and fast data sharing is approached. Both traceability and evidence collection are available. For a network with 500 participants and size of maximum token set is 5 × 2 10 5×210 , data sharing can be completed within 13.04 microseconds while traceability can be completed within 3.61 microseconds of extra time consumption. Ensuring safety and traceability, the time consumption of the scheme is proved to be linear with the increasing population of participants. It is also tested with real physical health test data, proving that the scheme is available in reality.},
  archive      = {J_COMCOM},
  author       = {Xiangjie Wang and Yifeng Lin and Yuer Yang and Hui Xu and Zhenpeng Luo},
  doi          = {10.1016/j.comcom.2023.06.019},
  journal      = {Computer Communications},
  pages        = {444-454},
  shortjournal = {Comput. Commun.},
  title        = {A secure physical health test data sharing scheme based on token distribution and programmable blockchains},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empirical evaluation of 5G and wi-fi mesh interworking for
integrated access and backhaul networking paradigm. <em>COMCOM</em>,
<em>209</em>, 429–443. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fifth Generation (5G) of mobile networks and beyond have emerged with ambitions to facilitate the deployment and evolution of a wide spectrum of applications such as Industry 4.0 and 5.0 use cases. Despite this trend of increasing importance to upgrade the networked applications to the next generation, the use of 5G and beyond technologies can be a prohibitive barrier for some business sectors due to the high deployment costs that it can incur. To overcome this obstacle, more cost-effective approaches in networking are entailed. In this work, an innovative approach coupling 5G and Wi-Fi mesh networking is proposed and developed as a promising solution to extend 5G services to the indoor use case scenarios whilst being capable of keeping the capital expenditure of the network infrastructure significantly lower. In order to empirically validate and evaluate this new networking paradigm, a number of experiments have been performed over a testbed with a demanding video application as a representative use case. The experimental results prove the gained benefits from this new approach, especially, video users can be more than twice as far away without compromising the quality of the video consumption experience. Specifically, the results show that users can be 29\% further away using a single router, and 100\% further away if a second router is added.},
  archive      = {J_COMCOM},
  author       = {Mohamed Khadmaoui-Bichouna and Jose M. Alcaraz-Calero and Qi Wang},
  doi          = {10.1016/j.comcom.2023.07.007},
  journal      = {Computer Communications},
  pages        = {429-443},
  shortjournal = {Comput. Commun.},
  title        = {Empirical evaluation of 5G and wi-fi mesh interworking for integrated access and backhaul networking paradigm},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FAMG: A flow-aware and mixed granularity method for
load-balancing in data center networks. <em>COMCOM</em>, <em>209</em>,
415–428. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current Data center Networks (DCNs), Equal-Cost Multipath (ECMP) is a default load-balancing scheme. However, using ECMP may result in the rapid growth of Flow Completion Time (FCT) due to its well-known drawbacks. In order to solve the problems of ECMP, load balancing schemes with per-packet granularity (such as RPS) and per-flowlet granularity (such as LetFlow) are proposed. These two different granularities have their features, but there is no existing load-balancing scheme considering both their advantages. Moreover, these schemes ignore the traffic characteristics in DCNs. In this paper, we mix per-flowlet and per-packet granularity in scheduling decisions and consider traffic characteristics. Based on this idea, we propose a flow-aware and mixed granularity method for load-balancing, referred to as FAMG. The key idea of FAMG is to distinguish elephant and mouse flows and schedule them with different granularities. We evaluate FAMG in simulations with NS-3. Experimental results show that FAMG slightly sacrifices elephant flows FCT but it is worth it since our results demonstrate that FAMG is a good trade-off that outperforms RPS, LetFlow, and TLB in terms of mouse flows FCT in both symmetric and asymmetric network topology .},
  archive      = {J_COMCOM},
  author       = {Yifei Lu and Zhengzhi Xu and Xu Ma},
  doi          = {10.1016/j.comcom.2023.07.018},
  journal      = {Computer Communications},
  pages        = {415-428},
  shortjournal = {Comput. Commun.},
  title        = {FAMG: A flow-aware and mixed granularity method for load-balancing in data center networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). IoT-based generalized multi-granulation sequential
three-way decisions. <em>COMCOM</em>, <em>209</em>, 402–414. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the Internet of Things (IoT), decision models and GRC (Governance, Risk, and Compliance) can be used for data processing and decision-making processes. The IoT involves a large number of sensors and devices, some of which enable “device-free” functionality, meaning that specific devices or sensors are not required to obtain the necessary information. This technology utilizes the signals present in the environment to determine the location of individuals without them carrying any specific devices. By incorporating “device-free” sensor data into decision models, we can process IoT data more efficiently and achieve accurate location positioning. The prevailing hierarchical sequential three-way decision models (STDM) follow a common pattern of constructing a hierarchical tree of conditional attributes to acquire knowledge. However, these models fail to meet the diverse needs of users when it comes to rule mining complex hierarchical data with multiple levels and views in real-world applications. Additionally, the pursuit of simplified rules remains a significant topic of interest. To address these challenges, we propose a novel approach that integrates multi-granularity, device-free localization , and STDM. By leveraging IoT and device-free localization , we collect data from various devices and sensors to accurately determine the positions of individuals. Using this data, we construct a generalized hierarchical decision table by creating a concept hierarchical tree with decision attributes . This framework offers a comprehensive solution for overcoming the challenges of device-free localization and IoT. Subsequently, we establish a granular structure by partitioning the complete set of attributes. In this paper, our proposed approach improves the efficiency and accuracy of data processing in machine learning models and enables users to better process and understand complex datasets. Finally, the performance is experimentally validated for the models in this paper. Such models greatly enrich the theoretical framework of three-way decision making. Overall, the abstract provides a concise and succinct overview, outlining the motivation behind the research and presenting the contributions made.},
  archive      = {J_COMCOM},
  author       = {Yongjing Zhang and Guannan Li and Wangchen Dai and Chengxin Hong and Jin Qian and Zhaoyang Han},
  doi          = {10.1016/j.comcom.2023.07.031},
  journal      = {Computer Communications},
  pages        = {402-414},
  shortjournal = {Comput. Commun.},
  title        = {IoT-based generalized multi-granulation sequential three-way decisions},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Average age upon decisions with truncated HARQ and
optimization in the finite blocklength regime. <em>COMCOM</em>,
<em>209</em>, 387–401. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an update-and-decide IoT-based wireless network, where information packets generated from dual sources are co-stored in the transmitter’s buffer, while decisions are made at the destination. Two practical assumptions about the communications between the transmitter and destination are taken into account: the communications are operating with finite blocklength (FBL) codes, and truncated hybrid automatic repeat request (HARQ) schemes are exploited to improve the FBL reliability, i.e., the number of allowed rounds of (re)transmissions is finite. For the first time, this paper characterizes the timeliness of status updates, namely age upon decisions (AuD) (which highlights the timeliness of the information at decisions in comparison to the concept of age of information), for such truncated HARQ-assisted wireless network. First, we characterize the inter-arrival time between two adjacent successfully transmitted packets, while taking into consideration the preemption policy and the randomness of the number of preempted packets from the same source. In particular, the probability density function , statistical performance of such inter-arrival time are derived. Following these characterizations, we propose a new approach to determine the average AuD and obtain a closed-form expression accordingly. Subsequently, a joint blocklength and arrival rate optimization problem is considered to minimize the obtained average AuD. Before addressing the formulated problem, we prove the convexity of decoding error probability in CC-HARQ and IR-HARQ with respect to blocklength. Then, we prove the convexity of average AuD with respect to blocklength and arrival rate, respectively. By an efficient alternating algorithm, we obtain a efficient solution. Via simulations, we evaluate the performance and conclude a set of guidelines for designs on the considered network.},
  archive      = {J_COMCOM},
  author       = {Zhiwei Bao and Yulin Hu and Peng Sun and Azzedine Boukerche and Anke Schmeink},
  doi          = {10.1016/j.comcom.2023.07.010},
  journal      = {Computer Communications},
  pages        = {387-401},
  shortjournal = {Comput. Commun.},
  title        = {Average age upon decisions with truncated HARQ and optimization in the finite blocklength regime},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel hybrid method for achieving accurate and timeliness
vehicular traffic flow prediction in road networks. <em>COMCOM</em>,
<em>209</em>, 378–386. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient and smooth operation of the transportation system is crucial for ensuring the normal functioning of modern society and people’s daily lives. However, the increase in vehicles has led to more frequent and severe traffic congestion due to the limited capacity of existing road networks . Therefore, effectively utilizing the limited traffic capacity of the existing system and reducing the probability of congestion through proper traffic flow management is a critical problem to be solved. In this context, timely and accurate traffic flow forecasting plays an essential role in specifying reasonable traffic control strategies and directly impacting the effectiveness of traffic control measures. However, as a typical large-scale complex network, the movement of traffic on different sections of the road network affects each other. Existing common single-point-based traffic flow prediction methods lack the ability to predict short-term anomalies in traffic flow due to a limited understanding of real-time changes in co-existing traffic flows within the system. Additionally, the scale of the road network directly affects the computational complexity of existing machine learning-based prediction algorithms, resulting in limited scalability. To address these issues, this paper proposes a mixed method that incorporates both historical data and road networks . Our mixed method approach effectively incorporates prior knowledge of the road network into the prediction process, resulting in more accurate predictions and better scalability. Simulation experiments based on a real-world dataset demonstrate a substantial improvement in the prediction accuracy of our method compared to conventional machine learning-based methods. This indicates significant potential benefits, as our method can provide decision-makers with more accurate and timely information for managing traffic flow, leading to improved efficiency, reduced energy consumption, and minimized environmental impact. Therefore, our proposed mixed method has the potential to make a meaningful impact on traffic management and related fields, ultimately contributing to a more sustainable and efficient transportation system.},
  archive      = {J_COMCOM},
  author       = {Zepu Wang and Peng Sun and Yulin Hu and Azzedine Boukerche},
  doi          = {10.1016/j.comcom.2023.07.019},
  journal      = {Computer Communications},
  pages        = {378-386},
  shortjournal = {Comput. Commun.},
  title        = {A novel hybrid method for achieving accurate and timeliness vehicular traffic flow prediction in road networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Throughput maximization for downlink DF MIMO relay based
SWIPT cognitive radio networks. <em>COMCOM</em>, <em>209</em>, 368–377.
(<a href="https://doi.org/10.1016/j.comcom.2023.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the throughput maximization problem for downlink transmission in decode-and-forward (DF) relay-assisted cognitive radio networks (CRNs) based on simultaneous wireless information and power transfer (SWIPT) capability. In this assumed network, the multiple-input multiple-output (MIMO) relay and secondary user (SU) equipment are designed to operate with the energy harvest from radio frequency (RF) signals as well as SWIPT capability. Also, the cognitive base station (CBS) communicates with SU just through a MIMO relay. Based on the considered network model, multiple combinatorial constraints in the main problem complicate the solution. So, this paper applies heuristic policies in the convex optimization framework to tackle this complexity. First, the throughput maximization problem is considered for the two sides of the relay separately. Second, we progress towards solving each side’s complicated problem optimally by recruiting subproblems-solving strategy. Finally, these optimum solutions are integrated by proposing a heuristic iterative power assignment algorithm in which combinatorial constraints are satisfied with low convergence time. The proposed algorithm performance is evaluated in comparison with benchmark algorithms via numerical outcomes in respect of optimality , convergence time, and compliance with constraints.},
  archive      = {J_COMCOM},
  author       = {Mahla Mohammadi and Seyed Mehdi Hosseini Andargoli},
  doi          = {10.1016/j.comcom.2023.07.009},
  journal      = {Computer Communications},
  pages        = {368-377},
  shortjournal = {Comput. Commun.},
  title        = {Throughput maximization for downlink DF MIMO relay based SWIPT cognitive radio networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blind-trust: Raising awareness of the dangers of using
unsecured public wi-fi networks. <em>COMCOM</em>, <em>209</em>, 359–367.
(<a href="https://doi.org/10.1016/j.comcom.2023.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of public wireless access points (APs) has likely increased with the rise of wireless devices such as smartphones, tablets, smartwatches, etc. These devices make it easy for people to access the internet while on the go, and public Wi-Fi hotspots can provide a convenient way to do so. Even though public Wi-Fi networks provide free access to the internet as opposed to mobile data plans or data roaming, the use of public Wi-Fi hotspots can also pose risks to the security and privacy of users. Public Wi-Fi hotspots are often unsecured, meaning that anyone on the same network can potentially see the data being transmitted. This paper serves two main goals — to determine the risk awareness among users of public Wi-Fi networks and whether they still decide to connect to these networks when they are made aware of the possible risks to their data. For this purpose, we set up an experimental free wireless AP across three different locations for a total of 10 non-consecutive days and found that people mostly connected to use social media apps and search engines. After conducting these experiments and gathering sufficient data, we informed the users about their leaked credentials, and private data and observed their risk awareness and behavioral changes after being exposed to these risks. We found out that our findings align with the protection motivation theory (PMT) and heuristic approach.},
  archive      = {J_COMCOM},
  author       = {Muhammad Sangeen and Naveed Anwar Bhatti and Kashif Kifayat and Abeer Abdullah Alsadhan and Haoda Wang},
  doi          = {10.1016/j.comcom.2023.07.011},
  journal      = {Computer Communications},
  pages        = {359-367},
  shortjournal = {Comput. Commun.},
  title        = {Blind-trust: Raising awareness of the dangers of using unsecured public wi-fi networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-efficient slicing in virtual radio access networks.
<em>COMCOM</em>, <em>209</em>, 349–358. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is a promising technique that has vastly increased the manifoldness of network services to be supported through isolated slices in a shared radio access network (RAN). Due to resource isolation, effective resource allocation for coexisting multiple network slices is essential to maximize network resource efficiency. However, the increased network flexibility and programmability offered by virtualized radio access networks (vRANs) come at the expense of a higher consumption of computing resources at the network edge. Additionally, the relationship between resource efficiency and computing cost minimization is still fuzzy. In this paper, we first perform extensive experiments using the vRAN testbed we developed and assess the vRAN resource consumption under different settings and a varying number of users. Then, leveraging our experimental findings, we formulate the problem of cost-efficient network slice dimensioning, named cost-efficient slicing (CES), which maximizes the difference between total utility and CPU cost of network slices. Numerical results confirm that our solution leads to a cost-efficient resource slicing, while also accomplishing performance isolation and guaranteeing the target data rate and delay specified in the service level agreements .},
  archive      = {J_COMCOM},
  author       = {Somreeta Pramanik and Adlen Ksentini and Carla Fabiana Chiasserini},
  doi          = {10.1016/j.comcom.2023.07.004},
  journal      = {Computer Communications},
  pages        = {349-358},
  shortjournal = {Comput. Commun.},
  title        = {Cost-efficient slicing in virtual radio access networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to improve the rumor-confutation ability of official
rumor-refuting account on social media: A chinese case study.
<em>COMCOM</em>, <em>209</em>, 331–348. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated by the widespread use of social networks, the propagation of rumors has been a destructive factor in the Internet environment. Chinese governments at different administrative levels have established official rumor-refuting accounts on social media to confute rumors, but the spread of rumors in social networks remains a significant concern due to the immature construction of rumor-refuting accounts. To identify the deficiencies of official rumor-refuting accounts at different administrative levels and how to improve their rumor-confutation ability, this paper applies a multi-criteria decision-making (MCDM) method to evaluate Chinese official rumor-refuting accounts on social media, then discusses the differences among them to provide targeted suggestions for improvement. Based on the 10,253 rumor-refuting articles published by 26 Chinese official rumor-refuting accounts on WeChat (a combined social media of Twitter and WhatsApp in China) in 2021, this paper first proposes a targeted rumor-refuting ability evaluation criteria system based on Source-Message-Channel-Receiver (SMCR) theory of communication. Then it applies the Stochastic Multi-criteria Acceptability Analysis-Analytic Hierarchy Process Sort-II with intuitionistic trapezoidal fuzzy numbers (SMAA-AHPSort-II-ITFN) method to evaluate and compare the official rumor-refuting accounts at different administrative levels. The evaluation results show significant differences in the ability of various rumor-refuting accounts to confute rumors. Finally, specific suggestions are provided for improving the ability of official rumor-refuting accounts.},
  archive      = {J_COMCOM},
  author       = {Yan Tu and Linqi Cheng and Liyi Liu and Zongmin Li and Benjamin Lev},
  doi          = {10.1016/j.comcom.2023.07.021},
  journal      = {Computer Communications},
  pages        = {331-348},
  shortjournal = {Comput. Commun.},
  title        = {How to improve the rumor-confutation ability of official rumor-refuting account on social media: A chinese case study},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel EBBDSA based resource allocation technique for
interference mitigation in 5G heterogeneous network. <em>COMCOM</em>,
<em>209</em>, 320–330. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telecom operators have applied the cell association and load balancing technique with femtocell (HeNB) in urban dense 5G Heterogeneous networks (HetNets) for reducing loads of macrocells (MeNBs) and improving the total capacity. However, this deployment creates an interference scenario; mainly, the problem occurs severely from the users of the connected and autonomous vehicle in MeNBs coverage to HeNBs coverage. The main reason is that deploying HeNBs in MeNBs applies the co-subchannels by HeNB and MeNB, leading to severe cross-tier interference (CSI) in the OFDMA subcarrier . Due to this CSI occurrence, the spectral efficiency, Signal to Interference Noise Ratio (SINR), and system outages affect severely. As a result, the MeNB User Equipment (MUEs) and HeNB User Equipment (HUEs) suffer from service disruption , and the entire system’s performance degrades. This challenge has attracted the attention of many researchers and the telecom industries. As a result, several interferences controlling methods such as enhanced frequency allocation strategies, power management, control technique, resource scheduling, spectrum sharing , and Cognitive Radio (CR) techniques have been suggested for 5G communications. However, the study suggests that downlink interference is still challenging in dense urban zones, especially in multilevel and high-rise complex buildings. This paper proposes the EBBDSA-CSI technique where a novel Evolutionary Biogeography-based Dynamic Subcarrier Allocation (EBBDSA) algorithm is introduced for reducing the cross-tier Subcarriers interference (CSI) issues in MeNB and HeNB. The system-level simulation results suggest that the proposed algorithm reduces the system outage to 88.1\%, and total spectral efficiency achieves 83.6\% compared to the existing techniques.},
  archive      = {J_COMCOM},
  author       = {Mohammad Kamrul Hasan and Shayla Islam and Thippa Reddy Gadekallu and Ahmad Fadzil Ismail and Sanaz Amanlou and Siti Norul Huda Sheikh Abdullah},
  doi          = {10.1016/j.comcom.2023.07.012},
  journal      = {Computer Communications},
  pages        = {320-330},
  shortjournal = {Comput. Commun.},
  title        = {Novel EBBDSA based resource allocation technique for interference mitigation in 5G heterogeneous network},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach for ray tracing optimization in wireless
communication. <em>COMCOM</em>, <em>209</em>, 309–319. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ray Tracing is a propagation modeling approach that accurately estimates the signal power received by end users while considering the details of the environment in their vicinity. The accuracy of this estimation is at the cost of high computational load and high memory consumption due to the heavy computation performed by processes such as the Ray Generation. In this paper, we introduce a site-specific ray generation technique able to generate up to 1 million rays within 5 seconds and a root mean square error for bandwidth estimation within 2 Mbps. Depending on the location of the antenna, the coverage area, the type of the terrain and the computational resources available, our technique gives the minimum possible number of rays required to accurately estimate end-users’ signal power received and their download bitrate.},
  archive      = {J_COMCOM},
  author       = {Bernard Tamba Sandouno and Yamen Alsaba and Chadi Barakat and Walid Dabbous and Thierry Turletti},
  doi          = {10.1016/j.comcom.2023.07.016},
  journal      = {Computer Communications},
  pages        = {309-319},
  shortjournal = {Comput. Commun.},
  title        = {A novel approach for ray tracing optimization in wireless communication},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NOMA-based energy efficient resource allocation in wireless
energy harvesting sensor networks. <em>COMCOM</em>, <em>209</em>,
302–308. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non Orthogonal Multiple Access (NOMA) and energy harvesting are two widely used concepts to improve the performance of Wireless Sensor Networks . In this paper, we consider a NOMA-based Wireless Energy Harvesting Sensor Network (WEHSN), in which all sensors harvest their energy from a Hybrid Access Point (HAP), and then, simultaneously transmit their information in NOMA-based protocol to the HAP, if their harvested energy are sufficient and they have data for transmission. We formulate the resource allocation as an optimization problem to maximize the energy efficiency, defined as system throughput per total energy consumption in the network, in which the total energy consumption is summation of energy consumption in downlink and uplink phases. The optimization problem constraints are the time scheduling parameters and the sensors’ transmission powers. To solve this maximization problem, we use the Dinkelbach method to convert it to the parametric form and then, we apply the Karush–Kuhn–Tucker (KKT) conditions in the parametric optimization problem to derive the closed form expression for optimization problem. We evaluate the performance of proposed scheme in relation to energy efficiency, throughput and total energy consumption, and we compare them with OFDMA and TDMA-based WEHSN method. The numerical results show that, NOMA-based WEHSN has better performance in compared with OFDMA and TDMA-based WEHSN. Also, the effect of different amount of data at the sensors has been considered in numerical result which shows that when the sensors always have data for transmission, the system has better performance in terms of energy efficiency and throughput but it has more total energy consumption in comparison to the situation that the sensors occasionally have data for transmission.},
  archive      = {J_COMCOM},
  author       = {Hosein Azarhava and Javad Musevi Niya and Mohammad Ali Tinati},
  doi          = {10.1016/j.comcom.2023.06.028},
  journal      = {Computer Communications},
  pages        = {302-308},
  shortjournal = {Comput. Commun.},
  title        = {NOMA-based energy efficient resource allocation in wireless energy harvesting sensor networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous confrontation strategy learning evolution
mechanism of unmanned system group under actual combat in the loop.
<em>COMCOM</em>, <em>209</em>, 283–301. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confrontation of unmanned system group (USG) is an important combat pattern in future aerial combat, and autonomous confrontation strategy learning evolution is the pre-foundation of USG for actual combat application, researching multiple problems concerning the realization of USG autonomous confrontation strategy active learning evolution in high-dynamic actual combat scenario through continuous interaction with commander, an Autonomous Confrontation Strategy learning evolution mechanism of USG under Actual Combat in the Loop (ACS-ACL) was thence proposed. Select the Multi Agent Deep Deterministic Policy Gradient (MADDPG) algorithm as the baseline algorithm, introduce the Parallel Decoupling Reward Mechanism (PDRM) to make applicability improvement on MADDPG algorithm, establish the generation model of USG autonomous confrontation strategy; after generating the initial autonomous confrontation strategy, USG proactive initiation the Continuous Interaction (CI) with the commander of actual combat in the loop, and uploads the perception information of recessive battlefield situation, whereas commander makes proofreading supplement for the information of battlefield situation, and transmits them back to USG with combat intention together; USG updates replay experience pool, updates autonomous confrontation strategy according to the combat intention, and updates simultaneously interaction strategy with actual combat commander in the loop, and then establishes the autonomous confrontation strategy benign closed-loop learning evolution mechanism of USG. Assume USG execution of collaborative search moving target mission against the enemy, and a visual USG autonomous collaborative search dynamic confrontation game environment is constructed, and carry out a series of simulation validation experiments. By observation and comparison, the convergence efficiency and execution quality of autonomous confrontation strategy driven by combat intention are improved significantly, the autonomous confrontation strategy learning has the benign evolution trend, and further improves the credibility of actual combat application of autonomous confrontation strategy of USG.},
  archive      = {J_COMCOM},
  author       = {Zhenhua Wang and Yan Guo and Ning Li and Hao Yuan and Shiguang Hu and Binghan Lei and Jianyu Wei},
  doi          = {10.1016/j.comcom.2023.07.006},
  journal      = {Computer Communications},
  pages        = {283-301},
  shortjournal = {Comput. Commun.},
  title        = {Autonomous confrontation strategy learning evolution mechanism of unmanned system group under actual combat in the loop},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time-shift: Pseudo-real-time event scheduling for the
split-protocol-stack radio-in-the-loop emulation. <em>COMCOM</em>,
<em>209</em>, 273–282. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The incorporation of real radio hardware and physical emulated radio links into higher layer network and protocol simulation studies has been a largely untouched area of research so far. The Split-Protocol-Stack Radio-in-the-Loop emulation combines pure discrete-event protocol simulation with a hardware-based radio link emulation. Since the basic techniques involve contrary time concepts, event communication between the two domains requires a rethink of scheduling and synchronization. With the Real-Time-Shift conservative synchronization and time compensation scheme, the simulator is decoupled from real-time constraints and limitations by introducing predetermined pause times for event execution. In this work, we present the core synchronization and event scheduling approach allowing for scalable pseudo-real-time simulations with radio hardware in the loop . This enables discrete-event simulations for wireless host systems and networks with link-level emulation accuracy, accompanied by an overall high modeling flexibility.},
  archive      = {J_COMCOM},
  author       = {Sebastian Böhm and Hartmut König},
  doi          = {10.1016/j.comcom.2023.07.013},
  journal      = {Computer Communications},
  pages        = {273-282},
  shortjournal = {Comput. Commun.},
  title        = {Real-time-shift: Pseudo-real-time event scheduling for the split-protocol-stack radio-in-the-loop emulation},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trustworthy artificial intelligence classification-based
equivalent bandwidth control. <em>COMCOM</em>, <em>209</em>, 260–272.
(<a href="https://doi.org/10.1016/j.comcom.2023.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, machine learning (ML) is a viable solution for the allocation of equivalent bandwidth (EqB) in telecommunication networks , i.e. the minimum service rate required by a traffic buffer to guarantee a satisfactory Quality of Service (QoS). Moreover, trustworthy artificial intelligence (AI) is gaining importance in regulating the implementation of ML models, requiring explainable AI (XAI) and uncertainty handling. The paper extends prior works on the combined usage of control and rule-based classification for the EqB allocation , by adding the perspective of trustworthy AI. Simulation-based data collection is performed under a large setting of traffic conditions. Clopper–Pearson generalization bound is used as an efficient tool to select a rule-based model with adequate performance, also determining the minimum amount of data required for model training, which resulted in 3000 samples ( ≈ ≈ 3.3 h of simulation). Also, robustness in terms of the model’s capability to recognize out-of-distribution samples is studied, by comparing the different rates of satisfaction of rules in presence of training or operational data, which is quantified via mutual information, ℓ 1 ℓ1 and ℓ 2 ℓ2 norms. Results show that, while norms are more likely to capture the difference between training and operational data distribution, regardless its entity, mutual information seems sensitive to the entity of the separation between the training and the operational domains .},
  archive      = {J_COMCOM},
  author       = {Sara Narteni and Marco Muselli and Fabrizio Dabbene and Maurizio Mongelli},
  doi          = {10.1016/j.comcom.2023.07.005},
  journal      = {Computer Communications},
  pages        = {260-272},
  shortjournal = {Comput. Commun.},
  title        = {Trustworthy artificial intelligence classification-based equivalent bandwidth control},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COUNT: Blockchain framework for resource accountability in
e-healthcare. <em>COMCOM</em>, <em>209</em>, 249–259. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progress of sensor devices, digital communication, and computing techniques enhances the functionalities of e-healthcare, where the medical facilities are manageable based on the Internet. Now a days, e-healthcare not only helps in disease diagnosis, or tele-medicine, but also implies to general medical scope and medical resource management. Thus, Internet-of-Medical Thing (IoMT) becomes an umbrella term to connect e-healthcare, tele-medicine, and medical supply chain management. In the recent past, the outbreak of COVID-19 pandemic has shown the loopholes in the existing medical systems such as the mismanagement of medical resources and the unavailability of the basic requirements for patients. The medical infrastructure urges for the new frameworks to handle accountability and transparent governance in a medical emergency situation. This motivates us to address medical resource accountability in e-healthcare. In the present work, we introduce the first resource accountability framework to balance the demand–supply of medical facilities in an e-healthcare and IoMT ecosystem. Our solution is based on blockchain and we call it bloCkchained framework for resOUrce accouNTability (COUNT) . We use a customized Proof of Vote (PoV) for consensus in COUNT. We call this consensus COUNT-PoV, which is another direction of novelty in our solution. The blocks in the proposed COUNT contain the resource requirements and their availability–production–supply status. Multiple stakeholders are involved in the process based on providing pre-attained credentials; thus, COUNT supports consortium blockchain . Existing literature shows a number of studies for blockchain-based frameworks for e-healthcare; however, those works do not address the accountability and transparency issues of medical e-governance and balancing the demand–supply of the medical facilities/resources. Therefore, our proposed COUNT is beneficial for e-healthcare and IoMTs. We use signcryption process to reduce the complexity of the cryptographic processes, which is an add-on to the contribution. We evaluate our proposed framework based on the Hyperledger Caliper benchmark test with latency, throughput, and resource consumption. Additionally, we also analyse the cost of the implementation. The comparative analysis of our consensus with other stake-based consensus protocols on the COUNT framework shows that our consensus, COUNT-PoV is efficient and suitable for the use of e-healthcare and IoMTs. Moreover, being a generic framework, COUNT is helpful for the e-governance of medical facilities, the vaccination process, and COVID passports.},
  archive      = {J_COMCOM},
  author       = {Gulshan Kumar and Rahul Saha and Mauro Conti and Tannishtha Devgun and Rekha Goyat and Joel J.P.C. Rodrigues},
  doi          = {10.1016/j.comcom.2023.07.017},
  journal      = {Computer Communications},
  pages        = {249-259},
  shortjournal = {Comput. Commun.},
  title        = {COUNT: Blockchain framework for resource accountability in e-healthcare},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic conjunctive keywords searchable symmetric
encryption scheme for multiple users in cloud computing.
<em>COMCOM</em>, <em>209</em>, 239–248. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of cloud computing technology, massive amounts of heterogeneous data are being outsourced to cloud servers. Dynamic searchable symmetric encryption (DSSE) is widely used for dynamically updating and searching ciphertext . Although many DSSE schemes have been developed in recent years, few can satisfy the requirement for multi-keyword search. To address this problem, we proposed a novel Dynamic Searchable Symmetric Encryption scheme supporting Multiple users and Multiple keywords (MMDSSE) in this paper, specifically designed for the multi-keyword cloud computing paradigm , while also enabling multi-user search capability. To achieve multi-keyword search, a tag is computed using an iso-or operation that records the relationship between the keyword and identifier. In addition, a unique key is assigned by the data owner to each user with different file access rights. This allows users to gain fine-grained control over the ciphertext , and the essential security properties, including forward and backward privacy, can be achieved accordingly. Furthermore, this scheme is resistant to collusion attacks. Finally, the experiment was simulated on the IDEA platform using the Java language . The experimental results demonstrate that this scheme performs well.},
  archive      = {J_COMCOM},
  author       = {Sichun Lv and Haowen Tan and Wenying Zheng and Tao Zhang and Menglei Wang},
  doi          = {10.1016/j.comcom.2023.07.008},
  journal      = {Computer Communications},
  pages        = {239-248},
  shortjournal = {Comput. Commun.},
  title        = {A dynamic conjunctive keywords searchable symmetric encryption scheme for multiple users in cloud computing},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Graphic image classification method based on an attention
mechanism and fusion of multilevel and multiscale deep features.
<em>COMCOM</em>, <em>209</em>, 230–238. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper explores the issue of privacy breaches caused by sharing and publishing images on social media platforms , which is becoming increasingly serious. While existing deep learning-based methods have achieved remarkable results in private image classification , they only consider single-level and single-scale features and neglect the problem of multilevel and multiscale features. To address this limitation, the paper proposes a novel graphic image classification method that fuses multilevel and multiscale deep features. The proposed method employs a multibranch convolutional neural network to extract multilevel features, which are then processed by a multilevel pooling layer to obtain multiscale features. Additionally, two feature fusion methods based on attention mechanism , Privacy-MSML (Privacy Multi-Scale Multi-Level) and Privacy-MLMS (Privacy Multi-Level Multi-Scale), are designed to fuse the multilevel and multiscale features for image classification. Both methods utilize Bi-LSTM and a self-attention mechanism to capture feature dependencies. The experimental results on public datasets demonstrate that the proposed methods effectively fuse multilevel and multiscale features, leading to a significant improvement in classification performance, which highlights the innovative contribution of the paper in addressing the issue of multilevel and multiscale features in private image classification.},
  archive      = {J_COMCOM},
  author       = {Shan Liu and Qi Zhang and Lingling Huang},
  doi          = {10.1016/j.comcom.2023.07.001},
  journal      = {Computer Communications},
  pages        = {230-238},
  shortjournal = {Comput. Commun.},
  title        = {Graphic image classification method based on an attention mechanism and fusion of multilevel and multiscale deep features},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient trust-based decision-making approach for WSNs:
Machine learning oriented approach. <em>COMCOM</em>, <em>209</em>,
217–229. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Networks (WSNs) are often used for critical applications where trust and security are of paramount importance. Trust evaluation is one of the key mechanisms to ensure the security and reliability of WSNs. Traditional trust evaluation schemes rely on fixed, predetermined thresholds, or rules and static attack models, which may not be suitable for all situations such as dynamic and heterogeneous network environments with new and unknown attack scenarios as well as have several problems such as limited security and scalability, limited accuracy, incomplete coverage, lack of adaptability that can limit their effectiveness. Machine Learning (ML) has been shown to be an effective tool for trust evaluation in WSNs, offering several benefits over existing schemes such as greater adaptability, scalability, and accuracy since ML algorithms can analyze and learn from the data collected in real-time from multiple sources (sensor readings, network traffic, and user behavior) enabling them to dynamically adjust their decision-making criteria based on the current network conditions. Trust-aware ML-based security mechanisms achieve safety and efficient decision-making by reducing uncertainty and risk to accomplish real-world tasks. This paper presents a Machine Learning (ML)-based trust evaluation model in the unattended autonomous WSN environment to achieve reliability, adaptability, scalability, and accuracy by generating quick and reliable trust values dynamically. The proposed machine learning algorithm extracts various trust features such as Co-Location Relationship (CLR), Co-Work Relationship (CWR), Cooperativeness-Frequency-Duration (CFD), and Reward (R) to obtain a robust trust rating of sensor devices and predict future misbehavior. These trust features are combined to generate a final trust rating before making any decision about the reliability of any sensor device. Moreover, the projected trust model (ETDMA) integrate direct communication trust and indirect trust with the help of a logical time window that periodically records the trustworthy and suspicious interactions. Simulation experiments exhibit the effectiveness of the proposed trust evaluation method in terms of change in trust values, malicious nodes detection (94\%), FNR (0.9\%), F1-Score (0.6), and accuracy (92\%) in the presence of 50 malicious nodes.},
  archive      = {J_COMCOM},
  author       = {Tayyab Khan and Karan Singh and Mohd Shariq and Khaleel Ahmad and K.S. Savita and Ali Ahmadian and Soheil Salahshour and Mauro Conti},
  doi          = {10.1016/j.comcom.2023.06.014},
  journal      = {Computer Communications},
  pages        = {217-229},
  shortjournal = {Comput. Commun.},
  title        = {An efficient trust-based decision-making approach for WSNs: Machine learning oriented approach},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Focusing on information context for ITS using a spatial age
of information model. <em>COMCOM</em>, <em>209</em>, 203–216. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New technologies for sensing and communication act as enablers for cooperative driving applications. Sensors are able to detect objects in the surrounding environment and information such as their current location is exchanged among vehicles. In order to cope with the vehicles’ mobility, such information is required to be as fresh as possible for proper operation of cooperative driving applications. The age of information (AoI) has been proposed as a metric for evaluating freshness of information; recently also within the context of intelligent transportation systems (ITS). We investigate mechanisms to reduce the AoI of data transported in form of beacon messages while controlling their emission rate. We aim to balance packet collision probability and beacon frequency using the average peak age of information (PAoI) as a metric. This metric, however, only accounts for the generation time of the data but not for application-specific aspects, such as the location of the transmitting vehicle. We thus propose a new way of interpreting the AoI by considering information context, thereby incorporating vehicles’ locations. As an example, we characterize such importance using the orientation and the distance of the involved vehicles. In particular, we introduce a weighting coefficient used in combination with the PAoI to evaluate the information freshness, thus emphasizing on information from more important neighbors. We further design the beaconing approach in a way to meet a given AoI requirement, thus, saving resources on the wireless channel while keeping the AoI minimal. We illustrate the effectiveness of our approach in Manhattan-like urban scenarios, reaching pre-specified targets for the AoI of beacon messages.},
  archive      = {J_COMCOM},
  author       = {Julian Heinovski and Jorge Torres Gómez and Falko Dressler},
  doi          = {10.1016/j.comcom.2023.07.002},
  journal      = {Computer Communications},
  pages        = {203-216},
  shortjournal = {Comput. Commun.},
  title        = {Focusing on information context for ITS using a spatial age of information model},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive context-aware vertical handover decision
algorithm for heterogeneous networks. <em>COMCOM</em>, <em>209</em>,
188–202. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a heterogeneous network , vertical handovers occur frequently between different access technologies to meet the requirements of mobile users. The selection of an access technology for a mobile user for handover largely depends on the contextual information of the user and network at that point of time. In this work, we have proposed an adaptive, context-aware vertical handover decision algorithm for heterogeneous networks. Both static and dynamic contextual information about the user and the network are given as inputs to our proposed algorithm. They are provided proactively by the networks to the mobile nodes on a timely basis. An optimal network scanning procedure is introduced as part of the handover process to minimize the energy consumption of the nodes. We also propose a utility function-based cell load balancing mechanism to improve overall resource utilization. In addition, a delay-sensitive handover traffic-based hybrid channel allocation mechanism is proposed. The simulation results clearly suggest that the proposed work significantly enhanced the overall performance of the system as compared to some existing work in terms of call blocking probability , channel utilization rate, consumed energy, and handover delays.},
  archive      = {J_COMCOM},
  author       = {Pratyashi Satapathy and Judhistir Mahapatro},
  doi          = {10.1016/j.comcom.2023.06.029},
  journal      = {Computer Communications},
  pages        = {188-202},
  shortjournal = {Comput. Commun.},
  title        = {An adaptive context-aware vertical handover decision algorithm for heterogeneous networks},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved TDD operation on software-defined radio platforms
towards future wireless standards. <em>COMCOM</em>, <em>209</em>,
178–187. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Radio (SDR) platforms are valuable for research and development activities or high-end systems that demand flexible wireless protocols. While low-latency digital baseband processing can be achieved using a dedicated processing unit, like an FPGA or hardware accelerator , its multi-purpose Radio Frequency (RF) front-end often poses a limitation. Zero Intermediate Frequency (ZIF) transceivers are favorable for SDR, however, even for Time Division Duplex (TDD) systems, these transceivers suffer from self-interference when the transmitting and receiving Local Oscillator (LO) is set to the same frequency. To achieve low self-interference, switching from receiving to transmitting mode is needed. However, the time this takes (turnaround time, TT) for configurable RF front-ends often violates the strict timing requirements of protocols like Wi-Fi and 5G , which require response times in the order of microseconds . In this work, we first evaluate the advantages and disadvantages of several methods to suppress self-interference of a ZIF transceiver. Next, a novel approach is proposed, which can reduce the TT to as low as 640 ns using the widely used AD9361 configurable ZIF RF front-end, while the noise floor is at the same level as achieved by the conventional way of switching between transmit and receive mode. We have realized and validated this approach using openwifi — an open-source Wi-Fi implementation on SDR. As a result, the receiver sensitivity is improved by up to 17 dB in the 2.4 GHz band and 9.5 dB in the 5 GHz band, for over-the-air transmissions.},
  archive      = {J_COMCOM},
  author       = {Thijs Havinga and Xianjun Jiao and Muhammad Aslam and Wei Liu and Ingrid Moerman},
  doi          = {10.1016/j.comcom.2023.06.026},
  journal      = {Computer Communications},
  pages        = {178-187},
  shortjournal = {Comput. Commun.},
  title        = {Improved TDD operation on software-defined radio platforms towards future wireless standards},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PROSE: Multi-round fair coflow scheduling without prior
knowledge. <em>COMCOM</em>, <em>209</em>, 163–177. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coflow scheduling plays a crucial role in enhancing network-level performance in datacenter applications. Existing works on coflow scheduling predominantly concentrate on clairvoyant schedulers, which assume prior knowledge of coflow sizes before transmission. However, this assumption is often unrealistic in practical scenarios like pipelined computation. Consequently, research on non-clairvoyant schedulers has garnered increasing attention. Most studies on non-clairvoyant schedulers focus on either improving efficiency or promoting fairness, with limited emphasis on striking a balance between the two objectives. Additionally, for non-cooperative environments like multi-tenant datacenter networks, it is essential to ensure strategy-proofness in coflow scheduling. Without adequate measures, tenants may manipulate their reported requirements to gain an unfair advantage in resource allocation. Ideally, in non-cooperative environments, a coflow scheduler should achieve efficiency, fairness, and strategy-proofness, all without prior knowledge of coflow sizes. However, few existing efforts have managed to simultaneously achieve these objectives. To fill this gap, we develop PROSE, a non-clairvoyant scheduler for non-cooperative environments. PROSE divides the scheduling time into multiple rounds and employs a two-stage scheduling mechanism based on these rounds. In the first stage, PROSE estimates the sizes of the coflows with unknown sizes by transmitting the probe flows, using a policy that combines coflow competition with batch processing. Once the information about coflow sizes is obtained, PROSE prioritizes coflows among tenants for bandwidth allocation in the second stage, aiming to accelerate coflow completion within each round. Furthermore, PROSE ensures optimal isolation guarantee by fairly limiting the total data transfer for each tenant and dynamically adjusts the scheduling order of coflows in a round-robin manner across multiple rounds. PROSE prevents tenants from attempting to consume excessive network resources through manipulative actions that deceive the scheduler. Trace-driven simulations demonstrate that PROSE outperforms the fair scheduler, achieving up to a 2.37 × × reduction in average coflow completion time, indicating substantial efficiency improvements.},
  archive      = {J_COMCOM},
  author       = {Fan Zhang and Yazhe Tang and Xun Li and Chengchen Hu},
  doi          = {10.1016/j.comcom.2023.06.025},
  journal      = {Computer Communications},
  pages        = {163-177},
  shortjournal = {Comput. Commun.},
  title        = {PROSE: Multi-round fair coflow scheduling without prior knowledge},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AD2S: Adaptive anomaly detection on sporadic data streams.
<em>COMCOM</em>, <em>209</em>, 151–162. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of Internet applications, ensuring the quality and reliability of online services has become increasingly important. Therefore, anomaly detection methods play a critical role in identifying potential anomalies in the data streams of infrastructure systems and service applications. However, most of known detection methods have an underlying assumption that the data streams are continuous. In practice, we learn that many real-world data streams can be sporadic. It incurs particular challenges for the task of anomaly detection , for which the common preprocessing of downsampling on sporadic data can omit potential anomalies and delay alarms. In this paper, we propose an ensemble learning-based anomaly detection method on sporadic data streams named AD 2 S AD2S . It consists of two modules: a monitor module to continuously and adaptively determine the measure windows for observations, and a detection module that utilizes an isolation partition strategy to estimate the anomaly degree of each incoming observation. Based on experimental results on eight synthetic and public real-world datasets, our method outperforms other state-of-the-art methods with an average AUC score of 0.923. Additionally, our analysis demonstrates that the proposed method has constant amortized time and space complexity, enabling once detection within an average of 9.9 ms and maximum memory usage of 26.14 KB. The code of AD 2 S AD2S is open-sourced for further research.},
  archive      = {J_COMCOM},
  author       = {Fengrui Liu and Yang Wang and Zhenyu Li and Hongtao Guan and Gaogang Xie},
  doi          = {10.1016/j.comcom.2023.06.027},
  journal      = {Computer Communications},
  pages        = {151-162},
  shortjournal = {Comput. Commun.},
  title        = {AD2S: Adaptive anomaly detection on sporadic data streams},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto-scaling techniques in container-based cloud and
edge/fog computing: Taxonomy and survey. <em>COMCOM</em>, <em>209</em>,
120–150. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-held dream of computing as a service was realized with the emergence of cloud computing . Recently, fog and edge computing have been introduced as extensions of cloud networks, providing networking, processing, data management, and storage on network nodes near Internet of Things (IoT) devices to bridge the gap between the cloud and IoT devices. As the foundation of distributed computing , virtualization enables more effective use of physical computer hardware. Operating system virtualization through containers has recently been proposed as a promising alternative to virtual machines (VMs). Containers are lightweight packages that contain all application dependencies, system libraries, and third-party software packages. This research aims to review auto-scaling solutions for container-based virtualization in cloud and edge/fog computing applications. Auto-scaling plays a crucial role in the broad adoption of cloud computing by allocating and releasing computing resources in response to fluctuating resource requirements. However, designing and implementing an efficient auto-scaler for container-based applications in cloud and edge/fog computing presents challenges due to diverse application resource requirements and dynamic workload characteristics. Our research presents a comprehensive classification system for articles, covering key parameters such as auto-scaling techniques, experiments, workloads, and metrics, among others. We provide a detailed analysis of the results, offering valuable insights into open challenges and identifying promising directions for future research in this field.},
  archive      = {J_COMCOM},
  author       = {Javad Dogani and Reza Namvar and Farshad Khunjush},
  doi          = {10.1016/j.comcom.2023.06.010},
  journal      = {Computer Communications},
  pages        = {120-150},
  shortjournal = {Comput. Commun.},
  title        = {Auto-scaling techniques in container-based cloud and edge/fog computing: Taxonomy and survey},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance analysis of general p4 forwarding devices with
controller feedback: Single- and multi-data plane cases.
<em>COMCOM</em>, <em>209</em>, 102–119. (<a
href="https://doi.org/10.1016/j.comcom.2023.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Networking (SDN) lays the foundation for the operation of future networking applications. The separation of the control plane from the programmable data plane increases the flexibility in network operation, and hence, improves the overall performance. One of the most used languages for describing the packet behavior in the data plane is P4. It allows both protocol and hardware independent programming. With the expanding deployment of P4 programmable devices, it is of utmost importance to understand their achievable performance and limitations in order to design a network and provide Quality of Service (QoS) guarantees in terms of different metrics of interest to users communicating in the network. One of the most important figure of merits is the mean sojourn time of a packet in a P4 device. While previous works already modeled the sojourn time in P4 devices with controller feedback , those models were rather oversimplified and could not capture the real system behavior for general cases, resulting this way in a potentially high inaccuracy in performance prediction. To bridge this gap, in this paper, we consider the system behavior of P4 devices for the general case, i.e., under general input parameter distributions. To that end, we model the system behavior with a queueing network with feedback. First, we do this for a single data plane, and then we extend the analysis to the case when there are multiple data planes sending occasional packets to the same controller. Due to the fact that it is impossible to provide closed-form solutions in the general case, we consider different approximation approaches for the mean sojourn time and show which one is better for a given scenario. We validate our results against extensive realistic simulations, capturing different behaviors in the data and control planes. Results show that the most accurate approximation in most cases is the one in which queueing networks are decoupled and considered as independent queues despite the fact that there are considerable dependencies involved. The level of discrepancy with the best approximating approach in the worst case for a single data plane does not exceed 18.2\% for service times distributions with a coefficient of variation not greater than 1, whereas when dealing with multiple data planes, the discrepancy is usually higher, but with the best-approximating approach in each case rarely exceeds 14\%.},
  archive      = {J_COMCOM},
  author       = {Nicolai Kröger and Fidan Mehmeti and Hasanin Harkous and Wolfgang Kellerer},
  doi          = {10.1016/j.comcom.2023.07.003},
  journal      = {Computer Communications},
  pages        = {102-119},
  shortjournal = {Comput. Commun.},
  title        = {Performance analysis of general p4 forwarding devices with controller feedback: Single- and multi-data plane cases},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constructions of broadcast encryption with personalized
messages from bilinear map. <em>COMCOM</em>, <em>209</em>, 91–101. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Broadcast encryption (BE) is a one to many encryption protocol which sends a message to desired set of users securely and efficiently. Broadcast encryption with personalized messages (BEPMsg) is a cryptographic primitive that sends a common broadcast message together with user-specific personalized messages to a group of users. This work contains the transformation of broadcast encryption from the bilinear map to broadcast encryption with personalized messages. The transformation of BE from bilinear map to BEPMsg helps to design BEPMsg with different flavors. We have also proposed two BEPMsg constructions and discussed their security. Our first construction is an adaptive secure BEPMsg scheme. It also supports inclusive–exclusive property in encryption algorithm as the broadcaster can select the set of subscribed or revoked users to decrease the encryption cost depending on the size of the set of subscribed and revoked users. More positively, it has a constant header size. Our second construction provides outsider anonymity in BEPMsg by performing the decryption without using the set of subscribed or revoked users.},
  archive      = {J_COMCOM},
  author       = {Kamalesh Acharya and Amit Kumar Singh and Sourav Mukhopadhyay},
  doi          = {10.1016/j.comcom.2023.06.023},
  journal      = {Computer Communications},
  pages        = {91-101},
  shortjournal = {Comput. Commun.},
  title        = {Constructions of broadcast encryption with personalized messages from bilinear map},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative cloud-edge-end task offloading with task
dependency based on deep reinforcement learning. <em>COMCOM</em>,
<em>209</em>, 78–90. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of the Internet of Things (IoT), IoT devices generate massive amounts of data and demand, which poses a huge challenge to IoT devices with limited CPU computing capability and battery capacity. Due to the dependency relationship in complex applications and network environments, effective offloading in such scenarios is complicated. In this paper, we address the problem of computation offloading with task dependency in the cloud–edge-end collaboration scenario including multi-user, multi-core edge servers, and a cloud server. We model multiple task dependencies using the directed acyclic graph (DAG) and formalize the offloading problem as a multi-objective mixed integer optimization problem . To solve this problem, a Task Priority and Deep Reinforcement learning-based Task Offloading algorithm (TPDRTO) is proposed. The task offloading decision is represented as a Markov Decision Process (MDP). Meanwhile, based on the task priority, an optimized Deep Reinforcement Learning (DRL) method with action-mask is proposed to leverage the computing resources of the cloud and edge servers and obtain the optimal policies of computation offloading . Experimental results show that the TPDRTO algorithm can effectively tradeoff and reduce the average energy consumption and time delay of IoT devices.},
  archive      = {J_COMCOM},
  author       = {Tiantian Tang and Chao Li and Fagui Liu},
  doi          = {10.1016/j.comcom.2023.06.021},
  journal      = {Computer Communications},
  pages        = {78-90},
  shortjournal = {Comput. Commun.},
  title        = {Collaborative cloud-edge-end task offloading with task dependency based on deep reinforcement learning},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BSAPM: BlockChain based secured authentication protocol for
large scale WSN with FPGA implementation. <em>COMCOM</em>, <em>209</em>,
63–77. (<a href="https://doi.org/10.1016/j.comcom.2023.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to state-of-the-art, several multiple base station-based authentication and key agreement protocols exist. Still, most of the protocols are computationally inefficient and suffer from several potential attacks. In this research work, we consider WSN architecture, where multiple base stations are located in a distributed way. After that, we proposed a blockchain-based authentication and key agreement protocol for establishing secure communication. This protocol utilizes a smart contract facility during the registration of the sensor node . Our work also stores records of the sensor node in blockchain network. Most importantly, we have verified the proposed protocol using FPGA implementation, which further confirms the correctness of the generated session keys. BSAPM is demonstrated in terms of security threats using the Scyther tool, and we have also shown formal security analysis using the Real-or-Random(ROR) model. The informal security analysis demonstrates the protection against security threats. BSAPM is better in terms of communication, computation overheads, and functionality features when compared to the related other competitive schemes.},
  archive      = {J_COMCOM},
  author       = {Mohammad Abdussami and Ruhul Amin and P. Saravanan and Satyanarayana Vollala},
  doi          = {10.1016/j.comcom.2023.06.011},
  journal      = {Computer Communications},
  pages        = {63-77},
  shortjournal = {Comput. Commun.},
  title        = {BSAPM: BlockChain based secured authentication protocol for large scale WSN with FPGA implementation},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved authentication and key management scheme in
context of IoT-based wireless sensor network using ECC. <em>COMCOM</em>,
<em>209</em>, 47–62. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor network (WSN) is used for sensing data/information from the nearby environment with the help of various spatially dispersed miniature sensor devices/nodes and forwarding the data to the direction of the base station which is connected to the network through the Internet for processing the collected data. Currently, Internet of Things (IoT) based smart computing applications are becoming popular in which WSN is used to run a process to output information from sensed data or events. However, these sensor nodes have less computation ability and low power capacity. In these resource-constrained WSNs, remote user authentication is a serious challenge that demands the attention of the researchers’ community. Several authentication protocols which address a variety of security concerns are present in the literature. Ghani et al. proposed an efficient authentication and key management scheme that claims to be resistant to known attacks. During the study of their scheme, we found that it is efficient and effective for protecting against a few attacks. However, it does not have the ability to counter several security threats like user impersonation attack, insider attack, database attack, and stolen smart card attack. To counter these aforesaid security loopholes, in this paper a new ECC integrated scheme suitable for IoT-based WSN with a higher level of protection is proposed. The informal security analysis using mathematical approaches and the formal security analysis using a well-accepted AVISPA simulator and BAN logic ensures that the suggested work is highly secure from all the relevant threats. Moreover, the performance analysis and comparison of the proposed scheme with the most recent relevant schemes proves that our scheme is much more efficient in terms of computation, communication, and storage overheads than all the available schemes in the literature.},
  archive      = {J_COMCOM},
  author       = {Uddalak Chatterjee and Sangram Ray and Sharmistha Adhikari and Muhammad Khurram Khan and Mou Dasgupta},
  doi          = {10.1016/j.comcom.2023.06.017},
  journal      = {Computer Communications},
  pages        = {47-62},
  shortjournal = {Comput. Commun.},
  title        = {An improved authentication and key management scheme in context of IoT-based wireless sensor network using ECC},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint task offloading and resource allocation in mixed
edge/cloud computing and blockchain empowered device-free sensing
systems. <em>COMCOM</em>, <em>209</em>, 38–46. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that wireless signals can not only be used as communication medium for data transmission, but also can be employed for device-free sensing (DFS). However, the sensed raw big data processing needs large amount of processing resources. Moreover, the sensed data are usually privacy-sensitive, where blockchain can be employed to guarantee the privacy related issues , and however, blockchain tasks are usually also computation-intensive. To provide the processing capabilities for sensed big data processing and blockchain task executing in DFS, mobile edge computing (MEC) and cloud computing are essential enabling technologies. In this paper, we consider a multiuser mobile offloading network consisting a edge node (or fog node) and a remote cloud server. The tasks could be processed locally, or offloaded to the edge nodes, or further migrated to the cloud relayed by the edge node. We formulate the offloading problem as the joint optimization of task offloading decision making of all the users, the computation resource allocation among the edge-executing applications, and radio resource assignment among all the remote-processing applications, aiming to minimize the maximum weighted cost of all the users. It is demonstrated that the problem is NP-hard. To tackle this challenge, we decoupled the problem into subproblems , where offloading decisions are obtained using semi-definite relaxation (SDR), and next, a swarm intelligence algorithm , i.e., fireworks algorithm, is adopted in radio resource allocation. Simulation results exhibit that as a result of the collaboration of the collaborate of edge and cloud, the proposed joint algorithm could achieves nearly optimal performance in the aspects of energy consumption and delay compared with other benchmark algorithms.},
  archive      = {J_COMCOM},
  author       = {Jianbo Du and Wenjie Cheng and Shulei Li},
  doi          = {10.1016/j.comcom.2023.06.015},
  journal      = {Computer Communications},
  pages        = {38-46},
  shortjournal = {Comput. Commun.},
  title        = {Joint task offloading and resource allocation in mixed edge/cloud computing and blockchain empowered device-free sensing systems},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial offloading in device-to-device-assisted MEC network:
A utility optimization approach. <em>COMCOM</em>, <em>209</em>, 26–37.
(<a href="https://doi.org/10.1016/j.comcom.2023.06.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to meet the growing demand for faster and more efficient communication and computing in internet of things networks, the device-to-device (D2D)-assisted multi-access edge computing (MEC) network has emerged as a promising solution. However, most existing literature overlooks the monetary cost associated with computing and communication resources when it comes to computation offloading . Motivated by this, we first formulate a novel utility function which is defined as the tradeoff between the benefit of the delay reduction via partial offloading and the cost of the purchased computing and communication resources. Then, we propose the partial offloading-based utility optimization (POUO) algorithm to solve the utility optimization problem . Afterward, we determine the resource allocation policy and offloading ratios while taking into account the constraints of the utility optimization problem . In addition, the offloading pairing decision in the device-to-edge server (D2E) offloading mode is modeled as a knapsack problem and solved using a greedy algorithm . For D2D offloading mode, we transform the offloading pairing decision into an assignment problem and apply the Hungarian algorithm to solve it. Finally, we validate and evaluate the proposed POUO algorithm across a range of system parameters. Simulation results show that the POUO algorithm improves the completed tasks ratio, the total utility, and the average delay.},
  archive      = {J_COMCOM},
  author       = {Ruidong Zhang and Jiadong Zhang and Wenxiao Shi},
  doi          = {10.1016/j.comcom.2023.06.022},
  journal      = {Computer Communications},
  pages        = {26-37},
  shortjournal = {Comput. Commun.},
  title        = {Partial offloading in device-to-device-assisted MEC network: A utility optimization approach},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced resource allocation in distributed cloud using
fuzzy meta-heuristics optimization. <em>COMCOM</em>, <em>209</em>,
14–25. (<a href="https://doi.org/10.1016/j.comcom.2023.06.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is a modern technology that has become popular today. A large number of requests has made it essential to propose a resources allocation framework for arriving requests. The network can be made more efficient and less costly this way. The cloud–edge paradigm has been considered a growing research area in the computing industry in recent years. The increase in the number of customers and requests for cloud data centers (CDCs) has created the need for robust servers and low power consumption mechanisms. Ways to reduce energy in the CDC having appropriate algorithms for resource allocation. The purpose of this study was to develop an intelligent method for dynamic resource allocation using Takagi–Sugeno–Kang (TSK) neural-fuzzy systems and ant colony optimization (ACO) techniques to reduce energy consumption by optimizing resource allocation in cloud networks. It predicts future loads using a drop-down window to track CPU usage. By optimizing virtual machine migration , ACO can reduce energy consumption. Simulations are provided by examining the implementation and a variety of parameters such as the number of requests made wasted resources, and requests rejected. In this paper, we propose the use of virtual machine migration to accomplish two main goals: evacuating additional and non-optimal virtual machines (scaling and shutting down additional active physical machines) and solving the resource granulation problem. We evaluated and compared our results with literature for rejection rates of virtual and physical machine applications. The performances of our algorithms are compared to different criteria such as performance in request rejection, dynamic CPU resource allocation with reinforcement learning , multi-objective resource allocation, NSGAIII, Whale optimization and Forecast Particle Swarm allocation. A comparison of some evaluation criteria showed that the proposed method is more efficient than other methods.},
  archive      = {J_COMCOM},
  author       = {Arun Kumar Sangaiah and Amir Javadpour and Pedro Pinto and Samira Rezaei and Weizhe Zhang},
  doi          = {10.1016/j.comcom.2023.06.018},
  journal      = {Computer Communications},
  pages        = {14-25},
  shortjournal = {Comput. Commun.},
  title        = {Enhanced resource allocation in distributed cloud using fuzzy meta-heuristics optimization},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain based secure medical data outsourcing with data
deduplication in cloud environment. <em>COMCOM</em>, <em>209</em>, 1–13.
(<a href="https://doi.org/10.1016/j.comcom.2023.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of cloud computing , an increasing number of individuals and businesses are opting to store their vast data on the cloud in order to cut data maintenance costs and obtain more easy access. The cloud-based Electronic Health Records (EHR) sharing scheme has provided many benefits in recent years, but the centralization of the cloud raises challenges to privacy protection and data security. Blockchain technology is viewed as a possible solution to these challenges because of its unique properties of unforgeability, anonymity, and verifiability. Therefore, a blockchain based secure data outsourcing scheme is proposed in this paper. In the proposed scheme, each activity on outsourcing EHRs is recorded as a transaction on the public blockchain, authenticity is ensured by registration, and secure data access is ensured by keyword search . Initially, the EHR is encrypted by the proposed Enhanced Map Chaotic Encryption (EMCE) algorithm to ensure confidentiality . Then, data de-duplication is performed by the proposed Tri-Level Chunk Hashing (TL-CH) technique to eliminated duplicate records and reduce the storage space. Finally, the integrity of outsourced EHRs is validated by the Third-Party Auditor (TPA). The performance and security analysis show that the proposed scheme can provide solid security against various attacks, and the storage space is reduced.},
  archive      = {J_COMCOM},
  author       = {T. Benil and J. Jasper},
  doi          = {10.1016/j.comcom.2023.06.013},
  journal      = {Computer Communications},
  pages        = {1-13},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain based secure medical data outsourcing with data deduplication in cloud environment},
  volume       = {209},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cybersecurity for industrial IoT (IIoT): Threats,
countermeasures, challenges and future directions. <em>COMCOM</em>,
<em>208</em>, 294–320. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IIoT is evolving as the future of system and process integration across Operations Technology (OT) and Information Technology (IT). The dynamic nature of OT and IT across diverse industrial sectors has led to the emergence of evolving threats which are challenging to address. We present a holistic analysis of the IIoT paradigm, identifying specific domains of IIoT adoption, assessing threats and vulnerabilities, and providing a detailed analysis of existing countermeasures . We also identify future research directions in IIoT cybersecurity to address future threats and vulnerabilities.},
  archive      = {J_COMCOM},
  author       = {Sri Harsha Mekala and Zubair Baig and Adnan Anwar and Sherali Zeadally},
  doi          = {10.1016/j.comcom.2023.06.020},
  journal      = {Computer Communications},
  pages        = {294-320},
  shortjournal = {Comput. Commun.},
  title        = {Cybersecurity for industrial IoT (IIoT): Threats, countermeasures, challenges and future directions},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resource allocation and device pairing for energy-efficient
NOMA-enabled federated edge learning. <em>COMCOM</em>, <em>208</em>,
283–293. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Fifth Generation of cellular networks (5G), Internet of Things (IoT) devices are becoming much more popular and accumulate a lot of data. With the advantages of privacy protection and resource saving, Federated Edge Learning (FEEL) is a promising machine learning paradigm which helps improve the service quality of deep learning-based applications and makes people’s lives intelligent by processing these data. In FEEL, users only train the models locally without uploading their own data, thus alleviating the data security issue. Nevertheless, this paradigm also faces a major challenge. The training tasks are carried out on the mobile devices , which poses great pressure to their limited battery lives. In 5G , Non-Orthogonal Multiple Access (NOMA) has become an enabling technology to support massive connections because of its higher spectral efficiency and throughput over OMA technologies. Considering the potential of FEEL and the broad application prospects of NOMA, we design a NOMA-enabled FEEL framework and minimize its Energy Consumption (EC) by optimizing the device pairing, communication resource (i.e., transmission powers and time slots) and computation resource (i.e., CPU frequencies and local training threshold). The original problem can be decomposed into two subproblems : resource allocation and device pairing subproblems . For the resource allocation subproblem, with the help of the linear search method, we can find a near-optimal solution. Then, the optimality of the proposed solution (i.e., how far is the near-optimal from the optimum) is proved by sensitivity analysis. For the device pairing subproblem, we propose two low-complexity pairing schemes, which have their own characteristics. Simulation results demonstrate the effectiveness of the proposed resource allocation and device pairing strategies and highlight the advantage of the NOMA-enabled FEEL framework over the existing TDMA-enabled framework in terms of EC.},
  archive      = {J_COMCOM},
  author       = {Youqiang Hu and Hejiao Huang and Nuo Yu},
  doi          = {10.1016/j.comcom.2023.06.024},
  journal      = {Computer Communications},
  pages        = {283-293},
  shortjournal = {Comput. Commun.},
  title        = {Resource allocation and device pairing for energy-efficient NOMA-enabled federated edge learning},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sub-rate linear network coding. <em>COMCOM</em>,
<em>208</em>, 271–282. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing network capacity is undoubtedly a critical aspect of modern communication systems . In this paper, the concept of sub-rate coding and decoding in the framework of linear network coding (LNC) is introduced for single-source multiple-sinks finite acyclic networks . This methodology offers a modification to existing LNC so that the source transmits its messages in a manner in which a given set of sinks (termed sub-rate sinks), whose max-flow is smaller than the source’s message-rate, can still decode a portion of the transmitted messages, without degrading the maximum achievable rate of LNC sinks whose max-flow is equal (or greater) than the source rate. The paper sets the conditions which a set of sub-rate sinks has to satisfy in order for that methodology to be applicable.},
  archive      = {J_COMCOM},
  author       = {B. Grinboim and I. Shrem and O. Amrani},
  doi          = {10.1016/j.comcom.2023.06.009},
  journal      = {Computer Communications},
  pages        = {271-282},
  shortjournal = {Comput. Commun.},
  title        = {Sub-rate linear network coding},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wi-fi frame detection via spiking neural networks with
memristive synapses. <em>COMCOM</em>, <em>208</em>, 256–270. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing performance of deep learning , researchers have employed Deep Neural Networks (DNNs) for wireless communications . In particular, mechanisms for detecting Wi-Fi frames that use DNNs demonstrate their excellent performances in terms of detection accuracy. However, DNNs require significant amount of computation resources. Thus, if the DNN-based mechanisms are used in mobile devices or low-end devices, their battery would be quickly depleted. Spiking Neural Networks (SNNs), which are regarded as the next generation of neural network, have advantages over DNNs: low energy consumption and limited computational complexity . Motivated by these advantages, in this paper, we propose a mechanism to detect a Wi-Fi frame using SNNs and show the feasibility of SNNs for Wi-Fi detection. The mechanism is composed of a preprocessing module for converting collected signals and an SNN module for detecting Wi-Fi frames. The SNN module employs Leaky Integrate-and-Fire (LIF) neurons and Spike-Timing-Dependent Plasticity (STDP) learning rule . To reflect the features of an actual neuromorphic hardware system , our SNN module considers memristive synaptic features such as non-linear weight updates. Our experimental study demonstrates the detection capabilities of the proposed mechanism are comparable to those of previous mechanisms that use DNNs, CNNs and RNNs while consuming definitely less energy than the previous mechanism.},
  archive      = {J_COMCOM},
  author       = {Hyun-Jong Lee and Dong-Hoon Kim and Jae-Han Lim},
  doi          = {10.1016/j.comcom.2023.06.006},
  journal      = {Computer Communications},
  pages        = {256-270},
  shortjournal = {Comput. Commun.},
  title        = {Wi-fi frame detection via spiking neural networks with memristive synapses},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensor-based implicit authentication through learning user
physiological and behavioral characteristics. <em>COMCOM</em>,
<em>208</em>, 244–255. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, a large amount of users’privacy data is stored in mobile intelligent devices, and authentication systems have become an important defense to protect that data. The traditional explicit authentication , however, is under significant threat from a great deal of side channel attacks , such as those based on computer vision and touch screen attacks. Users’authentication keys are facing a high risk of leakage, whereas implicit authentication is one of the effective methods to resist various side channel attacks . Nevertheless, existing user implicit authentication approaches still have weaknesses in security and usability, which increase users’authentication burdens and pose potential safety issues. To address this problem, our paper proposes a user implicit authentication approach based on physiological and behavioral characteristics, using the most commonly performed actions in daily life to pre-authenticate users. In our method, multiple sensors are utilized to extract physiological and behavioral characteristics of users, and the sensor data is fused by aligning the extracted characteristic data and using the Kalman filter to reduce its noise. Users’authentication results can be determined by comparing the similarities between their reference and authentication samples using the siamese neural network . The experimental results show that our approach improves the performance of implicit authentication while ensuring its security and usability.},
  archive      = {J_COMCOM},
  author       = {Jinghui Zhang and Zichao Li and Hancheng Zhang and Wei Zhang and Zhen Ling and Ming Yang},
  doi          = {10.1016/j.comcom.2023.06.016},
  journal      = {Computer Communications},
  pages        = {244-255},
  shortjournal = {Comput. Commun.},
  title        = {Sensor-based implicit authentication through learning user physiological and behavioral characteristics},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards 5G new radio sidelink communications: A versatile
link-level simulator and performance evaluation. <em>COMCOM</em>,
<em>208</em>, 231–243. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sidelink in cellular networks enables direct exchange of data packets between devices without the need for network infrastructure, resulting in various benefits, including communication in out-of-coverage areas and possible decrease in latency by a considerable extent. Thus, sidelink is a favorable choice for applications like public safety communications and Vehicle-to-Everything (V2X) communications. As 4G Long Term Evolution (LTE) advanced to 5G New Radio (NR) under the Third Generation Partnership Project (3GPP), several new features were introduced to sidelink, such as two-stage Sidelink Control Informations (SCIs), data and control multiplexing, and feedback-based Hybrid Automatic Repeat Request (HARQ) with a configurable maximum number of transmissions. To conduct extensive NR sidelink link-level evaluations, a comprehensive simulation platform is essential. In this paper, we introduce the first publicly accessible 5G NR Link-Level Simulator (LLS) that supports major 5G NR sidelink features and complies with the 3GPP standards. This MATLAB-based simulator allows for flexible control over various Physical Layer (PHY) configurations, facilitating customized simulations on algorithm development and performance evaluations. We discuss the simulator’s structure and the sidelink features implemented in detail and evaluate the 5G NR sidelink performance using the developed simulator. Our simulation results indicate that the Block Error Rate (BLER) curves are insensitive to error-prone 2nd-stage Sidelink Control Information (SCI2) and number of Resource Blocks (RBs) allocated; however, the sidelink communication range is sensitive to the deployment environment . We also highlight the need for a careful choice of numerology, device power class and HARQ configuration to balance performance metrics for a variety of services based on 5G NR sidelink deployment scenarios .},
  archive      = {J_COMCOM},
  author       = {Peng Liu and Chen Shen and Chunmei Liu and Fernando J. Cintrón and Lyutianyang Zhang and Liu Cao and Richard Rouil and Sumit Roy},
  doi          = {10.1016/j.comcom.2023.06.005},
  journal      = {Computer Communications},
  pages        = {231-243},
  shortjournal = {Comput. Commun.},
  title        = {Towards 5G new radio sidelink communications: A versatile link-level simulator and performance evaluation},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Opportunistic airborne virtual network infrastructure for
urban wireless networks. <em>COMCOM</em>, <em>208</em>, 220–230. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the suitability of Unmanned Aerial Vehicles (UAVs) as purely opportunistic airborne virtual network infrastructure to support urban wireless networks, specifically in two Vehicle-to-Everything (V2X) use cases: First, UAVs being used as relays for cooperative awareness applications; second, UAVs being used to coordinate channel access for platooning in urban areas. We do not require that these UAVs alter trajectory nor speed from those of their random, unrelated primary missions, so that these additional tasks can be executed with close-to-zero impact on the execution of their primary missions. Based on extensive computer simulations we show that, within a wide band of acceptable speeds, flight routes (up to a standard deviation of 300 m from the optimum) as well as altitudes, opportunistic relaying of transmissions via UAVs can yield a benefit to system performance that is on the same order of magnitude as that of optimally deployed UAVs. We further show that an opportunistic channel access control can reduce the total number of packet collisions by approx. 86\% compared to a scenario without any UAVs. Moreover, much of the reduction in impact due to suboptimal missions can be recovered simply by moderately increasing the number of UAVs.},
  archive      = {J_COMCOM},
  author       = {Tobias Hardes and Christoph Sommer},
  doi          = {10.1016/j.comcom.2023.06.003},
  journal      = {Computer Communications},
  pages        = {220-230},
  shortjournal = {Comput. Commun.},
  title        = {Opportunistic airborne virtual network infrastructure for urban wireless networks},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of no-light fault prediction of PON based on
deep learning method. <em>COMCOM</em>, <em>208</em>, 210–219. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased demand for broadband communication services has led to a wider deployment of fiber in the last mile. However, this expansion comes with the challenge of high maintenance costs for optical networks . To address this issue, there is a need for cost-efficient and reliable monitoring solutions that can detect and locate faults in different fiber access architectures. One specific concern in the operation of Passive Optical Networks (PONs) is the occurrence of no-light faults in the PON ports. To tackle this practical problem, we propose an intelligent scheme based on Long Short-Term Memory Networks (LSTMs) and Gated Recurrent Units (GRUs). These deep learning models are trained on historical data of the Optical Line Terminal (OLT) and PON behavior to predict future fault occurrences in real-time. To evaluate the performance of our proposed scheme, we conducted extensive experiments using real-world data. The results demonstrate the superiority of our approach over traditional methods and state-of-the-art models, as evidenced by improved accuracy, precision, recall, and F1 measure.},
  archive      = {J_COMCOM},
  author       = {Wei Jiang and Qiulin Qian and Wangwen Yong and Rajesh Kumar and Junrui Wu and Hanwen Zhang},
  doi          = {10.1016/j.comcom.2023.06.002},
  journal      = {Computer Communications},
  pages        = {210-219},
  shortjournal = {Comput. Commun.},
  title        = {Application of no-light fault prediction of PON based on deep learning method},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced propagation model constrained RSS fingerprints
patching with map assistance for wi-fi positioning. <em>COMCOM</em>,
<em>208</em>, 200–209. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi fingerprint-based positioning is widely used caused by its low cost and ease of implementation when Global Positioning System technology struggles to obtain signal and accuracy in indoor environments. A prerequisite for Wi-Fi fingerprint-based positioning is the availability of an accurate fingerprint database . However, in practice, access to a large proportion of the area is restricted, leading to a rapid degradation of the positioning accuracy of existing interpolation techniques in the absence of reference data in the vicinity of the area. In this paper, we propose an enhanced propagation model constrained fingerprint patching algorithm for incomplete RSS fingerprint databases, aiming to extend the coverage of fingerprints and reduce the overall workload of survey. The algorithm integrates a map-assisted wireless signal propagation models with incomplete spatially sampled fingerprint data to construct a propagation model for each Wi-Fi access point (AP). Recalculate the received signal strength at each reference point for each AP in the fingerprint database according to the specific model. The fingerprint information for the unrecorded regions is then refined by interpolation and the fingerprints initially collected are optimized. We experimentally validate the effectiveness of the proposed interpolation algorithm and compare it with the conventional Support Vector Regression (SVR), Gaussian Process Regression (GPR), and rational quadratic kernel (RQK) methods. The results show that the positioning performance of proposed method is improved by 18.1\%, 23.1\% and 39.7\% in the blank area relative to RQK, GPR and SVR, respectively. In addition, our method outperforms other algorithms with stable performance in different scenarios, especially in scenarios containing obstacle walls, and the performance benefit appears more notable in the complicated indoor scenario with cramped rooms.},
  archive      = {J_COMCOM},
  author       = {Xueting Xu and Chenxin Zhang and Ao Peng},
  doi          = {10.1016/j.comcom.2023.05.020},
  journal      = {Computer Communications},
  pages        = {200-209},
  shortjournal = {Comput. Commun.},
  title        = {Enhanced propagation model constrained RSS fingerprints patching with map assistance for wi-fi positioning},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An o-MAPPO scheme for joint computation offloading and
resources allocation in UAV assisted MEC systems. <em>COMCOM</em>,
<em>208</em>, 190–199. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of mobile edge computing (MEC) systems with unmanned aerial vehicle (UAV) has gained a high profile in recent years due to its high flexibility and ability to handle intensive tasks. The computation offloading strategy and the resource allocation scheme affect the system performance significantly. Moreover, the system complexity increases with the numbers of users and servers exponentially. It is challenging to consider jointly the computation offloading and the resource allocation in MEC systems that have multiple users and multiple servers. This paper formulates the joint resources management in the UAV assisted MEC network as a partially observable markov decision process and proposes an online multi-agent proximal policy optimization (O-MAPPO) scheme to improve the energy efficiency while guaranteeing the requirements in task, power consumption , computation, and time. Specifically, users and servers are set as agents. All agents cooperatively make decisions of computation offloading and resource allocation to maximize the energy efficiency. Simulation results show that the O-MAPPO scheme significantly outperforms benchmark algorithms in robustness and stability.},
  archive      = {J_COMCOM},
  author       = {Ming Cheng and Canlin Zhu and Min Lin and Jun-Bo Wang and Wei-Ping Zhu},
  doi          = {10.1016/j.comcom.2023.06.008},
  journal      = {Computer Communications},
  pages        = {190-199},
  shortjournal = {Comput. Commun.},
  title        = {An O-MAPPO scheme for joint computation offloading and resources allocation in UAV assisted MEC systems},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online and reliable SFC protection scheme of distributed
cloud network for future IoT application. <em>COMCOM</em>, <em>208</em>,
179–189. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The implementation of intelligent device-free sensing (IDFS) necessitates a substantial number of edge IoT devices to extend the service coverage. Prior to deploying these IoT applications on a large scale, a reliable service provision scheme is imperative. In recent years, network function virtualization (NFV) technologies enable fast and convenient service provision by instantiating service function chain (SFC) in the distributed cloud network. However, ensuring resilient service provision is a challenge in NFV environment due to the vulnerability of software-defined network functions. Furthermore, the sequential arrangement of virtual network functions (VNFs) in SFC exacerbates the situation, as the malfunction of any VNFs within the SFC could result in the collapse of the associated service. In this paper, we firstly built a mathematical model for SFC online reliability protection and resource consumption optimization. We then propose a multi-mode VNF backup scheme to guarantee the different reliability requirements for various services. Furthermore, a novel SFC Online Reliability Protection (SORP) scheme is introduced to handle the dynamic SFC requests in distributed cloud network. The simulation results evince the efficacy of the SORP scheme in mitigating service failures and achieving a superior request acceptance ratio, while concurrently reducing average bandwidth consumption, which is particularly noteworthy in high latency scenarios.},
  archive      = {J_COMCOM},
  author       = {Chenjing Tian and Haotong Cao and Yinjin Fu and Sahil Garg and Georges Kaddoum and Mohammad Mehedi Hassan},
  doi          = {10.1016/j.comcom.2023.06.007},
  journal      = {Computer Communications},
  pages        = {179-189},
  shortjournal = {Comput. Commun.},
  title        = {Online and reliable SFC protection scheme of distributed cloud network for future IoT application},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient privacy-preserving authentication scheme with
enhanced security for IoMT applications. <em>COMCOM</em>, <em>208</em>,
171–178. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Medical Things (IoMT), an essential application of Internet of Things (IoT) technology in the health sector, is of great practical importance in improving healthcare quality and reducing healthcare costs and errors. For years, researchers have designed many cryptographic schemes for securing IoMT applications. However, in this work, we reveal that the state-of-the-art cloud-centric IoMT-based smart healthcare system cannot be deployed in real-world scenarios by proposing a concrete signature forgery attack on their underlying pairing-based homomorphic certificateless signature (CLS) scheme. Then we present a new CLS scheme without pairings and formally prove its security under the standard cryptographic assumption. Based on our design, we put forward a new secure and efficient IoMT-based smart healthcare system. The performance comparison results illustrate the practicality of our construction.},
  archive      = {J_COMCOM},
  author       = {Feihong Xu and Shubo Liu and Xu Yang},
  doi          = {10.1016/j.comcom.2023.06.012},
  journal      = {Computer Communications},
  pages        = {171-178},
  shortjournal = {Comput. Commun.},
  title        = {An efficient privacy-preserving authentication scheme with enhanced security for IoMT applications},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of a decentralized bayesian optimization algorithm
for improving spatial reuse in dense WLANs. <em>COMCOM</em>,
<em>208</em>, 158–170. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite representing the prominent means of accessing the Internet, WLANs remain subject to performance issues, which may be mitigated through more efficient spatial reuse of radio channels. In this perspective, the IEEE 802.11ax amendment enables the dynamic update of two key parameters in wireless transmission: the transmission power ( TX_POWER ) and the sensitivity threshold ( OBSS_PD ). In this paper, we present INSPIRE , a distributed online learning solution performing local Bayesian optimizations based on Gaussian processes to improve spatial reuse in WLANs . INSPIRE makes no explicit assumptions on the WLANs’ topology and favors altruistic behaviors of the access points in their search for adequate configurations of their TX_POWER and OBSS_PD parameters. INSPIRE can easily be extended to work with a limited number of observations to throttle its computational complexity . We demonstrate the superiority of INSPIRE over other state-of-the-art strategies using the ns-3 simulator and two examples inspired by real-life deployments of dense WLANs. Our results show that, in only a few seconds, INSPIRE is able to drastically increase the quality of service of operational WLANs by improving their fairness and throughput. Finally, we discuss the configurations recommended by INSPIRE . We show that they comply with an 802.11ax empirical recommendation, and we correlate their values with some graph-based metrics of the WLAN topologies.},
  archive      = {J_COMCOM},
  author       = {Anthony Bardou and Thomas Begin},
  doi          = {10.1016/j.comcom.2023.06.004},
  journal      = {Computer Communications},
  pages        = {158-170},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of a decentralized bayesian optimization algorithm for improving spatial reuse in dense WLANs},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Throughput maximization and reliable wireless communication
in NOMA using chained fog structure and weighted energy efficiency power
allocation approach. <em>COMCOM</em>, <em>208</em>, 147–157. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel radio access strategies must be developed to support an unprecedented number of connected devices to increase radio spectral efficiency of 5G and outside, and non-orthogonal multiple features (NOMA) arisen as a possible contender. To support cognitive NOMA networks, power allocation and NOMA-secondary user allocation is the effective technique for enhancing the resource utilization proficiency in power and spectrum domain. NOMA approach provides throughput enhancement to fulfil the demands of next group of Wireless Communication Networks. In this manuscript, a Chained Fog Structure (CFS) with Weighted Energy Efficiency Power Allocation (WEE-PA) method is proposed for throughput maximization and reliable wireless communication (CFS-WEE-NOMA). The aim of this study is “to enhance the throughput by involving underlay NOMA with Chained Fog Structure and WEE-PA method”. Here, the proposed WEE-PA approach is applied to multicarrier NOMA with chained fog structure for enhancing the throughput. During this analysis, the computational complexity due to the consideration of high users and different subcarrier values are solved by the proposed CFS-WEE-NOMA approach. Thus, the proper user pairing, power allocation , and bandwidth sharing of the proposed methodology ensure seamless communication. The simulation of this work is done in MATLAB and the performance metrics like throughput, computational complexity , power consumption , energy efficiency, delay, sum rate is analysed. The proposed CFS-WEE-NOMA approach has achieved 8.6\%, 7.7\%, and 6.7\% high throughput based on transmitted power, 13.6\%, 8.6\%, and 11.7\% high throughput based on subcarrier , and 13.6\%, 9.6\%, and 12.7\% high energy efficiency compared with the existing methods, like Dynamic Network Resource Allocation (DNRA) algorithm (DNRA-NOMA), Multiple Carrier Cell-Less Non-Orthogonal Multiple Access (MC-CL-NOMA), dynamic programming (DP) recursion framework based multicarrier NOMA systems (DPRF-MC-NOMA) methods respectively.},
  archive      = {J_COMCOM},
  author       = {Supraja G. and Jeyalakshmi V.},
  doi          = {10.1016/j.comcom.2023.05.024},
  journal      = {Computer Communications},
  pages        = {147-157},
  shortjournal = {Comput. Commun.},
  title        = {Throughput maximization and reliable wireless communication in NOMA using chained fog structure and weighted energy efficiency power allocation approach},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mobility-aware edge server placement for mobile edge
computing. <em>COMCOM</em>, <em>208</em>, 136–146. (<a
href="https://doi.org/10.1016/j.comcom.2023.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge server placement plays a vital role in mobile edge computing to enable low-latency and high-throughput services by deploying edge servers at suitable geographical locations. Exiting work mainly focused on satisfying the QoS metrics of deployment while the impact of the dynamic movement of users is ignored. Dynamic movements result from different lifestyles and routines of mobile users whose points of interest thus changes over time which leads to significant unbalanced workload for edge servers. To address this challenge, we present a mobility-aware edge server placement method with two key features: (1) in the offline phase, we designed a fast heuristic algorithm to generate an edge server deployment strategy for massive data; (2) in the online phase, we designed a fine-tuning mechanism of deployment strategy based on cooperative game theory to adapt to users’ dynamic movements. To demonstrate the utility of the proposed method, we have performed a comprehensive experimental evaluation on a large-scale dataset of over 1500 edge devices collected over a year. Experimental results show our method outperforms all baselines significantly in terms of query throughput and response time.},
  archive      = {J_COMCOM},
  author       = {Yuanyi Chen and Dezhi Wang and Nailong Wu and Zhengzhe Xiang},
  doi          = {10.1016/j.comcom.2023.06.001},
  journal      = {Computer Communications},
  pages        = {136-146},
  shortjournal = {Comput. Commun.},
  title        = {Mobility-aware edge server placement for mobile edge computing},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LVSegNet: A novel deep learning-based framework for left
ventricle automatic segmentation using magnetic resonance imaging.
<em>COMCOM</em>, <em>208</em>, 124–135. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of the left ventricle(LV) using magnetic resonance image (MRI) is essential for assessing cardiac functionality such as area, volume, and ejection fraction in clinical applications. With the development of deep learning(DL), LV segmentation has achieved a series of huge breakthroughs and promising performance in recent years. However, it also encounters some dilemmas in which ill-defined borders and operator dependence problems. U-net is a well-known architecture in medical image segmentation based on an encoder–decoder method, which model achieves outstanding performance. However, the most current methods that follow the existing U-net-based paradigm ignore the contribution of context semantic strengths during the segmentation, which cannot explicitly and thoroughly exploit the intrinsic similarity of medical images. To tackle these problems, we propose LVSegNet, a comprehensive model, which explicitly fulfills the segmentation of LV using MRI via elaborately combining various semantic learning modules. This model includes three parts: encoder, center, and decoder. Specifically, the ResLink block of the encoder part designs pipeline attention, which integrates the context feature across channels. The dense atrous spatial pyramid pooling (DenseASPP) module of the center part is leveraged to magnify the receptive field. The dense up-sampling convolution (DUC) module of the decoder is utilized to replace common up-sampling, which recovers the feature information of images to the greatest extent. Moreover, we also conduct extensive experiments on public LV segmentation benchmarks of Sunnybrook Cardiac Data and our clinical dataset. We demonstrate and validate the performance superiority of LVSegNet, outperforming the previous outstanding counterpart methods by 15.4\%, 6.6\%, and 5.3\% in F1-score compared to U-net, deeplabV3, and deeplab V3+ respectively. Experimental results have verified that the proposed LV automatic segmentation model is more robust than various encoder–decoder counterparts.},
  archive      = {J_COMCOM},
  author       = {Hao Dang and Miao Li and Xingxiang Tao and Ge Zhang and Xingqun Qi},
  doi          = {10.1016/j.comcom.2023.05.011},
  journal      = {Computer Communications},
  pages        = {124-135},
  shortjournal = {Comput. Commun.},
  title        = {LVSegNet: A novel deep learning-based framework for left ventricle automatic segmentation using magnetic resonance imaging},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MDAP: Module dependency based anomaly prediction.
<em>COMCOM</em>, <em>208</em>, 111–123. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale distributed systems with multiple interconnected modules, failure of even a single module might have a cascading effect and might result in overall system failure. Naturally, timely identification and resolution of these system-wide anomaly is highly crucial for ensuring robust functioning of a system. However, while many frameworks for detecting anomalous behaviour exist today, very few frameworks are proposed which can leverage those anomalies for ultimately detecting system failures (note that some anomalies might not lead to a failure too). To fill this gap, in our work, we propose M D A P MDAP , a system anomaly-prediction framework for large-scale distributed systems. Specifically M D A P MDAP made the following contributions: (a) It detects anomalous behaviour in a system by considering behavioural changes of the interdependencies across different modules of the system with only information from system logs. (b) Provides the ability to raise early failure alerts. (c) Finally evaluating with three different datasets, M D A P MDAP achieves 39\%–52\% better accuracy compared to the baseline algorithms in identifying periods when the system is performing abnormally—we achieve a high true positive rate of 70.78\% and low false positive rate of 3.56\% using information about only a few bugs.},
  archive      = {J_COMCOM},
  author       = {Harsh Borse and Bikash Sahoo and Prateek Chanda and Soumik Sinha and Mainack Mondal and Bivas Mitra},
  doi          = {10.1016/j.comcom.2023.05.023},
  journal      = {Computer Communications},
  pages        = {111-123},
  shortjournal = {Comput. Commun.},
  title        = {MDAP: Module dependency based anomaly prediction},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tutorial on reinforcement learning in selected aspects of
communications and networking. <em>COMCOM</em>, <em>208</em>, 89–110.
(<a href="https://doi.org/10.1016/j.comcom.2023.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telecommunication systems are increasingly complex, dynamic, and heterogeneous. Tools are needed to efficiently support and automate complex control and management processes. Reinforcement learning becomes one of the most attractive and popular solutions applicable to a wide variety of different aspects of communications and networking. Its development is further boosted by the favorable conditions created by both the existing IT infrastructure and evolving network architectures . The aim of this tutorial is twofold. Firstly, to provide fundamentals regarding the Reinforcement Learning (RL) method. Secondly, to comprehensively study the examples of applying RL-based solutions to solve problems in different aspects of communications and networking. Studies are supplemented with additional explanations, figures and critical considerations, including pros and cons of using selected methods for particular purposes. This part uniquely complements the tutorial one and facilitates in-depth understanding of RL. Based on the conducted studies, we draw a comparative analysis, summaries and expected future research topics and challenges of using RL in communications and networking.},
  archive      = {J_COMCOM},
  author       = {Piotr Boryło and Edyta Biernacka and Jerzy Domżał and Bartosz Ka̧dziołka and Mirosław Kantor and Krzysztof Rusek and Maciej Skała and Krzysztof Wajda and Robert Wójcik and Wojciech Za̧bek},
  doi          = {10.1016/j.comcom.2023.05.019},
  journal      = {Computer Communications},
  pages        = {89-110},
  shortjournal = {Comput. Commun.},
  title        = {A tutorial on reinforcement learning in selected aspects of communications and networking},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear complementary pair of codes based lightweight RFID
protocol. <em>COMCOM</em>, <em>208</em>, 79–88. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data security and privacy is an essential topic in today’s life. It is necessary to prevent secret data from leakage. There are many services where data transfer is done without any physical contact and at a low-cost. Radio frequency identification (RFID) is one of the basic technical knowledge that solves the problem of data privacy. Many RFID protocols have been designed to ensure data security and privacy. But, a large portion of these protocols suffers from common security attacks and high computational costs. We present a lightweight RFID protocol based on linear complementary pair of codes to overcome these issues. This protocol ensures formal security using a random oracle model and BAN logic. For security verification of our protocol, we use the Scyther simulation tool. In addition, we compare the performance of our protocol in terms of storage costs, communication costs and computational costs with related existing protocols. We determine that our protocol gives preferable security and effectiveness over related competing protocols and is applicable for RFID systems.},
  archive      = {J_COMCOM},
  author       = {Haradhan Ghosh and Pramod Kumar Maurya and Satya Bagchi},
  doi          = {10.1016/j.comcom.2023.05.022},
  journal      = {Computer Communications},
  pages        = {79-88},
  shortjournal = {Comput. Commun.},
  title        = {Linear complementary pair of codes based lightweight RFID protocol},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on UAV-assisted wireless communications: Recent
advances and future trends. <em>COMCOM</em>, <em>208</em>, 44–78. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their characteristics of mobility, flexibility and adaptive altitude, unmanned aerial vehicles (UAVs), also known as drones, have immense potential applications in wireless systems . In particular, UAVs can operate as flying mobile terminals within a cellular network , and such cellular-connected UAVs have enabled several applications ranging from real-time video streaming to item delivery. Furthermore, the deployment of UAVs in wireless networks as flying base stations (BSs), servers or relays is expected to enhance network performance in terms of coverage, capacity, reliability and energy-efficiency. In this paper, a comprehensive tutorial on the potential applications and benefits of UAVs is presented. Specifically, UAVs as aerial BSs collecting data from ground sensors, transmitting power to energy-constrained devices and broadcasting information to ground users are explored along with representative design objectives. Then, UAVs as relay nodes for coverage extension, pathloss compensation and interference mitigation are surveyed comprehensively. Besides, UAVs as mobile servers performing edge computing for ground users are introduced, and various aerial offloading modes are reviewed thoroughly. Furthermore, radio and computation resource optimization, as well as mathematical tools and developed algorithms, are described in UAV-assisted communicating, relaying, and computing systems. Finally, open problems and potential research directions pertaining UAV-assisted communications are presented. In a nutshell, this tutorial provides key guidelines on how to design, analyze and optimize UAV-assisted wireless communication systems.},
  archive      = {J_COMCOM},
  author       = {Xiaohui Gu and Guoan Zhang},
  doi          = {10.1016/j.comcom.2023.05.013},
  journal      = {Computer Communications},
  pages        = {44-78},
  shortjournal = {Comput. Commun.},
  title        = {A survey on UAV-assisted wireless communications: Recent advances and future trends},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task offloading scheme combining deep reinforcement learning
and convolutional neural networks for vehicle trajectory prediction in
smart cities. <em>COMCOM</em>, <em>208</em>, 29–43. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Artificial Intelligence , the intelligent vehicle with diverse functions and complex system architectures brings more computing tasks to vehicles. Due to insufficient local computing resources for vehicles, mobile edge computing is seen as a solution to relieve local computing pressure. In the background of Telematics, when the vehicle offloads the computation task to the edge server, the communication time between the vehicle and the base station will become shorter due to the high-speed movement of the vehicle. If the vehicle leaves the current base station before the computation is completed, the vehicle will not be able to obtain the computation results in time. Therefore, a task offloading scheme based on trajectory prediction in the context of Telematics is proposed to solve the problem of short communication time between vehicles and base stations due to high-speed movement of vehicles. The solution combines Long Short Term Memory and convolutional neural networks to predict the base station the vehicle will pass and the time to reach it, which enables the return of calculation results through the communication between the base stations and enables tasks with larger data volumes to be offloaded to the edge server. After simulation experiments, the results can prove that the scheme proposed in this paper is adapted to the intelligent vehicle environment, shows greater stability in the face of large computational tasks and reduces about 25\% task latency compared to the traditional task offloading scheme.},
  archive      = {J_COMCOM},
  author       = {Jiachen Zeng and Fangfang Gou and Jia Wu},
  doi          = {10.1016/j.comcom.2023.05.021},
  journal      = {Computer Communications},
  pages        = {29-43},
  shortjournal = {Comput. Commun.},
  title        = {Task offloading scheme combining deep reinforcement learning and convolutional neural networks for vehicle trajectory prediction in smart cities},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Placement of SDN controllers based on network setup cost and
latency of control packets. <em>COMCOM</em>, <em>208</em>, 15–28. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networking (SDN) was developed to simplify the management of computer networks. In the SDN architecture, the control layer is completely decoupled from the data layer. Moreover, network software configuration is also possible. To setup an SDN, you need a device called controller, which is not available in traditional networks. In the control layer, there are several controllers that control the data layer switches. The type, number and place of these controllers directly affect the cost and average latency of control packets . Since achieving the lowest setup cost and the minimum latency of the control packets (between switches and controllers, and among controllers) is of great importance at the same time, in this paper, a binary linear programming model is proposed to achieve the trade-off between them in wide area networks(WAN). The model determines the type, place and minimum number of required controllers by receiving the place of the switches, the amount of processing capacity of the controllers and the setup cost of each controller. Accordingly, it reduces the setup cost, average latency between switches and controllers as well as average latency among controllers in comparison with previous methods. The results of the study show that the model has the lowest setup cost and the minimum average latency of control packets (switch-to-controller and controller-to-controller) in controllers placement compared to the previous methods.},
  archive      = {J_COMCOM},
  author       = {Abdullah Naseri and Mahmood Ahmadi and Latif PourKarimi},
  doi          = {10.1016/j.comcom.2023.05.015},
  journal      = {Computer Communications},
  pages        = {15-28},
  shortjournal = {Comput. Commun.},
  title        = {Placement of SDN controllers based on network setup cost and latency of control packets},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new platform for machine-learning-based network traffic
classification. <em>COMCOM</em>, <em>208</em>, 1–14. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study provides a new platform for classifying encrypted network traffic based on machine learning (ML) techniques. The architecture of the platform is designed for real-world network traffic classification problems with performance-oriented, practical, and up-to-date software technologies. In addition, this study introduces a new feature extraction method to the literature. The proposed platform applies ML techniques with flow-based statistical features of encrypted network traffic and new feature extraction. It takes network traffic packets as input and passes them through feature extraction, data preparation, and ML stages. In the feature extraction stage, network flows are extracted from the network traffic data by calculating their features with the NFStream tool. During the data preparation stage , the dataset is transformed into a processable state for the ML algorithm with the Apache Spark framework. This stage also includes the feature selection operation. The ML stage runs GBTree, LightGBM, and XGBoost algorithms. Moreover, we use the MLflow framework in the proposed process management to observe the ML lifecycle, including experimentation, reproducibility, and deployment. The experimental results show that the XGBoost algorithm achieves the best result with an F1 score of above 99\%.},
  archive      = {J_COMCOM},
  author       = {Ramazan Bozkır and Murtaza Ci̇ci̇oğlu and Ali Çalhan and Cengiz Toğay},
  doi          = {10.1016/j.comcom.2023.05.010},
  journal      = {Computer Communications},
  pages        = {1-14},
  shortjournal = {Comput. Commun.},
  title        = {A new platform for machine-learning-based network traffic classification},
  volume       = {208},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Construction of FANETs for user coverage and information
transmission in disaster rescue scenarios. <em>COMCOM</em>,
<em>207</em>, 164–174. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a flying ad hoc network (FANET) construction problem in a post-disaster scenario where ground users (GUs) lose contact with the outside world. The main objective of the FANET is to provide communication coverage for as many as GUs, and to transmit disaster information back to the rescue center as soon as possible. To solve this FANET construction problem, we first propose a FANET construction algorithm based on particle swarm optimization (PSO) algorithm. To improve the performance of the PSO-based FANET construction algorithm , we further model the FANET construction problem as an ordinary potential game (OPG), and propose a better-response-based FANET construction algorithm. Several simulation experiments are presented to evaluate the effectiveness of these proposed two algorithms. Simulation results demonstrate that both the PSO-based FANET construction algorithm and the better-response-based FANET construction algorithm can be used to maximize the coverage rate of GUs and minimize the average transmission delay from rescue UAVs to the gateway UAV. Moreover, the better-response-based FANET construction algorithm has a fast convergence rate, and performs better than the PSO-based FANET construction algorithm and other baselines in terms of coverage rate and average transmission delay.},
  archive      = {J_COMCOM},
  author       = {Yaqun Liu and Jun Xie and Changyou Xing and Shengxu Xie and Baoan Ni},
  doi          = {10.1016/j.comcom.2023.05.014},
  journal      = {Computer Communications},
  pages        = {164-174},
  shortjournal = {Comput. Commun.},
  title        = {Construction of FANETs for user coverage and information transmission in disaster rescue scenarios},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A crowd cooperative defense model for mitigating DDoS
attacks in mobile crowdsensing networks. <em>COMCOM</em>, <em>207</em>,
150–163. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowdsensing (MCS) is rapidly growing up while encountering severe security threats simultaneously. DDoS attacks tend to disrupt large-scale sensing tasks to the MCS and reduce the availability of MCS. However, the existing centralized defense technology has the problem of high cost and the existing research lacks the application of distributed cooperative characteristics of MCS. A crowd cooperative defense model for mitigating DDoS attacks in MCS is proposed based on these. Firstly, the crowdsensing networks are formally described. The cooperative mechanism of sensing nodes participating in overlapping sensing coalition is generalized, which breaks the independence between the distributed sensing nodes and enables the sensing nodes to cooperate against DDoS attacks. Then, the crowd cooperative defense problem is transformed into a multi-objective combinatorial optimization problem . A multi-objective discrete Harris Hawk optimization (MODHHO) algorithm is designed. The algorithm applies non-dominated sorting and crowding strategy to evaluate the advantages and disadvantages of each cooperative defense solution on the cooperative defense objectives. In addition, the global exploration, local exploitation, and besiege phases are used to continuously update the cooperative defense solutions, and finally obtains the global optimal cooperative defense solution after a certain number of iterations. The simulation results show that the proposed model can minimize the defense cost compared to non-cooperative and non-overlapping sensing coalition methods. Compared with centralized defense, the loss cost of the proposed model is reduced by 61.8\%. In addition, the average throughput of the sensing network is increased by 35.5\%, which enhances the availability of the sensing network.},
  archive      = {J_COMCOM},
  author       = {Guosheng Zhao and Ming Gao and Jian Wang},
  doi          = {10.1016/j.comcom.2023.05.017},
  journal      = {Computer Communications},
  pages        = {150-163},
  shortjournal = {Comput. Commun.},
  title        = {A crowd cooperative defense model for mitigating DDoS attacks in mobile crowdsensing networks},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data placement and transmission scheduling for coded
multicast in mobile edge networks. <em>COMCOM</em>, <em>207</em>,
140–149. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge caching is a promising strategy to reduce the traffic load in mobile core networks , by caching popular contents at the edge of mobile networks, e.g., base stations (BSs). For the BSs to transmit data to user equipment (UEs), coded multicast can be used to improve the transmission efficiency. Moreover, in future densely deployed networks, the coverage areas of adjacent BSs are overlapped. Therefore, these cache-aided BSs can collaboratively carry out coded multicast to improve the performance of data transmission. In this paper, we consider the Data Placement and Transmission Scheduling (DPTS) problem for cached-aided coded multicast in mobile edge networks. Our objective is to minimize the total cost of data download from the data source to BSs and data transmission from the BSs to UEs. The DPTS problem is proved to be NP-hard and we first propose an Iterative Relaxation Linear Programming (IRLP) algorithm to solve it. Since the complexity of IRLP algorithm is high, we also propose another two low-complexity algorithms to solve the DPTS problem. Performance evaluation by simulation shows that the proposed algorithms can achieve a substantial reduction in total cost of data download and transmission, compared with the existing methods.},
  archive      = {J_COMCOM},
  author       = {Zhongzheng Tang and Nuo Yu and Xiaohua Jia and Xiaodong Hu},
  doi          = {10.1016/j.comcom.2023.05.016},
  journal      = {Computer Communications},
  pages        = {140-149},
  shortjournal = {Comput. Commun.},
  title        = {Data placement and transmission scheduling for coded multicast in mobile edge networks},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate compressed traffic detection via traffic analysis
using graph convolutional network based on graph structure feature.
<em>COMCOM</em>, <em>207</em>, 128–139. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the application of data compression technology expands in areas such as IoT , webpage, and video data transmission, there are problems such as leakage of compressed but unencrypted user data, difficulty in supervising compressed data, and confusion between compressed and private encrypted traffic. Existing compressed traffic detection methods are significantly affected by data length and rely on binary classification in one step using random tests. It remains a challenging task to conduct accurate and efficient compressed traffic detection. In this paper, we present GCN-RTG, a compressed traffic detection method using Graph Convolutional Network. We investigate the randomness feature transformation pattern of packet sequences, propose a graph structure based on this pattern, and design a powerful GCN-based classifier to detect compressed traffic. The experimental results show that GCN-RTG achieves 94\% accuracy in compressed traffic detection, remarkably improving by nearly 10\% accuracy compared with traditional machine learning methods and approximately 5\% compared with CNN and LSTM . Considering the effect of private encrypted traffic, GCN-RTG attains an accuracy of 89\% for detecting compressed traffic. Furthermore, GCN-RTG can maintain an 83\% accuracy even in the most extreme packet loss scenario and reach an outstanding accuracy of up to 95\% of compressed traffic detection in real-world network data sized 4GB from the Jiangsu education backbone network in China.},
  archive      = {J_COMCOM},
  author       = {Nan Fu and Guang Cheng and Xinyue Su},
  doi          = {10.1016/j.comcom.2023.04.031},
  journal      = {Computer Communications},
  pages        = {128-139},
  shortjournal = {Comput. Commun.},
  title        = {Accurate compressed traffic detection via traffic analysis using graph convolutional network based on graph structure feature},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on federated learning for security and privacy in
healthcare applications. <em>COMCOM</em>, <em>207</em>, 113–127. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advances in smart devices and applications targeting the Internet of Healthcare Things provide a perfect environment for using Machine Learning-based techniques. However, traditional ML solutions operate on centralized data collection and processing. Federated Learning (FL) is a promising solution to train ML models on multiple decentralized devices without effectively sharing private data. Therefore, FL offers a secure architecture to handle highly sensitive data in the IoHT context. This survey comprehensively reviews emerging data security and privacy applications for FL in IoHT networks. First, we present a background overview of the basic concepts of FL applied in IoHT. In particular, we rigorously investigate and evaluate the main solutions to IoHT data security and privacy issues. In addition, we categorize the most relevant publications related to IoHT data security, whether due to advances in the architecture of FL or data protection. Next, we list several IoHT network datasets for model training. Finally, we highlight the essential lessons from this review, highlighting current challenges and possible directions for future research in data security and privacy in IoHT networks using FL.},
  archive      = {J_COMCOM},
  author       = {Kristtopher K. Coelho and Michele Nogueira and Alex B. Vieira and Edelberto F. Silva and José Augusto M. Nacif},
  doi          = {10.1016/j.comcom.2023.05.012},
  journal      = {Computer Communications},
  pages        = {113-127},
  shortjournal = {Comput. Commun.},
  title        = {A survey on federated learning for security and privacy in healthcare applications},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent routing algorithm for wireless sensor networks
dynamically guided by distributed neural networks. <em>COMCOM</em>,
<em>207</em>, 100–112. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using reinforcement learning to adjust the power balance of sensor nodes dynamically is an essential approach for extending the lifetime of wireless sensor networks (WSNs), which makes sensor nodes adapt to changing environments by calculating optimal decision solutions in real time. However, as WSN grows, error propagation cannot keep up with topological changes in the networks, which prevents the sensor nodes’ routing decisions from being timely optimized. In this paper, we propose the DNN-HR algorithm, which optimizes the reinforcement learning process of WSN in three ways. First, this paper constructs a two-level learning structure . Discrete nodes perform reinforcement learning during the interaction with their neighboring nodes. Simultaneously, the neural networks that fit the V values guide the underlying nodes’ learning. Second, the algorithm divides the sensor networks into two-level subspaces according to their geographic locations, and cluster heads are elected in each subspace to form weight incremental propagation trees. Third, each sensor node sends the weight increments it learned to its first-level cluster head. At the first-level cluster head, the weight increments sent by multiple nodes are received and accumulated, and then the results are sent to the second-level cluster head. The base station acquires the weight increments of all the second-level cluster heads and then broadcasts the updated neural network weights to all the WSN nodes. Experiments determine the appropriate heuristic parameters. Compared to other algorithms, the experiments show that our proposed scheme significantly improves the packet delivery rate, node survival rate, and energy standard deviation of nodes.},
  archive      = {J_COMCOM},
  author       = {Zhibin Liu and Yuhan Liu and Xinshui Wang},
  doi          = {10.1016/j.comcom.2023.05.018},
  journal      = {Computer Communications},
  pages        = {100-112},
  shortjournal = {Comput. Commun.},
  title        = {Intelligent routing algorithm for wireless sensor networks dynamically guided by distributed neural networks},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient consensus algorithm based on improved DPoS in
UAV-assisted mobile edge computing. <em>COMCOM</em>, <em>207</em>,
86–99. (<a href="https://doi.org/10.1016/j.comcom.2023.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problems of low decentralization, low motivation for node voting, and malicious behavior of nodes for the traditional DPoS consensus mechanism in the blockchain-based UAV-assisted mobile edge computing environment, this paper proposes an improved DPoS-based consensus mechanism approach. First, the framework of a blockchain-based UAV-assisted mobile edge computing system is given, and the consensus mechanism design problem in this framework is analyzed. Second, a proxy node selection model is established based on the rights and votes obtained by the blockchain nodes, and the proxy nodes are selected to participate in the consensus process of the current cycle. Then, the voting behavior, block generation behavior, and block verification behavior of nodes are classified into positive and malicious behaviors to reward and punish the reputation value of nodes. Finally, a blockchain-based UAV-assisted mobile edge computing experimental environment is built, and the TDPoS algorithm, ADRP algorithm, and RDPoS algorithm are used as benchmark algorithms to experimentally compare with the proposed improved DPoS consensus-based algorithm. The experimental results show that the algorithms in this paper can improve network throughput, reduce block-out delay, and increase the proportion of secure proxy nodes.},
  archive      = {J_COMCOM},
  author       = {Mingyang Song and Chunlin Li and Jingsong Ye and Xunqiang Gong and Youlong Luo},
  doi          = {10.1016/j.comcom.2023.05.008},
  journal      = {Computer Communications},
  pages        = {86-99},
  shortjournal = {Comput. Commun.},
  title        = {Efficient consensus algorithm based on improved DPoS in UAV-assisted mobile edge computing},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Circular time shift modulation for robust underwater
acoustic communications in doubly spread channels. <em>COMCOM</em>,
<em>207</em>, 77–85. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulse Position Modulation (PPM) has shown great advantages by being non-coherently implementable. However, PPM suffers from the high multipath delay in underwater acoustic channels . To be robust against the doubly spread effects in underwater acoustic communications, this project proposes a novel modulation scheme that is non-linear and that can be implemented non-coherently, called the Circular Time Shift Modulation (CTSM) based on the high auto-correlation property of Zero-Correlation-Zone (ZCZ) signals. The performance of CTSM in terms of bit error rate is investigated by emulations and at-sea experiments. The results have shown that the CTSM system has higher robustness compared with PPM in underwater acoustic communications.},
  archive      = {J_COMCOM},
  author       = {Zhuoran Qi and Dario Pompili},
  doi          = {10.1016/j.comcom.2023.05.009},
  journal      = {Computer Communications},
  pages        = {77-85},
  shortjournal = {Comput. Commun.},
  title        = {Circular time shift modulation for robust underwater acoustic communications in doubly spread channels},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). F2-ToF: A feature-alignment and frequency-division
time-of-flight data denoise network. <em>COMCOM</em>, <em>207</em>,
66–76. (<a href="https://doi.org/10.1016/j.comcom.2023.04.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared wave time-of-flight (ToF) imaging is a important method to sense the 3D information of scene for Internet of Things (IoT) and artificial intelligent (AI). Driven by heavy demands from industry and users, ToF imaging has received significant research attention in recent year, but the artifacts of depth image still remain and need to be improved. Removing multiple artifacts of ToF data is usually treated as a multi-stage stitching problem for deep learning methods. However, the multi-stage cascade and cross-domain refinement architecture could increase the difficulty of model fitting and hurt the effect of noise reduction. In this paper, we classify the artifacts of ToF data as temporal-related or modulation frequency-related noise and propose a ToF denoising convolutional neural network ( f 2 f2 -ToF) to reduce multiple artifacts simultaneously. Specifically, a frequency-division structure is designed to reduce the influence of frequency-related noise in different modulation frequencies. For efficient correcting misalignment data and ensuring a one-stage end-to-end training, the feature-wise alignment module is proposed. In experiments, every proposed module effectively performed its designed task, and the whole framework achieved strong performance in ToF image refinement.},
  archive      = {J_COMCOM},
  author       = {Yanfeng Tong and Jing Chen and Zhen Leng and Bo Liu and Yongtian Wang},
  doi          = {10.1016/j.comcom.2023.04.033},
  journal      = {Computer Communications},
  pages        = {66-76},
  shortjournal = {Comput. Commun.},
  title        = {F2-ToF: A feature-alignment and frequency-division time-of-flight data denoise network},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Agile: A high-scalable and low-jitter flow tables lifecycle
management framework for multi-core programmable data plane.
<em>COMCOM</em>, <em>207</em>, 56–65. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software-defined networking (SDN), the common flow table updates have constrained its performance. The current southbound protocol determines neither the priority nor the method of flow table update, which could result in latency-sensitive applications not being served timely. Moreover, such kind of update process causes severe jitters in the data plane. In this case, this paper presents Agile as a solution. Agile is a lifecycle management framework for flow tables based on software programmable data plane. It introduces a scheduling architecture to allow the controller to freely configure the flow table update method and determine the update priority, minimizing the impact on forwarding. Moreover, an aging scheme is also designed to maintain the consistency of multi-core platforms. Agile implements with the extent of southbound interface. Extensive experimental results show that Agile can reduce the average forwarding jitter brought about by flow table updates to 2.2 μ μ s and save the update time by 56.9\%.},
  archive      = {J_COMCOM},
  author       = {Dengyu Ran and Xiao Chen and Lei Song},
  doi          = {10.1016/j.comcom.2023.05.002},
  journal      = {Computer Communications},
  pages        = {56-65},
  shortjournal = {Comput. Commun.},
  title        = {Agile: A high-scalable and low-jitter flow tables lifecycle management framework for multi-core programmable data plane},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI-based energy-efficient path planning of multiple
logistics UAVs in intelligent transportation systems. <em>COMCOM</em>,
<em>207</em>, 46–55. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of Industry 5.0, artificial intelligence (AI)-based logistics Unmanned Aerial Vehicles (UAVs) have been widely applied in intelligent transportation systems due to their advantages of faster speed, lower cost, more environment-friendly, and less manpower needed. Whereas, most of the existing logistics UAV delivery models have not taken the energy consumption of the logistics UAVs and mixed time windows of the customers, which leads to their models cannot be applied in practical transportation systems. Therefore, we propose to minimize the total energy cost of multiple logistics UAVs during the customized products delivery period for a smart transportation system . Taking the energy consumption variation of the logistics UAVs, mixed time windows of the customers, as well as simultaneous delivery and pick up into consideration, we formulate a cooperative path planning problem via jointly optimizing the route of the logistics UAVs and the service allocation. To solve this large-scale integer programming problem, we employ the Large Neighborhood Search Algorithm (LNS) to accelerate the convergence rate of Genetic Algorithm (GA), and then develop an improved GA based cooperative path planning algorithm (IGCPA). The optimization procedure of the proposed algorithm IGCPA is divided into two phases, using the GA crossover operator and variational operator in the global search phase and LNS operator in the local search phase , and validating the integer programming model and the effectiveness of the solution algorithm based on different scale cases. Finally, abundant simulation results show that the energy cost of IGCPA is reduced by 17.35\%, 15.18\% and 9.99\% compared with GA, LNS and Particle Swarm Optimization (PSO), respectively. Furthermore, the IGCPA is validated using Solomn standard data, which further verifies that the IGCPA can enhance the convergence rate of GA as well as obtain a lower delivery cost. Sensitivity analysis of the maximum UAV load and battery capacity reveals that the distribution cost tends to decrease and then increase as the increase of maximum load and battery capacity.},
  archive      = {J_COMCOM},
  author       = {Pengfei Du and Xiang He and Haotong Cao and Sahil Garg and Georges Kaddoum and Mohammad Mehedi Hassan},
  doi          = {10.1016/j.comcom.2023.04.032},
  journal      = {Computer Communications},
  pages        = {46-55},
  shortjournal = {Comput. Commun.},
  title        = {AI-based energy-efficient path planning of multiple logistics UAVs in intelligent transportation systems},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A game theory-based COVID-19 close contact detecting method
with edge computing collaboration. <em>COMCOM</em>, <em>207</em>, 36–45.
(<a href="https://doi.org/10.1016/j.comcom.2023.04.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People all throughout the world have suffered from the COVID-19 pandemic. People can be infected after brief contact, so how to assess the risk of infection for everyone effectively is a tricky challenge. In view of this challenge, the combination of wireless networks with edge computing provides new possibilities for solving the COVID-19 prevention problem. With this observation, this paper proposed a game theory-based COVID-19 close contact detecting method with edge computing collaboration, named GCDM. The GCDM method is an efficient method for detecting COVID-19 close contact infection with users’ location information. With the help of edge computing’s feature, the GCDM can deal with the detecting requirements of computing and storage and relieve the user privacy problem. Technically, as the game reaches equilibrium, the GCDM method can maximize close contact detection completion rate while minimizing the latency and cost of the evaluation process in a decentralized manner. The GCDM is described in detail and the performance of GCDM is analyzed theoretically. Extensive experiments were conducted and experimental results demonstrate the superior performance of GCDM over other three representative methods through comprehensive analysis.},
  archive      = {J_COMCOM},
  author       = {Yue Shen and Bowen Liu and Xiaoyu Xia and Lianyong Qi and Xiaolong Xu and Wanchun Dou},
  doi          = {10.1016/j.comcom.2023.04.029},
  journal      = {Computer Communications},
  pages        = {36-45},
  shortjournal = {Comput. Commun.},
  title        = {A game theory-based COVID-19 close contact detecting method with edge computing collaboration},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A security-friendly privacy-preserving solution for
federated learning. <em>COMCOM</em>, <em>207</em>, 27–35. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a privacy-aware collaborative machine learning method where the clients collaborate on constructing a global model by performing local model training using their training data and sending the local model updates to the server. Although it enhances privacy by letting the clients collaborate without sharing their training data, it is still prone to sophisticated privacy attacks because of possible information leakage from the local model updates sent to the server. To prevent such attacks, generally secure aggregation protocols are proposed so that the server will not be able to access the individual local model updates but the aggregated result. However, such secure aggregation approaches may not allow the execution of security mechanisms against some security attacks to model training, such as poisoning and backdoor attacks, because the server cannot access the individual local model updates and; therefore, cannot analyze them to detect anomalies resulting from these attacks. Thus, solutions that satisfy privacy and security at the same time or new privacy-preserving solutions that allow the server to execute some analysis on the local model updates without violating privacy are needed for federated learning. In this paper, we introduce a novel security-friendly privacy solution for federated learning based on multi-hop communication to hide clients’ identities. Our solution ensures that the forwardee clients in the path between the source client and the server cannot execute malicious activities by altering model updates and contributing to the global model construction with more than one local model update in one FL round. We then propose two different approaches to make the solution also robust against possible malicious packet drop behaviors by the forwardee clients.},
  archive      = {J_COMCOM},
  author       = {Ferhat Karakoç and Leyli Karaçay and Pinar Çomak De Cnudde and Utku Gülen and Ramin Fuladi and Elif Ustundag Soykan},
  doi          = {10.1016/j.comcom.2023.05.004},
  journal      = {Computer Communications},
  pages        = {27-35},
  shortjournal = {Comput. Commun.},
  title        = {A security-friendly privacy-preserving solution for federated learning},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SANet: A novel segmented attention mechanism and multi-level
information fusion network for 6D object pose estimation.
<em>COMCOM</em>, <em>207</em>, 19–26. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliably and rapidly estimating the 6D position of an object is a critical challenge when using Internet of Things (IoT) technologies for monitoring. Nowadays, the prevalent 6D pose estimation architecture is based on a two-stage technique, which requires a significant amount of time for both training and deploying the algorithm in actual applications. Additionally, the majority of approaches include intricate high-low level features in the network that greatly influence training but contribute less to testing. To enable more accurate 6D object pose estimation while shortening the deployment time, we used a single-stage end-to-end algorithm to design the network. In this paper, we propose SANet, which is composed of a segmented attention module and a multi-level information fusion module. Specifically, by extracting high-level semantic information from images before fusing them to the decoder, and by removing redundant information using the multi-level information fusion module, the feature fusion complexity of the model is reduced by extracting high level features. In addition, the segmented attention module can suppress unreliable information to enhance network learning of channel and spatial information, enabling the network to more accurately understand the geometric aspects of the object. Extensive experiments on LM and LMO datasets demonstrate that our method outperforms state-of-the-art baselines, ranking 1st in both speed and accuracy.},
  archive      = {J_COMCOM},
  author       = {Xinbo Geng and Fan Shi and Xu Cheng and Chen Jia and Mianzhao Wang and Shengyong Chen and Hongning Dai},
  doi          = {10.1016/j.comcom.2023.05.003},
  journal      = {Computer Communications},
  pages        = {19-26},
  shortjournal = {Comput. Commun.},
  title        = {SANet: A novel segmented attention mechanism and multi-level information fusion network for 6D object pose estimation},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Secure, accurate and privacy-aware fully decentralized
learning via co-utility. <em>COMCOM</em>, <em>207</em>, 1–18. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully decentralized learning is a setting in which each peer in a P2P network trains a machine learning model with the help of the other peers. Each peer acts as a model manager by periodically sending her current model to other peers, who answer by returning model updates they compute on their private data. This creates a tension among privacy, accuracy and security. The privacy risk is that model updates returned by a peer can leak some of the peer’s private data. Unfortunately, distorting model updates to protect privacy works against the accuracy of the trained model. On the other hand, aggregating the updates of several peers and then sending the aggregate to the model manager may preserve privacy but it goes against security, because the model manager cannot filter out individual bad updates. Also, peers are autonomous and hence it cannot be taken for granted that they will honestly supply model updates to help the model manager train her model. To reconcile accuracy, privacy and security, we present a fully decentralized learning protocol such that: (i) it allows perfectly accurate individual updates to be returned by peers to the model manager in a privacy-preserving manner; (ii) it is co-utile by design, that is, it incentivizes rational peers to follow the protocol without deviating. The latter feature discourages rational attacks that might compromise security and it also deters free riding , thereby ensuring the sustainability of the protocol.},
  archive      = {J_COMCOM},
  author       = {Jesús Manjón and Josep Domingo-Ferrer and David Sánchez and Alberto Blanco-Justicia},
  doi          = {10.1016/j.comcom.2023.05.006},
  journal      = {Computer Communications},
  pages        = {1-18},
  shortjournal = {Comput. Commun.},
  title        = {Secure, accurate and privacy-aware fully decentralized learning via co-utility},
  volume       = {207},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RIS-assisted edge-D2D cooperative edge computing for
industrial applications. <em>COMCOM</em>, <em>206</em>, 178–188. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligence and low latency are particularly desired in the vision of industrial intelligence. To support intelligence, massive amount of data is generated from distributed Internet of Things (IoT) devices, and expected to quickly process with artificial intelligence (AI) for data value maximization. To reduce the network transmission delay from data source to computing nodes, edge computing is promising. However, the scarce and dynamic wireless resource as well as limited computing capability of edge servers challenge the edge intelligence . This paper studies a reconfigurable intelligent surface (RIS)-assisted edge-device-to-device (D2D) cooperative edge computing for end-to-end delay reduction in industrial environments. In special, we consider such an edge-D2D cooperative edge computing system, where the IoT devices could offload their tasks to edge server via cellular links for edge computing, or transmit the tasks to helper nodes via D2D links for local computing. We also consider the base station (BS)-based and D2D-based RISs deployed in cellular and D2D networks respectively. We formulate the joint computation offloading , beamforming optimization of both BS-based and D2D based RISs , and CPU resource allocation problem for average end-to-end delay minimization. Then, a distributed and cooperative scheme, called RIS-assisted edge-D2D cooperative computation offloading (RIS-assisted EDCO), is proposed to address the problem. The simulation results have illustrated the efficiency of the proposal for low end-to-end delay performance provisioning.},
  archive      = {J_COMCOM},
  author       = {Mian Guo and Mithun Mukherjee and Jaime Lloret},
  doi          = {10.1016/j.comcom.2023.05.007},
  journal      = {Computer Communications},
  pages        = {178-188},
  shortjournal = {Comput. Commun.},
  title        = {RIS-assisted edge-D2D cooperative edge computing for industrial applications},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliable and efficient emergency rescue networks: A
blockchain and fireworks algorithm-based approach. <em>COMCOM</em>,
<em>206</em>, 172–177. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, coronavirus disease 2019 (COVID-19) has been a severe issue the world faces. Emergency rescue networks concerning the distribution of relief materials have gained extensive attention to tackle COVID-19 and related emergency issues. However, it is challenging to establish reliable and efficient emergency rescue networks due to information asymmetry and lack of trust among different rescue stations. In this work, we propose blockchain-based emergency rescue networks to track every transaction of the relief materials reliably and make decisions to deliver relief materials efficiently. More specifically, we propose a hybrid blockchain architecture that employs on-chain data verification to authenticate data records and off-chain data storage to reduce storage overhead . Furthermore, we propose a fireworks algorithm to efficiently calculate the optimal allocation strategies for relief materials. The algorithm provides chaotic random screening and node request guarantee techniques with good convergence. The simulation results show that integrating blockchain technology and the fireworks algorithm can significantly improve relief materials’ operation efficiency and distribution quality.},
  archive      = {J_COMCOM},
  author       = {Bin Chen and Weihua Zhang and Yijin Shi and Di Lv and Zilan Yang},
  doi          = {10.1016/j.comcom.2023.05.005},
  journal      = {Computer Communications},
  pages        = {172-177},
  shortjournal = {Comput. Commun.},
  title        = {Reliable and efficient emergency rescue networks: A blockchain and fireworks algorithm-based approach},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Game-theoretic approach to epidemic modeling of
countermeasures against future malware evolution. <em>COMCOM</em>,
<em>206</em>, 160–171. (<a
href="https://doi.org/10.1016/j.comcom.2023.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, vulnerability mining techniques that discover unknown vulnerabilities based on machine learning have been attracted much attention for protecting software. Although we benefit from these techniques for cyber security , they could be exploited by malicious attackers. For example, the literature has introduced a concept of future malware exploiting vulnerability mining techniques. It discovers vulnerabilities of hosts by performing vulnerability mining with the use of the computing resources of hosts infected with the malware . In this paper, we propose a game-theoretic approach to epidemic modeling for discussing how to counter such future malware evolution. In the proposed approach, we consider a countermeasure model that constructs a countermeasure group aiming to discover vulnerabilities earlier than malware or malicious attackers, and repair them to protect hosts not to get infected with the malware. This paper provides stochastic epidemic modeling for the countermeasure model, which represents the infection dynamics of future malware based on a continuous-time Markov chain under countermeasure environments. Furthermore, we apply evolutionary games on complex networks to the epidemic model in order to represent the selfish behavior of hosts participating in the countermeasure group. Through simulation experiments, we reveal strategies to efficiently counter the future malware evolution.},
  archive      = {J_COMCOM},
  author       = {Hideyoshi Miura and Tomotaka Kimura and Hirohisa Aman and Kouji Hirata},
  doi          = {10.1016/j.comcom.2023.05.001},
  journal      = {Computer Communications},
  pages        = {160-171},
  shortjournal = {Comput. Commun.},
  title        = {Game-theoretic approach to epidemic modeling of countermeasures against future malware evolution},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online learning resource recommendation method based on
multi-similarity metric optimization under the COVID-19 epidemic.
<em>COMCOM</em>, <em>206</em>, 152–159. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous COVID-19 pneumonia epidemic, online learning has become a normal choice for many learners. However, the problems of information overload and knowledge maze have been aggravated in the process of online learning. A learning resource recommendation method based on multi similarity measure optimization is proposed in this paper. We optimize the user score similarity by introducing information entropy, and use particle swarm optimization algorithm to determine the comprehensive similarity weight, and determine the nearest neighbor user with both score similarity and interest similarity through secondary screening in this method. The ultimate goal is to improve the accuracy of recommendation results, and help learners learn more effectively. We conduct experiments on public data sets. The experimental results show that the algorithm in this paper can significantly improve the recommendation accuracy on the basis of maintaining a stable recommendation coverage.},
  archive      = {J_COMCOM},
  author       = {Jia Wang and Shuhao Jiang and Jincheng Ding},
  doi          = {10.1016/j.comcom.2023.04.024},
  journal      = {Computer Communications},
  pages        = {152-159},
  shortjournal = {Comput. Commun.},
  title        = {Online learning resource recommendation method based on multi-similarity metric optimization under the COVID-19 epidemic},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trusted and privacy-preserving sensor data onloading.
<em>COMCOM</em>, <em>206</em>, 133–151. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To personalize their services (e.g., advertisement, navigation, healthcare), mobile apps collect sensor data . Typically, they upload the collected sensor data to the cloud, which returns the inferred user profiles required to personalize mobile services . However, privacy concerns and network connectivity/congestion issues can render cloud-based processing inapplicable. If different apps collect the same type of sensor data, app providers can collaborate by combining their data collections to infer on-device the user profiles required for personalization. Although major mobile platforms provide on-device data sharing mechanisms, these direct data exchanges provide no privacy protection. As an alternative to direct data sharing, we present differentially privatized sensor data onloading for app providers’ collaboration. With our approach, app providers can safely collaborate by using shared sensor data to personalize their mobile services . We realize our approach as a middleware that acts as a trusted intermediary. The middleware aggregates the sensor data contributed by individual apps, which execute statistical queries against the combined datasets. Furthermore, the middleware’s adaptive privacy-preserving scheme (1) computes and adds the required amount of noise to the query results so as to balance utility and privacy; (2) introduces a Trust-Data Theory so as to detect and remove spurious data from the combined collections; (3) rewards active contributing app providers so as to incentivize data contribution; (4) integrates a Trusted Execution Environment (TEE) so as to secure all data processing. Our evaluation shows that it is feasible and useful to personalize mobile services while protecting data privacy: queries’ execution time is within 10 ms; participants’ dissimilar privacy/utility requirements are satisfied; untrustworthy data are effectively detected; mobile services are personalized, and data privacy of both app providers and users are preserved. 1},
  archive      = {J_COMCOM},
  author       = {Yin Liu and Breno Dantas Cruz and Eli Tilevich},
  doi          = {10.1016/j.comcom.2023.04.027},
  journal      = {Computer Communications},
  pages        = {133-151},
  shortjournal = {Comput. Commun.},
  title        = {Trusted and privacy-preserving sensor data onloading},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-precision real-time UAV target recognition based on
improved YOLOv4. <em>COMCOM</em>, <em>206</em>, 124–132. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, unmanned aerial vehicles (UAVs) have gained widespread use in both military and civilian fields with the advancement of aviation technology and improved communication capabilities. However, the phenomenon of unauthorized UAV flights, or “black flying”, poses a serious threat to the safe flight of aircraft in airspace and public safety. To effectively interfere with and attack UAV targets, it is crucial to enhance the detection and identification of “low, slow and small” UAVs. This study focuses on achieving high-precision and lightweight detection and identification of four-rotor, six-rotor, and fixed-wing UAVs in low-altitude complex environments. By combining deep learning target detection with superresolution feature enhancement, a lightweight UAV detection model is designed and field-tested for verification. To address the challenge of detecting small UAV targets with limited information, the feature fusion network is enhanced based on the traditional YOLOv4 algorithm to improve the detection ability of small targets via small target enhancement and candidate box adjustment. The feasibility of the improved network is quantitatively and qualitatively analyzed. Channel pruning and layer pruning are then applied to the network, significantly reducing its depth and width and realizing a lightweight network. Finally, reasoning quantification is conducted on the embedded platform to enable end-side deployment of the target detection algorithm .},
  archive      = {J_COMCOM},
  author       = {Yuxing Dong and Yujie Ma and Yan Li and Zhen Li},
  doi          = {10.1016/j.comcom.2023.04.019},
  journal      = {Computer Communications},
  pages        = {124-132},
  shortjournal = {Comput. Commun.},
  title        = {High-precision real-time UAV target recognition based on improved YOLOv4},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TRMaxAlloc: Maximum task allocation using reassignment
algorithm in multi-UAV system. <em>COMCOM</em>, <em>206</em>, 110–123.
(<a href="https://doi.org/10.1016/j.comcom.2023.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The on-demand flexibility of UAVs and their line-of-sight communication capabilities have made them an effective solution to provide emergency services such as search and rescue in emergencies. The maximum task allocation to a multi-UAV system under time constraints has gained significant interest from academia and industry to enhance the quality of service requirements. To accomplish this goal, we propose the maximum task allocation (TRMaxAlloc) algorithm, which works in two phases: task assignment and reassignment. The first phase assigns the task using the performance impact (PI) algorithm. In the second phase, the assigned task is reassigned to other UAVs to create feasible time slots for the unassigned tasks, increasing the number of assigned tasks. The reassignment of tasks is based on a novel task costing mechanism, which is the main difference between the two assignment methods. The paper identified task selection during the reassignment phase as a critical parameter for maximizing tasks and proposed three methods for task selection. Following that, to evaluate the performance of the proposed TRMaxAlloc algorithm, numerical results are compared with other benchmark schemes under different task selection techniques. The simulation results confirm an improvement of up to 22.41\% in task allocation compared to the PI algorithm and 2.41\% compared to the PI-MaxAss algorithm under various constraints.},
  archive      = {J_COMCOM},
  author       = {Rahim Ali Qamar and Mubashar Sarfraz and Sajjad A. Ghauri and Asad Mahmood},
  doi          = {10.1016/j.comcom.2023.04.025},
  journal      = {Computer Communications},
  pages        = {110-123},
  shortjournal = {Comput. Commun.},
  title        = {TRMaxAlloc: Maximum task allocation using reassignment algorithm in multi-UAV system},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new federated learning-based wireless communication and
client scheduling solution for combating COVID-19. <em>COMCOM</em>,
<em>206</em>, 101–109. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a machine learning method that can break the data island. Its inherent privacy-preserving property has an important role in training medical image models. However, federated learning requires frequent communication, which incur high communication costs. Moreover, the data is heterogeneous due to different users’ preferences, which may degrade the performance of models. To address the problem of statistical heterogeneity, we propose FedUC, an algorithm to control the uploaded updates for federated learning, where a client scheduling method is made on the basis of weight divergence, update increment, and loss. We also balance the local data of the clients by image augmentation to mitigate the impact of the non-independently identically distribution. The server assigns compression thresholds to the clients based on the weight divergence and update increment of the models for gradient compression to reduce the wireless communication costs. Finally, based on the weight divergence, update increment and accuracy, the server dynamically assigns weights to the model parameters for the aggregation. Simulation and analysis utilizing a publicly available chest disease dataset containing COVID-19 are compared with existing federated learning methods. Experimental results show that our proposed strategy has better training performance in improving model accuracy and reducing wireless communication costs.},
  archive      = {J_COMCOM},
  author       = {Shuhong Chen and Zhiyong Jie and Guojun Wang and Kuan-Ching Li and Jiawei Yang and Xulang Liu},
  doi          = {10.1016/j.comcom.2023.04.023},
  journal      = {Computer Communications},
  pages        = {101-109},
  shortjournal = {Comput. Commun.},
  title        = {A new federated learning-based wireless communication and client scheduling solution for combating COVID-19},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semi-supervised sensing rate learning based CMAB scheme to
combat COVID-19 by trustful data collection in the crowd.
<em>COMCOM</em>, <em>206</em>, 85–100. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recruitment of trustworthy and high-quality workers is an important research issue for MCS. Previous studies either assume that the qualities of workers are known in advance, or assume that the platform knows the qualities of workers once it receives their collected data. In reality, to reduce costs and thus maximize revenue, many strategic workers do not perform their sensing tasks honestly and report fake data to the platform, which is called False data attacks. And it is very hard for the platform to evaluate the authenticity of the received data In this paper, an incentive mechanism named Semi-supervision based Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed to solve the recruitment problem of multiple unknown and strategic workers in MCS. First, we model the worker recruitment as a multi-armed bandit reverse auction problem and design an UCB-based algorithm to separate the exploration and exploitation, regarding the Sensing Rates (SRs) of recruited workers as the gain of the bandit Next, a Semi-supervised Sensing Rate Learning (SSRL) approach is proposed to quickly and accurately obtain the workers’ SRs, which consists of two phases, supervision and self-supervision. Last, SCMABA is designed organically combining the SRs acquisition mechanism with multi-armed bandit reverse auction, where supervised SR learning is used in the exploration, and the self-supervised one is used in the exploitation. We theoretically prove that our SCMABA achieves truthfulness and individual rationality and exhibits outstanding performances of the SCMABA mechanism through in-depth simulations of real-world data traces.},
  archive      = {J_COMCOM},
  author       = {Jianheng Tang and Kejia Fan and Wenxuan Xie and Luomin Zeng and Feijiang Han and Guosheng Huang and Tian Wang and Anfeng Liu and Shaobo Zhang},
  doi          = {10.1016/j.comcom.2023.04.030},
  journal      = {Computer Communications},
  pages        = {85-100},
  shortjournal = {Comput. Commun.},
  title        = {A semi-supervised sensing rate learning based CMAB scheme to combat COVID-19 by trustful data collection in the crowd},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometry-guided multilevel RGBD fusion for surface normal
estimation. <em>COMCOM</em>, <em>206</em>, 73–84. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developments in 3D computer vision have advanced scene understanding and 3D modeling. Surface normal estimation is a basic task in these fields. In this paper, we propose a geometry-guided multilevel fusion scheme for high-quality surface normal estimation by exploiting texture and geometry information from color and depth images. The surface normal is progressively predicted with a coarse-to-fine strategy. First, an initial surface normal (IniNormal) N i n i Nini is predicted by a hierarchical confidence reweighting convolution neural network to merge texture and geometry information in a CNN feature level. Although a general accuracy is achieved, the long tail problem makes the IniNormal always fails in special areas where the depth map is high-quality while the intensity interference is challenging, such as repeating textures and abnormal exposures. Further, a traditional geometry-consistent based surface normal(GeoNormal) N g e o Ngeo is calculated based on traditional constraints, and a surface normal level fusion module is designed to remap the depth to different representations and reconsider scene information. Then, the final clear surface normal N N is estimated by adaptively reintegrating the IniNormal and GeoNormal in a decision level. To overcome disturbances in the dataset and ensure the trainability of the network, a carefully designed hybrid objective function and an annealed term are applied. An explainable analysis is attached. The experimental results on two benchmark datasets demonstrate that the proposed GMLF(geometry-guided multilevel RGBD fusion for surface normal estimation) can achieve better quantitative and qualitative performance. The proposed method may be useful for robots and auto-driving which can be applied in the next-generation Internet-of-Things (NG-IoT).},
  archive      = {J_COMCOM},
  author       = {Yanfeng Tong and Jing Chen and Yongtian Wang},
  doi          = {10.1016/j.comcom.2023.04.014},
  journal      = {Computer Communications},
  pages        = {73-84},
  shortjournal = {Comput. Commun.},
  title        = {Geometry-guided multilevel RGBD fusion for surface normal estimation},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task recommendation method for fusion of multi-view social
relationship learning and reasoning in the mobile crowd sensing system.
<em>COMCOM</em>, <em>206</em>, 60–72. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the powerful sensing ability of mobile smart devices , sensing users can obtain crowd sensing services for intelligent devices in the Internet of Things . However, due to the low accuracy of task recommendations, the uneven quality of data provided by sensing users has always been a problem. In this paper, a novel mobile crowd sensing task recommendation framework is proposed by integrating multi-view social relationship reasoning, which aims to construct a social relation network through multi-view to ensure task recommendation accuracy and improve data quality. Firstly, the social relationship network is jointly built by location matching view, time series view, and preference view. Secondly, the sensing user preferences and candidate task representations are learned by extracting task name features, task subject features, and task category features. Finally, the key nodes are used to divide the social relationship sub-network. The preference representation of the key nodes and the candidate task representation is used to do the inner product to obtain the probability of the social relationship sub-network selection task, and then complete the task recommendation. Evaluations based on two real datasets, Gowalla and Brightkite, show that the average recommendation accuracy is about 93\%, and the participation rate of sensing users is about 97\%. At the same time, the running time is reduced by 15\% on average compared with the baseline algorithm, and the mobile cost of sensing users is reduced.},
  archive      = {J_COMCOM},
  author       = {Jian Wang and Zhe Zhang and Guosheng Zhao},
  doi          = {10.1016/j.comcom.2023.04.028},
  journal      = {Computer Communications},
  pages        = {60-72},
  shortjournal = {Comput. Commun.},
  title        = {Task recommendation method for fusion of multi-view social relationship learning and reasoning in the mobile crowd sensing system},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved differential evolution for RSSD-based localization
in gaussian mixture noise. <em>COMCOM</em>, <em>206</em>, 51–59. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signal strength-based localization approaches are prevalent in wireless sensor networks due to their low cost and simplicity. However, factors such as human interference and heterogeneous sources make approaches based on Gaussian noise and known transmit power unreliable. To address these issues, a received signal strength difference (RSSD) based approach is proposed to localize a source with unknown transmit power and Gaussian mixture noise. First, an RSSD-based nonconvex maximum likelihood (ML) problem is formulated which does not require an approximation or good initial point. Then, an improved differential evolution (IDE) method is given to obtain a global solution. Opposition-based learning (OL) combined with a chaotic map (CM) is used to obtain a robust population and adaptive mutation (AM) with two subpopulations is employed to balance global exploration and convergence. The corresponding Cramér–Rao lower bound (CRLB) for Gaussian mixture noise is derived for comparison purposes. Numerical results are presented which show that the proposed OLAM-IDE method provides better localization accuracy than state-of-the-art approaches.},
  archive      = {J_COMCOM},
  author       = {Yuanyuan Zhang and Huafeng Wu and T. Aaron Gulliver and Jiangfeng Xian and Linian Liang},
  doi          = {10.1016/j.comcom.2023.04.021},
  journal      = {Computer Communications},
  pages        = {51-59},
  shortjournal = {Comput. Commun.},
  title        = {Improved differential evolution for RSSD-based localization in gaussian mixture noise},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RLCS: Towards a robust and efficient mobile edge computing
resource scheduling and task offloading system based on graph neural
network. <em>COMCOM</em>, <em>206</em>, 38–50. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Internet of Things (IoT) keeps evolving, next-generation IoT (NG-IoT) scenarios empower various applications which require low-latency connections or high bandwidth support. Mobile edge computing (MEC) is then proposed to provide users with low-latency computing services. However, the massive and heterogeneous nature of user devices and MEC servers also brings some new challenges for resource management and task offloading . Existing works have some shortcomings because of adopting a coarse-grained task model or neglecting task graph information . The booming of artificial intelligence (AI) provides us with a more robust approach to addressing these issues. In this paper, we propose an NG-IoT user task offloading and resource scheduling architecture in the MEC scenario. We formulate our problem objective as minimizing average user task completion time (TCT). To solve the problem, we propose a Reinforcement Learning based algorithm for Container Scheduling (RLCS) and cooperating with the graph convolutional network (GCN) technique. We perform RLCS training and evaluate RLCS performance in the simulated environment. Evaluation results indicate that RLCS outperforms other baselines (e.g., reinforcement learning based algorithm, heuristic algorithm) in multiple experimental settings.},
  archive      = {J_COMCOM},
  author       = {Shu Yang and Limin Zhang and Laizhong Cui and Qingzhen Dong and Wei Xiao and Chengwen Luo},
  doi          = {10.1016/j.comcom.2023.04.020},
  journal      = {Computer Communications},
  pages        = {38-50},
  shortjournal = {Comput. Commun.},
  title        = {RLCS: Towards a robust and efficient mobile edge computing resource scheduling and task offloading system based on graph neural network},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SRAKN: Secure roaming authentication and key negotiation
protocol for space information network. <em>COMCOM</em>, <em>206</em>,
22–37. (<a href="https://doi.org/10.1016/j.comcom.2023.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the Space Information Network (SIN) with global signal coverage, large capacity, bandwidth-on-demand flexibility, and multiple services has attracted more and more users to enjoy real-time services without geographical restrictions. However, due to the openness of the satellite-to-ground wireless link , users are vulnerable to various attacks when accessing SIN and obtaining subscription services , which may pose security threats to user privacy, message integrity and confidentiality. Although many authentication protocols have been proposed to protect the security of access and data transmission, most of them are only applicable to the authentication scenarios in the home domain, without considering the security threats of roaming authentication in foreign domains. Moreover, the existing roaming authentication protocols still have some defects such as security vulnerabilities , long authentication delay, and high communication overhead . Therefore, in this paper, we propose a new secure roaming authentication and key negotiation protocol based on elliptic curve cryptography (ECC) for SIN, namely the SRAKN protocol, which also supports conditional anonymity and batch verification. In the roaming authentication phase, not only fast and low-overhead mutual authentication between the roaming user, satellite node and the foreign terrestrial control station (FTCS) is realized, but also a secure session key is jointly negotiated by the roaming user and FTCS to protect the subscription service transmission. The results of security analysis and performance comparison show that the SRAKN protocol is not only secure and resistant to various known attacks, but also has shorter roaming authentication delay, lower batch authentication overhead and communication overhead , making it more suitable for roaming users to access SIN in foreign domains.},
  archive      = {J_COMCOM},
  author       = {Junyan Guo and Ye Du and Zhichao Sun and Runfang Wu and Xuesong Wu and Le Zhang and Tianshuai Zheng},
  doi          = {10.1016/j.comcom.2023.04.011},
  journal      = {Computer Communications},
  pages        = {22-37},
  shortjournal = {Comput. Commun.},
  title        = {SRAKN: Secure roaming authentication and key negotiation protocol for space information network},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proxy smart contracts for zero trust architecture
implementation in decentralised oracle networks based applications.
<em>COMCOM</em>, <em>206</em>, 10–21. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementation of blockchain is quickly finding popularity in several real-world applications. As a result, in order to execute and process smart contracts , blockchains frequently necessitate integrating them with real-world data from various sources or employing third-party services. Through hybrid smart contracts , Decentralized Oracle Networks (DONs) make it possible for on-chain code to communicate with off-chain infrastructure and services, assisting in the realisation of this vision. Although the blockchain itself is a secure ecosystem, its reliance on external services and real-world data exposes it to security risks. These dangers reveal themselves in improper smart contract execution as a result of malevolent actors manipulating data from outside sources. Therefore, it is necessary to design a Zero Trust Architecture to provide end-to-end security for DON-based applications. The idea of Proxy Smart Contracts (PSC), which offers a one-level indirection to the execution of the actual smart contracts, is introduced in this article. Prior to the actual smart contract execution, the PSC hypothetically runs the smart contract logic and displays the result state to the interested parties for their approval. Thus, inconsistent end states can be identified and the smart contract’s actual execution can be stopped. Solidity is used in the initial implementation to show that the suggested strategy is workable. The current work is significant since it presents a potential remedy to overcome this crucial gap that can limit widespread acceptance of dApps-based use-cases in the real world.},
  archive      = {J_COMCOM},
  author       = {Ankur Gupta and Rajesh Gupta and Dhairya Jadav and Sudeep Tanwar and Neeraj Kumar and Mohammad Shabaz},
  doi          = {10.1016/j.comcom.2023.04.022},
  journal      = {Computer Communications},
  pages        = {10-21},
  shortjournal = {Comput. Commun.},
  title        = {Proxy smart contracts for zero trust architecture implementation in decentralised oracle networks based applications},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NVAS: A non-interactive verifiable federated learning
aggregation scheme for COVID-19 based on game theory. <em>COMCOM</em>,
<em>206</em>, 1–9. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continued spread of COVID-19 seriously endangers the physical and mental health of people in all countries. It is an important method to establish inter agency COVID-19 detection and prevention system based on game theory through wireless communication and artificial intelligence . Federated learning (FL) as a privacy preserving machine learning framework has received extensive attention. From the perspective of game theory, FL can be regarded as a process in which multiple participants play games against each other to maximize their own interests. This requires that the user’s data is not leaked during the training process. However, existing studies have proved that the privacy protection capability of FL is insufficient. In addition, the existing way of realizing privacy protection through multiple rounds of communication between participants increases the burden of wireless communication. To this end, this paper considers the security model of FL based on game theory, and proposes our scheme, NVAS, a non-interactive verifiable privacy-preserving FL aggregation scheme in wireless communication environments. The NVAS can protect user privacy during FL training without unnecessary interaction between participants, which can better motivate more participants to join and provide high-quality training data. Furthermore, we designed a concise and efficient verification algorithm to ensure the correctness of model aggregation. Finally, the security and feasibility of the scheme are analyzed.},
  archive      = {J_COMCOM},
  author       = {Haitao Deng and Jing Hu and Rohit Sharma and Mingsen Mo and Yongjun Ren},
  doi          = {10.1016/j.comcom.2023.04.026},
  journal      = {Computer Communications},
  pages        = {1-9},
  shortjournal = {Comput. Commun.},
  title        = {NVAS: A non-interactive verifiable federated learning aggregation scheme for COVID-19 based on game theory},
  volume       = {206},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CANET: A hierarchical CNN-attention model for network
intrusion detection. <em>COMCOM</em>, <em>205</em>, 170–181. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Intrusion Detection (NID) is an important defense strategy in modern networks to detect malicious activities in large-scale cyberspace. The current NID methods suffer from a high false positive rate, which significantly reduces the overall effectiveness of network intrusion detection systems and simultaneously increases the maintenance cost. Furthermore, the class imbalance problem associated with the intrusion detection dataset limits the detection rate for the minority classes. This paper proposes a novel hierarchical CNN-Attention network, CANET. In CANET, CNN and the Attention mechanism mingle to form a CA Block that focuses on local spatio-temporal feature extraction. The multi-layer CA Block combination can fully learn the multi-level spatio-temporal features of network attack data, which is more suitable for modern large-scale NID. Besides, for the class imbalance problem , we propose to use Equalization Loss v2 (EQL v2) to increase the minority class weight and balance the learning attention on minority classes. Extensive experiments demonstrate that CANET outperforms the state-of-the-art methods in terms of accuracy, detection rate, and false positive rate. And it efficiently improves the detection rate of minority classes. The source code for the proposed CANET models is publicly available at https://github.com/yuanshuai666/CANET .},
  archive      = {J_COMCOM},
  author       = {Keyan Ren and Shuai Yuan and Chun Zhang and Yu Shi and Zhiqing Huang},
  doi          = {10.1016/j.comcom.2023.04.018},
  journal      = {Computer Communications},
  pages        = {170-181},
  shortjournal = {Comput. Commun.},
  title        = {CANET: A hierarchical CNN-attention model for network intrusion detection},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multidomain blockchain-based intelligent routing in UAV-IoT
networks. <em>COMCOM</em>, <em>205</em>, 158–169. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The culmination of Unmanned Aerial Vehicle (UAV) and Internet of Things (IoT) is being utilized considerably in a wide industrial domain due to the development of 5G technologies. In the UAV-IoT domain, Software-defined networking (SDN) presents viable networking architecture as it decouples the application execution from the data repository. For effective deployment, the SDN network must be divided into numerous domains, each of which is handled by a controller. Moreover, the network topology is shared by all controller nodes , allowing for cross-domain path routing to be implemented. Furthermore, since the coordinates of the UAV are known, any change in the location due to UAV mobility can be detected. It will trigger the sharing of the updated routing tables in the UAV network. However, cross-domain routing necessitates a certain confidence level in controllers. Conspicuously, vulnerable controllers may exchange incorrect typologies with other controllers, causing them to inadvertently cross domain boundaries. In the current paper, a blockchain-inspired framework is proposed to provide safe routing across several applicability domains of UAV-IoT. Using the smart contract , all SDN controllers post abstract typologies to the blockchain . As a result, the blockchain provides a viable scenario for the network because of its consensus and immutability. Additionally, it is utilized to ensure routing dependability and managing global reputation in the blockchain. The proposed model achieved enhanced measures of accuracy (96.58\%), precision (96.66\%), recall (96.69\%), and F1-measure (96.61\%). The simulation results demonstrate that the proposed technique can successfully develop trust between numerous controllers and enable safe routing across different domains when compared to benchmark systems in terms of temporal efficacy, stability analysis, reliability analysis, and statistical parameters.},
  archive      = {J_COMCOM},
  author       = {Abdulaziz Aldaej and Mohammed Atiquzzaman and Tariq Ahamed Ahanger and Piyush Kumar Shukla},
  doi          = {10.1016/j.comcom.2023.04.016},
  journal      = {Computer Communications},
  pages        = {158-169},
  shortjournal = {Comput. Commun.},
  title        = {Multidomain blockchain-based intelligent routing in UAV-IoT networks},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient and reliable ultralightweight RFID
authentication scheme for healthcare systems. <em>COMCOM</em>,
<em>205</em>, 147–157. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the Internet of Communication Technologies (ICT), the Internet is becoming more popular and widely used across the world. Radio Frequency IDentification (RFID) has become a prominent technology in healthcare systems for identifying tagged objects. The RFID tags are attached to the billions of different healthcare devices or things in several associated applications. However, RFID tags’ security and privacy are regarded as the two biggest concerns. An adversary might eavesdrop, tamper, or even intercept the transmitted messages in RFID systems. Also, the privacy of the users (patients, doctors, and nurses) may breach. In past years, numerous ultralightweight RFID authentication schemes have been proposed in the healthcare sector . However, all these schemes were pointed out as insecure under several known security attacks namely, replay, impersonation, full-disclosure, and de-synchronization attacks. Keeping in view such security flaws, we present an efficient and reliable ultralightweight RFID authentication scheme (E R 2 R2 AS) for healthcare systems to enhance patients’ medication safety. The scheme employs bitwise XOR, circular left–right rotations, and our proposed ultralighweight reformation operation to achieve higher-level security. The security and privacy evaluations demonstrate that E R 2 R2 AS scheme resists several known security attacks. The performance analysis also demonstrates that it incurs lower computation and storage overhead on the RFID tags, thus making it practical to be implemented in real-time healthcare environments.},
  archive      = {J_COMCOM},
  author       = {Anand Kumar and Karan Singh and Mohd Shariq and Chhagan Lal and Mauro Conti and Ruhul Amin and Shehzad Ashraf Chaudhry},
  doi          = {10.1016/j.comcom.2023.04.013},
  journal      = {Computer Communications},
  pages        = {147-157},
  shortjournal = {Comput. Commun.},
  title        = {An efficient and reliable ultralightweight RFID authentication scheme for healthcare systems},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covert channels in blockchain and blockchain based covert
communication: Overview, state-of-the-art, and future directions.
<em>COMCOM</em>, <em>205</em>, 136–146. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional network covert channels have become insecure due to the continuous improvement of traffic analysis techniques. As an emerging technology combined with cryptographic techniques , consensus algorithms , P2P network , blockchain has features like decentralization, traceability, immutability, anonymity, transparency, and security, which makes blockchain an ideal platform for covert channel and covert communication . Benefits of blockchain for covert communication include wide access, high capacity covert channels , identity anonymity and information concealment, and robust communication channel. In the paper, we conduct a systematic analysis on covert channels in blockchain from the layer architecture of blockchain. Covert channels are present in data layer, network layer, incentive layer and contract layer, as block structure, transaction structure, cryptographic schemes, P2P network , transaction fee, and smart contract . There are also various covert channels in different layers of blockchain applications. We make a literature review on covert channels in blockchain applications and blockchain based covert communication schemes. Current researches on blockchain based covert communication mainly focus on blockchain based cryptocurrencies , including Bitcoin , Ethereum , Zcash and Monero . There are also some explorations which combine blockchain with images to achieve higher channel capacity for covert communication. Finally, open challenges and future directions on blockchain based covert communication are discussed.},
  archive      = {J_COMCOM},
  author       = {Tao Zhang and Bingyu Li and Yan Zhu and Tianxu Han and Qianhong Wu},
  doi          = {10.1016/j.comcom.2023.04.001},
  journal      = {Computer Communications},
  pages        = {136-146},
  shortjournal = {Comput. Commun.},
  title        = {Covert channels in blockchain and blockchain based covert communication: Overview, state-of-the-art, and future directions},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Securing the internet of things-enabled smart city
infrastructure using a hybrid framework. <em>COMCOM</em>, <em>205</em>,
127–135. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the Internet of Things (IoT) employs a diverse range of new technologies, it is impossible to build a single recommended design adopted as a master plan for all possible requests. Some possible IoT application areas have not been looked into yet or do not have enough information on how to approach them. This shows that more research needs to be done in this difficult area to find new and potentially big benefits for society. Although smart cities offer residents and providers of capital several advantages, there are numerous ways that breaches could compromise the safety and security of individuals. As a result, several different recommendation designs can coexist in the IoT. This research examines the effects of ethics and technology on the security of IoT-enabled systems in smart city infrastructure. Hence provides a secure IoT network architecture for smart cities combining blockchain and deep learning to safeguard privacy and credibility. This research presents a Secure Smart City Infrastructure using Blockchain and Deep Learning (SSCI-BDL) framework to ensure privacy protection and trustworthiness among IoT communication in smart cities. This framework involves the blockchain network for security management in the smart city infrastructure. This framework integrates the deep learning model with an optimization algorithm that maintains efficient resource utilization in the smart city infrastructure. The simulation results show that the system has high security of 99.5\% and the lowest latency rate of 4.1\% compared to existing models. Overall, the proposed framework’s efficiency gives the highest rate of 99.8\%.},
  archive      = {J_COMCOM},
  author       = {Achyut Shankar and Carsten Maple},
  doi          = {10.1016/j.comcom.2023.04.008},
  journal      = {Computer Communications},
  pages        = {127-135},
  shortjournal = {Comput. Commun.},
  title        = {Securing the internet of things-enabled smart city infrastructure using a hybrid framework},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-based multi-user collaborative searchable
encryption in COVID-19. <em>COMCOM</em>, <em>205</em>, 118–126. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the outbreak of COVID-19, the government has been forced to collect a large amount of detailed information about patients in order to effectively curb the epidemic of the disease, including private data of patients. Searchable encryption is an essential technology for ciphertext retrieval in cloud computing environments , and many searchable encryption schemes are based on attributes to control user‘s search permissions to protect their data privacy. The existing attribute-based searchable encryption (ABSE) scheme can only implement the situation where the search permission of one person meets the search policy and does not support users to obtain the search permission through collaboration. In this paper, we proposed a new attribute-based collaborative searchable encryption scheme in multi-user setting (ABCSE-MU), which takes the access tree as the access policy and introduces the translation nodes to implement collaborative search. The cooperation can only be reached on the translation node and the flexibility of search permission is achieved on the premise of data security. ABCSE-MU scheme solves the problem that a single user has insufficient search permissions but still needs to search, making the user’s access policy more flexible. We use random blinding to ensure the confidentiality and security of the secret key, further prove that our scheme is secure under the Decisional Bilinear Diffie–Hellman (DBDH) assumption. Security analysis further shows that the scheme can ensure the confidentiality of data under chosen-keyword attacks and resist collusion attacks.},
  archive      = {J_COMCOM},
  author       = {Fan Zhao and Changgen Peng and Dequan Xu and Yicen Liu and Kun Niu and Hanlin Tang},
  doi          = {10.1016/j.comcom.2023.04.003},
  journal      = {Computer Communications},
  pages        = {118-126},
  shortjournal = {Comput. Commun.},
  title        = {Attribute-based multi-user collaborative searchable encryption in COVID-19},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedMBC: Personalized federated learning via mutually
beneficial collaboration. <em>COMCOM</em>, <em>205</em>, 108–117. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity is a challenge of federated learning . Traditional federated learning aims to obtain a global model, but a single global model cannot meet the needs of all clients when the clients’ local data are distributed differently. To alleviate this problem, we propose a m utually b eneficial c ollaboration method for personalized fed erated learning (FedMBC), which provides each client with a personalized model by enhancing collaboration among similar clients. First, we use the task layer outputs and soft outputs of the client model to measure the similarity of the clients. Then, for each client, we adopt a dynamic aggregation method based on the similarity of clients on the server in each communication round to aggregate a model suitable for its local data distribution. That is, the aggregated model is a personalized model of the client. Furthermore, since the data heterogeneity and the different clients selected for each communication round may lead to slow convergence of the aggregated model, we adopt the aggregated model from the previous round in the local update stage of the client to accelerate the convergence of the model. Finally, we compare our method with different federated learning algorithms on various datasets in a variety of settings, and the results show that our method is superior to them in terms of test performance and communication efficiency. In particular, when the distributions of data among clients are diverse, FedMBC can improve the test accuracy by approximately 2.3\% and reduce the number of communication rounds required by up to 35\% compared with FedAvg on the CIFAR-10 dataset.},
  archive      = {J_COMCOM},
  author       = {Yanxia Gong and Xianxian Li and Li-e Wang},
  doi          = {10.1016/j.comcom.2023.04.012},
  journal      = {Computer Communications},
  pages        = {108-117},
  shortjournal = {Comput. Commun.},
  title        = {FedMBC: Personalized federated learning via mutually beneficial collaboration},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Achieving realistic cyclist behavior in SUMO using the SimRa
dataset. <em>COMCOM</em>, <em>205</em>, 97–107. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing the modal share of bicycle traffic to reduce carbon emissions, reduce urban car traffic, and to improve the health of citizens, requires a shift away from car-centric city planning. For this, traffic planners often rely on simulation tools such as SUMO which allow them to study the effects of construction changes before implementing them. Similarly, studies of vulnerable road users, here cyclists, also use such models to assess the performance of communication-based road traffic safety systems. The cyclist model in SUMO, however, is very imprecise as SUMO cyclists behave either like slow cars or fast pedestrians, thus, casting doubt on simulation results for bicycle traffic. In this paper, we analyze acceleration, deceleration, velocity, and intersection left-turn behavior of cyclists in a large dataset of real world cycle tracks. We use the results to improve the existing cyclist model in SUMO and add three more detailed cyclist models and implement them in SUMO.},
  archive      = {J_COMCOM},
  author       = {Ahmet-Serdar Karakaya and Ioan-Alexandru Stef and Konstantin Köhler and Julian Heinovski and Falko Dressler and David Bermbach},
  doi          = {10.1016/j.comcom.2023.04.015},
  journal      = {Computer Communications},
  pages        = {97-107},
  shortjournal = {Comput. Commun.},
  title        = {Achieving realistic cyclist behavior in SUMO using the SimRa dataset},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Copy-CAV: V2X-enabled wireless towing for emergency
transport. <em>COMCOM</em>, <em>205</em>, 87–96. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As smart connected vehicles become increasingly common and pave the way for the autonomous vehicles of the future, their ability to provide enhanced safety and assistance services has improved. One such service is the emergency transport of drivers in medical distress: as a positive solution of the distress is typically more likely after timely response, an autonomous vehicle could cut on emergency response times, and thus play a key role in saving the life of its driver. In this paper, we show how such an autonomous emergency transport service can be run from a wireless cellular network , and discuss the importance of having a human in the loop in order to expedite driving. We present a Monte-Carlo-based driver assessment system that the network can use when selecting the most suitable candidate to wirelessly tow an autonomous vehicle with an incapacitated driver. We show that this mechanism results in a selection policy that ensures better cohesion between the vehicles, thereby significantly improving service reliability by reducing the chances of disruptions by intervening traffic.},
  archive      = {J_COMCOM},
  author       = {Constantine Ayimba and Valerio Cislaghi and Christian Quadri and Paolo Casari and Vincenzo Mancuso},
  doi          = {10.1016/j.comcom.2023.04.009},
  journal      = {Computer Communications},
  pages        = {87-96},
  shortjournal = {Comput. Commun.},
  title        = {Copy-CAV: V2X-enabled wireless towing for emergency transport},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A secure access control scheme with batch verification for
VANETs. <em>COMCOM</em>, <em>205</em>, 79–86. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When an onboard unit (OBU) moves into the area of a roadside unit (RSU), authentication occurs. In areas with heavy traffic, e.g., intersections, an RSU may receive thousands of requests in a short period . Verifying each request separately can lead to wasted resources and cause RSU service bottlenecks. Because these requests have stringent real-time requirements, if they are not processed in time, vehicles may not be able to change their driving status and could encounter large-scale congestion or even dangerous areas. To improve efficiency, this study investigated a secure access control scheme with batch verification for vehicular ad hoc networks . The proposed method requires only two bilinear pairing operations to verify n n requests, which is the same number for a single verification. Through a performance analysis, it was confirmed that the proposed scheme has a lower computation cost and the least communication overhead among existing schemes. Furthermore, extensive simulations demonstrated that the scheme has the lowest processing delay, and as the number of batch verifications increases, its processing delay growth range also diminishes. A detailed security analysis shows that the scheme achieves collusion resistance, signature unforgeability, and replaying resistance.},
  archive      = {J_COMCOM},
  author       = {Tao Wang and Li Kang and Jiang Duan},
  doi          = {10.1016/j.comcom.2023.04.017},
  journal      = {Computer Communications},
  pages        = {79-86},
  shortjournal = {Comput. Commun.},
  title        = {A secure access control scheme with batch verification for VANETs},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Generating live commentary for marine traffic scenarios
based on multi-model learning. <em>COMCOM</em>, <em>205</em>, 69–78. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT), which allows for seamless communication and interaction with an array of smart devices, has rapidly become an indispensable part of our daily lives. Maritime transportation is one of the primary modes of transportation that heavily relies on Internet of Things (IoT) technologies to obtain crucial information such as atmospheric, oceanographic, navigation, and state data. By utilizing these IoT technologies, marine traffic scenarios can be constructed to obtain a comprehensive understanding of the traffic flow. Having a thorough understanding of marine traffic scenarios is crucial for improving the efficiency and safety of maritime transportation. Due to the time lag and data incompleteness inherent in marine traffic broadcasting and manual marine monitoring systems, it can be challenging to rely on them entirely. Meanwhile, the required resource to analyze marine traffic scenarios can be enormous. To address these challenges,we propose an approach based on multi-model learning for the automatic interpretation of marine traffic scenarios and developed an innovative dataset of multi-model marine traffic scenarios. Specifically, we have aligned maritime meteorological data with existing marine video data to create a comprehensive dataset. This dataset includes both raw data and manually created professional annotations to address the lack of benchmark datasets for multi-modal marine traffic scenarios. In addition to the dataset, we have developed a text generation model based on a multi-modal Transformer architecture. This model extracts the characteristics of multi-scale objects from visual marine frames by a parallel convolution mechanism, converts meteorological indexes into post-numeric codes, and combines the coding results of each mode through a multi-head self-attention mechanism to form fine-grained interpretation text. To train our text generation model for marine traffic scenarios, the threshold cross-entropy loss function is used to compensate the class imbalance. Our experimental results demonstrate the effectiveness of our approach, showing that our model can efficiently generate accurate and informative descriptions of maritime activity.},
  archive      = {J_COMCOM},
  author       = {Rui Zhang and Xiaojie Li and Yifan Zhuo and Kezhong Liu and Xian Zhong and Shaohua Wan},
  doi          = {10.1016/j.comcom.2023.04.007},
  journal      = {Computer Communications},
  pages        = {69-78},
  shortjournal = {Comput. Commun.},
  title        = {Generating live commentary for marine traffic scenarios based on multi-model learning},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fog-enabled private blockchain-based identity authentication
scheme for smart home. <em>COMCOM</em>, <em>205</em>, 58–68. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Internet of Things (IoT) technology continues to advance, it has found widespread application in various fields, including smart health, smart cities, and smart transportation. Among these applications, smart homes are particularly noteworthy due to their intimate connection to our daily lives. However, the use of smart devices in a home environment exposes users to various security threats, such as impersonation attacks and insider privilege attacks, as users must communicate with multiple devices through a public channel. Additionally, traditional authentication schemes that rely on trusted third-party present a single point of failure , as users and smart devices must be registered and authenticated by a central authority. Blockchain technology offers a decentralized, tamper-proof, and flexible solution for authentication and access control of data. By using blockchain, the single point of failure problem in traditional authentication schemes can be resolved. In the context of smart homes, the real-time nature of the environment necessitates the use of fog nodes to provide localized computing services. Fog nodes are closer to IoT devices than cloud nodes, making fog computing more efficient and faster than cloud computing . This paper proposes an authentication scheme for blockchain-enabled fog nodes in smart homes. The scheme involves registering all fog nodes and intelligent devices on a local private blockchain, and authentication is performed jointly by smart contracts on the blockchain and off-chain operations. The scheme provides comprehensive security and better performance, as demonstrated by security analysis and performance evaluation. Moreover, the proposed scheme offers a certain level of user privacy protection.},
  archive      = {J_COMCOM},
  author       = {Xianbin Xu and Yajun Guo and Yimin Guo},
  doi          = {10.1016/j.comcom.2023.04.005},
  journal      = {Computer Communications},
  pages        = {58-68},
  shortjournal = {Comput. Commun.},
  title        = {Fog-enabled private blockchain-based identity authentication scheme for smart home},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial intelligence of things at the edge: Scalable and
efficient distributed learning for massive scenarios. <em>COMCOM</em>,
<em>205</em>, 45–57. (<a
href="https://doi.org/10.1016/j.comcom.2023.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a distributed optimization method in which multiple client nodes collaborate to train a machine learning model without sharing data with a central server. However, communication between numerous clients and the central aggregation server to share model parameters can cause several problems, including latency and network congestion . To address these issues, we propose a scalable communication infrastructure based on Information-Centric Networking built and tested on Apache Kafka®. The proposed architecture consists of a two-tier communication model. In the first layer, client updates are cached at the edge between clients and the server, while in the second layer, the server computes global model updates by aggregating the cached models. The data stored in the intermediate nodes at the edge enables reliable and effective data transmission and solves the problem of intermittent connectivity of mobile nodes. While many local model updates provided by clients can result in a more accurate global model in FL, they can also result in massive data traffic that negatively impacts congestion at the edge. For this reason, we couple a client selection procedure based on a congestion control mechanism at the edge for the given architecture of FL. The proposed algorithm selects a subset of clients based on their resources through a time-based backoff system to account for the time-averaged accuracy of FL while limiting the traffic load. Experiments show that our proposed architecture has an improvement of over 40\% over the network-centric based FL architecture, i.e., Flower. The architecture also provides scalability and reliability in the case of mobile nodes. It also improves client resource utilization, avoids overflow, and ensures fairness in client selection. The experiments show that the proposed algorithm leads to the desired client selection patterns and is adaptable to changing network environments.},
  archive      = {J_COMCOM},
  author       = {Saira Bano and Nicola Tonellotto and Pietro Cassarà and Alberto Gotta},
  doi          = {10.1016/j.comcom.2023.04.010},
  journal      = {Computer Communications},
  pages        = {45-57},
  shortjournal = {Comput. Commun.},
  title        = {Artificial intelligence of things at the edge: Scalable and efficient distributed learning for massive scenarios},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent reinforcement learning enabled link scheduling
for next generation internet of things. <em>COMCOM</em>, <em>205</em>,
35–44. (<a href="https://doi.org/10.1016/j.comcom.2023.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In next generation Internet of Things (NG-IoT) networks, numerous pieces of information are aggregated from the user devices and sensor nodes to the local computing units for further computing to support high-level applications. Those multitudinous transmission demands have raised new challenges for current link scheduling protocols. The centralized link scheduling protocols are inappropriate in some large-scale NG-IoT scenarios. The previously distributed link scheduling uses the randomized transmission scheme to avoid interference, making it hard to utilize the bandwidth resources fully. The multi-agent machine learning (MAML) technique is a potential approach to finding the most optimal link scheduling strategy. At the same time, the over-large state space will take a long time for them to approach the optimal solution, which reduces the practicality of the MAML . To fully utilize the bandwidth resource and improve the efficiency of link scheduling, this paper studies a multi-agent reinforcement learning enabled link scheduling problem. Different from the conventional MAML techniques that randomly select a state to do their exploration, in our multi-agent reinforcement learning algorithm, a good state is firstly obtained within polynomial time steps by executing a distributed and randomized sub-algorithm. We say a state is good if it is not far from the optimal state. Then, our multi-agent reinforcement learning scheme starts from the good state and does its exploration with an ɛ ɛ ɛ greedy scheme, which significantly reduces the time steps to get close to the optimal link scheduling strategy. Extensive simulations are conducted to investigate the performance of our work.},
  archive      = {J_COMCOM},
  author       = {Yifei Zou and Haofei Yin and Yanwei Zheng and Falko Dressler},
  doi          = {10.1016/j.comcom.2023.04.006},
  journal      = {Computer Communications},
  pages        = {35-44},
  shortjournal = {Comput. Commun.},
  title        = {Multi-agent reinforcement learning enabled link scheduling for next generation internet of things},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint task assignment and resource allocation in VFC based
on mobility prediction information. <em>COMCOM</em>, <em>205</em>,
24–34. (<a href="https://doi.org/10.1016/j.comcom.2023.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Fog Computing (VFC) is a novel architecture that utilizes end-user clients or near-user edge devices to reduce the processing and transmission time of vehicular tasks. However, the high mobility of vehicles in VFC presents a challenge in coordinating vehicles and managing multi-dimensional resources. In this paper, we investigate the joint task assignment and resource allocation optimization problem by incorporating mobility prediction information. We propose two schemes that consider execution speed and accuracy. The first scheme decomposes the original problem into two sub-problems: subtask assignment and bandwidth resource allocation, solved by matching and convex optimization methods. The second scheme uses an iterative optimization process based on 0–1 integer linear programming . Simulation results based on real-world mobility datasets demonstrate the effectiveness and complementarity of the proposed schemes.},
  archive      = {J_COMCOM},
  author       = {Xianjing Wu and Shengjie Zhao and Hao Deng},
  doi          = {10.1016/j.comcom.2023.04.004},
  journal      = {Computer Communications},
  pages        = {24-34},
  shortjournal = {Comput. Commun.},
  title        = {Joint task assignment and resource allocation in VFC based on mobility prediction information},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DRL: Dynamic rebalance learning for adversarial robustness
of UAV with long-tailed distribution. <em>COMCOM</em>, <em>205</em>,
14–23. (<a href="https://doi.org/10.1016/j.comcom.2023.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial robustness has attracted extensive studies in various fields by increasing the interpretability of deep learning and enhancing the understanding of neural network models . In realistic scenarios such as UAV control system, imbalanced datasets are a consensus. Therefore, how to solve the adversarial robustness on imbalanced datasets is a more and more inescapable problem in UAV control system. There have been some works on adversarial robustness on imbalanced datasets, which bring us a deeper understanding of vulnerability of deep neural networks and the generation of adversarial examples . To adjust the classification plane after training, the long-tailed robustness framework is usually designed to be multi-stage, and different classifiers are used in different stages, which can improve the robustness through multi learning. The existing methods are hardly considered to effectively handle the long-tailed robustness problem in UAV control system. To explore the intrinsic features of long-tailed robustness, we propose a one-stage robustness framework. First, we study different classifiers and propose a general cosine classifier. By changing the general cosine classifier adaptively, the model obtains a more robust classification. Then, we analyze the scalability of the focal loss and design a focal-margin loss. Finally, we design a category focus mobile learning strategy, obtained more robust features by changing the learning emphasis with this strategy. From this, we design a simple and efficient one-stage dynamic adversarial robustness method DRL under long-tailed distribution, which consists of an adaptive cosine classifier and a focal-margin loss under long-tailed mobile learning. The extended experiments demonstrate the superiority of our approach over other state-of-the-art methods, and the effectiveness of the designed module. This method can effectively solve the long-tailed robustness problem on UAV control system and other terminals.},
  archive      = {J_COMCOM},
  author       = {Yancheng Sun and Yuling Chen and Peng Wu and Xuewei Wang and Qi Wang},
  doi          = {10.1016/j.comcom.2023.04.002},
  journal      = {Computer Communications},
  pages        = {14-23},
  shortjournal = {Comput. Commun.},
  title        = {DRL: Dynamic rebalance learning for adversarial robustness of UAV with long-tailed distribution},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wi-fi localization obfuscation: An implementation in
openwifi. <em>COMCOM</em>, <em>205</em>, 1–13. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi sensing as a side-effect of communications is opening new opportunities for smart services integrating communications with environmental properties, first and foremost the position of devices and people. At the same time, this technology represents an unprecedented threat to people’s privacy, as personal information can be collected directly at the physical layer without any possibility to hide or protect it. Several works already discussed the possibility of safeguarding users’ privacy without hampering communication performance, using signal pre-processing at the transmitter side to introduce pseudo-random (artificial) patterns in the channel response estimated at the receiver, preventing the extraction of meaningful information from the channel state, a process called obfuscation . One step beyond the proof-of-concept for obfuscation feasibility, is its implementation in working systems. In this work, we present the implementation of a location obfuscation technique within the openwifi project that enables fine manipulation of the radio signal at transmitter side and yields acceptable, if not good, performance, the system has been implemented for both 802.11a/g/h and 802.11n systems, including MPDU aggregation, while implementation for 802.11ac or ax is still not feasible because openwifi does not support 40 MHz channelization and beyond. This contribution discusses the implementation of the obfuscation subsystem, its performance, possible improvements, and further steps to allow authorized devices to “de-obfuscate” the signal and retrieve the sensed information.},
  archive      = {J_COMCOM},
  author       = {Lorenzo Ghiro and Marco Cominelli and Francesco Gringoli and Renato Lo Cigno},
  doi          = {10.1016/j.comcom.2023.03.026},
  journal      = {Computer Communications},
  pages        = {1-13},
  shortjournal = {Comput. Commun.},
  title        = {Wi-fi localization obfuscation: An implementation in openwifi},
  volume       = {205},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A blockchain-based data-driven fault-tolerant control system
for smart factories in industry 4.0. <em>COMCOM</em>, <em>204</em>,
158–171. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern technologies and data-driven approaches have enabled fault-tolerant controllers in Industry 4.0 smart factories to detect, identify, and mitigate anomalies in real-time with a high level of accuracy. However, this has also presented new challenges and requirements for cybersecurity, data analytics, and computational complexity for Data-Driven Fault-Tolerant Controllers (DD-FTC) in smart factories. To address these issues, a Blockchain-Based Data-Driven Fault-Tolerant Control (BB-DD-FTC) framework for smart factories is proposed in this paper. Blockchain ensures the integrity of data logs via its immutable ledger and decentralized architecture. Moreover, the blockchain smart contract functionality, embedded with a Data-Driven Intrusion Detection System (DD-IDS), and reconfiguration conditions, realizes DD-FTC and undertakes the mitigation response in case of cyber-attacks. The DD-IDS mechanism utilizes the principal component analysis technique and observer models, trained via neural networks , to detect an attack and identify the compromised component. The Tennessee Eastman (TE) industrial benchmark process is considered a case study to investigate the performance of the proposed framework. Two kinds of integrity attacks are applied to the sensors of the TE process with simulation results demonstrating the effectiveness of the method in mitigating the adversarial effect of the applied attacks on the overall system performance. As feedback delays can negatively impact performance, a detailed delay analysis is performed using network calculus . The security advantages and limitations of the proposed method are finally highlighted in the performed security analysis. The results are encouraging for the wider adoption of the control-over-the-blockchain concept.},
  archive      = {J_COMCOM},
  author       = {Abdullah Bin Masood and Ammar Hasan and Vasos Vassiliou and Marios Lestas},
  doi          = {10.1016/j.comcom.2023.03.017},
  journal      = {Computer Communications},
  pages        = {158-171},
  shortjournal = {Comput. Commun.},
  title        = {A blockchain-based data-driven fault-tolerant control system for smart factories in industry 4.0},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PTF: Popularity-topology-freshness-based caching strategy
for ICN-IoT networks. <em>COMCOM</em>, <em>204</em>, 147–157. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Information-Centric Networking (ICN) has been widely considered as an ideal architecture for the Internet of Things (IoT) due to the fact that ICN’s in-network caching can enhance the efficiency of content delivery and friendly to resource-constrained IoT devices. Meanwhile, on the whole, the major factors considered by current ICN-IoT caching schemes involve content popularity, network topology and content freshness. However, current methods mainly focus on some of them, lack of overall consideration. To this end, we propose a comprehensive solution by considering all three factors. Specifically, we define cache benefit by considering content popularity, network topology and content freshness simultaneously, and let routers cache contents in order of their cache benefits. We also devise a prediction method to make routers obtain cache benefits of new contents early. Extensive evaluations reveal that PTF can effectively improve the response latency , average hop reduction and cache hit ratio, compared with other representative strategies.},
  archive      = {J_COMCOM},
  author       = {Haibo Wu and Yaogong Xu and Jun Li},
  doi          = {10.1016/j.comcom.2023.03.023},
  journal      = {Computer Communications},
  pages        = {147-157},
  shortjournal = {Comput. Commun.},
  title        = {PTF: Popularity-topology-freshness-based caching strategy for ICN-IoT networks},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint on-demand data gathering and recharging by multiple
mobile vehicles in delay sensitive WRSN using variable length GA.
<em>COMCOM</em>, <em>204</em>, 130–146. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous state-of-the-art works on joint data gathering and charging ( D G A C DGAC ) believe that the mobile vehicle ( M V MV ) has sufficient energy to charge the sensors and collect data from them by travelling across the path. The existing works also considered that the sensors always get full charge from M V MV which produces negligible dead period and minimum data gathering delay in Wireless Rechargeable Sensor Network ( W R S N WRSN ). However, the aforementioned considerations are not always practical in large-scale W R S N WRSN . In this article, we introduce an efficient on-demand partial D G A C DGAC scheme using battery limited multiple M V s MVs . The proposed scheme executes the joint D G A C DGAC process such that the total dead periods of the sensors and the overall data gathering delay is reduced. We use multi objective-based genetic algorithm ( G A GA ) to optimize the entire D G A C DGAC process. The simulation is performed to demonstrate the usefulness and competitiveness of the proposed scheme. We also verify the statistical significance of the simulation results using ANalysis Of VAriance ( A N O V A ANOVA ). The comparative analysis reveals that the proposed work improves the D G A C DGAC efficiency by reducing the dead period of sensors, data gathering delay, and energy consumption of M V s MVs in comparison with the existing works.},
  archive      = {J_COMCOM},
  author       = {Rupayan Das and Dinesh Dash},
  doi          = {10.1016/j.comcom.2023.03.022},
  journal      = {Computer Communications},
  pages        = {130-146},
  shortjournal = {Comput. Commun.},
  title        = {Joint on-demand data gathering and recharging by multiple mobile vehicles in delay sensitive WRSN using variable length GA},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wi-fi device identification based on multi-domain physical
layer fingerprint. <em>COMCOM</em>, <em>204</em>, 118–129. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical layer fingerprinting is a promising solution for improving the security of Wi-Fi device in Internet of Things (IoT) scenarios, as well as enhancing the usability of Wi-Fi-based applications such as user tracking, accountability, and computer forensics . The existing methods typically require the use of expensive signal analyzers to extract features, which is not conducive to practical engineering implementation and needs improvements in accuracy and speed for wireless devices with complex physical layer modulation. We propose a device fingerprinting technique that utilizes various multi-domain features extracted from the physical layer preamble. These features differ from the signal parts or feature types mostly used in previous works. Furthermore, the robustness of the selected features is evaluated in this paper. Using random forest model to identify the fingerprints, the experimental results show that the accuracy of the proposed scheme can reach 98\% for 15 different types of IoT Wi-Fi devices, and 90.76\% for 10 network cards with the same type of chips.},
  archive      = {J_COMCOM},
  author       = {Jinghui Zhang and Zhengjia Xu and Junhe Li and Qiangsheng Dai and Zhen Ling and Ming Yang},
  doi          = {10.1016/j.comcom.2023.03.024},
  journal      = {Computer Communications},
  pages        = {118-129},
  shortjournal = {Comput. Commun.},
  title        = {Wi-fi device identification based on multi-domain physical layer fingerprint},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TraceGra: A trace-based anomaly detection for microservice
using graph deep learning. <em>COMCOM</em>, <em>204</em>, 109–117. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trace is widely used to detect anomalies in distributed microservice systems because of the capability of precisely reconstructing user request paths. However, most existing trace-based anomaly detection approaches treat the trace as a sequence of microservice invocations with response time information, which ignores the graph structure of trace and abnormal resource consumption of the complex distributed deployment environment of microservice. In this paper, we propose TraceGra, an unsupervised encoder–decoder anomaly detection approach. TraceGra first provides a unified graph representation to combine traces and performance metrics of the container. Then, it introduces the graph neural network (GNN) and long short-term memory network (LSTM) to extract the topology and temporal features, respectively. Finally, it adds the two-part loss value with two hyperparameters as the anomaly score. The evaluation results on an open-source dataset and a local dataset collected from an ARM server cluster show that TraceGra achieves a high precision (0.97) and recall (0.93), outperforming some state-of-the-art approaches with an average increase of 0.1 in F1-score.},
  archive      = {J_COMCOM},
  author       = {Jian Chen and Fagui Liu and Jun Jiang and Guoxiang Zhong and Dishi Xu and Zhuanglun Tan and Shangsong Shi},
  doi          = {10.1016/j.comcom.2023.03.028},
  journal      = {Computer Communications},
  pages        = {109-117},
  shortjournal = {Comput. Commun.},
  title        = {TraceGra: A trace-based anomaly detection for microservice using graph deep learning},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive global coordination of local routing policies for
communication networks. <em>COMCOM</em>, <em>204</em>, 101–108. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider optimal routing of data packets in communication networks featuring time-variable flow rates and bandwidth limitations. Taking into account recent programmability developments in communication systems , we propose a two-level control scheme: routers with a programmable data plane implement local proportional control policies that forward the incoming data to different available output interfaces at line rate. The local controllers’ parameters are adapted periodically on a slower time scale by a logically centralized (software-defined) network controller running a global coordination algorithm that keeps the routing feasible and optimal with respect to a network metric, such as the average packet delay . A robust optimization approach is selected to handle traffic variations in-between global adaptation steps. The outcome is a non-convex Quadratically Constrained Quadratic Program (QCQP), for which we present an iterative solution approach that is computationally suitable for realistically-sized backbone communication networks. With simulation experiments, we demonstrate the advantages of adaptive, global routing coordination compared to fixed, globally or locally-determined policies, especially concerning packet loss .},
  archive      = {J_COMCOM},
  author       = {Allan Santos and Amr Rizk and Florian Steinke},
  doi          = {10.1016/j.comcom.2023.03.027},
  journal      = {Computer Communications},
  pages        = {101-108},
  shortjournal = {Comput. Commun.},
  title        = {Adaptive global coordination of local routing policies for communication networks},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-LightGCN: Dual light graph convolutional network for
discriminative recommendation. <em>COMCOM</em>, <em>204</em>, 89–100.
(<a href="https://doi.org/10.1016/j.comcom.2023.03.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, graph neural networks have played a very important role in graph data analysis, and the application of graph convolutional networks (GCN) to recommender systems has been extensively investigated by recent studies. GCNs also recently demonstrated their potential to be analyzed from the point of view of Explainable Artificial Intelligence because of their underlying structure. However, most of the existing GCN-based methods are aggregated of information in one scale space and did not consider aggregation of information in multi-scale space. On this basis, this paper proposes an innovative dual light graph convolutional network model called Dual-LightGCN, which explicitly filters out items disliked by users to ensure more discriminative recommendation. Particularly, our model divides the original user–item interaction graph into two bipartite subgraphs, one subgraph is used to model the preferences between users and items, while the other is used to model the dislike relationships between them. For these two subgraphs, the LightGCN model recommendation is performed on them respectively. In the Movielens-1M dataset, the F1-score in Dual-LightGCN has increased by an average of 26\%. We conducted a comprehensive evaluation of the proposed method on two datasets of different sizes and compared it with several state-of-the-art recommendation algorithms , and the results showed that the accuracy and F1-score results were significantly higher than those of other recommendation algorithms . The significantly low computational time required makes the proposed method suitable for successful deployment in various IoT scenarios.},
  archive      = {J_COMCOM},
  author       = {Wenqing Huang and Fei Hao and Jiaxing Shang and Wangyang Yu and Shengke Zeng and Carmen Bisogni and Vincenzo Loia},
  doi          = {10.1016/j.comcom.2023.03.018},
  journal      = {Computer Communications},
  pages        = {89-100},
  shortjournal = {Comput. Commun.},
  title        = {Dual-LightGCN: Dual light graph convolutional network for discriminative recommendation},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survivable multipath resource allocation strategy based on
fragmentation-sensitive fragmentation-aware in space division
multiplexing elastic optical networks. <em>COMCOM</em>, <em>204</em>,
78–88. (<a href="https://doi.org/10.1016/j.comcom.2023.03.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The space division multiplexing elastic optical network (SDM-EONs) can effectively supply the large-capacity for the current optical network . If a link failure occurs, it will bring huge data losses or disruptions for the operator. We propose a survivable multipath fragmentation-sensitive fragmentation-aware routing, core and spectrum assignment (SM-FSFA-RSCA) scheme to solving the problem of survivability of the request in SDM-EONs and spectrum fragmentation that caused by the dynamic allocation and release of spectrum resources. We design a fragmentation-sensitive fragmentation metric (FSFM) to estimate the spectrum fragmentation in the established lightpaths , and design a dynamic routing and core selection strategy for the request to control spectrum fragmentation in the core according to FSFM. We consider survivable multipath routing strategy for saving spectrum resources, each path of the multipath can be used as a working path or a protection path. In addition, for spectrum allocation, a survivable multipath with shared spectrum allocation strategy is adopted to maximize the utilization of frequency resources. The simulation results show that the algorithm proposed in this paper significantly reduces the bandwidth blocking probability ,fragmentation ratio and also improves the spectrum utilization.},
  archive      = {J_COMCOM},
  author       = {Huanlin Liu and Chang Tang and Yong Chen and Mingming Tan and Yan Qiu and Haonan Chen and Junling Hu},
  doi          = {10.1016/j.comcom.2023.03.025},
  journal      = {Computer Communications},
  pages        = {78-88},
  shortjournal = {Comput. Commun.},
  title        = {A survivable multipath resource allocation strategy based on fragmentation-sensitive fragmentation-aware in space division multiplexing elastic optical networks},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reliable and energy-efficient UAV-assisted air-to-ground
transmission: Design, modeling and analysis. <em>COMCOM</em>,
<em>204</em>, 66–77. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel reliable and energy-efficient transmission scheme is proposed for unmanned aerial vehicle (UAV) assisted air-to-ground (A2G) communication. The scheme is tailored to handle burst errors in transmission caused by network collision, switching or interference. Application Layer Forward Error Correction (AL-FEC) mechanism is proposed to enhance the reliability of the A2G communication. By avoiding retransmissions , energy efficiency can be guaranteed. Based on Gilbert-Elliott channel model, we perform a detailed mathematical analysis on the performance of the proposed AL-FEC scheme. The packet delivery ratio and energy efficiency are studied and validated by comprehensive simulations. An application layer Automatic Repeat reQuest (AL-ARQ) protocol based on selective retransmission is used as a baseline scheme and compared with the proposed scheme. It is shown that the proposed AL-FEC scheme can significantly reduce the energy consumption and ensure the same or even higher application packet delivery ratio compared with the baseline scheme. In addition, the proposed scheme is especially preferable when the channel quality is poor. These specific findings can provide guidance for practical applications relying on UAV-assisted A2G communication, which require reliable and energy-efficient UAV-to-ground transmission in harsh wireless conditions.},
  archive      = {J_COMCOM},
  author       = {Xiaomin Chen and Qinbin Zhou and Zhiheng Wang and Qiang Sun and Miaomiao Xu},
  doi          = {10.1016/j.comcom.2023.03.019},
  journal      = {Computer Communications},
  pages        = {66-77},
  shortjournal = {Comput. Commun.},
  title        = {Reliable and energy-efficient UAV-assisted air-to-ground transmission: Design, modeling and analysis},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-connectivity in 5G new radio: Optimal resource
allocation for split bearer and data duplication. <em>COMCOM</em>,
<em>204</em>, 52–65. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile radio networks have been evolving towards the integration of services and devices with a diverse set of throughput, latency, and reliability requirements. To support these requirements, 3GPP has introduced Multi Connectivity (MC) as a more flexible architecture for 5G New Radio (NR), where multiple radio links can be simultaneously activated to split or duplicate data traffic. Multi connectivity improves single user performance at the cost of higher interference due to the increase of radio transmissions, which negatively affects system throughput. This paper analyzes the problem of admission control and resource allocation in multi connectivity scenarios, considering different requirements and 5G NR features. Specifically, we formulate two optimization problems that leverage the features of the Packet Data Convergence Protocol (PDCP) layer, which controls the flow of data packets of the data radio bearer : the PDCP Split-Bearer Decision (PSD) and the PDCP Duplication Decision (PDD) problems, which are tailored for the enhanced Mobile Broadband (eMBB) and Ultra Reliable Low Latency Communications (uRLLC) services, respectively. We further provide heuristic approaches, specifically designed for the PSD and PDD problems, to effectively solve both these problems. Numerical results in realistic network deployments confirm that our solutions can effectively allocate radio resources increasing admission rate and system throughput, while guaranteeing the required reliability level.},
  archive      = {J_COMCOM},
  author       = {Jocelyne Elias and Fabio Martignon and Stefano Paris},
  doi          = {10.1016/j.comcom.2023.03.021},
  journal      = {Computer Communications},
  pages        = {52-65},
  shortjournal = {Comput. Commun.},
  title        = {Multi-connectivity in 5G new radio: Optimal resource allocation for split bearer and data duplication},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantum fingerprinting for heterogeneous devices
localization. <em>COMCOM</em>, <em>204</em>, 43–51. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present QHFP , a device-independent quantum fingerprint matching algorithm that addresses two of the issues for realizing worldwide ubiquitous large-scale location tracking systems: scalability of storage space and running time as well as devices heterogeneity. In particular, we present a quantum algorithm with a complexity that is exponentially better than the classical techniques, both in space and running time. QHFP also has provisions for handling the inherent localization error due to building the large-scale fingerprint using heterogeneous devices. We give the details of the entire system starting from extracting device-independent features from the raw received signal strength , mapping the classical feature vectors to their quantum counterparts, and showing a quantum cosine similarity algorithm for fingerprint matching. We implement our quantum algorithm and deploy it in a real testbed using the IBM Quantum machine simulator. We also study the effect of the different quantum error sources, such as quantum gates error and quantum measurement error, on the system accuracy. Based on this, we discuss the suitable provisions to mitigate the effect of the quantum error sources on the localization accuracy. Results confirm the ability of QHFP to obtain the correct estimated location with an exponential improvement in space and running time compared to the traditional classical counterparts. In addition, the proposed device-independent features lead to more than 20\% better accuracy in median error. This highlights the promise of our algorithm for future ubiquitous large-scale worldwide device-independent fingerprinting localization systems.},
  archive      = {J_COMCOM},
  author       = {Ahmed Shokry and Moustafa Youssef},
  doi          = {10.1016/j.comcom.2023.03.010},
  journal      = {Computer Communications},
  pages        = {43-51},
  shortjournal = {Comput. Commun.},
  title        = {Quantum fingerprinting for heterogeneous devices localization},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federal learning edge network based sentiment analysis
combating global COVID-19. <em>COMCOM</em>, <em>204</em>, 33–42. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the important research topics in the field of natural language processing , sentiment analysis aims to analyze web data related to COVID-19, e.g., supporting China government agencies combating COVID-19. There are popular sentiment analysis models based on deep learning techniques, but their performance is limited by the size and distribution of the dataset. In this study, we propose a model based on a federal learning framework with Bert and multi-scale convolutional neural network (Fed_BERT_MSCNN), which contains a Bidirectional Encoder Representations from Transformer modules and a multi-scale convolution layer . The federal learning framework contains a central server and local deep learning machines that train local datasets. Parameter communications were processed through edge networks. The weighted average of each participant’s model parameters was communicated in the edge network for final utilization. The proposed federal network not only solves the problem of insufficient data, but also ensures the data privacy of the social platform during the training process and improve the communication efficiency. In the experiment, we used datasets of six social platforms, and used accuracy and F1-score as evaluation criteria to conduct comparative studies. The performance of the proposed Fed_BERT_MSCNN model was generally superior than the existing models in the literature.},
  archive      = {J_COMCOM},
  author       = {Wei Liang and Xiaohong Chen and Suzhen Huang and Guanghao Xiong and Ke Yan and Xiaokang Zhou},
  doi          = {10.1016/j.comcom.2023.03.009},
  journal      = {Computer Communications},
  pages        = {33-42},
  shortjournal = {Comput. Commun.},
  title        = {Federal learning edge network based sentiment analysis combating global COVID-19},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GooseBt: A programmable malware detection framework based on
process, file, registry, and COM monitoring. <em>COMCOM</em>,
<em>204</em>, 24–32. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the networks is becoming faster and more convenient, computers is communicating with each other more and more frequent. However, security of computer communication is required to be concerned. Thus, this paper introduces a new programmable malware detection framework under Windows platform named GooseBt based on process, file, registry, and COM monitoring, which provides a new thought to perform malware detection. Usually, it is quite difficult to design with various kernel drivers on different platform versions, which are required to be developed before malware detection methods could be addressed. Moreover, most kinds of the existing anti-virus software are not programmable. In GooseBt framework, users can directly use easy codes to design their malware detection methods and make it run in Windows kernel mode . Static rules and dynamic rules are provided as interfaces based on message mapping. Moreover, third-party anti-virus manufacturers can directly call the API of the framework. The framework is proved to be effective and stable.},
  archive      = {J_COMCOM},
  author       = {Yuer Yang and Yifeng Lin and Zhiying Li and Liangtian Zhao and Mengting Yao and Yixi Lai and Peiya Li},
  doi          = {10.1016/j.comcom.2023.03.011},
  journal      = {Computer Communications},
  pages        = {24-32},
  shortjournal = {Comput. Commun.},
  title        = {GooseBt: A programmable malware detection framework based on process, file, registry, and COM monitoring},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient graph-based gateway placement for large-scale
LoRaWAN deployments. <em>COMCOM</em>, <em>204</em>, 11–23. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Power Wide Area Networks with Long Range Wide Area Networks (LoRaWANs) as one of their most prominent representatives are promising solutions for future Internet of Things deployments. The technology is characterized by low energy requirements leading to long battery lifetimes. However, the drawbacks are limited throughput rates and the unreliable nature of the LoRa technology. Specifically its random channel access approach leads to significant message collisions and thus, data loss in larger deployments. From a network planning point of view, the reduction of potentially colliding messages at the frequency bands in combination with limiting the transmission duration of messages is a promising approach. For that reason, we present a novel graph-based gateway placement approach. We show that our approach performs similar to state-of-the-art related work in the worst case and reduces the required number of gateways by up to 40\% while reducing the collision probability by up to 70\%. Furthermore, we overcome the challenge of scalability of our approach when placing gateways in very large networks. By splitting the problem into smaller problem instances by pre-clustering, we can solve arbitrarily large instances efficiently without a significant increase in number of required gateways in real networks.},
  archive      = {J_COMCOM},
  author       = {Frank Loh and Noah Mehling and Stefan Geißler and Tobias Hoßfeld},
  doi          = {10.1016/j.comcom.2023.03.015},
  journal      = {Computer Communications},
  pages        = {11-23},
  shortjournal = {Comput. Commun.},
  title        = {Efficient graph-based gateway placement for large-scale LoRaWAN deployments},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards perpetual sensor networks via overlapped mobile
charging. <em>COMCOM</em>, <em>204</em>, 1–10. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless power transfer has given a promising alternative to replenish battery-powered wireless sensor networks (WSNs). We can employ mobile vehicles, robots, or unmanned aerial vehicle as mobile chargers to wirelessly charge sensor nodes . Most of previous works either assume that a mobile charger can charge only one sensor node at a time, or optimize for charging delay, radiation safety, etc. In this paper, we consider a fundamental problem: given one mobile charger that can charge multiple sensor nodes simultaneously, how we can schedule it to charge a given WSN to maximize the energy usage effectiveness (EUE)? We propose a novel charging paradigm, Overlapped Mobile Charging (OMC), the first of its kind to the best of our knowledge. Firstly, OMC clusters sensor nodes into multiple non-overlapped sets using k-means evaluated by the Davies–Bouldin Index, such that the sensor nodes in each set have similar recharging cycles. Secondly, for each set of sensor nodes, OMC further divides them into multiple overlapped groups, and charges each group at different locations for different time durations to make sure that each overlapped sensor node just receives its required energy from multiple charging locations. Simulations confirm the advantages of OMC in terms of EUE.},
  archive      = {J_COMCOM},
  author       = {Yu Liang and Yang Lu and Mingjun Shi},
  doi          = {10.1016/j.comcom.2023.03.020},
  journal      = {Computer Communications},
  pages        = {1-10},
  shortjournal = {Comput. Commun.},
  title        = {Towards perpetual sensor networks via overlapped mobile charging},
  volume       = {204},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint resource optimization and trajectory design for energy
minimization in UAV-assisted mobile-edge computing systems.
<em>COMCOM</em>, <em>203</em>, 312–323. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies an unmanned aerial vehicle (UAV)-assisted mobile-edge computing (MEC) system, where a rotary-wing UAV equipped with computing platforms is used to assist ground users (GUs) with insufficient computing resource. Each GU offloads part of the task to the UAV for computation through the uplink, and the remaining computation task is computed by itself. Then, UAV will return the results to the corresponding GU via downlink. Considering the results for each GU is not only used by itself, but may also be sent to other GUs for data fusion, we thus design two specific scenarios in this paper: (1) homologous UAV-assisted MEC scenario, i.e. UAV receives the computation task of GUs and then returns the results to the identical GUs, and (2) non-homologous UAV-assisted MEC scenario, i.e., UAV receives the computation task of source GUs and returns the results to the corresponding destination GUs. For these two scenarios, we propose an energy minimization problem by jointly optimizing task partition, the time slot length and UAV trajectory. In particular, we use the path discretization approach to convert the problem as a problem form which is finite variables. Since the problem is non-convex, we use successive convex approximation (SCA) techniques to tackle the non-convexity. Considering the initial trajectory has an important affect on the experimental result, then a specific trajectory initialization design via combining with the Pickup-and-Delivery Problem (PDP) is proposed. Numerical results proof our proposed design is superior compared with the benchmark cases.},
  archive      = {J_COMCOM},
  author       = {Bangfu Zuo and Yu Xu and Dingcheng Yang and Lin Xiao and Tiankui Zhang},
  doi          = {10.1016/j.comcom.2023.03.013},
  journal      = {Computer Communications},
  pages        = {312-323},
  shortjournal = {Comput. Commun.},
  title        = {Joint resource optimization and trajectory design for energy minimization in UAV-assisted mobile-edge computing systems},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI-assisted traffic matrix prediction using GA-enabled deep
ensemble learning for hybrid SDN. <em>COMCOM</em>, <em>203</em>,
298–311. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hybrid software-defined network (SDN), which is a network where traditional routers and SDN protocols coexist during the incremental deployment of SDNs, requires real-time link traffic information for effective deployment. This has called for the need of accurate real-time data analytics and traffic prediction methods. To date, various traffic prediction frameworks have been studied to facilitate analysis and extraction of valuable information from huge sets of incomplete and noisy data. However, due to the linear nature of network design, mainly characterized by manual control plane forwarding configurations, existing traffic prediction frameworks cannot perform consistent traffic prediction over multiple datasets in modern dynamic networks. To address this issue, ensemble-driven approaches based on deep learning (DL) have recently been suggested as a promising solution. Nevertheless, determining the most appropriate combination of baseline DL architectures to be adopted for accurate traffic prediction remains a challenge. This paper proposes a novel DL framework for improved traffic prediction in hybrid SDNs. The framework combines a deep ensemble learning model utilizing multiple dimensionality reduction algorithms and a genetic algorithm (GA). The multi-objective GA is used to perform dynamic optimization of the connection weights and thresholds of the deep ensemble learning model while overcoming the local optima problem. Experimental results show that the proposed approach can achieve more accurate forecast of link traffic than the traditional baseline DL frameworks.},
  archive      = {J_COMCOM},
  author       = {Richard Etengu and Saw Chin Tan and Teong Chee Chuah and Ying Loong Lee and Jaime Galán-Jiménez},
  doi          = {10.1016/j.comcom.2023.03.014},
  journal      = {Computer Communications},
  pages        = {298-311},
  shortjournal = {Comput. Commun.},
  title        = {AI-assisted traffic matrix prediction using GA-enabled deep ensemble learning for hybrid SDN},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling value of information in remote sensing from
correlated sources. <em>COMCOM</em>, <em>203</em>, 289–297. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates data correlation in remote sensing networks and how it can be characterized through diverse models quantifying the Value of Information (VoI), a metric that describes how informative the data transmitted by the sensors are. For each sensor, the VoI evaluations comprise the average node-specific Age of Information (AoI), the average cost spent for sending updates, and the AoI of neighbor nodes, assumed to be correlated sources of information and therefore benefiting the VoI of other sensors nearby. We discuss how this metric can be tracked through a two-dimensional Markov chain , but we also show how this representation can be simplified by including the impact of neighbor nodes within the transition probabilities, so as to obtain a simpler model that gives the same insight in terms of VoI evaluations.},
  archive      = {J_COMCOM},
  author       = {Alberto Zancanaro and Giulia Cisotto and Leonardo Badia},
  doi          = {10.1016/j.comcom.2023.03.008},
  journal      = {Computer Communications},
  pages        = {289-297},
  shortjournal = {Comput. Commun.},
  title        = {Modeling value of information in remote sensing from correlated sources},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IoT-assisted feature learning for surface settlement
prediction caused by shield tunnelling. <em>COMCOM</em>, <em>203</em>,
276–288. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface settlement caused by shield tunnelling may lead to collapse disaster, threatening the safety of ground and underground structures . Therefore, the prediction of surface settlement has attracted much attention, but the problem has not been well solved. Based on the Internet of Things and data driven method, this paper constructs a data model to predict the surface settlement on the basis of fully considering the time–space relationship of surface settlement caused by shield tunnelling . Then, based on this model, the convolutional neural network is used to learn time series features, the attention mechanism is used to realize feature interaction between parameters, and the tensorized long-term and short-term memory network are used to complete feature fusion between rings. A tensorized long-term and short-term memory network based on convolution feature adaptive interaction (TLCFAI) is proposed. Taking the tunnelling process of 10 303 rings in 13 shield tunnel sections in Changzhou, China as an example, TLCFAI is obviously better than the other 9 typical algorithms, and the prediction effect is good. The research shows that this paper provides a relatively complete method and technical means for the prediction of surface settlement caused by shield tunnelling from the construction of data model to feature learning algorithm.},
  archive      = {J_COMCOM},
  author       = {Zhu Wen and Limei Guo and Sipei Meng and Xiaoli Rong and Yehui Shi},
  doi          = {10.1016/j.comcom.2023.03.007},
  journal      = {Computer Communications},
  pages        = {276-288},
  shortjournal = {Comput. Commun.},
  title        = {IoT-assisted feature learning for surface settlement prediction caused by shield tunnelling},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving data dissemination scheme based on
searchable encryption, publish–subscribe model, and edge computing.
<em>COMCOM</em>, <em>203</em>, 262–275. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based Searchable Encryption is emerging as a promising cryptographic technique supporting data protection, flexible access control, and keyword search over encrypted data . The current scientific literature already investigated its adoption in cloud-based services and additionally explored the usage of edge computing to implement some of the cryptographic tasks in scenarios with limited computational capabilities (such as Internet of Things). In the majority of the available solutions, however, the remote cloud is still responsible for data storage, keyword search over encrypted data, and delivery tasks. Indeed, the heavy computational load generated in scenarios with multiple data producers and data consumers (never studied yet) and large communication latencies can inevitably compromise the overall system performance. To bridge this gap, this work proposes a decentralized service architecture offering privacy-preserving data dissemination, by jointly leveraging attribute-based Searchable Encryption techniques , publish–subscribe communication model, and edge computing capabilities. Here, customized Edge Servers are deployed at the network edge to (i) collect subscription requests encoded via Searchable Encryption Trapdoors, (ii) receive data publications, encrypted via Attribute-based Searchable Encryption scheme , (iii) implement keyword search over encrypted data, and (iv) deliver encrypted data only to authorized requesters. Experimental tests explored the impact of network configuration and traffic load on both communication latency and energy consumption. Obtained results demonstrated the unique ability of the proposed solution to achieve shorter data delivery delays as well as less energy consumption with respect to cloud-based alternatives.},
  archive      = {J_COMCOM},
  author       = {Ingrid Huso and Daniele Sparapano and Giuseppe Piro and Gennaro Boggia},
  doi          = {10.1016/j.comcom.2023.03.006},
  journal      = {Computer Communications},
  pages        = {262-275},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-preserving data dissemination scheme based on searchable encryption, publish–subscribe model, and edge computing},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parallel computing based model for online binary
computation offloading in mobile edge computing. <em>COMCOM</em>,
<em>203</em>, 248–261. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile-edge computing (MEC) with wireless power transfer has recently emerged as a viable concept for improving the data processing capacity of limited powered networks like wireless sensor networks (WSN) and the internet of things (IoT). In this research, we explore a wireless MEC network with a binary offloading strategy. Each mobile device’s (MDs) computation task is either performed locally or entirely offloaded to a MEC server. We aim to develop an online system that adapts task offloading decisions and resource allocations to changing wireless channel conditions in real-time. This necessitates solving difficult combinatorial optimization problems quickly within the channel coherence time , which is difficult to achieve with traditional optimization approaches. To address this issue, we offer a parallel computing architecture in which several parallel offloading actors as deep neural networks (DNNs) are used as a scalable method to learn binary offloading decisions from experience. It avoids the need to solve combinatorial optimization issues, reducing computational complexity significantly, especially in large networks. Compared to existing optimization approaches, numerical results demonstrate that the proposed algorithm can achieve optimal performance while reducing computing time by an acceptable margin. For instance, our algorithm achieves a latency rate of 0.033 s in a network of 30 MDs.},
  archive      = {J_COMCOM},
  author       = {Abednego Acheampong and Yiwen Zhang and Xiaolong Xu},
  doi          = {10.1016/j.comcom.2023.03.004},
  journal      = {Computer Communications},
  pages        = {248-261},
  shortjournal = {Comput. Commun.},
  title        = {A parallel computing based model for online binary computation offloading in mobile edge computing},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforced practical byzantine fault tolerance consensus
protocol for cyber physical systems. <em>COMCOM</em>, <em>203</em>,
238–247. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber–physical systems (CPS) are becoming an essential component of modern life. CPS services enable information to be exchanged between physical devices and virtual systems. The increasing malicious activities causing confidential data leakage and incorrect performance of devices are posing challenges for protection against cyber threats. Therefore, development of effective solutions that can protect both CPS data and data exchange networks are extremely urgent. This study makes some improvements on Practical Byzantine Fault Tolerance (PBFT), and provides a critical analysis of the feasibility of using blockchain technology to protect constrained CPS data. It also justifies the choice of the improved PBFT for implementation on such devices and simulates the main distributed ledger scenarios. The improved PBFT works as follows: first, the client broadcasts the signed transaction data to the entire network, rather than just the master node, followed by a hash value comparison verification process; second, select the master node based on the longest chain principle, and punish the malicious node via the “blacklist” mechanism; third, add the data synchronization and verification process, as well as dynamically entering and leaving nodes via state transitions. Ultimately, the PBFT commit phase is eliminated, resulting in a two-stage process. In comparison to the original PBFT protocol, the Reinforced Practical Byzantine Fault Tolerance (RPBFT) protocol may substantially enhance system throughput and minimize consensus communication time, thereby improving overall system efficiency while guaranteeing security.},
  archive      = {J_COMCOM},
  author       = {Yun Wu and Liangshun Wu and Hengjin Cai},
  doi          = {10.1016/j.comcom.2023.03.016},
  journal      = {Computer Communications},
  pages        = {238-247},
  shortjournal = {Comput. Commun.},
  title        = {Reinforced practical byzantine fault tolerance consensus protocol for cyber physical systems},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cost-efficient hybrid redundancy coding scheme for
wireless storage systems. <em>COMCOM</em>, <em>203</em>, 226–237. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With distributed storage technique becoming a promising technology for storing massive data in wireless environment, how to improve the reliability of the storage systems has become the focus of researchers. As a redundancy scheme, the erasure coding has been widely used because of its high fault tolerance and low storage overhead performance. However, a large amount of repair overhead consumed to reconstruct lost data has become the most significant issue in erasure-coded storage systems. In this paper, a wireless heterogeneous storage network with two-layer is designed, where processing units and storage nodes communicate cooperatively. Furthermore, with the consideration of the access frequency skew of stored information and the performance difference of storage nodes, we propose a novel repairable fountain code scheme based on partial duplication technique (PD-RFC), which can modify the repair bandwidth and improve the availability of links between nodes without almost increasing additional storage space overhead. Furthermore, a hybrid code scheme combined by PD-RFC scheme and RFC scheme is designed for the heterogeneous two-layer storage network. The simulation results have shown that the hybrid scheme significantly outperforms other redundant schemes in energy communication cost saving.},
  archive      = {J_COMCOM},
  author       = {Anan Zhou and Nanhao Zhou and Benshun Yi and Chao Zhu},
  doi          = {10.1016/j.comcom.2023.03.012},
  journal      = {Computer Communications},
  pages        = {226-237},
  shortjournal = {Comput. Commun.},
  title        = {A cost-efficient hybrid redundancy coding scheme for wireless storage systems},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Content privacy enforcement models in decentralized online
social networks: State of play, solutions, limitations, and future
directions. <em>COMCOM</em>, <em>203</em>, 199–225. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Decentralized Online Social Networks (DOSNs) have been attracting the attention of many users because they reduce the risk of censorship, surveillance, and information leakage from the service provider. In contrast to the most popular Online Social Networks, which are based on centralized architectures (e.g., Facebook, Twitter, or Instagram), DOSNs are not based on a single service provider acting as a central authority. Indeed, the contents that are published on DOSNs are stored on the devices made available by their users, which cooperate to execute the tasks needed to provide the service. A specific form of cooperation is to store the content published by a user on other peers’ devices as well, hence dramatically enhancing availability. Consequently, such contents must be properly protected by the DOSN infrastructure, in order to ensure that they can be really accessed only by users who have the permission of the publishers. As a consequence, DOSNs require efficient solutions for protecting the privacy of the contents published by each user with respect to the other users of the social network. This is exactly the focus of this paper. In particular, we investigate and compare the principal content privacy enforcement models adopted by current DOSNs evaluating their suitability to support different types of privacy policies based on user groups. Such evaluation is carried out by implementing several models and comparing their performance for the typical operations performed on groups, i.e., content publish, user join, and user leave. In detail, we show that the join operation incurs a similar cost for all the privacy enforcement models and groups, while for the leave operation performance is greatly affected by the selected solution, which must be evaluated on a case-by-case basis depending on both the type and the activity level of the group—as analytically detailed in our contribution. Further, we also highlight the limitations of current approaches and show future research directions. The provided contributions, other than being interesting on their own, set a blueprint for researchers and practitioners interested in implementing DOSNs, and highlight a few open research directions.},
  archive      = {J_COMCOM},
  author       = {Andrea De Salve and Paolo Mori and Laura Ricci and Roberto Di Pietro},
  doi          = {10.1016/j.comcom.2023.02.023},
  journal      = {Computer Communications},
  pages        = {199-225},
  shortjournal = {Comput. Commun.},
  title        = {Content privacy enforcement models in decentralized online social networks: State of play, solutions, limitations, and future directions},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable intrusion detection for next generation of
internet of things. <em>COMCOM</em>, <em>203</em>, 192–198. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new framework for intrusion detection in the next-generation Internet of Things . MinMax normalization strategy is used to collect and preprocess data. The Marine Predator algorithm is then used to select relevant features to be used in the learning process. The selected features are then trained with an advanced and state-of-the-art recurrent neural network that includes an attention mechanism . Finally, Shapely values are calculated to determine how much each feature contributes to the final output. The dataset NSL-KDD was used for intensive simulations. The results show the advantages of the proposed system as well as its superiority over state-of-the-art methods. In fact, the proposed solution achieved a rate of more than 94\% for both true negative and true position, while the rates of the existing solutions are below 90\% for the challenging NSL-KDD datasets.},
  archive      = {J_COMCOM},
  author       = {Youcef Djenouri and Asma Belhadi and Gautam Srivastava and Jerry Chun-Wei Lin and Anis Yazidi},
  doi          = {10.1016/j.comcom.2023.03.005},
  journal      = {Computer Communications},
  pages        = {192-198},
  shortjournal = {Comput. Commun.},
  title        = {Interpretable intrusion detection for next generation of internet of things},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locality-aware deployment of application microservices for
multi-domain fog computing. <em>COMCOM</em>, <em>203</em>, 180–191. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In fog computing customers’ microservices may demand access to connected objects, data sources and computing resources outside the domain of their fog provider In practice, the locality of connected objects renders mandatory a multi-domain approach in order to broaden the scope of resources available to a single-domain fog provider. We consider a scenario where assets from other domains can be leased across a federation of cloud–fog infrastructures. In this context, a fog provider aims to minimize the quantity of external resources to be rented to satisfy the applications’ demands while meeting their requirements. We first introduce a general framework for the deployment of applications across multiple domains owned by multiple cloud–fog providers. Hence, the resource allocation problem is formulated in the form of an integer linear program . We provide a novel heuristic method that explores the resource assignment space in a breadth-first fashion to ensure that locality constraints are met. Extensive numerical results evaluate deployment costs and feasibility of the proposed solution demonstrating that it outperforms the standard approaches adopted in the literature.},
  archive      = {J_COMCOM},
  author       = {Francescomaria Faticanti and Marco Savi and Francesco De Pellegrini and Domenico Siracusa},
  doi          = {10.1016/j.comcom.2023.02.012},
  journal      = {Computer Communications},
  pages        = {180-191},
  shortjournal = {Comput. Commun.},
  title        = {Locality-aware deployment of application microservices for multi-domain fog computing},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust decentralized stochastic gradient descent over
unstable networks. <em>COMCOM</em>, <em>203</em>, 163–179. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized learning is essential for large-scale deep learning due to its great advantage in breaking the communication bottleneck. Most decentralized learning algorithms focus on reducing the communication overhead without taking into account the possibility of a shaky network connection, and existing analyses over unstable networks have various limitations such as centralized settings, strong unrealistic assumptions, etc. Hence, in this work, we study a non-convex optimization problem over unstable networks that fully consider unstable factors including unstable network connections, communication and artificially injected noise. Specifically, we focus on the most commonly used Stochastic Gradient Descent (SGD) algorithm in a mild decentralized setting and propose a robust algorithm to handle unstable networks. It is shown that our algorithm can attain a convergence rate which has the same order as decentralized algorithms over stable networks, and achieves linear speedup comparing with centralized ones. Moreover, the proposed algorithm also applies to the general case that the data are not independently and identically distributed. Extensive experiments on image classification demonstrate that the practical performance of our algorithm is comparable with the state-of-art decentralized algorithms in stable networks with only a little accuracy loss.},
  archive      = {J_COMCOM},
  author       = {Yanwei Zheng and Liangxu Zhang and Shuzhen Chen and Xiao Zhang and Zhipeng Cai and Xiuzhen Cheng},
  doi          = {10.1016/j.comcom.2023.02.025},
  journal      = {Computer Communications},
  pages        = {163-179},
  shortjournal = {Comput. Commun.},
  title        = {Robust decentralized stochastic gradient descent over unstable networks},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A temporal–spatial analysis on the socioeconomic development
of rural villages in thailand and vietnam based on satellite image data.
<em>COMCOM</em>, <em>203</em>, 146–162. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining accurate and timely estimates of socio-economic status at fine geographical resolutions is essential for global sustainable development and the fight against poverty. However, data related to local socio-economic dynamics in rural villages is often either unavailable or outdated. To fill this gap, predicting local economic well-being with satellite imagery and machine learning has shown promising results. While most state-of-the-art analyses focus on predicting the levels of socio-economic status, finding temporal changes in rural villages’ economic well-being is essential for tracking the impacts of public policies (targeting e.g., poverty alleviation or access to various public services). In this paper, we propose an approach that utilizes pixel-wise differences in satellite images to classify temporal changes in average and median consumption expenditures (and income) in rural villages in Thailand and Vietnam between 2007 and 2017. This approach is shown to be able to distinguish between “Decline”, “Stagnation” and “Growth” in these outcomes with an F1 score of up to 63.2\% using an Extreme Gradient Boosting Classifier model . In addition, we employ regression-based approaches which achieve an R 2 R2 of up to 39.5\% when predicting actual changes in these outcomes with an Extreme Gradient Boosting Regressor . Our study demonstrates the feasibility of satellite-based estimates for measuring changes in local socio-economic dynamics.},
  archive      = {J_COMCOM},
  author       = {Fabian Wölk and Tingting Yuan and Krisztina Kis-Katos and Xiaoming Fu},
  doi          = {10.1016/j.comcom.2023.02.017},
  journal      = {Computer Communications},
  pages        = {146-162},
  shortjournal = {Comput. Commun.},
  title        = {A temporal–spatial analysis on the socioeconomic development of rural villages in thailand and vietnam based on satellite image data},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An overview of MAC energy-saving mechanisms in wi-fi.
<em>COMCOM</em>, <em>203</em>, 129–145. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IEEE 802.11 standard has rapidly become the de facto standard for Wireless Local Area Networks (WLANs) under the commercial name Wi-Fi. Despite IEEE’s regular efforts to improve the energy efficiency of Wi-Fi, many energy-saving mechanisms remain relatively unknown to the community, which contributed to hamper their large-scale adoption by chipset manufacturers. In this paper, we describe the main MAC mechanisms that have been introduced by the IEEE 802.11 standards (and its amendments) for the sake of saving energy. This includes diverse mechanisms where either the stations (STAs) prompt the access point (AP), the AP notifies the STAs, or the STAs and the AP make an appointment. In every case, we illustrate the mechanism’s principle through examples. We also present a set of metrics to compare the effectiveness of energy-saving mechanisms with each other and discuss how the energy-saving mechanisms of IEEE were adopted by the Wi-Fi Alliance, and hence by chip manufacturers. Overall, we believe that this paper, which accurately details these mechanisms, can help researchers, manufacturers, and network operators to have a clear understanding of what Wi-Fi is readily capable of doing when it comes to energy efficiency, and thus foster their use in actual and upcoming WLANs.},
  archive      = {J_COMCOM},
  author       = {Esther Guérin and Thomas Begin and Isabelle Guérin Lassous},
  doi          = {10.1016/j.comcom.2023.03.003},
  journal      = {Computer Communications},
  pages        = {129-145},
  shortjournal = {Comput. Commun.},
  title        = {An overview of MAC energy-saving mechanisms in wi-fi},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint think locally and globally: Communication-efficient
federated learning with feature-aligned filter selection.
<em>COMCOM</em>, <em>203</em>, 119–128. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning , as a distributed machine learning framework that can protect data privacy, has recently received increasing attention. In federated learning, a shared global model is obtained through parameter interaction, which leads to frequent parameter communication during the training process. The limited bandwidth of IoT and edge devices that deploy federated learning further affects communication and learning efficiency. In this paper, an enhanced federated learning technique is presented by proposing a feature-aligned filter selection method. Besides, it is believed that the training gap between the global model and the local model on each node should be focused on during the training process. Then, we can select the local contribution parameters to improve the communication efficiency while ensuring the performance of the shared global model. Therefore, the Geometric Median of each layer in the global model is adopted as the criterion to select important filters in the local model, and then the parameters interact with other nodes to achieve efficient communication. The results under a variety of experimental settings demonstrate that our proposed federated learning scheme can effectively enhance communication efficiency and ensure the performance of the global model. Compared with the state-of-the-art methods, a maximum of 6.5 × × improvements in communication efficiency can be obtained.},
  archive      = {J_COMCOM},
  author       = {Zhao Yang and Qingshuang Sun},
  doi          = {10.1016/j.comcom.2023.03.002},
  journal      = {Computer Communications},
  pages        = {119-128},
  shortjournal = {Comput. Commun.},
  title        = {Joint think locally and globally: Communication-efficient federated learning with feature-aligned filter selection},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained frequencies meet synchronous transmission.
<em>COMCOM</em>, <em>203</em>, 99–118. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained frequencies have been used for IEEE 802.15.4 based communication in several recent works to enhance network throughput as well as combat Cross Technology Interference (CTI) in licence free ISM bands. However, the works demonstrate that as the Center Frequency Distance (CFD) goes very low ( &amp;lt; 3 MHz), adjacent channels start severely interfering with each other which heavily limits the use of the fine-grained frequencies. All the studies so far on fine-grained frequencies consider the underlying communication to be based on traditional Asynchronous-Transmission (AT). Synchronous-Transmission (ST) based communication, in contrast, has shown its superiority over AT-based strategies in several different aspects in many recent works. It has been shown that the ability to make efficient use of special physical layer phenomena called Capture-Effect (CE), enables ST-based strategies to carry out more in-parallel communication even under the same channel/frequency (under certain constraints), compared to AT. In this work, we study how this special capability of ST can be exploited in the context of multi-channel or multi-frequency communication with fine-grained frequencies, in particular, when the CFD goes below 3 MHz. We explicitly study how a very narrow band of frequencies can be effectively used for in-parallel communication. Based on extensive local and controlled experiment-based studies, we conclude that the construction of the groups plays a major role in exploiting the fine-grained frequencies. To fulfill the requirements, we design and develop a simple, distributed, and efficient group formation strategy , that can divide a given IoT-network into a desired number of groups and maximize the benefit of the conjunction of ST and the fine-grained frequencies. Through extensive experiment-based evaluations in IoT-testbeds, we show that the groups constructed by the proposed strategy can make very efficient use of the fine-grained frequencies with a CFD as low as 1 MHz to execute in-parallel communication tasks quite reliably and independently. We also demonstrate that even under severe CTI, when the traditional standard channel-based mechanism drastically fails or results in considerably poor performance, communication under fine-grained frequencies under the proposed mechanism can sustain pretty well.},
  archive      = {J_COMCOM},
  author       = {Jagnyashini Debadarshini and Sudipta Saha},
  doi          = {10.1016/j.comcom.2023.02.026},
  journal      = {Computer Communications},
  pages        = {99-118},
  shortjournal = {Comput. Commun.},
  title        = {Fine-grained frequencies meet synchronous transmission},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TCAM-based packet classification for many-field rules of
SDNs. <em>COMCOM</em>, <em>203</em>, 89–98. (<a
href="https://doi.org/10.1016/j.comcom.2023.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networking (SDN) provides an emerging paradigm for future network architectures . OpenFlow is a widely deployed south-bound protocol for SDN. It uses rule-based packet forwarding and each rule may support many header fields for fulfilling various SDN applications. As the number of inspected header fields increases, these long rules may complicate the procedure of packet classification based on TCAM because TCAM only supports limited word sizes. In this paper, we propose a TCAM-based packet classification scheme to support many-field rules. Our scheme incorporates multiple stages of TCAM accesses, where each stage only compares selected bits of rules. Although our scheme may result in additional TCAM accesses, it enables packet classification upon rules longer than TCAM words. The experimental results show that our scheme can search 10K 247-bit rules by using 72-bit TCAM entries, where each packet classification can be accomplished with less than ten TCAM accesses. It also reduces the required TCAM storage by a magnitude of order. In summary, with our scheme, both feasibility and scalability of TCAM-based packet classification for SDN rules are significantly improved.},
  archive      = {J_COMCOM},
  author       = {Hsin-Tsung Lin and Pi-Chung Wang},
  doi          = {10.1016/j.comcom.2023.03.001},
  journal      = {Computer Communications},
  pages        = {89-98},
  shortjournal = {Comput. Commun.},
  title        = {TCAM-based packet classification for many-field rules of SDNs},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-UAV WRSN charging path planning based on improved heed
and IA-DRL. <em>COMCOM</em>, <em>203</em>, 77–88. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of how to improve the energy utilization rate of UAV (Unmanned Aerial Vehicle) in the charging process of wireless rechargeable sensor network, a charging path planning scheme for multi-UAV wireless rechargeable sensor network based on deep reinforcement learning is proposed. Firstly, the multi-UAV path planning problem is described and a network model is established, and then the network model is optimized by using the improved dynamic clustering algorithm of HEED, and then an intelligent path optimization algorithm based on the intelligent algorithm and deep reinforcement learning IA-DRL is proposed for the problem model. According to this algorithm, the optimal charging path of multiple drones is obtained, and finally the drones charge each node to be charged in the network. The experimental results show, compared with other traditional heuristic methods , IA-DRL has more advantages in solving small and medium-scale UAV path planning problems, and compared with the neural network model without PSO , the minimum AVG performance of IA-DRL is improved by about 3.8\% and has a faster convergence speed about 550 episodes.},
  archive      = {J_COMCOM},
  author       = {Tianle Shan and Yang Wang and Chuanxin Zhao and Yingchun Li and Guanghai Zhang and Qiangjun Zhu},
  doi          = {10.1016/j.comcom.2023.02.021},
  journal      = {Computer Communications},
  pages        = {77-88},
  shortjournal = {Comput. Commun.},
  title        = {Multi-UAV WRSN charging path planning based on improved heed and IA-DRL},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling privacy by anonymization in the collection of
similar data in multi-domain IoT. <em>COMCOM</em>, <em>203</em>, 60–76.
(<a href="https://doi.org/10.1016/j.comcom.2023.02.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of Internet of Things (IoT) systems heavily relies on possessing trustworthy and up-to-date data. Thus, data collection represents a key feature in IoT applications and services that rely on data-streams. Reusing the same data-stream for distinct queries and services represents a promising way to save energy and reduce operational cost for the deployment. However, acquiring data from different owners through distinct providers incurs privacy issues. Indeed, data crosses the borders of different systems, and the end consumer may get knowledge on the original producer. In-network data aggregation enables the compression of information and improves the privacy while reducing the network load. However, the same data should not be aggregated several times, to provide correct and consistent operations. In this paper, we provide a pub/sub routing scheme for IoT systems that respects privacy constraints. The routers publish offers that describe the data they can export: they may aggregate several offers to respect the privacy constraints as defined by the producers. Each producer has a disposable ID, only used to detect and avoid overlaps in the data-streams. Our performance evaluation highlights the efficiency of our pub-sub routing solution to construct a valid aggregation (without overlap) distributively, while respecting privacy constraints.},
  archive      = {J_COMCOM},
  author       = {Renato Caminha Juacaba Neto and Pascal Mérindol and Fabrice Theoleyre},
  doi          = {10.1016/j.comcom.2023.02.022},
  journal      = {Computer Communications},
  pages        = {60-76},
  shortjournal = {Comput. Commun.},
  title        = {Enabling privacy by anonymization in the collection of similar data in multi-domain IoT},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex parts machining path planning through cross-source
point cloud processing. <em>COMCOM</em>, <em>203</em>, 48–59. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When cleaning cast parts, production machinery such as robots and smart machines must be able to operate tools along appropriate machining paths. Planning the machining path using 3D point cloud processing techniques is a suitable approach. The local point clouds generated from in situ scans and the standard point clouds, however, differ in scale and accuracy, and these issues make the registration of cross-source point clouds a difficult academic topic. The study presented here suggests a novel cross-source point cloud registration technique that combines a point cloud slicing algorithm and a curve fitting algorithm to plan machining paths for complex cast parts. The results demonstrate the practicality of the proposed method, with the uniformity of the surface profile and the operational effectiveness of the machining process being 57.90\% and 67.74\% higher, respectively, for the machining paths generated using the proposed method compared to the manual method. By comparing trials of various machining path curve fitting algorithms, we find that the Fourier fitting approach performs best. In addition, the co-planar features chosen for coarse registration will be better registered if they are closer to the plane containing the intended machining path. The proposed method may be useful for online inspection by UAVs and interactive robots.},
  archive      = {J_COMCOM},
  author       = {Shipu Diao and Yong Yang and Guanqun Cui and Yubing Chen},
  doi          = {10.1016/j.comcom.2023.02.024},
  journal      = {Computer Communications},
  pages        = {48-59},
  shortjournal = {Comput. Commun.},
  title        = {Complex parts machining path planning through cross-source point cloud processing},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-user edge service orchestration based on deep
reinforcement learning. <em>COMCOM</em>, <em>203</em>, 30–47. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fifth generation (5G) of mobile network offers a remarkable degree of flexibility to mobile operators, enabling them to provide users with effective and tailored network services . Software Defined Networking (SDN), Network Function Virtualization (NFV), and edge computing have given the operator the opportunity to easily bring computational capacity to the edge and to support latency-sensitive services. While 5G standards have defined the technological and architectural frameworks to orchestrate services, finding effective resources management and QoS optimization policies is still an open research issue. In this paper, we propose an online orchestration methodology for a multi-user edge service. The orchestrator goal is to simultaneously maximize the QoS, and minimize the amount of resources needed. We provide a mathematical formulation to compute an optimal offline policy and derive an online approach based on a model-free Deep Reinforcement Learning (DRL) framework. As a novel feature, the DRL agent action is modeled as a parametric combinatorial problem . A tailored multi-objective reward function leads the agent towards an effective choice of parameters for such a model. Our models are built, trained and fine-tuned by exploiting real data. Extensive simulations in diverse scenarios show that our DRL online approach produces solutions with small gaps to the optimal offline ones, enabling the operator to both save resources and grant the users an adequate QoS level.},
  archive      = {J_COMCOM},
  author       = {Christian Quadri and Alberto Ceselli and Gian Paolo Rossi},
  doi          = {10.1016/j.comcom.2023.02.027},
  journal      = {Computer Communications},
  pages        = {30-47},
  shortjournal = {Comput. Commun.},
  title        = {Multi-user edge service orchestration based on deep reinforcement learning},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coordinated network partition detection and bi-connected
inter-partition topology creation in damaged sensor networks using
multiple UAVs. <em>COMCOM</em>, <em>203</em>, 15–29. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable connectivity restoration mechanism is essential for retaining the prolonged network functioning of Wireless Sensor and Actor Networks (WSANs). WSANs operate in harsh or inhospitable environments and are susceptible to multiple node failures that may fragment the fully connected network into disparate fragments. The well-known network recovery techniques deploy new relay nodes for recovery and introduce more cut vertices in the network that generates the resulting network topology vulnerable to further partitioning. Therefore, it is advantageous to identify the network partitions in less time and establish a fault-tolerant topology during the network restoration. This paper provides a new coordinated network partition detection and Bi-connected Inter-partition Topology creation solution in damaged sensor networks using multiple UAVs (UAV-BITS) that operates in different phases. UAV-BITS employs multiple UAVs to identify the network partitions and engages network nodes to self-identify their partitions by creating Connected Component Sets (CCSs) iteratively. During the network recovery process, UAV-BITS creates two-vertex disjoint paths between the network partitions to make the recovered network topology fault-tolerant. UAV-BITS is assessed according to various parameters like the number of deployed relay nodes , the number of messages transmitted during detection and recovery phases, network partition detection and recovery time, and distance traveled by UAVs. The performance of CCS based partition detection mechanism is also compared with a similar UAVs-assisted partition detection approach. The working of existing bi-connected topology creation algorithms that employ representative nodes is also analyzed and compared with the proposed recovery algorithm . The simulation results illustrate that UAV-BITS provides a better fault-tolerant network topology than already proposed solutions. The algorithm also reduces the travel distance of UAVs and detection time.},
  archive      = {J_COMCOM},
  author       = {Aditi Zear and Virender Ranga and Kriti Bhushan},
  doi          = {10.1016/j.comcom.2023.02.018},
  journal      = {Computer Communications},
  pages        = {15-29},
  shortjournal = {Comput. Commun.},
  title        = {Coordinated network partition detection and bi-connected inter-partition topology creation in damaged sensor networks using multiple UAVs},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of three-factor secure and efficient authentication
and key-sharing protocol for IoT devices. <em>COMCOM</em>, <em>203</em>,
1–14. (<a href="https://doi.org/10.1016/j.comcom.2023.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable advance of the Internet of Things (IoT) has smoothed the way to the interconnection of various mobile devices in secure access and communication. In order to guarantee user privacy and anonymity in public networks, a large number of mutual authentication and key-sharing protocols between different IoT devices and multi-servers have been proposed. Due to resource-constrained and inefficient IoT devices, most previous protocols can confront assorted malicious attacks , such as eavesdropping, counterfeiting, chip cloning, device forgery, and other attacks. These attacks may be exposed the user’s private key or other sensitive data. To solve these problems, physical unclonable function (PUF) is a lightweight security primitive that utilizes random process deviations that cannot be controlled during chip manufacturing to generate device-unique digital signatures . In this paper, we combined the key-sharing scheme based on PUF on the hardware side, which solves relevant security problems such as device cloning and key tampering. Furthermore, we propose a three-factor secure and efficient authentication and key-sharing protocol, leveraging the inherent security properties of passwords, biometrics , and PUFs. We demonstrate the security of our proposed protocol based on computational Bilinear Diffie–Hellman Problem (BDH) and k k -CAA hard problems and the Proverif tool. Compared with existing relevant protocols, our protocol meets various security properties and defends against varied security threats. The low computational cost, communication overhead , and device storage indicate that our protocol is applicable to resource-constrained IoT devices.},
  archive      = {J_COMCOM},
  author       = {Zhenyu Wang and Ding Deng and Shen Hou and Yang Guo and Shaoqing Li},
  doi          = {10.1016/j.comcom.2023.02.015},
  journal      = {Computer Communications},
  pages        = {1-14},
  shortjournal = {Comput. Commun.},
  title        = {Design of three-factor secure and efficient authentication and key-sharing protocol for IoT devices},
  volume       = {203},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly detection for fault detection in wireless community
networks using machine learning. <em>COMCOM</em>, <em>202</em>, 191–203.
(<a href="https://doi.org/10.1016/j.comcom.2023.02.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has received increasing attention in computer science in recent years and many types of methods have been proposed. In computer networks, little attention has been paid to the use of ML for fault detection, the main reason being the lack of datasets. This is motivated by the reluctance of network operators to share data about their infrastructure and network failures. In this paper, we attempt to fill this gap using anomaly detection techniques to discern hardware failure events in wireless community networks. For this purpose we use 4 unsupervised machine learning, ML, approaches based on different principles. We have built a dataset from a production wireless community network, gathering traffic and non-traffic features, e.g. CPU and memory. For the numerical analysis we investigated the ability of the different ML approaches to detect an unprovoked gateway failure that occurred during data collection. Our numerical results show that all the tested approaches improve to detect the gateway failure when non-traffic features are also considered. We see that, when properly tuned, all ML methods are effective to detect the failure. Nonetheless, using decision boundaries and other analysis techniques we observe significant different behavior among the ML methods.},
  archive      = {J_COMCOM},
  author       = {Llorenç Cerdà-Alabern and Gabriel Iuhasz and Gabriele Gemmi},
  doi          = {10.1016/j.comcom.2023.02.019},
  journal      = {Computer Communications},
  pages        = {191-203},
  shortjournal = {Comput. Commun.},
  title        = {Anomaly detection for fault detection in wireless community networks using machine learning},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sum-rate maximization for cognitive relay NOMA systems with
channel uncertainty. <em>COMCOM</em>, <em>202</em>, 183–190. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the sum-rate maximization problem for relay cognitive network based on decoding and forwarding (DF), where the primary network is implemented with the orthogonal frequency division multiple access (OFDMA) technology and the secondary users are in the underlying accessing mode. In the system, multiple primary users are included and one of them plays a role of relay. The relay forwards information to secondary users using the non-orthogonal multiple access (NOMA) technology. We attempt to improve the transmission rate of both primary and secondary users. The block power control strategy is developed by addressing interference of primary users caused by the channel sharing with secondary users. To tackle the tricky issue caused by the channel uncertainty, the worst-case method is adopted when solving the proposed sum-rate maximization problem. Considering the nonlinear objective function and constraints, the successive convex approximation (SCA) method is introduced to determine a suboptimal solution. Also, the Lagrangian dual algorithm is utilized to obtain the closed-form solutions of the transmission power and subchannel allocation. Numerical results verify that the proposed scheme significantly improves the sum-rate under the uncertainty channel gains compared to benchmarks.},
  archive      = {J_COMCOM},
  author       = {Fenglei Li and Jiayuan Zhang and Zhixin Liu and Kit Yan Chan and Yi Yang and Yuan’ai Xie},
  doi          = {10.1016/j.comcom.2023.02.020},
  journal      = {Computer Communications},
  pages        = {183-190},
  shortjournal = {Comput. Commun.},
  title        = {Sum-rate maximization for cognitive relay NOMA systems with channel uncertainty},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy aware clustered blockchain data for IoT: An
end-to-end lightweight secure &amp; enroute filtering approach.
<em>COMCOM</em>, <em>202</em>, 166–182. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of things (IoT) include smart homes, smart buildings and smart factories, in which all the devices are connected over the internet through wireless network for data transmission or sharing. IoT faces many security challenges due to sharing the data in an open source. Security and energy are the most focusing challenges in IoT due to its resource constrained nature. IoT devices have low energy that is wasted by the attackers, and the data also gets hacked by the attacker, which leads to poor security. This paper proposed CLIENT ( Cl ustered blockchain I oT with En -rou t e) to provide energy efficient and secure communication. For that, this research proposed five processes. Such as Chaotic map based Elgamal authentication , credit score based clustering, capuchin search optimization based packets routing, signature based Enroute filtering and deep leaning based anomaly packet detection. First process is Chaotic Map based Elgamal Authentication, in which chaotic map is used in Elgamal key generation. Second process is Credit score based clustering which minimizes the complexity and overhead during packet transmission . Third process is Capuchin search optimization based packets routing which is used to select optimal route from cluster member to cluster head (CH). Fourth process is signature based Enroute filtering which used to detect and drop false report from the network. Edwards signature based digital signature algorithm (EdDSA) is used for verification purpose. And final process is deep learning based anomaly packets detection, this process used Katz centrality based feed forward small world neural network (FFSWNN) for packet classification (normal or malicious). All the transactions are stored in blockchain to enhance security. The proposed system implemented by using NS3. The result shows that proposed system achieves superior performance compared to existing works in terms of energy consumption, packet loss rate, end to end delay , routing overhead and network life time, precision, recall, accuracy, false alarm rate , encryption time, decryption time and security strength.},
  archive      = {J_COMCOM},
  author       = {Ramamoorthi S. and Muthu Kumar B. and Ahilan Appathurai},
  doi          = {10.1016/j.comcom.2023.02.010},
  journal      = {Computer Communications},
  pages        = {166-182},
  shortjournal = {Comput. Commun.},
  title        = {Energy aware clustered blockchain data for IoT: An end-to-end lightweight secure &amp; enroute filtering approach},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent and survivable resource slicing for 6G-oriented
UAV-assisted edge computing networks. <em>COMCOM</em>, <em>202</em>,
154–165. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sixth-generation (6G) communication networks are envisioned to build a cloud–edge-terminal ecosystem that can provide various AI services for end devices. With the development of 6G networks, massive devices will consume incredible computing and network resources for customized services. Mobile edge computing (MEC) has been regarded as a promising solution to relieve the pressure of the core network. The combination of unmanned aerial vehicles (UAVs) and MEC systems makes the edge server greatly improved in mobility and flexibility. However, it is difficult to manage and allocate appropriate resources for a large number of end devices. Meanwhile, the UAVs are limited in energy capacities and less stable than the fixed edge server. In this work, we propose a survivable resource slice embedding (SRSE) algorithm for UAV-assisted edge computing systems by leveraging network slicing technologies. A long short-term memory (LSTM) network is developed to obtain the future workloads of resource slices to reduce the slice embedding and re-embedding energy consumption simultaneously. By leveraging an open-source 5G trajectory dataset, the proposed SRSE algorithm is compared with two benchmark schemes. Finally, we provide extensive experimental results to illustrate that the SRSE algorithm improves the request acceptance ratio, slice recovery ratio, and reduces the slice recovery energy consumption significantly.},
  archive      = {J_COMCOM},
  author       = {Guoquan Wu and Bing Zhang and Ya Li},
  doi          = {10.1016/j.comcom.2023.02.006},
  journal      = {Computer Communications},
  pages        = {154-165},
  shortjournal = {Comput. Commun.},
  title        = {Intelligent and survivable resource slicing for 6G-oriented UAV-assisted edge computing networks},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal cluster based feature selection for intrusion
detection system in web and cloud computing environment using hybrid
teacher learning optimization enables deep recurrent neural network.
<em>COMCOM</em>, <em>202</em>, 145–153. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information technology organizations have experienced rapid growth in recent years, resulting in scalability, mobility, and flexibility challenges. Those organizations move their data to the cloud because security and privacy are major concerns. As cloud computing becomes more popular, security has become an important concern. These confidential data are vulnerable to attacks/malicious or intruders due to the characteristics of the cloud. In order to address the growing concern of real-time intruders, a variety of intrusion detection systems (IDS) used specifically for cloud environments with the aim of enhancing overall security. There are, however, some limitations and known attacks that can be overcome by those IDSs. We recently proposed a hybrid soft computing based IDS (ST-IDS) for web and cloud environments, but missed some novel web and cloud attacks. Using hybrid teacher learning enabled deep recurrent neural networks and cluster based feature optimization, we propose an IDS scheme for web and cloud computing environments. MMFO (modified manta-ray foraging optimization) is used after feature extraction to select optimal features for further detection. To classify the intrusion in the web-cloud environment, a hybrid teacher-learning enabled deep recurrent neural network (TL-DRNN) is introduced. Our proposed IDS scheme has been validated using benchmark datasets including DARPA LLS DDoS-1.0, CICIDS-2017, and CSIC-2010. The performance of our proposed IDS scheme has been compared to existing IDS schemes using various quality measures.},
  archive      = {J_COMCOM},
  author       = {K.G. Maheswari and C. Siva and G. Nalinipriya},
  doi          = {10.1016/j.comcom.2023.02.003},
  journal      = {Computer Communications},
  pages        = {145-153},
  shortjournal = {Comput. Commun.},
  title        = {Optimal cluster based feature selection for intrusion detection system in web and cloud computing environment using hybrid teacher learning optimization enables deep recurrent neural network},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast and accurate RFID tag positioning method based on AoA
hologram and hashtables. <em>COMCOM</em>, <em>202</em>, 135–144. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RFID-based item localization for large-scale warehousing and inventorying applications has been gradually developing over the years. In these settings, fast and accurate item localization with low deployment cost is vital for inventory visibility and warehouse efficiency. However, existing tag positioning methods based on a single antenna offer either fast inference with low accuracy or high accuracy with slow inference. In this paper, we propose a phase-based SAR tag positioning method that offers both merits. Firstly, the traditional holograms formed in existing SAR methods are completely replaced by a one-step version of the recently proposed hologram mask (Angle-of-Arrival hologram), to accelerate the tag positioning task. Secondly, a pair of hashtables is implemented to output directly hologram grids from phase measurements, reducing further the hologram construction time. Thirdly, a pre-processing cross-validation stage tests the measurements at each operating frequency to determine the one leading to the lowest positioning error. After this, the model is trained and tested solely on this frequency, limiting both the inference time and on-site collection time. Finally, we efficiently infer the height information of tags from a single antenna moving along a rectilinear horizontal trajectory. Our comparative experiments show that our model achieves similar positioning accuracy to the state-of-the-art accuracy methods while offering a much lower inference time, in both short-range and long-range settings.},
  archive      = {J_COMCOM},
  author       = {Eric Rigall and Xianglong Wang and Shu Zhang and Junyu Dong},
  doi          = {10.1016/j.comcom.2023.01.020},
  journal      = {Computer Communications},
  pages        = {135-144},
  shortjournal = {Comput. Commun.},
  title        = {A fast and accurate RFID tag positioning method based on AoA hologram and hashtables},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective cooperative computation offloading for MEC
in UAVs hybrid networks via integrated optimization framework.
<em>COMCOM</em>, <em>202</em>, 124–134. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous expansion of the application field of the Internet of Things (IoT), mobile edge computing (MEC) is regarded as a promising technique to reduce the time-delay and energy-consumption of application. However, the conventional MEC infrastructure is lack of flexibility, and failed to meet the different requires of mobile device . UAVs have the distinct features of high scalability and mobility for communications, which can act as the complement of conventional MEC infrastructure. This paper investigates the issues of multi-objective cooperative computation offloading for MEC in UAVs hybrid Networks. The proposed UAVs hybrid MEC system enables edge-cloud and UAVs cooperation to address the flexible limitations of conventional MEC infrastructure and the efficient computation offloading of computation task. To support good-quality services in a cost-effective manner, we model the computation offloading problem as a multi-objective optimization process, and propose an intelligent computation offloading algorithms based on integrated optimization framework, including mixed integer transformation solving framework, improved multi-adaptive MOEA/D-DE(MOEA/D-MSDE) and Grey Relational Projection (GRP). Evaluation results show that the proposed algorithms outperform in solving multi-objective cooperative computation offloading problem in terms of service time-delay, energy-consumption and server-cost.},
  archive      = {J_COMCOM},
  author       = {Zheng Yao and Huaiyu Wu and Yang Chen},
  doi          = {10.1016/j.comcom.2023.01.006},
  journal      = {Computer Communications},
  pages        = {124-134},
  shortjournal = {Comput. Commun.},
  title        = {Multi-objective cooperative computation offloading for MEC in UAVs hybrid networks via integrated optimization framework},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint QoS and energy-efficient resource allocation and
scheduling in 5G network slicing. <em>COMCOM</em>, <em>202</em>,
110–123. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Slicing (NS) is fast evolving as a prominent enabler for providing tailored services in the Fifth Generation of cellular networks (5G). Network Slices are virtualized network entities formed over physical substrates, deployed for the customized application use cases. A Network Slice needs to exhibit end to end capabilities and meet Quality of Service (QoS) specifications and Service Level Agreements (SLAs). To provide end-to-end traffic management capabilities in the network slice, firstly, traffic flows are categorized into different priority traffic classes, and their severity levels are assessed. Priorities can be applied across cellular and IP based systems. Machine Learning (ML) algorithms are employed on QoS profile attributes in establishing traffic priorities in slices. Secondly, we propose a novel algorithm for NS Resource Partitioning and User Allocation. We put forward an online virtual backbone based solution for resource allocation and priority class-based packet scheduling . This joint QoS and energy efficiency driven approach is built on top of established traffic classes and dynamic power savings techniques. Finally, through Cognitive Cycles (CC), we devise better network re-configuration to obtain more energy savings . Traffic classifier modules are implemented using Jupyter notebook and Python API. Scheduling and resource allocation modules in networks slices are emulated in Mininet, Flowvisor, and Beacon and POX controllers. The simulation results reveal the reduced node consumption is achieved through the evolutionary CC algorithm, and it outperforms other standard approaches by at least 23\%. Similarly, for the traffic priority prediction, from the results, we could infer Gradient Boosting and Random Forest Regressors exhibit superior accuracy with the root mean square deviation of 2.2\% and 1.2\% respectively when compared to other standard ML algorithms.},
  archive      = {J_COMCOM},
  author       = {Saibharath S. and Sudeepta Mishra and Chittaranjan Hota},
  doi          = {10.1016/j.comcom.2023.02.009},
  journal      = {Computer Communications},
  pages        = {110-123},
  shortjournal = {Comput. Commun.},
  title        = {Joint QoS and energy-efficient resource allocation and scheduling in 5G network slicing},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal service decomposition for mobile augmented reality
with edge cloud support. <em>COMCOM</em>, <em>202</em>, 97–109. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile augmented reality (MAR) applications are starting to attract significant attention due to the enhanced capabilities stemming from both the network and the end devices that propel their realization. However, despite the progress on the end user devices , MAR applications are inherently hugely demanding in terms of computational and memory requirements since they combine, inter alia , video streams, computer generated images, intense computer vision algorithms and geolocation . To this end, edge cloud computing is envisioned as a key technology for supporting such applications where part of the computationally demanding algorithms could be offloaded to suitably selected edge clouds. Within that context the inherent user mobility should be considered to allow an efficient service continuum between edge and the end-terminal. To this end, in this paper, an optimal edge cloud resource MAR service decomposition is presented that takes explicitly into account the AR service composition as well as the inherent user mobility to proactively allocate resources to satisfy the required strict latency and frame accuracy requirements of MAR applications. In addition to the optimal decision making using mathematical programming , and as a mean to provide real-time decision making two advanced heuristic techniques are proposed. A Simulated Annealing based mobility aware AR algorithm (SAMAR) is developed to enhance computing efficiency and a Long Short-Term Memory (LSTM) neural network which is trained offline with optimal solutions. Numerical investigations reveal that significant gains can be achieved by the proposed schemes compare to a number of baseline previously proposed techniques.},
  archive      = {J_COMCOM},
  author       = {Zhaohui Huang and Vasilis Friderikos},
  doi          = {10.1016/j.comcom.2023.02.002},
  journal      = {Computer Communications},
  pages        = {97-109},
  shortjournal = {Comput. Commun.},
  title        = {Optimal service decomposition for mobile augmented reality with edge cloud support},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic collaborative optimization of end-to-end delay and
power consumption in wireless sensor networks for smart distribution
grids. <em>COMCOM</em>, <em>202</em>, 87–96. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor network (WSN) technology is poised to be widely adopted in the smart distribution grid (SDG), which has strict requirements regarding communication delays. However, delays in WSNs are easily affected by dynamic interference factors (such as channel access competition, transmitting power, and node failure), and these dynamic characteristics make the traditional offline optimization methods unsuitable. Besides, the reinforcement learning (RL) based online optimization methods have dimension explosion and convergence problems . In this paper, we propose a dynamic collaborative optimization of the end-to-end delay and power consumption of the WSNs based on grouped RL. In particular, we first build an environment model for evaluating the values of the optimization objective . Those values are used to calculate the rewards for the RL algorithm. To accelerate the convergence of RL swamped by the dimensions of the action space, a novel grouped RL is proposed. Then iterative learning is performed to balance the end-to-end delay and power consumption by adjusting the transmitting power of each node. The simulation results show that the proposed algorithm is able to meet the SDG delay requirements with low power consumption when the communication is dynamically affected. The developed algorithm achieves a maximum end-to-end delay reduction of 20.3\% and a computational cost reduction of 6.2\% to 52.7\% compared with the other two RL algorithms.},
  archive      = {J_COMCOM},
  author       = {Wei Sun and Lei Zhang and Qiushuo Lv and Zhi Liu and Weitao Li and Qiyue Li},
  doi          = {10.1016/j.comcom.2023.02.016},
  journal      = {Computer Communications},
  pages        = {87-96},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic collaborative optimization of end-to-end delay and power consumption in wireless sensor networks for smart distribution grids},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning based flow and energy management in
resource-constrained wireless networks. <em>COMCOM</em>, <em>202</em>,
73–86. (<a href="https://doi.org/10.1016/j.comcom.2023.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at developing a learning-based framework for MAC sleep–listen–transmit scheduling in wireless networks. The Reinforcement Learning-based paradigm is shown to work in the absence of network time-synchronization and other complex hardware features, such as carrier-sensing, thus making it suitable for low-cost transceivers for IoT and wireless sensor nodes . The framework allows wireless nodes to learn policies that can support throughput-sustainable flows while minimizing node energy expenditure and sleep-induced packet drops and delays. Each node independently learns a scheduling policy without explicit communication with other network nodes. The trade-off between packet drops and energy efficiency is analyzed, and an application-specific solution is proposed for handling the trade-off. It is shown how this model allows users to prioritize between energy efficiency and packet drops depending on specific application requirements. An analytical model is developed for understanding the underlying system dynamics , which is also validated using extensive simulation experiments. Moreover, the developed mechanism is experimented with for heterogeneous network topologies and traffic patterns},
  archive      = {J_COMCOM},
  author       = {Hrishikesh Dutta and Amit Kumar Bhuyan and Subir Biswas},
  doi          = {10.1016/j.comcom.2023.02.011},
  journal      = {Computer Communications},
  pages        = {73-86},
  shortjournal = {Comput. Commun.},
  title        = {Reinforcement learning based flow and energy management in resource-constrained wireless networks},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Batch classifier with adaptive update for backbone traffic
classification. <em>COMCOM</em>, <em>202</em>, 57–72. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network traffic analysis is an important method for ISPs to know the status of the network. However, the high speed and rapid evolution of backbone network traffic have brought new challenges to traffic analysis. Most existing traffic classifiers are based on full traffic, while processing all the large-scale backbone network traffic is time-intensive and resource-consuming. Moreover, the relationship between feature vectors and classification labels drifts with the rapid evolution of backbone network traffic, which leads to the degradation of the performance of the classification model . This paper presents a model that can implement real-time classification of sampled backbone network traffic and adaptively update the classification model when concept drift occurs. To accurately classify the backbone network traffic in real time, we propose the Multiple Counter Sketch (MC Sketch) to quickly extract feature vectors from the sampled traffic and design an Adaptive Batch Classifier based on Agglomerative Clustering (A-BCAC) to perform unsupervised batch clustering of feature vectors. Using the labels obtained from A-BCAC, we trained the classifier to classify the backbone traffic. In addition, we design an adaptive batch update model based on concept drift detection to solve the concept drift problem. The experimental results on sampled traffic collected on a 10Gbps link and the comparisons with state-of-the-art studies demonstrate the performance and efficiency of our method.},
  archive      = {J_COMCOM},
  author       = {Hua Wu and Weina Li and Xiying Chen and Guang Cheng and Xiaoyan Hu and Youqiong Zhuang},
  doi          = {10.1016/j.comcom.2023.02.013},
  journal      = {Computer Communications},
  pages        = {57-72},
  shortjournal = {Comput. Commun.},
  title        = {Batch classifier with adaptive update for backbone traffic classification},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Device discovery and tracing in the bluetooth low energy
domain. <em>COMCOM</em>, <em>202</em>, 42–56. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bluetooth Low Energy (BLE) is a pervasive wireless technology all around us today. It is included in most commercial consumer electronic devices manufactured in the last years, and billions of BLE-enabled devices are produced every year, mostly wearable or portable ones like smartphones, smartwatches, and smartbands. The success of BLE as a cornerstone in the Internet of Things (IoT) and consumer electronics is both an advantage, enabling short range, low cost, and low power consumption wireless communications , and a disadvantage, from a security and privacy standpoint. BLE exposes packets that enable a potential attacker to detect, enquire and fingerprint actual devices despite manufacturers’ attempts to avoid detection and tracking. Medium Access Control (MAC) address randomization was introduced in the BLE standard to solve some of these issues. In this paper we discuss how to detect and fingerprint BLE devices, basing our analysis and data collection on interactions allowed by the standard. In our study, we propose the Bluetooth Low Energy Nodes Detect, Enquire, (and) Recognition (BLENDER) framework for enumerating and fingerprinting BLE devices for crowd monitoring and recognition purposes, based on four different strategies used to analyze BLE-enabled devices. We will show that it is possible to associate BLE randomized MAC addresses to actual devices. We will then describe a proof of concept for large-scale data collection. In addition, to determine the spots where the stations could be optimally positioned, we created a synthetic dataset based on mobility models and then we emulated the BLENDER approach. The latter allowed training Machine Learning models to predict the expected number of devices appearing at any particular position, day, and hour.},
  archive      = {J_COMCOM},
  author       = {Pierluigi Locatelli and Massimo Perri and Daniel Mauricio Jimenez Gutierrez and Andrea Lacava and Francesca Cuomo},
  doi          = {10.1016/j.comcom.2023.02.008},
  journal      = {Computer Communications},
  pages        = {42-56},
  shortjournal = {Comput. Commun.},
  title        = {Device discovery and tracing in the bluetooth low energy domain},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pilot spoofing detection for massive MIMO mmWave
communication systems with a cooperative relay. <em>COMCOM</em>,
<em>202</em>, 33–41. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In massive millimeter-wave (mmWave) multiple-input–multiple-output (MIMO) systems, eavesdroppers may transmit the identical pilot sequences as legitimate users to manipulate channel estimation in pilot phase and afterwards intercept the transmitted information in data transmission stage, constituting a severe attack for secure communications. In this paper, for a three-terminal massive mmWave MIMO system with a cooperative relay, the achievable secrecy rate under the presence of a pilot spoofing attack is investigated, and then an accurate detection scheme is proposed. Specifically, in the first slot of the uplink pilot stage, the full-duplex relay sends pilot sequence to the base station simultaneously when it receives the transmitted pilot signal from the user, through which the CSI from the relay to the base station is obtained. In the second slot, after the received pilot signal has been forwarded from the cooperative relay, the equivalent CSI from the base station to the user is obtained, and the joint CSI from the user/attacker to the relay is derived, based on which the likelihood ratio method is exploited to detect the potential pilot spoofing attack . Finally, numerical simulation is presented to show the performance of the proposed scheme in terms of miss detection and false detection events.},
  archive      = {J_COMCOM},
  author       = {Shiguo Wang and Xuewen Fu and Rukhsana Ruby and Zhetao Li},
  doi          = {10.1016/j.comcom.2023.02.014},
  journal      = {Computer Communications},
  pages        = {33-41},
  shortjournal = {Comput. Commun.},
  title        = {Pilot spoofing detection for massive MIMO mmWave communication systems with a cooperative relay},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Opportunistic RIS-assisted rate splitting transmission in
coordinated multiple points networks. <em>COMCOM</em>, <em>202</em>,
23–32. (<a href="https://doi.org/10.1016/j.comcom.2023.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable intelligent surfaces (RIS) have attracted a great deal of interests due to its potential contributions to the next-generation mobile networks. The deployment of RIS in multi-user wireless networks promises to reduce system hardware costs, signal processing complexity, as well as energy consumption due to its small size, lightweight and ability to actively shape the wireless propagation environment . Based on the ability to suitably adjust the phase shifts according to the dynamic wireless environment, the transmission rate and energy efficiency can be improved by deploying software-controlled meta-surfaces to reflect signals from the source to the destination. This paper proposes a RIS-assisted rate splitting-based transmission strategy in the Coordinated Multiple Points (CoMP) transmission network . The obtained results demonstrated that the proposed opportunistic RIS-assisted rate splitting transmission outperforms the conventional jointly transmission Non-Orthogonal Multiple Access (NOMA) strategy and the mutual-aided NOMA strategy in terms of outage probability under a target power level when the obstacle exists. Along with the one-off control RIS, the network spectral efficiency can be improved significantly by the proposed opportunistic RIS-assisted (O-RIS) rate splitting strategy under the target channel interference levels.},
  archive      = {J_COMCOM},
  author       = {Yue Tian and Baiyun Xiao and Xianling Wang and Yau Hee Kho and Chen Zhu and Wenda Li and Qinying Li and Xuejie Hu},
  doi          = {10.1016/j.comcom.2023.01.023},
  journal      = {Computer Communications},
  pages        = {23-32},
  shortjournal = {Comput. Commun.},
  title        = {Opportunistic RIS-assisted rate splitting transmission in coordinated multiple points networks},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty quantification and consideration in ML-aided
traffic-driven service provisioning. <em>COMCOM</em>, <em>202</em>,
13–22. (<a href="https://doi.org/10.1016/j.comcom.2023.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network traffic prediction problem has been extensively studied in the literature through the application of statistical linear models and more recently through the application of machine learning (ML). In fact, ML has proven its capabilities on accurately modeling the non-linear nature of network traffic, outperforming conventional statistical linear models. Without doubt, model accuracy constitutes an important evaluation metric . In a network environment, however, where uncertainty may lead to erroneous service provisioning decisions (e.g., violations of the quality-of-service (QoS) requirements), it does not provide any information of how much the traffic predictions can be trusted. Hence, in this work the focus is on addressing traffic prediction uncertainty by leveraging the capabilities of Monte Carlo (MC) dropout inference. The proposed framework is compared with a margin-based technique, traditionally used to compensate for traffic prediction uncertainty, demonstrating that the MC dropout inference framework results in significant spectrum savings, as opposed to the baseline, myopic, margin-based scheme. Even though a small penalty is observed on the unpredictable traffic, this is successfully handled on-line with a reduced operational overhead compared to the case where prediction uncertainty is completely ignored. Importantly, it is shown that, unlike the margin-based framework, the MC dropout inference framework can be used for the provisioning of services with diverse QoS requirements.},
  archive      = {J_COMCOM},
  author       = {Hafsa Maryam and Tania Panayiotou and Georgios Ellinas},
  doi          = {10.1016/j.comcom.2023.02.007},
  journal      = {Computer Communications},
  pages        = {13-22},
  shortjournal = {Comput. Commun.},
  title        = {Uncertainty quantification and consideration in ML-aided traffic-driven service provisioning},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task offloading in multiple-services mobile edge computing:
A deep reinforcement learning algorithm. <em>COMCOM</em>, <em>202</em>,
1–12. (<a href="https://doi.org/10.1016/j.comcom.2023.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple-Services Mobile Edge Computing enables task-relate services cached in edge server to be dynamically updated, and thus provides great opportunities to offload tasks to edge server for execution. However, the requirements and popularity of services, the computing requirement and the amount of data transferred from users to edge server are dynamic with time. How to adaptively adjust the subset of total service types in the resource-limited edge server and determine the task offloading destination and resource allocation decisions to improve the overall system performance is a challenge problem. To solve this challenge, we firstly convert it into a Markov decision process , then propose a soft actor–critic deep reinforcement learning-based algorithm, called DSOR, to jointly determine not only the discrete decisions of service caching and task offloading but also the continuous allocation of bandwidth and computing resource. To improve the accuracy of our algorithm, we employ an efficient trick of converting the discrete action selection into a continuous space to deal with the key design challenge that arises from continuous-discrete hybrid action space. Additionally, to improve resource utilization, a novel reward function is integrated to our algorithm to speed up the convergence of training while making full use of system resources. Extensive numerical results show that compared with other baseline algorithms, our algorithm can effectively reduce the long-term average completion delay of tasks while accessing excellent performance in terms of stability.},
  archive      = {J_COMCOM},
  author       = {Ziyu Peng and Gaocai Wang and Wang Nong and Yu Qiu and Shuqiang Huang},
  doi          = {10.1016/j.comcom.2023.02.001},
  journal      = {Computer Communications},
  pages        = {1-12},
  shortjournal = {Comput. Commun.},
  title        = {Task offloading in multiple-services mobile edge computing: A deep reinforcement learning algorithm},
  volume       = {202},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network architecture and ROA protection of government mail
domains: A case study. <em>COMCOM</em>, <em>201</em>, 143–161. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Email is a crucial technology used in daily interactions of citizens, enterprises and organizations with their respective governments. In this work we are concerned with the country-wide network architecture of mail domains of public administrations . We analyze a dataset of government mail domains in Italy, Germany , the United Kingdom and the United States of America in order to investigate the opportunities for a network attacker to violate security properties of email communication, including availability, in large portions of a country. Issues of this kind are particularly relevant in times of high international tension and in which every country should treat its networks as a potential target for other countries. We define a framework for describing the opportunities for a network attacker in the resolution of mail domain names, resolution of mail server names, access to a mail server. Based on this framework, we investigate in detail a number of issues related to redundancy and distribution of dependencies among networks and autonomous systems . We also analyze the usage in the access to mail domains of Route Origin Authorization (ROA) , an important defensive technology for detecting attacks at the IP routing level. Our analysis allows gaining important insights into the actual network architecture of such an important piece of critical infrastructure as government mail domains.},
  archive      = {J_COMCOM},
  author       = {Alberto Bartoli},
  doi          = {10.1016/j.comcom.2023.02.004},
  journal      = {Computer Communications},
  pages        = {143-161},
  shortjournal = {Comput. Commun.},
  title        = {Network architecture and ROA protection of government mail domains: A case study},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improvement of blockchain-based multi-layer location data
sharing scheme for internet of things. <em>COMCOM</em>, <em>201</em>,
131–142. (<a
href="https://doi.org/10.1016/j.comcom.2023.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of the Internet of Things , user location data sharing has attracted increasing attention in various fields. However, current state-of-the-art data sharing system rely on centralized servers. This can create a single point of failure , with deleted data being unrecoverable. In addition, when performing data integrity verification , present schemes are only applicable to static scenarios. Dynamic update scenarios will incur enormous computational and communication costs. In this paper, we design a multi-layer location sharing scheme based on blockchain , which solves the single point of failure problem well. Our proposed scheme can also be adapted to dynamic scenarios. We utilize an accumulator to make our scheme more efficient in data updating and verification. The scheme also has a constant communication cost. Finally, we adopt a formal security analysis based on Real-Or-Random (ROR), and evaluate the performance of our scheme. The experimental results show that our scheme in comparison with existing methods, finding it to be superior in terms of computational cost, communication cost, safety, and efficiency.},
  archive      = {J_COMCOM},
  author       = {Hongyang Liu and Hui Huang and Yuping Zhou and Qunshan Chen},
  doi          = {10.1016/j.comcom.2023.02.005},
  journal      = {Computer Communications},
  pages        = {131-142},
  shortjournal = {Comput. Commun.},
  title        = {Improvement of blockchain-based multi-layer location data sharing scheme for internet of things},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Allocation of edge computing tasks for UAV-aided target
tracking. <em>COMCOM</em>, <em>201</em>, 123–130. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV)-aided target tracking has been applied to many practical scenarios such as search and rescue missions. Edge computing has emerged as a promising solution to improve the performance of UAV-aided target tracking and to facilitate computational offloading from the UAV to the edge nodes (ENs). However, due to the mobility of UAVs as well as the limited energy and coverage of ENs, the design of offloading policies remains challenging. To address these challenges, this paper studies the problem of UAVs tasks allocation by jointly considering which EN to execute an arriving video task and how to adjust the transmit power of the UAV to achieve successful tracking. The problem is modeled as a Markov Decision Process (MDP), which attempts to balance energy cost and time cost. We propose a Q-learning based approach to solve this problem. Numerical simulation results demonstrate that compared with baseline methods , our algorithm can achieve significant improvement.},
  archive      = {J_COMCOM},
  author       = {Xiaoheng Deng and Jun Li and Ying Ma and Peiyuan Guan and Haichuan Ding},
  doi          = {10.1016/j.comcom.2023.01.021},
  journal      = {Computer Communications},
  pages        = {123-130},
  shortjournal = {Comput. Commun.},
  title        = {Allocation of edge computing tasks for UAV-aided target tracking},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach for flow analysis in software-based
networks using l-moments theory. <em>COMCOM</em>, <em>201</em>, 116–122.
(<a href="https://doi.org/10.1016/j.comcom.2023.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous increase in the number of devices connected to the Internet, together with the growth of applications and services, has made the tasks of network traffic analysis and classification essential in any environment. The deployment of 5G networks has prompted the research community to establish the pillars of Next-Generation Networks. These include intelligent systems, providing the network with intelligence in management and security tasks. In addition, these tasks require mechanisms capable of characterizing traffic in order to make network decisions. In this context, this paper proposes a novel methodology for processing network traffic using the L-moments theory and Machine Learning algorithms . This methodology is robust to outliers, requires few data to characterize the flows and subsequently fit the classification models . The results show that L-moments are particularly useful for processing network flows, and the classification algorithms obtain very high-quality results. Moreover, we show that the considered statistical tools also allow for a better understanding of the attack behaviour, leading the way to the improvement of the feature selection in similar problems.},
  archive      = {J_COMCOM},
  author       = {Jesús Galeano-Brajones and Mihaela I. Chidean and Francisco Luna and Javier Carmona-Murillo},
  doi          = {10.1016/j.comcom.2023.01.022},
  journal      = {Computer Communications},
  pages        = {116-122},
  shortjournal = {Comput. Commun.},
  title        = {A novel approach for flow analysis in software-based networks using L-moments theory},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparative study on consensus mechanism with security
threats and future scopes: blockchain. <em>COMCOM</em>, <em>201</em>,
102–115. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, blockchain has emerged as a peer-to-peer (P2P) immutable distributed ledger technology-based network, and its consensus mechanism is playing an important role in managing decentralized data. The use of the consensus process for open blockchains, on the other hand, has revitalized the industry and spurred new architectures. As part of this research, we have analyzed and categorized a wide range of consensus mechanisms in order to contribute to the process of developing blockchain applications and determining the extent of their influence. Critical technical considerations regarding performance, scalability, and other quality aspects that a blockchain-based system must possess may be assisted by this scientific categorization and comprehensive comparison of consensus algorithms . In this paper, we have mainly depicted: (i) proof-of-work (PoW) along with its enhancement rules like greedy heaviest-observed sub-tree (GHOST) and Bitcoin NG (next generation); (ii) proof-of-Stake (PoS) along with chain-based PoS, committee-based PoS, byzantine fault tolerance (BFT) based extended proof-of-stake (EPoS), and delegated proof of stake (DPoS), i.e. energy-efficient alternative to PoW; (iii) BFT consensus to handle crash fault and Byzantine fault ; and (iv) hybrid protocols , which are a combination of the best features of PoW, PoS, and BFT consensus mechanism . To evaluate the performance, scalability, security, and design properties of the system, we have considered five components of consensus algorithms .},
  archive      = {J_COMCOM},
  author       = {Ashok Kumar Yadav and Karan Singh and Ali H. Amin and Laila Almutairi and Theyab R. Alsenani and Ali Ahmadian},
  doi          = {10.1016/j.comcom.2023.01.018},
  journal      = {Computer Communications},
  pages        = {102-115},
  shortjournal = {Comput. Commun.},
  title        = {A comparative study on consensus mechanism with security threats and future scopes: Blockchain},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent scheduling framework for DNN task
acceleration in heterogeneous edge networks. <em>COMCOM</em>,
<em>201</em>, 91–101. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the upgrade of hardware architecture and device capacities , many accelerator-based hardware platforms have been widely deployed in Mobile Edge Computing (MEC) environments. The execution time of many computation-intensive applications (e.g., face recognition and pedestrian detection) can be significantly reduced when deployed on these heterogeneous devices. Moreover, thanks to the popularity of Deep Learning (DL), most terminal applications are integrated with Deep Neural Networks (DNN) and can be divided into interdependent tasks. The structure of these applications can be represented as the Directed Acyclic Graph (DAG). Therefore, it is critical to seek the optimal scheduling order and execution placement of tasks according to the acceleration effects of edge servers and the task dependency . However, conventional scheduling strategies focus on the short-term performance, potentially leading to service quality degradation in the long term. Besides, many studies use Deep Reinforcement Learning (DRL) algorithms to seek a long-term optimal scheduling strategy but ignore the device acceleration and the task dependency . Furthermore, training a well-performed DRL agent is time-consuming, and the large scale of trial-and-error will take up tremendous computation and storage resources. In this paper, we model the scheduling process as a Markov Decision Process (MDP) and design an adaptive scheduling framework for task acceleration. Fully considering the data dependencies , resource conditions, and network conditions, the proposed scheduling algorithm called Meta-AC uses policy gradient combined with meta-learning to minimize the average task delay and the ratio of time-out tasks. As a hierarchical DRL algorithm, Meta-AC uses meta data to learn directed exploration strategies in the high-level agent, improving the learning efficiency from experience samples. Extensive simulations demonstrate the superiority of the proposed method over the counterpart methods.},
  archive      = {J_COMCOM},
  author       = {Yiming Feng and Shihong Hu and Lingqiang Chen and Guanghui Li},
  doi          = {10.1016/j.comcom.2023.01.019},
  journal      = {Computer Communications},
  pages        = {91-101},
  shortjournal = {Comput. Commun.},
  title        = {An intelligent scheduling framework for DNN task acceleration in heterogeneous edge networks},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Routing and slot allocation in 5G hard slicing.
<em>COMCOM</em>, <em>201</em>, 72–90. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current network slicing solutions suffer from poor inter-slice isolation, as the performance of one slice can be influenced by the traffic in other slices. New technologies such as Flex Ethernet can offer hard isolation via dedicated resources at the physical and MAC layers. However, to create cost-efficient hard slices in large 5G access networks, a “routing and slot allocation” must be solved quickly. While the underlying network design problem is not new, two extra constraints need to be considered: a specific order in slot activations and a bandwidth allocation policy with statistical multiplexing . We propose a compact and extended formulation to derive FlexE-CG, an algorithm based on column-generation to solve large instances. We reinforce the extended formulation to improve the lower bound by deriving valid inequalities , and we provide necessary and sufficient conditions under which the inequalities are facet-defining. We show that these inequalities improve the lower bound by more than 20\% on various IP-Radio Access Networks (RAN). We also show that FlexE-CG can provide solutions within an optimality gap of 10\% in a few minutes.},
  archive      = {J_COMCOM},
  author       = {Nicolas Huin and Jérémie Leguay and Sébastien Martin and Paolo Medagliani},
  doi          = {10.1016/j.comcom.2023.01.008},
  journal      = {Computer Communications},
  pages        = {72-90},
  shortjournal = {Comput. Commun.},
  title        = {Routing and slot allocation in 5G hard slicing},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A steiner tree based efficient network infrastructure design
in 5G urban vehicular networks. <em>COMCOM</em>, <em>201</em>, 59–71.
(<a href="https://doi.org/10.1016/j.comcom.2023.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous or assisted driving under next-generation vehicular applications requires a very high data rate with ultra-reliable low latency. The design of a proper network infrastructure plays a primary role in supporting these demands. This necessitates the network infrastructure nodes, i.e., the edge nodes and 5G RSUs, to be efficiently placed and connected across a smart city in a cost-effective manner. Such planning under a restricted capital expenditure budget presents a significant research challenge. We address this issue by formulating it as a Steiner Tree with Profits, Budget and Hop constraint problem for designing cost-efficient network infrastructure in an urban vehicular scenario . The optimization maximizes the long-term profits under the budget and hops constraint. The problem under consideration is a known NP-hard problem, and therefore, we have employed a Breakout Local Search based heuristic to solve it. Performance evaluation with actual traffic in real cities with diverse setups reveals crucial insights for designing cost-efficient network infrastructure in 5G urban vehicular networks .},
  archive      = {J_COMCOM},
  author       = {Moyukh Laha and Raja Datta},
  doi          = {10.1016/j.comcom.2023.01.016},
  journal      = {Computer Communications},
  pages        = {59-71},
  shortjournal = {Comput. Commun.},
  title        = {A steiner tree based efficient network infrastructure design in 5G urban vehicular networks},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empirical blockage characterization and detection in indoor
sub-THz communications. <em>COMCOM</em>, <em>201</em>, 48–58. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next step in the last mile wireless access is utilization of the terahertz (THz) frequency band spanning from 0.1 to 3 THz, specifically, its lower part (up to 300 GHz) also known as sub-THz frequencies. At these frequencies, communication systems can offer tens of consecutive gigahertz potentially allowing to further improve access rates at the air interface to dozens of gigabits-per-second. However, the effect of blockage evident already at millimeter waves is expected to be much more pronounced at frequencies beyond 100 GHz. In this paper, we empirically investigate the characteristics of the human body blockage by conducting a measurement campaign at carrier frequency of 156 GHz in the indoor environment. We concentrate on both mean attenuation and time-related metrics of the blockage process including the signal fade and rise times, and blockage duration. For a point-to-point transmission over a distance of 3–7 m, we find that the mean attenuation is in the range of 8–15 dB depending on the line-of-sight (LoS) height and the transmitter-to-receiver (Tx-to-Rx) distance. The blockage duration varies within 5\%–10\% for different Tx-to-Rx distances (with corresponding nominal values of 360–390 ms) while the signal rise and fall times gradually increase from 60 to 100 ms with the Tx-to-Rx distance growth and remain unchanged for different LoS heights. The developed blockage detection algorithm allows one to identify the blockage occurrence with a probability of 0.96–0.98 within 1–3 ms at the channel sampling rate of 500 ksample/s and 3–5 event/s of false alarm rate which is on par with modern machine learning based approaches .},
  archive      = {J_COMCOM},
  author       = {Alexander Shurakov and Dmitri Moltchanov and Anatoliy Prikhodko and Abdukodir Khakimov and Evgeny Mokrov and Vyacheslav Begishev and Ivan Belikov and Yevgeni Koucheryavy and Gregory Gol’tsman},
  doi          = {10.1016/j.comcom.2023.01.017},
  journal      = {Computer Communications},
  pages        = {48-58},
  shortjournal = {Comput. Commun.},
  title        = {Empirical blockage characterization and detection in indoor sub-THz communications},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed deep learning-based signal classification for
time–frequency synchronization in wireless networks. <em>COMCOM</em>,
<em>201</em>, 37–47. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel distributed deep learning (DL) network for signal classification to achieve accurate time–frequency synchronization in wireless communication networks. Restricted by the non-random signal generated by the transmitter, the existing time–frequency synchronization algorithms cannot obtain reliable synchronization results. With the development of DL, feature-based signal classification has become an effective way to improve the precision of time–frequency synchronization. Considering the multipath effect in the practical propagation environments , a distributed DL network for wireless communication is proposed, where the received signal is classified by the distributed DL network. Specifically, preprocessed images are obtained by applying the discrete Fourier transform (DFT) and short-time Fourier transform (STFT) to facilitate the signal feature extraction. Convolutional neural network (CNN) architecture is designed for feature extraction on the preprocessed images. Meanwhile, a feature fusion module based on attention mechanisms is proposed to fuse data adaptively and obtain the final signal classification results . Simulation results show that the proposed distributed DL network can achieve higher classification accuracy than traditional DL networks under multiple modulation schemes . More importantly, reliable signal classification can effectively improve the performance of time–frequency synchronization in wireless communication networks.},
  archive      = {J_COMCOM},
  author       = {Qin Zhang and Yutong Guan and Hai Li and Kanghua Xiong and Zhengyu Song},
  doi          = {10.1016/j.comcom.2023.01.014},
  journal      = {Computer Communications},
  pages        = {37-47},
  shortjournal = {Comput. Commun.},
  title        = {Distributed deep learning-based signal classification for time–frequency synchronization in wireless networks},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multipath transmission aware ABR algorithm for SVC HAS.
<em>COMCOM</em>, <em>201</em>, 20–36. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive video streaming over Hypertext Transfer Protocol (HTTP) is one of the most popular applications that have been used on the Internet in the last decade. In such applications, clients change the quality of the received video throughout the streaming session with respect to current network conditions. In these systems, the use of scalable coded video is one of the alternatives that can obtain video files encoded at different qualities. In this paper, we propose an adaptive bitrate (ABR) algorithm that aims to improve the quality of not only future segments but also the segments in the buffer. The quality of the already buffered segments is improved by downloading additional layers of the SVC-coded video, hence, preventing network resources from being wasted while increasing Quality of Experience (QoE). We use multiple paths between the server and the clients for transferring the video packets. In such systems, the ABR algorithm should determine which segment and layer should be requested from which path and when. The proposed ABR algorithm selects a group of segments to increase the quality of the buffered segments and for future segments, and selects paths for each segment in this group. Simulation results show that clients achieve higher QoE by using our approach when compared with conventional ABR algorithms. Furthermore, we show that using multiple paths cannot guarantee that QoE is maximized unless the end-system is aware of multipath transmission.},
  archive      = {J_COMCOM},
  author       = {Simge Gizem Ozcan and Muge Sayit},
  doi          = {10.1016/j.comcom.2023.01.015},
  journal      = {Computer Communications},
  pages        = {20-36},
  shortjournal = {Comput. Commun.},
  title        = {Multipath transmission aware ABR algorithm for SVC HAS},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint energy and throughput optimization for MEC-enabled
multi-UAV IoRT networks. <em>COMCOM</em>, <em>201</em>, 1–19. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study an Unmanned Aerial Vehicle (UAV) enabled Mobile Edge Computing (MEC) service provisioning to the Internet of Remote Things (IoRT) devices spread randomly on the ground in a remote area. The data generated by the IoRT devices is collected by the UAVs, which immediately relay the data collected to an MEC device installed on the ground at a nearby location. The MEC device receives the data from the UAVs, and sends the results back to the UAVs, which in turn relay them to IoRT devices. We aim to minimize the energy consumption by the IoRT devices and the UAVs, while maximizing the system throughput subject to bandwidth, power, information-causality, and UAVs’ trajectory constraints. We formulate the problem as a Mixed Integer Non Linear Programming problem , which is a complex and non-convex optimization problem . To make the problem tractable, we use variable relaxation. We further develop an iterative algorithm based on Block Coordinate Descent method, to jointly optimize the connection scheduling, power control, bit transmission scheduling, bandwidth allocation , and trajectories of the UAVs. Numerical results demonstrate the convergence of the algorithm and superiority of the proposed model with respect to conventional methods. Our proposed system model of placing MEC at ground shows 9\% improvement in energy consumption when compared to carrying out computations at MEC carried by UAV and a 99\% improvement when compared to placing MEC at the satellite. The proposed system model shows a 0.2\% lower system throughput on average, compared to placing MEC at UAV, which is tolerable considering gains in terms of energy consumption.},
  archive      = {J_COMCOM},
  author       = {Sriharsha Chigullapally and C. Siva Ram Murthy},
  doi          = {10.1016/j.comcom.2023.01.012},
  journal      = {Computer Communications},
  pages        = {1-19},
  shortjournal = {Comput. Commun.},
  title        = {Joint energy and throughput optimization for MEC-enabled multi-UAV IoRT networks},
  volume       = {201},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous collaborative combat strategy of unmanned system
group in continuous dynamic environment based on PD-MADDPG.
<em>COMCOM</em>, <em>200</em>, 182–204. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we studied the Unmanned System Group (USG) Autonomous Collaborative Combat Strategy (ACCS) and the Parallel Decoupling-Multi-agent Deep Deterministic Policy Gradient (PD-MADDPG) algorithm was proposed. This was conducted on the background of USG confrontation game problem between ourselves and the enemy in the continuous dynamic environment of future aerial combat. An independent Parallel Benchmark Critic (PB-Critic) network and Parallel Decoupling Critic (PD-Critic) network for each member of USG was developed to maximize the whole reward and member reward of USG under time parallelism . By integrating Forerunner Mechanism (FM) into the parallel decoupling reward function, the reward sparse problem confronted by parallel decoupling reward function at the beginning of the algorithmic training was controlled as well as the convergence efficiency of USG in the continuous dynamic environment of future aerial combat. Introduction of Symmetric Attention Mechanism (SAM) to the Critic-network and Actor-network shortened the screening radius of helpful information related to the confrontation game of USG of both sides of ourselves and enemy in the continuous dynamic environment. Assuming the multiple typical confrontation game control strategies of USG of the enemy in the continuous dynamic environment, the effectiveness and feasibility of the strategies were simulated and verified. The PD-MADDPG improved the multiple inadequate natural endowments of Multi-Agent Deep Deterministic Policy Gradient (MADDPG) which showed that the policy training convergence and execution stability were greatly enhanced, and USG’s behavioral autonomy for collaborative combat in the continuous dynamic environment was further improved.},
  archive      = {J_COMCOM},
  author       = {Zhenhua Wang and Yan Guo and Ning Li and Shiguang Hu and Meng Wang},
  doi          = {10.1016/j.comcom.2023.01.009},
  journal      = {Computer Communications},
  pages        = {182-204},
  shortjournal = {Comput. Commun.},
  title        = {Autonomous collaborative combat strategy of unmanned system group in continuous dynamic environment based on PD-MADDPG},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IBA: A secure and efficient device-to-device
interaction-based authentication scheme for internet of things.
<em>COMCOM</em>, <em>200</em>, 171–181. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) enables a variety of services through frequent communication and interaction between devices, bringing a great deal of benefits to human life. The complexity and variability of network environments, together with the diversity of IoT devices introduce a large range of security and efficiency concerns to device authentication . In this paper, a novel interaction-based authentication (IBA) scheme is proposed to respond to the above problems. Device characteristics that can be used as identity credentials are extracted from previous interactions, and secure authentication is realized through elements matching. IoT devices can independently authenticate the identity of other devices without persistent connection to trusted third parties. From the informal analysis and simulation results, IBA supports dynamic adaptability, and high scalability, and resists several well-known attacks, such as reply attacks, impersonation attacks, and man-in-the-middle attacks. The comparison analysis indicates that IBA has better security and performance compared with related authentication schemes, thereby demonstrating it is more suitable for the actual IoT environment.},
  archive      = {J_COMCOM},
  author       = {Shuo Yang and Xinran Zheng and Guining Liu and Xingjun Wang},
  doi          = {10.1016/j.comcom.2023.01.013},
  journal      = {Computer Communications},
  pages        = {171-181},
  shortjournal = {Comput. Commun.},
  title        = {IBA: A secure and efficient device-to-device interaction-based authentication scheme for internet of things},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCMA-TPGAN: A new perspective on sparse codebook multiple
access for UAV system. <em>COMCOM</em>, <em>200</em>, 161–170. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As more and more devices are connected to wireless networks, channel resources are getting tighter and tighter. UAVs , as a new type of mobile device , will play a larger role in the future and will require a larger connection size. Sparse codebook multiple access as a non-orthogonal technique in the code domain can exactly solve this problem. In order to improve the bit error rate performance of sparse code multiple access (SCMA) access systems in uplink Rayleigh fading channels , we propose a new deep learning-based SCMA encoding and decoding scheme. At the SCMA encoder, the Transformer is used as the generator of the generative adversarial network model (GAN), in order to solve the problem that the sequence of the generator is too long in the information processing process. An additional noise layer is introduced at the input of the encoder, resulting in a robust representation of the encoder output, improving the noise immunity of the system. At the decoder, PatchGAN is used as the discriminator of the generative adversarial network to reduce the amount of network model parameters and computation. Simultaneous equalization network and multi-user detection network constitute the decoder network. At the same time, an attention mechanism is added between the generator and the discriminator of the generative adversarial network model to enhance the details of the information part and improve the bit error rate performance. The scheme proposed in this paper is composed of a network-assisted decoder balanced network and an autoencoder-based generative contradiction network (SCMA-TPGAN). Through experimental modeling and comparative analysis of different methods, we conclude that SCMA-TPGAN can reduce detection time and improve system bit error rate performance.},
  archive      = {J_COMCOM},
  author       = {Chao Duan and Shuyue Zhang and Panpan Yin and Xingxing Li and Jun Luo},
  doi          = {10.1016/j.comcom.2023.01.005},
  journal      = {Computer Communications},
  pages        = {161-170},
  shortjournal = {Comput. Commun.},
  title        = {SCMA-TPGAN: A new perspective on sparse codebook multiple access for UAV system},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A privacy-dependent condition-based privacy-preserving
information sharing scheme in online social networks. <em>COMCOM</em>,
<em>200</em>, 149–160. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy leakage resulting from information sharing in online social networks (OSNs) is a serious concern for individuals. One of the culprits behind the problem is that existing privacy policies developed for OSNs are not fine-grained or flexible enough, resulting in privacy settings that could hardly meet the privacy requirements of individuals. Neither would such privacy settings allow individuals to control where the information could go. In addition, there are hardly any effective mechanisms for measuring potential threats to privacy during information propagation. To alleviate the situation, in this paper, we propose a novel privacy-preserving information sharing scheme for OSNs in which information flow can be controlled according to the privacy requirements of the information owner and the context of the information flow. Specifically, we first formally define the privacy-dependent condition (PDC) for information sharing in OSNs and then design a PDC-based privacy-preserving information sharing scheme (PDC-InfoSharing) to protect the privacy of individuals according to the heterogeneous privacy requirements of individuals as well as the potential threats that they may face. Furthermore, to balance information sharing and privacy protection, the techniques of reinforcement learning is utilized to help individuals reach a trade-off. PDC-InfoSharing would allow the privacy policies for specific information audience to be derived based on PDC to achieve dynamical control of the flow of information. Theoretical analysis proves that the proposed scheme can assist individuals in adopting fine-grained privacy policies and experiment shows that it can adapt to different situations to help individuals achieve the trade-off between information sharing and privacy protection.},
  archive      = {J_COMCOM},
  author       = {Yuzi Yi and Nafei Zhu and Jingsha He and Anca Delia Jurcut and Xiangjun Ma and Yehong Luo},
  doi          = {10.1016/j.comcom.2023.01.010},
  journal      = {Computer Communications},
  pages        = {149-160},
  shortjournal = {Comput. Commun.},
  title        = {A privacy-dependent condition-based privacy-preserving information sharing scheme in online social networks},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical mission replanning for multiple UAV formations
performing tasks in dynamic situation. <em>COMCOM</em>, <em>200</em>,
132–148. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV should be capable of responding to the dynamic changes, timely and accurately. The mission replanning problem for multiple UAV formations is studied in this paper, and a hierarchical method is proposed to have a comprehensive consideration of the dynamic changes caused by the task and environment. First, three types of task change are analyzed, and the task reassignment approach is designed to determine the opportunity of conducting path replanning, which can make the best of the predetermined path and save more computational time. Moreover, a hierarchical strategy is presented to determine the most suitable UAV formation to perform a new task. Then a two-step path replanning approach combining the efficient rapid-exploration random tree (ERRT) algorithm and the modified consensus (MC) algorithm is developed. The waypoints are determined by the ERRT algorithm, and then the MC algorithm is applied to make the UAV formation head for the waypoints one after another. The MC algorithm also can be used in the UAV formation’s dismission and a single UAV’s landing. Simulation results demonstrate that the proposed hierarchical mission replanning method performs well under complicated dynamic changes. The task list of UAV formation can be updated timely and reasonably, and feasible paths can be generated rapidly by the ERRT algorithm considering different pairs of start point and destination.},
  archive      = {J_COMCOM},
  author       = {Yu Wu and Jinzhan Gou and Honglei Ji and Jianing Deng},
  doi          = {10.1016/j.comcom.2023.01.011},
  journal      = {Computer Communications},
  pages        = {132-148},
  shortjournal = {Comput. Commun.},
  title        = {Hierarchical mission replanning for multiple UAV formations performing tasks in dynamic situation},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable machine learning for performance anomaly
detection and classification in mobile networks. <em>COMCOM</em>,
<em>200</em>, 113–131. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile communication providers continuously collect many parameters, statistics, and key performance indicators (KPIs) with the goal of identifying operation scenarios that can affect the quality of Internet-based services. In this regard, anomaly detection and classification in mobile networks have become challenging tasks due to both the huge number of involved variables and the unknown distributions exhibited by input features. This paper introduces an unsupervised methodology based on both a data-cleaning strategy and explainable machine learning models to detect and classify performance anomalies in mobile networks. Specifically, this methodology dubbed explainable machine learning for anomaly detection and classification (XMLAD) aims at identifying features and operation scenarios characterizing performance anomalies without resorting to parameter tuning. To this end, this approach includes a data cleaning stage that extracts and removes outliers from experiments and features to train the anomaly detection engine with the cleanest possible dataset. Moreover, the methodology considers the differences between discretized values of the target KPI and labels predicted by the anomaly detection engine to build the anomaly classification engine which identifies features and thresholds that could cause performance anomalies. The proposed methodology incorporates two decision tree classifiers to build explainable models of anomaly detection and classification engines whose decision structures recognize features and thresholds describing both normal behaviors and performance anomalies. We evaluate the XMLAD methodology on real datasets captured by operational tests in commercial networks. In addition, we present a testbed that generates synthetic data using a known TCP throughput model to assess the accuracy of the proposed approach.},
  archive      = {J_COMCOM},
  author       = {Juan M. Ramírez and Fernando Díez and Pablo Rojo and Vincenzo Mancuso and Antonio Fernández-Anta},
  doi          = {10.1016/j.comcom.2023.01.003},
  journal      = {Computer Communications},
  pages        = {113-131},
  shortjournal = {Comput. Commun.},
  title        = {Explainable machine learning for performance anomaly detection and classification in mobile networks},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bandwidth-aware service migration method in LEO satellite
edge computing network. <em>COMCOM</em>, <em>200</em>, 104–112. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of modern sensing technology, large amounts of data are generated in areas that lack terrestrial networks , such as the field and space. This data is transmitted by satellite networks, where the interesting data is only a small part. By introducing edge computing technology, identifying and transmitting interesting data will improve the efficiency of satellite network and the quality of service for users. A satellite edge computing (SatEC) system is proposed in this paper that provides computing services through LEO satellite network. For satellite characteristics with limited resources and mobility, a bandwidth-aware service migration problem is proposed to minimize the long-term total delay cost of the system while all tasks do not exceed its deadline. Converting long-term optimization problem into a series of one-time MINLP problems, an online lazy migration algorithm is firstly proposed to solve optimal service migration decision scheme with ϵ ( 1 + 1 β ) ϵ(1+1β) -competitive ratio. Then a hierarchical iterative algorithm is proposed to decouple communication and computing resources. Finally, a bandwidth-aware virtual CPU scheduling for post-copy migration algorithm is proposed to improve the migration speed. Simulation results show that the proposed scheme effectively deal with the topology change of satellite network and reduce the long-term total delay cost of the SatEC system.},
  archive      = {J_COMCOM},
  author       = {Peng Deng and Xiangyang Gong and Xirong Que},
  doi          = {10.1016/j.comcom.2023.01.007},
  journal      = {Computer Communications},
  pages        = {104-112},
  shortjournal = {Comput. Commun.},
  title        = {A bandwidth-aware service migration method in LEO satellite edge computing network},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BBR-with enhanced fairness (BBR-EFRA): A new enhanced RTT
fairness for BBR congestion control algorithm. <em>COMCOM</em>,
<em>200</em>, 95–103. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Towards the end of 2016, the Google research team proposed and developed a new state-of-the-art TCP congestion control algorithm called Bottleneck Bandwidth and Round-trip propagation time (BBR). When deployed on various Google internal servers , BBR attained higher throughput and low latency performances on modern-day and sophisticated networks than traditional congestion control algorithms like Cubic, Reno, or Vegas. Unlike conventional congestion control algorithms, BBR controls data transmission by maximizing delivery rate and minimizing the round-trip time (RTT), therefore maximizing bandwidth utilization and improving throughput and latency delay performances. However, some experiments have reported persistent queue formation and massive packet retransmissions rate in the bottleneck link , which happens to be the main cause of unfairness between different RTT flows. BBR prefers and favors long RTT over short RTT flows; therefore, long RTT flows are allocated more bandwidth when competing with short RTT flows in the bottleneck link . It was also noted that even the minor difference between the two competing flows could be a source of throughput imbalance and unfairness. The dominance of long RTT flows is the central origin of the high queuing delay , packet loss and retransmission rates, and even severe BBR vulnerability that malicious users can exploit to obtain a larger share of bandwidth simply by increasing the RTT (delay). Therefore, we propose a BBR-With Enhanced Fairness (BBR-EFRA) to mitigate this major concern challenge. Our proposed algorithm adaptively controls the congestion window (CWND) by adjusting the bandwidth delay product (BDP) values of each BBR flow during data transmission based on buffer queue status computation. Our proposed approach guarantees different RTT flows to compete for the available bottleneck bandwidth more equally, hence improving the BBR fairness issue. We evaluated our algorithm on the NS3 simulating environment. BBR-EFRA shows improved RTT fairness by more than 16\%, reduced retransmission rate by more than 20\%, and queuing delay by more than 18\%, and finally increased Jain’s fairness index by more than 1.3 times compared with other recently published BBR variants like an adaptive congestion window of BBR(BBR-ACW). Therefore, with BBR-EFRA, short RTT flows can fairly compete for the available bandwidth even with higher RTT differences under various network scenarios.},
  archive      = {J_COMCOM},
  author       = {Charles Kihungi Njogu and Wang Yang and Humphrey Waita Njogu and Adrian Bosire},
  doi          = {10.1016/j.comcom.2022.12.015},
  journal      = {Computer Communications},
  pages        = {95-103},
  shortjournal = {Comput. Commun.},
  title        = {BBR-with enhanced fairness (BBR-EFRA): A new enhanced RTT fairness for BBR congestion control algorithm},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AERF: Adaptive ensemble random fuzzy algorithm for anomaly
detection in cloud computing. <em>COMCOM</em>, <em>200</em>, 86–94. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The abnormality of system behavior is inevitable in cloud computing because of its complexity and scale. How to perform anomaly detection on the system’s operating data to discover abnormal behavior has become a popular research field. However, anomaly detection is a challenging research problem because data in cloud computing scenarios is continuous, imbalanced, and cannot be acquired at once. In this work, an adaptive ensemble random fuzzy (AERF) algorithm is proposed for anomaly detection in cloud computing systems . The random fuzzy rule-based method in the AERF selects samples randomly to enhance the diversity of base classifiers for dealing with disturbances caused by abnormal sample distribution effectively. A dynamic weighting strategy is used for fuzzy classifier ensembles to improve processing efficiency and anomaly detection accuracy. Experimental results show that the AERF yields 10\% higher AUC and G-mean than existing methods on SMD and EMOSCloud datasets and 20\% higher AUC and G-mean on five benchmarking datasets . The AERF yields 20\% higher F1 than existing methods on EMOSCloud dataset. For all datasets, the AERF yields 50\% faster training time than other methods.},
  archive      = {J_COMCOM},
  author       = {Jun Jiang and Fagui Liu and Wing W.Y. Ng and Quan Tang and Guoxiang Zhong and Xuhao Tang and Bin Wang},
  doi          = {10.1016/j.comcom.2023.01.004},
  journal      = {Computer Communications},
  pages        = {86-94},
  shortjournal = {Comput. Commun.},
  title        = {AERF: Adaptive ensemble random fuzzy algorithm for anomaly detection in cloud computing},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing communication and computational resource
allocations in network slicing using twin-GAN-based DRL for 5G hybrid
c-RAN. <em>COMCOM</em>, <em>200</em>, 66–85. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The network slicing defined from 3GPP Rel. 15 is one important feature and function for 5G networks . In this paper, a new machine learning scheme is proposed by extending existing generative adversarial network (GAN) based deep reinforcement learning (DRL) result, namely Twin-GAN-based DRL (TGDRL) scheme, by utilizing two GAN-based DRLs to jointly allocate wireless bandwidth resources and computational resources. Existing resource allocation results are just only consider the bandwidth allocation , or just only consider the computational resource allocation. The main contribution of the proposed TGDRL scheme is to simultaneously investigate the bandwidth allocation and computational resource allocation by utilizing a multi-objective optimization algorithm, which aims to improve the efficiency of spectrum and reduce the consumption of computational resources. In our simulation, the total delay, the spectral efficiency, and the computational consumption of our proposed scheme is improved by 10.2\%, 15.7\%, and 12.8\%, compared to existing schemes.},
  archive      = {J_COMCOM},
  author       = {Yuh-Shyan Chen and Chih-Shun Hsu and Hsiang-Ching Hung},
  doi          = {10.1016/j.comcom.2023.01.002},
  journal      = {Computer Communications},
  pages        = {66-85},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing communication and computational resource allocations in network slicing using twin-GAN-based DRL for 5G hybrid C-RAN},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mobility aware resource allocation for millimeter-wave D2D
communications in presence of obstacles. <em>COMCOM</em>, <em>200</em>,
54–65. (<a href="https://doi.org/10.1016/j.comcom.2022.12.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The device-to-device (D2D) communication is envisioned as the solution to the bandwidth scarcity problem in the era of exponentially growing smart handheld devices. Although D2D communication in high frequency millimeter wave (mmWave) band offers high data rate, it requires obstacle free line-of-sight (LOS) communication path due to high propagation and penetration losses. The relay selection problem in D2D communication deals with careful selection of relay devices in order to establish a communication link when the direct LOS path is blocked or suffers from high interference from neighboring devices. With the limited number of available frequency channels, the channel allocation problem aims to judicially allocate the frequency channels to the activated D2D communication links. Due to the interdependence of the aforesaid two problems, it is a challenging task to optimally allocate frequency channels to the requesting D2D pairs along with proper selection of relay devices, specially in the presence of obstacles and user mobility. In this paper, we have proposed a mobility and obstacle aware base-station controlled centralized framework which jointly deals the channel allocation and relay selection problems for mmWave D2D communication. After proving the hardness of this joint problem, we provide a greedy solution along with its approximation bound. Our proposed algorithm has been shown to outperform an existing work through extensive simulations.},
  archive      = {J_COMCOM},
  author       = {Rathindra Nath Dutta and Sasthi C. Ghosh},
  doi          = {10.1016/j.comcom.2022.12.025},
  journal      = {Computer Communications},
  pages        = {54-65},
  shortjournal = {Comput. Commun.},
  title        = {Mobility aware resource allocation for millimeter-wave D2D communications in presence of obstacles},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved authentication scheme for BLE devices with no
i/o capabilities. <em>COMCOM</em>, <em>200</em>, 42–53. (<a
href="https://doi.org/10.1016/j.comcom.2023.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bluetooth Low Energy (BLE) devices have become very popular because of their Low energy consumption and prolonged battery life. They are being used in smart wearable devices, home automation systems , beacons, and many more areas. BLE uses pairing mechanisms to achieve a level of peer entity authentication as well as encryption. Although there are a set of pairing mechanisms available, BLE devices with no keyboard or display mechanism (and hence using the Just Works pairing) are still vulnerable. In this paper, we propose and implement a light-weight digital certificate-based authentication mechanism for the BLE devices using the Just Works model. The proposed model is an add-on to the existing pairing mechanism and can be easily incorporated into the existing BLE stack. To counter the Man-in-The-Middle attack scenario in Just Works pairing (device spoofing), our proposed model allows the client and peripheral to use the popular Public Key Infrastructure (PKI) to establish peer entity authentication and a secure cryptographic tunnel for communication. We have also developed a light-weight BLE profiled digital certificate containing the bare minimum fields required for resource-constrained devices, which significantly reduces the memory (about 90\% reduction) and energy consumption. We have experimentally evaluated the device’s energy consumption and execution time using the proposed pairing mechanism to demonstrate that the model can be easily deployed with fewer changes to the power requirements of the chips. The model has been formally verified using an automatic verification tool for protocol testing.},
  archive      = {J_COMCOM},
  author       = {Chandranshu Gupta and Gaurav Varshney},
  doi          = {10.1016/j.comcom.2023.01.001},
  journal      = {Computer Communications},
  pages        = {42-53},
  shortjournal = {Comput. Commun.},
  title        = {An improved authentication scheme for BLE devices with no I/O capabilities},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepBF: Malicious URL detection using learned bloom filter
and evolutionary deep learning. <em>COMCOM</em>, <em>200</em>, 30–41.
(<a href="https://doi.org/10.1016/j.comcom.2022.12.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malicious URL detection is an emerging research area due to the continuous modernization of various systems, for instance, Edge Computing . This article presents a novel malicious URL detection technique called deepBF (deep learning and Bloom Filter). deepBF is presented in two-fold. Firstly, we propose a learned Bloom Filter using a 2-dimensional Bloom Filter. We experimentally decide the best non-cryptography string hash function . Then, we derive a modified non-cryptography string hash function from the selected hash function for deepBF by introducing biases in the hashing method and compared among the string hash functions. The modified string hash function is compared to other variants of diverse non-cryptography string hash functions. It is also compared with various filters, particularly counting Bloom Filter, Kirsch et al., and Cuckoo Filter using various use cases. The use cases unearth the weakness and strengths of the filters. Secondly, we propose a malicious URL detection mechanism using Deep Learning. We apply the evolutionary convolutional neural network to identify malicious URLs. The evolutionary convolutional neural network is trained and tested with malicious URL datasets. The output is tested in deepBF for accuracy. Moreover, the Bloom Filters of deepBF continuously update upon encountering a new URL. Otherwise, Bloom Filters can answer the queries to avoid unnecessary load on Deep Learning. Furthermore, we have achieved many conclusions from our experimental evaluation and results and are able to reach various conclusive decisions, which are presented in the article.},
  archive      = {J_COMCOM},
  author       = {Ripon Patgiri and Anupam Biswas and Sabuzima Nayak},
  doi          = {10.1016/j.comcom.2022.12.027},
  journal      = {Computer Communications},
  pages        = {30-41},
  shortjournal = {Comput. Commun.},
  title        = {DeepBF: Malicious URL detection using learned bloom filter and evolutionary deep learning},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain for achieving accountable outsourcing
computations in edge computing. <em>COMCOM</em>, <em>200</em>, 17–29.
(<a href="https://doi.org/10.1016/j.comcom.2022.12.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Computing as a paradigm, provides services of outsourcing computations to a large number of end users. Since edge nodes are trustless, the integration of sampling-based replication calculation and the blockchain is used to verify the correctness of computation results in a trustless environment. However, the blockchain with the nature of decentralization, is confronted with some problems of high resource consumption, such that the verification with computational overhead cannot be directly deployed on the blockchain. Thus, we propose an accountable verification scheme based on an off-chain block. The off-chain block meets some requirements of Edge Computing, i.e., reduced latency of services, and edge nodes with heterogeneous resources. The off-chain block tries to address two challenges for reliable outsourcing computations: (i) how to generate the block efficiently and securely, and (ii) how to achieve accountable verification. In detail, the block is based on a Directed Acyclic Graph, in which the transactions of computation results and verification reports are updated in full decentralization. The hash of the block is recorded on the blockchain. Moreover, the integration of off-chain verification and on-chain arbitration provides reliable verification. A trust evaluation model achieves accountability for edge nodes. Besides, we conducted the security analysis based on some performance properties. Finally, the Raspberry Pis are leveraged to simulate lightweight edge nodes to prove the scalability of our outsourcing computations. A consortium blockchain with groups is also implemented to reveal the efficiency of blockchain updates of the proposed scheme.},
  archive      = {J_COMCOM},
  author       = {Ruilin Lai and Gansen Zhao},
  doi          = {10.1016/j.comcom.2022.12.024},
  journal      = {Computer Communications},
  pages        = {17-29},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain for achieving accountable outsourcing computations in edge computing},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mobile-chain: Secure blockchain based decentralized
authentication system for global roaming in mobility networks.
<em>COMCOM</em>, <em>200</em>, 1–16. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a secure and efficient authentication protocol is crucial and challenging in the mobility network . Due to the seamless roaming of mobile users over multiple foreign agents and the broadcast nature of the communication channel, the mobile networks are often exposed to several network attacks. To achieve perfect authentication and secure communication among mobility entities like MU (Mobile User), FA (Foreign Agent) and HA (Home Agent), the researchers have proposed numerous authentication protocols in the past. However, the existing protocols for the mobility environments are insufficient to address the fundamental security concerns and an adversary can impersonate the mobile user at anytime. Thus, we propose Mobile-Chain, a secure blockchain-based authentication system for mobility environments. The proposed Mobile-Chain is designed to protect user privacy and guarantees provable security like authentication, anonymity, untraceability, confidentiality, data integrity, and decentralization. The implementation of the security framework has been done on the ethereum blockchain platform using smart contracts written in a solidity programming language . The security analysis reveals that Mobile-Chain is robust against various security threats to which mobility networks are vulnerable. Besides, the authentication framework has been measured through a formal security verification tool known as Automated Validation of Internet Security Protocol and Application (AVISPA). Notably, the performance evaluation of the proposed protocol proves that it maintains performance gain, computationally efficient, and implementable in resource-limited wireless and mobility environments.},
  archive      = {J_COMCOM},
  author       = {Indushree M. and Manish Raj and Vipul Kumar Mishra and Shashidhara R. and Ashok Kumar Das and Vivekananda Bhat K.},
  doi          = {10.1016/j.comcom.2022.12.026},
  journal      = {Computer Communications},
  pages        = {1-16},
  shortjournal = {Comput. Commun.},
  title        = {Mobile-chain: Secure blockchain based decentralized authentication system for global roaming in mobility networks},
  volume       = {200},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight real-time stereo matching algorithm for AI
chips. <em>COMCOM</em>, <em>199</em>, 210–217. (<a
href="https://doi.org/10.1016/j.comcom.2022.06.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI chips have developed rapidly and achieved remarkable acceleration effects in the corresponding algorithm field in recent years. However, deep learning algorithms are changing rapidly, including many operators that AI chips and inference frameworks cannot use in the short term. To solve the problem that it is challenging to deploy a stereo matching algorithm based on binocular vision on AI chips, this paper proposes a multi-stage unsupervised lightweight real-time depth estimation algorithm for AI chips called TradNet. TradNet combines the traditional matching algorithm with a convolutional neural network and uses convolution directly supported by AI chips to realize the structure of the traditional matching algorithm . TradNet is composed of operators directly supported by current AI chips, which reduces the computational complexity of the algorithm, and greatly improves the compatibility of the stereo matching algorithm with existing AI chips. Compared with the deep learning-based multi-stage binocular disparity algorithm AnyNet, the accuracy is improved by 5.12\%, and the inference speed is only 12.7\%. Compared with the matching-based binocular disparity algorithm BM, the accuracy is improved by 25.24\%, and the inference speed is only 48.7\%. Our final model can process 1280&amp;#x00D7;720 resolution images within a range of 60&amp;#x2013;80 FPS on an NVIDIA TITAN Xp. It achieves 28FPS on a 1TOPS (Tera Operations Per Second) custom AI chip, and the power consumption is 0.88&amp;#x00A0;W.},
  archive      = {J_COMCOM},
  author       = {Yi Liu and Wenhao Wang and Xintao Xu and Xiaozhou Guo and Guoliang Gong and Huaxiang Lu},
  doi          = {10.1016/j.comcom.2022.06.018},
  journal      = {Computer Communications},
  pages        = {210-217},
  shortjournal = {Comput. Commun.},
  title        = {Lightweight real-time stereo matching algorithm for AI chips},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster service with less resource: A resource efficient
blockchain framework for edge computing. <em>COMCOM</em>, <em>199</em>,
196–209. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By integrating edge computing with blockchain technology , traceable and immutable services can be provided to address the distrust issue between edge devices. However, the contradiction between the computing and storage consumption of blockchain deployment and the constrained resources of edge devices is the greatest obstacle standing in the way. Reducing blockchain demand for computing power and storage becomes the main challenge to ameliorate the current dilemma. To address the above challenge, this article proposes a resource-efficient blockchain framework for edge computing . By analyzing the necessity of resources in the consensus, we divide the resource bottleneck of edge devices into the unnecessary computing resource and necessary storage resource to provide different resource reduction approaches. For unnecessary computing resource, a novel cloud–edge collaboration consensus protocol called the Proof-of-Ticket (PoT) has been proposed to completely offload the computing consumption in the execution of consensus protocol from edge devices to the cloud. For necessary storage resource, we propose a framework division mechanism based on the closure of service scenarios called the edge–terminal consensus zone (ETCZ). The storage requirement for edge devices working as full nodes in each ETCZ has significant decline. The extensive experiments are tested to demonstrate the effectiveness of our framework. Experiments show that the individual computing cost of edge devices has been completely offload to the cloud, and the minimum storage cost for a full node is reduced into 1 K 1K of the traditional blockchain in a K-ETCZ framework.},
  archive      = {J_COMCOM},
  author       = {Kaiyu Wang and Zhiying Tu and Zhenzhou Ji and Shufan He},
  doi          = {10.1016/j.comcom.2022.12.014},
  journal      = {Computer Communications},
  pages        = {196-209},
  shortjournal = {Comput. Commun.},
  title        = {Faster service with less resource: A resource efficient blockchain framework for edge computing},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WaterPurifier: A scalable system to prevent the DNS water
torture attack in 5G-enabled SIoT network. <em>COMCOM</em>,
<em>199</em>, 186–195. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social Internet of Things (SIoT) network contains a huge number of smart devices and is rich in social behavior relationships between these objects, especially in 5G-abled SIoT network. However, vulnerable devices and assailable applications or services in SIoT give attackers the opportunity to conduct DNS water torture (DNSWT) attacks for domain name system (DNS) infrastructures, which can cause the device to be unavailable, and moreover, previous approaches designed for normal Internet environment cannot reach the high level of demand on scalability and flexibility in SIoT network. In this work, we comprehensively analyze the characteristics of DNSWT attacks in 5G-enabled SIoT network and propose a collaborative and hierarchical defensive system called WaterPurifier to protect the SIoT network from DNSWT attacks. WaterPurifier consists of Gateway Layer, Server Layer and Cloud Layer. An asynchronously communication mechanism is implemented in the defensive system to ensure its flexibility. The components in Gateway Layer and Server Layer execute lightweight functions, like flow forwarding and attack monitoring, to guarantee a high level of scalability. Cloud Layer deploys an end-to-end domain encoding and a real-time training process, which can filter out DNSWT attack traffic effectively and efficiently. We implement a prototype of WaterPurifier in an in-lab environment and do an evaluation on the experiment result with performance indicators like packet loss rate. Both results and evaluations show high effectiveness and efficacy of the proposed system.},
  archive      = {J_COMCOM},
  author       = {Lihua Yin and Muyijie Zhu and Wenxin Liu and Xi Luo and Chonghua Wang and Yangyang Li},
  doi          = {10.1016/j.comcom.2022.12.019},
  journal      = {Computer Communications},
  pages        = {186-195},
  shortjournal = {Comput. Commun.},
  title        = {WaterPurifier: A scalable system to prevent the DNS water torture attack in 5G-enabled SIoT network},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generic parallel optimization framework for solving hard
problems in optical networks. <em>COMCOM</em>, <em>199</em>, 177–185.
(<a href="https://doi.org/10.1016/j.comcom.2022.12.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many optimization problems in optical networks are either NP-hard or take a long time to solve. In this paper, we develop a generic parallel computing framework which either solves these problems completely or at least improves the quality of the solution. To verify the effectiveness of the proposed framework, we build a miniaturized parallel computing system and test our approach based on different heuristic and meta-heuristic algorithms for the bin-packing problems, mixed integer linear programming (MILP) models, and distributed machine learning . Experimental studies show that the proposed parallel computing framework based on the miniaturized parallel computing system is effective in so far as it either significantly reduces the computing time or improves the quality of the solution as compared to when only a single machine is used.},
  archive      = {J_COMCOM},
  author       = {Longfei Li and Yongcheng Li and Sanjay K. Bose and Gangxiang Shen},
  doi          = {10.1016/j.comcom.2022.12.023},
  journal      = {Computer Communications},
  pages        = {177-185},
  shortjournal = {Comput. Commun.},
  title        = {A generic parallel optimization framework for solving hard problems in optical networks},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A coalitional game-based joint monitoring mechanism for
combating COVID-19. <em>COMCOM</em>, <em>199</em>, 168–176. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the absence of effective treatment for COVID-19, disease prevention and control have become a top priority across the world. However, the general lack of effective cooperation between communities makes it difficult to suppress the community spread of the global pandemic; hence repeated outbreaks of COVID-19 have become the norm. To address this problem, this paper considers community cooperation in disease monitoring and designs a joint epidemic monitoring mechanism, in which adjacent communities cooperate to enhance their monitoring capability. In this work, we formulate the epidemiological monitoring process as a coalitional game. Then, we propose a Shapley value-based payoffs distribution scheme for the coalitional game. A comprehensive analytical framework is developed to evaluate the advantages and sustainability of the cooperation between communities. Experimental results show that the proposed mechanism performs much better than the conventional non-cooperative monitoring design and can greatly increase each community’s payoffs.},
  archive      = {J_COMCOM},
  author       = {Da-Wen Huang and Bing Liu and Jichao Bi and Jingpei Wang and Mengzhi Wang and Huan Wang},
  doi          = {10.1016/j.comcom.2022.12.020},
  journal      = {Computer Communications},
  pages        = {168-176},
  shortjournal = {Comput. Commun.},
  title        = {A coalitional game-based joint monitoring mechanism for combating COVID-19},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pricing strategy for electric vehicle charging in
residential areas considering the uncertainty of charging time and
demand. <em>COMCOM</em>, <em>199</em>, 153–167. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In residential areas, the hours when commuters charge their electric vehicles (EVs) after work often coincide highly with the peak residential electricity consumption hours, which will create security risks to the power system and affect the normal operation of the power grid. In contrast to the inevitability and immutability of the use of household appliances, the duration and time period of charging electric vehicles can be changed. This paper addresses the aforementioned problem by time-of-use (TOU) electricity price, which can guide residents to stagger hours to charge, achieving the objective of peak load shifting. Taking residential areas as the research scope, considering the uncertainty of EV users’ homecoming time, departure time and charging demand, the article proposes a bi-level planning model for pricing electric vehicle charging. The lower level is a minimization model of charging costs for individual users, which can acquire the charging load of all vehicles for multiple days. The upper level is a robust optimization model of TOU electricity price considering uncertain factors, which obtains the fast and slow charging prices. It has been demonstrated that the model increases grid stability by 43.46\%.},
  archive      = {J_COMCOM},
  author       = {Shidong Liang and Bingqing Zhu and Jianjia He and Shengxue He and Minghui Ma},
  doi          = {10.1016/j.comcom.2022.12.018},
  journal      = {Computer Communications},
  pages        = {153-167},
  shortjournal = {Comput. Commun.},
  title        = {A pricing strategy for electric vehicle charging in residential areas considering the uncertainty of charging time and demand},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint perception data caching and computation offloading in
MEC-enabled vehicular networks. <em>COMCOM</em>, <em>199</em>, 139–152.
(<a href="https://doi.org/10.1016/j.comcom.2022.12.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated use of intelligent vehicles and the advancement of self-driving technologies have posed significant problems to the provision of real-time vehicular services, such as enormous amounts of computation, long transmission delay, and integration of sensor data. In this context, we propose to solve these problems to guarantee the Quality of Services (QoS) using computation offloading and perception data caching methods , where perception data means combinatorial sensor data, including sensor values and corresponding locations in an area. At first, we present a cooperative vehicular network framework with edge computing and sensor devices. Taking into account vehicle mobility, distributed resources, task properties, and user preferences, we jointly formulate a computation offloading strategy and a perception data caching strategy to minimize the average execution latency, which can be considered a Mixed-Integer Non-Linear Programming (MINLP) problem. We first propose a Genetic Algorithm (GA)-based scheme to solve our formulated problem. Then we propose a heuristic named Correlation-Monte Carlo Fast Search (CMCFS) method to obtain an effective strategy with low complexity. Simulation results reveal that both GA and CMCFS achieve less latency than other baseline schemes.},
  archive      = {J_COMCOM},
  author       = {Bo Li and Ruizhi Wu},
  doi          = {10.1016/j.comcom.2022.12.021},
  journal      = {Computer Communications},
  pages        = {139-152},
  shortjournal = {Comput. Commun.},
  title        = {Joint perception data caching and computation offloading in MEC-enabled vehicular networks},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and implementation of p4 virtual switches and p4
virtual networks. <em>COMCOM</em>, <em>199</em>, 126–138. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the design and implementation of P4 virtual switches (VSWs) and P4 virtual networks (VNs). The proposed switch virtualization supports multiple VSWs embedded in a single P4 physical switch (PSW). Similar to a PSW, users use standard P4 language to program their VSWs. Given a set of VSWs and their target PSW, our hypervisor integrates all the VSW programs to generate configuration files that enable the target PSW to become a platform to realize switch virtualization . Users can define customized header types and metadata for their VSWs. In addition, our system supports live reconfiguration. Reconfiguring a VSW in a PSW will not interrupt the other VSWs in the PSW. Based on the proposed switch virtualization scheme, we present the way to share multiple VNs in a P4 physical network to provide multitenant services. A VN is composed of multiple VSWs and multiple virtual links (VL), and each VN is provisioned with guaranteed bandwidth for their VLs. Each tenant can control and manage its own VN—similar to a dedicated P4 network. In addition, each tenant can assign a priority to its traffic. The proposed network virtualization guarantees traffic isolation, preventing inter-VN interference. Results on an experimental network used for performance evaluation show that our system can successfully generate VSWs and VNs. The proposed hitless reconfiguration method can prevent service interruptions between VSWs during VSW reconfiguration. The experimental results also show that our network virtualization can accurately guarantee the provisioned bandwidths of VNs in a physical P4 network.},
  archive      = {J_COMCOM},
  author       = {Kwan-Yee Chan and Steven S.W. Lee},
  doi          = {10.1016/j.comcom.2022.12.016},
  journal      = {Computer Communications},
  pages        = {126-138},
  shortjournal = {Comput. Commun.},
  title        = {Design and implementation of p4 virtual switches and p4 virtual networks},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning technique for intrusion detection system
using a recurrent neural networks based framework. <em>COMCOM</em>,
<em>199</em>, 113–125. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the spike in the amount of information transmitted through communication infrastructures has increased due to the advances in technologies such as cloud computing , vehicular networks systems, the Internet of Things (IoT), etc. As a result, attackers have multiplied their efforts for the purpose of rendering network systems vulnerable. Therefore, it is of utmost importance to improve the security of those network systems. In this study, an IDS framework using Machine Learning (ML) techniques is implemented. This framework uses different types of Recurrent Neural Networks (RNNs), namely, Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and Simple RNN. To assess the performance of the proposed IDS framework, the NSL-KDD and the UNSW-NB15 benchmark datasets are considered. Moreover, existing IDSs suffer from low test accuracy scores in detecting new attacks as the feature dimension grows. In this study, an XGBoost-based feature selection algorithm was implemented to reduce the feature space of each dataset. Following that process, 17 and 22 relevant attributes were picked from the UNSW-NB15 and NSL-KDD, respectively. The accuracy obtained through the test subsets was used as the main performance metric in conjunction with the F1-Score, the validation accuracy, and the training time (in seconds). The results showed that for the binary classification tasks using the NSL-KDD, the XGBoost-LSTM achieved the best performance with a test accuracy (TAC) of 88.13\%, a validation accuracy (VAC) of 99.49\% and a training time of 225.46 s. For the UNSW-NB15, the XGBoost-Simple-RNN was the most efficient model with a TAC of 87.07\%. For the multiclass classification scheme, the XGBoost-LSTM achieved a TAC of 86.93\% over the NSL-KDD and the XGBoost-GRU obtained a TAC of 78.40\% over the UNSW-NB15 dataset. These results demonstrated that our proposed IDS framework performed optimally in comparison to existing methods.},
  archive      = {J_COMCOM},
  author       = {Sydney Mambwe Kasongo},
  doi          = {10.1016/j.comcom.2022.12.010},
  journal      = {Computer Communications},
  pages        = {113-125},
  shortjournal = {Comput. Commun.},
  title        = {A deep learning technique for intrusion detection system using a recurrent neural networks based framework},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy harvesting cognitive radio networks with strategic
users: A two-class queueing model with retrials. <em>COMCOM</em>,
<em>199</em>, 98–112. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the rapid growth in wireless communication systems, we develop a single-server queueing model with service retrials for the cognitive radio network (CRN). A key appeal of CRN is its flexible allocation scheme of limited spectrum for two user groups: (1) the licensed user, hereby referred to as primary user (PU), and (2) unlicensed users, referred to as secondary users (SUs). The PU take priority over SUs: when a PU arrives and observes a busy server currently occupied by an SU, it will take over the server by preempting the SU out of service. SUs are strategic and delay sensitive: they decide immediately upon arrivals on whether to join the CRN or to balk, based on the anticipated expected utility (service reward minus delay cost). In case the server is unavailable, SUs temporarily join an orbit queue where they wait for future accesses to service. Another realistic feature of our model is that, after successfully serving a user (PU or SU), the server becomes unavailable and undergoes an energy harvesting phase of a random time. We characterize the equilibrium system performance in the following three steps: First, we obtain the equilibrium joining behavior for utility-maximizing SUs; second, we derive the optimal joining strategy from the perspective of a social planner; and finally, we enforce that users adopt the socially optimal strategy by imposing an admission fee. We also investigate the impact of the energy harvesting time on users?? equilibrium strategies and report numerical experiments to provide qualitative and quantitative insights of our results.},
  archive      = {J_COMCOM},
  author       = {Ke Sun and Yunan Liu and Kaili Li},
  doi          = {10.1016/j.comcom.2022.12.017},
  journal      = {Computer Communications},
  pages        = {98-112},
  shortjournal = {Comput. Commun.},
  title        = {Energy harvesting cognitive radio networks with strategic users: A two-class queueing model with retrials},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COVID-19 health data analysis and personal data preserving:
A homomorphic privacy enforcement approach. <em>COMCOM</em>,
<em>199</em>, 87–97. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 data analysis and prediction from patient data repository collected from hospitals and health organizations. Users’ credentials and personal information are at risk; it could be an unrecoverable issue worldwide. A Homomorphic identification of possible breaches could be more appropriate for minimizing the risk factors in preventing personal data. Individual user privacy preservation is a must-needed research focus in various fields. Health data generated and collected information from multiple scenarios increasing the complexity involved in maintaining secret patient information. A homomorphic-based systematic approach with a deep learning process could reduce depicts and illegal functionality of unknown organizations trying to have relation to the environment and physical and social relations. This article addresses the homomorphic standard system functionality, which refers to all the functional aspects of deep learning system requirements in COVID-19 health management. Moreover, this paper spotlights the metric privacy incorporation for improving the Deep Learning System (DPLS) approaches for solving the healthcare system’s complex issues. It is absorbed from the result analysis Homomorphic-based privacy observation metric gradually improves the effectiveness of the deep learning process in COVID-19-health care management.},
  archive      = {J_COMCOM},
  author       = {Chandramohan Dhasarathan and Mohammad Kamrul Hasan and Shayla Islam and Salwani Abdullah and Umi Asma Mokhtar and Abdul Rehman Javed and Sam Goundar},
  doi          = {10.1016/j.comcom.2022.12.004},
  journal      = {Computer Communications},
  pages        = {87-97},
  shortjournal = {Comput. Commun.},
  title        = {COVID-19 health data analysis and personal data preserving: A homomorphic privacy enforcement approach},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MRT-LoRa: A multi-hop real-time communication protocol for
industrial IoT applications over LoRa networks. <em>COMCOM</em>,
<em>199</em>, 72–86. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent literature proposed MAC protocols running on top of LoRa that are able to offer real-time communications for Industrial Internet of Things (IIoT) applications. However, such solutions work over single-hop star topologies and therefore, to reach a wide coverage through a single hop, they require to configure the LoRa radio parameters in a way that reduces the maximum supported bit rate, thus increasing the message time on air. Consequently, to comply with the maximum duty cycle constraints imposed on LoRa devices by the current regulations, the maximum number of messages that can be sent per hour is also reduced. However, industrial environments typically cover very large areas and require bit rate values in the order of a few thousands of bits per second for LoRa-based networks. To cope with both coverage and bit rate requirements of IIoT, a multi-hop protocol able to provide bounded delays to the real-time flows typical of industrial applications is a better solution than a single-hop approach. For this reason, this paper proposes MRT-LoRa, a multi-hop communication protocol running on top of LoRa that provides support for real-time flows. The MRT-LoRa protocol enables long-range communications while maintaining shorter time on air at each hop, thus lowering the impact on the duty cycle of each node. The paper describes the MRT-LoRa design and configuration, and discusses the performance obtained through OMNeT++ simulations in realistic industrial IoT scenarios.},
  archive      = {J_COMCOM},
  author       = {Luca Leonardi and Lucia Lo Bello and Gaetano Patti},
  doi          = {10.1016/j.comcom.2022.12.013},
  journal      = {Computer Communications},
  pages        = {72-86},
  shortjournal = {Comput. Commun.},
  title        = {MRT-LoRa: A multi-hop real-time communication protocol for industrial IoT applications over LoRa networks},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Overlapping community detection on complex networks with
graph convolutional networks. <em>COMCOM</em>, <em>199</em>, 62–71. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering the community structure within networks is of significance with respect to many realistic applications, like recommendation systems and cyberattack detection. In this study, we propose an overlapping community detection method CDMG based on Graph Convolutional Network (GCN) from the perspective of maximizing the Markov stability of community structure, which is defined in terms of the clustered autocovariance of a Markov process taking place on the network. Extensive experiments on both the attributed networks and the normal networks with different scales demonstrate the superiority of CDMG compared to other established community detection algorithms . Additionally, the Markov stability of the community structure relies on a time parameter, Markov time , and we observe that the performance of CDMG can be further improved by utilizing the optimal Markov time . According to the variation curve that demonstrates the influence of Markov time t t on the performance of CDMG, we propose a trichotomy-based method to search for the optimal Markov time for our method.},
  archive      = {J_COMCOM},
  author       = {Shunjie Yuan and Hefeng Zeng and Ziyang Zuo and Chao Wang},
  doi          = {10.1016/j.comcom.2022.12.008},
  journal      = {Computer Communications},
  pages        = {62-71},
  shortjournal = {Comput. Commun.},
  title        = {Overlapping community detection on complex networks with graph convolutional networks},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A contract-based energy harvesting mechanism in UAV
communication network. <em>COMCOM</em>, <em>199</em>, 50–61. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The energy harvesting of unmanned aerial vehicle (UAV) has been researched extensively in recent years. However, the existing energy harvesting between the base station and UAVs does not consider the information asymmetry factor, which means the base station provides the radio frequency (RF) energy for UAVs in the context of UAVs’ partial private information. In order to maximize the base station’s utility or payoff, it is crucial for the base station to motivate more UAVs to harvest RF energy. In the paper, we propose an effective incentive energy harvesting mechanism in UAV communication network, which is a challenging problem since there exist interest conflicts that the base station and UAVs are rational individuals who maximize their utilities. Our objective is to make the base station’s utility maximum via balancing the tradeoff between transmit power cost and charged price benefit, while incentivizing UAVs to purchase transmit power. We design a series of optimal energy harvesting contract with different price discounts targeting different types of UAVs by contract theory. Owing to information asymmetry, we analyze two different information scenarios: complete and incomplete information. We suppose the base station knows each UAV’s type in complete information, then we analyze the practical case that the base station is aware of incomplete information of UAV’s private information. The base station aims to maximize its utility by providing contract. The UAVs choose the contract meeting the individual rationality (IR) and incentive compatibility (IC) rules while maximizing their utilities. Our simulation shows that the energy harvesting mechanism maximizes the base station’s utility and stimulates UAVs to purchase RF energy transmit power in different scenarios. Compared with other methods, our proposed optimal contract can improve the utility of the base station while maximizing the utility of UAVs.},
  archive      = {J_COMCOM},
  author       = {Wanyu Qiu and Chuanhe Huang and Yanjiao Chen and Shidong Huang and Haizhou Bao and Zhengfa Li},
  doi          = {10.1016/j.comcom.2022.12.003},
  journal      = {Computer Communications},
  pages        = {50-61},
  shortjournal = {Comput. Commun.},
  title        = {A contract-based energy harvesting mechanism in UAV communication network},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed optimization for intelligent IoT under unstable
communication conditions. <em>COMCOM</em>, <em>199</em>, 42–49. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of distributed optimization algorithms for intelligent Internet of Thing (IIoT) environments has attracted extensive attention. However, the dynamic network environment, the delay of information transmission between devices and the risk of privacy disclosure pose great challenges to algorithm designs . To address this problem, we propose a distributed gradient descent algorithm, which can converge under very mild assumptions that there will be an unbounded (stochastic) delay from sender to receiver. To the best of our knowledge, our proposed algorithm is the first work to take unbounded stochastic communication delays into consideration, in addition to time-varying networks and privacy disclosure . Rigorous analysis shows that our algorithm can converge under such mild assumptions. Extensive experiments exhibit that the proposed algorithm performs well in complex IIoT environments.},
  archive      = {J_COMCOM},
  author       = {Yuan Yuan and Jiguo Yu and Liangxu Zhang and Zhipeng Cai},
  doi          = {10.1016/j.comcom.2022.12.012},
  journal      = {Computer Communications},
  pages        = {42-49},
  shortjournal = {Comput. Commun.},
  title        = {Distributed optimization for intelligent IoT under unstable communication conditions},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D face recognition algorithm based on deep laplacian
pyramid under the normalization of epidemic control. <em>COMCOM</em>,
<em>199</em>, 30–41. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the normalization of epidemic control in COVID-19, it is essential to realize fast and high-precision face recognition without feeling for epidemic prevention and control. This paper proposes an innovative Laplacian pyramid algorithm for deep 3D face recognition , which can be used in public. Through multi-mode fusion, dense 3D alignment and multi-scale residual fusion are ensured. Firstly, the 2D to 3D structure representation method is used to fully correlate the information of crucial points, and dense alignment modeling is carried out. Then, based on the 3D critical point model, a five-layer Laplacian depth network is constructed. High-precision recognition can be achieved by multi-scale and multi-modal mapping and reconstruction of 3D face depth images. Finally, in the training process, the multi-scale residual weight is embedded into the loss function to improve the network’s performance. In addition, to achieve high real-time performance, our network is designed in an end-to-end cascade. While ensuring the accuracy of identification, it guarantees personnel screening under the normalization of epidemic control. This ensures fast and high-precision face recognition and establishes a 3D face database. This method is adaptable and robust in harsh, low light, and noise environments. Moreover, it can complete face reconstruction and recognize various skin colors and postures.},
  archive      = {J_COMCOM},
  author       = {Weiyi Kong and Zhisheng You and Xuebin Lv},
  doi          = {10.1016/j.comcom.2022.12.011},
  journal      = {Computer Communications},
  pages        = {30-41},
  shortjournal = {Comput. Commun.},
  title        = {3D face recognition algorithm based on deep laplacian pyramid under the normalization of epidemic control},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sec-edge: Trusted blockchain system for enabling the
identification and authentication of edge based 5G networks.
<em>COMCOM</em>, <em>199</em>, 10–29. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 5th generation 5G technology, also known as the 2020 telecommunications system , represents the next exciting phase of the evolution of telecommunications, with the recent successful distribution on nearly every continent. 5G is believed to bring multiple advancements such as greater transmission speeds, ultra-low latency, higher network capacity, etc. These advancements will directly support the industry-5.0 and Internet of Things (IoT) in the coming future. Today, IoT is one of the fastest-growing technologies because of its ability to deliver multiple applications across various devices. The emergence of 5G networks will be a major force for IoT development. Subsequently, Edge computing as an extension of cloud computing has emerged as a promising technology to empower 5G and IoT services due to its close vicinity to IoT Devices. However, specific challenging issues plague the integration of 5G, IoT, and Edge Computing, among which security is one of the biggest concerns. Recently, blockchain has been provided as a foundational and decentralized distributed ledger technology that can be used for solving the challenges of IoT, 5G, and Edge Computing integration. This paper proposes a trusted blockchain system for edge-based 5G networks that eliminates the limitations of integrating the Edge Computing, IoT, and 5G networks. The proposed blockchain system can efficiently identify the edge devices, authenticate the devices, and assign the addresses to the edge devices on a demand basis. Further, the proposed work also provides the secure communication among the edge devices that can protect from DDOS and Side-channel attacks.},
  archive      = {J_COMCOM},
  author       = {Erukala Suresh Babu and Amogh Barthwal and Rajesh Kaluri},
  doi          = {10.1016/j.comcom.2022.12.001},
  journal      = {Computer Communications},
  pages        = {10-29},
  shortjournal = {Comput. Commun.},
  title        = {Sec-edge: Trusted blockchain system for enabling the identification and authentication of edge based 5G networks},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CANTO: An actor model-based distributed fog framework
supporting neural networks training in IoT applications.
<em>COMCOM</em>, <em>199</em>, 1–9. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large volumes of Internet of Things (IoT) data transmission to and from the cloud leads to one of cloud-centric processing’s major drawbacks: latency. Fog computing gives a solution to this by bringing the processing closer to the edge devices. Although a lot of work has been done which makes use of fog nodes like aggregators and offloaders, data intensive tasks like machine learning are still, for the most part, being performed on cloud. In a fog network, there are multiple fog devices, albeit resource-constrained ones. We can make use of distributed processing to collectively utilize these resources. So, we propose CANTO, a general distributed framework which uses the actor model to train neural networks on a network of fog devices in an IoT setting. Since the actors communicate using messages and act according to the type of message they receive, the framework provides a new message type wherein we can specify parameters like the dataset, the size of each dataset part, the activation function , learning rate, etc. The framework is containerized and deployed in a docker swarm. The framework is demonstrated with the use case on IoT-based forest fire prediction. In addition, the effectiveness of the framework is demonstrated with respect to accuracy, latency and load distribution.},
  archive      = {J_COMCOM},
  author       = {Satish Narayana Srirama and Deepika Vemuri},
  doi          = {10.1016/j.comcom.2022.12.007},
  journal      = {Computer Communications},
  pages        = {1-9},
  shortjournal = {Comput. Commun.},
  title        = {CANTO: An actor model-based distributed fog framework supporting neural networks training in IoT applications},
  volume       = {199},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pairing-free data authentication and aggregation mechanism
for intelligent healthcare system. <em>COMCOM</em>, <em>198</em>,
282–296. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intelligent Healthcare System (IHS), a prominent medical area underlying the Internet of Things (IoT), might be deployed to gather and analyze health information across a range of sources to deliver improved as well as more economical medical treatment. Managing all the health-related information over the internet becomes a challenging chore as numerous people rely on it implicitly or explicitly. The significant challenges of IHS comprise the privacy and security along with protection of aggregation of patients’ data as sensors obtain and transmit patient’s health information. The few of surveyed existing works based on security and privacy goals for IHS are inefficient due to cumbersome of huge computational cost and non-resilience of different attacks. Therefore, to resolve such challenges: to provide the robustness, preserve security goals and improve the efficiency, we introduced an efficient data authentication and aggregation protocol for IHS without pairing and ID-based cryptography. Provable security has been provided with Random Oracle model (ROM), which proofs the robustness of the presented scheme. Thus, the defined security requirements of our proposed work have been achieved. The performance analysis through MIRACL shows that the scheme presented outperforms the similar prior mechanisms in terms of computational-communicational cost and energy overheads.},
  archive      = {J_COMCOM},
  author       = {Pooja Verma and Daya Sagar Gupta},
  doi          = {10.1016/j.comcom.2022.12.009},
  journal      = {Computer Communications},
  pages        = {282-296},
  shortjournal = {Comput. Commun.},
  title        = {A pairing-free data authentication and aggregation mechanism for intelligent healthcare system},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparative analysis of new meta-heuristic-variants for
privacy preservation in wireless mobile adhoc networks for IoT
applications. <em>COMCOM</em>, <em>198</em>, 262–281. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Mobile Ad Hoc Network (MANET)” is a self-configurable, self-repairing, self-maintaining, highly mobile, decentralized, and independent wireless network, which has the liberty to move from one to another place. Such networks do not have any pre-existing infrastructure. The adoption of a smart environment in MANET requires new protocols to connect the gadgets to the internet. A smart environment with routing protocols should assure the following properties like connectivity among the nodes, “Quality of Service (QoS)”, and fairness, both in access points and ad-hoc networks. Combination with the Internet of Things (IoT) and MANET generates a novel MANET-IoT system, which focuses on reducing the implementing costs of the network and providing better mobility for users. The necessity of these integrated networks is increasing in military operations, rescue operations, personal area networks, emergency rooms, and meeting rooms. Routing in MANETs is a not simple job and has projected a huge range of attention from researchers around the world. Thus, the intention of this task is a development of a security protocol in MANET for the IoT platform. For dealing with encryption and decryption strategies to handle MANET and IoT data, a new approach is suggested through the enhanced chaotic map . Here, three improved algorithms are implemented for proposing the optimized key management scheme under a chaotic map , which is the Modified Updating-based Harris Hawks Optimization Algorithm (MU-HHO), Mean Solution-based Averaging Sailfish Optimizer (MS-ASFO), Adaptive Basic Reproduction Rate-based Coronavirus Herd Immunity Optimizer (ABRR-CHIO). In the convergence evaluation, while taking the length of plain text as 40, ABRR-CHIO shows superior performance over other techniques at the 60th iteration, which is 96\%, 95\%, 93\%, 96\%, and 80\% superior to HHO, SFO, CHIO, SA-SFO, and CHHSO. Finally, the performance evaluation is performed regarding “statistical analysis, convergence analysis , and communication overhead” to reveal the superiority of the designed model.},
  archive      = {J_COMCOM},
  author       = {P. Satyanarayana and G. Diwakar and B.V. Subbayamma and N.V. Phani Sai Kumar and M. Arun and S. Gopalakrishnan},
  doi          = {10.1016/j.comcom.2022.12.006},
  journal      = {Computer Communications},
  pages        = {262-281},
  shortjournal = {Comput. Commun.},
  title        = {Comparative analysis of new meta-heuristic-variants for privacy preservation in wireless mobile adhoc networks for IoT applications},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VoIPChain: A decentralized identity authentication in voice
over IP using blockchain. <em>COMCOM</em>, <em>198</em>, 247–261. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidentiality, availability, integrity, authentication , and non-repudiation when together became the top priority for secure communication. However, authentication is the first line of defense in all these security parameters. Unfortunately, several authentication mechanisms in the traditional methods cannot provide secure communication due to various vulnerabilities such as single point of failure and privacy issues. Furthermore, key distribution mechanisms such as Certificate Authority (CA) and Trusted Third Party (TTP) distribute keys over the central architecture fail, thus secure communication cannot occur. As a solution to these drawbacks, this paper proposes a new decentralized blockchain-based identity authentication mechanism for VoIP networks, VoIPChain. The proposed scheme utilizes the main features of the blockchain platform, such as immutability, transparency, and fault tolerance , to provide data privacy and secure communication in VoIP applications. The proposed scheme is evaluated as an actual implementation using the virtual Ethereum platform and Python language . The experimental results show that the proposed scheme is an efficient and cost-effective solution for the call process as a decentralized identity authentication system in VoIP. In addition, end-to-end secure call performance outperforms the existing blockchain-based authentication mechanisms by 30\%–70\% in terms of average time delay . The proposed scheme is almost ten times faster than the TLS process to authenticate between parties. Moreover, compared to fast but less secure basic methods over the SIP authentication the proposed scheme has an acceptable time delay in a call process.},
  archive      = {J_COMCOM},
  author       = {Mustafa Kara and Hisham R.J. Merzeh and Muhammed Ali Aydın and Hasan Hüseyin Balık},
  doi          = {10.1016/j.comcom.2022.11.019},
  journal      = {Computer Communications},
  pages        = {247-261},
  shortjournal = {Comput. Commun.},
  title        = {VoIPChain: A decentralized identity authentication in voice over IP using blockchain},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance evaluation methodologies for smart grid
substation communication networks: A survey. <em>COMCOM</em>,
<em>198</em>, 228–246. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IEC 61850 is becoming a cornerstone standard defining communication protocols and common data and device models for ensuring critical real-time monitoring and control of smart grids. Although the interoperability can be achieved by conforming to this standard, the underlying communication network still needs, however, careful configuration and dimensioning for guaranteeing reliable and real-time data exchanges. In this paper, we focus on the substation automation part of electric smart grids where data communication requires milliseconds order delay and full reliability guarantees. This arises challenging issues on the underlying Ethernet-based network architecture design, especially concerning its performance evaluation. The contributions of this paper are threefold. First, it proposes a tutorial introduction to IEC 61850 international standard for communication within substations . Second, it surveys the performance evaluation of switched Ethernet for IEC 61850 substation automation with the analysis of both network calculus and simulation based state-of-the-art approaches. Third, it discusses future research directions such as the need of in-depth traffic scheduling analysis, the potentials of introducing the Time Sensitive Networking standard into substation automation and the co-simulation of both electric grid and communication network. This paper can serve as a basis for understanding the challenging issues for modern smart Substation Communication Network design through performance evaluation approaches and how the current state-of-the-art solutions can be used or should be further extended.},
  archive      = {J_COMCOM},
  author       = {Théo Docquier and Ye-Qiong Song and Vincent Chevrier and Ludovic Pontnau and Abdelaziz Ahmed-Nacer},
  doi          = {10.1016/j.comcom.2022.11.005},
  journal      = {Computer Communications},
  pages        = {228-246},
  shortjournal = {Comput. Commun.},
  title        = {Performance evaluation methodologies for smart grid substation communication networks: A survey},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-organizing maps and full GPU parallel approach to graph
matching. <em>COMCOM</em>, <em>198</em>, 217–227. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching is an essential problem in computer science and communications. It can be applied to a variety of issues such as artificial intelligence , computer vision , and communication systems . In this paper, we propose a new Graphics Processing Unit framework written in CUDA C++ specifically dedicated to geometric graph matching but providing new parallel algorithms , with low computational complexity , as the self-organizing map in the plane, and a distributed local search method . Unlike state-of-the-art graph matching algorithms , available from Matlab platform, that most often need at least O ( N 2 ) O(N2) memory size, with N N the problem size, our proposals only require O ( N ) O(N) space and allows massively parallel execution. These parallel algorithms are evaluated and compared to the state-of-the-art methods available for graph matching and following the same experimental protocol.},
  archive      = {J_COMCOM},
  author       = {Beibei Cui and Jean-Charles Créput and Lei Zhang},
  doi          = {10.1016/j.comcom.2022.12.005},
  journal      = {Computer Communications},
  pages        = {217-227},
  shortjournal = {Comput. Commun.},
  title        = {Self-organizing maps and full GPU parallel approach to graph matching},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network traffic anomaly detection method based on
multi-scale residual classifier. <em>COMCOM</em>, <em>198</em>, 206–216.
(<a href="https://doi.org/10.1016/j.comcom.2022.10.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the current research seldom consider the multi-scale characteristics of network traffic, which may lead to an inaccurate classification of anomalies and a high false alarm rate . In this paper, a network traffic anomaly detection method based on the multi-scale residual classifier (MSRC) is proposed. We use sliding windows to divide the network traffic into subsequences with different observation scales, use the wavelet transform technology to obtain the time–frequency information of each subsequence on multiple decomposition scales, design a stacked automatic encoder (SAE) to learn the distribution of input data, calculate the reconstruction error vector by using the constructed feature space, and learn the feature information of different scales in the reconstruction error vector by using the multipath residual group, and complete traffic anomaly detection through the lightweight classifier. Experimental results show that the detection performance of the proposed method for abnormal network traffic is improved compared with the traditional method. It is proved that large observation scales and more transformation scales have positive effects on discovering the potential diversity information in the original network traffic.},
  archive      = {J_COMCOM},
  author       = {Xueyuan Duan and Yu Fu and Kun Wang},
  doi          = {10.1016/j.comcom.2022.10.024},
  journal      = {Computer Communications},
  pages        = {206-216},
  shortjournal = {Comput. Commun.},
  title        = {Network traffic anomaly detection method based on multi-scale residual classifier},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Road crash risk prediction during COVID-19 for flash crowd
traffic prevention: The case of los angeles. <em>COMCOM</em>,
<em>198</em>, 195–205. (<a
href="https://doi.org/10.1016/j.comcom.2022.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road crashes are a major problem for traffic safety management, which usually causes flash crowd traffic with a profound influence on traffic management and communication systems . In 2020, the sudden outbreak of the novel coronavirus disease (COVID-19) pandemic led to significant changes in road traffic conditions. In this paper, by analyzing crash data from 2016 to 2020 and new COVID-19 case data in 2020, we find that the average crash severity and crash deaths during this period (a rapid increase of new COVID-19 cases in 2020) are higher than those in previous four years. Hence, it is necessary to exploit a novel road crash risk prediction model for such an emergency. We propose a novel data-adaptive fatigue focal loss (DA-FFL) method by fusing fatigue factors to establish a road crash risk prediction model under the scenario of large-scale emergencies. Finally, the experimental results demonstrate that DA-FFL performs better than the other typical methods in terms of area under curve (AUC) and false alarm rate (FAR) for imbalanced data . Furthermore, DA-FFL has better prediction performance in convolutional neural networks-long short-term memory (CNN-LSTM).},
  archive      = {J_COMCOM},
  author       = {Junbo Wang and Xiusong Yang and Songcan Yu and Qing Yuan and Zhuotao Lian and Qinglin Yang},
  doi          = {10.1016/j.comcom.2022.12.002},
  journal      = {Computer Communications},
  pages        = {195-205},
  shortjournal = {Comput. Commun.},
  title        = {Road crash risk prediction during COVID-19 for flash crowd traffic prevention: The case of los angeles},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling ambient intelligence of things (AIoT) healthcare
system architectures. <em>COMCOM</em>, <em>198</em>, 186–194. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the last decade, significant research has been conducted in the fields of healthcare and the administration of cutting-edge ambient intelligence (AmI) technology. Healthcare experts explored smart gadgets and other medical technologies, as well as ambient intelligence using Internet of Things (IoT) (AIoT). The potential for linking the two regions and improving care for individuals living in rural and distant areas is apparent. The healthcare industry has seen tremendous advances in efficiency, affordability, and usefulness as a result of new growth possibilities and drastic cost reductions. This includes a how-to guide on how medical advancements may be both helpful and detrimental as a result of (AIoT-based) technology. The AIoT-H application is anticipated to be covered in the last section since it has the ability to assist with current and varied technologies, and relevant approaches for solving healthcare problems are examined. Furthermore, many potential problems and inconsistencies with the AIoT-H method are often identified. Current and future academics interested in the subject will be able to examine a wide range of possible AIoT-H implementations in order to get a better understanding of the effect and influence on any of the many AIoT applications.},
  archive      = {J_COMCOM},
  author       = {Anil Pise and Byungun Yoon and Saurabh Singh},
  doi          = {10.1016/j.comcom.2022.10.029},
  journal      = {Computer Communications},
  pages        = {186-194},
  shortjournal = {Comput. Commun.},
  title        = {Enabling ambient intelligence of things (AIoT) healthcare system architectures},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review of machine learning-based zero-day attack
detection: Challenges and future directions. <em>COMCOM</em>,
<em>198</em>, 175–185. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-day attacks exploit unknown vulnerabilities so as to avoid being detected by cybersecurity detection tools. The studies (Bilge and Dumitraş, 2012, Google, 0000, Ponemon Sullivan Privacy Report, 2020) show that zero-day attacks are wide spread and are one of the major threats to computer security. The traditional signature-based detection method is not effective in detecting zero-day attacks as the signatures of zero-day attacks are typically not available beforehand. Machine Learning (ML)-based detection method is capable of capturing attacks’ statistical characteristics and is, hence, promising for zero-day attack detection. In this survey paper, a comprehensive review of ML-based zero-day attack detection approaches is conducted, and their ML models, training and testing data sets used, and evaluation results are compared. While significant efforts have been put forth to develop accurate and robust zero-attack detection tools, the existing methods fall short in accuracy, recall, and uniformity against different types of zero-day attacks. Major challenges toward the ML-based methods are identified and future research directions are recommended at last.},
  archive      = {J_COMCOM},
  author       = {Yang Guo},
  doi          = {10.1016/j.comcom.2022.11.001},
  journal      = {Computer Communications},
  pages        = {175-185},
  shortjournal = {Comput. Commun.},
  title        = {A review of machine learning-based zero-day attack detection: Challenges and future directions},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Host load prediction in cloud computing with discrete
wavelet transformation (DWT) and bidirectional gated recurrent unit
(BiGRU) network. <em>COMCOM</em>, <em>198</em>, 157–174. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing pay-as-you-go storage and computing services have contributed to the widespread adoption of cloud computing . Using virtualization technology, cloud service providers can execute several instances on a single physical server, maximizing resource utilization. A challenging issue in cloud data centers is that available resources are rarely fully utilized. The server utilization rate is poor and often below 30\%. An accurate host workload prediction enhances resource allocation resulting in more efficient resource utilization. Recently, numerous methods based on deep learning for predicting cloud computing workload have been developed. An efficient strategy must predict long-term dependencies on nonstationary host workload data and be quick enough to respond to incoming requests. This study employs a Bidirectional Gated-Recurrent Unit (BiGRU), Discrete Wavelet Transformation (DWT), and an attention mechanism to improve the host load prediction accuracy. DWT is used to decompose input data into sub-bands with different frequencies and to extract patterns from nonlinear and nonstationary data in order to improve prediction accuracy. The extracted features are fed into BiGRu to predict future workload. The attention mechanism is used in order to extract the temporal correlation features. This hybrid model was evaluated with cluster data sets from Google and Alibaba. Experimental results reveal that our method improves prediction accuracy by 3\% to 56\% compared to a variety of state-of-the-art methods.},
  archive      = {J_COMCOM},
  author       = {Javad Dogani and Farshad Khunjush and Mehdi Seydali},
  doi          = {10.1016/j.comcom.2022.11.018},
  journal      = {Computer Communications},
  pages        = {157-174},
  shortjournal = {Comput. Commun.},
  title        = {Host load prediction in cloud computing with discrete wavelet transformation (DWT) and bidirectional gated recurrent unit (BiGRU) network},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Viral marketing branching processes. <em>COMCOM</em>,
<em>198</em>, 140–156. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the inherent timeline structure of the appearance of content in online social networks (OSNs) while studying content propagation. We model the propagation of a post/content of interest by an appropriate multi-type branching process. The branching process allows one to predict the emergence of global macro properties (e.g., the spread of a post in the network) from the laws and parameters that determine local interactions. The local interactions largely depend upon the timeline (an inverse stack capable of holding many posts and one dedicated to each user) structure and the number of friends (i.e., connections) of users, etc. We explore the use of multi-type branching processes to analyze the viral properties of the post, e.g., to derive the expected number of shares, the probability of virality of the content, etc. In OSNs, the new posts push down the existing contents in timelines, which can greatly influence content propagation; our analysis considers this influence. We find that one leads to draw incorrect conclusions when the timeline (TL) structure is ignored: (a) for instance, even less attractive posts are shown to get viral; (b) ignoring TL structure also indicates erroneous growth rates. More importantly, one cannot capture some interesting paradigm shifts/phase transitions; for example, virality chances are not monotone with network activity parameter, as shown by analysis including TL influence. In the last part, we integrate the online auctions into our viral marketing model. We study the optimization problem considering real-time bidding. We again compared the study with and without considering the TL structure for varying activity levels of the network. We find that the analysis without TL structure fails to capture the relevant phase transitions, thereby making the study incomplete.},
  archive      = {J_COMCOM},
  author       = {Ranbir Dhounchak and Veeraruna Kavitha and Eitan Altman},
  doi          = {10.1016/j.comcom.2022.11.015},
  journal      = {Computer Communications},
  pages        = {140-156},
  shortjournal = {Comput. Commun.},
  title        = {Viral marketing branching processes},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of LoRaWAN class b performances and its
optimization for better support of actuators. <em>COMCOM</em>,
<em>198</em>, 128–139. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWAN technology is of particular importance in the Internet of Things realm. It is a simple network architecture with a default-working mode optimized to sensors with limited power autonomy and a few downlink exchanges with the network server. In addition, LoRaWAN offers an optional operation mode, Class B, suitable for IoT applications based on actuators, generating periodic downlink data and requiring limited end-to-end delay. Today, Class B performance is not adequately quantified. This paper proposes a LoRaWAN Class B performance evaluation that considers the downlink transmission efficiency and delivery delay. We provide optimization for the class B working mechanism to enhance the performance. In addition, we evaluate the effects of class B configurable parameters: DL transmission opportunities and retransmission retries. The results and conclusions help optimally configure a LoRaWAN network for actuator-based IoT applications.},
  archive      = {J_COMCOM},
  author       = {Houssem Eddin Elbsir and Mohamed Kassab and Sami Bhiri and Mohamed Hedi Bedoui},
  doi          = {10.1016/j.comcom.2022.11.016},
  journal      = {Computer Communications},
  pages        = {128-139},
  shortjournal = {Comput. Commun.},
  title        = {Evaluation of LoRaWAN class b performances and its optimization for better support of actuators},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spectral clustering algorithm for intelligent grouping in
dense wireless networks. <em>COMCOM</em>, <em>198</em>, 117–127. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density of wireless networks has been increasing with the popularization of mobile devices . Dense wireless networks (DWN) present challenges such as the current spectral scarcity and the growing demand for capacity. The Restricted Access Window (RAW) mechanism was introduced by the IEEE 802.11ah amendment to improve DWN performance. RAW restricts the number of stations that can access the channel by arbitrarily separating them into groups. K-Means clustering has shown potential to find more efficient groups using the geographical coordinates of each station. However, due to the mobile and dynamic nature of such networks, location information is difficult to obtain in practice. In this paper, we consider the use of spectral clustering to increase the performance of DWN with hidden terminals . We discuss how a spectral clustering algorithm that generates RAW groups can be implemented in practice without the geographic location of each node. We also compare the performance of the spectral clustering algorithm with the standard grouping method used in IEEE 802.11ah, with the K-Means clustering ( i.e. , based on node location information), and with the hidden matrix-based regrouping (HMR) algorithm. Simulation results considering several density levels, different traffic patterns, and different propagation models indicate that spectral clustering significantly outperforms both the standard grouping and HMR in terms of collision rate , throughput, and delay. It also closely approximates – and sometimes surpasses – the performance of the K-Means clustering while being much more practical to implement because it does not require knowledge on nodes’ geographical coordinates.},
  archive      = {J_COMCOM},
  author       = {Bruna Toledo Guedes and Diego Passos and Fernanda G.O. Passos},
  doi          = {10.1016/j.comcom.2022.11.017},
  journal      = {Computer Communications},
  pages        = {117-127},
  shortjournal = {Comput. Commun.},
  title        = {A spectral clustering algorithm for intelligent grouping in dense wireless networks},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ensemble deep federated learning cyber-threat hunting
model for industrial internet of things. <em>COMCOM</em>, <em>198</em>,
108–116. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial Internet of Things (IIoT) is an emerging technology with prompt evolution in diverse applications, including critical infrastructure. While the increasing number of IIoT devices in today’s critical infrastructure enhances their efficiency and reliability, it also increases their vulnerability towards cyber-attacks. Ambient Intelligence (AmI), including machine learning techniques, is a way to handle such challenges with minimizing the human role. Although using machine learning-based techniques is increased in some applications these days, they are not widely used in IIoT environments due to the privacy issues of transferring all the data into a single machine to train the models. This paper proposes an ensemble-based deep federated learning cyber-threat hunting model to hunt the attack samples without data sharing. The proposed hunting model consists of two parallel federated-based components, one analyzes the IIoT status based on the normal situation of the network, and the other analyzes it with considering the threat situation. This model used an ensemble of classifiers to make the final decision. The proposed cyber-threat hunting model is evaluated using two test cases and compared with some works in the literature and outperformed them in the f1-score metric. Moreover, evaluations show that the proposed model acts stable in facing different numbers of clients, and its training time is faster than the centralized models with the same computational complexity .},
  archive      = {J_COMCOM},
  author       = {Amir Namavar Jahromi and Hadis Karimipour and Ali Dehghantanha},
  doi          = {10.1016/j.comcom.2022.11.009},
  journal      = {Computer Communications},
  pages        = {108-116},
  shortjournal = {Comput. Commun.},
  title        = {An ensemble deep federated learning cyber-threat hunting model for industrial internet of things},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating interactions between target users and opinion
leaders for better recommendations: An opinion dynamics approach.
<em>COMCOM</em>, <em>198</em>, 98–107. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social recommender system can accurately recommend information to users, according to their interests based on the characteristics of their social network, however, the interaction between users has not been fully captured in the existing social recommender systems. This study contributes to the literature by proposing a social recommendation method on the basis of opinion dynamics, which captures the information on the interactions between target users and opinion leaders. In our model, the impact of opinion leaders and the evolutionary opinion dynamics between opinion leaders and the target user are integrated to make a recommendation. Experiments based on two real rating datasets, Epinions and FilmTrust were conducted to test the proposed model. The results show that our proposed method can effectively solve the cold-start problem and outperforms the baseline models .},
  archive      = {J_COMCOM},
  author       = {Lijuan Weng and Qishan Zhang and Zhibin Lin and Ling Wu and Jin-Hua Zhang},
  doi          = {10.1016/j.comcom.2022.11.011},
  journal      = {Computer Communications},
  pages        = {98-107},
  shortjournal = {Comput. Commun.},
  title        = {Integrating interactions between target users and opinion leaders for better recommendations: An opinion dynamics approach},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A max plus algebra based scheduling algorithm for supporting
time triggered services in ethernet networks. <em>COMCOM</em>,
<em>198</em>, 85–97. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aerospace industry is interested in low-cost Ethernet technologies for their use as telecommunication networks within spacecraft (launchers and satellites). Current networks (e.g. 1553B bus) are characterized by low performance in terms of supported bit rates and very high maintenance costs. Some evolutions of the Ethernet standard (ARINC 664, TTEthernet, Time Sensitive Networking) have been proposing for some years now to meet the real time requirements of aerospace applications . In this paper we propose a more cost-effective solution that uses traditional Ethernet switches and defines a message scheduling algorithm capable of meeting real time requirements. Message scheduling is formulated as an optimization problem whose high computational complexity leads us to define a heuristic of polynomial complexity and based on max-plus algebra evaluations. We evaluated the scheduling algorithm for the scenario of a launcher network and observed that the performance obtained in terms of bandwidth consumption does not differ from that obtained in TTEthernet-based networks.},
  archive      = {J_COMCOM},
  author       = {Vincenzo Eramo and Tiziana Fiori and Francesco G. Lavacca and Francesco Valente and Andrea Baiocchi and Simone Ciabuschi and Marta Albano and Enrico Cavallini},
  doi          = {10.1016/j.comcom.2022.11.014},
  journal      = {Computer Communications},
  pages        = {85-97},
  shortjournal = {Comput. Commun.},
  title        = {A max plus algebra based scheduling algorithm for supporting time triggered services in ethernet networks},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain-based distributed operation and incentive
solution for p-RAN. <em>COMCOM</em>, <em>198</em>, 77–84. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operators are coming up with new solutions that use grid topology as an innovative response to future mobility requirements and the increasing management costs for wireless access networks . Proximity Radio Access Network (P-RAN), which was proposed by the 3rd Generation Partnership Project (3GPP), expands the cellular structure from base station to intelligent terminal equipment through device to device (D2D) technology, which provides a cost-effective deployment solution for the 5G and 6G. The extensive deployment of P-RAN solution should be based on the active participation of a large number of distributed intelligent equipment users. Fair, reasonable and credible incentive solution is the key to realize the sustainable operation of this distributed system. This paper introduces a technical solution that applying blockchain technology to design P-RAN operating and incentive mechanism. Through blockchain technology, the current centralized P-RAN operation and incentive can be achieved in point-to-point way. By utilizing blockchain-based identity authentication and asymmetric key pairs, user and relay can confirm and sign the network usage information in sequence to reduce disputes. The abstract of all transactions related to P-RAN services can be recorded on blockchain immutably as the calculation basis of relay rewards. Through smart contracts and P-RAN incentive policy, relays that provide more and higher quality P-RAN service will be rewarded more, which can help to create positive feedback in this decentralized system. The tokens awarded to relays will be able to exchange through the blockchain network for the increase of the whole ecosystem utility. Blockchain-based P-RAN operation and incentive solution pioneer a way to the future heterogeneous network resource sharing and collaboration for supporting Web3.0 and Metaverse.},
  archive      = {J_COMCOM},
  author       = {Xiaoou Liu and Xiaoyi Chen and Qi Bi and Wei Liang and Jingwen Li and Zheng Zhang},
  doi          = {10.1016/j.comcom.2022.11.008},
  journal      = {Computer Communications},
  pages        = {77-84},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain-based distributed operation and incentive solution for P-RAN},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-network aggregation for data center networks: A survey.
<em>COMCOM</em>, <em>198</em>, 63–76. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aggregation applications are widely deployed in data centers , such as distributed machine learning and MapReduce-like framework. These applications typically have large communication overhead , which brings extra delay and traffic pressure to data center networks. In-network aggregation (INA) technology is a new approach to accelerate aggregation tasks and reduce traffic by offloading aggregation function on network switches. In this paper, we concentrate on two aspects of INA. The first aspect is INA implementation methods. We summarize key points of INA designing, and classify INA methods into three categories according to different types of hardware: commodity programmable switch, middle box and new switch architecture. The second aspect is INA algorithm. Building aggregation tree effectively is essential to the performance of INA. Finally, we make comparisons and propose some potential challenges and opportunities for future INA research.},
  archive      = {J_COMCOM},
  author       = {Aoxiang Feng and Dezun Dong and Fei Lei and Junchao Ma and Enda Yu and Ruiqi Wang},
  doi          = {10.1016/j.comcom.2022.11.004},
  journal      = {Computer Communications},
  pages        = {63-76},
  shortjournal = {Comput. Commun.},
  title        = {In-network aggregation for data center networks: A survey},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative computation offloading and resource allocation
based on dynamic pricing in mobile edge computing. <em>COMCOM</em>,
<em>198</em>, 52–62. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of collaborative computing architecture in mobile edge computing provides an effective solution for the task processing of mobile devices with low computing capability, which can make up for computation resource and physical size restrictions of mobile devices and improve the performance of mobile communication networks in the future. In this paper, we focus on network congestion caused by increased load in hotspots. In particular, we consider the integrated network architecture of mobile edge computing (MEC) and device-to-device (D2D) communication to reduce the sum overhead of all task request users (TRUs). Accordingly, the computing offloading decision, wireless resource allocation strategy, computing resource allocation strategy, and assistant selection decision are optimized to minimize the sum overhead of the energy consumption and the cost of purchasing computing resource of all TRUs. The formulated overhead minimization problem is a mixed integer nonlinear programming problem. In order to tackle this problem, an algorithm based on the distance and the transaction price is first proposed to determine the target assistant user (TAU) of each TRU. Second, for a given TAU selection strategy, the original problem is further transformed into a convex problem and decomposed to make it more tractable. Finally, a distributed algorithm based on the alternating direction method of multipliers (ADMM) is adopted to solve the optimization problem . The simulation results prove the effectiveness of the proposed scheme in reducing user overhead.},
  archive      = {J_COMCOM},
  author       = {Jianbin Xue and Xiangrui Guan},
  doi          = {10.1016/j.comcom.2022.11.012},
  journal      = {Computer Communications},
  pages        = {52-62},
  shortjournal = {Comput. Commun.},
  title        = {Collaborative computation offloading and resource allocation based on dynamic pricing in mobile edge computing},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of DDoS attacks in D2D communications using
machine learning approach. <em>COMCOM</em>, <em>198</em>, 32–51. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In device-to-device (D2D) communications, distributed Denial-of-Service (DDoS) attacks can be quite detrimental because it can result in network structure destruction. Towards this end, the research objective of this paper is to identify and prevent DDoS and Denial-of-Service (DoS) attacks (i.e., SYN, Slowloris) in a D2D communication environment. Specifically, by replicating a real-world scenario, we emulate SLowloris attacks in a D2D communication network and generate a D2D Network-specific Slowloris dataset. This dataset along with the CICDDoS2019 dataset was then used to train our proposed Machine learning (ML) model that aids in the detection and prevention of DDoS attacks (Slowloris and SYN) in the considered D2D framework. The whole process of how to construct an emulation network for D2D communication and test it against a variety of attacks and implementations is also demonstrated in the paper. To quantify the detection accuracy in the context of DDoS and DoS attacks, we use various ML algorithms such as Random Forest , Light GBM, XGBoost , and Ada Boost and study their performance with the aid of extensive emulation. The results collected revealed that both Slowloris and CICDDoS2019 datasets achieve greater accuracy with Random Forest . Consequently, the results compel us to develop a technique for combining the identification of DDoS and DoS attacks in binary classification Random Forests with the binary decision. The proposed technique has been evaluated and compared with other related approaches in the open literature demonstrating significant performance in terms of identification and prevention time, processing and memory resources required, and device battery consumption, without affecting the accuracy of the attack identification. Hence, we advocate that our proposed technique can be extremely beneficial in preventing DDoS and DoS attacks in a D2D communication environment, where its lifetime and capabilities are mainly associated with the resources of the D2D device (i.e., CPU , Memory, and battery life).},
  archive      = {J_COMCOM},
  author       = {S.V. Jansi Rani and Iacovos Ioannou and Prabagarane Nagaradjane and Christophoros Christophorou and Vasos Vassiliou and Sai Charan and Sai Prakash and Niel Parekh and Andreas Pitsillides},
  doi          = {10.1016/j.comcom.2022.11.013},
  journal      = {Computer Communications},
  pages        = {32-51},
  shortjournal = {Comput. Commun.},
  title        = {Detection of DDoS attacks in D2D communications using machine learning approach},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards SDN-based smart contract solution for IoT access
control. <em>COMCOM</em>, <em>198</em>, 1–31. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access control is essential for the IoT environment to ensure that only approved and trusted parties are able to configure devices, access sensor information, and command actuators to execute activities. The IoT ecosystem is subject to various access control complications due to the limited latency between IoT devices and the Internet, low energy requirements of IoT devices, the distributed framework, ad-hoc networks, and an exceptionally large number of heterogeneous IoT devices that need to be managed. The motivation for this proposed work is to resolve the incurring challenges of IoT associated with management and access control security. Each IoT domain implementation has particular features and needs separate access control policies to be considered in order to design a secure solution. This research work aims to resolve the intricacy of policies management, forged policies, dissemination, tracking of access control policies, automation, and central management of IoT nodes and provides a trackable and auditable access control policy management system that prevents forged policy dissemination by applying Software Defined Network (SDN) and blockchain technology in an IoT environment. Integration of SDN and blockchain provides a robust solution for IoT environment security. Recently, smart contracts have become one of blockchain technology’s most promising applications. The integration of smart contracts with blockchain technology provides the capability of designing tamper-proof and independently verifiable policies. In this paper, we propose a novel, scalable solution for implementing immutable, verifiable, adaptive, and automated access control policies for IoT devices together with a successful proof of concept that demonstrates the scalability of the proposed solution. The performance of the proposed solution is evaluated in terms of throughput and resource access delay between the blockchain component and the controller as well as from node to node. The number of nodes in the IoT network and the number of resource access requests were independently and systematically increased during the evaluations. The results illustrate that the resource access delay and throughput were affected neither linearly nor exponentially; hence, the proposed solution shows no significant degradation in performance with an increase in the number of nodes and/or requests.},
  archive      = {J_COMCOM},
  author       = {Mizna Khalid and Sufian Hameed and Abdul Qadir and Syed Attique Shah and Dirk Draheim},
  doi          = {10.1016/j.comcom.2022.11.007},
  journal      = {Computer Communications},
  pages        = {1-31},
  shortjournal = {Comput. Commun.},
  title        = {Towards SDN-based smart contract solution for IoT access control},
  volume       = {198},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On performance of multi-RIS assisted multi-user
nonorthogonal multiple access system over nakagami-m fading channels.
<em>COMCOM</em>, <em>197</em>, 294–305. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, multiple reconfigurable intelligent surfaces (RISs) may be deployed to assist wireless systems . Unfortunately, the mathematical analysis of multiple RISs aided wireless systems is challenging. In this paper, multiple RISs are utilized in a multi-user nonorthogonal multiple access (NOMA) system. Particularly, multiple users combine signals traveling on the transmitter–user direct paths and the reflected paths from the RISs. We successfully derive the closed-form expressions of the outage probability (OP) and ergodic capacity (EC) of this multi-RIS aided multi-user NOMA (RIS-NOMA) system. We also obtain the OP and EC expressions of the RIS-NOMA system in the case that there are only two NOMA users, where the specific derived forms depend on the exact values of power allocation coefficients and threshold. We validate all derived expressions through Monte-Carlo simulations. Numerical results show that compared with the case without RISs, the performance in terms of OP and EC of the NOMA system with RISs are significantly improved. In particular, the OP in the case with RISs is greatly lower than that in the case without RISs for the considered range of transmission power. However, the EC in the case with RISs is only significantly higher than that in the case without RISs in the low transmission power regime. In the high transmission power regime, they are similar. Consequently, depending on the certain requirements in practical scenarios where the RIS-NOMA system is deployed, we can use a suitable transmission power to achieve the OP and EC performance target and save the power consumption .},
  archive      = {J_COMCOM},
  author       = {Ba Cao Nguyen and Le The Dung and Tran Manh Hoang and Nguyen Van Vinh and Gia Thien Luu},
  doi          = {10.1016/j.comcom.2022.11.010},
  journal      = {Computer Communications},
  pages        = {294-305},
  shortjournal = {Comput. Commun.},
  title        = {On performance of multi-RIS assisted multi-user nonorthogonal multiple access system over nakagami-m fading channels},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CO-CAC: A new approach to call admission control for VoIP in
5G/WiFi UAV-based relay networks. <em>COMCOM</em>, <em>197</em>,
284–293. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voice over IP (VoIP) requires a Call Admission Control (CAC) mechanism in WiFi networks to preserve VoIP packet flows from excessive network delay or packet loss . Ideally, this mechanism should be integrated with the operational scenario, guarantee the quality of service of active calls, and maximize the number of concurrent calls. This paper presents a novel CAC scheme for VoIP in the context of a WiFi access network deployed with Unmanned Aerial Vehicles (UAVs) that relay to a backhaul 5G network . Our system, named Codec-Optimization CAC (CO-CAC), is integrated into each drone. It intercepts VoIP call control messages and decides on the admission of every new call based on a prediction of the WiFi network’s congestion level and the minimum quality of service desired for VoIP calls. To maximize the number of concurrent calls, CO-CAC proactively optimizes the codec settings of active calls by exchanging signaling with VoIP users. We have simulated CO-CAC in a 50 m × × 50 m scenario with four UAVs providing VoIP service to up to 200 ground users with IEEE 802.11ac WiFi terminals. Our results show that without CAC, the number of calls that did not meet a minimum quality level during the simulation was 10\% and 90\%, for 50 and 200 users, respectively. However, when CO-CAC was in place, all calls achieved minimum quality for up to 90 users without rejecting any call. For 200 users, only 25\% of call attempts were rejected by the admission control scheme. These results were narrowly worse when the ground users moved randomly in the scenario.},
  archive      = {J_COMCOM},
  author       = {Vicente Mayor and Rafael Estepa and Antonio Estepa},
  doi          = {10.1016/j.comcom.2022.11.006},
  journal      = {Computer Communications},
  pages        = {284-293},
  shortjournal = {Comput. Commun.},
  title        = {CO-CAC: A new approach to call admission control for VoIP in 5G/WiFi UAV-based relay networks},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cellular traffic prediction via deep state space models with
attention mechanism. <em>COMCOM</em>, <em>197</em>, 276–283. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular traffic prediction is of great importance for operators to manage network resources and make decisions. Traffic is highly dynamic and influenced by many exogenous factors , which would lead to the degradation of traffic prediction accuracy. This paper proposes an end-to-end framework with two variants to explicitly characterize the spatiotemporal patterns of cellular traffic among neighboring cells. It uses convolutional neural networks with an attention mechanism to capture the spatial dynamics and Kalman filter for temporal modeling . Besides, we can fully exploit the auxiliary information such as social activities to improve prediction performance. We conduct extensive experiments on three real-world datasets. The results show that our proposed models outperform the state-of-the-art machine learning techniques in terms of prediction accuracy.},
  archive      = {J_COMCOM},
  author       = {Hui Ma and Kai Yang and Man-On Pun},
  doi          = {10.1016/j.comcom.2022.10.023},
  journal      = {Computer Communications},
  pages        = {276-283},
  shortjournal = {Comput. Commun.},
  title        = {Cellular traffic prediction via deep state space models with attention mechanism},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Static vulnerability mining of IoT devices based on control
flow graph construction and graph embedding network. <em>COMCOM</em>,
<em>197</em>, 267–275. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic static vulnerability analysis for IoT devices is always an important and challenging research problem. Traditional vulnerability finding methods are primarily based on manually built structures, which have limitations in accuracy and lack consideration of environmental information. In this paper, we propose a new approach that generates an ACFG (attributed control flow graph) that combines ambient information with binary code information, which aims to discover vulnerabilities from binaries deeply and accurately. The graph is then transformed by a graph-embedding algorithm and analyzed by a deep neural network . This approach has scalability on IoT devices when cross-architecture and cross-system binaries are considered. Ambient information makes this model able to detect environment-aware vulnerabilities. Experiment shows that this method has outperformed the state-of-the-art methods in terms of accuracy, efficiency, and scalability, with an average accuracy of over 80\% on real-world vulnerability datasets.},
  archive      = {J_COMCOM},
  author       = {Yuan Cheng and Baojiang Cui and Chen Chen and Thar Baker and Tao Qi},
  doi          = {10.1016/j.comcom.2022.10.021},
  journal      = {Computer Communications},
  pages        = {267-275},
  shortjournal = {Comput. Commun.},
  title        = {Static vulnerability mining of IoT devices based on control flow graph construction and graph embedding network},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An energy-optimized embedded load balancing using DVFS
computing in cloud data centers. <em>COMCOM</em>, <em>197</em>, 255–266.
(<a href="https://doi.org/10.1016/j.comcom.2022.10.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task scheduling is a significant challenge in the cloud environment as it affects the network’s performance regarding the workload of the cloud machines. It also directly impacts the consumed energy, therefore the profit of the cloud provider. This paper proposed an algorithm that prioritizes the tasks regarding their execution deadline. We also categorize the physical machines considering their configuration status. Henceforth, the proposed method assigns the jobs to the physical machines with the same priority class close to the user. Furthermore, we reduce the consumed energy of the machines processing the low-priority tasks using the DVFS method. The proposed method migrates the jobs to maintain the workload balance, or if the machines’ class changed according to their scores. We have evaluated and validated the proposed method in the CloudSim library. The simulation results demonstrate that the proposed method optimized energy consumption by 12\% and power consumption by 20\%.},
  archive      = {J_COMCOM},
  author       = {Amir Javadpour and Arun Kumar Sangaiah and Pedro Pinto and Forough Ja’fari and Weizhe Zhang and Ali Majed Hossein Abadi and HamidReza Ahmadi},
  doi          = {10.1016/j.comcom.2022.10.019},
  journal      = {Computer Communications},
  pages        = {255-266},
  shortjournal = {Comput. Commun.},
  title        = {An energy-optimized embedded load balancing using DVFS computing in cloud data centers},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification and regression tree (CART) based resource
allocation scheme for wireless sensor networks. <em>COMCOM</em>,
<em>197</em>, 242–254. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 21st century is the era of smart sensors , intelligent computations, and communication technologies. Wireless Sensor Networks (WSN) play a vital role in performing many remote applications without human intervention. WSNs are an integral part of an ubiquitous computing and Internet of Everything (IoE). A WSN is a decentralized network comprised of several tiny but powerful sensor nodes . Sensor nodes are restricted in terms of battery life, communication range, bandwidth, processing latency, and memory. Effective usage of WSN resources is a challenging task to enhance network lifespan, increase throughput, reduce computational delay and minimize control overheads. Several intelligent strategies are proposed to improve WSN performance and enhance the lifespan of the network by adopting intelligent resource management schemes. In WSN, effective and intelligent resource management involves resource discovery, resource scheduling, and resource allocation. In this paper, a Classification and Regression Tree (CART) supervised machine learning algorithm is used to deal with incomplete information about the network i.e., uncertainty and dynamicity for effective resource allocation. The scheme operates as follows: The k-means clustering algorithm is applied to the network, clusters are formed, the Cluster Head (CH) is selected, and the k-NN algorithm is applied to find the number of neighbor nodes i.e., Cluster Members (CM) in the cluster. The attributes of CH like distance from base station , degree of connectivity, congestion rate, data type and size aggregated at CH after performing a task and channel quality are calculated. Aggregation and classification of CM and CH attributes (data sets) use intelligent search and feature selection algorithms . The data sets are then processed for the training and prediction phases . A decision tree model is built using target attribute, which is resource (bandwidth) allocation. A heat map and confusion matrix are generated and a performance evaluation of the proposed scheme is done. Simulation results show that the performance of the proposed CART based resource allocation approach is better as compared with the “Linear Regression (LR), Iterative Dichotomiser 3 (ID3), and Neural Network (NN)” schemes in terms of resource allocation accuracy, allocation computational delay, data transmission efficiency, etc.},
  archive      = {J_COMCOM},
  author       = {Gururaj S. Kori and Mahabaleshwar S. Kakkasageri},
  doi          = {10.1016/j.comcom.2022.11.003},
  journal      = {Computer Communications},
  pages        = {242-254},
  shortjournal = {Comput. Commun.},
  title        = {Classification and regression tree (CART) based resource allocation scheme for wireless sensor networks},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving spam filtering using homomorphic and
functional encryption. <em>COMCOM</em>, <em>197</em>, 230–241. (<a
href="https://doi.org/10.1016/j.comcom.2022.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional spam classification requires the end-users to reveal the content of incoming emails to a classifier so that text analysis can be performed. On the other hand, new cryptographic primitives allow this classification task to be performed on encrypted emails without revealing the email contents, hence preserves user data privacy. In this paper, we construct a spam classification framework that enables the classification of encrypted emails. Our model is based on a neural network with a quadratic network component and a multi-layer perceptron network component. The quadratic network architecture is compatible with the operation of an existing quadratic functional encryption scheme . To protect email content privacy, we proposed two spam classification solutions based on homomorphic encryption (HE) and functional encryption (FE) that enables our classifiers to predict the label of encrypted emails. The evaluation results on real-world spam datasets indicate that our proposed spam classification solutions achieve accuracies over 95\%. Our performance study and security analysis provide pros and cons of each proposed solution. For instance, the FE solution predicts a label of an encrypted email in less than 31 s whereas the HE solution takes up to 265 s to do so. Nonetheless, the HE solution is not prone to potential information leakage as the FE solution.},
  archive      = {J_COMCOM},
  author       = {Tham Nguyen and Naveen Karunanayake and Sicong Wang and Suranga Seneviratne and Peizhao Hu},
  doi          = {10.1016/j.comcom.2022.11.002},
  journal      = {Computer Communications},
  pages        = {230-241},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-preserving spam filtering using homomorphic and functional encryption},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LK-AKA: A lightweight location key-based authentication and
key agreement protocol for S2S communication. <em>COMCOM</em>,
<em>197</em>, 214–229. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a supplement to terrestrial networks , satellite communication networks have a lot of advantages, such as wide coverage, large communication capacity, reliable communication quality, and seamless global connectivity, which are highly competitive with traditional communication technologies. However, there are still many security issues and efficiency problems in satellite networks due to the characteristics of satellite communications, such as highly exposed satellite links, dynamical topology, and the limited computational power and storage resources of satellites. Therefore, it is a crucial issue to design a lightweight and secure authentication mechanism to ensure the security of satellite-to-satellite communication (S2S). This paper proposes a lightweight authentication and key agreement scheme for S2S communication based on the symmetric cryptographic system and the satellite location key (LK) constructed by satellite orbital parameters and location information. The formal verification tool Scyther is employed to evaluate the proposed scheme’s security. The security analysis results show that it has good performance in providing security properties and resistance to typical attacks, such as replay attacks, impersonation attacks, and Man-In-The-Middle (MITM) attacks. In addition, the performance analysis results demonstrate that our scheme has desired efficiency in terms of signaling overhead, computational overhead, and bandwidth overhead.},
  archive      = {J_COMCOM},
  author       = {Yuanyuan Yang and Jin Cao and Xiongpeng Ren and Ben Niu and Yinghui Zhang and Hui Li},
  doi          = {10.1016/j.comcom.2022.10.028},
  journal      = {Computer Communications},
  pages        = {214-229},
  shortjournal = {Comput. Commun.},
  title        = {LK-AKA: A lightweight location key-based authentication and key agreement protocol for S2S communication},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey on age of information in massive IoT
networks. <em>COMCOM</em>, <em>197</em>, 199–213. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambient intelligence (AmI) represents the future vision of intelligent computing that can bring intelligence to our daily life through various domains. In such applications, AmI is often subject to the freshness of information collected, which is commonly quantified by a relatively newer metric called age of information (AoI). In the data aggregation and analytics for Internet-of-Things (IoT) AmI, AoI should be well managed, because information update should be as timely as possible to achieve optimal performances. AoI has been studied in various applications using different queuing policies, scheduling algorithms , and multiple access schemes , in which each component of communication and information systems are designed and analyzed to improve the AoI. This paper provides a comprehensive overview of literature on the AoI and its variants in large-scale networks. AoI in IoT systems depends on the arrival rate at the source nodes, queuing policy adopted at the nodes, the scheduling of nodes for information transmission and the access scheme adopted by the nodes. To better design and operate the AmI applications that require the freshness of information, we discuss the impacts of the queuing policy, stochastic modeling, scheduling, and multiple access schemes. In particular, non-orthogonal multiple access (NOMA), which is regarded as one of the key technologies in beyond 5G and 6G, and it hybrid version combined with the conventional orthogonal multiple access (OMA) are discussed in the context of AoI. In addition, we identify promising research opportunities in potential age-sensitive applications. Thus, compared to the existing surveys on AoI, this paper provide more practical and up-to-date design guidelines for the applications with the information freshness requirements.},
  archive      = {J_COMCOM},
  author       = {Qamar Abbas and Syed Ali Hassan and Hassaan Khaliq Qureshi and Kapal Dev and Haejoon Jung},
  doi          = {10.1016/j.comcom.2022.10.018},
  journal      = {Computer Communications},
  pages        = {199-213},
  shortjournal = {Comput. Commun.},
  title        = {A comprehensive survey on age of information in massive IoT networks},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid caching strategy for information-centric satellite
networks based on node classification and popular content awareness.
<em>COMCOM</em>, <em>197</em>, 186–198. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellite networks are inexpensive to deploy and have wide coverage, and are widely available for differentiated content distribution services. With the development of on-board storage and computing capabilities, in-network caching technology in information centric networking (ICN) has proven to be an effective way to increase the throughput and content distribution efficiency of satellite networks. However, traditional caching and distribution schemes, which do not take into account the high-speed motion of satellite nodes and dynamic changes in topology, are not applicable to satellite networks. To address these issues, a hybrid caching strategy for satellite networks based on node classification and popular content awareness (NCPCA) is proposed. Firstly, the time slot partitioning method based on interlayer similarity is proposed to transform the dynamically changing process into a set of time slots with stable topology. Next, the satellite nodes are dynamically divided into two categories by fully considering the changes in connection relationships and interaction order during the spatial–temporal evolution of the satellite nodes. Nodes with satellite topological and functional characteristics are screened out as core nodes with TOPSIS algorithm, and the remaining nodes are regarded as edge nodes. Finally, a probabilistic caching scheme based on content popularity is adopted with core nodes as caching nodes to ensure caching performance and promote the diversity of cached content . Simulation results show that this strategy can effectively improve cache hit rate , reduce user request delay and content fetching hops, and promote the stable operation of satellite networks compared with other caching strategies.},
  archive      = {J_COMCOM},
  author       = {Rui Xu and Xiaoqiang Di and Jing Chen and Haowei Wang and Hao Luo and Hui Qi and Xiongwen He and Wenping Lei and Shiwei Zhang},
  doi          = {10.1016/j.comcom.2022.10.025},
  journal      = {Computer Communications},
  pages        = {186-198},
  shortjournal = {Comput. Commun.},
  title        = {A hybrid caching strategy for information-centric satellite networks based on node classification and popular content awareness},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impact on blockchain-based AI/ML-enabled big data analytics
for cognitive internet of things environment. <em>COMCOM</em>,
<em>197</em>, 173–185. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive Internet of Things (CIoT) supports the organizations to learn from the information (data) arriving from various connected devices, sensors, machines and other sources, and at the same time it inspires intelligence into different business operations, products, customer experiences, and people. Data poising attacks are very serious concerns because they may play a significant factor for businesses and organizations for both financial terms and damaging their reputations, when the Big data analytics on the analyzed data is itself corrupted. To mitigate this issue, in this paper, we suggest a blockchain-based Artificial Intelligence(AI)/Machine Learning(ML)-enabled Big data analytics mechanism for CIoT environment. The comprehensive experimental results have been provided under two circumstances: (1) performance of the ML model under data poisoning attacks and (2) performance of the ML model without data poisoning attacks. In the first case, we show how the data poison attacks can effect the ML model when the data is on some cloud storage (i.e. not in the blockchains), whereas in the second case we show the effect when the data is in the blockchains (i.e., without data poisoning attacks). The experimental results demonstrate that we have significant gains in performance in terms of accuracy, recall, precision and F1 score when there are no data poisoning attacks on the data. Moreover, a detailed blockchain simulation has carried out to demonstrate the practical aspects of the proposed security framework.},
  archive      = {J_COMCOM},
  author       = {Ankush Mitra and Basudeb Bera and Ashok Kumar Das and Sajjad Shaukat Jamal and Ilsun You},
  doi          = {10.1016/j.comcom.2022.10.010},
  journal      = {Computer Communications},
  pages        = {173-185},
  shortjournal = {Comput. Commun.},
  title        = {Impact on blockchain-based AI/ML-enabled big data analytics for cognitive internet of things environment},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReTREM: A responsibility based trust revision model for
determining trustworthiness of fog nodes. <em>COMCOM</em>, <em>197</em>,
159–172. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing complements the Internet of Things (IoT)-Cloud paradigm by enabling local computation near IoT devices. Fog nodes support IoT applications such as health care monitoring, drone surveillance , and accident reporting. A minor breach or inaccurate results (caused when applications are placed at malicious fog nodes) can cause chaos or severe damage to users. Hence, it is of utmost vital that these applications must be processed at trustful fog nodes. In this work, we have proposed a responsibility-based trust revision model, named ReTREM, for determining and monitoring the trustworthiness of fog nodes. ReTREM categorizes fog nodes based on their response when they violate the promised service quality, i.e., their intention behind malicious behavior . A separate penalty mechanism is proposed for each category of fog node to discourage their malicious behavior. ReTREM considers two types of trust scores: T S 1 TS1 and T S 2 TS2 . T S 1 TS1 is calculated based on various metrics related to trust, such as reliability decay, data integrity, social rating, and feedback rating, while the fog broker gives T S 2 TS2 . Trust score ( T S ) (TS) , i.e., the sum of T S 1 TS1 and T S 2 TS2 , is revised based on various proposed penalty mechanisms. The proposed model rewards the fog node that does not violate the promised service. The proposed model is simulated and compared with the two variants of application placement, and it outperforms. The proposed model encourages fog nodes to provide the best service and increases the users’ faith. The proposed model also tackles various trust-based attacks resulting in fair competition and opportunity among fog nodes.},
  archive      = {J_COMCOM},
  author       = {Ravi Yadav and Gaurav Baranwal},
  doi          = {10.1016/j.comcom.2022.10.022},
  journal      = {Computer Communications},
  pages        = {159-172},
  shortjournal = {Comput. Commun.},
  title        = {ReTREM: A responsibility based trust revision model for determining trustworthiness of fog nodes},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A location-aware RF-assisted MAC protocol for sectorized
vehicular visible light communications. <em>COMCOM</em>, <em>197</em>,
151–158. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Visible Light Communications (V-VLC) has emerged as a viable technology complementing RF-based communication in automotive scenarios. This is mainly due to properties such as the large unlicensed spectrum and the intrinsic security due to the Line Of Sight (LOS) requirement. In this context, one aspect of V-VLC needs further attention given the current state of the art, namely medium access under multi-user interference. In this paper, we extensively study interference in typical vehicular scenarios . Based on the findings from this study, we propose a novel approach for medium access. We follow a location-aware cross-layer approach that exploits the Space Division Multiple Access (SDMA) feature of modern matrix lighting modules to avoid interference and thus collisions. Making use of heterogeneous communication concepts, in which vehicles share their positions via the Radio Frequency (RF) channel, V-VLC transmissions can be scheduled accordingly. In an extensive simulation study using a realistic urban scenario, we first identify critical interference scenarios and then assess the efficacy of our proposed solution. We also investigate the impact of position uncertainty due to, e.g., GPS errors. Our results clearly indicate the benefits of a location-aware protocol that exploits the space-division features of the matrix lights.},
  archive      = {J_COMCOM},
  author       = {Agon Memedi and Falko Dressler},
  doi          = {10.1016/j.comcom.2022.10.020},
  journal      = {Computer Communications},
  pages        = {151-158},
  shortjournal = {Comput. Commun.},
  title        = {A location-aware RF-assisted MAC protocol for sectorized vehicular visible light communications},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network load prediction and anomaly detection using ensemble
learning in 5G cellular networks. <em>COMCOM</em>, <em>197</em>,
141–150. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network data analytics significantly improved the 5G cellular networks . Data analytics allows network administrators and operators to use the machine and deep learning to analyse the network data efficiently. The standard protocols defined by the 3rd Generation Partnership Project (3GPP) for the network data analytics function are discussed to incorporate into the dataset. The dataset is based on cells in the network considering anomalies and fields of 3GPP, i.e., data rates and information related to the network area. Moreover, machine and deep learning techniques can be used to classify the anomalies. In this regard, we employed Decision trees (DT), Random Forest (RF), Support Vector Machines (SVM) and ensemble learning (EL) to enhance the network prediction performance. For this purpose, we used machine and deep learning techniques, i.e., one-dimensional Convolutional Neural Networks (1D CNN), Multi-Layer Perceptron (MLP), and k-Nearest Neighbours (kNN), respectively. We also used bagging-based three regressors , i.e., 1D CNN, MLP, and kNN, to predict the network load. In addition, we addressed both anomaly detection and load prediction because the presence of anomalies results in high load. The accurate detection of anomalies will result in less network load. Thus, anomalies like a sudden increase in network traffic from a certain cell are also added based on the network traffic pattern to make the dataset more realistic. The simulation results showed that the bagging-based EL outperformed the existing techniques in predicting network load. Moreover, the voting technique outperforms in the case of anomaly detection.},
  archive      = {J_COMCOM},
  author       = {Usman Haider and Muhammad Waqas and Muhammad Hanif and Hisham Alasmary and Saeed Mian Qaisar},
  doi          = {10.1016/j.comcom.2022.10.017},
  journal      = {Computer Communications},
  pages        = {141-150},
  shortjournal = {Comput. Commun.},
  title        = {Network load prediction and anomaly detection using ensemble learning in 5G cellular networks},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of secured blockchain based decentralized
authentication protocol for sensor networks with auditing and
accountability. <em>COMCOM</em>, <em>197</em>, 124–140. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional system designed by incorporating the Wireless Sensor Network (WSN) suffers from important issues such as centralization of information, single source of trust, and fails to provide a trusted data-auditing solution due to the involvement of Third Party Auditor (TPA). This article proposes a novel solution by leveraging the concept of blockchain and Distributed Data Storage Service (DDSS) system to mitigate the issues mentioned above. In our model, the sensor nodes are accountable for their activity because the blockchain permanently records the logs of sensor nodes. The proposed solution provides the decentralized authentication of sensor nodes and the information stored in the fully distributed system. Furthermore, a decentralized data-auditing scheme without involving the TPA is also suggested, which finally benefits the user in terms of bandwidth and money. The formal verification of the proposed protocol is done using the Scyther simulator tool, which indicates that the protocol is safe and can protect against the relevant attacks with 100\% accuracy. We have estimated the proposed scheme’s time complexity and computational efficiencies and compared them with the existing one. The computation and communication overhead of the proposed method is reduced by 22.143\% and 12.5\% compared with the Khalid et al. (2020) scheme. Lastly, the ethereum platform simulates our solution, confirming that less than USD 2 is required to execute the proposed solution.},
  archive      = {J_COMCOM},
  author       = {Sanjeev Kumar Dwivedi and Ruhul Amin and Satyanarayana Vollala},
  doi          = {10.1016/j.comcom.2022.10.016},
  journal      = {Computer Communications},
  pages        = {124-140},
  shortjournal = {Comput. Commun.},
  title        = {Design of secured blockchain based decentralized authentication protocol for sensor networks with auditing and accountability},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient computation offloading and resource allocation
algorithm in RIS empowered MEC. <em>COMCOM</em>, <em>197</em>, 113–123.
(<a href="https://doi.org/10.1016/j.comcom.2022.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) enables mobile devices (MDs) to offload computation-intensive tasks to edge servers to support a variety of latency-sensitive emerging applications (such as the Internet of Vehicles, real-time video analytics , etc.). However, the time-varying communication link environment of signal occlusion and interference between MDs and edge servers often leads to disappointing offloading benefits. Reconfigurable intelligent surface (RIS) is recognized as a promising technology in sixth-generation communication networks, with great potential to intelligently adjust the phase shift and amplitude of reflective elements to enhance wireless network capabilities. This paper proposes a novel computation offloading algorithm for RIS empowered MEC networks. Specifically, we comprehensively consider the optimization problems of delay, energy consumption, and operator cost in the process of computation offloading, and model it as a Markov decision process . To overcome the continuous action space challenge, we propose a computation offloading algorithm based on Deep Deterministic Policy Gradient (DDPG) to jointly optimize the phase shift and amplitude of RIS, offloading decision, and MEC resource allocation strategy. Finally, compared with various other benchmark algorithms, our proposed algorithm has a significant performance improvement over non-RIS learning algorithms and other classical algorithms, and maintains the optimal performance.},
  archive      = {J_COMCOM},
  author       = {Xiangjun Zhang and Weiguo Wu and Song Liu and Jinyu Wang},
  doi          = {10.1016/j.comcom.2022.10.012},
  journal      = {Computer Communications},
  pages        = {113-123},
  shortjournal = {Comput. Commun.},
  title        = {An efficient computation offloading and resource allocation algorithm in RIS empowered MEC},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on blockchain solutions in DDoS attacks mitigation:
Techniques, open challenges and future directions. <em>COMCOM</em>,
<em>197</em>, 96–112. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of new technologies such as the Internet of Things (IoT) and Software-Defined Networking (SDN) in recent years, the Distributed Denial of Service (DDoS) attack vector has broadened and opened new opportunities for more sophisticated DDoS attacks on the targeted victims. The new attack vector includes unsecured and vulnerable IoT devices connected to the internet, and denial of service vulnerabilities like southbound channel saturation in the SDN architecture. Given the high-volume and pervasive nature of these attacks, it is beneficial for stakeholders to collaborate in detecting and mitigating the denial of service attacks promptly. Blockchain technology is considered to improve the security aspects owing to the decentralized design , secured distributed storage, and privacy. A thorough exploration and classification of blockchain techniques used for DDoS attack mitigation are not explored in the prior art. This paper reviews and categorizes state-of-the-art DDoS mitigation solutions based on blockchain technology. The DDoS mitigation techniques are classified based on the solution deployment location i.e. network-based, near attacker location, near victim location, and hybrid solutions in the network architecture with emphasis on the IoT and SDN architectures. Additionally, based on our study, the research challenges and future directions to implement the blockchain based DDoS mitigation solutions are discussed.},
  archive      = {J_COMCOM},
  author       = {Rajasekhar Chaganti and Bharat Bhushan and Vinayakumar Ravi},
  doi          = {10.1016/j.comcom.2022.10.026},
  journal      = {Computer Communications},
  pages        = {96-112},
  shortjournal = {Comput. Commun.},
  title        = {A survey on blockchain solutions in DDoS attacks mitigation: Techniques, open challenges and future directions},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSI-based location-independent human activity recognition
with parallel convolutional networks. <em>COMCOM</em>, <em>197</em>,
87–95. (<a href="https://doi.org/10.1016/j.comcom.2022.10.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) based on Wi-Fi has a broad application prospect in human–computer interaction. Since Wi-Fi signals are sensitive to the environmental changes, the features of the same category of human activity at different locations have significant difference. The existing HAR systems based on Wi-Fi need to re-collect samples or retrain models when recognizing the same activity at new locations, which reduces their practicability in human–computer interaction. To address this challenge, this paper proposes a CSI-based Parallel Convolutional Networks-based location-independent HAR system (CSI-PCNH). CSI-PCNH enhances the inter-class difference by extracting the inter-class features of the different activity samples. In addition, CSI-PCNH improves the generalization ability of activity recognition at any location by extracting the intra-class features of the same category of activity at different locations. In order to obtain the inter-class features and intra-class features of activity samples, we design a parallel convolutional network model which is composed of 3DCNN combined with Channel Attention Mechanism (CAM) and 2DCNN with LSTM to extract the global and local spatial–temporal features of the activity samples. The experimental results show that in the 8 m × 7 m indoor area, the proposed HAR system trained by the activity samples at 12 known locations, the average recognition accuracy for 6 categories of activities at any other 10 locations can reach 91.7\%.},
  archive      = {J_COMCOM},
  author       = {Yong Zhang and Yuqing Yin and Yujie Wang and Jiaqiu Ai and Dingchao Wu},
  doi          = {10.1016/j.comcom.2022.10.027},
  journal      = {Computer Communications},
  pages        = {87-95},
  shortjournal = {Comput. Commun.},
  title        = {CSI-based location-independent human activity recognition with parallel convolutional networks},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An area autonomous routing protocol based on multi-objective
optimization methods for field observation instrument network.
<em>COMCOM</em>, <em>197</em>, 71–86. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cold and arid regions of China occupy a large proportion of the total land area, are rich in resources and have a prominent strategic position, but their fragile ecological environment seriously affects the information collection and lifecycle of the field observation instrument network (FOIN), which affects the in-depth research for cold and arid regions. To balance the energy consumption and improve performance of the FOIN, an area autonomous routing protocol based on multi-objective optimization methods for FOIN (FOI-MOC) was proposed Firstly, in the network preparation stage, the FOI-MOC algorithm calculates the number of optimal cluster heads , evenly partitions for FOIN, and allocates the number of regional cluster heads. Then, in the cluster establishment stage, the different objective functions are constructed based on the residual energy , distance, and density of nodes in their respective regions. Multi-objective optimization algorithms, NSGAII and PSO are utilized to address the Pareto optimal solution set The Pareto optimal solution set is scored by dynamically assigning weights to each objective function through the entropy method , and final cluster heads are elected for each region. Finally, in the data transmission stage, single-hop transmission is adopted within clusters, and single-hop or multi-hop transmission is employed between clusters according to the distance between cluster head and base station . The experimental results indicate that the developed protocol based on multi-objective optimization methods can efficiently balance network energy consumption and prolong network lifecycle.},
  archive      = {J_COMCOM},
  author       = {Jiuyuan Huo and Shubin Lu and Jiguang Yang and Lei Wang and Hamzah Murad Mohammed AL-Neshmi},
  doi          = {10.1016/j.comcom.2022.10.015},
  journal      = {Computer Communications},
  pages        = {71-86},
  shortjournal = {Comput. Commun.},
  title        = {An area autonomous routing protocol based on multi-objective optimization methods for field observation instrument network},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Femtocell deployment for scalable video transmission in 5G
networks. <em>COMCOM</em>, <em>197</em>, 61–70. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a femtocell deployment (FD) scheme to transmit scalable video traffic in 5G Heterogeneous Networks (HetNets). Our goal is to use the flexibility of scalable video coding (SVC) for providing video service for the maximum number of users with the best quality of experience (QoE) and the lowest power consumption . The number of required Femto Base Stations (FBSs) is determined by solving a special case of a multiple fractional knapsack problem (MFKP) with three objectives called MU, MQ, and MP. They are defined as maximizing the number of users receiving video service, maximizing the mean QoE, and minimizing power consumption, respectively. To this end, first, a near-optimal solution to the problem is achieved by genetic algorithm (GA). Then, we propose a maximum resource-efficient (ME) solution based on a greedy algorithm which is close to the near-optimal solution with lower computational complexity . The performance evaluation of the network with FD shows that deploying FBSs provides video streaming services for a greater number of users with higher QoE and lower power consumption compared to the network without FD. Moreover, the combination of FD and SVC improves the network performance compared to the combination with non-scalable video bitstreams . Due to the flexibility of scalable video, by using a scalable video bitstream, instead of increasing the number of FBSs, enough capacity for serving more users is provided by decreasing the number of received video layers.},
  archive      = {J_COMCOM},
  author       = {Majid Abiri and Mehri Mehrjoo and Mehdi Rezaei},
  doi          = {10.1016/j.comcom.2022.10.008},
  journal      = {Computer Communications},
  pages        = {61-70},
  shortjournal = {Comput. Commun.},
  title        = {Femtocell deployment for scalable video transmission in 5G networks},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Swin-t-NFC CRFs: An encoder–decoder neural model for
high-precision UAV positioning via point cloud super resolution and
image semantic segmentation. <em>COMCOM</em>, <em>197</em>, 52–60. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud and remote sensing images have been the primary data types for the development of high-precision positioning systems. Equipped with a LiDAR and a camera, an Unmanned Aerial Vehicle (UAV) can explore an uncharted territory and gather both 3D scans and aerial images in real time to dynamically inspect the surroundings. However, the high cost of a high-resolution LiDAR hinders the development of the perception module of an UAV. Also, it is essential to adopt accurate image semantic segmentation (SemSeg) algorithms to better understand the sensing environment. As hardware advancement is ongoing, support from the software side is crucial. A promising strategy for cost control in building a LiDAR-based positioning system is through point cloud super-resolution (SupRes), a technique that improves the point cloud resolution via algorithms. This study investigates a deep learning-based framework that adopts a classic encoder–decoder structure for both point cloud SupRes and image SemSeg. Unlike prior studies that mainly use convolutional neural networks (CNNs) for feature extraction, our model, named Swin-T-NFC CRFs, consists of a Vision Transformer (ViT)-based encoder and a fully connected conditional random fields (FC-CRFs)-based decoder, connected via a pyramid pooling module and multiple skip connections. Moreover, both encoder and decoder are coupled with a shifted window strategy that allows cross-window connection. As such, patches from different windows of the feature map can participate in self-attention computation, leading to more powerful modeling ability. Experimental results demonstrate that our method can effectively boost the prediction accuracy, reduce the error, and consistently outperform the state-of-the-art methods on simulated/real-world point cloud datasets and the urban drone dataset version 6.},
  archive      = {J_COMCOM},
  author       = {Suhong Wang and Hongqing Wang and Shufeng She and Yanping Zhang and Qingju Qiu and Zhifeng Xiao},
  doi          = {10.1016/j.comcom.2022.10.011},
  journal      = {Computer Communications},
  pages        = {52-60},
  shortjournal = {Comput. Commun.},
  title        = {Swin-T-NFC CRFs: An encoder–decoder neural model for high-precision UAV positioning via point cloud super resolution and image semantic segmentation},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive review on variants of SARS-CoVs-2:
Challenges, solutions and open issues. <em>COMCOM</em>, <em>197</em>,
34–51. (<a href="https://doi.org/10.1016/j.comcom.2022.10.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SARS-CoV-2 is an infected disease caused by one of the variants of Coronavirus which emerged in December 2019. It is declared a pandemic by WHO in March 2020. COVID-19 outbreak has put the world on a halt and is a major threat to the public health system. It has shattered the world with its effects on different areas as the pandemic hit the world in a number of waves with different variants and mutations. Each variant and mutation have different transmission and infection rates in the human population. More than 609 million people have tested positive and more than 6.5 million people have died due to this disease as per 14th September 2022. Despite of numerous efforts, precautions and vaccination the infection has grown rapidly in the world. In this paper, we aim to give a holistic overview of COVID-19 its variants, game theory perspective, effects on the different social and economic areas, diagnostic advancements, treatment methods. A taxonomy is made for the proper insight of the work demonstrated in the paper. Finally, we discuss the open issues associated with COVID-19 in different fields and futuristic research trends in the area. The main aim of the paper is to provide comprehensive literature that covers all the areas and provide an expert understanding of the COVID-19 techniques and potentially be further utilized to combat the outbreak of COVID-19.},
  archive      = {J_COMCOM},
  author       = {Deepanshi and Ishan Budhiraja and Deepak Garg and Neeraj Kumar and Rohit Sharma},
  doi          = {10.1016/j.comcom.2022.10.013},
  journal      = {Computer Communications},
  pages        = {34-51},
  shortjournal = {Comput. Commun.},
  title        = {A comprehensive review on variants of SARS-CoVs-2: Challenges, solutions and open issues},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-efficient resource allocation in NOMA-integrated V2X
networks. <em>COMCOM</em>, <em>197</em>, 23–33. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Vehicle-to-Everything (V2X) communication networks, cellular Device-to-Device (D2D) communication can improve the spectrum efficiency, and non-orthogonal multiple access (NOMA) can further enhance the connection density. However, for the cellular D2D communications, the involved execution condition for NOMA may be affected by the additional interference introduced by cellular links in V2X. In this paper, we study the energy-efficient resource allocation problem in cellular D2D-aided V2X networks with NOMA. To efficiently meet the quality of service (QoS) requirements while taking into account the maximum performance from the user’s perspective, we are committed to maximizing the minimum energy efficiency (EE) of each matching link, by jointly optimizing power allocation and spectrum reusing at Vehicle-to-Infrastructure (V2I) links and cellular D2D-based Vehicle-to-Vehicle (V2V) links. Since this problem is mathematically mixed-integer and non-convex, we first develop a two-layer block coordinate descent (BCD) scheme to solve power allocation subproblem . Specifically, the optimal intra-group power allocation strategy is derived in the inner-layer. Then, Dinkelbach and concave–convex procedure (CCCP) are employed to tighten the lower bound of original problem in the outer-layer. Followed by this, we address the remaining spectrum sharing subproblem based on the principle of minimum EE maximization. Simulation results show that the proposed algorithm can achieve excellent performance and outperform the other benchmark schemes significantly.},
  archive      = {J_COMCOM},
  author       = {Liqing Shan and Songtao Gao and Shuaishuai Chen and Mingkai Xu and Fenghui Zhang and Xuecai Bao and Ming Chen},
  doi          = {10.1016/j.comcom.2022.10.005},
  journal      = {Computer Communications},
  pages        = {23-33},
  shortjournal = {Comput. Commun.},
  title        = {Energy-efficient resource allocation in NOMA-integrated V2X networks},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance evaluation of mobile RPL-based IoT networks
under version number attack. <em>COMCOM</em>, <em>197</em>, 12–22. (<a
href="https://doi.org/10.1016/j.comcom.2022.10.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has a vital role in communication and has many cross-platform applications which generate a massive volume of data. IoT interconnects various devices from small to big without the direct intervention of humans. The resource-constrained environment poses a significant problem in IoT applications, and it is challenging to develop secure applications. The Internet community endeavours to cope with such challenges by developing different internet protocols . IETF ROLL working group standardized a mechanism called IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN) to carry IPv6 packets over IEEE 802.15.4. 6LoWPAN which supports the constrained environment uses the Routing Protocol for Low Power and Lossy Networks (RPL) as a routing protocol. It is essential to secure such applications since the malicious attacker can breach the privacy and security of humans through a small device. Traditional security mechanisms are not prominent in a resource-constrained context. Version attack is one of the most common attacks in RPL based 6LoWPAN. The network becomes unstable due to the version attack, which results in a Denial of Service attack . The integrity of the version number is not provided by RPL specifications, leading to threats for IoT applications. The impact of a version number attack on an RPL-based network is demonstrated in this study. The implications on the constrained network when the nodes are mobile is the main objective of this paper. In many IoT applications nodes move and it is vital to address the impact of mobility in a constrained environment. This paper investigates the network’s performance in terms of packet delivery, delay, and power consumption in RPL based IoT when there is version attack. Version attacks must be prevented as quickly as possible since they have the potential to significantly disrupt mobile networks. The main contribution of this research is a performance metric-based analysis of mobile RPL-based IoT networks under attack.},
  archive      = {J_COMCOM},
  author       = {Girish Sharma and Jyoti Grover and Abhishek Verma},
  doi          = {10.1016/j.comcom.2022.10.014},
  journal      = {Computer Communications},
  pages        = {12-22},
  shortjournal = {Comput. Commun.},
  title        = {Performance evaluation of mobile RPL-based IoT networks under version number attack},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental comparison of migration strategies for
MEC-assisted 5G-V2X applications. <em>COMCOM</em>, <em>197</em>, 1–11.
(<a href="https://doi.org/10.1016/j.comcom.2022.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of 5G technology enables new V2X services requiring reliable and extremely low latency communications . To satisfy these requirements computing elements need to be located at the edge of the network, according to the Multi-access Edge Computing (MEC) paradigm. The user mobility and the MEC approach lead to the need to carefully analysing the procedures for the migration of applications necessary to maintain the service proximity, fundamental to guarantee low latency. The paper provides an experimental comparison of three different migration strategies. The comparison is performed considering three different containerized MEC applications that can be used for developing V2X services. The experimental study is carried out by means of a testbed where the user mobility is emulated by the ETSI MEC Sandbox. The three strategies are compared considering the viability, the observed service downtime, and the amount of state preserved after the migration. The obtained results point out some trade-offs to consider in any migration scenario.},
  archive      = {J_COMCOM},
  author       = {Mohammed A. Hathibelagal and Rosario G. Garroppo and Gianfranco Nencioni},
  doi          = {10.1016/j.comcom.2022.10.009},
  journal      = {Computer Communications},
  pages        = {1-11},
  shortjournal = {Comput. Commun.},
  title        = {Experimental comparison of migration strategies for MEC-assisted 5G-V2X applications},
  volume       = {197},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
