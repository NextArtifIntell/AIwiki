<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CVIU_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cviu---176">CVIU - 176</h2>
<ul>
<li><details>
<summary>
(2023). Fully-attentive iterative networks for region-based
controllable image and video captioning. <em>CVIU</em>, <em>237</em>,
103857. (<a href="https://doi.org/10.1016/j.cviu.2023.103857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllable image captioning has recently gained attention as a way to increase the diversity and the applicability to real-world scenarios of image captioning algorithms. In this task, a captioner is conditioned on an external control signal, which needs to be followed during the generation of the caption. We aim to overcome the limitations of current controllable captioning methods by proposing a fully-attentive and iterative network that can generate grounded and controllable captions from a control signal given as a sequence of visual regions from the image. Our architecture is based on a set of novel attention operators, which take into account the hierarchical nature of the control signal, and is endowed with a decoder which explicitly focuses on each part of the control signal. We demonstrate the effectiveness of the proposed approach by conducting experiments on three datasets, where our model surpasses the performances of previous methods and achieves a new state of the art on both image and video controllable captioning.},
  archive      = {J_CVIU},
  author       = {Marcella Cornia and Lorenzo Baraldi and Ayellet Tal and Rita Cucchiara},
  doi          = {10.1016/j.cviu.2023.103857},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103857},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fully-attentive iterative networks for region-based controllable image and video captioning},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LKDA-GAN: Cross-modality image synthesis via generative
adversarial network aggregating large kernel decomposable attention
bottleneck block. <em>CVIU</em>, <em>237</em>, 103856. (<a
href="https://doi.org/10.1016/j.cviu.2023.103856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mainstream image synthesis methods fail to capture local contextual information long-range dependence, and adaptability, especially under the influence of computation overload and random noise distribution, leading to the loss of contrast and detailed information. To alleviate these issues, we propose a Generative Adversarial Network aggregating large kernel decomposable attention (LKDA-GAN) bottleneck block for cross-modality image synthesis. Initially, a novel LKDA module is proposed by combining a spatial local convolution, a spatial long-range convolution, and a channel convolution, which aims to balance the local contextual information and the long-range dependence, and enhance the channel adaptability by enlarging the receptive field. Subsequently, a bottleneck block is designed and integrated into the LKDA module by feature dimensional transformation to capture rich semantic information and alleviate computational overhead. Ultimately, an auxiliary registration network (ARN) based on the noise transition matrix is put forward to learn the prior knowledge of the noise distribution and produce the unique optimal solution. Furthermore, it is based on Res-UNet to avoid network degradation. By analyzing qualitative assessment and quantitative measurement , extensive experiments demonstrate the proposed approach outperforms the state-of-the-art methods, and ablation studies also show the superiority of LKDA. LKDA-GAN provides a suitable way to synthesize images between different modalities, which is conductive to indicate disease areas and improve diagnosis accuracy.},
  archive      = {J_CVIU},
  author       = {Haiyan Li and Yongqiang Han and Lei Guo and Mingchuan Tan and Liping Zhou},
  doi          = {10.1016/j.cviu.2023.103856},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103856},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LKDA-GAN: Cross-modality image synthesis via generative adversarial network aggregating large kernel decomposable attention bottleneck block},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ARCTIC: A knowledge distillation approach via
attention-based relation matching and activation region constraint for
RGB-to-infrared videos action recognition. <em>CVIU</em>, <em>237</em>,
103853. (<a href="https://doi.org/10.1016/j.cviu.2023.103853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition effect of existing infrared-based action recognition is greatly reduced when clear appearance and texture are required. To address this limitation, the amalgamation of RGB data presents an opportunity to compensate for this deficiency. However, effectively leveraging RGB data to bridge the gap between RGB and infrared modalities poses a significant challenge. In this paper, we propose a knowledge distillation method based on attention relation matching and activation region consistency constraint (ARCTIC) for RGB-to-Infrared action recognition, which guides infrared data for recognition by acquiring the information of RGB data modality. To enhance the precision of knowledge screening across different modalities at various feature levels and ensure accurate information transfer, we constructed attention-based relation matching modules between different feature layers of the teacher and student networks. To minimize the dissimilarity between the two modalities and obtain more advantageous complementary information, we consider the spatial activation region consistency constraint to maintain the consistency between the most salient features of the teacher network and the student network, and employ knowledge distillation loss to favor the selection of accurate predictions while judiciously mitigating the occurrence of erroneous logical outputs. Our experimentation substantiates that the ARCTIC method surpasses the performance of state-of-the-art action recognition techniques across the NTU RGB+D and PKU-MMD datasets.},
  archive      = {J_CVIU},
  author       = {Zhenzhen Quan and Qingshan Chen and Yujun Li and Zhi Liu and Yan Cui},
  doi          = {10.1016/j.cviu.2023.103853},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103853},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {ARCTIC: A knowledge distillation approach via attention-based relation matching and activation region constraint for RGB-to-infrared videos action recognition},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced OCT chorio-retinal segmentation in low-data
settings with semi-supervised GAN augmentation using cross-localisation.
<em>CVIU</em>, <em>237</em>, 103852. (<a
href="https://doi.org/10.1016/j.cviu.2023.103852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep learning methods for optical coherence tomography (OCT) retinal and choroidal layer segmentation is a challenge when data is scarce. In medical image analysis, this is often the case with a lack of data sharing due to confidentiality agreements and data privacy concerns which is further exacerbated in cases of rare pathologies. Even where OCT data is readily available, performing the requisite annotations is time consuming, costly, and error-prone. Data augmentation and semi-supervised learning (SSL) are two techniques employed in deep learning to enhance training in these situations. In this study, we extend our previous work proposing an enhanced StyleGAN2-based data augmentation method for OCT images by employing SSL through a novel cross-localisation technique. The technique increases the diversity of the synthetic data by automatically incorporating styles from unlabelled data with those from labelled data. The method can be used to extend StyleGAN2 as the core idea is simple, yet highly performant. In this work, we optimise the method through a set of ablations and propose the use of a targeted task-specific model selection technique for more optimal generator selection, further boosting performance. The method is applied to OCT retinal and choroidal layer segmentation, demonstrating its effectiveness through substantial patch classification performance improvements as well as significant reductions in choroidal layer segmentation error.},
  archive      = {J_CVIU},
  author       = {Jason Kugelman and David Alonso-Caneiro and Scott A. Read and Stephen J. Vincent and Michael J. Collins},
  doi          = {10.1016/j.cviu.2023.103852},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103852},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhanced OCT chorio-retinal segmentation in low-data settings with semi-supervised GAN augmentation using cross-localisation},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial constraint for efficient semi-supervised video
object segmentation. <em>CVIU</em>, <em>237</em>, 103843. (<a
href="https://doi.org/10.1016/j.cviu.2023.103843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised video object segmentation is the process of tracking and segmenting objects in a video sequence based on annotated masks for one or more frames. Recently, memory-based methods have attracted a significant amount of attention due to their strong performance. Having too much redundant information stored in memory, however, makes such methods inefficient and inaccurate. Moreover, a global matching strategy is usually used for memory reading, so these methods are susceptible to interference from semantically similar objects and are prone to incorrect segmentation. We propose a spatial constraint network to overcome these problems. In particular, we introduce a time-varying sensor and a dynamic feature memory to adaptively store pixel information to facilitate the modeling of the target object, which greatly reduces information redundancy in the memory without missing critical information. Furthermore, we propose an efficient memory reader that is less computationally intensive and has a smaller footprint. More importantly, we introduce a spatial constraint module to learn spatial consistency to obtain more precise segmentation; the target and distractors can be identified by the learned spatial response . The experimental results indicate that our method is competitive with state-of-the-art methods on several benchmark datasets. Our method also achieves an approximately 30 FPS FPS inference speed, which is close to the requirement for real-time systems.},
  archive      = {J_CVIU},
  author       = {Yadang Chen and Chuanjun Ji and Zhi-Xin Yang and Enhua Wu},
  doi          = {10.1016/j.cviu.2023.103843},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103843},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatial constraint for efficient semi-supervised video object segmentation},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empirical study on using adapters for debiased visual
question answering. <em>CVIU</em>, <em>237</em>, 103842. (<a
href="https://doi.org/10.1016/j.cviu.2023.103842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we empirically study debiased Visual Question Answering (VQA) works with Adapters. Most VQA debiasing works sacrifice in-distribution (ID) performance for the sake of out-of-distribution (OOD) performance. Hence, we explore and experiment with the use of adapters to preserve the ID performance by training only a simple adapter network to debias and recreate performance. We conduct an extensive empirical study on recent well-established VQA debiasing works and show that the entirety of the debiasing information from the proposed debiasing methods can be captured and modelled using a single fully connected layer while preserving original network performance by skipping the adapters. Through our exploration, we find that different placements of adapters are required for different debiasing techniques and show the different possibilities of using adapters for debiasing through our experiments. We believe our findings in this work open up more questions to be asked and explored for the VQA community.},
  archive      = {J_CVIU},
  author       = {Jae Won Cho and Dawit Mureja Argaw and Youngtaek Oh and Dong-Jin Kim and In So Kweon},
  doi          = {10.1016/j.cviu.2023.103842},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103842},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Empirical study on using adapters for debiased visual question answering},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth super-resolution from explicit and implicit
high-frequency features. <em>CVIU</em>, <em>237</em>, 103841. (<a
href="https://doi.org/10.1016/j.cviu.2023.103841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided depth super-resolution aims at using a low-resolution depth map and an associated high-resolution RGB image to recover a high-resolution depth map. However, restoring precise and sharp edges near depth discontinuities and fine structures is still challenging for state-of-the-art methods. To alleviate this issue, we propose a novel multi-stage depth super-resolution network, which progressively reconstructs HR depth maps from explicit and implicit high-frequency information. We introduce an efficient transformer to obtain explicit high-frequency information. The shape bias and global context of the transformer allow our model to focus on high-frequency details between objects, i.e., depth discontinuities, rather than texture within objects. Furthermore, we project the input color images into the frequency domain for additional implicit high-frequency cues extraction. Finally, to incorporate the structural details, we develop a fusion strategy that combines depth features and high-frequency information in the multi-stage-scale framework. Exhaustive experiments on the main benchmarks show that our approach establishes a new state-of-the-art. Code will be publicly available at https://github.com/wudiqx106/DSR-EI .},
  archive      = {J_CVIU},
  author       = {Xin Qiao and Chenyang Ge and Youmin Zhang and Yanhui Zhou and Fabio Tosi and Matteo Poggi and Stefano Mattoccia},
  doi          = {10.1016/j.cviu.2023.103841},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103841},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Depth super-resolution from explicit and implicit high-frequency features},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An image denoising method based on the nonlinear schrödinger
equation and spectral subband decomposition. <em>CVIU</em>,
<em>237</em>, 103840. (<a
href="https://doi.org/10.1016/j.cviu.2023.103840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most denoising methods inevitably smooth the image while denoising, which makes it difficult to effectively maintain the structural information and detail information of the original noise-free image. Stochastic resonance can directly convert the noise energy into signal energy, and denoising by stochastic resonance retains the signal energy. Therefore, this paper presents an image denoising method based on the nonlinear Schrödinger equation. First, by analyzing the dynamic stochastic resonance effect in a nonlinear optical system, the nonlinear Schrödinger equation is derived to describe the signal propagation in the system, and the equation is extended to construct a two-dimensional image denoising model. Then, we propose an image denoising algorithm based on image spectral subband decomposition. The algorithm decomposes the image spectrum into different frequency domain subbands and uses the Schrödinger denoising model to denoise the high-frequency subbands containing the rich structure and detailed information of the image. A series of comparative experiments show that the proposed method can significantly improve the image signal-to-noise ratio while effectively restoring image details and has better performance than other typical methods in the case of high-intensity noise.},
  archive      = {J_CVIU},
  author       = {Fangxun Bao and Yifan Lei and Yiqiao Jia and Hongwei Du and Chengyong Gao and Yunfeng Zhang},
  doi          = {10.1016/j.cviu.2023.103840},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103840},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An image denoising method based on the nonlinear schrödinger equation and spectral subband decomposition},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IIANet: Information interactivity attention network with
adversarial learning for infrared small object detection. <em>CVIU</em>,
<em>237</em>, 103839. (<a
href="https://doi.org/10.1016/j.cviu.2023.103839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared small object detection plays an important role in maritime surveillance , early warning systems , and precise positioning. However, existing deep learning methods lack attention to local information and overlook the interactivity between local and global information. To solve this problem and explore a more effective feature extraction network for infrared small object detection, we propose an information interactivity attention network (IIANet), which decouples the optimization of miss detection and false alarm by adversarial learning. Specifically, the information interactivity attention is achieved by the combination of the global spatial attention module and pixel-wise local attention module where they can complement each other to generate finer attention. Correspondingly, to make full use of the extracted interactive features, a more sophisticated information aggregation discriminator is designed by using the intensive skip-addition module which further realizes information communication among different semantic information. Finally, we apply the pixel equalization strategy to further promote the learning ability of the data-oriented model by enhancing the visual contrast between the objects and the background. Extensive experiments and visualization results demonstrate that our approach can achieve state-of-the-art performance with a higher detection rate and lower false alarm rate . Besides, ablation experiments verify the effectiveness of each module.},
  archive      = {J_CVIU},
  author       = {Jin Jiang and Xiaoyuan Yang and Yixiao Li},
  doi          = {10.1016/j.cviu.2023.103839},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103839},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {IIANet: Information interactivity attention network with adversarial learning for infrared small object detection},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constituent attention for vision transformers.
<em>CVIU</em>, <em>237</em>, 103838. (<a
href="https://doi.org/10.1016/j.cviu.2023.103838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-head self-attention (MSA) endows vision Transformers (ViTs) with the ability of modeling long-range interactions between tokens. However, recent works have uncovered that current arbitrary attention makes tokens distracted by extraneous dependencies, complicating the understanding of ViTs and hindering the overall optimization of ViTs. In this paper, we propose Constituent Attention (CA), a straightforward yet effective constraint on MSA to resolve aforementioned issues in different variants of ViTs with negligible overhead. Specifically, CA is designed to encourage the mutual attention between spatially connectable tokens via gathering such token pairs to the same group (termed constituent ). Furthermore, with the layers going deeper in ViTs, small adjacent constituents are gradually merged into larger ones, exploring hierarchical structures in the visual objectives. Extensive experiments on four popular benchmarks demonstrate that CA assists the optimization of ViTs and contributes to significant performance gains especially at data-scarce scenarios. Moreover, the layer-wise reasoning mechanism aids ViTs in more consistent and progressive attention across different layers, rendering the inner workings easier to interpret. Our code is publicly available at https://github.com/zju-vipa/ConstituentAttention .},
  archive      = {J_CVIU},
  author       = {Haoling Li and Mengqi Xue and Jie Song and Haofei Zhang and Wenqi Huang and Lingyu Liang and Mingli Song},
  doi          = {10.1016/j.cviu.2023.103838},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103838},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Constituent attention for vision transformers},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coping with change: Learning invariant and minimum
sufficient representations for fine-grained visual categorization.
<em>CVIU</em>, <em>237</em>, 103837. (<a
href="https://doi.org/10.1016/j.cviu.2023.103837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization (FGVC) is a challenging task due to similar visual appearances between various species. Previous studies always implicitly assume that the training and test data have the same underlying distributions, and that features extracted by modern backbone architectures remain discriminative and generalize well to unseen test data. However, we empirically justify that these conditions are not always true on benchmark datasets. To this end, we combine the merits of invariant risk minimization (IRM) and information bottleneck (IB) principle to learn invariant and minimum sufficient (IMS) representations for FGVC, such that the overall model can always discover the most succinct and consistent fine-grained features. We apply the matrix-based Rényi’s α α -order entropy to simplify and stabilize the training of IB; we also design a “soft” environment partition scheme to make IRM applicable to FGVC task. To the best of our knowledge, we are the first to address the problem of FGVC from a generalization perspective and develop a new information-theoretic solution accordingly. Extensive experiments demonstrate the consistent performance gain offered by our IMS. Code is available at: https://github.com/SYe-hub/IMS .},
  archive      = {J_CVIU},
  author       = {Shuo Ye and Shujian Yu and Wenjin Hou and Yu Wang and Xinge You},
  doi          = {10.1016/j.cviu.2023.103837},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103837},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Coping with change: Learning invariant and minimum sufficient representations for fine-grained visual categorization},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 4DHumanOutfit: A multi-subject 4D dataset of human motion
sequences in varying outfits exhibiting large displacements.
<em>CVIU</em>, <em>237</em>, 103836. (<a
href="https://doi.org/10.1016/j.cviu.2023.103836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new dataset of densely sampled spatio-temporal 4D human motion data of different actors, outfits and motions. The dataset contains different actors wearing different outfits while performing different motions in each outfit. It samples the space of 4D human motion along 3 axes with identity, outfit and motion, therefore providing a cube of data where each identity is represented by a planar slice of the cube with all outfits and for all motions. The dataset has numerous potential applications for the processing and creation of digital humans including augmented reality , avatar creation and virtual try on. 4DHumanOutfit is released for research purposes at https://kinovis.inria.fr/4dhumanoutfit/ . In addition to image data and 4D reconstructions, the dataset includes reference solutions for each axis. We present independent baselines along each axis that demonstrate the value of these reference solutions for evaluation tasks.},
  archive      = {J_CVIU},
  author       = {Matthieu Armando and Laurence Boissieux and Edmond Boyer and Jean-Sébastien Franco and Martin Humenberger and Christophe Legras and Vincent Leroy and Mathieu Marsot and Julien Pansiot and Sergi Pujades and Rim Rekik and Grégory Rogez and Anilkumar Swamy and Stefanie Wuhrer},
  doi          = {10.1016/j.cviu.2023.103836},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103836},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {4DHumanOutfit: A multi-subject 4D dataset of human motion sequences in varying outfits exhibiting large displacements},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimum error adaptive RGB calibration in a context of
colorimetric uncertainty for cultural heritage preservation.
<em>CVIU</em>, <em>237</em>, 103835. (<a
href="https://doi.org/10.1016/j.cviu.2023.103835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, two aims are sought. First, the path to a minimum-error colour calibration and its feasibility for non-invasive cultural heritage conservation is explored. Using RGB information as temporal cues of the conservation state of the pieces under monitoring, an adaptive calibration function , working in syntony with a colour calibration chart destined to general usage, has been proposed. Since the context involves citizen participation, the pictures to calibrate are crowdsourced and the cameras of origin unknown; therefore, the requirements of the work context assume images that imply a heavy colorimetric uncertainty whilst maintaining their structural content. Therefore,a differentiable, multidimensional, adaptive transfer curve that offers the best trade-off between calibration error (staying in minimum perceivable colour differences for tangible materials) and calculation requirements is refined. The conception of this system offers flexible possibilities of an effective cultural heritage conservation without heavy costs on money and human effort or relying on external solutions, while standing on par in quality with state-of-art technologies, achieving therefore a balance between generalist usage and specific solutions. Secondly, a neural network-based final process performs the final tuning of the calibration functions, and its conclusions shed light on how a calibration process works together with the colour chart on an algebraic level. The verification of this phenomenon opens possibilities of different paths on how to proceed with colour calibrations depending on the conditions of acquisition and the content of interest, so the minimum error can be always achieved regardless of the case. The ideas presented here are an expansion and conclusion of the developments and results previously published by the same authors.},
  archive      = {J_CVIU},
  author       = {Miguel Antonio Barbero-Álvarez and Juan Antonio Rodrigo and José Manuel Menéndez},
  doi          = {10.1016/j.cviu.2023.103835},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103835},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Minimum error adaptive RGB calibration in a context of colorimetric uncertainty for cultural heritage preservation},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extending function mixture network for improved spectral
super-resolution. <em>CVIU</em>, <em>237</em>, 103834. (<a
href="https://doi.org/10.1016/j.cviu.2023.103834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral Image reconstruction from RGB images is a low-cost and convenient alternative to acquiring hyperspectral images directly. The challenge in estimating the spectral response function and using it for generating the hyperspectral image data is addressed effectively by the use of convolutional neural networks for the task. The accuracy of the reconstruction techniques is improving and effective architectures are being adapted for the purpose. Specifically, in a recent work the authors propose the use of Function Mixture Network, to model the mapping from RGB to hyperspectral. This technique benefits from the use of an adaptive spatial receptive field, however, given that the goal is spectral superresolution additional use of adaptive spectral field is prudent. Accordingly, in this paper we propose an improved Function Mixture Network based reconstruction technique by additionally incorporating a variable spectral receptive field. The proposed updates have resulted in significant gains in performance as is demonstrated by the experiments conducted over multiple datasets. An additional update to the architecture is the use of non-convolutional branches in the function mixture network. The experiments of three standard datasets, clearly demonstrate the superiority of the proposed technique for hyperspectral image reconstruction based on RGB data.},
  archive      = {J_CVIU},
  author       = {Sadia Hussain and Brejesh Lall},
  doi          = {10.1016/j.cviu.2023.103834},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103834},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Extending function mixture network for improved spectral super-resolution},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On learning distribution alignment for video-based
visible-infrared person re-identification. <em>CVIU</em>, <em>237</em>,
103833. (<a href="https://doi.org/10.1016/j.cviu.2023.103833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the matching problem of cross-modality video data from a discrete distribution alignment view. Central to this discussion is the visible-infrared person re-identification (VI-reID), a crucial feature that bolsters surveillance systems’ efficacy in monitoring individuals across diverse lighting conditions. Going beyond traditional image-to-image matching paradigms, a recent study shows that temporal information can bring richer cues to encode the pedestrian representation, improving the representation power of deep neural networks . However, this integration further complicates cross-modality data matching due to the joint processing of spatial and temporal information. This paper formulates the video data as a discrete distribution and aligns the cross-modality video representation by reducing the matching cost between the two distributions. To this end, a natural idea for aligning the videos is to reduce the divergence of distributions. Moreover, the powerful optimal transport (OT) scheme, which generates the optimal matching flows and establishes the relevance of two distributions, is also employed as a way to measure the distance of distributions. Nevertheless, we observe that endowing the OT in the advanced VI-reID feature extractor leads to a non-symmetric measurement. To mitigate this, the paper introduces a new metric, namely symmetric optimal transport (SOT), reformulating OT into a symmetric form . Thorough analyses and empirical studies affirm the superiority of the proposed SOT, which significantly outperforms the current state-of-the-art methods according to standard benchmarking evaluations.},
  archive      = {J_CVIU},
  author       = {Pengfei Fang and Yaojun Hu and Shipeng Zhu and Hui Xue},
  doi          = {10.1016/j.cviu.2023.103833},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103833},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {On learning distribution alignment for video-based visible-infrared person re-identification},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-world efficient fall detection: Balancing performance
and complexity with FDGA workflow. <em>CVIU</em>, <em>237</em>, 103832.
(<a href="https://doi.org/10.1016/j.cviu.2023.103832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the decrease in mobility, falls have become more prevalent and painful for both men and women. Most current approaches for fall detection in daily living employ either simple thresholds or complex networks. However, these methods suffer from an inability to consider the association among data or the disadvantage of excessive complexity in practical use. In this paper, we propose a compromise workflow called FDGA for fall detection that addresses these issues. Our approach first roughly determines the bounding box of the fallen target using the lightweight Fall-ROI extraction, and then further determines the precise skeletal position using a fine top-down skeletal estimator. Additionally, the genetic-algorithm-based classifier minimizes the effect of redundant features. Since no heavy network is required, the proposed workflow maintains an excellent performance-speed trade-off. We validate our approach using two fall detection datasets and a new large-scale fall detection dataset we created. The experimental results demonstrate the efficiency and effectiveness of our approach.},
  archive      = {J_CVIU},
  author       = {Guotian Zeng and Bi Zeng and Huiting Hu},
  doi          = {10.1016/j.cviu.2023.103832},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103832},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Real-world efficient fall detection: Balancing performance and complexity with FDGA workflow},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated unsupervised cluster-contrastive learning for
person re-identification: A coarse-to-fine approach. <em>CVIU</em>,
<em>237</em>, 103831. (<a
href="https://doi.org/10.1016/j.cviu.2023.103831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-identification (ReID) has attracted considerable interests in recent years, largely driven by the escalating demand for public safety measures. However, the acquisition and handling of sensitive personal data can trigger significant privacy concerns. Federated learning has been introduced as a potential solution to this problem, with the goal of limiting the exposure of sensitive data across different participating entities (clients). Existing methods often depend on labour-intensive data annotations and face difficulties in maintaining cross-domain uniformity. To tackle these challenges, we propose a Federated Unsupervised Cluster-Contrastive (FedUCC) method based on deep learning for Person ReID that follows a generic-to-specific learning strategy. First, FedUCC procures generic knowledge from a conventional federated learning scheme to aggregate and distribute parameters across local clients. Second, specialized knowledge is explored to facilitate client personalization by disentangling client-specific knowledge from generic knowledge through parameter localization . Third, to further enhance effective fine-grained patterns instead of overfitting on specialized client knowledge, we investigate two key aspects: patch-level feature alignment and camera-invariant learning . Comprehensive experiments on eight public benchmark datasets demonstrate the state-of-the-art performance of our proposed method.},
  archive      = {J_CVIU},
  author       = {Jianfeng Weng and Kun Hu and Tingting Yao and Jingya Wang and Zhiyong Wang},
  doi          = {10.1016/j.cviu.2023.103831},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103831},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Federated unsupervised cluster-contrastive learning for person re-identification: A coarse-to-fine approach},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatio-temporal two-stage fusion for video question
answering. <em>CVIU</em>, <em>237</em>, 103821. (<a
href="https://doi.org/10.1016/j.cviu.2023.103821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video question answering (VideoQA) has attracted much interest from of scholars as one of the most representative multimodal tasks in recent years. The task requires the model to interact and reason between the video and the question. Most known approaches use pre-trained networks to extract complex embeddings of videos and questions independently before performing multimodal fusion. However, they overlook two factors: (1) These feature extractors are pre-trained for the image or video classification task without taking the question into consideration, therefore may not be suitable for VideoQA task. (2) Using multiple feature extractors to extract features at different levels introduce more irrelevant information to some extent, thus making the task more difficult. For the above reasons, we propose a new model named Spatio-Temporal Two-Stage Fusion, which ties together multiple levels of feature extraction processes and divides them into two distinct stages: spatial fusion and temporal fusion. Specifically, in the spatial fusion stage, we use Vision Transformer to integrate the intra-frame information to generate frame-level features. At the same time, we design a multimodal temporal fusion module that enables the video to fuse textual information and assign different levels of attention to each frame. Then the obtained frame-level features are used to generate global video features by another Vision Transformer. In order to efficiently generate modal interaction information, we design a video–text symmetric fusion module to retain the most relevant information by mutual guidance between the two modalities. Our method is evaluated on three benchmark datasets: MSVD-QA, MSRVTT-QA and TGIF-QA, and achieves competitive results.},
  archive      = {J_CVIU},
  author       = {Feifei Xu and Yitao Zhu and Chun Wang and Yangze Cao and Zheng Zhong and Xiongmin Li},
  doi          = {10.1016/j.cviu.2023.103821},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103821},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatio-temporal two-stage fusion for video question answering},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LOFReg: An outlier-based regulariser for deep metric
learning. <em>CVIU</em>, <em>237</em>, 103820. (<a
href="https://doi.org/10.1016/j.cviu.2023.103820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep metric learning aims to create a feature space where the projected samples have maximised inter-class and minimised intra-class distance. Most approaches employ only distance-based metrics to achieve this objective, but neglect other properties of the projections in the embedding space, such as their density, sparsity and presence of outliers. In this paper, we propose a novel density-based regulariser, LOFReg, designed to be used as a complement to previously proposed distance-based metric learning loss functions for the re-identification (ReID) and few-shot classification (FSC) tasks. Our method is based on the well-known, in anomaly detection literature, local outlier factor (LOF) algorithm, which estimates the local density deviation of a data point with respect to its neighbours. These measurements are used in our regularisation methodology to achieve an embedding space of evenly distributed samples and to increase the generalisation ability of the model compared to solely distance-based learning. Comprehensive experiments on four publicly available datasets for ReID and FSC, demonstrate consistent improvement against previously proposed metric learning loss functions. Particularly, our experiments show up to 5\% improvement in ReID settings, up to 13\% in FSC settings and up to 6\% improvement against other state-of-art regularisers.},
  archive      = {J_CVIU},
  author       = {Eleni Kamenou and Jesus Martinez del Rincon and Paul Miller and Patricia Devlin-Hill and Samuel Budgett and Federico Angelini and Charlotte Grinyer},
  doi          = {10.1016/j.cviu.2023.103820},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103820},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LOFReg: An outlier-based regulariser for deep metric learning},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CPNet: Continuity preservation network for infrared video
colorization. <em>CVIU</em>, <em>237</em>, 103816. (<a
href="https://doi.org/10.1016/j.cviu.2023.103816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared video colorization can significantly improve perceptual quality by predicting reasonable colors and restoring vivid details, especially in harsh environments. However, as an inconspicuous computer vision task, there is no specialized method. Besides, directly applying current grayscale colorization methods may generate structurally obscure and temporally inconsistent frames. In this paper, we design an infrared video colorization network CPNet aims to generate visually plausible and spatial–temporal consistent colorized videos. To achieve this, a feature fusion module and a hierarchical colorizer are designed to learn the importance of each consecutive frame and the local and global correlation of the integrated features, respectively. In addition, to consolidate the temporal consistency at a fine-grained level, we further introduce a composite loss function to narrow the distance between high-level feature representations while retaining pixel-wise correspondence. Moreover, a new metric named Mean Temporal Variation Similarity (MTVS) is proposed for effectively evaluating the degree of video continuity. Comprehensive experiments conducted on the KAIST dataset demonstrate the superiority of CPNet to produce more authentic colorized videos than state-of-the-art colorization methods. In terms of quantitative comparison , CPNet achieves improvements of at least 0.89 dB on PSNR , 0.016 on SSIM, and a significant promotion on MTVS. In addition, experiments conducted on the DAVIS dataset also prove the applicability of CPNet for grayscale video colorization task.},
  archive      = {J_CVIU},
  author       = {Cheng Cheng and Hang Wang and Xiang Liao and Gang Cheng and Hongbin Sun},
  doi          = {10.1016/j.cviu.2023.103816},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103816},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CPNet: Continuity preservation network for infrared video colorization},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Swin-ResUNet+: An edge enhancement module for road
extraction from remote sensing images. <em>CVIU</em>, <em>237</em>,
103807. (<a href="https://doi.org/10.1016/j.cviu.2023.103807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road extraction from remote sensing images is very important in navigation, urban planning, traffic management and other fields. Deep learning methods have achieved great success in computer vision tasks. Therefore, road extraction from remote sensing images using deep learning methods can significantly improve the road extraction accuracy. However, these methods generally have problems such as low road extraction accuracy, slow training speed, high computational complexity , and poor road topology connectivity. In order to solve the above issues, we propose a Swin-ResUNet+ structure and use the new paradigm Swin-Transformer to extract roads in remote sensing images. Specifically, we construct an Edge Enhancement module based on residual connection and add this module to each stage of the encoder, which can obtain the edge information in remote sensing images. Based on the Edge Enhancement module, we propose a Swin-ResUNet+ structure in order to better capture the topology of roads. On the Massachusetts road dataset, our model has the least computational cost with only less than one percent accuracy decrease. On the DeepGlobe2018 road dataset, our model not only has the least computational complexity but also achieves the highest values of mIOU, mDC, mPA and F1-score. In a word, Swin-ResUNet+ obtains a much better trade-off between accuracy and efficiency than previous CNN-based and Transformer-based methods.},
  archive      = {J_CVIU},
  author       = {Yingshan Jing and Ting Zhang and Zhaoying Liu and Yuewu Hou and Changming Sun},
  doi          = {10.1016/j.cviu.2023.103807},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103807},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Swin-ResUNet+: An edge enhancement module for road extraction from remote sensing images},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “improved domain adaptive object detector via
adversarial feature learning” [comput. Vis. Image underst. 230 (2023)
103660]. <em>CVIU</em>, <em>237</em>, 103685. (<a
href="https://doi.org/10.1016/j.cviu.2023.103685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CVIU},
  author       = {MohamedAmine Marnissi and Hajer Fradi and Anis Sahbani and Najoua Essoukri Ben Amara},
  doi          = {10.1016/j.cviu.2023.103685},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103685},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Corrigendum to “Improved domain adaptive object detector via adversarial feature learning” [Comput. vis. image underst. 230 (2023) 103660]},
  volume       = {237},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). View consistency aware holistic triangulation for 3D human
pose estimation. <em>CVIU</em>, <em>236</em>, 103830. (<a
href="https://doi.org/10.1016/j.cviu.2023.103830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of multi-view 3D human pose estimation (HPE) is attributed to the maturation of monocular 2D HPE and the geometry of 3D reconstruction . However, 2D detection outliers in occluded views due to neglect of view consistency, and 3D implausible poses due to lack of pose coherence, remain challenges. To solve this, we introduce a Multi-View Fusion module to refine 2D results by establishing view correlations. Then, Holistic Triangulation is proposed to infer the whole pose as an entirety, and anatomy prior is injected to maintain the pose coherence and improve the plausibility. Anatomy prior is extracted by PCA whose input is skeletal structure features, which can factor out global context and joint-by-joint relationship from abstract to concrete. Benefiting from the closed-form solution, the whole framework is trained end-to-end. Our method outperforms the state of the art in both precision and plausibility which is assessed by a new metric.},
  archive      = {J_CVIU},
  author       = {Xiaoyue Wan and Zhuo Chen and Xu Zhao},
  doi          = {10.1016/j.cviu.2023.103830},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103830},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {View consistency aware holistic triangulation for 3D human pose estimation},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view clustering with laplacian rank constraint based
on symmetric and nonnegative low-rank representation. <em>CVIU</em>,
<em>236</em>, 103829. (<a
href="https://doi.org/10.1016/j.cviu.2023.103829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view spectral clustering has attracted more and more attention due to its promising advantages in data clustering , and many related methods have been proposed, among which the method based on low-rank representation have become increasingly popular. However, the similarity matrices learned by the existing methods inevitably contain some negative entries and are generally nonsymmetric. In order to guarantee weight consistency for each pair of data points and highly correlated data points to be together, we propose a new multi-view spectral clustering model based on the symmetric and nonnegative low-rank representation. This model can learn the view-specific consistent similarity matrix and also capture the local structure and discriminative information among data by imposing symmetric and nonnegative constraints on the representation matrix . By employing the compatible and complementary information from multi-view data points, it can also learn the consensus similarity matrix for all views and perform the spectral clustering simultaneously. By the augmented Lagrangian multiplier method, an alternating iterative optimization algorithm is designed for solving this model and its computational complexity and convergence is analyzed. Comprehensive experiments on multiple multi-view data sets also verify the effectiveness and superiority of the proposed approach.},
  archive      = {J_CVIU},
  author       = {Chiwei Gao and Ziwei Xu and Xiuhong Chen},
  doi          = {10.1016/j.cviu.2023.103829},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103829},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-view clustering with laplacian rank constraint based on symmetric and nonnegative low-rank representation},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature independent filter pruning by successive layers
analysis. <em>CVIU</em>, <em>236</em>, 103828. (<a
href="https://doi.org/10.1016/j.cviu.2023.103828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have become deeper and wider over time. However due to low computational power, mobile devices or embedded systems cannot use very deep models. Filter pruning solves this by eliminating redundant filters. Pruning can be performed in a feature dependent or independent manner. Feature dependent methods require extensive time to determine the filter importance as these methods require generation and processing of feature maps for each example. Additionally, in iterative pruning, filter importance is computed several times based on the current state. This increases algorithm execution time further. However, existing feature independent methods are fast, but they perform poor as they compute importance using only current layer filter weights. However, our analysis suggests that both the current and succeeding layer filters are crucial to determine filter importance. We propose ‘Filter Pruning by Successive Layers analysis’ (FPSL), a novel feature independent algorithm, that considers the effect of pruning a filter on the generation of feature maps for the first time. Moreover, FPSL does not require layer-wise retraining, rigorous hyperparameter search for fine-tuning, or human intervention to set the pruning percentage per layer. These make FPSL extremely fast, efficient, and adaptive. Thus it follows iterative pruning and retraining. FPSL outperforms the state-of-the-art (SOTA) methods on extensive experiments with different datasets (CIFAR, ImageNet) and architectures (VGG, ResNet , MobileNet). It decreases the computational burden of VGG16 by half but improves CIFAR10 and CIFAR100 accuracy. Even for ImageNet, FPSL reduces 42.7\% floating point operations (FLOPs) while maintaining top-1 accuracy for ResNet50 .},
  archive      = {J_CVIU},
  author       = {Milton Mondal and Bishshoy Das and Brejesh Lall and Pushpendra Singh and Sumantra Dutta Roy and Shiv Dutt Joshi},
  doi          = {10.1016/j.cviu.2023.103828},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103828},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Feature independent filter pruning by successive layers analysis},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Snow mask guided adaptive residual network for image snow
removal. <em>CVIU</em>, <em>236</em>, 103819. (<a
href="https://doi.org/10.1016/j.cviu.2023.103819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration under severe weather is a challenging task. Most of the past works focused on removing rain and haze phenomena in images. However, snow is also an extremely common atmospheric phenomenon that will seriously affect the performance of high-level computer vision tasks, such as object detection and semantic segmentation . Recently, some methods have been proposed for snow removing, and most methods deal with snow images directly as the optimization object. However, the distribution of snow location and shape is complex. Therefore, failure to detect snowflakes/snow streak effectively will affect snow removing and limit the model performance. To solve these issues, we propose a Snow Mask Guided Adaptive Residual Network (SMGARN). Specifically, SMGARN consists of three parts, Mask-Net, Guidance-Fusion Network (GF-Net), and Reconstruct-Net. Firstly, we build a Mask-Net with Self-pixel Attention (SA) and Cross-pixel Attention (CA) to capture the features of snowflakes and accurately localized the location of the snow, thus predicting an accurate snow mask. Secondly, the predicted snow mask is sent into the specially designed GF-Net to adaptively guide the model to remove snow. Finally, an efficient Reconstruct-Net is used to remove the veiling effect and correct the image to reconstruct the final snow-free image. Furthermore, we propose a more refined dataset of real snow images, SnowWorld24, to provide faster evaluation of snow-free images. Extensive experiments show that our SMGARN numerically outperforms all existing snow removal methods, and the reconstructed images are clearer in visual contrast. All codes are available at https://github.com/MIVRC/SMGARN .},
  archive      = {J_CVIU},
  author       = {Bodong Cheng and Juncheng Li and Ying Chen and Tieyong Zeng},
  doi          = {10.1016/j.cviu.2023.103819},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103819},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Snow mask guided adaptive residual network for image snow removal},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Glitch in the matrix: A large scale benchmark for content
driven audio–visual forgery detection and localization. <em>CVIU</em>,
<em>236</em>, 103818. (<a
href="https://doi.org/10.1016/j.cviu.2023.103818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deepfake detection methods focus on detecting spatial and/or spatio-temporal changes in facial attributes and are centered around the binary classification task of detecting whether a video is real or fake. This is because available benchmark datasets contain mostly visual-only modifications present in the entirety of the video. However, a sophisticated deepfake may include small segments of audio or audio–visual manipulations that can completely change the meaning of the video content. To addresses this gap, we propose and benchmark a new dataset, Localized Audio Visual DeepFake (LAV-DF), consisting of strategic content-driven audio, visual and audio–visual manipulations. The proposed baseline method, Boundary Aware Temporal Forgery Detection (BA-TFD), is a 3D Convolutional Neural Network-based architecture which effectively captures multimodal manipulations. We further improve (i.e. BA-TFD + ) the baseline method by replacing the backbone with a Multiscale Vision Transformer and guide the training process with contrastive, frame classification, boundary matching and multimodal boundary matching loss functions. The quantitative analysis demonstrates the superiority of BA-TFD + on temporal forgery localization and deepfake detection tasks using several benchmark datasets including our newly proposed dataset. The dataset, models and code are available at https://github.com/ControlNet/LAV-DF .},
  archive      = {J_CVIU},
  author       = {Zhixi Cai and Shreya Ghosh and Abhinav Dhall and Tom Gedeon and Kalin Stefanov and Munawar Hayat},
  doi          = {10.1016/j.cviu.2023.103818},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103818},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Glitch in the matrix: A large scale benchmark for content driven audio–visual forgery detection and localization},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-timescale boosting for efficient and improved event
camera face pose alignment. <em>CVIU</em>, <em>236</em>, 103817. (<a
href="https://doi.org/10.1016/j.cviu.2023.103817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of event camera (EC) vision in certain types of applications has been steadily shown thanks to energy-efficient sparse sensing, high dynamic range, and extremely high temporal resolution. However, the utilization of ECs for facial processing tasks has remained rather limited. To enable high energy efficiency for large face pose alignment, which is a crucial facial pre-processing stage, we aim at leveraging EC by effective adaptation of the processing rate proportional to facial movement intensity. For this purpose, we propose a novel alternative to the commonly employed constant time frame and event count frame strategies which combines their advantages and provides the benefits of supervised learning. This is realized by a multi-timescale boosting framework that can generate highly sparse pose-events at a variable rate via detection-based online timescale selection. Although detectors of multiple scales with boosted sensitivities operate as a cascade, our method provides minimal delay essential for real-time applications. Comprehensive evaluations show that the proposed multi-timescale processing substantially improves the performance–efficiency trade-off over single-timescale frames and markedly over event count frames. Mega-floating-point-operations-per-second ranges from 2.5 at the moderate motion clips to 6.5 at the intense motion clips, with negligible computation in the absence of activity. Also, alignment errors are considerably reduced by online selection of small timescales at fast head motion and of bigger timescales at slower motion or local activity of lips and eyes. Being orthogonal and complementary to spatial domain techniques, the proposed approach can also be conveniently integrated with future advances for further performance/efficiency improvements or for alignment extensions.},
  archive      = {J_CVIU},
  author       = {Arman Savran},
  doi          = {10.1016/j.cviu.2023.103817},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103817},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-timescale boosting for efficient and improved event camera face pose alignment},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised fine-grained semantic segmentation via
spatial correlation-guided learning. <em>CVIU</em>, <em>236</em>,
103815. (<a href="https://doi.org/10.1016/j.cviu.2023.103815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) mainly adopts class activation map (CAM) to recognize different categories with generated pseudo masks of only image-level labels. Recently, many advanced works focus on learning the semantic correlation to refine the conventional CAM, which only identify sparse and discriminative semantic regions, severely weakening the further learning ability of spatial features . To copy with the above problem, a spatial correlation-guided learning framework is proposed to exploit the spatial and semantic correlation between adjacent pixels for weakly supervised fine-grained semantic segmentation (WSFGSS). From the spatial perspective, self-supervised multi-view clustering (SMC) is designed to fully mine spatial correlation by clustering of multiple views including scale, angle and position. Moreover, a hybrid self-supervised (HS) loss function is used to further promote the optimized speed and accuracy of three spatial representations . From the semantic perspective, the affinity matrix is applied to describe the semantic similarity between different pixels by building a weighted graph , and combine above robust pseudo label into a probability transition matrix . Therefore, the initial CAM is gradually corrected during the iterative optimization by the random walk algorithm. Finally, the refined CAM is utilized as supervision information for training the standard segmentation network effectively. Sufficient experimental results on BSDS500, PASCAL VOC 2012 and MS COCO datasets show that the proposed SMC method obtains more accurate pseudo labels than the recent unsupervised segmentation models . Meantime, with these pseudo labels, the proposed fine-grained framework achieves the state-of-the-art performance for WSFGSS.},
  archive      = {J_CVIU},
  author       = {Zihao Dong and Tiyu Fang and Jinping Li and Xiuli Shao},
  doi          = {10.1016/j.cviu.2023.103815},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103815},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Weakly supervised fine-grained semantic segmentation via spatial correlation-guided learning},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the vertical direction in a photogrammetric 3D
model, with application to visualization. <em>CVIU</em>, <em>236</em>,
103814. (<a href="https://doi.org/10.1016/j.cviu.2023.103814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of building a Virtual Reality (VR) environment from images involves several steps: choose experimental conditions (scene, camera, trajectory, weather), take the images, reconstruct a textured 3D model thanks to a photogrammetry software, and import the 3D model into a game engine. This paper focuses on a postprocessing technique for the photogrammetry step, mostly for outdoor environments that cannot be reconstructed using an unmanned aerial vehicle . As visualization applications (including VR) need a 3D model with a known vertical direction, a method is introduced to compute it. The method is based on 3D principal component analysis and a 2D Hough transform . In the experiments, we first reconstruct both man-made and natural immersive environments using a helmet-held 360 camera, then we import the 3D models in good coordinate systems (i.e. with a vertical axis and a plausible scale) into Unity, and finally we use VR headsets to explore the scenes like a pedestrian. We also experiment on scanner data and show that our method is competitive with previous work.},
  archive      = {J_CVIU},
  author       = {Maxime Lhuillier},
  doi          = {10.1016/j.cviu.2023.103814},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103814},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Estimating the vertical direction in a photogrammetric 3D model, with application to visualization},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale semantic and detail extraction network for
lightweight person re-identification. <em>CVIU</em>, <em>236</em>,
103813. (<a href="https://doi.org/10.1016/j.cviu.2023.103813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring multi-level information to obtain fine-grained features is the key factor to improve the performance of person re-identification (Re-ID). However, existing models for person Re-ID only focus on learning the high-level semantic information while neglecting the low-level detail information. To alleviate this issue, we propose a lightweight person Re-ID method termed Multi-Scale Semantic and Detail Extraction Network (MSDENet) to obtain robustness and discriminative feature representation for the Re-ID task. Specifically, we design a Series Channel-Spatial Attention (SCSA) and embed it into the lightweight backbone network to focus on the key parts of pedestrian images. Meanwhile, we propose a Multi-Scale Semantic and Detail Extraction (MSDE) method to extract multi-scale features of semantic information and detail information, which can effectively capture the feature diversity of pedestrian images. Furthermore, we design a Feature Enhancement Fusion (FEF), which enhances and fuses the fine-grained features of semantic extraction and detail extraction branches to better obtain the discriminative feature representation. Extensive experiments conducted on popular datasets Market1501, MSMT17, and CUHK03 demonstrate that the proposed MSDENet has competitive performance compared with the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Yunzuo Zhang and Weili Kang and Yameng Liu and Pengfei Zhu},
  doi          = {10.1016/j.cviu.2023.103813},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103813},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-scale semantic and detail extraction network for lightweight person re-identification},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized framework for image and video object
segmentation using affinity learning and message passing GNNS.
<em>CVIU</em>, <em>236</em>, 103812. (<a
href="https://doi.org/10.1016/j.cviu.2023.103812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant amount of work reported in the computer vision literature, segmenting images or videos based on multiple cues such as objectness, texture and motion, is still a challenge. This is particularly true when the number of objects to be segmented is not known or there are objects that are not classified in the training data (unknown objects). A possible remedy to this problem is to utilize graph-based clustering techniques such as Correlation Clustering. It is known that using long range affinities (Lifted multicut), makes correlation clustering more accurate than using only adjacent affinities (Multicut). However, the former is computationally expensive and hard to use. In this paper, we introduce a new framework to perform image/motion segmentation using an affinity learning module and a Message Passing Graph Neural Network (MPGNN). The affinity learning module uses a permutation invariant affinity representation to overcome the multi-object problem. The paper shows, both theoretically and empirically, that the proposed MPGNN aggregates higher order information and thereby converts the Lifted Multicut Problem (LMP) to a Multicut Problem (MP), which is easier and faster to solve. Importantly, the proposed method can be generalized to deal with different clustering problems with the same MPGNN architecture. For instance, our method produces competitive results for single image segmentation (on BSDS dataset) as well as unsupervised video object segmentation (on DAVIS17 dataset), by only changing the feature extraction part. In addition, using an ablation study on the proposed MPGNN architecture, we show that the way we update the parameterized affinities directly contributes to the accuracy of the results.},
  archive      = {J_CVIU},
  author       = {Sundaram Muthu and Ruwan Tennakoon and Tharindu Rathnayake and Reza Hoseinnezhad and David Suter and Alireza Bab-Hadiashar},
  doi          = {10.1016/j.cviu.2023.103812},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103812},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Generalized framework for image and video object segmentation using affinity learning and message passing GNNS},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature fine-tuning and attribute representation
transformation for zero-shot learning. <em>CVIU</em>, <em>236</em>,
103811. (<a href="https://doi.org/10.1016/j.cviu.2023.103811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Learning (ZSL) aims to generalize a pretrained classification model to unseen classes with the help of auxiliary semantic information. Recent generative methods are based on the paradigm of synthesizing unseen visual data from class attributes. A mapping is learnt from semantic attributes to visual features extracted by a pre-trained backbone such as ResNet101 by training a generative adversarial network . Considering the domain-shift problem between pre-trained backbone and task ZSL dataset as well as the information asymmetry problem between images and attributes, this manuscript suggests that the visual-semantic balance should be learnt separately from the ZSL models. In particular, we propose a plug-and-play Attribute Representation Transformation (ART) framework to pre-process visual features with a contrastive regression module and an attribute place-holder module. Our contrastive regression loss is a tailored design for visual-attribute transformation, which gains favorable properties from both classification and regression losses. As for the attribute place-holder module, an end-to-end mapping loss function is introduced to build the relationship between transformed features and semantic attributes. Experiments conducted on five popular benchmarks manifest that the proposed ART framework can significantly benefit existing generative models in both ZSL and generalized ZSL settings.},
  archive      = {J_CVIU},
  author       = {Shanmin Pang and Xin He and Wenyu Hao and Yang Long},
  doi          = {10.1016/j.cviu.2023.103811},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103811},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Feature fine-tuning and attribute representation transformation for zero-shot learning},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Webly-supervised semantic segmentation via curriculum
learning. <em>CVIU</em>, <em>236</em>, 103810. (<a
href="https://doi.org/10.1016/j.cviu.2023.103810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a weakly supervised semantic segmentation method by directly learning from web images, which are crawled from the Internet by using text queries, without any explicit user annotation or even data filtering. With the goal of handling the massive amount of noisy labels in web images, we design a three-stage approach for weakly-supervised semantic segmentation based on curriculum learning. We first generate pixel-level masks for the training images via a popular weakly-supervised semantic segmentation framework. Then, we consider the noise of the web data in two ways. At the image-level, the complexity of data is measured using its distribution density in a classification feature space. At the pixel-level, the complexity of the mask is evaluated by exploiting the relationship between the saliency map and those segmented images in an unsupervised manner . The key insight to this design is that, common and simple object patterns in images should be salient with both the saliency detector and weakly supervised DCNNs , where they should be sparse with high regional consistency between them. This allows for an efficient implementation of curriculum learning from noisy web images. Experiments on the popular PASCAL VOC 2012 benchmark show that we achieve very competitive performance with scores of 64.0\% mIoU using our pure web dataset, which contains noisy, single-label images. We further improve the performance to 69.7\% mIoU by using the CurriculumWebSegNet fine-tuned on the PASCAL VOC dataset, which has more precise multi-label supervision.},
  archive      = {J_CVIU},
  author       = {Zuxian Huang and Gangshan Wu and Limin Wang},
  doi          = {10.1016/j.cviu.2023.103810},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103810},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Webly-supervised semantic segmentation via curriculum learning},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online object tracking based interactive attention.
<em>CVIU</em>, <em>236</em>, 103809. (<a
href="https://doi.org/10.1016/j.cviu.2023.103809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By embedding Transformer into the Siamese tracking framework, some Transformer-based Siamese tracking network are proposed, such as TransT and SwinTrack. Nevertheless, the existing Transformer-based Siamese tracking networks do not fully utilize the object information of template branch, and their position encoding methods cannot accurately perceive the object position. Aiming at these problems, a novel Transformer-based Siamese tracking network, which includes feature extraction, feature fusion and prediction head, is proposed in this work. Firstly, an interactive attention calculation module for template branch and search branch is designed to enhance the feature extraction capability of the network for the object region and suppress the background interference. In addition, to address the problem that the existing Transformer-based feature fusion network is not sensitive enough to the object location region, a position encoding method that characterizes the relative distance is proposed to enhance the perception ability of the object location and reduce network parameters. Then, the contrastive loss is introduced to enhance the discriminative ability of the classification layer between foreground and background, and to effectively deal with the interference of similar objects in the background. Extensive experiments with state-of-the-art trackers are carried out on four challenging visual object tracking benchmarks: GOT-10k, LaSOText, TLP, and TrackingNet. Experimental results demonstrate the proposed method is more robust on multiple challenges and can achieve considerable performances with AO of 70.1\% on GOT-10k, SUC score of 45.5\% on LaSOT ext ext , 56.9\% on TLP, and 79.7\% on TrackingNet datasets. It runs faster (7.1G MACs) and occupies less memory (21M), making it more suitable for practical applications.},
  archive      = {J_CVIU},
  author       = {Hongmei Wang and Fan Guo},
  doi          = {10.1016/j.cviu.2023.103809},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103809},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Online object tracking based interactive attention},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D detection transformer: Set prediction of objects using
point clouds. <em>CVIU</em>, <em>236</em>, 103808. (<a
href="https://doi.org/10.1016/j.cviu.2023.103808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in 3D scenes rely on two main methods: detection based on proposals (two-stage detectors) or detections based on anchors (single-stage detectors), similar to approaches for object detection in 2D. In this paper, we propose the 3DeTR framework that produces 3D detections without the use of anchors or proposals, allowing training of the entire neural network in an end-to-end manner. Raw point cloud scenes are augmented and input into distance-and-reflectiveness-based feature extractor to produce representative points. Then, a transformer encoder–decoder module learns the local object relations and global context to generate parallel detections, which are then passed to a set-based loss function to map predictions to the set of ground truth labels uniquely. The model’s architecture produces 3D detections by regressing directly with the set of ground truths without the need for anchors or proposals, which are bottlenecks for object detection performances. We tested the framework on the KITTI Vision Benchmark Suite 3D object detection dataset, achieving results on par with the state-of-the-art: 80.37 AP on Cars (Moderate) class and 47.92 AP on Pedestrians (Moderate) class.},
  archive      = {J_CVIU},
  author       = {Thon Tan and Joanne Mun-Yee Lim and Ji Jinn Foo and Ramachandran Muniandy},
  doi          = {10.1016/j.cviu.2023.103808},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103808},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {3D detection transformer: Set prediction of objects using point clouds},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face identity and expression consistency for game character
face swapping. <em>CVIU</em>, <em>236</em>, 103806. (<a
href="https://doi.org/10.1016/j.cviu.2023.103806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customizing the appearance of game characters according to individual preferences is an important application in the gaming industry. The traditional solutions such as manual editing within the game engine require much time and some professional experience. To address this problem, this paper proposes a novel face-swapping framework to swap real faces onto game characters while maintaining the art style of the game. This approach helps players speed up the character customization process, as they can input any face and quickly see the results of swapping it onto the character. Specifically, our framework extracts a more robust identity embedding from compound face identity models. Then the identity embedding is sent into AdaIN for feature fusion and changes the identity attribute for the output of the decoder. To maintain the expression consistency between the swapped and target faces, we utilize a novel expression embedding loss which effectively constrains the fine-grain expression similarity. To reduce the cross-domain gap between human and game faces, we also construct a game face dataset and propose to use fine-tuning to improve the image quality of cross-domain face swapping. Extensive qualitative and quantitative experiments indicate that our method achieves leading swapping results in both pure natural human faces and game faces datasets.},
  archive      = {J_CVIU},
  author       = {Hao Zeng and Wei Zhang and Keyu Chen and Zhimeng Zhang and Lincheng Li and Yu Ding},
  doi          = {10.1016/j.cviu.2023.103806},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103806},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Face identity and expression consistency for game character face swapping},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modal alignment and translation for missing modality
action recognition. <em>CVIU</em>, <em>236</em>, 103805. (<a
href="https://doi.org/10.1016/j.cviu.2023.103805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal data provide complementary information on the same context, leading to performance improvement in video action recognition . However, in reality, not all modalities are available at test time. To this end, we propose Cross-Modal Alignment and Translation (CMAT) framework for action recognition that is robust to missing modalities. Specifically, our framework first aligns representations of multiple modalities from the same video sample through contrastive learning , effectively alleviating the bias with respect to the type of missing modality. Then, CMAT learns to translate representations of one modality into that of another modality. This allows the representations of the missing modalities to be generated from the remaining modalities during the testing. Accordingly, CMAT fully utilizes multimodal information obtained through abundant interactions across modalities. The proposed CMAT achieves state-of-the-art performances in both complete and missing modality settings on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets. Moreover, extensive ablation studies demonstrate the effectiveness of our design.},
  archive      = {J_CVIU},
  author       = {Yeonju Park and Sangmin Woo and Sumin Lee and Muhammad Adi Nugroho and Changick Kim},
  doi          = {10.1016/j.cviu.2023.103805},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103805},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cross-modal alignment and translation for missing modality action recognition},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global-aware and local-aware enhancement network for person
search. <em>CVIU</em>, <em>236</em>, 103804. (<a
href="https://doi.org/10.1016/j.cviu.2023.103804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search, one of the increasingly important tasks of computer vision , performs pedestrian localization and re-identification simultaneously. Several studies incorporate anchor-free detectors into person search models and achieve good results. However, these methods ignore learning the contextual information of images, which is essential for improving the performance. To solve this problem, we propose a global-aware and local-aware enhancement network for person search. The stem network is constructed using hierarchical vision transformers to adaptively learn the global context. A local-aware enhancement module is proposed to learn the local context and incorporate multi-level features. To facilitate adequate learning on the limited training data, we design a symmetric online instance matching (SOIM) loss for end-to-end training the whole model. We conduct experiments on two benchmark datasets (the CUHK-SYSU and the PRW dataset) in comparison with the state-of-the-art methods. The results demonstrate the comparable performance of our proposed method.},
  archive      = {J_CVIU},
  author       = {Ning Lv and Xuezhi Xiang and Xinyao Wang and Yulong Qiao and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.cviu.2023.103804},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103804},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Global-aware and local-aware enhancement network for person search},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end learning for weakly supervised video anomaly
detection using absorbing markov chain. <em>CVIU</em>, <em>236</em>,
103798. (<a href="https://doi.org/10.1016/j.cviu.2023.103798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a principled deep neural network framework with Absorbing Markov Chain (AMC) for weakly supervised anomaly detection in surveillance videos . Our model consists of both a weakly supervised binary classification network and a Graph Convolutional Network (GCN), which are jointly optimized by backpropagation . Unlike the previous works that employ AMC for label noise filtering in a post-processing step, the proposed framework migrates the component inside the GCN part of our model and realizes an end-to-end learning network. The integration of the AMC module into our deep neural network model enables us to learn the associated parameters automatically, which is helpful to improve the quality of segment-wise label estimation via tightly-coupled processing between the main network and the AMC module. In addition, we introduce a pseudo-labeling strategy based on Gaussian mixture model to fully utilize examples in abnormal videos. Our algorithm achieves outstanding performance compared to the state-of-the-art weakly supervised anomaly detection methods on UCF-Crime and ShanghaiTech datasets.},
  archive      = {J_CVIU},
  author       = {Jaeyoo Park and Junha Kim and Bohyung Han},
  doi          = {10.1016/j.cviu.2023.103798},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103798},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {End-to-end learning for weakly supervised video anomaly detection using absorbing markov chain},
  volume       = {236},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving sparse graph attention for feature matching by
informative keypoints exploration. <em>CVIU</em>, <em>235</em>, 103803.
(<a href="https://doi.org/10.1016/j.cviu.2023.103803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching refers to building correct point correspondences from two-view images. Typical methods create coarse matches merely based on the similarity of local descriptors , then additionally use geometrical constraints or coherence to get a clean match set. However, this indirect paradigm would be strictly limited by the pre-matching results. This can be alleviated by a GNN-based matching network that constructs correct correspondences directly from feature positions and descriptions. Attentional pooling is utilized to reduce the high computational complexity and memory usage by sparsing the graph attention. In this paper, we propose an enhanced sparse GNN , namely KeyGNN, for fast feature matching, by designing Guided Attentional Pooling to emphasize the informative cues in GNN layers. Specifically, we propose Informative Keypoints Exploration Module and Guided Sparse Module to respectively extract information-rich points and use them to guide the attentional pooling for better information preservation. This process is constrained by the proposed Structural Importance Ranking Loss and Descriptor Distribution Loss , that ensure the structural information can be correctly extracted and the generated feature vectors be well distributed in the deep space. Unlike existing techniques, our method can focus and preserve more on the main information in the sparse GNN stage, thus avoiding information loss and enhancing the accuracy. Extensive experiments on large and public datasets reveal that our method can obtain better performance than the state-of-the-arts in the tasks of camera pose estimation, fundamental matrix estimation, and visual localization . The runtime and memory test shows that our network takes low computational and memory usage, which is friendly to real-time vision tasks.},
  archive      = {J_CVIU},
  author       = {Xingyu Jiang and Shihua Zhang and Xiao-Ping Zhang and Jiayi Ma},
  doi          = {10.1016/j.cviu.2023.103803},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103803},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improving sparse graph attention for feature matching by informative keypoints exploration},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blur aware metric depth estimation with multi-focus
plenoptic cameras. <em>CVIU</em>, <em>235</em>, 103802. (<a
href="https://doi.org/10.1016/j.cviu.2023.103802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While a traditional camera only captures one point of view of a scene, a plenoptic or light-field camera, is able to capture spatial and angular information in a single snapshot, enabling depth estimation from a single acquisition. In this paper, we present a new metric depth estimation algorithm using only raw images from a multi-focus plenoptic camera. The proposed approach is especially suited for the multi-focus configuration where several micro-lenses with different focal lengths are used. The main goal of our blur aware depth estimation (BLADE) approach is to improve disparity estimation for defocus stereo images by integrating both correspondence and defocus cues. We thus leverage blur information where it was previously considered a drawback. We explicitly derive an inverse projection model including the defocus blur providing depth estimates up to a scale factor. A method to calibrate the inverse model is then proposed. We thus take into account depth scaling to achieve precise and accurate metric depth estimates. Our results show that introducing defocus cues improves the depth estimation. We demonstrate the effectiveness of our framework and depth scaling calibration on relative depth estimation setups and on real-world 3D complex scenes with ground truth acquired with a 3D lidar scanner.},
  archive      = {J_CVIU},
  author       = {Mathieu Labussière and Céline Teulière and Omar Ait-Aider},
  doi          = {10.1016/j.cviu.2023.103802},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103802},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Blur aware metric depth estimation with multi-focus plenoptic cameras},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep bregman divergence for self-supervised representations
learning. <em>CVIU</em>, <em>235</em>, 103801. (<a
href="https://doi.org/10.1016/j.cviu.2023.103801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Bregman divergence measures the divergence of data points using convex neural networks , which is beyond Euclidean distance and capable of capturing divergence over distributions. The non-Euclidean geometry is not well explored in deep representation learning and remains a challenging endeavor for self-supervised representation learning. In this paper, we propose deep Bregman divergences for self-supervised pretext task learning, where we aim to enhance self-supervised embedding representation by training additional networks based on functional Bregman divergences. Our framework can capture the divergence of embedding distributions and improve the quality of learned representation using an arbitrary Bregman divergence over data embedding. Specifically, we develop a novel self-supervised architecture and a new divergence loss that measures the asymmetric distance of arbitrary Bergman divergences of neural networks. We show that the combination of self-supervised contrastive learning and our proposed method outperforms the baseline as well as most established methods for self-supervised and semi-supervised learning on multiple classifications and object detection tasks and datasets. Moreover, the learned representations generalize well when transferred to other datasets and tasks.},
  archive      = {J_CVIU},
  author       = {Mina Rezaei and Farzin Soleymani and Bernd Bischl and Shekoofeh Azizi},
  doi          = {10.1016/j.cviu.2023.103801},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103801},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep bregman divergence for self-supervised representations learning},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the differences in adversarial robustness between
ViT- and CNN-based models using novel metrics. <em>CVIU</em>,
<em>235</em>, 103800. (<a
href="https://doi.org/10.1016/j.cviu.2023.103800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning models have demonstrated remarkable performance in a variety of fields, owing to advancements in computational power and the availability of extensive datasets for training large-scale models. Nonetheless, these models inherently possess a vulnerability wherein even small alterations to the input can lead to substantially different outputs. Consequently, it is imperative to assess the robustness of deep-learning models prior to relying on their decision-making capabilities. In this study, we investigate the adversarial robustness of convolutional neural networks (CNNs), vision transformers (ViTs), and hybrid CNNs ＋ViTs, which represent prevalent architectures in computer vision . Our evaluation is grounded on four novel model-sensitivity metrics that we introduce. These metrics are evaluated in the context of random noise and gradient-based adversarial perturbations. To ensure a fair comparison, we employ models with comparable capacities within each group and conduct experiments separately, utilizing ImageNet-1K and ImageNet-21K as pretraining data. Our fair experimental results provide empirical evidence that ViT-based models exhibit higher adversarial robustness than CNN-based counterparts, helping to dispel doubts about the findings of prior studies. Additionally, we introduce novel metrics that contribute new insights into the previously unconfirmed characteristics of these models.},
  archive      = {J_CVIU},
  author       = {Jaehyuk Heo and Seungwan Seo and Pilsung Kang},
  doi          = {10.1016/j.cviu.2023.103800},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103800},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Exploring the differences in adversarial robustness between ViT- and CNN-based models using novel metrics},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Collaborative three-stream transformers for video
captioning. <em>CVIU</em>, <em>235</em>, 103799. (<a
href="https://doi.org/10.1016/j.cviu.2023.103799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the most critical components in a sentence, subject, predicate and object require special attention in the video captioning task. To implement this idea, we design a novel framework, named COllaborative three-Stream Transformers (COST), to model the three parts separately and complement each other for better representation. Specifically, COST is formed by three branches of transformers to exploit the visual-linguistic interactions of different granularities in spatial–temporal domain between videos and text, detected objects and text, and actions and text. Meanwhile, we propose a cross-granularity attention module to align the interactions modeled by the three branches of transformers, then the three branches of transformers can support each other to exploit the most discriminative semantic information of different granularities for accurate predictions of captions. The whole model is trained in an end-to-end fashion. Extensive experiments conducted on three large-scale challenging datasets, i.e. , YouCookII, ActivityNet Captions and MSVD, demonstrate that the proposed method performs favorably against the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Hao Wang and Libo Zhang and Heng Fan and Tiejian Luo},
  doi          = {10.1016/j.cviu.2023.103799},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103799},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Collaborative three-stream transformers for video captioning},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved short-term dense bottleneck network for efficient
scene analysis. <em>CVIU</em>, <em>235</em>, 103795. (<a
href="https://doi.org/10.1016/j.cviu.2023.103795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual scene understanding mainly depends on pixel-wise classification obtained from a deep convolutional neural network . However, existing semantic segmentation models often face difficulties in real-time applications due to their large network architecture . Although there are real-time semantic segmentation models available, their shallow backbone can degrade the performance considerably. This paper introduces SDBNetV2, a lightweight semantic segmentation model designed to improve real-time performance without increasing computational costs. A key contribution is a novel Short-term Dense Bottleneck (SDB) module in the encoder, which provides varied field-of-views to capture different geometrical objects in a complex scene. Additionally, we propose dense feature refinement and improved semantic aggregation modules at the decoder end to enhance contextualization and object localization . We evaluate the proposed model’s performance on several indoor and outdoor datasets in structured and unstructured environments. The results show that SDBNetV2 achieves superior segmentation performance over other real-time models with less than 2 million parameters.},
  archive      = {J_CVIU},
  author       = {Tanmay Singha and Duc-Son Pham and Aneesh Krishna},
  doi          = {10.1016/j.cviu.2023.103795},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103795},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improved short-term dense bottleneck network for efficient scene analysis},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust teacher: Self-correcting pseudo-label-guided
semi-supervised learning for object detection. <em>CVIU</em>,
<em>235</em>, 103788. (<a
href="https://doi.org/10.1016/j.cviu.2023.103788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised object detection methods have become increasingly popular in computer vision owing to their success in reducing data labeling costs. However, low-quality pseudo-labels exhibit difficulties in correction and severely limit the performance of existing state-of-the-art models. To address these problems, based on the contemporary teacher–student dual framework, we develop a novel self-correcting pseudo-label module to generate more reliable predictions for unlabeled data . Simultaneously, to mitigate the inherent class bias in pseudo-labels, we integrate multi-label classification results for measuring the re-balanced focal loss to enable class-agnostic, robust semi-supervised learning. Furthermore, pseudo-label-guided copy-paste is proposed as a simple yet efficient data augmentation technique to enhance instance representation learning within diverse complex scenes. The above key designs constitute our proposed approach, which we call Robust Teacher. Extensive experiments on PASCAL VOC and MS COCO demonstrate that Robust Teacher achieves competitive results and outperforms state-of-the-art models by a large margin. Our comprehensive ablation studies further verify the effectiveness of the key components. The code will be publicly available at https://github.com/Complicateddd/RobustT .},
  archive      = {J_CVIU},
  author       = {Shijie Li and Junmin Liu and Weilin Shen and Jianyong Sun and Chengli Tan},
  doi          = {10.1016/j.cviu.2023.103788},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103788},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Robust teacher: Self-correcting pseudo-label-guided semi-supervised learning for object detection},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3DF-FCOS: Small object detection with 3D features based on
FCOS. <em>CVIU</em>, <em>235</em>, 103787. (<a
href="https://doi.org/10.1016/j.cviu.2023.103787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower resolution and fewer feature details appear in small objects, so achieving small object detection is a challenging task. To improve the detection ability in the case of low-resolution object details, we propose to exploit 3D features with FCOS for representing small objects. The proposed method employs a Global Spatial Block (GS-Block) in the backbone to guide the network to learn the picture’s shallow features and increase the model’s perceived capability for small objects. In addition, we introduce a 3D sparse convolution in the shallow input features of the detection heads to better capture the global and contextual information of the shallow feature elements and further improve the feature representation of small objects in the backgrounds, and the whole network is referred to as 3DF-FCOS. Finally, we conducted extensive experiments to verify the effectiveness of 3DF-FCOS on VisDrone2019Det. The results show that our method achieves a remarkable improvement in AP and A P s APs compared to the baselines of 1.2\% and 1.2\%, respectively. Compared with current state-of-the-art (SOTA) algorithms, the performance of our proposed 3DF-FCOS is comparable.},
  archive      = {J_CVIU},
  author       = {Xiaobao Yang and Yulong He and Junsheng Wu and Wei Sun and Tianyu Liu and Sugang Ma},
  doi          = {10.1016/j.cviu.2023.103787},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103787},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {3DF-FCOS: Small object detection with 3D features based on FCOS},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object tracking based on siamese network with 3D attention
and multiple graph attention. <em>CVIU</em>, <em>235</em>, 103786. (<a
href="https://doi.org/10.1016/j.cviu.2023.103786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the object tracking algorithm based on the siamese network is the most popular research direction of object tracking. However, most siamese network trackers are unable to update the template, resulting in hardly dealing with the fuzzy target well, which requires the trackers to extract the target features in the template more accurately. Moreover, most of the siamese network trackers compute the similarity between the target template and the search region in a global matching way. Thus they cannot well handle the rapid change of the target. To address the above limitations, we propose an object tracking algorithm based on 3D attention SimAM and multiple graph attention. We combine siamese networks with 3D attention to improve the feature extraction capability of the network. We frame the target template and search region to improve feature-matching accuracy by introducing multiple graph attention mechanisms . We test our model against three authoritative benchmarks, GOT10K, UAV123 and OTB100, and compare it with advanced trackers, and the results show that our trackers achieve better results.},
  archive      = {J_CVIU},
  author       = {Shilei Yan and Yujuan Qi and Mengxue Liu and Yanjiang Wang and Baodi Liu},
  doi          = {10.1016/j.cviu.2023.103786},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103786},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Object tracking based on siamese network with 3D attention and multiple graph attention},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structural reasoning for image-based social relation
recognition. <em>CVIU</em>, <em>235</em>, 103785. (<a
href="https://doi.org/10.1016/j.cviu.2023.103785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern societies are composed of complex structures that emerge from the relationships between individuals, and the comprehension of these arrangements has the potential to become a powerful tool for intelligent systems. Current image-based social relation recognition methods isolate specific information from the input to capture essential aspects defining these relationships. However, this is an inaccurate approach since the interaction between all these parts form an intricate structure, which is as valuable as the data each piece carries individually. Consequently, capturing this implicit social structure is essential to achieve the high-level reasoning required to identify relationships adequately. In this work, we propose a novel approach to interpret relationships based on three distinct scopes considering individual, relative, and general information. Additionally, it also takes into account prior knowledge and data dependencies between all these different social perspectives. The Social Knowledge Graph (SKG) is proposed based on these concepts, producing a representation capable of replicating the original social structure. This unique representation is exploited with the Social Graph Network (SGN) by employing specific feature aggregation strategies according to the information embedded into the graph. The performance of the proposed method was evaluated in well-known benchmarks for social relation recognition, achieving a new state-of-the-art. Finally, a deep analysis of the methodology and its main concepts is conducted, delivering results that support our interpretation of social relationships.},
  archive      = {J_CVIU},
  author       = {Eduardo V. Sousa and Douglas G. Macharet},
  doi          = {10.1016/j.cviu.2023.103785},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103785},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Structural reasoning for image-based social relation recognition},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capturing geometric structure change through deformation
aware correlation. <em>CVIU</em>, <em>235</em>, 103784. (<a
href="https://doi.org/10.1016/j.cviu.2023.103784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing the inner structure change of the target is crucial for a siamese tracker confronted with severe deformation or occlusion. The conventional siamese tracker utilizes a strict grid-to-grid correlation to pass the template information towards the current candidate, which may suffer severe mismatch compared to the initial frame caused by undesirable conditions such as rotation and deformation. Thus such mismatch would lead to insufficient information passed from the template and eventually end with a poor performance in classification and regression branches. The inherent CNN structure with the restricted grid-to-grid matching mechanism during correlation is not able to capture the corresponding change of the target structure. Motivated by DCNv2, we propose a novel deformation aware correlation to replace the widely-used depthwise-correlation. An offset-prediction module is proposed based on the similarity matrix to compensate for the mismatch of the grid assignment between template and candidate. Then we utilize an importance bilinear sampling algorithm to assess the similarity of the matching pair, alleviating the occlusion drift problem. Finally, a modulated deformable correlation is fed into the aforementioned input and realizes a better information modulation. We replace the correlation operator on SiamRPN++ with our deformation aware correlation operator and obtain our tracker, SiamDAC. Massive experiments on OTB100, UAV123, VOT2019, LaSOT and DTB70 validate the effectiveness of our proposed operator, especially confronted with severe deformation or occlusion.},
  archive      = {J_CVIU},
  author       = {Jiahao Wu and Bo Ma and Yuping Zhang and Xin Yi},
  doi          = {10.1016/j.cviu.2023.103784},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103784},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Capturing geometric structure change through deformation aware correlation},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unpaired sonar image denoising with simultaneous contrastive
learning. <em>CVIU</em>, <em>235</em>, 103783. (<a
href="https://doi.org/10.1016/j.cviu.2023.103783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implementing underwater sonar image denoising algorithms through unpaired clean and noisy images is feasible and meaningful, since compliant paired real underwater sonar data is nearly impractical. Nevertheless, acquiring high-performance denoising algorithm is challenging without pairs of data as learning objects . In addition, the wide domain gap between noisy and clean samples as well as the noise contained in underwater sonar images is significantly more complex than that involved in normal images. Therefore, directly utilizing current unpaired optimization algorithms (e.g. generative adversarial learning and circular consistency learning) in underwater sonar image denoising tasks is not sufficient to explore the essential connection from noisy inputs to clean outputs. In this paper, we propose a novel framework for unpaired sonar image denoising which probes the same features of unpaired samples through a dual contrast optimization approach in latent space, named as NCD-GAN. The designed algorithm is composed of two basic parts: Simultaneous Transition Structure (STS) and Separable Contrast Unit (SCU). Separately, STS adopts a cyclic structure of multi-domain transformation to generate valuable surface information, and mines potential feature distributions between different domains via the addition of bidirectional mappings. Meanwhile, SCU introduces additional constraints in layer domains to encourage closer background layers between adjacent domains, while increasing the distance between noise layers so as to better promote the removal of noise and assist in image restoration. Sufficient experiments confirm that our algorithm provides better performance than existing unpaired denoising algorithms on multiple publicly available sonar image datasets and exhibits excellent robustness in the processing of multiple noises. Compared to the latest denoising algorithms, there is at most a 26.43\% improvement in the evaluation metrics . Furthermore, it gains a 22.75\% mAP boost on the subsequent advanced visual tasks.},
  archive      = {J_CVIU},
  author       = {Boyu Zhao and Qian Zhou and Lijun Huang and Qiang Zhang},
  doi          = {10.1016/j.cviu.2023.103783},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103783},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unpaired sonar image denoising with simultaneous contrastive learning},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards explainable deep visual saliency models.
<em>CVIU</em>, <em>235</em>, 103782. (<a
href="https://doi.org/10.1016/j.cviu.2023.103782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have shown their profound impact on achieving human-level performance in visual saliency prediction. However, it is still unclear how they learn their task and what it means in terms of understanding human visual system . In this work, we propose a framework to derive explainable saliency models from their corresponding deep architectures. Mainly, we explain a deep saliency model by understanding its four different aspects: (1) intermediate activation maps of deep layers, (2) biologically plausible Log-Gabor ( LG ) filters for salient region identification, (3) positional biased behavior of Log-Gabor filters and (4) processing of color information by establishing a relevance with human visual system. We consider four state-of-the-art (SOTA) deep saliency models, namely CMRNet, UNISAL, DeepGaze IIE, and MSI-Net for their interpretation using our proposed framework. We observe that explainable models perform way better than the classical SOTA models. We also find that CMRNet transforms the input RGB space to a representation after the input layer, which is very close to YUV space of a color image. Then, we discuss about the biological consideration and relevance of our framework for its possible anatomical substratum of visual attention. We find a good correlation between components of HVS and the base operations of the proposed technique. Hence, we say that this generic explainable framework provides a new perspective to see relationship between classical methods/human visual system and DNN based ones.},
  archive      = {J_CVIU},
  author       = {Sai Phani Kumar Malladi and Jayanta Mukherjee and Mohamed-Chaker Larabi and Santanu Chaudhury},
  doi          = {10.1016/j.cviu.2023.103782},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103782},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Towards explainable deep visual saliency models},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint coupled dictionaries-based visible-infrared image
fusion method via texture preservation structure in sparse domain.
<em>CVIU</em>, <em>235</em>, 103781. (<a
href="https://doi.org/10.1016/j.cviu.2023.103781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint sparse coding (JSC) and coupled dictionary learning (CDL) have been successful in visible and infrared image fusion tasks. However, JSC fusion methods using a single dictionary cannot perfectly characterize different modal signals. The CDL-based fusion methods ignore the role of high-frequency components in texture preservation and sharpness. In this study, we design fusion rules while considering the uniqueness and connection of different feature spaces to alleviate this issue. In the proposed fusion method, coupled dictionaries are first used to establish the relationship between infrared and visible images. Then, we use joint sparse coding and coupled dictionaries to calculate sparse coefficients via the joint sparse model. Finally, the improved coefficient fusion strategy realizes the conversion and synthesis between different spatial images . Experimental results on the TNO and RoadScene datasets show that the proposed method achieves promising results in terms of objective and subjective performance compared with state-of-the-art algorithms.},
  archive      = {J_CVIU},
  author       = {Chengfang Zhang and Haoyue Li and Ziliang Feng and Sidi He},
  doi          = {10.1016/j.cviu.2023.103781},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103781},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Joint coupled dictionaries-based visible-infrared image fusion method via texture preservation structure in sparse domain},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based estimation of whole-body kinematics from
multi-view images. <em>CVIU</em>, <em>235</em>, 103780. (<a
href="https://doi.org/10.1016/j.cviu.2023.103780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is necessary to analyze the whole-body kinematics (including joint locations and joint angles) to assess risks of fatal and musculoskeletal injuries in occupational tasks. Human pose estimation has gotten more attention in recent years as a method to minimize the errors in determining joint locations. However, the joint angles are not often estimated, nor is the quality of joint angle estimation assessed. In this paper, we presented an end-to-end approach on direct joint angle estimation from multi-view images. Our method leveraged the volumetric pose representation and mapped the rotation representation to a continuous space where each rotation was uniquely represented. We also presented a new kinematic dataset in the domain of residential roofing with a data processing pipeline to generate necessary annotations for the supervised training procedure on direct joint angle estimation. We achieved a mean angle error of 7 . 19 ° 7.19° on the new Roofing dataset and 8 . 41 ° 8.41° on the Human3.6M dataset, paving the way for employment of on-site kinematic analysis using multi-view images.},
  archive      = {J_CVIU},
  author       = {Kien X. Nguyen and Liying Zheng and Ashley L. Hawke and Robert E. Carey and Scott P. Breloff and Kang Li and Xi Peng},
  doi          = {10.1016/j.cviu.2023.103780},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103780},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep learning-based estimation of whole-body kinematics from multi-view images},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdvFAS: A robust face anti-spoofing framework against
adversarial examples. <em>CVIU</em>, <em>235</em>, 103779. (<a
href="https://doi.org/10.1016/j.cviu.2023.103779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the reliability of face recognition systems against presentation attacks necessitates the deployment of face anti-spoofing techniques. Despite considerable advancements in this domain, the ability of even the most state-of-the-art methods to defend against adversarial examples remains elusive. While several adversarial defense strategies have been proposed, they typically suffer from constrained practicability due to inevitable trade-offs between universality, effectiveness, and efficiency. To overcome these challenges, we thoroughly delve into the coupled relationship between adversarial detection and face anti-spoofing. Based on this, we propose a robust face anti-spoofing framework, namely AdvFAS, that leverages two coupled scores to accurately distinguish between correctly detected and wrongly detected face images. Extensive experiments demonstrate the effectiveness of our framework in a variety of settings, including different attacks, datasets, and backbones, meanwhile enjoying high accuracy on clean examples. Moreover, we successfully apply the proposed method to detect real-world adversarial examples.},
  archive      = {J_CVIU},
  author       = {Jiawei Chen and Xiao Yang and Heng Yin and Mingzhi Ma and Bihui Chen and Jianteng Peng and Yandong Guo and Zhaoxia Yin and Hang Su},
  doi          = {10.1016/j.cviu.2023.103779},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103779},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AdvFAS: A robust face anti-spoofing framework against adversarial examples},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive neural painting. <em>CVIU</em>, <em>235</em>,
103778. (<a href="https://doi.org/10.1016/j.cviu.2023.103778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, Neural Painting (NP) techniques became capable of producing extremely realistic artworks. This paper advances the state of the art in this emerging research domain by proposing the first approach for Interactive NP. Considering a setting where a user looks at a scene and tries to reproduce it on a painting, our objective is to develop a computational framework to assist the user’s creativity by suggesting the next strokes to paint, that can be possibly used to complete the artwork. To accomplish such a task, we propose I-Paint, a novel method based on a conditional transformer Variational AutoEncoder (VAE) architecture with a two-stage decoder. To evaluate the proposed approach and stimulate research in this area, we also introduce two novel datasets. Our experiments show that our approach provides good stroke suggestions and compares favorably to the state of the art.},
  archive      = {J_CVIU},
  author       = {Elia Peruzzo and Willi Menapace and Vidit Goel and Federica Arrigoni and Hao Tang and Xingqian Xu and Arman Chopikyan and Nikita Orlov and Yuxiao Hu and Humphrey Shi and Nicu Sebe and Elisa Ricci},
  doi          = {10.1016/j.cviu.2023.103778},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103778},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Interactive neural painting},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain fashion cloth retrieval via novel
attention-guided cascade neural network and clothing parsing.
<em>CVIU</em>, <em>235</em>, 103777. (<a
href="https://doi.org/10.1016/j.cviu.2023.103777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain fashion retrieval is a feasible method for finding the required clothing among a large number of products. Computer vision-based retrieval algorithms have been developed by many researchers. However, the precise retrieval process still faces challenges such as corrupted query images, ambiguous patterns, and an array of attributes. In this work, a novel attention-guided cascaded network has been proposed for efficient cloth retrieval by overcoming the aforementioned challenges. Initially, the input images are pre-processed with Wavelet integrated Retinex algorithm (WRA) to enhance the query cloth image without diminishing the important information. The attention-guided cascaded network is designed with global feature descriptor (GFD) and local feature descriptor (LFD) modules for extracting the domain-enriched features. These two modules are integrated with an attention block to reduce complexity and improve performance using selected features. In the LFD module, the Clothing Parsing Encoder-Decoder network (CPED-Net) is used to separate the bottom and top regions and sends them into the respective network for fine-grained feature analysis. Finally, global and local features from the modules are combined in similarity analysis for cloth retrieval. The proposed model achieves an overall accuracy of 98.54\% based on the fashion retrieval benchmarks dataset. From the experimental results, the proposed model is superior to the state-of-the-art models.},
  archive      = {J_CVIU},
  author       = {Saranya M.S. and Geetha P.},
  doi          = {10.1016/j.cviu.2023.103777},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103777},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cross-domain fashion cloth retrieval via novel attention-guided cascade neural network and clothing parsing},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing multiple salient object detection via dual-space
long-range dependencies. <em>CVIU</em>, <em>235</em>, 103776. (<a
href="https://doi.org/10.1016/j.cviu.2023.103776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection plays an important role in many downstream tasks. However, complex real-world scenes with varying scales and numbers of salient objects still pose a challenge. In this paper, we directly address the problem of detecting multiple salient objects across complex scenes. We propose a network architecture incorporating non-local feature information in both the spatial and channel spaces, capturing the long-range dependencies between separate objects. Traditional bottom-up and non-local features are combined with edge features within a feature fusion gate that progressively refines the salient object prediction in the decoder. We show that our approach accurately locates multiple salient regions even in complex scenarios. To demonstrate the efficacy of our approach to the multiple salient objects problem, we curate a new dataset containing only multiple salient objects. Our experiments demonstrate the proposed method presents state-of-the-art results on five widely used datasets without any pre-processing and post-processing. We obtain a further performance improvement against competing techniques on our multi-objects dataset. The dataset and source code are available at: https://github.com/EricDengbowen/DSLRDNet .},
  archive      = {J_CVIU},
  author       = {Bowen Deng and Andrew P. French and Michael P. Pound},
  doi          = {10.1016/j.cviu.2023.103776},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103776},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Addressing multiple salient object detection via dual-space long-range dependencies},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised cycle-GAN for face photo-sketch translation
in the wild. <em>CVIU</em>, <em>235</em>, 103775. (<a
href="https://doi.org/10.1016/j.cviu.2023.103775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of face photo-sketch translation has improved a lot thanks to deep neural networks . GAN based methods trained on paired images can produce high-quality results under laboratory settings. Such paired datasets are, however, often very small and lack diversity. Meanwhile, Cycle-GANs trained with unpaired photo-sketch datasets suffer from the steganography phenomenon, which makes them not effective to face photos in the wild. In this paper, we introduce a semi-supervised approach with a noise-injection strategy, named Semi-Cycle-GAN (SCG), to tackle these problems. For the first problem, we propose a pseudo sketch feature representation for each input photo composed from a small reference set of photo-sketch pairs, and use the resulting pseudo pairs to supervise a photo-to-sketch generator G p 2 s Gp2s . The outputs of G p 2 s Gp2s can in turn help to train a sketch-to-photo generator G s 2 p Gs2p in a self-supervised manner. This allows us to train G p 2 s Gp2s and G s 2 p Gs2p using a small reference set of photo-sketch pairs together with a large face photo dataset (without ground-truth sketches). For the second problem, we show that the simple noise-injection strategy works well to alleviate the steganography effect in SCG and helps to produce more reasonable sketch-to-photo results with less overfitting than fully supervised approaches. Experiments show that SCG achieves competitive performance on public benchmarks and superior results on photos in the wild.},
  archive      = {J_CVIU},
  author       = {Chaofeng Chen and Wei Liu and Xiao Tan and Kwan-Yee K. Wong},
  doi          = {10.1016/j.cviu.2023.103775},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103775},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semi-supervised cycle-GAN for face photo-sketch translation in the wild},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental few-shot object detection with scale- and
centerness-aware weight generation. <em>CVIU</em>, <em>235</em>, 103774.
(<a href="https://doi.org/10.1016/j.cviu.2023.103774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work targets the task of Incremental Few-Shot Detection (iFSD), which requires quickly detecting objects of novel classes after seeing only a few training examples, while maintaining the ability to detect old classes. Different from re-training a new detector to incorporate few-shot novel categories, we tackle iFSD in a direct weight generation way. Specifically, we propose to reorganize base weights by leveraging responses of novel region features, hence transferring learned information from base weights to the novel but relevant ones. To obtain representative region features for the novel weight generation, two strategies are further presented. The scale-aware strategy respects the scale variation in object detection, and assigns each object to its corresponding pyramid to obtain the scale-specific region feature. The centerness-aware strategy reorganizes region features in the spatial range, since the localization quality of each pixel on the feature map varies. Extensive experiments are conducted on the challenging MS COCO and PASCAL VOC datasets, and our method achieves promising performances under various settings with a direct weight generation fashion. Furthermore, our generated weights can be used as good initialization for fine-tuning to induce faster convergence, thus mitigating the performance drop on old classes and improving overall results in the incremental setting.},
  archive      = {J_CVIU},
  author       = {Lu Zhang and Xu Yang and Lu Qi and Shaofeng Zeng and Zhiyong Liu},
  doi          = {10.1016/j.cviu.2023.103774},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103774},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Incremental few-shot object detection with scale- and centerness-aware weight generation},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MeT: A graph transformer for semantic segmentation of 3D
meshes. <em>CVIU</em>, <em>235</em>, 103773. (<a
href="https://doi.org/10.1016/j.cviu.2023.103773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polygonal meshes have become the standard for discretely approximating 3D shapes, thanks to their efficiency and high flexibility in capturing non-uniform shapes. This non-uniformity, however, leads to irregularity in the mesh structure, making tasks like segmentation of 3D meshes particularly challenging. Semantic segmentation of 3D mesh has been typically addressed through CNN-based approaches, leading to good accuracy. Recently, transformers have gained enough momentum both in NLP and computer vision fields, achieving performance at least on par with CNN models, supporting the long-sought architecture universalism. Following this trend, we propose a transformer-based method for semantic segmentation of 3D mesh motivated by a better modeling of the graph structure of meshes, by means of global attention mechanisms . In order to address the limitations of standard transformer architectures in modeling relative positions of non-sequential data, as in the case of 3D meshes, as well as in capturing the local context, we perform positional encoding by means the Laplacian eigenvectors of the adjacency matrix , replacing the traditional sinusoidal positional encodings, and by introducing clustering-based features into the self-attention and cross-attention operators. Experimental results, carried out on three sets of the Shape COSEG Dataset (Wang et al., 2012), on the human segmentation dataset proposed in Maron et al. (2017) and on the ShapeNet benchmark (Chang et al., 2015), show how the proposed approach yields state-of-the-art performance on semantic segmentation of 3D meshes.},
  archive      = {J_CVIU},
  author       = {Giuseppe Vecchio and Luca Prezzavento and Carmelo Pino and Francesco Rundo and Simone Palazzo and Concetto Spampinato},
  doi          = {10.1016/j.cviu.2023.103773},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103773},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MeT: A graph transformer for semantic segmentation of 3D meshes},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalizing proposals using classifiers for semi-supervised
object detection. <em>CVIU</em>, <em>235</em>, 103772. (<a
href="https://doi.org/10.1016/j.cviu.2023.103772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining gold standard annotated data for object detection is often costly, involving human-level effort. Semi-supervised object detection algorithms solve the problem with a small amount of gold-standard labels and a large unlabeled dataset used to generate silver-standard labels. But training on the silver standard labels does not produce good results, because they are machine-generated annotations. In this work, we design a modified loss function to train on large silver standard annotated sets generated by a weak annotator . We include a confidence metric associated with the annotation as an additional term in the loss function, signifying the quality of the annotation. We test the effectiveness of our approach on various test sets and use numerous variations to compare the results with some of the current approaches to object detection. In comparison with the baseline where no confidence metric is used, we achieved a 4\% gain in mAP with 25\% labeled data and 10\% gain in mAP with 50\% labeled data by using the proposed confidence metric.},
  archive      = {J_CVIU},
  author       = {Somnath Hazra and Pallab Dasgupta},
  doi          = {10.1016/j.cviu.2023.103772},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103772},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Penalizing proposals using classifiers for semi-supervised object detection},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GEIKD: Self-knowledge distillation based on gated ensemble
networks and influences-based label noise removal. <em>CVIU</em>,
<em>235</em>, 103771. (<a
href="https://doi.org/10.1016/j.cviu.2023.103771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-distillation has gained widespread attention in recent years because it progressively transfers the knowledge in end-to-end training schemes within one network. However, self-distillation methods are susceptible to label noise hence leading to poor generalization performance . To address this problem, this paper proposes a novel self-distillation method, called GEIKD , which combines a gated ensemble self-teacher network and the influences-based label noise removal. Specifically, we design a gated ensemble self-teacher network composed of multiple teacher branches, which allows a gated fused knowledge based on a weighted bi-directional feature pyramid network. Moreover, we introduce influences estimation into the distillation process to quantify the effect of noisy labels on the distillation loss, and then reject the unfavorable instances as noisy labeled samples according to the calculated influences. Our influences-based label noise removal can be integrated with any existing knowledge distillation training schemes. The impact of noisy labels on knowledge distillation can be significantly alleviated by the proposed noisy instances removal with little extra training efforts. Experiments show that the proposed GEIKD method outperforms the state-of-the-art methods on CIFAR-100, TinyimageNet and fine-grained datasets CUB200, MIT-67, Stanford40 and FERC dataset, using clean data and data with noisy labels.},
  archive      = {J_CVIU},
  author       = {Fuchang Liu and Yu Wang and Zheng Li and Zhigeng Pan},
  doi          = {10.1016/j.cviu.2023.103771},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103771},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GEIKD: Self-knowledge distillation based on gated ensemble networks and influences-based label noise removal},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-patch multi-scale model for motion deblurring with
high-frequency information. <em>CVIU</em>, <em>235</em>, 103770. (<a
href="https://doi.org/10.1016/j.cviu.2023.103770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays deep learning-based models have made encouraging accomplishments in the field of image motion deblurring, but this type of algorithms does not effectively use the known prior knowledge and the physical model of motion blur . So as to settle the above disputes, this paper suggests a multi-patch multi-scale input model that fuses high-frequency information, structural self-similarity (SSS), multi-scale channel spatial attention (MSCSA), and superpixel gradient loss for motion deblurring. The multi-patch multi-scale input model can fully extract the salient information, scale information, and high-frequency information of the image to restore the sharp image. The SSS module can take full advantage of the prior information of the structure self-similarity of the image. And the MSCSA module dynamically and selectively enhances relevant features and suppresses irrelevant features by fusing spatial, scale, and channel characteristics. Moreover, this paper advises a superpixel gradient loss function, which can remove motion blur and restore sharp images more effectively. And to increase the diversity of scenes, for the first time, we propose a motion blur dataset (Underwater-Blur) specifically for underwater scenes. Massive comparative experiments conducted on three public datasets of GoPro, RealBlur, RWBI, and the Underwater-Blur dataset prove that our model outperforms the state-of-the-art in terms of comprehensive performance.},
  archive      = {J_CVIU},
  author       = {Nianzu Qiao and Jia Sun and Duo Xu and Changyin Sun},
  doi          = {10.1016/j.cviu.2023.103770},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103770},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-patch multi-scale model for motion deblurring with high-frequency information},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical image peeling: A flexible scale-space filtering
framework. <em>CVIU</em>, <em>235</em>, 103769. (<a
href="https://doi.org/10.1016/j.cviu.2023.103769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of hierarchical image organization has been witnessed by a wide spectrum of applications in computer vision and graphics. Different from image segmentation with the spatial whole-part consideration, this work designs a modern framework for disassembling an image into a family of derived signals from a scale-space perspective. Specifically, we first formulate the goal of the hierarchical image organization problem. Then, by concerning desired properties, such as peeling hierarchy and structure preservation, we convert the original complex problem into a series of two-component separation sub-problems, significantly reducing the complexity. The proposed framework is flexible to be trained on both paired and unpaired data. A compact recurrent network , namely hierarchical image peeling net, is customized to efficiently and effectively fulfill the task, which is about 3.5Mb in size, and can handle 1080p images in more than 60 fps per recurrence on a GTX 2080Ti GPU , making it attractive for practical use. Both theoretical findings and experimental results are provided to demonstrate the efficacy of the proposed framework, reveal its superiority over other state-of-the-art alternatives, and show its potential to various applicable scenarios. The codes have been made publicly available at https://github.com/ForawardStar/HIPe .},
  archive      = {J_CVIU},
  author       = {Yuanbin Fu and Jiayi Ma and Xiaojie Guo},
  doi          = {10.1016/j.cviu.2023.103769},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103769},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hierarchical image peeling: A flexible scale-space filtering framework},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fréchet AutoEncoder distance: A new approach for evaluation
of generative adversarial networks. <em>CVIU</em>, <em>235</em>, 103768.
(<a href="https://doi.org/10.1016/j.cviu.2023.103768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation measures of Generative Adversarial Networks (GANs) have been an active area of research and, currently, there are several measures to evaluate them. The most used GANs evaluation measure is the Fréchet Inception Distance (FID). Measures such as FID are known as model-agnostic methods, where the generator is used as a black box to sample the generated images. Like other measures of model-agnostic, FID uses a deep supervised model for mapping real and generated samples to a feature space. We proposed an approach here with a deep unsupervised model, the Vector Quantised-Variational Autoencoder (VQ-VAE), for estimating the mean and the covariance matrix of the Fréchet Distance and named it Fréchet AutoEncoder Distance (FAED). Our experimental results highlighted that the feature space of the VQ-VAE describes a clustering domain-specific representation more intuitive and visually plausible than the Inception network used by the benchmark FID.},
  archive      = {J_CVIU},
  author       = {Lucas F. Buzuti and Carlos E. Thomaz},
  doi          = {10.1016/j.cviu.2023.103768},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103768},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fréchet AutoEncoder distance: A new approach for evaluation of generative adversarial networks},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient 6-DoF camera pose tracking with circular edges.
<em>CVIU</em>, <em>235</em>, 103767. (<a
href="https://doi.org/10.1016/j.cviu.2023.103767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera pose tracking attracts much interest from both academic and industrial communities, of which the methods based on planar markers are easy to be implemented. However, most existing methods need to identify multiple points in the marker images for matching to space points. Then, PnP methods are used to compute the camera poses. If cameras move fast or are far away from the markers, the matching is easy to generate errors. To address these problems, we design circular markers and represent 6D camera pose analytically as concise forms from each marker by projective invariance. Afterwards, the pose is further optimized by a cost function based on a polar-n-direction geometric distance . The proposed method is from imaged circular edges, which makes camera pose tracking more robust to noise, blur and distance from camera to marker than existing methods. Extensive experimental results show that the proposed 6-DoF camera pose tracking method outperforms state-of-the-art methods in terms of noise, blur, and distance from camera to marker. Simultaneously, it efficiently runs at about 100 FPS on a consumer computer.},
  archive      = {J_CVIU},
  author       = {Fulin Tang and Shaohuan Wu and Zhengda Qian and Yihong Wu},
  doi          = {10.1016/j.cviu.2023.103767},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103767},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient 6-DoF camera pose tracking with circular edges},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory-efficient multi-scale residual dense network for
single image rain removal. <em>CVIU</em>, <em>235</em>, 103766. (<a
href="https://doi.org/10.1016/j.cviu.2023.103766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outdoor photography is greatly affected by bad rainfall, which degrades the quality of captured images and hinders practical application. In response to the above problems, we propose a memory-efficient multi-scale residual dense network (MMRD-Net) for single image rain removal. Firstly, we use a combination or fusion of extended convolution blocks of different sizes to accurately extract multi-scale features of rain streaks and raindrops while retaining rich local detail. Secondly, we add attention modules to the residual block, which performs adaptive operations on rainfall and non-rainfall areas to extract more and more accurate image features and details of rain streaks and raindrops. During this period, we opened up a space for shared use by all the concatenation layers in the network, which reduces the memory consumption of the network. It allows for better aggregation of stormwater features at multiple levels and also allows for faster convergence of the network. Finally, we tested on both synthetic and real datasets to obtain subjective results and objective evaluations. The experimental results show that the performance of our algorithm is good and robust for joint rain removal tasks of different densities. Our algorithm outperforms twelve other different classical algorithms in terms of its ability to remove rain streaks. It can produce visually sharper deraining images and the enhanced visibility of the images is effective for computer vision applications (Google Vision API).},
  archive      = {J_CVIU},
  author       = {Ziyang Zheng and Zhixiang Chen and Shuqi Wang and Wenpeng Wang and Hui Wang},
  doi          = {10.1016/j.cviu.2023.103766},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103766},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Memory-efficient multi-scale residual dense network for single image rain removal},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full-parameter adaptive fuzzy clustering for noise image
segmentation based on non-local and local spatial information.
<em>CVIU</em>, <em>235</em>, 103765. (<a
href="https://doi.org/10.1016/j.cviu.2023.103765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy clustering C-means (FCM) algorithm is a compelling image segmentation method in image segmentation. However, the algorithm is less robust for images containing noise. This paper proposes a non-local full-parameter adaptive spatial information combined with a local fuzzy factor for noisy image segmentation. Firstly, we realize the adaptive calculation of the smoothing parameter, search term window, and neighborhood window in the non-local spatial information by defining the smoothness and designing the adaptive matching function. Secondly, the image’s non-local and local spatial information is considered comprehensively to reduce noise interference and segmentation ambiguity. Finally, the weighted average membership linking is used as the denominator of the objective function to reduce the number of iterations. The results in synthetic noise image and color image segmentation experiments show that the proposed algorithm has outstanding performance in various evaluation metrics and visual effects, outperforming most other variants of fuzzy clustering-based algorithms.},
  archive      = {J_CVIU},
  author       = {Jiaxin Wu and Xiaopeng Wang and Tongyi Wei and Chao Fang},
  doi          = {10.1016/j.cviu.2023.103765},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103765},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Full-parameter adaptive fuzzy clustering for noise image segmentation based on non-local and local spatial information},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MECCANO: A multimodal egocentric dataset for humans behavior
understanding in the industrial-like domain. <em>CVIU</em>,
<em>235</em>, 103764. (<a
href="https://doi.org/10.1016/j.cviu.2023.103764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable cameras allow to acquire images and videos from the user’s perspective. These data can be processed to understand humans behavior. Despite human behavior analysis has been thoroughly investigated in third person vision, it is still understudied in egocentric settings and in particular in industrial scenarios. To encourage research in this field, we present MECCANO, a multimodal dataset of egocentric videos to study humans behavior understanding in industrial-like settings. The multimodality is characterized by the presence of gaze signals, depth maps and RGB videos acquired simultaneously with a custom headset. The dataset has been explicitly labeled for fundamental tasks in the context of human behavior understanding from a first person view, such as recognizing and anticipating human–object interactions. With the MECCANO dataset, we explored six different tasks including (1) Action Recognition, (2) Active Objects Detection and Recognition, (3) Egocentric Human–Objects Interaction Detection, (4) Egocentric Gaze Estimation, (5) Action Anticipation and (6) Next-Active Objects Detection. We propose a benchmark aimed to study human behavior in the considered industrial-like scenario which demonstrates that the investigated tasks and the considered scenario are challenging for state-of-the-art algorithms. To support research in this field, we publicy release the dataset at https://iplab.dmi.unict.it/MECCANO/ .},
  archive      = {J_CVIU},
  author       = {Francesco Ragusa and Antonino Furnari and Giovanni Maria Farinella},
  doi          = {10.1016/j.cviu.2023.103764},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103764},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MECCANO: A multimodal egocentric dataset for humans behavior understanding in the industrial-like domain},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIERRA: A robust bilateral feature upsampler for dense
prediction. <em>CVIU</em>, <em>235</em>, 103762. (<a
href="https://doi.org/10.1016/j.cviu.2023.103762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature upsampling is a fundamental operation in modern deep network architectures . Existing upsamplers, however, are prone to cause negative upsampling—performance of an upsampler falls behind naive interpolation. For instance, the recent dynamic upsampler CARAFE is the best performing operator in semantic segmentation , but it turns out to be the worst one in image matting. In this work, we present robuSt bIlatERal featuRe upsAmpler (SIERRA), a simple, task-robust, plug-and-play, and ultra lightweight upsampler. Its key idea is to use an efficient gradient-prior kernel to modulate a (shifted) distance-prior kernel to control which feature points participate in interpolation, which shares a similar spirit to joint bilateral filtering (JBF). Yet, in contrast to JBF that requires high-res guidance, SIERRA generates kernels from the low-res decoder feature alone. Extensive experiments demonstrate the superiority and robustness of SIERRA on five dense prediction tasks. Code will be available online.},
  archive      = {J_CVIU},
  author       = {Hongtao Fu and Wenze Liu and Yuliang Liu and Zhiguo Cao and Hao Lu},
  doi          = {10.1016/j.cviu.2023.103762},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103762},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SIERRA: A robust bilateral feature upsampler for dense prediction},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based inexact graph matching on top of DNNs for
semantic scene understanding. <em>CVIU</em>, <em>235</em>, 103744. (<a
href="https://doi.org/10.1016/j.cviu.2023.103744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based pipelines for semantic segmentation often ignore structural information available on annotated images used for training. We propose a novel post-processing module enforcing structural knowledge about the objects of interest to improve segmentation results provided by deep neural networks (DNNs). This module corresponds to a “many-to-one-or-none” inexact graph matching approach, and is formulated as a quadratic assignment problem . Our approach is compared to a DNN-based segmentation on two public datasets, one for face segmentation from 2D RGB images (FASSEG), and the other for brain segmentation from 3D MRIs (IBSR). Evaluations are performed using two types of structural information: distances and directional relations that are user defined, this choice being a hyper-parameter of our proposed generic framework. On FASSEG data, results show that our module improves accuracy of the DNN by about 6.3\% i.e. the Hausdorff distance (HD) decreases from 22.11 to 20.71 on average. With IBSR data, the improvement is of 51\% better accuracy with HD decreasing from 11.01 to 5.4. Finally, our approach is shown to be resilient to small training datasets that often limit the performance of deep learning methods: the improvement increases as the size of the training dataset decreases.},
  archive      = {J_CVIU},
  author       = {Jeremy Chopin and Jean-Baptiste Fasquel and Harold Mouchère and Rozenn Dahyot and Isabelle Bloch},
  doi          = {10.1016/j.cviu.2023.103744},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103744},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Model-based inexact graph matching on top of DNNs for semantic scene understanding},
  volume       = {235},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Streaming egocentric action anticipation: An evaluation
scheme and approach. <em>CVIU</em>, <em>234</em>, 103763. (<a
href="https://doi.org/10.1016/j.cviu.2023.103763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Egocentric action anticipation aims to predict the future actions the camera wearer will perform from the observation of the past. While predictions about the future should be available before the predicted events take place, most approaches do not pay attention to the computational time required to make such predictions. As a result, current evaluation schemes assume that predictions are available right after the input video is observed, i.e., presuming a negligible runtime, which may lead to overly optimistic evaluations. We propose a streaming egocentric action evaluation scheme which assumes that predictions are performed online and made available only after the model has processed the current input segment, which depends on its runtime. To evaluate all models considering the same prediction horizon, we hence propose that slower models should base their predictions on temporal segments sampled ahead of time. Based on the observation that model runtime can affect performance in the considered streaming evaluation scenario, we further propose a lightweight action anticipation model based on feed-forward 3D CNNs which is optimized using knowledge distillation techniques with a novel past-to-future distillation loss. Experiments on the three popular datasets EPIC-KITCHENS-55, EPIC-KITCHENS-100 and EGTEA Gaze+ show that (i) the proposed evaluation scheme induces a different ranking on state-of-the-art methods as compared to classic evaluations, (ii) lightweight approaches tend to outmatch more computationally expensive ones, and (iii) the proposed model based on feed-forward 3D CNNs and knowledge distillation outperforms current art in the streaming egocentric action anticipation scenario.},
  archive      = {J_CVIU},
  author       = {Antonino Furnari and Giovanni Maria Farinella},
  doi          = {10.1016/j.cviu.2023.103763},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103763},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Streaming egocentric action anticipation: An evaluation scheme and approach},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cutout with patch-loss augmentation for improving generative
adversarial networks against instability. <em>CVIU</em>, <em>234</em>,
103761. (<a href="https://doi.org/10.1016/j.cviu.2023.103761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks heavily rely on large datasets and carefully chosen model parameters to avoid model overfitting or mode collapse. Cutout with patch-loss augmentation, a dataset augmentation designed for generative adversarial networks that applies cutout to both the discriminator and the generator with a patch-loss structure and a new loss function, is proposed as a solution to the issue. It can enhance the performance of generative adversarial networks on full datasets and promote better convergence and stability on limited datasets. Additionally, the tensor value clamp is proposed, accelerating training speed without compromising quality. The proposed method can be successfully used with various generative adversarial networks, according to experiments. The performance of generative adversarial networks trained with full data on CIFAR-10 is matched by our method with only 20\% of the training data. Finally, combined with our approach, StyleGAN2-ADA’s Fréchet Inception Distance (FID) results on the CIFAR-10, LSUN-CAT, and FFHQ-256 datasets can be further enhanced.},
  archive      = {J_CVIU},
  author       = {Mengchen Shi and Fei Xie and Jiquan Yang and Jing Zhao and Xixiang Liu and Fan Wang},
  doi          = {10.1016/j.cviu.2023.103761},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103761},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cutout with patch-loss augmentation for improving generative adversarial networks against instability},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAMA: Geometric analysis based motion-aware architecture for
moving object segmentation. <em>CVIU</em>, <em>234</em>, 103751. (<a
href="https://doi.org/10.1016/j.cviu.2023.103751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving object segmentation in real-world scenes is of critical significance for many computer vision applications. However, there are many challenges in moving object segmentation. It is difficult to distinguish objects with motion degeneracy. Besides, complex scenes and noisy 2D optical flows also effect the result of moving object segmentation. In this paper, to address difficulties caused by motion degeneracy, we analyze the classic motion degeneracy from a new geometric perspective. To identify objects with motion degeneracy, we propose a reprojection cost and an optical flow contrast cost which are fed into the network to enrich motion features. Furthermore, a novel geometric constraint called bidirectional motion constraint is proposed to detect moving objects with weak motion features. In order to tackle more complex scenes, we also introduce a motion-aware architecture to predict instance masks of moving objects. Extensive experiments are conducted on the KITTI dataset, the JNU-UISEE dataset and the KittiMoSeg dataset, and our proposed method achieves excellent performance.},
  archive      = {J_CVIU},
  author       = {Bangwu Xu and Qin Wu and Zhilei Chai and Xueliang Guo and Jianbo Shi},
  doi          = {10.1016/j.cviu.2023.103751},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103751},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GAMA: Geometric analysis based motion-aware architecture for moving object segmentation},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MAL-net: Multiscale attention link network for accurate eye
center detection. <em>CVIU</em>, <em>234</em>, 103750. (<a
href="https://doi.org/10.1016/j.cviu.2023.103750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, due to wide potential applications like human–computer interactions, eye center detection has received growing research interest. Although numerous approaches have been proposed, achieving high accuracy in the wild remains challenging because of the variations in appearance, shape, and illumination. In this paper, we formulate the eye center detection problem as an approximate segmentation task . We propose a novel method to detect pupil positions from a face image using a deep neural network with synthetic data. In particular, we introduce a deep neural network named Multiscale-Attention-Link Network (MAL-Net), where we design a Link Attention Module (LAM) and a novel Multiscale Link Structure (MLS) for accurate and robust eye center detection. Besides, a weighted loss is proposed to make the deep model pay more attention to eye centers during training. Furthermore, to address the problem of insufficient training data with enough variations in eye shape and illumination, we propose a GAN-based method named shape-GAN to generate synthetic eye images with various shapes for training. The proposed MAL-Net is evaluated on widely-used benchmarks such as TFV, GI4E, and BioID. The results demonstrate that our proposed method outperforms state-of-the-art methods for eye center detection.},
  archive      = {J_CVIU},
  author       = {Chao Gou and Rui Zhong and Yuezhao Yu},
  doi          = {10.1016/j.cviu.2023.103750},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103750},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MAL-net: Multiscale attention link network for accurate eye center detection},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time distributed video analytics for privacy-aware
person search. <em>CVIU</em>, <em>234</em>, 103749. (<a
href="https://doi.org/10.1016/j.cviu.2023.103749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a novel distributed privacy-aware person search (PAPS) model has been proposed which circumvents the privacy risks. An intelligent IoT surveillance system has been designed to integrate the PAPS model for real-time distributed privacy-aware person search from surveillance videos . An important aspect of the intelligent surveillance system, particularly person search, is the visual feedback at the output, with ranked results of person images at the user-end. Therefore, even if edge processing is performed, there is still a need to store and transmit the cropped person images to the cloud server for displaying the results at the user-end. However, storing or transmission of videos/images to cloud-servers leads to privacy issues. The proposed PAPS model eliminates the need to store or transmit the images/videos while performing person search, thereby addressing the privacy concerns. The proposed system is easily scalable to incorporate more camera nodes to enhance the surveillance coverage as majority of the processing is performed at the edge servers, with a small amount of fog-processing. A very minimal amount of cloud-processing is performed only when a query is raised at the user-end. Only the processed and encoded data is transmitted across the edge, fog and the cloud servers, which protects privacy and significantly reduces bandwidth costs. Further, a new evaluation criterion, Person Capacity, has been proposed to evaluate the feasibility of an edge-based system to be deployed at crowded locations. The performance evaluation of our system, on our own video dataset, as well as the PRW, and CUHK-SYSU dataset for person search demonstrates that the proposed system achieves state-of-the-art or competitive performance while performing in real-time for practical scenarios.},
  archive      = {J_CVIU},
  author       = {Bipin Gaikwad and Abhijit Karmakar},
  doi          = {10.1016/j.cviu.2023.103749},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103749},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Real-time distributed video analytics for privacy-aware person search},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Α-EGAN: Α-energy distance GAN with an early stopping rule.
<em>CVIU</em>, <em>234</em>, 103748. (<a
href="https://doi.org/10.1016/j.cviu.2023.103748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) are popular tools for learning the distribution of real samples and generating new ones, where the qualities of the generated images and the degree of preserved variation are two major concerns currently. In view of this, we propose an α α -EGAN with energy distance as the loss function of the generator, which is proven to be effective in mitigating mode collapse. Moreover, an early stopping rule is proposed in the frame of hypothesis testing to avoid the unhealthy competition between the generator and the discriminator , thus achieving a trade-off between image qualities and variations. As a byproduct, the energy distance under the Euclidean norm can serve as a novel metric for evaluating generated samples in GAN. Experiments are conducted on simulated manifold datasets, as well as real MNIST and CelebA face datasets, showing that the proposed α α -EGAN outperforms several competitors in both training stability and image quality.},
  archive      = {J_CVIU},
  author       = {Fangting Ji and Xin Zhang and Junlong Zhao},
  doi          = {10.1016/j.cviu.2023.103748},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103748},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {α-EGAN: α-energy distance GAN with an early stopping rule},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Casting a BAIT for offline and online source-free domain
adaptation. <em>CVIU</em>, <em>234</em>, 103747. (<a
href="https://doi.org/10.1016/j.cviu.2023.103747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the source-free domain adaptation (SFDA) problem, where only the source model is available during adaptation to the target domain. We consider two settings: the offline setting where all target data can be visited multiple times (epochs) to arrive at a prediction for each target sample, and the online setting where the target data needs to be directly classified upon arrival. Inspired by diverse classifier based domain adaptation methods, in this paper we introduce a second classifier, but with another classifier head fixed. When adapting to the target domain, the additional classifier initialized from source classifier is expected to find misclassified features. Next, when updating the feature extractor, those features will be pushed towards the right side of the source decision boundary, thus achieving source-free domain adaptation. Experimental results show that the proposed method achieves competitive results for offline SFDA on several benchmark datasets compared with existing DA and SFDA methods, and our method surpasses by a large margin other SFDA methods under online source-free domain adaptation setting.},
  archive      = {J_CVIU},
  author       = {Shiqi Yang and Yaxing Wang and Luis Herranz and Shangling Jui and Joost van de Weijer},
  doi          = {10.1016/j.cviu.2023.103747},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103747},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Casting a BAIT for offline and online source-free domain adaptation},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). POEM: A prototype cross and emphasis network for few-shot
semantic segmentation. <em>CVIU</em>, <em>234</em>, 103746. (<a
href="https://doi.org/10.1016/j.cviu.2023.103746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle the demand for large labeled datasets and poor unseen class generalization for segmentation tasks , few-shot segmentation (FSS) has received widespread attention, which utilizes prototypical learning to achieve significant progress. However, the prototypes generated by averaging the global object information still suffer from the appearance diversity within the target class. Motivated by the fact that humans only need part attributes rather than all attributions to recognize objects from a particular class, we propose an effective prototype cross and emphasis segmentation network (POEM) to provide more precise guidance in the feature matching process. Firstly, we present a regional crossed referencing module to improve the prototypical learning strategy, which mines the relevant attributes of the target objects in both support and query images and customizes the prototypes for the query images. Secondly, we develop a foreground-emphasized reasoning module to reduce the negative influence caused by complex background clutter, in which the foreground and background attributions are regarded as fundamental information and auxiliary information, respectively. Extensive experiments on two standard datasets have demonstrated the superiority of POEM over the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Xu Cheng and Haoyuan Li and Shuya Deng and Yonghong Peng},
  doi          = {10.1016/j.cviu.2023.103746},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103746},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {POEM: A prototype cross and emphasis network for few-shot semantic segmentation},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting image translations via ensemble self-supervised
learning for unsupervised domain adaptation. <em>CVIU</em>,
<em>234</em>, 103745. (<a
href="https://doi.org/10.1016/j.cviu.2023.103745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation (UDA) aims to improve the generalization capacity of models when they are tested on a real-world target domain by learning a model on a source labeled domain. Recently, a UDA method was proposed that addresses the adaptation problem by combining ensemble learning with self-supervised learning. However, this method uses only the source domain to pretrain the model and employs a limited amount of classifiers to create target pseudo labels. To mitigate these deficiencies, in this work, we explore the usage of image translations in combination with ensemble learning and self-supervised learning. To increase the model’s exposure to more variable pretraining data, our method creates multiple diverse image translations, which encourages the learning of domain-invariant features, desired to increase generalization. With these image translations, we are able to learn translation-specific classifiers, which also allows to maximize the amount of ensemble’s classifiers resulting in more robust target pseudo labels. In addition, we propose to use the target domain in pretraining stage to mitigate source domain bias in the network. We evaluate our method on the standard UDA benchmarks, i.e., adapting GTA V and Synthia to Cityscapes, and achieve state-of-the-art results on the mIoU metric. Extensive ablation experiments are reported to highlight the advantageous properties of our UDA strategy.},
  archive      = {J_CVIU},
  author       = {Fabrizio J. Piva and Gijs Dubbelman},
  doi          = {10.1016/j.cviu.2023.103745},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103745},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Exploiting image translations via ensemble self-supervised learning for unsupervised domain adaptation},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised domain adaptation for semantic segmentation via
cross-region alignment. <em>CVIU</em>, <em>234</em>, 103743. (<a
href="https://doi.org/10.1016/j.cviu.2023.103743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation requires a lot of training data, which necessitates costly annotation. There have been many studies on unsupervised domain adaptation (UDA) from one domain to another, e.g., from computer graphics to real images. However, there is still a gap in accuracy between UDA and supervised training on native domain data. It is arguably attributable to the class-level misalignment between the source and target domain data . To cope with this, we propose a method that applies adversarial training to align two feature distributions in the target domain . It uses a self-training framework to split the image into two regions (i.e., trusted and untrusted), which form two distributions to align in the feature space. We term this approach cross-region adaptation (CRA) to distinguish it from the previous methods of aligning different domain distributions, which we call cross-domain adaptation (CDA). CRA can be applied after any CDA method. Experimental results show that this always improves the accuracy of the combined CDA method.},
  archive      = {J_CVIU},
  author       = {Zhijie Wang and Xing Liu and Masanori Suganuma and Takayuki Okatani},
  doi          = {10.1016/j.cviu.2023.103743},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103743},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised domain adaptation for semantic segmentation via cross-region alignment},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep continual hashing for real-world multi-label image
retrieval. <em>CVIU</em>, <em>234</em>, 103742. (<a
href="https://doi.org/10.1016/j.cviu.2023.103742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Hashing has achieved great success in large-scale image retrieval due to binary code’s storage and computation efficiency. However, its learning paradigm under real-world environments is less studied, and most existing approaches are developed in the closed scenario, e.g., simple and unchanging semantics. When images of new classes emerge, they have to retrain the model on all history training datasets, but the constant data uploading makes this impractical. This paper proposes a novel method, called continual deep semantic hashing (CDSH), for learning binary codes of multi-label images with increasing classes. The CDSH consists of two hashing networks. One learns to hash the increasing semantics of data, i.e., label, into the semantic codes and accumulates label-code pairs as long-term knowledge, incorporating empirically verified loss and designed special regularization to ensure encoding old labels unchanged. The other learns to map images to the corresponding semantic code from a probabilistic view and solid knowledge via retaining history exemplar samples and projecting model gradients. We theoretically prove this improves the probability of old data’s code unchanged after the model is updated. Extensive experiments on four widely used image datasets demonstrate that the CDSH method can continually learn hash functions and yield state-of-the-art retrieval performance .},
  archive      = {J_CVIU},
  author       = {Ge Song and Kai Huang and Hanwen Su and Fengyi Song and Ming Yang},
  doi          = {10.1016/j.cviu.2023.103742},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103742},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep continual hashing for real-world multi-label image retrieval},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention guided domain alignment for conditional face image
generation. <em>CVIU</em>, <em>234</em>, 103740. (<a
href="https://doi.org/10.1016/j.cviu.2023.103740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great success of Generative Adversarial Network (GAN) in face image generation , it is still a challenge to generate a faithful yet high-fidelity face image given an exemplar image and a conditional input from a distinct domain ( e.g. , a semantic segmentation mask or an edge map). Existing methods learn image-level features to align distinct domains in an intermediate domain, which ignore the spatial relationship of the facial semantic parts and therefore lead to semantic mismatching problems. In addition, it is computationally expensive to establish dense correspondences in the intermediate domain, especially for high-resolution face images. To address these problems, we propose a novel attention guided domain alignment method for conditional face image generation, which aligns two domains directly under the guidance of the local attention learned from semantically similar face parts. In particular, we assign a dedicated index for each feature block and adopt a top-k ranking operation to correspond block-wise features in two distinct domains, which exploits the spatial relationship of facial parts and preserves the texture structure during alignment. The local attention is then learned from the retrieved blocks, which reduces computation complexity substantially and therefore enables to build alignment in a high resolution. The aligned features are finally fused with adaptive weights learned from their long-range correlation coefficients, which capture the semantic coherence of the style features between the two domains. Extensive experimental results on the CelebAMask-HQ dataset demonstrate that the proposed method is superior to the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Zonglin Li and Shengping Zhang and Zhaoxin Zhang and Quanling Meng and Qinglin Liu and Huiyu Zhou},
  doi          = {10.1016/j.cviu.2023.103740},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103740},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Attention guided domain alignment for conditional face image generation},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised real image super-resolution via knowledge
distillation network. <em>CVIU</em>, <em>234</em>, 103736. (<a
href="https://doi.org/10.1016/j.cviu.2023.103736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution convolutional neural networks recently have demonstrated high-quality restoration for single images. Despite existing methods have achieved remarkable performance based on synthetic datasets , the performance is poor on real-world or natural data. To address this issue, zero-shot super-resolution (ZSSR) has been proposed for adaptive learning. However, ZSSR is unable to keep the simulated degradation process consistent with the degradation kernel of the real degradation process. Furthermore, the learned mapping of ZSSR is different from the desired mapping. In this paper, an unsupervised image super-resolution via knowledge distillation network (USRKDN) is proposed. Specifically, the proposed degradation module generates an image-specific degradation kernel and corresponding degenerated images. Moreover, the knowledge distillation module is proposed to solve the issue that the mapping cannot be completely equivalent, which transfers the learned map by knowledge distillation. The full convolution module is also explored to help the reconstruction of information. Extensive experimental results on synthetic and real datasets demonstrate the effectiveness of USRKDN. In addition, USRKDN is proven to be good at reconstructing image details in real scenes, which provides an effective method for generating information learning tasks with fewer samples.},
  archive      = {J_CVIU},
  author       = {Nianzeng Yuan and Bangyong Sun and Xiangtao Zheng},
  doi          = {10.1016/j.cviu.2023.103736},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103736},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised real image super-resolution via knowledge distillation network},
  volume       = {234},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human–object interaction prediction in videos through gaze
following. <em>CVIU</em>, <em>233</em>, 103741. (<a
href="https://doi.org/10.1016/j.cviu.2023.103741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the human–object interactions (HOIs) from a video is essential to fully comprehend a visual scene. This line of research has been addressed by detecting HOIs from images and lately from videos. However, the video-based HOI anticipation task in the third-person view remains understudied. In this paper, we design a framework to detect current HOIs and anticipate future HOIs in videos. We propose to leverage human gaze information since people often fixate on an object before interacting with it. These gaze features together with the scene contexts and the visual appearances of human–object pairs are fused through a spatio-temporal transformer. To evaluate the model in the HOI anticipation task in a multi-person scenario, we propose a set of person-wise multi-label metrics. Our model is trained and validated on the VidHOI dataset, which contains videos capturing daily life and is currently the largest video HOI dataset. Experimental results in the HOI detection task show that our approach improves the baseline by a great margin of 36.3\% relatively. Moreover, we conduct an extensive ablation study to demonstrate the effectiveness of our modifications and extensions to the spatio-temporal transformer. Our code is publicly available on .},
  archive      = {J_CVIU},
  author       = {Zhifan Ni and Esteve Valls Mascaró and Hyemin Ahn and Dongheui Lee},
  doi          = {10.1016/j.cviu.2023.103741},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103741},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Human–object interaction prediction in videos through gaze following},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human skeletons and change detection for efficient violence
detection in surveillance videos. <em>CVIU</em>, <em>233</em>, 103739.
(<a href="https://doi.org/10.1016/j.cviu.2023.103739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In our constantly monitored world, surveillance cameras play a crucial role in curbing crime and violence in public spaces by serving as a deterrent. To enhance their effectiveness, there is a growing need for automated tools that can detect crimes in real time. In this paper, we propose a novel deep learning architecture that accurately and efficiently detects violent crimes in surveillance videos. We rely on what we believe are the most essential pieces of information to detect violence, namely: human bodies and their interaction. To this end, we employ human pose extractors and change detectors as the input of our proposal. Subsequently, we combine them using a novel method, which relies on additions instead of multiplications to guarantee the transmission of information even when one of the inputs provides a zero-valued signal; outperforming other combination alternatives of the literature. Finally, to account for both spatial and temporal information, we use a convolutional alternative of the standard LSTM, the ConvLSTM. The experiments performed on several benchmark datasets demonstrate the efficacy and efficiency of our proposal, achieving state-of-the-art results with much fewer trainable parameters. We release the code to replicate the proposed architecture at https://github.com/atmguille/Violence-Detection-With-Human-Skeletons .},
  archive      = {J_CVIU},
  author       = {Guillermo Garcia-Cobo and Juan C. SanMiguel},
  doi          = {10.1016/j.cviu.2023.103739},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103739},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Human skeletons and change detection for efficient violence detection in surveillance videos},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing lower half facial gestures for lip reading
applications: Survey on vision techniques. <em>CVIU</em>, <em>233</em>,
103738. (<a href="https://doi.org/10.1016/j.cviu.2023.103738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lip reading has gained popularity due to the proliferation of emerging real-world applications. This article provides a comprehensive review of benchmark datasets available for lip-reading applications and pioneering works that analyze lower facial cues for lip-reading applications. A comprehensive review of lip reading applications is broadly classified into five distinct applications: Lip Reading Biometrics (LRB), Audio Visual Speech Recognition (AVSR), Silent Speech Recognition (SSR), Voice from Lips, and Lip HCI (Human–computer interaction). LRB entails extensive research in the fields of authentication and liveness detection. AVSR covers key findings that have contributed significantly to applications such as voice assistants, video-to-text transcription, hearing aids, and pronunciation-correcting systems. SSR analyzes the efforts made for silent-video-to-text transcription and surveillance camera applications. The voice from lips section discusses applications such as voice for the voiceless and vision-infused speech inpainting. In lip HCI, LR-HCI for smartphones, smart TVs, computers, robots, and musical instruments is reviewed in detail. Comprehensive coverage is given to cutting-edge techniques in computer vision , signal processing, machine learning , and deep learning . The advancements that aid the system in learning to lip-read and authenticate lip gestures, generate text transcription, synthesize voice based on lip movements, and control systems via lip movements (lip HCI) are covered. The work concludes by highlighting the limitations of existing frameworks, the road maps of each application illustrating the evolution of techniques employed over time, and future research avenues in lip-reading applications.},
  archive      = {J_CVIU},
  author       = {Preethi S.J. and Niranjana Krupa B.},
  doi          = {10.1016/j.cviu.2023.103738},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103738},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Analyzing lower half facial gestures for lip reading applications: Survey on vision techniques},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Cross-domain few-shot action recognition with unlabeled
videos. <em>CVIU</em>, <em>233</em>, 103737. (<a
href="https://doi.org/10.1016/j.cviu.2023.103737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current few-shot action recognition approaches have achieved impressive performance using only a few labeled examples. However, they usually assume the base (train) and target (test) videos typically come from the same domain, which may limit their further applications. In this paper, we introduce a new practical task, termed as cross-domain few-shot action recognition, and hypothesize there is a domain shift between the base and target videos and the unlabeled target videos are available. To address this task, we further propose a S elf-supervised learning E nhanced t E mporal N etwork (SEEN), which incorporates temporal modeling and self-supervised learning techniques to learn more transferable representations. Concretely, the temporal modeling mechanism aims to learn long-range temporal semantics from the features output by the backbone, and the self-supervised learning focuses on exploring the underlying data patterns to reduce domain shifts under the few-shot setting, which can help to improve the generalization ability . Therefore, the proposed SEEN can capture broader variations of the feature distributions and is more appropriate for the cross-domain few-shot action recognition task. Extensive experiments on multiple cross-domain benchmarks show that our SEEN consistently outperforms several strong baseline methods by a convincing margin.},
  archive      = {J_CVIU},
  author       = {Xiang Wang and Shiwei Zhang and Zhiwu Qing and Yiliang Lv and Changxin Gao and Nong Sang},
  doi          = {10.1016/j.cviu.2023.103737},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103737},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cross-domain few-shot action recognition with unlabeled videos},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the planarity and sharpness of monocularly
estimated depth images using the phong reflection model. <em>CVIU</em>,
<em>233</em>, 103726. (<a
href="https://doi.org/10.1016/j.cviu.2023.103726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation based on a single RGB image (a.k.a. monocular depth estimation) is a challenging task, with applications in robotics, autonomous vehicles, and other areas. With the advance of deep learning , several monocular depth estimation approaches have emerged with remarkable results. However, it is still possible to observe deficiencies in the depth maps generated by current techniques. We present a deep neural network that takes the result produced by some existing monocular depth estimation approach and enhances it by adding the details that the depth map requires to be sharp. We train the proposed Depth Enhancer Neural Network (DENN) using a new loss function that compares the input color image of the scene to the color image produced by rendering it using the Phong reflection model. Our experiments show a clear visual improvement in the sharpness of the depth image produced by the DENN, leading to edge enhancement and regularity of planar surfaces without compromising non-planar objects. On average, the standard metrics show a 5\% reduction in error when considering the quantitative results. This reduction is dependent on the initial monocular depth estimation technique used by DENN to train the model.},
  archive      = {J_CVIU},
  author       = {Roger Ripas and Leandro A.F. Fernandes},
  doi          = {10.1016/j.cviu.2023.103726},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103726},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improving the planarity and sharpness of monocularly estimated depth images using the phong reflection model},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local to non-local: Multi-scale progressive attention
network for image restoration. <em>CVIU</em>, <em>233</em>, 103725. (<a
href="https://doi.org/10.1016/j.cviu.2023.103725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration (IR) tasks aim to form a balance between complex textures and spatial details . To this end, the combination of local and non-local attention mechanisms has been well studied in recent years. However, existing local attention-based modules ignore the interaction between channel and spatial attention , while non-local attention operations solely focus on short-range or long-range dependency. To overcome these problems, a novel multi-scale progressive attention network is proposed in this paper, which is termed as MPANet. The proposed MPANet is composed of two parts, a local multi-scale feature extractor and a window-dilation self-attention module. In the local multi-scale feature extractor, a single-scale feature enhancement strategy is designed to model the correlation between channel and spatial dimensions, and a multi-scale feature fusion strategy is applied to further exchange contextual information across all the scales. Furthermore, the window-dilation self-attention is introduced to establish global representation while preserving local details. Experimental results on four IR tasks demonstrate that the proposed MPANet outperforms the state-of-the-art methods in both quantitative results and visual perception.},
  archive      = {J_CVIU},
  author       = {Lili Shen and Bo Zhao and Qunxia Li and Chuhe Zhang and Xichun Sun and Bo Peng},
  doi          = {10.1016/j.cviu.2023.103725},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103725},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Local to non-local: Multi-scale progressive attention network for image restoration},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EFSCNN: Encoded feature sphere convolution neural network
for fast non-rigid 3D models classification and retrieval.
<em>CVIU</em>, <em>233</em>, 103724. (<a
href="https://doi.org/10.1016/j.cviu.2023.103724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods of classification and retrieval for non-rigid 3D models, such as topological structure-based methods and spectral analysis-based methods, high rely on manual interventions, which leads to the fail of generalization. Recently, methods based on deep learning have been proposed for rigid 3D models, but they are usually not suitable for non-rigid 3D models. Due to excessive redundant calculations, point clouds-based methods and multi-view-based methods are also inefficient. In this paper, a novel deep learning framework for non-rigid 3D model classifications is presented. Different from other point clouds-based methods and multi-view-based method, this framework utilize a 3D sphere view and a new convolution way. First, an encoded feature sphere (EFS) is constructed to express the non-rigid 3D model through feature encoding, spherical projection, and crucial points extraction. Then, loop convolution and longitudinal–latitudinal loop convolution are combined into EFSCNN to extract the discriminative feature descriptor of the non-rigid 3D model. EFSCNN is a hierarchical network based on multi-direction convolution, which expands the receptive field and enhances the ability to capture local features . With the single view input EFS, EFSCNN achieves competitive performance on major non-rigid model classification and retrieval benchmarks with significantly faster runtime than previous studies.},
  archive      = {J_CVIU},
  author       = {Yan Zhou and Zhaolong Dang and Huaidong Zhang and Xuemiao Xu and Jing Qin and Wenjun Li and Fanzhi Zeng and Xiangyu Liu},
  doi          = {10.1016/j.cviu.2023.103724},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103724},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {EFSCNN: Encoded feature sphere convolution neural network for fast non-rigid 3D models classification and retrieval},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully synthetic training for image restoration tasks.
<em>CVIU</em>, <em>233</em>, 103723. (<a
href="https://doi.org/10.1016/j.cviu.2023.103723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we show that neural networks aimed at solving various image restoration tasks can be successfully trained on fully synthetic data. In order to do so, we rely on a generative model of images, the scaling dead leaves model, which is obtained by superimposing disks whose size distribution is scale-invariant. Pairs of clean and corrupted synthetic images can then be obtained by a careful simulation of the degradation process . We show on various restoration tasks that such a synthetic training yields results that are only slightly inferior to those obtained when the training is performed on large natural image databases. This implies that, for restoration tasks, the geometric contents of natural images can be nailed down to only a simple generative model and a few parameters. This prior can then be used to train neural networks for specific modality, without having to rely on demanding campaigns of natural images acquisition. We demonstrate the feasibility of this approach on difficult restoration tasks, including the denoising of smartphone RAW images and the full development of low-light images.},
  archive      = {J_CVIU},
  author       = {Raphaël Achddou and Yann Gousseau and Saïd Ladjal},
  doi          = {10.1016/j.cviu.2023.103723},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103723},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fully synthetic training for image restoration tasks},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Action capsules: Human skeleton action recognition.
<em>CVIU</em>, <em>233</em>, 103722. (<a
href="https://doi.org/10.1016/j.cviu.2023.103722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the compact and rich high-level representations offered, skeleton-based human action recognition has recently gained more attraction. Although joint relationships investigation in spatial and temporal dimensions provides effective information critical to action recognition, effectively encoding global dependencies of joints during spatio-temporal feature extraction is a prohibitive task. In this paper, we introduce Action Capsule which identifies action-related key joints by considering the latent correlation of joints in a skeleton sequence. We show that, during inference, our end-to-end network pays attention to a set of joints specific to each action, whose encoded spatio-temporal features are aggregated to recognize the action. Additionally, the use of multiple stages of action capsules enhances the ability of the network to classify similar actions. A comparative analysis of our capsule-based approach with other widely-used methods in skeleton action recognition is given, highlighting the advantages of the proposed approach in handling missing skeleton data by leveraging iterative processing. Consequently, our network outperforms the state-of-the-art approaches on the N-UCLA dataset and obtains competitive results on the NTURGBD dataset. This is while our approach has significantly lower computational requirements based on GFLOPs measurements.},
  archive      = {J_CVIU},
  author       = {Ali Farajzadeh Bavil and Hamed Damirchi and Hamid D. Taghirad},
  doi          = {10.1016/j.cviu.2023.103722},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103722},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Action capsules: Human skeleton action recognition},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer-based image generation from scene graphs.
<em>CVIU</em>, <em>233</em>, 103721. (<a
href="https://doi.org/10.1016/j.cviu.2023.103721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-structured scene descriptions can be efficiently used in generative models to control the composition of the generated image. Previous approaches are based on the combination of graph convolutional networks and adversarial methods for layout prediction and image generation , respectively. In this work, we show how employing multi-head attention to encode the graph information , as well as using a transformer-based model in the latent space for image generation can improve the quality of the sampled data, without the need to employ adversarial models with the subsequent advantage in terms of training stability. The proposed approach, specifically, is entirely based on transformer architectures both for encoding scene graphs into intermediate object layouts and for decoding these layouts into images, passing through a lower dimensional space learned by a vector-quantized variational autoencoder . Our approach shows an improved image quality with respect to state-of-the-art methods as well as a higher degree of diversity among multiple generations from the same scene graph. We evaluate our approach on three public datasets: Visual Genome, COCO, and CLEVR. We achieve an Inception Score of 13.7 and 12.8, and an FID of 52.3 and 60.3, on COCO and Visual Genome, respectively. We perform ablation studies on our contributions to assess the impact of each component. Code is available at https://github.com/perceivelab/trf-sg2im .},
  archive      = {J_CVIU},
  author       = {Renato Sortino and Simone Palazzo and Francesco Rundo and Concetto Spampinato},
  doi          = {10.1016/j.cviu.2023.103721},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103721},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Transformer-based image generation from scene graphs},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-knowledge distillation via dropout. <em>CVIU</em>,
<em>233</em>, 103720. (<a
href="https://doi.org/10.1016/j.cviu.2023.103720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To boost performance, deep neural networks require deeper or wider network structures that involve massive computational and memory costs. To alleviate this issue, the self-knowledge distillation method regularizes the model by distilling the internal knowledge of the model itself. Conventional self-knowledge distillation methods require additional trainable parameters or are dependent on the data. In this paper, we propose a simple and effective self-knowledge distillation method using a dropout ( SD-Dropout ). SD-Dropout distills the posterior distributions of multiple models through a dropout sampling. Our method does not require any additional trainable modules, does not rely on data, and requires only simple operations. Furthermore, this simple method can be easily combined with various self-knowledge distillation approaches. We provide a theoretical and experimental analysis of the effect of forward and reverse KL-divergences in our work. Extensive experiments on various vision tasks, i.e., image classification , object detection, and distribution shift, demonstrate that the proposed method can effectively improve the generalization of a single network. Further experiments show that the proposed method also improves calibration performance, adversarial robustness, and out-of-distribution detection ability.},
  archive      = {J_CVIU},
  author       = {Hyoje Lee and Yeachan Park and Hyun Seo and Myungjoo Kang},
  doi          = {10.1016/j.cviu.2023.103720},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103720},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-knowledge distillation via dropout},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-induced semantic and boundary interaction network
for camouflaged object detection. <em>CVIU</em>, <em>233</em>, 103719.
(<a href="https://doi.org/10.1016/j.cviu.2023.103719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high intrinsic similarity between camouflaged objects and the background makes camouflaged object detection (COD) more challenging than traditional salient object detection (SOD). Existing deep-learning methods often fall into the following shortcomings: (1) When dealing with high-level features, it is difficult to fully and accurately extract semantic information, which is crucial for locating camouflaged targets. (2) The camouflaged targets identified by existing methods in complex scenes have rough boundaries and incomplete spatial information. (3) Existing methods cannot effectively exert the uniqueness and complementarity of multiple features when integrating multiple features. To this end, we propose an attention-induced semantic and boundary interaction network for accurately identifying camouflaged objects. Specifically, we propose a contrastive positioning module (CPM), which adopts a contrastive learning way to separate the camouflaged object from the background, thereby obtaining the exact location of the camouflaged target and retaining more semantic information. Then, we design a boundary exploration module (BEM), which can reduce background noise interference, focus on the structural details of camouflaged targets, and explore rich fine-grained spatial information. Finally, we propose an attention-induced interaction module (AIM) to fuse multivariate information (i.e., semantic location information, boundary information, and the side output information of the backbone), and multivariate information is complementarily fused under attention guidance to generate more powerful camouflaged target features. Extensive experiments show that the proposed method on the different backbones (i.e., ResNet-50, Res2Net-50, PVTv2-B2, EfficientNet-B1) produces state-of-the-art results on several camouflaged benchmarks. The code is available https://github.com/zhangqiao970914/ASBI .},
  archive      = {J_CVIU},
  author       = {Qiao Zhang and Xiaoxiao Sun and Yurui Chen and Yanliang Ge and Hongbo Bi},
  doi          = {10.1016/j.cviu.2023.103719},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103719},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Attention-induced semantic and boundary interaction network for camouflaged object detection},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based blind image super-resolution with
iterative kernel reconstruction and noise estimation. <em>CVIU</em>,
<em>233</em>, 103718. (<a
href="https://doi.org/10.1016/j.cviu.2023.103718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind single image super-resolution (SISR) is a challenging task in image processing due to the ill-posed nature of the inverse problem . Complex degradations present in real life images make it difficult to solve this problem using naïve deep learning approaches, where models are often trained on synthetically generated image pairs. Most of the effort so far has been focused on solving the inverse problem under some constraints, such as for a limited space of blur kernels and/or assuming noise-free input images. Yet, there is a gap in the literature to provide a well-generalized deep learning-based solution that performs well on images with unknown and highly complex degradations. In this paper, we propose IKR-Net (Iterative Kernel Reconstruction Network) for blind SISR. In the proposed approach, kernel and noise estimation and high-resolution image reconstruction are carried out iteratively using dedicated deep models. The iterative refinement provides significant improvement in both the reconstructed image and the estimated blur kernel even for noisy inputs. IKR-Net provides a generalized solution that can handle any type of blur and level of noise in the input low-resolution image. IKR-Net achieves state-of-the-art results in blind SISR, especially for noisy images with motion blur .},
  archive      = {J_CVIU},
  author       = {Hasan F. Ates and Suleyman Yildirim and Bahadir K. Gunturk},
  doi          = {10.1016/j.cviu.2023.103718},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103718},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep learning-based blind image super-resolution with iterative kernel reconstruction and noise estimation},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust attention ranking architecture with frequency-domain
transform to defend against adversarial samples. <em>CVIU</em>,
<em>233</em>, 103717. (<a
href="https://doi.org/10.1016/j.cviu.2023.103717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have powerful capabilities, but they are vulnerable to adversarial attacks . For image classification tasks, if a small disturbance is introduced into the input image, the model is likely to be misled and causes misclassification . In this paper, we develop a robust attention ranking architecture with frequency-domain transform to defend against adversarial samples, which is called RARFTA. We import the discrete cosine transform as the activation layer after the first convolutional layer , which effectively suppresses the attack based on the gradient method . To eliminate the interference of residual adversarial noise, we dynamically select key points from the feature map for classification with the attention mechanism to reduce the impact of other attacked pixels. Experimental results on different datasets show that our method is superior to the existing defense methods in both black-box and white-box attacks and significantly improves the robustness of the deep neural network model. The code for our work is available at https://github.com/lixiaowenaaa/RARTFA/tree/master .},
  archive      = {J_CVIU},
  author       = {Wen Li and Hengyou Wang and Lianzhi Huo and Qiang He and Changlun Zhang},
  doi          = {10.1016/j.cviu.2023.103717},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103717},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Robust attention ranking architecture with frequency-domain transform to defend against adversarial samples},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight feature point detection network with channel
enhancement. <em>CVIU</em>, <em>233</em>, 103716. (<a
href="https://doi.org/10.1016/j.cviu.2023.103716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based local feature point extraction work has made great research progress in related computer vision tasks. However, it is difficult to balance the detection speed and detection efficiency of general deep neural networks , which are affected by illumination and viewpoint changes in outdoor environments. To this end, a lightweight image feature point efficient detection network jointly trained by detector–descriptor is proposed. The shared encoding of the feature map is carried out through the multi-branch structure, and a shuffle block model is designed to enhance the channel features. The enhanced feature is divided into three branches to further encode the feature map. One branch improves the network’s ability to identify feature information through a lightweight attention mechanism , and the other two branches perform feature aggregation on the detector and the descriptor through skip connection layers to enrich image features. Considering the problem of network operation efficiency, the multi-branch coding structure is coupled into a set of parameters corresponding to the plane forward propagation structure through the structure re-parameterization technology, which can reduce the complexity of the network and improve the inference speed of the network. In addition, the correct convergence of the network is guaranteed by constraints on the classification loss and position loss, and the sparsely sampled descriptors are used for training to reduce the computational cost. Experimental results on the Hpatches and KITTI datasets show that the proposed method reduces the parameters and GFLOPs by 10.7\% and 61.6\%, respectively, compared with recent lightweight work, while maintaining comparable accuracy. Moreover, the proposed method achieves a detection speed of 55FPS on images with a resolution of 480×640. https://github.com/SpiritAshes/Channel-Enhancement .},
  archive      = {J_CVIU},
  author       = {Zhaoyang Li and Xue Zhao and Chun Bao and Jie Cao and Dongxing Li and Qun Hao},
  doi          = {10.1016/j.cviu.2023.103716},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103716},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lightweight feature point detection network with channel enhancement},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PoseGU: 3D human pose estimation with novel human pose
generator and unbiased learning. <em>CVIU</em>, <em>233</em>, 103715.
(<a href="https://doi.org/10.1016/j.cviu.2023.103715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D pose estimation has recently gained substantial interests in computer vision domain. Existing 3D pose estimation methods have a strong reliance on large size well-annotated 3D pose datasets, and they suffer poor model generalization on unseen poses due to limited diversity of 3D poses in training sets. In this work, we propose PoseGU , a novel human pose generator that generates diverse poses with access only to a small size of seed samples, while equipping the Counterfactual Risk Minimization to pursue an unbiased evaluation objective. Extensive experiments demonstrate PoseGU outperforms almost all the state-of-the-art 3D human pose methods under consideration over three popular benchmark datasets. Empirical analysis also proves PoseGU generates 3D poses with improved data diversity and better generalization ability .},
  archive      = {J_CVIU},
  author       = {Shannan Guan and Haiyan Lu and Linchao Zhu and Gengfa Fang},
  doi          = {10.1016/j.cviu.2023.103715},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103715},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PoseGU: 3D human pose estimation with novel human pose generator and unbiased learning},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the inductive biases of deep domain adaptation.
<em>CVIU</em>, <em>233</em>, 103714. (<a
href="https://doi.org/10.1016/j.cviu.2023.103714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even if it is not always stated explicitly, the majority of recent approaches to domain adaptation are based on theoretically unjustified assumptions, on the one hand, and (often hidden) inductive biases on the other hand. This paper highlights that achieving feature alignment, which is commonly assumed to minimize theoretical upper bounds on risk in the target domain, does not guarantee low target risk. Furthermore, through a series of experiments, this paper reveals that deep domain adaptation methods heavily rely on hidden inductive biases present in common practices, including model pretraining and encoder architecture design. In a third step, the paper argues that using handcrafted priors might not be sufficient to bridge distant domains: powerful parametric priors can be instead learned from data, leading to a large improvement in target accuracy. This paper proposes a meta-learning strategy for discovering inductive biases that effectively solve specific domain transfers. It outperforms handcrafted priors on several image classification tasks .},
  archive      = {J_CVIU},
  author       = {Rodrigue Siry and Louis Hémadou and Loïc Simon and Frédéric Jurie},
  doi          = {10.1016/j.cviu.2023.103714},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103714},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {On the inductive biases of deep domain adaptation},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised soft-to-hard hashing with contrastive learning.
<em>CVIU</em>, <em>233</em>, 103713. (<a
href="https://doi.org/10.1016/j.cviu.2023.103713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to optimize a binary hash code with deep neural networks , greedy optimization with the discrete constraint relaxation is widely used due to its efficiency. However, noises from skipping gradients may destabilize the optimization in contrastive learning since the noises are superimposed on embedding from two distorted views. To reduce such undesirable impact from the noises, we propose a novel soft-to-hard hashing method by introducing an auxiliary loss in the penultimate block of the model based on the information bottleneck principle, which not only propagates the correlations between positives but also makes the embedding to be non-redundant to the samples in a high-dimensional continuous space. Our method beats the state-of-the-art result on three public datasets and demonstrates how soft code can help greedy back-propagation find a better solution during optimization. Another benefit of our method is that it enables a joint training of unsupervised representational learning and hash code generation, and obtains 5.4\% mAP gain over conventional step-by-step training on the CIFAR-10 dataset.},
  archive      = {J_CVIU},
  author       = {Wonju Lee and Seok-Yong Byun and Minje Park},
  doi          = {10.1016/j.cviu.2023.103713},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103713},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised soft-to-hard hashing with contrastive learning},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive scene text erasing with self-supervision.
<em>CVIU</em>, <em>233</em>, 103712. (<a
href="https://doi.org/10.1016/j.cviu.2023.103712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text erasing seeks to erase text contents from scene images and current state-of-the-art text erasing models are trained on large-scale synthetic data. Although data synthetic engines can provide vast amounts of annotated training samples, there are differences between synthetic and real-world data. In this paper, we employ self-supervision for feature representation on unlabeled real-world scene text images. A novel pretext task is designed to keep consistent among text stroke masks of image variants. We design the Progressive Erasing Network in order to remove residual texts. The scene text is erased progressively by leveraging the intermediate generated results which provide the foundation for subsequent higher quality results. Experiments show that our method significantly improves the generalization of the text erasing task and achieves state-of-the-art performance on public benchmarks.},
  archive      = {J_CVIU},
  author       = {Xiangcheng Du and Zhao Zhou and Yingbin Zheng and Xingjiao Wu and Tianlong Ma and Cheng Jin},
  doi          = {10.1016/j.cviu.2023.103712},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103712},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Progressive scene text erasing with self-supervision},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single and multiple illuminant estimation using convex
functions. <em>CVIU</em>, <em>233</em>, 103711. (<a
href="https://doi.org/10.1016/j.cviu.2023.103711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lighting situation in which a picture was taken has an impact on its color. Illuminant estimation is crucial in computer vision because the colors of objects vary as illumination changes. For this reason, numerous methods for estimating the illuminant have been suggested. In this paper, we suggest a novel statistic-based method for estimating single and multiple illuminants using convex functions . In this respect, convex functions are used in the two subsequent steps of normalization and weight creation. After using weighted K-means to segment the picture, each segment’s associated illuminations are determined. The illumination map for the input image is estimated as a final stage. In this study, we also analyze the effect of convexity on color constancy algorithms and present proofs for the convexity of some statistic-based algorithms. Four different single and multi-illuminant datasets have been used to evaluate the proposed algorithm in terms of two evaluation metrics ; recovery and reproduction angular error . We believe that the proposed method could be considered one of the statistical state-of-the-art algorithms. In addition, it has competitive results when compared to most learning-based and deep-learning methods. Further advantages of the proposed algorithm include its simplicity of implementation and low execution time.},
  archive      = {J_CVIU},
  author       = {Zeinab Abedini and Mansour Jamzad},
  doi          = {10.1016/j.cviu.2023.103711},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103711},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Single and multiple illuminant estimation using convex functions},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature preserving 3D mesh denoising with a dense local
graph neural network. <em>CVIU</em>, <em>233</em>, 103710. (<a
href="https://doi.org/10.1016/j.cviu.2023.103710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are ideally suited for mesh denoising . However, existing solutions such as those based on graph convolutional networks (GCNs) are built for a fixed graph thus making them not naturally generalizable to unseen meshes. Furthermore, their graph Laplacian based global node embedding algorithms can cause excessive smoothing while achieving feature preserving mesh denoising requires a GNN to possess local processing capability. This paper presents a mesh denoising method via a new Dense lOcal Graph neural NETwork (DOGNET). DOGNET implements a local node embedding algorithm that generates node embeddings through aggregating information from a node’s connected local neighbours which automatically make DOGNET inductive as well as effective for feature preserving mesh denoising. We present extensive experimental results to demonstrate quantitatively and qualitatively that DOGNET is superior to SOTA meshing denoising techniques.},
  archive      = {J_CVIU},
  author       = {Wenming Tang and Yuanhao Gong and Guoping Qiu},
  doi          = {10.1016/j.cviu.2023.103710},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103710},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Feature preserving 3D mesh denoising with a dense local graph neural network},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain adaptive multigranularity proposal network for text
detection under extreme traffic scenes. <em>CVIU</em>, <em>233</em>,
103709. (<a href="https://doi.org/10.1016/j.cviu.2023.103709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic text detection is an important and meaningful research task as it can provide abundant semantic information for autonomous driving . Although major breakthroughs have been achieved on conventional datasets, existing scene text detectors generally suffer from significant performance degradation in real-life traffic scenes since various extreme scenarios may cause a large domain shift. To realize robust traffic text detection under scene changes, we propose a novel network for cross-domain traffic text detection, which integrates both text detection and domain adaptation into one framework. In the text detection pipeline , we introduce a Multigranularity Text Proposal Network (MG-TPN) to generate fine-grained and coarse-grained text proposals, which could deeply interact during both training and inference stages, benefiting the pipeline in learning more robust text features and generating accurate detection results. To transfer the text detection ability from common scenes to unlabeled extreme traffic scenes, we propose an inter&amp;intradomain adaptation (I 2 2 -DA) strategy, which adequately excavates domain-invariant features between the source domain and target domain (interdomain), as well as multiple extreme scenarios of the target domain (intradomain). To the best of our knowledge, this is the first study on cross-domain text detection under extreme traffic scenes. Extensive experiments on the traffic text datasets and standard benchmarks, including SynthText, VISD, ICDAR2013 and ICDAR2015 validate the superiority of our method. The proposed two datasets (CTST and ES-CTST) are available at https://github.com/pummi823/test .},
  archive      = {J_CVIU},
  author       = {Xuan He and Zhiyong Li and Jiacheng Lin and Ke Nai and Jin Yuan and Yifan Li and Runmin Wang},
  doi          = {10.1016/j.cviu.2023.103709},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103709},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Domain adaptive multigranularity proposal network for text detection under extreme traffic scenes},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient multi-stage network with pixel-wise degradation
prediction for real-time motion deblurring. <em>CVIU</em>, <em>233</em>,
103693. (<a href="https://doi.org/10.1016/j.cviu.2023.103693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion deblurring is an indispensable task in perceptual systems because motion blur can seriously influence the visual effect and the quality of subsequent perception tasks . In practical application, efficiency and effect of motion deblurring are both important. However, among the existing methods, there is no design that can meet the real-time and visual quality requirements at the same time. Thus, an efficient motion deblurring network is proposed by leveraging multi-stage pixel-wise degradation prediction. Specifically, some lightweight modules are designed to accelerate processing while attention and multi-scale mechanism are introduced to maintain quality. In addition, a pixel-wise degradation prediction module and a spatial-channel compensation module are further employed to improve the deblurring quality, such as the distortion of moving objects in the restoration images. Extensive experimental results show that the proposed network can achieve the same PSNR level as the SOTA lightweight deblurring methods and is far faster (5.3 times for DMPHN, 6.7 times for IFI-RNN). Therefore, the proposed design achieves a balance between quality and speed compared with the existing methods.},
  archive      = {J_CVIU},
  author       = {Zeyu Hao and Hang Wang and Xuchong Zhang and Yuhai Li and Hongbin Sun},
  doi          = {10.1016/j.cviu.2023.103693},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103693},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient multi-stage network with pixel-wise degradation prediction for real-time motion deblurring},
  volume       = {233},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M2FINet: Modality-specific and modality-shared features
interaction network for RGB-IR person re-identification. <em>CVIU</em>,
<em>232</em>, 103708. (<a
href="https://doi.org/10.1016/j.cviu.2023.103708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning rich and discriminative person-related modality-shared feature representations to distinguish the same person in different modalities is significant for RGB-Infrared Person Re-IDentification (RGB-IR ReID). However, most existing models often directly extract modality-shared features from those modality-specific features without considering their interactions. This will result in the ineffective exploration of person-related modality-shared features. To address such a problem, a novel Modality-specific and Modality-shared Features Interaction Network (M2FINet) model is proposed for RGB-IR ReID in this paper. Especially, in the proposed M2FINet model, a Cross-level Feature Guidance and Injection (CFGI) module is carefully designed to establish and exploit the interactions between the middle-level modality-specific features and the high-level modality-shared features. Specifically, the proposed CFGI module mainly consists of two streams, a shared-to-specific feature guidance (H2P) stream and a specific-to-shared feature injection (P2H) stream. Among that, the H2P stream aims to take high-level modality-shared features as the prior information to guide the re-exploration of more rich and discriminative modality-shared semantic information from such middle-level modality-specific features. The P2H stream aims to enhance the representation ability and discriminability of high-level modality-shared features by introducing more modality-shared detail information from the middle-level modality-specific features. On top of that, a simple but effective feature aggregation module, i.e. , Focusing on Person (FOP), is further designed in our proposed model to reinforce such discriminative modality-shared features within those person-related regions via a multi-pooling feature aggregation manner. Extensive experiments on two public benchmarks, i.e, SYSU-MM01 and RegDB, show that our proposed model consistently improves the accuracy of RGB-IR ReID. Without bells and whistles, it achieves Rank-1/mAP by 74.73\%/68.96\% on the large-scale SYSU-MM01 dataset.},
  archive      = {J_CVIU},
  author       = {Jianan Liu and Jian Liu and Qiang Zhang},
  doi          = {10.1016/j.cviu.2023.103708},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103708},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {M2FINet: Modality-specific and modality-shared features interaction network for RGB-IR person re-identification},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient framework for few-shot skeleton-based temporal
action segmentation. <em>CVIU</em>, <em>232</em>, 103707. (<a
href="https://doi.org/10.1016/j.cviu.2023.103707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action segmentation (TAS) aims to classify and locate actions in the long untrimmed action sequence. With the success of deep learning , many deep models for action segmentation have emerged. However, few-shot TAS is still a challenging problem. This study proposes an efficient framework for the few-shot skeleton-based TAS, including a data augmentation method and an improved model. The data augmentation approach based on motion interpolation is presented here to solve the problem of insufficient data, and can increase the number of samples significantly by synthesizing action sequences. Besides, we concatenate a Connectionist Temporal Classification (CTC) layer with a network designed for skeleton-based TAS to obtain an optimized model. Leveraging CTC can enhance the temporal alignment between prediction and ground truth and further improve the segment-wise metrics of segmentation results. Extensive experiments on both public and self-constructed datasets, including two small-scale datasets and one large-scale dataset, show the effectiveness of two proposed methods in improving the performance of the few-shot skeleton-based TAS task .},
  archive      = {J_CVIU},
  author       = {Leiyang Xu and Qiang Wang and Xiaotian Lin and Lin Yuan},
  doi          = {10.1016/j.cviu.2023.103707},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103707},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An efficient framework for few-shot skeleton-based temporal action segmentation},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end weakly-supervised single-stage multiple 3D hand
mesh reconstruction from a single RGB image. <em>CVIU</em>,
<em>232</em>, 103706. (<a
href="https://doi.org/10.1016/j.cviu.2023.103706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the challenging task of simultaneously locating and recovering multiple hands from a single 2D image. Previous studies either focus on single hand reconstruction or solve this problem in a multi-stage way. Moreover, the conventional two-stage pipeline firstly detects hand areas, and then estimates 3D hand pose from each cropped patch. To reduce the computational redundancy in preprocessing and feature extraction, for the first time, we propose a concise but efficient single-stage pipeline for multi-hand reconstruction. Specifically, we design a multi-head auto-encoder structure, where each head network shares the same feature map and outputs the hand center, pose and texture, respectively. Besides, we adopt a weakly-supervised scheme to alleviate the burden of expensive 3D real-world data annotations. To this end, we propose a series of losses optimized by a stage-wise training scheme, where a multi-hand dataset with 2D annotations is generated based on the publicly available single hand datasets. In order to further improve the accuracy of the weakly supervised model, we adopt several feature consistency constraints in both single and multiple hand settings. Specifically, the keypoints of each hand estimated from local features should be consistent with the re-projected points predicted from global features. Extensive experiments on public benchmarks including FreiHAND, HO3D, InterHand2.6M and RHD demonstrate that our method outperforms the state-of-the-art model-based methods in both weakly-supervised and fully-supervised manners. The code and models are available at https://github.com/zijinxuxu/SMHR .},
  archive      = {J_CVIU},
  author       = {Jinwei Ren and Jianke Zhu and Jialiang Zhang},
  doi          = {10.1016/j.cviu.2023.103706},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103706},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {End-to-end weakly-supervised single-stage multiple 3D hand mesh reconstruction from a single RGB image},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spectrum-irrelevant fine-grained representation for
visible–infrared person re-identification. <em>CVIU</em>, <em>232</em>,
103703. (<a href="https://doi.org/10.1016/j.cviu.2023.103703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–infrared person re-identification (VI-ReID) is an important and practical task for full-time intelligent surveillance systems. Compared to visible person re-identification, it is more challenging due to the large cross-modal discrepancy. Existing VI-ReID methods suffer from heterogeneous structures and the different spectra of visible and infrared images. In this work, we propose the Spectrum-Insensitive Data Augmentation (SIDA) strategy, which effectively alleviates the disturbance in the visible and infrared spectra and forces the network to learn spectrum-irrelevant features. The network also compares samples with both global and local features . We devise a Feature Relation Reasoning (FRR) module to learn discriminative fine-grained representations according to the graph reasoning principle. Compared to the most commonly used uniform partition, our FRR better adopts to the case of VI-ReID, in which human bodies are difficult to align. Furthermore, we design the dual center loss for learning the global feature in order to maintain the intra-modality relations, while learning the cross-modal similarities. Our method achieves better convergence in training. Extensive experiments demonstrate that our method achieves state-of-the-art performance on two visible–infrared cross-modal Re-ID datasets.},
  archive      = {J_CVIU},
  author       = {Jiahao Gong and Sanyuan Zhao and Kin-Man Lam and Xin Gao and Jianbing Shen},
  doi          = {10.1016/j.cviu.2023.103703},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103703},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spectrum-irrelevant fine-grained representation for visible–infrared person re-identification},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PGF-BIQA: Blind image quality assessment via probability
multi-grained cascade forest. <em>CVIU</em>, <em>232</em>, 103695. (<a
href="https://doi.org/10.1016/j.cviu.2023.103695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind image quality assessment (BIQA) aims to automatically predict the perceptual quality of a digital image without accessing its pristine reference and plays an important role in computer vision and image analysis. In fact, the difference in the subjective perception of image quality by evaluators greatly affected quality scores of the images. Aiming at the problem that the existing models cannot take into account the subjective perception differences in people, a blind image quality assessment algorithm via probability gcForest (PGF-BIQA) is proposed. Specifically, we use five qualitative labels to replace the specific quality score and extract image color and texture features to represent image quality. Unlike the widely used neural network , gcForest is ensemble by decision trees which randomly extract different features of various modes to learn the relationship between label and quality features. Therefore, a probability gcForest is designed, combining the frequency probability model and the gcForest, using probability confidence to represent the multiple decision modes results, and describing the subjective perception differences in evaluators. Next, through quality anchors and probability confidences of different classes, PGF-BIQA achieves image quality assessment. Moreover, this method proposes an image classification method that resolves the problem of an unbalanced number of images of different classes and the calculation process of the algorithm is simple and effective. Extensive experiments demonstrate that the proposed method is highly consistent with human perception and outperforms many state-of-the-art BIQA algorithms.},
  archive      = {J_CVIU},
  author       = {Hao Liu and Ce Li and Shangang Jin and Weizhe Gao and Fenghua Liu and Shaoyi Du and Shihui Ying},
  doi          = {10.1016/j.cviu.2023.103695},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103695},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PGF-BIQA: Blind image quality assessment via probability multi-grained cascade forest},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hi-ROS: Open-source multi-camera sensor fusion for real-time
people tracking. <em>CVIU</em>, <em>232</em>, 103694. (<a
href="https://doi.org/10.1016/j.cviu.2023.103694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents Hi-ROS (Human Interaction in ROS), an open source framework focused on real-time accurate assessment of human motion . The system offers a series of tools to track multiple people in real-time by exploiting a calibrated camera network. No assumptions are made about the typology or number of cameras, nor about the body pose estimation algorithm used to extract the 3D poses of the people in the scene. The tools provided by Hi-ROS include a Skeleton Tracker to ensure temporal consistency of the detected poses, a Skeleton Merger to fuse the tracks from multiple cameras, thus limiting flickering phenomena, a Skeleton Optimizer to ensure limb length consistency, and a Skeleton Filter to perform real-time smoothing of the detected joint trajectories. Accuracy, tracking robustness, and real-time performance of the proposed system were evaluated on a public dataset, containing both single-person and multi-person sequences with up to 4 people interacting. The results obtained using different subsets of the proposed tools show how the complete Hi-ROS pipeline provides accurate and reliable estimates also in challenging scenarios, with a reduction of the RMSE of up to 27\% with respect to a pure tracking approach. This work aims to push forward the development of unobtrusive human–robot interaction applications, multi-person automated posture analyses, rehabilitation performance assessments, and any possible application enabled by real-time accurate assessment of human motion via markerless motion capture.},
  archive      = {J_CVIU},
  author       = {Mattia Guidolin and Luca Tagliapietra and Emanuele Menegatti and Monica Reggiani},
  doi          = {10.1016/j.cviu.2023.103694},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103694},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hi-ROS: Open-source multi-camera sensor fusion for real-time people tracking},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BasicTAD: An astounding RGB-only baseline for temporal
action detection. <em>CVIU</em>, <em>232</em>, 103692. (<a
href="https://doi.org/10.1016/j.cviu.2023.103692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection (TAD) is extensively studied in the video understanding community by generally following the object detection pipeline in images. However, complex designs are not uncommon in TAD, such as two-stream feature extraction, multi-stage training, complex temporal modeling , and global context fusion. In this paper, we do not aim to introduce any novel technique for TAD. Instead, we study a simple, straightforward, yet must-known baseline given the current status of complex design and low detection efficiency in TAD. In our simple baseline (BasicTAD), we decompose the TAD pipeline into several essential components: data sampling, backbone design, neck construction, and detection head. We extensively investigate the existing techniques in each component for this baseline and, more importantly, perform end-to-end training over the entire pipeline thanks to the simplicity of design. As a result, this simple BasicTAD yields an astounding and real-time RGB-Only baseline very close to the state-of-the-art methods with two-stream inputs. In addition, we further improve the BasicTAD by preserving more temporal and spatial information in network representation (termed as PlusTAD). Empirical results demonstrate that our PlusTAD is very efficient and significantly outperforms the previous methods on the datasets of THUMOS14 and FineAction. Meanwhile, we also perform in-depth visualization and error analysis on our proposed method and try to provide more insights into the TAD problem. Our approach can serve as a strong baseline for future TAD research. The code and model are released at https://github.com/MCG-NJU/BasicTAD .},
  archive      = {J_CVIU},
  author       = {Min Yang and Guo Chen and Yin-Dong Zheng and Tong Lu and Limin Wang},
  doi          = {10.1016/j.cviu.2023.103692},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103692},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {BasicTAD: An astounding RGB-only baseline for temporal action detection},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Precondition and effect reasoning for action recognition.
<em>CVIU</em>, <em>232</em>, 103691. (<a
href="https://doi.org/10.1016/j.cviu.2023.103691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition has drawn a lot of attention in the recent years due to the research and application significance. Most existing works on action recognition focus on learning effective spatial–temporal features from videos, but neglect the strong causal relationship among the precondition, action and effect. Such relationships are also crucial to the accuracy of action recognition. In this paper, we propose to model the causal relationships based on the precondition and effect to improve the performance of action recognition. Specifically, a Cycle-Reasoning model is proposed to capture the causal relationships for action recognition. To this end, we annotate precondition and effect for a large-scale action dataset. Experimental results show that the proposed Cycle-Reasoning model can effectively reason about the precondition and effect and can enhance action recognition performance.},
  archive      = {J_CVIU},
  author       = {Hongsang Yoo and Haopeng Li and Qiuhong Ke and Liangchen Liu and Rui Zhang},
  doi          = {10.1016/j.cviu.2023.103691},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103691},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Precondition and effect reasoning for action recognition},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCA-net: Spatial and channel attention-based network for 3D
point clouds. <em>CVIU</em>, <em>232</em>, 103690. (<a
href="https://doi.org/10.1016/j.cviu.2023.103690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in data processing of 3D point cloud data is how to identify key regions in the point cloud from geometric relationships. Previous work has attempted to overcome this deficiency by slicing and dicing data. Such an approach limits the model’s ability to effectively learn global information. In this paper, in order to better extract features and obtain geometric information we propose a Point Attention (PointAT) model and propose Attention Value (AT value) model for feature fusion to apply geometric relationship to the data. Then, we propose a new Spatial and Channel Attention-based network (SCA). The SCA is the overall structure of the network, and the main purpose is to connect PointAT and AT value model, then capturing meaningful geometric information by applying the geometric relationship between point clouds patches to the model, then propose a auto pooling framework to extract to global features. In this work, we concentrate on learning geometric relationship between point cloud data. For this purpose, we introduce a point attention model based on spatial and channel attention to learn the geometric relationship between point clouds, and further combine the geometric relationship with the point cloud data by the AT Value Model. Finally, we introduce a adaptive downsampling structure, Autopooling. This downsampling structure consider each point’s importance weight and picking key points adaptively, which can be used with convolutional networks . Extensive experiments conducted on two benchmark datasets (ModelNet40 and ShapeNet) clearly demonstrate the effectiveness of our SCA and SCA-Auto (SCAA with Auto pooling) methods.},
  archive      = {J_CVIU},
  author       = {Xikai Tang and Karim Habashy and Fangzheng Huang and Chao Li and Dayan Ban},
  doi          = {10.1016/j.cviu.2023.103690},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103690},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SCA-net: Spatial and channel attention-based network for 3D point clouds},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tractable large-scale deep reinforcement learning.
<em>CVIU</em>, <em>232</em>, 103689. (<a
href="https://doi.org/10.1016/j.cviu.2023.103689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL) has emerged as one of the most promising and powerful techniques in deep learning . The training of intelligent agents requires a myriad of training examples which imposes a substantial computational cost. Consequently, RL is seldom applied to real-world problems and historically has been limited to computer vision tasks, similar to supervised learning. This work proposes an RL framework for complex, partially observable, large-scale environments. We introduce novel techniques for tractable training on commodity GPUs , and significantly reduce computational costs. Furthermore, we present a self-supervised loss that improves the learning stability in applications with a long-time horizon, shortening the training time. We demonstrate the effectiveness of the proposed solution on the application of road extraction from high-resolution satellite images. We present experiments on satellite images of fifteen cities that demonstrate comparable performance to state-of-the-art methods. To the best of our knowledge, this is the first time RL has been applied for extracting road networks . The code is publicly available at https://github.com/nsarang/road-extraction-rl .},
  archive      = {J_CVIU},
  author       = {Nima Sarang and Charalambos Poullis},
  doi          = {10.1016/j.cviu.2023.103689},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103689},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Tractable large-scale deep reinforcement learning},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D object feature extraction and classification using 3D
MF-DFA. <em>CVIU</em>, <em>232</em>, 103688. (<a
href="https://doi.org/10.1016/j.cviu.2023.103688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a three-dimensional (3D) object recognition method using multifractal properties. The proposed method is based on a 3D multifractal detrended fluctuation analysis (MF-DFA) using the voxel data of an object. We propose a 3D MF-DFA by extending a conventional MF-DFA, which is widely adopted for analyzing the time series. In the current study, the data processed by the model were changed from the time-series data in the original MF-DFA into the voxels of 3D objects. Thus, the steps in the original model were extended to a 3D processing structure. To voxelize the scatter point data, we apply a modified Allen–Cahn (AC) equation with the Neumann boundary condition to generate the object volume. Various 3D models after voxelization are used for the 3D MF-DFA to calculate the generalized Hurst exponents . The calculated generalized Hurst exponents of different categories show different distributions and are used as the training feature input vector into a multi-classification system, i.e., one-versus-one support vector machines (OvO-SVMs). The computational results show that the proposed method can effectively extract the features of an object. Metrics such as the accuracy, precision, and recall are utilized to measure the efficiency and robustness. When compared with state-of-art algorithms, our empirical tests show that the proposed 3D MF-DFA-OvO-SVM system is superior to most of the methods in terms of classification accuracy . In addition, we also confirm that our proposed model is applicable to object detection in 3D space. An efficient method may be useful for autonomous driving , robot cruises, and AR-based intelligent user interfaces .},
  archive      = {J_CVIU},
  author       = {Jian Wang and Ziwei Han and Wenjing Jiang and Junseok Kim},
  doi          = {10.1016/j.cviu.2023.103688},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103688},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {3D object feature extraction and classification using 3D MF-DFA},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-view-CNN framework for deep representation learning
in image classification. <em>CVIU</em>, <em>232</em>, 103687. (<a
href="https://doi.org/10.1016/j.cviu.2023.103687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep representation learning in image classification is an area in computer vision where deep Convolutional Neural Networks (CNNs) have flourished. Nevertheless, developing an efficient image recognition model for real world applications is a challenging task, since image datasets are characterized by instances with a large amount of noise and redundant information. Thus, it is essential to incorporate an intelligent feature extraction and filtering method in order to create robust and efficient image representations. In this work, we propose a Multi-View-CNN framework which drastically boosts the performance of pre-trained CNN models, such as ResNet and VGG in image classification applications. In this approach different type of views of the same initial image are used in order to extract different types of features utilizing pre-trained CNN models. However, in order to reduce the huge dimensional size of the raw CNN’s output features and create a robust image representation, the Principal Component Analysis (PCA) dimension reduction method is applied. Then, all these extracted feature vectors are concatenated building a final composite feature representation of the initial image dataset. Finally, this augmented feature vector is used for training a linear model (Logistic Regression) in order to perform the final classification tasks . The main findings of this work are summarized as follows. First, the proposed Multi-View-CNN framework managed to drastically increase the performance results of pre-trained CNN models. Second, the incorporation of PCA as a final layer into the main CNN topology, instead of using the classical dimension reduction layer components such as Averaging and Max Pooling operations, managed to significantly improve the performance. The whole implementation code of this framework alongside with the datasets used in our experimental simulations was uploaded to our public GitHub repository to the following link: https://github.com/EmmanuelPintelas/A-Multi-View-CNN-Framework-for-Deep-Representation-Learning-in-Image-Classification .},
  archive      = {J_CVIU},
  author       = {Emmanuel Pintelas and Ioannis E. Livieris and Sotiris Kotsiantis and Panagiotis Pintelas},
  doi          = {10.1016/j.cviu.2023.103687},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103687},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A multi-view-CNN framework for deep representation learning in image classification},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial composite prediction of normal video dynamics
for anomaly detection. <em>CVIU</em>, <em>232</em>, 103686. (<a
href="https://doi.org/10.1016/j.cviu.2023.103686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic modeling of normal videos is the key problem for unsupervised video anomaly detection . In this paper, we propose a novel adversarial composite prediction framework for unsupervised video anomaly detection, namely ACP-VAD. It consists of an adversarial composite prediction (ACP) module and a foreground motion metric learning (FMML) module. Given a video sequence, ACP module aims to learn reasonable normal video dynamics by using a generator for the sequential and composite predicting of video frames, and two discriminators for the frame-level and sequence-level adversarial learning. Moreover, to learn a more compact representation of normal video dynamics and improve the detection sensibility of motion anomalies, FMML module utilizes the sophisticated metric learning scheme over the latent space distributions of large foreground motion amplitude sequence, real normal sequence as well as real normal sequence containing previously sequential or composite prediction frames. Through these two modules, our framework can learn reasonable and compact spatial-temporal dynamics for normal videos, and the final anomaly detection task can be solved by computing the error between the predicted frame and its corresponding real frame. Comprehensive experimental results on UCSD Ped2, Avenue, and ShanghaiTech datasets demonstrate the effectiveness of both ACP module and FMML module, and our final ACP-VAD framework achieves impressive performance, surpassing most of the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Gang Li and Ping He and Huibin Li and Fan Zhang},
  doi          = {10.1016/j.cviu.2023.103686},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103686},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial composite prediction of normal video dynamics for anomaly detection},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random image frequency aggregation dropout in image
classification for deep convolutional neural networks. <em>CVIU</em>,
<em>232</em>, 103684. (<a
href="https://doi.org/10.1016/j.cviu.2023.103684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep neural networks are core technologies used in various fields of artificial intelligence . However, they require a sufficient dataset to leverage their high representation power and full potential. As a result, data augmentation is an essential method for fully utilizing these networks’ capabilities. This study proposes a novel data augmentation method called random image frequency aggregation dropout , (RIFAD). RIFAD consists of two sub-algorithms: Fourier spectrum analysis (FSA) and frequency aggregation dropout (FAD). In FSA, the angular distribution is extracted by analyzing the Fourier spectrum of the input image. In FAD, frequency aggregation is deleted after randomly selecting an angle from the angle distribution. Empirically, we demonstrated that our method significantly improves prior state-of-the-art data augmentation using various convolutional neural network (CNN) architectures for image classification . We achieved superior Top-1 errors of 5.14\%, 24.65\%, and 21.45\% on CIFAR-10, CIFAR-100, and STL-10 with Wide ResNet-40-2, respectively. We also demonstrated that the CNN with RIFAD had the best accuracy of 0.8961, precision of 0.9034, recall of 0.9046, f1-score of 0.8942, and area under the curve (AUC) of 0.9858 on a chest X-ray dataset.},
  archive      = {J_CVIU},
  author       = {Ju-Hyeon Nam and Sang-Chul Lee},
  doi          = {10.1016/j.cviu.2023.103684},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103684},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Random image frequency aggregation dropout in image classification for deep convolutional neural networks},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-light image enhancement by deep learning network for
improved illumination map. <em>CVIU</em>, <em>232</em>, 103681. (<a
href="https://doi.org/10.1016/j.cviu.2023.103681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the case of insufficient illumination conditions , the quality of the captured image is poor. At this time, increasing the brightness of the dark region of the whole image will inevitably aggravate noise pollution. Therefore, the ideal state of low-light image enhancement should be to make the dark areas brighter while not allowing the noise part to be enhanced as well. In this paper, we combine Retinex theory and convolutional neural networks to build a simple and effective model with three sub-modules: the decomposition module, illumination module, and reflection module, in which the decomposition module is used to decompose the input low-light image into illumination and reflection maps, the illumination module improves the feature extraction part of the simple convolutional layer used previously and constitutes a new feature extraction structure by Depthwise separable convolution, which makes up for the shortcoming that some dark areas are not well brightened and better adjusts the brightness of the illumination map, and the reflection module adjusts the texture details more clearly because it adds the illumination map as a reference. In this way, the original space is decoupled into two smaller subspaces to the extent that better results can be achieved. We demonstrate through extensive experiments the effectiveness of the design and its superiority over state-of-the-art techniques, especially in terms of robustness to severe visual defects and flexibility in adjusting luminance.},
  archive      = {J_CVIU},
  author       = {Manli Wang and Jiayue Li and Changsen Zhang},
  doi          = {10.1016/j.cviu.2023.103681},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103681},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Low-light image enhancement by deep learning network for improved illumination map},
  volume       = {232},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrared and visible image fusion via mutual information
maximization. <em>CVIU</em>, <em>231</em>, 103683. (<a
href="https://doi.org/10.1016/j.cviu.2023.103683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image fusion methods based on deep learning generally measure the similarity between the fusion results and the source images, ignoring the harmful information of source images. This paper presents a simple-yet-effective self-supervised image fusion optimization mechanism via directly maximizing the mutual information between the fused image and image samples, including positive and negative samples. The fusion optimization of positive samples has three steps, including visual fidelity item, quality perception item, and semantic perception item loss functions, aiming to reduce the distance between the fused representation and the real image quality. The fusion optimization of negative samples aims to enlarge the distance between the fusion results and the degraded image . Following InfoNCE, our framework is optimized via a surrogate contrastive loss , where the positive and negative selection underpins the real quality and visual fidelity information of fusion representation learning . Therefore, the stumbling blocks of deep learning in image fusion, i.e., similarity fusion optimization problems , are significantly mitigated. Extensive experiments demonstrate that fusion results neatly outperforms the state-of-the-art fusion optimization mechanisms in most metrics.},
  archive      = {J_CVIU},
  author       = {Aiqing Fang and Junsheng Wu and Ying Li and Ruimin Qiao},
  doi          = {10.1016/j.cviu.2023.103683},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103683},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Infrared and visible image fusion via mutual information maximization},
  volume       = {231},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grow-push-prune: Aligning deep discriminants for effective
structural network compression. <em>CVIU</em>, <em>231</em>, 103682. (<a
href="https://doi.org/10.1016/j.cviu.2023.103682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of today’s popular deep architectures are hand-engineered to be generalists. However, this design procedure usually leads to massive redundant, useless, or even harmful features for specific tasks. Unnecessarily high complexities render deep nets impractical for many real-world applications, especially those without powerful GPU support. In this paper, we attempt to derive task-dependent compact models from a deep discriminant analysis perspective. We propose an iterative and proactive approach for classification tasks which alternates between (1) a pushing step, with an objective to simultaneously maximize class separation, penalize co-variances, and push deep discriminants into alignment with a compact set of neurons, and (2) a pruning step, which discards less useful or even interfering neurons. Deconvolution is adopted to reverse ‘unimportant’ filters’ effects and recover useful contributing sources. A simple network growing strategy based on the basic Inception module is proposed for challenging tasks requiring larger capacity than what the base net can offer. Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach’s efficacy. On ImageNet, by pushing and pruning our grown Inception-88 model, we achieve more accurate models than Inception nets generated during growing, residual nets, and popular compact nets at similar sizes. We also show that our grown Inception nets (without hard-coded dimension alignment) clearly outperform residual nets of similar complexities.},
  archive      = {J_CVIU},
  author       = {Qing Tian and Tal Arbel and James J. Clark},
  doi          = {10.1016/j.cviu.2023.103682},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103682},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Grow-push-prune: Aligning deep discriminants for effective structural network compression},
  volume       = {231},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust kernel-based feature representation for 3D point
cloud analysis via circular convolutional network. <em>CVIU</em>,
<em>231</em>, 103678. (<a
href="https://doi.org/10.1016/j.cviu.2023.103678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature descriptors of point clouds are used in several applications, such as registration and part segmentation of 3D point clouds. Learning representations of local geometric features is unquestionably the most important task for accurate point cloud analyses. However, it is challenging to develop rotation or scale-invariant descriptors. Most previous studies have either ignored rotations or empirically studied optimal scale parameters, which hinders the applicability of the methods for real-world datasets. In this paper, we present a new local feature description method that is robust to rotation and scale variations. Moreover, we improve representations based on a global aggregation method. First, we place kernels aligned around each point in the normal direction. To avoid the sign problem of the normal vector, we use a symmetric kernel point distribution in the tangential plane. From each kernel point, we first project the points from the spatial space to the feature space, which is robust to multiple scales and rotation, based on angles and distances. Subsequently, we perform convolutions by considering local kernel point structures and long-range global context, obtained by a global aggregation method. We experimented with our proposed descriptors on benchmark datasets (i.e., ModelNet40 and ShapeNetPart) to evaluate the performance of registration, classification, and part segmentation on 3D point clouds. Our method showed superior performances when compared to the state-of-the-art methods by reducing 70\% of the rotation and translation errors in the registration task. Our method also showed comparable performance in the classification and part-segmentation tasks without any external data augmentation.},
  archive      = {J_CVIU},
  author       = {Seunghwan Jung and Yeong-Gil Shin and Minyoung Chung},
  doi          = {10.1016/j.cviu.2023.103678},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103678},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Robust kernel-based feature representation for 3D point cloud analysis via circular convolutional network},
  volume       = {231},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video captioning: A comparative review of where we are and
which could be the route. <em>CVIU</em>, <em>231</em>, 103671. (<a
href="https://doi.org/10.1016/j.cviu.2023.103671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning is the process of describing the content of a sequence of images capturing its semantic relationships and meanings. Dealing with this task with a single image is arduous, not to mention how difficult it is for a video (or image sequence). The amount and relevance of the applications of video captioning are vast, mainly to deal with a significant amount of video recordings in video surveillance, or assisting people visually impaired, to mention a few. To analyze where the efforts of our community to solve the video captioning task are, as well as what route could be better to follow, this manuscript presents an extensive review of more than 142 papers for the period of 2016 to 2022. As a result, the most-used datasets and metrics are identified and described. Also, the main approaches used and the best ones are analyzed and discussed. Furthermore, we compute a set of rankings based on several performance metrics to obtain, according to its reported performance, the best method with the best result on the video captioning task across of several datasets and metrics. Finally, some insights are concluded about which could be the next steps or opportunity areas to improve dealing with this complex task.},
  archive      = {J_CVIU},
  author       = {Daniela Moctezuma and Tania Ramírez-delReal and Guillermo Ruiz and Othón González-Chávez},
  doi          = {10.1016/j.cviu.2023.103671},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103671},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video captioning: A comparative review of where we are and which could be the route},
  volume       = {231},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extending class activation mapping using gaussian receptive
field. <em>CVIU</em>, <em>231</em>, 103663. (<a
href="https://doi.org/10.1016/j.cviu.2023.103663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the inner behaviors of deep neural networks is an important issue, and various visualization methods have been suggested. However, recent studies showed that most existing saliency methods have low visualization accuracy, even compared to a random importance assignment. In this study, we seek an improved saliency algorithm. Focusing on class activation mapping (CAM)-based saliency methods, we discuss two problems with the existing studies. First, we introduce conservativeness , a property that prevents redundancy and deficiency in saliency map and ensures that the saliency map is on the same scale as the prediction score. We identify that existing CAM studies do not satisfy the conservativeness and derive a new CAM equation with the improved theoretical property. Second, we discuss the common practice of using bilinear upsampling as problematic. We propose Gaussian upsampling, an improved upsampling method that reflects deep neural networks’ properties. Based on these two options, we propose Extended-CAM, an advanced CAM-based visualization method. In various visualization benchmarks, datasets, and architectures, our Extended-CAM presents a more accurate visualization than is obtainable with other existing methods.},
  archive      = {J_CVIU},
  author       = {Bum Jun Kim and Gyogwon Koo and Hyeyeon Choi and Sang Woo Kim},
  doi          = {10.1016/j.cviu.2023.103663},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103663},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Extending class activation mapping using gaussian receptive field},
  volume       = {231},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust kinship verification scheme using face age
transformation. <em>CVIU</em>, <em>231</em>, 103662. (<a
href="https://doi.org/10.1016/j.cviu.2023.103662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship verification determines the existence of a kin relationship between people through facial image analysis, and can be used in various real-life applications, such as finding missing family members, analyzing social media, and genealogy research. Recently, many convolutional neural network (CNN)-based kinship verification methods have been proposed, owing to the good image-processing performance of the CNNs. Nevertheless, insufficient labeled data and age differences in the kinship images, make kinship verification quite difficult. To mitigate these limitations, herein, we first propose a face age transformation model to generate facial images of various age groups. Then, we construct a cross-age kinship verification model constructed using the generated images as a training dataset. To show the effectiveness of the proposed scheme, we conducted various comparative experiments with other models using popular kinship datasets and confirmed that our proposed method exhibited an improved verification accuracy.},
  archive      = {J_CVIU},
  author       = {Hyeonwoo Kim and Hyungjoon Kim and Jonghwa Shim and Eenjun Hwang},
  doi          = {10.1016/j.cviu.2023.103662},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103662},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A robust kinship verification scheme using face age transformation},
  volume       = {231},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised multi-class semantic video segmentation
for road scenes. <em>CVIU</em>, <em>230</em>, 103664. (<a
href="https://doi.org/10.1016/j.cviu.2023.103664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised multi-class video segmentation is one of the most challenging yet least studied research problems in computer vision . This study aims to investigate two main items: (1) effective feature update for temporal changes combined with feature reuse between temporal frames; and (2) learn object patterns in complex scenes specifically for videos under weak supervision. Associating image tags to visual appearance is not a straightforward learning task, especially for complex scenes. Therefore, in this paper, we present manifold augmentations to obtain reliable pixel labels from image tags. We propose a framework comprised of two key modules: a temporal split module for efficient video processing and a pseudo per-pixel seed generation module for precise pixel-level supervision. Particularly, in our model, we utilize and explore temporal correlations via temporal split module and temporal attention. To reuse the extracted features and incorporate temporal updates for precise and fast computation, a channel-wise temporal split mechanism between successive video frames is presented. Furthermore, we evaluated proposed modules in two additional settings: (1) fully or sparsely supervised road scene video segmentation; and (2) weakly supervised segmentation for complex road scene images. Experiments are conducted on the Cityscapes and CamVid datasets, using DeepLabv3 as segmentation network and LiteFlowNet to compute motion vectors .},
  archive      = {J_CVIU},
  author       = {Mehwish Awan and Jitae Shin},
  doi          = {10.1016/j.cviu.2023.103664},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103664},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Weakly supervised multi-class semantic video segmentation for road scenes},
  volume       = {230},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved domain adaptive object detector via adversarial
feature learning. <em>CVIU</em>, <em>230</em>, 103660. (<a
href="https://doi.org/10.1016/j.cviu.2023.103660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation is a promising solution to adapt models to new domains or environments at no additional annotation cost. Such adaptation strategy has been investigated for detection to cope with the domain shift along the detection pipeline . To improve the transferability of object detectors between source and target domains, we propose in this paper to perform multi-level feature alignments via adversarial learning. The alignment process is guided by an uncertainty measure to adapt weights of well-aligned and misaligned target samples, accordingly. Furthermore, the proposed approach includes a consistency regularization to enforce the coherency between image and instance level domain classifiers. We also emphasize the need for leveraging contextual information that captures the scene structure for complementary aspects and to further improve the adversarial training process. These adaptation components are integrated into Faster R-CNN architecture at different levels. The resulting proposed adaptive detector has the advantage of covering different aspects of the domain shift in order to improve the overall performance. Extensive experiments of different domain shift scenarios demonstrate the effectiveness of our proposed adaptation scheme by obtaining better results compared to the baseline detector and to the current state-of-the-art adaptive detectors.},
  archive      = {J_CVIU},
  author       = {Mohamed Amine Marnissi and Hajer Fradi and Anis Sahbani and Najoua Essoukri Ben Amara},
  doi          = {10.1016/j.cviu.2023.103660},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103660},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improved domain adaptive object detector via adversarial feature learning},
  volume       = {230},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). To make yourself invisible with adversarial semantic
contours. <em>CVIU</em>, <em>230</em>, 103659. (<a
href="https://doi.org/10.1016/j.cviu.2023.103659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern object detectors are vulnerable to adversarial examples , which may bring risks to real-world applications. The sparse attack is an important task which, compared with the popular adversarial perturbation on the whole image, needs to select the potential pixels that is generally regularized by an ℓ 0 ℓ0 -norm constraint, and simultaneously optimize the corresponding texture. The non-differentiability of ℓ 0 ℓ0 norm brings challenges and many works on attacking object detection adopted manually-designed patterns to address them, which are meaningless and independent of objects, and therefore lead to relatively poor attack performance. In this paper, we propose Adversarial Semantic Contour (ASC), an MAP estimate of a Bayesian formulation of sparse attack with a deceived prior of object contour . The object contour prior effectively reduces the search space of pixel selection and improves the attack by introducing more semantic bias. Extensive experiments demonstrate that ASC can corrupt the prediction of 9 modern detectors with different architectures ( e.g ., one-stage, two-stage and Transformer) by modifying fewer than 5\% of the pixels of the object area in COCO in white-box scenario and around 10\% of those in black-box scenario. We further extend the attack to datasets for autonomous driving systems to verify the effectiveness. We conclude with cautions about contour being the common weakness of object detectors with various architecture and the care needed in applying them in safety-sensitive scenarios.},
  archive      = {J_CVIU},
  author       = {Yichi Zhang and Zijian Zhu and Hang Su and Jun Zhu and Shibao Zheng and Yuan He and Hui Xue},
  doi          = {10.1016/j.cviu.2023.103659},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103659},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {To make yourself invisible with adversarial semantic contours},
  volume       = {230},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DSDNet: Toward single image deraining with self-paced
curricular dual stimulations. <em>CVIU</em>, <em>230</em>, 103657. (<a
href="https://doi.org/10.1016/j.cviu.2023.103657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial challenge regarding the single image deraining task is to completely remove rain streaks while still preserving explicit image details. Due to the inherent overlapping between rain streaks and background scenes, the texture details could be inevitably lost when clearing rain away from the degraded image , making the two purposes contradictory. Existing deep learning based approaches endeavor to resolve the two issues successively in a cascaded framework or to treat them as independent tasks in a parallel structure. However, none of the models explores a proper interaction between rain distributions and hidden feature responses, which intuitively would provide more clues to facilitate the procedures of rain streak removal as well as detail restoration. In this paper, we investigate the impact of rain streak detection for single image deraining and propose a novel deep network with dual stimulations, namely, DSDNet. The proposed DSDNet utilizes a dual-stream pipeline to separately estimate rain streaks and a loss of details, and more importantly, an additional mask that indicates both location and intensity of rains is jointly predicted. In particular, the rain mask is involved in a tailored stimulation strategy that is deployed into each stream of the proposed model, serving as guidance for allowing the network to focus on rain removal and detail recovery in rain regions rather than non-rain areas. Moreover, we incorporate a self-paced semi-curriculum learning design to alleviate the learning ambiguity brought by the prediction of the rain mask and thus accelerate the training process. Extensive experiments demonstrate the proposed method outperforms the state-of-the-art methods on several benchmarks, including in both synthetic and real-world scenarios. The effectiveness of the proposed method is also validated via joint single image deraining, detection, and segmentation tasks .},
  archive      = {J_CVIU},
  author       = {Yong Du and Junjie Deng and Yulong Zheng and Junyu Dong and Shengfeng He},
  doi          = {10.1016/j.cviu.2023.103657},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103657},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DSDNet: Toward single image deraining with self-paced curricular dual stimulations},
  volume       = {230},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic texture classification based on bag-of-models with
mixture of student’s t-hidden markov models. <em>CVIU</em>,
<em>230</em>, 103653. (<a
href="https://doi.org/10.1016/j.cviu.2023.103653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov model (HMM) is a popular statistical approach for modeling sequential data comprising continuous attributes, and it has been applied in finance, machine vision and other fields. Due to higher tolerance to outliers than Gaussian distribution, the multivariate student’s t-distribution is used to serve as the observation emission distribution in continuous HMM (SHMM), which has been exploited to improve the performance for modeling sequential data. In this paper, a bag-of-models method based on the mixture of SHMMs is proposed to describe dynamic texture for dynamic texture classification , in which codebook is constructed with SHMMs. Specifically, a novel mixture model, mixture of SHMMs, is proposed to model and cluster the observation sequences of the dynamic texture. The parameter learning method is derived by using the expectation maximization (EM) algorithm. Then all components of the mixture of SHMMs are assembled to obtain the codewords and the bag-of-models based features are extracted for describing the dynamic texture. Finally, we demonstrate the effectiveness of the bag-of-models based on the mixture of SHMMs for dynamic texture classification on different benchmark datasets. The experimental results show the potential performance of the proposed approach by comparing with some existing dynamic texture classification methods.},
  archive      = {J_CVIU},
  author       = {Zhengyi Xing and Yulong Qiao and Yue Zhao and Wenhui Liu},
  doi          = {10.1016/j.cviu.2023.103653},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103653},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dynamic texture classification based on bag-of-models with mixture of student’s t-hidden markov models},
  volume       = {230},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trimap-guided feature mining and fusion network for natural
image matting. <em>CVIU</em>, <em>230</em>, 103645. (<a
href="https://doi.org/10.1016/j.cviu.2023.103645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mining interesting objects with trimap guidance and fusing multi-level features are two important issues for trimap-based matting. For feature mining, existing methods simply feed trimaps to an encoder network and mine features evenly in different regions with a decoding module, which lacks an efficient and effective way to mine interesting objects pointed out by trimaps. For emerging content-based feature fusion , most existing matting methods only focus on local features which lack the guidance of a global feature with strong semantic information related to the interesting object. In this paper, we propose a trimap-guided feature mining and fusion network consisting of our trimap-guided non-background multi-scale pooling (TMP) module and global-local context-aware fusion (GLF) modules. Considering that trimap provides strong semantic guidance, our TMP module focuses effective feature mining on interesting objects under the guidance of trimap without extra parameters. Furthermore, our GLF modules use global semantic information of interesting objects mined by our TMP module to guide an effective global-local context-aware multi-level feature fusion. In addition, we build a common interesting object matting (CIOM) dataset to advance high-quality image matting. Experimental results on the Composition-1k test set, Alphamatting benchmark, and our CIOM test set demonstrate that our method outperforms state-of-the-art approaches. Codes and models will be publicly available at https://github.com/Serge-weihao/TMF-Matting .},
  archive      = {J_CVIU},
  author       = {Weihao Jiang and Dongdong Yu and Zhaozhi Xie and Yaoyi Li and Zehuan Yuan and Hongtao Lu},
  doi          = {10.1016/j.cviu.2023.103645},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103645},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Trimap-guided feature mining and fusion network for natural image matting},
  volume       = {230},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image amodal completion: A survey. <em>CVIU</em>,
<em>229</em>, 103661. (<a
href="https://doi.org/10.1016/j.cviu.2023.103661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing computer vision systems can compete with humans in understanding the visible parts of objects, but still fall far short of humans when it comes to depicting the invisible parts of partially occluded objects. Image amodal completion aims to equip computers with human-like amodal completion functions to understand an intact object despite it being partially occluded. The main purpose of this survey is to provide an intuitive understanding of the research hotspots, key technologies and future trends in the field of image amodal completion. Firstly, we present a comprehensive review of the latest literature in this emerging field, exploring three key tasks in image amodal completion, including amodal shape completion, amodal appearance completion, and order perception. Then we examine popular datasets related to image amodal completion along with their common data collection methods and evaluation metrics . Finally, we discuss real-world applications and future research directions for image amodal completion, facilitating the reader’s understanding of the challenges of existing technologies and upcoming research trends.},
  archive      = {J_CVIU},
  author       = {Jiayang Ao and Qiuhong Ke and Krista A. Ehinger},
  doi          = {10.1016/j.cviu.2023.103661},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103661},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image amodal completion: A survey},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Siamese self-supervised learning for fine-grained visual
classification. <em>CVIU</em>, <em>229</em>, 103658. (<a
href="https://doi.org/10.1016/j.cviu.2023.103658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) is challenging to capture subtle yet distinct visual cues due to large intra-class and small inter-class variances. To this end, we propose a new Siamese Self-supervised Learning method to perform alignment between different views of one image. Specifically, we employ the attention mechanism to explore the semantic parts of one image, and then generate different views by crop and erase strategy. Meanwhile, we adopt the Siamese network to perform the feature alignment across various views and capture the view-invariant feature in a self-supervised way. Finally, we introduce the center loss to explicitly ensure consistency between different views. Extensive experimental results show the proposed method performs on par with the state-of-the-art methods on three public benchmarks including CUB-200-2011, FGVC-Aircraft, and Stanford Cars.},
  archive      = {J_CVIU},
  author       = {Ruyi Ji and Jiaying Li and Libo Zhang},
  doi          = {10.1016/j.cviu.2023.103658},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103658},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Siamese self-supervised learning for fine-grained visual classification},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SSMTL++: Revisiting self-supervised multi-task learning for
video anomaly detection. <em>CVIU</em>, <em>229</em>, 103656. (<a
href="https://doi.org/10.1016/j.cviu.2023.103656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A self-supervised multi-task learning (SSMTL) framework for video anomaly detection was recently introduced in literature. Due to its highly accurate results, the method attracted the attention of many researchers. In this work, we revisit the self-supervised multi-task learning framework, proposing several updates to the original method. First, we study various detection methods, e . g . based on detecting high-motion regions using optical flow or background subtraction , since we believe the currently used pre-trained YOLOv3 is suboptimal, e . g . objects in motion or objects from unknown classes are never detected. Second, we modernize the 3D convolutional backbone by introducing multi-head self-attention modules, inspired by the recent success of vision transformers . As such, we alternatively introduce both 2D and 3D convolutional vision transformer (CvT) blocks. Third, in our attempt to further improve the model, we study additional self-supervised learning tasks, such as predicting segmentation maps through knowledge distillation , solving jigsaw puzzles, estimating body pose through knowledge distillation, predicting masked regions (inpainting), and adversarial learning with pseudo-anomalies. We conduct experiments to assess the performance impact of the introduced changes. Upon finding more promising configurations of the framework, dubbed SSMTL++v1 and SSMTL++v2, we extend our preliminary experiments to more data sets, demonstrating that our performance gains are consistent across all data sets. In most cases, our results on Avenue, ShanghaiTech and UBnormal raise the state-of-the-art performance bar to a new level.},
  archive      = {J_CVIU},
  author       = {Antonio Barbalau and Radu Tudor Ionescu and Mariana-Iuliana Georgescu and Jacob Dueholm and Bharathkumar Ramachandra and Kamal Nasrollahi and Fahad Shahbaz Khan and Thomas B. Moeslund and Mubarak Shah},
  doi          = {10.1016/j.cviu.2023.103656},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103656},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SSMTL++: Revisiting self-supervised multi-task learning for video anomaly detection},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global–local contrastive multiview representation learning
for skeleton-based action recognition. <em>CVIU</em>, <em>229</em>,
103655. (<a href="https://doi.org/10.1016/j.cviu.2023.103655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human action recognition has been drawing more interest recently due to its low sensitivity to appearance changes and the accessibility of more skeleton data. However, the skeletons captured in practice are sensitive to the view of an actor, given the occlusion of different human-body joints and the errors in human joint localization . Each view is noisy and incomplete, but important factors, such as motion and semantics, should be shared between all views in action representation learning . We support the classic hypothesis that a powerful representation is one that models view-invariant factors, and so does unsupervised learning . Therefore, we study this hypothesis under the framework of contrastive multiview learning, where we learn a representation for action recognition that aims to maximize the mutual information between different views of the same action sequence. Apart from that, a global–local contrastive loss is proposed to model the multi-scale co-occurrence relationships in both spatial and temporal domains. Extensive experimental results show that the proposed method significantly boosts the performance of unsupervised skeleton-based human action methods on three challenging benchmarks of PKUMMD, NTU RGB+D 60, and NTU RGB+D 120.},
  archive      = {J_CVIU},
  author       = {Cunling Bian and Wei Feng and Fanbo Meng and Song Wang},
  doi          = {10.1016/j.cviu.2023.103655},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103655},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Global–local contrastive multiview representation learning for skeleton-based action recognition},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi proxy anchor family loss for several types of
gradients. <em>CVIU</em>, <em>229</em>, 103654. (<a
href="https://doi.org/10.1016/j.cviu.2023.103654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep metric learning (DML) objective is to learn a neural network that maps into an embedding space where similar data are near and dissimilar data are far. However, conventional proxy-based losses for DML have two problems: gradient problem and application of the real-world dataset with multiple local centers. Additionally, the performance metrics of DML also have some issues with stability and flexibility. This paper proposes three multi-proxies anchor (MPA) family losses and a normalized discounted cumulative gain (nDCG@k) metric. This paper makes three contributions. (1) MPA-family losses can learn using a real-world dataset with multi-local centers. (2) MPA-family losses improve the training capacity of a neural network owing to solving the gradient problem. (3) MPA-family losses have data-wise or class-wise characteristics with respect to gradient generation. Finally, we demonstrate the effectiveness of MPA-family losses, and MPA-family losses achieves higher accuracy on two datasets for fine-grained images.},
  archive      = {J_CVIU},
  author       = {Shozo Saeki and Minoru Kawahara and Hirohisa Aman},
  doi          = {10.1016/j.cviu.2023.103654},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103654},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi proxy anchor family loss for several types of gradients},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning transformer-based attention region with multiple
scales for occluded person re-identification. <em>CVIU</em>,
<em>229</em>, 103652. (<a
href="https://doi.org/10.1016/j.cviu.2023.103652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification(Re-ID), with the aim of matching occluded person pairs under cross-camera, remains challenging due to incomplete information and spatial misalignment. The state-of-the-art (SOTA) methods usually include a two-stage architecture based on the existing pose estimation models or the attention mechanism to generate human masks to extract features, which complicate the model and introduce additional biases. To address this issue, we propose a novel end-to-end transformer-based occluded person Re-ID model. Specifically, our model contains two crucial components: (1) the features of global and non-occluded person regions are extracted by two independent Transformer-based feature extraction networks respectively; (2) the distribution of common non-occluded human regions is learnt via a multiheaded self-attention mechanism, and then the Minimized Character-box Proposal (MCP) is utilized to generate accurate shared non-occluded crops. In our model, non-occluded human regions are not annotated and only weakly-supervision of ID labels with multiheaded self-attention are employed to jointly learn the distribution. Further, the human feature contains multi-scale information which is extracted from our dual-branch architecture. Extensive experiment results on four benchmarks of person Re-ID for two tasks (occluded, partial) demonstrate the effectiveness of our proposed framework which achieves the SOTA or the comparable performance on all benchmarks.},
  archive      = {J_CVIU},
  author       = {Zhi Liu and Xingyu Mu and Yunhua Lu and Tingting Zhang and Yingli Tian},
  doi          = {10.1016/j.cviu.2023.103652},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103652},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning transformer-based attention region with multiple scales for occluded person re-identification},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial location constraint prototype loss for open set
recognition. <em>CVIU</em>, <em>229</em>, 103651. (<a
href="https://doi.org/10.1016/j.cviu.2023.103651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenges in pattern recognition is open set recognition . Compared with closed set recognition, open set recognition needs to reduce not only the empirical risk, but also the open space risk, and how to reduce the open space risk is the key of open set recognition. Compared with the previous work, this paper analyzes the distribution rules of the known and unknown features, which is highly related to the open space risk. Then, this paper proposes the matching theory to explain the origin of the open space risk. On this basis, the spatial location constraint prototype loss function is proposed to reduce both risks simultaneously. Extensive experiments on multiple benchmark datasets and many visualization results verify the validity of the proposed matching theory and the effectiveness of the proposed method.},
  archive      = {J_CVIU},
  author       = {Ziheng Xia and Penghui Wang and Ganggang Dong and Hongwei Liu},
  doi          = {10.1016/j.cviu.2023.103651},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103651},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatial location constraint prototype loss for open set recognition},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-level contour combination features for shape
recognition. <em>CVIU</em>, <em>229</em>, 103650. (<a
href="https://doi.org/10.1016/j.cviu.2023.103650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel multi-level contour combination feature for shape recognition. This combination feature effectively solves large intra-class changes and nonlinear deformations of object shapes, thereby enhancing the performance of shape recognition. First, we divide the shape contour into two levels: the sampling points and the contour fragments, where sampling points are used to describe the detailed information of a shape and contour fragments are used to represent the global feature of a shape. Second, we employ the Fisher vector (FV) approach to encode the local sampling point feature and contour fragment feature as high-level characteristics. Finally, we combine the high-level characteristics after FV encoding and perform shape recognition through a linear support vector machine (SVM) model. The proposed method has been assessed on three benchmark shape datasets, including the Animal, MPEG-7,and ETH-80 datasets. Our method achieves 92.70\%, 99.26\% and 98.32\% classification accuracy on the Animal, MPEG-7, and ETH-80 datasets, respectively. In addition, our method can also be applied to the classification of objects in real-word scenes. We combine the Weizmann Horse and the ETHZ Cow real-world scene datasets, and our method achieves 99.25\% classification accuracy on the combined dataset. The recognition results of our approach are better than prior state-of-the-art shape recognition methods, which demonstrate the effectiveness and superiority of our approach.},
  archive      = {J_CVIU},
  author       = {Chengzhuan Yang and Lincong Fang and Benjie Fei and Qian Yu and Hui Wei},
  doi          = {10.1016/j.cviu.2023.103650},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103650},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-level contour combination features for shape recognition},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SSDA-YOLO: Semi-supervised domain adaptive YOLO for
cross-domain object detection. <em>CVIU</em>, <em>229</em>, 103649. (<a
href="https://doi.org/10.1016/j.cviu.2023.103649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive object detection (DAOD) aims to alleviate transfer performance degradation caused by the cross-domain discrepancy. However, most existing DAOD methods are dominated by outdated and computationally intensive two-stage Faster R-CNN, which is not the first choice for industrial applications. In this paper, we propose a novel semi-supervised domain adaptive YOLO (SSDA-YOLO) based method to improve cross-domain detection performance by integrating the compact one-stage stronger detector YOLOv5 with domain adaptation . Specifically, we adapt the knowledge distillation framework with the Mean Teacher model to assist the student model in obtaining instance-level features of the unlabeled target domain. We also utilize the scene style transfer to cross-generate pseudo images in different domains for remedying image-level differences. In addition, an intuitive consistency loss is proposed to further align cross-domain predictions. We evaluate SSDA-YOLO on public benchmarks including PascalVOC, Clipart1k, Cityscapes, and Foggy Cityscapes. Moreover, to verify its generalization, we conduct experiments on yawning detection datasets collected from various real classrooms. The results show considerable improvements of our method in these DAOD tasks, which reveals both the effectiveness of proposed adaptive modules and the urgency of applying more advanced detectors in DAOD. Our code is available on https://github.com/hnuzhy/SSDA-YOLO .},
  archive      = {J_CVIU},
  author       = {Huayi Zhou and Fei Jiang and Hongtao Lu},
  doi          = {10.1016/j.cviu.2023.103649},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103649},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SSDA-YOLO: Semi-supervised domain adaptive YOLO for cross-domain object detection},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WMCP-EM: An integrated dehazing framework for visibility
restoration in single image. <em>CVIU</em>, <em>229</em>, 103648. (<a
href="https://doi.org/10.1016/j.cviu.2023.103648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adopting local patches for varying haze conditions is crucial for optimizing the performance in single image dehazing. We propose a novel two-fold method with a prime focus on self-adaptive prior named Weighted Median Channel Prior (WMCP) to resolve the problems introduced by using a fixed size local-patch in the dehazing process. WMCP works by leveraging the spatially changing haze statistics such as inclusion–exclusion of related pixels for estimating depth-map in varying haze conditions. It is a scale-invariant technique that retains most of the information present in the local neighbourhood of the hazy input image for estimating scene depth, which traditional methods generally fail to preserve. In addition, an unsharp-masking based technique called edge-modulation (EM) promotes hidden or missing details such as microscopic edges and textures lost due to haze, making this scheme beneficial in ensuring a visually aesthetic and realistic dehazed image. This research also includes a set of ablation tests to assess the contributions of each module engaged in the dehazing process. We performed a comparative evaluation of our method with several state-of-the-art techniques, revealing its superiority in terms of visibility improvement and edge preservation , especially when the dense haze regions are taken into consideration.},
  archive      = {J_CVIU},
  author       = {Sidharth Gautam and Tapan Kumar Gandhi and B.K. Panigrahi},
  doi          = {10.1016/j.cviu.2023.103648},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103648},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {WMCP-EM: An integrated dehazing framework for visibility restoration in single image},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the robustness of adversarial attacks using an
affine-invariant gradient estimator. <em>CVIU</em>, <em>229</em>,
103647. (<a href="https://doi.org/10.1016/j.cviu.2023.103647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As designers of artificial intelligence try to outwit hackers, both sides continue to hone in on AI’s inherent vulnerabilities. Designed and trained from certain statistical distributions of data, deep neural networks (DNNs) remain vulnerable to deceptive inputs that violate a DNN’s statistical, predictive assumptions. Before being fed into a neural network , however, most existing adversarial examples cannot maintain malicious functionality when applied to an affine transformation. For practical purposes, maintaining that malicious functionality serves as an important measure of the robustness of adversarial attacks . To help DNNs learn to defend themselves more thoroughly against attacks, we propose an affine-invariant adversarial attack, which can consistently produce more robust adversarial examples over affine transformations. For efficiency, we propose to disentangle current affine-transformation strategies from the Euclidean geometry coordinate plane with its geometric translations, rotations and dilations; we reformulate the latter two in polar coordinates. Afterwards, we construct an affine-invariant gradient estimator by convolving the gradient at the original image with derived kernels, which can be integrated with any gradient-based attack methods. Extensive experiments on ImageNet, including some experiments under physical condition, demonstrate that our method can significantly improve the affine invariance of adversarial examples and, as a byproduct, improve the transferability of adversarial examples, compared with alternative state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Wenzhao Xiang and Hang Su and Chang Liu and Yandong Guo and Shibao Zheng},
  doi          = {10.1016/j.cviu.2023.103647},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103647},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improving the robustness of adversarial attacks using an affine-invariant gradient estimator},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context understanding in computer vision: A survey.
<em>CVIU</em>, <em>229</em>, 103646. (<a
href="https://doi.org/10.1016/j.cviu.2023.103646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual information plays an important role in many computer vision tasks, such as object detection, video action detection, image classification , etc. Recognizing a single object or action out of context could be sometimes very challenging, and context information may help improve the understanding of a scene or an event greatly. Appearance context information, e.g., colors or shapes of the background of an object can improve the recognition accuracy of the object in the scene. Semantic context (e.g. a keyboard on an empty desk vs. a keyboard next to a desktop computer ) will improve accuracy and exclude unrelated events. Context information that are not in the image itself, such as the time or location of an images captured, can also help to decide whether certain event or action should occur. Other types of context (e.g. 3D structure of a building) will also provide additional information to improve the accuracy. In this survey, different context information that has been used in computer vision tasks is reviewed. We categorize context into different types and different levels. We also review available machine learning models and image/video datasets that can employ context information. Furthermore, we compare context-based integration and context-free integration in mainly two classes of tasks: image-based and video-based. Finally, this survey is concluded by a set of promising future directions in context learning and utilization.},
  archive      = {J_CVIU},
  author       = {Xuan Wang and Zhigang Zhu},
  doi          = {10.1016/j.cviu.2023.103646},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103646},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Context understanding in computer vision: A survey},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning rotation equivalent scene representation from
instance-level semantics: A novel top-down perspective. <em>CVIU</em>,
<em>229</em>, 103635. (<a
href="https://doi.org/10.1016/j.cviu.2023.103635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on rotation variant scene recognition. Different from existing rotation invariant recognition approaches which learn from either rotated images or rotated convolutional filters in a bottom-up manner, a new top-down perspective by learning is explored from instance-level semantic representation. The goal is to eliminate the convolutional feature differences in bottom-up feature propagation caused by the rotation sensitive nature of convolution operation . Our rotation equivalent convolutional neural network (RE-CNN) scheme consists of three components. Firstly, our key instance selection module highlights the instances strongly related to the scene scheme regardless of their orientation. Secondly, our key instance aggregation module builds a scene representation invariant to the position change of each instance caused by rotation. Finally, our semantic fusion module allows the framework to be organized as a whole and implements rotation regularization . Notably, our RE-CNN scheme can be adapted to existing CNNs in a plug-in-and-play manner. Extensive experiments on rotation variant scene recognition benchmarks from four domains demonstrate the state-of-the-art performance and generalization capability of the proposed RE-CNN.},
  archive      = {J_CVIU},
  author       = {Qi Bi and Shaodi You and Wei Ji and Theo Gevers},
  doi          = {10.1016/j.cviu.2023.103635},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103635},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning rotation equivalent scene representation from instance-level semantics: A novel top-down perspective},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Siamese graph attention networks for robust visual object
tracking. <em>CVIU</em>, <em>229</em>, 103634. (<a
href="https://doi.org/10.1016/j.cviu.2023.103634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese-based trackers usually convert the object tracking task into a similarity matching problem between the target template and the search region. Since fixed or manually updated templates are not robust when tracking moving objects with dramatically changing appearance, this paper proposes an improved siamese graph attention network with adaptive template update called SiamGT. By establishing spatiotemporal and context dependencies between historical images and search regions, a frame selection mechanism is added to improve the richness of information. In addition, a graph attention network with residual connections is used in the template update mechanism which enables the propagation and aggregation of information to generate robust templates. Extensive experimental results on challenging benchmarks such as UAV123, OTB100, and VOT2019 demonstrate that the proposed SiamGT has achieved state-of-the-art performance in visual object tracking.},
  archive      = {J_CVIU},
  author       = {Junjie Lu and Shengyang Li and Weilong Guo and Manqi Zhao and Jian Yang and Yunfei Liu and Zhuang Zhou},
  doi          = {10.1016/j.cviu.2023.103634},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103634},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Siamese graph attention networks for robust visual object tracking},
  volume       = {229},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ParticleAugment: Sampling-based data augmentation.
<em>CVIU</em>, <em>228</em>, 103633. (<a
href="https://doi.org/10.1016/j.cviu.2023.103633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an automated data augmentation approach for image classification . The problem is formulated as a Monte Carlo sampling problem where the goal is to approximate the optimal augmentation policies using a policy mixture distribution. We propose a particle filter scheme for the policy search where the probability of applying a set of augmentation operations forms the state of the filter. The policy performance is measured based on the loss function difference between a reference model and the actual model. This performance measure is then used to re-weight the particles and finally update the policy distribution. In our experiments, we show that our formulation for automated augmentation reaches promising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the standard network architectures for this problem. By comparing with the related work, our method reaches a balance between the computational cost of policy search and the model performance. The source code of our approach is publicly available.},
  archive      = {J_CVIU},
  author       = {Alexander Tsaregorodtsev and Vasileios Belagiannis},
  doi          = {10.1016/j.cviu.2023.103633},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103633},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {ParticleAugment: Sampling-based data augmentation},
  volume       = {228},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Teacher or supervisor? Effective online knowledge
distillation via guided collaborative learning. <em>CVIU</em>,
<em>228</em>, 103632. (<a
href="https://doi.org/10.1016/j.cviu.2023.103632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a widely-used and effective technique to boost the performance of a lightweight student network, by having it mimic the behavior of a more powerful teacher network. This paper presents an end-to-end online knowledge distillation strategy, in which several peer students are trained together and their predictions are aggregated into a powerful teacher ensemble via an effective ensembling technique that uses an online supervisor network to determine the optimal way of combining the student logits. Intuitively, this supervisor network learns the area of expertise of each student and assigns a weight to each student accordingly►it has knowledge of the input image, the ground truth data, and the predictions of each individual student, and tries to answer the following question: “how much can we rely on each student’s prediction, given the current input image with this ground truth class?”. The proposed technique can be thought of as an inference optimization mechanism as it improves the overall accuracy over the same number of parameters. The experiments we performed show that the proposed knowledge distillation consistently improves the performance of the knowledge-distilled students vs. the independently trained students.},
  archive      = {J_CVIU},
  author       = {Diana Laura Borza and Tudor Alexandru Ileni and Alexandru Ion Marinescu and Sergiu Adrian Darabant},
  doi          = {10.1016/j.cviu.2023.103632},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103632},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Teacher or supervisor? effective online knowledge distillation via guided collaborative learning},
  volume       = {228},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diff attention: A novel attention scheme for person
re-identification. <em>CVIU</em>, <em>228</em>, 103623. (<a
href="https://doi.org/10.1016/j.cviu.2023.103623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Person Re-ID has made great progress benefiting from the advance of deep neural networks and the attention mechanism in recent years. However, existing models and attentions for person Re-ID only focus on learning the robust features while neglecting the difference between features of pairs, which is the core of feature similarity matching tasks. To address this problem, we propose the Diff Attention idea and design Diff Attention Module to guide the network to learn a more discriminative attention map based on the difference of feature vectors. Taking the Diff Attention Module, we develop Diff Attention Framework to match various backbone Re-ID models and train them. To efficiently train Diff Attention Framework, we also propose two training strategies: Joint Training and Separate Training. Our framework and strategies have achieved excellent performance on Market-1501, CUHK03, and MSMT17 datasets. Our code will be made publicly available at https://github.com/linxin98/ReID-DiffAttention .},
  archive      = {J_CVIU},
  author       = {Xin Lin and Li Zhu and Shuyu Yang and Yaxiong Wang},
  doi          = {10.1016/j.cviu.2023.103623},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103623},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Diff attention: A novel attention scheme for person re-identification},
  volume       = {228},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MATTE: Multi-task multi-scale attention. <em>CVIU</em>,
<em>228</em>, 103622. (<a
href="https://doi.org/10.1016/j.cviu.2023.103622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a general method for learning task and scale based attention representations in Multi-Task Learning (MTL) for vision. It relies on learning and maintaining cross-task and cross-scale representations of visual information, whose interaction contributes to a symmetrical improvement across the entire task pool. Apart from learning data representations, we additionally optimize for the most beneficial interaction between tasks and their representations at different scales. Our method adds an attention modulated feature as residual information to the processing of each scale stage within the model, including the final layer of task outputs. We empirically show the effectiveness of our method through experiments with current multi-modal and multi-scale architectures on diverse MTL datasets. We evaluate MATTE on high and low level vision MTL problems, against MTL and single task learning (STL) counterparts. For all experiments we report solid performance improvements in both qualitative and quantitative performance.},
  archive      = {J_CVIU},
  author       = {Gjorgji Strezoski and Nanne van Noord and Marcel Worring},
  doi          = {10.1016/j.cviu.2023.103622},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103622},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MATTE: Multi-task multi-scale attention},
  volume       = {228},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continual learning on 3D point clouds with random compressed
rehearsal. <em>CVIU</em>, <em>228</em>, 103621. (<a
href="https://doi.org/10.1016/j.cviu.2023.103621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary deep neural networks offer state-of-the-art results when applied to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds are an important data type for the precise modeling of three-dimensional environments, but effective processing of this type of data proves to be challenging. In the world of large, heavily-parameterized network architectures and continuously-streamed data, there is an increasing need for machine learning models that can be trained on additional data. Unfortunately, currently available models cannot fully leverage training on additional data without losing their past knowledge. Combating this phenomenon, called catastrophic forgetting , is one of the main objectives of continual learning. Continual learning for deep neural networks has been an active field of research, primarily in 2D computer vision, natural language processing, reinforcement learning, and robotics. However, in 3D computer vision, there are hardly any continual learning solutions specifically designed to take advantage of point cloud structure. This work proposes a novel neural network architecture capable of continual learning on 3D point cloud data. We utilize point cloud structure properties for preserving a heavily compressed set of past data. By using rehearsal and reconstruction as regularization methods of the learning process, our approach achieves a significant decrease of catastrophic forgetting compared to the existing solutions on several most popular point cloud datasets considering two continual learning settings: when a task is known beforehand, and in the challenging scenario of when task information is unknown to the model.},
  archive      = {J_CVIU},
  author       = {Maciej Zamorski and Michał Stypułkowski and Konrad Karanowski and Tomasz Trzciński and Maciej Zięba},
  doi          = {10.1016/j.cviu.2023.103621},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103621},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Continual learning on 3D point clouds with random compressed rehearsal},
  volume       = {228},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain multi-style merge for image captioning.
<em>CVIU</em>, <em>228</em>, 103617. (<a
href="https://doi.org/10.1016/j.cviu.2022.103617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-style image captioning has attracted wide attention recently. Existing approaches mainly rely on style synthetics within a single domain. They cannot deal with multiple styles combination since various styles naturally cannot be included in a uniform dataset. This paper is the first one to investigate the cross-domain multi-style merge for image captioning. Specifically, we propose a novel image caption model with a multi-style gated transformer block to fit the cross-domain caption generation task. Conventional generative adversarial learning for language methods may suffer from the distribution distortion problem, since real datasets do not contain captions with style combinations. Therefore, we devise a multi-stage self-learning framework for the proposed image caption model to exploit real corpus with pseudo styles gradually. Comprehensive experiments and ablation studies demonstrate the effectiveness of our proposed method on the multi-style merge for image captioning.},
  archive      = {J_CVIU},
  author       = {Yiqun Duan and Zhen Wang and Yi Li and Jingya Wang},
  doi          = {10.1016/j.cviu.2022.103617},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103617},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cross-domain multi-style merge for image captioning},
  volume       = {228},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Foreground discovery in streaming videos with dynamic
construction of content graphs. <em>CVIU</em>, <em>227</em>, 103620. (<a
href="https://doi.org/10.1016/j.cviu.2022.103620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of unknown foreground discovery in image streaming scenarios, where no prior information about the dynamic scene is assumed. Contrary to existing co-segmentation principles where the entire dataset is given, in streams new information emerges as content appears and disappears continually. Any object classes to be observed in the scene are unknown, therefore no detection model can be trained for the specific class set. We also assume there is no available repository of trained features from convolutional neural nets, i.e., transfer learning is not applicable. We focus on the progressive discovery of foreground, which may or may not correspond to contextual objects of interest, depending on the camera trajectory, or, in general, the perceived motion. Without any form of supervision, we construct in a bottom up fashion dynamic graphs that capture region saliency and relative topology. Such graphs are continually updated over time, and along with occlusion information, as fundamental property of the foreground–background relationship, foreground is computed for each frame of the stream. We validate our method using indoor and outdoor scenes of varying complexity with respect to content, objects motion, camera trajectory, and occlusions.},
  archive      = {J_CVIU},
  author       = {Sepehr Farhand and Gavriil Tsechpenakis},
  doi          = {10.1016/j.cviu.2022.103620},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103620},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Foreground discovery in streaming videos with dynamic construction of content graphs},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating structural prior for depth regularization in
shape from focus. <em>CVIU</em>, <em>227</em>, 103619. (<a
href="https://doi.org/10.1016/j.cviu.2022.103619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth map reconstructed through passive or active methods usually has noise and exhibits poor shape quality. In the case of shape from focus (SFF), improvement techniques can be divided mainly into two categories: one, which enhances the image focus volume, and second, which tries to refine the depth map. In both of these categories, no additional information about the shape of the object is taken into consideration, and hence, these techniques usually provide little improvement in the depth map. In this paper, we propose to incorporate a structural prior that helps to maintain the structural details in the recovered depth map. For this, we devise variations in the all-in-focus (AIF) image as the structural prior. By exploiting guided filtering, we improve the initial depth map through weighted least squares (WLS) based regularization for which our prior provides efficient weights. Experiments have been conducted on a variety of synthetic and real image sequences, and the results demonstrate that the proposed structural prior improves the accuracy of depth reconstruction.},
  archive      = {J_CVIU},
  author       = {Usman Ali and Ik Hyun Lee and Muhammad Tariq Mahmood},
  doi          = {10.1016/j.cviu.2022.103619},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103619},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Incorporating structural prior for depth regularization in shape from focus},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph convolutional networks based on manifold learning for
semi-supervised image classification. <em>CVIU</em>, <em>227</em>,
103618. (<a href="https://doi.org/10.1016/j.cviu.2022.103618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to a huge volume of information in many domains, the need for classification methods is imperious. In spite of many advances, most of the approaches require a large amount of labeled data, which is often not available, due to costs and difficulties of manual labeling processes. In this scenario, unsupervised and semi-supervised approaches have been gaining increasing attention. The GCNs (Graph Convolutional Neural Networks) represent a promising solution since they encode the neighborhood information and have achieved state-of-the-art results on scenarios with limited labeled data . However, since GCNs require graph-structured data, their use for semi-supervised image classification is still scarce in the literature. In this work, we propose a novel approach, the Manifold-GCN, based on GCNs for semi-supervised image classification. The main hypothesis of this paper is that the use of manifold learning to model the graph structure can further improve the GCN classification. To the best of our knowledge, this is the first framework that allows the combination of GCNs with different types of manifold learning approaches for image classification. All manifold learning algorithms employed are completely unsupervised, which is especially useful for scenarios where the availability of labeled data is a concern. A broad experimental evaluation was conducted considering 5 GCN models, 3 manifold learning approaches, 3 image datasets, and 5 deep features. The results reveal that our approach presents better accuracy than traditional and recent state-of-the-art methods with very efficient run times for both training and testing.},
  archive      = {J_CVIU},
  author       = {Lucas Pascotti Valem and Daniel Carlos Guimarães Pedronette and Longin Jan Latecki},
  doi          = {10.1016/j.cviu.2022.103618},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103618},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Graph convolutional networks based on manifold learning for semi-supervised image classification},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-granularity pseudo-label collaboration for
unsupervised person re-identification. <em>CVIU</em>, <em>227</em>,
103616. (<a href="https://doi.org/10.1016/j.cviu.2022.103616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (ReID) aims at training a model through using fully unlabeled training images. Most successful approaches combine clustering-based pseudo-label prediction with ReID model learning and perform the two steps in an alternating fashion. However, there are some person images, which have common appearance but different identities or have same identity but large intra-variations, and they are prone to be assigned wrong pseudo-labels. Existing methods typically infer person image pseudo-labels in a single pseudo-label space, which is insufficient due to losing the accurate labels of these hard images. This limits model’s power to cope with varying changes. To address this problem, we propose a Multi-granularity Pseudo-label Collaboration (MPC) method for unsupervised person ReID. Firstly, a multi-granularity pseudo-label prediction (MPP) method is proposed to predict the pseudo-labels of person images. MPP generates multiple pseudo-label spaces with each one inferring the person image pseudo-labels under a different granularity at cluster size. Secondly, the multiple pseudo-label spaces are applied to learn a mixture of experts (MoE) model. Each expert model is supervised by the pseudo-labels in a specific pseudo-label space, enabling MoE model to handle different variations. The output of these expert models are fused critically by a bilateral attention aggregation (BAA) to generate aggregation image features . Meanwhile, a label mix-up method is designed for refining pseudo-labels. It fuses labels across different pseudo-label spaces based on label co-occurrence to promote aggregation feature learning . We extensively evaluate the proposed MPC method under both image-based and video-based unsupervised person ReID settings. MPC significantly improves the current baseline method by a margin of 5.4\% and 3.7\% in mAP, and 3.4\% and 3.9\% in Rank1 accuracy on the image-based datasets Market-1501 and DukeMTMC-ReID, respectively. In particular, MPC method achieves state-of-the-art performance on video-based datasets with mAP of 71.4\% and 87.3\% on MARS and DukeMTMC-VideoReID, respectively.},
  archive      = {J_CVIU},
  author       = {Xiaobao Li and Qingyong Li and Fengjiao Liang and Wen Wang},
  doi          = {10.1016/j.cviu.2022.103616},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103616},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-granularity pseudo-label collaboration for unsupervised person re-identification},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised video anomaly detection based on
multi-timescale trajectory prediction. <em>CVIU</em>, <em>227</em>,
103615. (<a href="https://doi.org/10.1016/j.cviu.2022.103615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection refers to the identification of events that differ from normal behavior. Most of the commonly used methods currently use the reconstruction method based on appearance features. However, appearance features are unstructured signals that are sensitive to noise, and the redundant information contained will also increase the burden of distinguishing signals from noise during training. Reconstruction methods try to minimize the reconstruction error of the training data but cannot guarantee that the reconstruction error of anomalous events is large. From this, we propose an unsupervised video anomaly detection algorithm based on multi-timescale trajectory prediction. We use the object tracking network to detect and track pedestrians in the scene and send them to the multi-timescale trajectory prediction and velocity calculation modules for training. Due to the different motion durations, we add a multi-timescale mechanism to predict pedestrian trajectories and introduce step signals in digital signal processing for trajectory subsequence segmentation. During the testing of abnormal videos, the irregular motion behavior of pedestrians cannot be predicted by the normal model and will result in higher trajectory outliers. Similarly, the velocity calculation module constrains and calculates pedestrian velocities at different camera views, and events that differ from normal velocity can be detected under the dual constraints of space and motion (time). Compared with state-of-the-art methods and other anomalous event detection methods, the proposed model has certain advantages in frame-level AUC.},
  archive      = {J_CVIU},
  author       = {Qiyue Sun and Yang Yang},
  doi          = {10.1016/j.cviu.2022.103615},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103615},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised video anomaly detection based on multi-timescale trajectory prediction},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent context-aware multi-stage network for single image
deraining. <em>CVIU</em>, <em>227</em>, 103612. (<a
href="https://doi.org/10.1016/j.cviu.2022.103612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image rain streak removal is extremely necessary since rainy images can seriously affect many computer vision systems. In this paper, we propose a novel recurrent context-aware multi-stage network (ReCMN) for image rain removal that gradually predicts clean derained results. Specifically, the ReCMN introduces a multi-stage strategy to perform contextual relationship modeling. Firstly, we use the densely residual extraction block (DREB) to guide feature extraction. Then, a multi-scale context aggregation block (MCAB) is designed to utilize the long-distance dependencies and multiple scale features, which can fuse features of different levels to fully exploit contextual information. Finally, we develop a parallel attention block (PAB) to capture the channel and spatial information and only pass effective feature representation. Experimental results demonstrate that our method outperforms several state-of-the-art methods, based on both synthetic datasets and real-world rainy images.},
  archive      = {J_CVIU},
  author       = {Yuetong Liu and Rui Zhang and Yunfeng Zhang and Xiao Pan and Xunxiang Yao and Zhaorui Ni and Huijian Han},
  doi          = {10.1016/j.cviu.2022.103612},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103612},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Recurrent context-aware multi-stage network for single image deraining},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GSNNet: Group semantic-guided neighbor interaction network
for co-salient object detection. <em>CVIU</em>, <em>227</em>, 103611.
(<a href="https://doi.org/10.1016/j.cviu.2022.103611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The critical challenge for co-salient object detection (CoSOD) is to extract common saliency information from a group of relevant images. Most of the existing CoSOD methods do not fully explore the semantic commonality of co-salient objects, which can be strong guidance for collaborative feature learning , and do not take full advantage of rich hierarchical features of different layers, resulting in inferior performance. To this end, we propose a G roup S emantic-guided N eighbor interaction network ( GSNNet ) for co-salient object detection. Specifically, the proposed network contains the group semantic module (GSM), neighbor interaction module (NIM), and feature enhancement module (FEM). The network first learns semantic consensus from a group of relevant images by the GSM, which uses the reverse guidance strategy and the group-wise combination strategy to distill the group semantic cues from the forward and complementary features. With the powerful guidance of group semantic, the NIM is employed to conduct the neighbor feature interaction of adjacent layers to excavate the contextual information and enhance feature representation. Then the FEM is adopted to refine the critical cues by the attention mechanism , which enhances the compactness of feature representation. The proposed GSNNet is evaluated on three challenging CoSOD benchmark datasets using four widely-used metrics, which demonstrates that our proposed method is superior to the other twelve cutting-edge methods for co-salient object detection.},
  archive      = {J_CVIU},
  author       = {Yanliang Ge and Qiao Zhang and Tian-Zhu Xiang and Cong Zhang and Jing Zhang and Hongbo Bi},
  doi          = {10.1016/j.cviu.2022.103611},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103611},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GSNNet: Group semantic-guided neighbor interaction network for co-salient object detection},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse coding with morphology segmentation and multi-label
fusion for hyperspectral image super-resolution. <em>CVIU</em>,
<em>227</em>, 103603. (<a
href="https://doi.org/10.1016/j.cviu.2022.103603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) super-solution to reconstruct high spatial resolution HSIs has attracted increasing interest in recent years. In this paper, we propose a HSI super-resolution framework based on sparse coding with morphology segmentation and multi-label fusion (MSML), which is composed of four stages: (1) A spectral dictionary is learned by the online dictionary learning approach from the given HSI; (2) The morphology segmentation technique is introduced to divide each multispectral image (MSI) band into a series of regions associated with a label map; (3) A weighted voting based multi-label fusion model is constructed to combine multiple label maps from MSI bands to determine 3-D patches; (4) A sparse coding model is built to calculate sparse coefficients of 3-D patches that are used for the HSI super-solution. Compared with traditional sparse representation based algorithms, the novel MSML method can more fully utilize the local spatial information of the MSI to realize the super-resolution, relying on the sparse coding on unfixed-size patches adaptively obtained by the morphology segmentation and multi-label fusion. The Indian Pines, Salinas, Botswana, and Pavia University datasets are used to evaluate the performance of our method. Experimental results indicate that the MSML achieves better super-resolution performance in contrast to state-of-the-art algorithms.},
  archive      = {J_CVIU},
  author       = {Changda Xing and Meiling Wang and Yuhua Cong and Zhisheng Wang and Chaowei Duan and Yiliu Liu},
  doi          = {10.1016/j.cviu.2022.103603},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103603},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Sparse coding with morphology segmentation and multi-label fusion for hyperspectral image super-resolution},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised sound localization via iterative contrastive
learning. <em>CVIU</em>, <em>227</em>, 103602. (<a
href="https://doi.org/10.1016/j.cviu.2022.103602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound localization aims to find the source of the audio signal in the visual scene. However, it is labor-intensive to annotate the correlations between the signals sampled from the audio and visual modalities , thus making it difficult to supervise the learning of a machine for this task. In this work, we propose an iterative contrastive learning framework that requires no data annotations. At each iteration, the proposed method takes the (1) localization results in images predicted in the previous iteration, and (2) semantic relationships inferred from the audio signals as the pseudo-labels. We then use the pseudo-labels to learn the correlation between the visual and audio signals sampled from the same video (intra-frame sampling) as well as the association between those extracted across videos (inter-frame relation). Our iterative strategy gradually encourages the localization of the sounding objects and reduces the correlation between the non-sounding regions and the reference audio. Quantitative and qualitative experimental results demonstrate that the proposed framework performs favorably against existing unsupervised and weakly-supervised methods on the sound localization task.},
  archive      = {J_CVIU},
  author       = {Yan-Bo Lin and Hung-Yu Tseng and Hsin-Ying Lee and Yen-Yu Lin and Ming-Hsuan Yang},
  doi          = {10.1016/j.cviu.2022.103602},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103602},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised sound localization via iterative contrastive learning},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiDARTouch: Monocular metric depth estimation with a
few-beam LiDAR. <em>CVIU</em>, <em>227</em>, 103601. (<a
href="https://doi.org/10.1016/j.cviu.2022.103601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based depth estimation is a key feature in autonomous systems , which often relies on a single camera or several independent ones. In such a monocular setup, dense depth is obtained with either additional input from one or several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which suffer from scale-ambiguity and infinite-depth problems. In this paper, we propose a new alternative of densely estimating metric depth by combining a monocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of today’s automotive-grade mass-produced laser scanners . Inspired by recent self-supervised methods, we introduce a novel framework, called LiDARTouch , to estimate dense depth maps from monocular images with the help of “touches” of LiDAR, i.e., without the need for dense ground-truth depth. In our setup, the minimal LiDAR input contributes on three different levels: as an additional model’s input, in a self-supervised LiDAR reconstruction objective function, and to estimate changes of pose (a key component of self-supervised depth estimation architectures). Our LiDARTouch framework achieves new state of the art in self-supervised depth estimation on the KITTI dataset, thus supporting our choices of integrating the very sparse LiDAR signal with other visual features. Moreover, we show that the use of a few-beam LiDAR alleviates scale ambiguity and infinite-depth issues that camera-only methods suffer from. We also demonstrate that methods from the fully-supervised depth-completion literature can be adapted to a self-supervised regime with a minimal LiDAR signal.},
  archive      = {J_CVIU},
  author       = {Florent Bartoccioni and Éloi Zablocki and Patrick Pérez and Matthieu Cord and Karteek Alahari},
  doi          = {10.1016/j.cviu.2022.103601},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103601},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature reconstruction and metric based network for few-shot
object detection. <em>CVIU</em>, <em>227</em>, 103600. (<a
href="https://doi.org/10.1016/j.cviu.2022.103600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the object detection task, deep learning-based methods always need a large amount of annotated training data. However, annotating a large number of images is labor-intensive. In order to reduce the dependency of expensive annotations, we propose a novel end-to-end feature reconstruction and metric based network for few-shot object detection (FM-FSOD). FM-FSOD integrates metric learning and meta-learning to tackle the few-shot object detection task. FM-FSOD is a class-agnostic detection model that can accurately recognize novel categories without fine-tuning on novel categories. Specifically, to quickly learn the characteristics of novel categories, we propose a meta-representation module (MR module) to learn from intra-class mean prototypes and acquire the ability to reconstruct high-level features with the meta-learning method. To further conduct the similarity of features between support prototypes and query ROI features, we propose Pearson metric module (PR module), which serves as a classifier. Compared with the previous standard cosine distance module, the PR module enables the model to acquire robust ability for large bias features. We have conducted extensive experiments on benchmark datasets FSOD, MS COCO, and PASCAL VOC to demonstrate the feasibility and efficiency of our model. Comparing with the previous methods, FM-FSOD obtains comparable results.},
  archive      = {J_CVIU},
  author       = {Yuewen Li and Wenquan Feng and Shuchang Lyu and Qi Zhao},
  doi          = {10.1016/j.cviu.2022.103600},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103600},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Feature reconstruction and metric based network for few-shot object detection},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-free adversarial defense: Distance-based optimization
for model robustness without adversarial training. <em>CVIU</em>,
<em>227</em>, 103599. (<a
href="https://doi.org/10.1016/j.cviu.2022.103599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although convolutional neural networks (CNNs) have advanced to demonstrate superior performance in image classification tasks that often surpass human capability, the feature space of CNNs, which are trained using a typical training method, is limited by the smaller-than-expected inter-class variances. Consequently, CNNs are prone to misclassifying adversarial examples with high confidence, and the difference between an adversarial example and a normal input is indistinguishable by human beings. To alleviate this problem, we propose a training methodology that defends against adversarial attacks through a constraint that applies a class-specific differentiation to the feature space of CNNs. The proposed methodology first forces the feature representations that corresponding to each class to be localized on the hypersphere surface with a particular radius. The forced representation is then trained to be located as close to the center of the hypersphere as possible, resulting in feature representations with a small intra-class variance and large inter-class variances. The experimental results reveal that the proposed two-step training method enhances defense performance by 17.1\%p and demonstrates a training speed of up to 30 times faster than the existing distance-based adversarial defense methodology. The code is available at: https://github.com/lepoeme20/cost-free-adversarial-defense},
  archive      = {J_CVIU},
  author       = {Seungwan Seo and Yunseung Lee and Pilsung Kang},
  doi          = {10.1016/j.cviu.2022.103599},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103599},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cost-free adversarial defense: Distance-based optimization for model robustness without adversarial training},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrared and visible image fusion using a guiding network to
leverage perceptual similarity. <em>CVIU</em>, <em>227</em>, 103598. (<a
href="https://doi.org/10.1016/j.cviu.2022.103598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In infrared (IR) and visible image fusion, visual appearance of fused images produced by an end-to-end fusion network relies on a loss function that defines the desired properties of the fusion results. The previous approaches mainly focus on measuring the similarity of an output image with respective IR and visible images using pixel-wise loss functions such as mean square error or structural similarity. This can lead, however, to preserve only local information in the fused images, and it may lose semantic information in the scene. In this work, we propose a new joint loss function for the image fusion, which considers visual quality and subsequent applications simultaneously. Our joint loss function guides the fusion network by transferring the knowledge of a pre-trained network to leverage perceptual similarity. The contrast loss function serves to prevent the reduction of global and local image contrasts in the fused image. The conventional data fidelity is also considered in the proposed function. Then, the joint loss function is used to train our U-Net like fusion network, a hybrid of convolutional and transformer blocks. Experimental results show that the proposed method preserves detailed scene structure from the both of source images and demonstrates the better quantitative results. Furthermore, the proposed fusion schemes can be used for combining multi-exposure and multi-focus image pairs. To show the further effectiveness of IR and visible image fusion, we apply the results of fusion methods for downstream tasks so that the proposed method improves the cases of only using original modalities.},
  archive      = {J_CVIU},
  author       = {Jun-Hyung Kim and Youngbae Hwang},
  doi          = {10.1016/j.cviu.2022.103598},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103598},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Infrared and visible image fusion using a guiding network to leverage perceptual similarity},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning representational invariances for data-efficient
action recognition. <em>CVIU</em>, <em>227</em>, 103597. (<a
href="https://doi.org/10.1016/j.cviu.2022.103597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a ubiquitous technique for improving image classification when labeled data is scarce. Constraining the model predictions to be invariant to diverse data augmentations effectively injects the desired representational invariances to the model (e.g., invariance to photometric variations) and helps improve accuracy. Compared to image data, the appearance variations in videos are far more complex due to the additional temporal dimension. Yet, data augmentation methods for videos remain under-explored. This paper investigates various data augmentation strategies that capture different video invariances, including photometric, geometric, temporal, and actor/scene augmentations. When integrated with existing semi-supervised learning frameworks, we show that our data augmentation strategy leads to promising performance on the Kinetics-100/400, Mini-Something-v2, UCF-101, and HMDB-51 datasets in the low-label regime. We also validate our data augmentation strategy in the fully supervised setting and demonstrate improved performance.},
  archive      = {J_CVIU},
  author       = {Yuliang Zou and Jinwoo Choi and Qitong Wang and Jia-Bin Huang},
  doi          = {10.1016/j.cviu.2022.103597},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103597},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning representational invariances for data-efficient action recognition},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-object tracking with robust object regression and
association. <em>CVIU</em>, <em>227</em>, 103586. (<a
href="https://doi.org/10.1016/j.cviu.2022.103586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking-by-regression is a new paradigm for online Multi-Object Tracking (MOT). It unifies detection and tracking into a single network by associating targets through regression, significantly reducing the complexity of data association . However, owing to noisy features from nearby occlusions and distractors , the regression is vulnerable and unaware of the inter-object occlusions and intra-class distractors. Thus the regressed bounding boxes can be wrongly suppressed or easily drift. Meanwhile, the commonly used bounding box-based post-processing is unable to remedy false negatives and false assignments caused by regression. To address these challenges, we present to leverage regression tubes as input for the regression-based tracker, which provides spatial–temporal information to enhance the tracking performance. Specially, we propose a novel tube re-localization strategy that obtains robust regressions and recovers missed targets. A tube-based NMS (T-NMS) strategy to manage the regressions at the tube level is also proposed, including a tube IoU (T-IoU) scheme for measuring positional relation and tube re-scoring (T-RS) to evaluate the quality of candidate tubes. Finally, a tube re-assignment strategy is further employed for robust cost measurement and to revise false assignments using motion cues. We evaluate our method on benchmarks, including MOT16, MOT17, and MOT20. The results show that our method can significantly improve the baseline, mitigate the challenges of the regression-based tracker, and achieve very competitive tracking performance.},
  archive      = {J_CVIU},
  author       = {Yi-Fan Li and Hong-Bing Ji and Xi Chen and Yu-Kun Lai and Yong-Liang Yang},
  doi          = {10.1016/j.cviu.2022.103586},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103586},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-object tracking with robust object regression and association},
  volume       = {227},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate depth image generation via overfit training of
point cloud registration using local frame sets. <em>CVIU</em>,
<em>226</em>, 103588. (<a
href="https://doi.org/10.1016/j.cviu.2022.103588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate three-dimensional perception is a fundamental task in various computer vision applications. Recently, commercial RGB-depth (RGB-D) cameras have been widely adopted as single-view depth-sensing devices owing to their efficient depth-sensing abilities. However, the depth quality of most RGB-D sensors remains insufficient owing to the inherent noise from a single-view environment. Several recent studies have focused on the single-view depth enhancement based on modern deep-learning technologies. The approaches typically train networks using high-quality benchmark depth datasets, which indicates that the quality of the supervised depth dataset is a fundamental factor for the depth enhancement system; however, such high-quality depth datasets are difficult to obtain. In this study, we developed a novel method for high-quality depth image generation based on an RGB-D stream dataset. First, we defined consecutive depth frames in a local spatial region as a local frame set. Then, the depth frames were aligned to a certain frame in the local frame set using an unsupervised point cloud registration scheme. The registration parameters were trained based on an overfit-training scheme, which was primarily used to generate an enhanced depth image for each frame set. The final depth dataset was constructed using multiple local frame sets, and each local frame set was trained independently. The primary advantage of this study is that a high-quality depth dataset can be constructed under various scanning environments using only the RGB-D stream dataset. Moreover, our proposed method can be used as a new benchmark depth dataset for accurate performance evaluations. We evaluated our dataset on previously benchmarked depth datasets and demonstrated that our method is superior to state-of-the-art depth enhancement frameworks.},
  archive      = {J_CVIU},
  author       = {Jiwan Kim and Minchang Kim and Yeong-Gil Shin and Minyoung Chung},
  doi          = {10.1016/j.cviu.2022.103588},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103588},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Accurate depth image generation via overfit training of point cloud registration using local frame sets},
  volume       = {226},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A data augmentation framework by mining structured features
for fake face image detection. <em>CVIU</em>, <em>226</em>, 103587. (<a
href="https://doi.org/10.1016/j.cviu.2022.103587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For fake face image detection, most existing detectors exploit local artifacts, ignoring the mining of structured forgery clues existed in global images, which greatly constrains detection performance. In this work, we verify the importance of structured forgery clues for fake face image detection, and present a new data augmentation framework called Mining Structured Features (MSF) to promote the convolutional neural network (CNN) based detector. Specifically, MSF generates a position-sensitive artifact map, which is exploited to divide a candidate face image into strong correlation (SC) and weak correlation (WC) regions. By dynamically erasing some positions in SC and WC regions during the training process, MSF can promote the robustness of the detector to the above regions. In essence, MSF expands the attention range of the detector and fully mines the structured features from global images. MSF plays an auxiliary role, which can be seamlessly integrated into existing CNN-based detectors for end-to-end training. Extensive experimental results on four public datasets including HFFD, FF++, DFDC and Celeb-DF show that the detectors trained with the guidance of MSF can mine more useful clues distributed in fake face images to improve detection accuracies, achieving better results than state-of-the-art works. Our code will be available at https://github.com/EricGzq/MSF .},
  archive      = {J_CVIU},
  author       = {Zhiqing Guo and Gaobo Yang and Dewang Wang and Dengyong Zhang},
  doi          = {10.1016/j.cviu.2022.103587},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103587},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A data augmentation framework by mining structured features for fake face image detection},
  volume       = {226},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A global generalized maximum coverage-based solution to the
non-model-based view planning problem for object reconstruction.
<em>CVIU</em>, <em>226</em>, 103585. (<a
href="https://doi.org/10.1016/j.cviu.2022.103585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robotic active vision system can automatically perform the reconstruction task, but the core problem is the non-model-based view planning problem (NMVPP). Unlike the model-based one that requires a priori known 3D models of objects or environments, the NMVPP faces an unknown environment, making it more challenging. This is usually solved by an iterative greedy algorithm based on information gain (i.e., quantitative statistics of uncertainty in the environment). For better surface coverage, we seek a global optimization solution to overcome the bad performance of greedy selections. We first naturally generalized the model-based set covering optimization (SCO) problem, but it performs poorly because of uncertainty in the environment. Therefore, we present a generalized maximum coverage (GMC)-based solution to the NMVPP, which generates an optimal sequence of views capable of sensing all valuable areas of an unknown object placed in a 3D environment. Given the uncertainties in the environment, the goal is to maximize the total information of the optimal view subset, making NMVPP a class of GMC problems. For the sake of reconstruction efficiency, we introduce an additional subjection of the GMC model to balance the movement cost and surface coverage. We compared the results of our methods with state-of-the-art algorithms, both on a set of 3D objects and in the real world, and the results showed that our methods could outperform the others in most cases. Thus, the proposed active vision system can automatically complete an unknown object reconstruction task with higher coverage and competitive running time.},
  archive      = {J_CVIU},
  author       = {Sicong Pan and Hui Wei},
  doi          = {10.1016/j.cviu.2022.103585},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103585},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A global generalized maximum coverage-based solution to the non-model-based view planning problem for object reconstruction},
  volume       = {226},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor based completion meets adversarial learning: A
win–win solution for change detection on unseen videos. <em>CVIU</em>,
<em>226</em>, 103584. (<a
href="https://doi.org/10.1016/j.cviu.2022.103584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreground segmentation is an essential processing phase in several change detection-based applications. Classical foreground segmentation is highly dependent on the accuracy of the estimated background model and the procedures followed to subtract such model from the original frame. Obtaining good foreground masks via background subtraction remains a challengeable task where limitations such as incomplete foreground objects and foreground misdetection are presented. Due to their recent successes, deep learning approaches have been widely used recently to tackle the challenges related to foreground segmentation. However, recent studies have pointed out the fact that deep learning approaches are highly dependent on the followed training protocol where different protocols lead to clearly different results. Furthermore, several extensive experiments have shown the poor performances of deep learning approaches when processing “unseen videos”. Therefore, in this paper, we introduce a Generative adversarial network (GAN) based foreground enhancement framework that accepts multiple images as inputs. The GAN is designed and trained to refine initial foreground masks estimated via a hand-crafted background subtraction instead of generating them from scratch. The background that is fed into the network is initialized beforehand via a spatiotemporal slice-based singular value decomposition (SVD) and well updated when changes are present in the scene. The segmentation performance is evaluated qualitatively and quantitatively following scene-dependent and scene-independent scenarios, and the estimated results are compared with the existing state-of-the-art methods. From the obtained experimental results, it is evident that the proposed framework shows significant improvement in terms of F-measure and robust performance in the case of unseen scenarios.},
  archive      = {J_CVIU},
  author       = {Ibrahim Kajo and Mohamed Kas and Yassine Ruichek and Nidal Kamel},
  doi          = {10.1016/j.cviu.2022.103584},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103584},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Tensor based completion meets adversarial learning: A win–win solution for change detection on unseen videos},
  volume       = {226},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Periocular biometrics and its relevance to partially masked
faces: A survey. <em>CVIU</em>, <em>226</em>, 103583. (<a
href="https://doi.org/10.1016/j.cviu.2022.103583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periocular is one of the promising biometric traits for human recognition. It encompasses a surrounding area of eyes that includes eyebrows, eyelids, eyelashes, eye-folds, eyebrows, eye shape, and skin texture. Its relevance is more emphasize during the COVID-19 pandemic due to the masked faces. So, this article presents a detailed review of periocular biometrics to understand its current state. The paper first discusses the various face and periocular techniques, specially designed to recognize humans wearing a face mask. Then, different aspects of periocular biometrics are reviewed: (a) the anatomical cues present in the periocular region useful for recognition, (b) the various feature extraction and matching techniques developed, (c) recognition across different spectra, (d) fusion with other biometric modalities (face or iris), (e) recognition on mobile devices , (f) its usefulness in other applications, (g) periocular datasets, and (h) competitions organized for evaluating the efficacy of this biometric modality. Finally, various challenges and future directions in the field of periocular biometrics are presented.},
  archive      = {J_CVIU},
  author       = {Renu Sharma and Arun Ross},
  doi          = {10.1016/j.cviu.2022.103583},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103583},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Periocular biometrics and its relevance to partially masked faces: A survey},
  volume       = {226},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
