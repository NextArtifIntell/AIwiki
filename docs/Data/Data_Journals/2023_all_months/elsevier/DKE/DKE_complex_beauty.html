<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dke---76">DKE - 76</h2>
<ul>
<li><details>
<summary>
(2023). Mixed emotion extraction analysis and visualisation of
social media text. <em>DKE</em>, <em>148</em>, 102220. (<a
href="https://doi.org/10.1016/j.datak.2023.102220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of social media and accelerated development of artificial intelligence , sentiment analysis is regarded as an important way to help enterprises understand user needs and conduct brand monitoring . It can also assist businesses in making data-driven decisions about product development, marketing strategies, and customer service. However, as social media information continues to grow exponentially, and industry demands increase, sentiment analysis should no longer be limited to fundamental polarity classification of positive, neutral, and negative. Instead, it should move to more precise classification of emotions. Therefore, in this paper, we expand sentiment analysis to analysis of eight different emotions based on Plutchik&#39;s wheel of emotions, and define it as a multi-label classification task to identify complex and mixed emotions in text. We achieved an overall precision of 0.7958 for the eight emotions multi-label classification based on the attention-based bidirectional long short-term memory with convolution layer (AC-BiLSTM) model on the SemEval-2018 dataset. In addition, we proposed the introduction of the NRC emotion lexicon and emotion correlation constraints to optimise the emotion classification results . This ultimately increased the overall precision to 0.8228 demonstrating the effectiveness of our approach. Finally, we store and visualise the emotion analysis results in a graph structure, in order to achieve deductibility and traceability of emotions.},
  archive      = {J_DKE},
  author       = {Yuming Li and Johnny Chan and Gabrielle Peko and David Sundaram},
  doi          = {10.1016/j.datak.2023.102220},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102220},
  shortjournal = {Data Knowl. Eng.},
  title        = {Mixed emotion extraction analysis and visualisation of social media text},
  volume       = {148},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Legal powers, subjections, disabilities, and immunities:
Ontological analysis and modeling patterns. <em>DKE</em>, <em>148</em>,
102219. (<a href="https://doi.org/10.1016/j.datak.2023.102219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of dependable information systems in legal contexts requires a precise understanding of the subtleties of the underlying legal phenomena. According to a modern understanding in the philosophy of law, much of these phenomena are relational in nature. In this paper, we employ a theoretically well-grounded legal core ontology (UFO-L) to conduct an ontological analysis focused on fundamental legal relations, namely, the power–subjection and the disability–immunity relations. We show that in certain cases, power–subjection relations are primitive in the sense that by means of institutional acts other legal relations can be generated from them. Examples include relations of rights and duties, permissions and no-rights, liberties, secondary power–subjection, etc. We further show that legal disabilities (and their correlative immunities) are key in constraining the reach of legal powers; together with powers, they form a comprehensive framework for representing the grounds of valid legal acts and to account for the life-cycle of the legal positions that powers create, alter, and possibly extinguish. As a contribution to the practice of conceptual modeling , and leveraging the result of our analysis, we propose conceptual modeling patterns for legal relations, which are then applied to model a real-world case in tax law.},
  archive      = {J_DKE},
  author       = {Cristine Griffo and João Paulo A. Almeida and João A.O. Lima and Tiago Prince Sales and Giancarlo Guizzardi},
  doi          = {10.1016/j.datak.2023.102219},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102219},
  shortjournal = {Data Knowl. Eng.},
  title        = {Legal powers, subjections, disabilities, and immunities: Ontological analysis and modeling patterns},
  volume       = {148},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the intelligence capability of smart homes: A
conceptual modeling approach. <em>DKE</em>, <em>148</em>, 102218. (<a
href="https://doi.org/10.1016/j.datak.2023.102218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet of Things technology, smart homes have gradually become an integral part of people’s lives, and the market share of smart homes has experienced a significant surge in recent years. As a result, there is a growing need for both producers and end-users to evaluate the intelligence of smart homes. While existing studies focus on simulating smart home environments, they do not provide an approach for automatically evaluating the intelligence of smart homes. In this study, we systematically establish a conceptual model of smart homes based on a wide range of smart home definitions, focusing on examining the factors that contribute to users feeling satisfied with their smart homes. Additionally, we proposed a framework for evaluating the intelligence capability of smart homes. To validate the effectiveness of our framework, we conducted an empirical study using an online user survey and collected 300 questionnaires about user ratings of three smart home suites. Our empirical results demonstrate that our framework is consistent with users’ perceptions of the intelligence level of smart homes. In order to further explore why users feel satisfied with their smart homes, we held a workshop with five participants. The results of our discussion showed a correlation between why users feel satisfied with their smart homes and the user needs that smart homes can fulfill.},
  archive      = {J_DKE},
  author       = {Di Wu and Weite Feng and Tong Li and Zhen Yang},
  doi          = {10.1016/j.datak.2023.102218},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102218},
  shortjournal = {Data Knowl. Eng.},
  title        = {Evaluating the intelligence capability of smart homes: A conceptual modeling approach},
  volume       = {148},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A FAIR catalog of ontology-driven conceptual models.
<em>DKE</em>, <em>147</em>, 102210. (<a
href="https://doi.org/10.1016/j.datak.2023.102210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-domain model catalogs serve as empirical sources of knowledge and insights about specific domains, about the use of a modeling language’s constructs, as well as about the patterns and anti-patterns recurrent in the models of that language crosscutting different domains. They may support domain and language learning, model reuse , knowledge discovery for humans, and reliable automated processing and analysis if built following generally accepted quality requirements for scientific data management. More specifically, not unlike scientific (meta)data, models should be shared according to the FAIR principles (Findability, Accessibility, Interoperability, and Reusability). In this paper, we report on the construction of a FAIR model catalog for Ontology-Driven Conceptual Modeling research, a trending paradigm lying at the intersection of conceptual modeling and ontology engineering in which the Unified Foundational Ontology (UFO) and OntoUML emerged among the most adopted technologies. The catalog, publicly available at https://w3id.org/ontouml-models , currently includes over one hundred and forty models, developed in a variety of contexts and domains.},
  archive      = {J_DKE},
  author       = {Tiago Prince Sales and Pedro Paulo F. Barcelos and Claudenir M. Fonseca and Isadora Valle Souza and Elena Romanenko and César Henrique Bernabé and Luiz Olavo Bonino da Silva Santos and Mattia Fumagalli and Joshua Kritz and João Paulo A. Almeida and Giancarlo Guizzardi},
  doi          = {10.1016/j.datak.2023.102210},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102210},
  shortjournal = {Data Knowl. Eng.},
  title        = {A FAIR catalog of ontology-driven conceptual models},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An extended taxonomy of advanced information visualization
and interaction in conceptual modeling. <em>DKE</em>, <em>147</em>,
102209. (<a href="https://doi.org/10.1016/j.datak.2023.102209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conceptual modeling is integral to computer science research and is widely adopted in industrial practices, e.g., business process and enterprise architecture management. Providing adequate and usable modeling tools is necessary to adopt modeling languages efficiently. Meta-modeling platforms provide a rich and mature set of functionalities for realizing state-of-the-art modeling tools. These tools, albeit their stability and rich set of features, often lack a modern look and feel considering ( i ) how they visualize the models, and ( i i ) how modelers interact with the models. Current web technologies enable much richer, advanced opportunities for visualizing and interacting with conceptual models. However, a structured and comprehensive overview of possible information visualization and interaction techniques linked to conceptual models and modeling tools must be established. This paper aims to fill this gap by presenting an extended taxonomy of advanced information visualization and interaction in conceptual modeling. We present a generic taxonomy that is afterward contextualized within the specific domain of conceptual modeling. The taxonomy serves orientation in the vast developing field of information visualization and interaction and hopefully sparks innovation if future modeling tool development.},
  archive      = {J_DKE},
  author       = {Dominik Bork and Giuliano De Carlo},
  doi          = {10.1016/j.datak.2023.102209},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102209},
  shortjournal = {Data Knowl. Eng.},
  title        = {An extended taxonomy of advanced information visualization and interaction in conceptual modeling},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self supervised learning and the poverty of the stimulus.
<em>DKE</em>, <em>147</em>, 102208. (<a
href="https://doi.org/10.1016/j.datak.2023.102208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diathesis alternations are the possible expressions of the arguments of verbs in different, systematically related subcategorization frames. Semantically similar verbs such as spill and spray can behave differently with respect to the alternations they can participate in. For example one can “spill/spray water on the plant”, but while one can “spray the plant with water”, it is odd to say “spill the plant with water”. “Spray” is a verb which can alternate between syntactic frames while “spill” is not alternating. How human speakers learn the difference between such verbs is not clearly understood, because the primary linguistic data (PLD) they receive does not appear sufficient to infer the knowledge required for adult competence. More generally the poverty of the stimulus (POS) hypothesis states that the PLD is not sufficient for a learner to infer full adult competence of language. That is, learning relies on prior constraints introduced by the language faculty. We tested state-of-the-art machine learning models trained by self supervision, and found some evidence that they could in fact learn the correct pattern of acceptability judgement in the locative alternation . However, we argued that this was partially a result of fine-tuning which introduced negative evidence into the learning data, which facilitated shortcut learning . Large language models (LLMs) cannot learn some linguistic facts from normal language data, but they can compensate to some extent by learning spurious correlated features when negative feedback is introduced during the training cycle.},
  archive      = {J_DKE},
  author       = {Csaba Veres and Jennifer Sampson},
  doi          = {10.1016/j.datak.2023.102208},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102208},
  shortjournal = {Data Knowl. Eng.},
  title        = {Self supervised learning and the poverty of the stimulus},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empiric validation of linguistic features in machine
learning models for fake news detection. <em>DKE</em>, <em>147</em>,
102207. (<a href="https://doi.org/10.1016/j.datak.2023.102207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diffusion of fake news is a growing problem with a high and negative social impact. There are several approaches to address the detection of fake news. This work focuses on a hybrid approach based on functional linguistic features and machine learning . There are several recent works with this approach. However, there are no clear guidelines on which linguistic features are most appropriate nor how to justify their use. Furthermore, many classification results are modest compared to recent advances in natural language processing . Our proposal considers 88 features organized in surface information, part of speech, discursive characteristics, and readability indices. On a 42 677 news database, we show that the classification results outperform previous work, even outperforming state-of-the-art techniques such as BERT , reaching 99.99\% accuracy. A proper selection of linguistic features is crucial for interpretability as well as the performance of the models. In this sense, our proposal contributes to the intentional selection of linguistic features, overcoming current technical issues. We identified 32 features that show differences between the type of news. The results are highly competitive in the classification and simple to implement and interpret.},
  archive      = {J_DKE},
  author       = {Eduardo Puraivan and René Venegas and Fabián Riquelme},
  doi          = {10.1016/j.datak.2023.102207},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102207},
  shortjournal = {Data Knowl. Eng.},
  title        = {An empiric validation of linguistic features in machine learning models for fake news detection},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decisive skyline queries for truly balancing multiple
criteria. <em>DKE</em>, <em>147</em>, 102206. (<a
href="https://doi.org/10.1016/j.datak.2023.102206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skyline queries have emerged as an increasingly popular tool for identifying a set of interesting objects that balance different user-specified criteria. Although in several applications the user aims to detect data objects that have values as good as possible in all specified criteria, skyline queries fail to identify only those objects. Instead, objects whose values are good in a subset of the given criteria are also included in the skyline set, even though they may take arbitrarily bad values in the remaining criteria. To alleviate this shortcoming, we study the decisive subspaces that express the semantics of skyline points and determine skyline membership. We propose a novel alternative query, called decisive skyline query , which retrieves a set of points that balance all specified criteria. We study two variants of the proposed query, the strict variant, which retrieves only the subset of skyline points that have the full data space as decisive subspace, and the relaxed variant, which imposes the decisive semantics in a more flexible way. Furthermore, we present pruning properties that accelerate the process of finding the decisive skyline set. Capitalizing on these pruning properties, we propose a novel efficient algorithm for computing decisive skyline points. Our experimental study, which employs both synthetic and real data sets for various experimental setups, demonstrates the efficiency and effectiveness of our algorithm, and shows that the newly proposed query is more intuitive and informative for the user.},
  archive      = {J_DKE},
  author       = {Akrivi Vlachou and Christos Doulkeridis and João B. Rocha-Junior and Kjetil Nørvåg},
  doi          = {10.1016/j.datak.2023.102206},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102206},
  shortjournal = {Data Knowl. Eng.},
  title        = {Decisive skyline queries for truly balancing multiple criteria},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modelling temporal goals in runtime goal models.
<em>DKE</em>, <em>147</em>, 102205. (<a
href="https://doi.org/10.1016/j.datak.2023.102205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving real-time agility and adaptation with respect to changing requirements in existing IT infrastructure can pose a complex challenge. We describe a goal-oriented approach to manage this complexity. We argue that a goal-oriented perspective can form an effective basis for devising and deploying responses to changed requirements at runtime. We offer an extended vocabulary of goal types by presenting two novel conceptions: differential goals and integral goals, which we formalize in both linear-time and branching-time settings. We describe goal lifecycles and interactions and the extended notion of context for the representation of rapidly changing, complex operating environments. We then illustrate the working of the approach by presenting a detailed scenario of adaptation in a Kubernetes setting, in the face of a Distributed Denial-of-Service (DDoS) attack.},
  archive      = {J_DKE},
  author       = {Rebecca Morgan and Simon Pulawski and Matt Selway and Aditya Ghose and Georg Grossmann and Wolfgang Mayer and Markus Stumptner and Ross Kyprianou},
  doi          = {10.1016/j.datak.2023.102205},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102205},
  shortjournal = {Data Knowl. Eng.},
  title        = {Modelling temporal goals in runtime goal models},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BT-CKBQA: An efficient approach for chinese knowledge base
question answering. <em>DKE</em>, <em>147</em>, 102204. (<a
href="https://doi.org/10.1016/j.datak.2023.102204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Base Question Answering (KBQA), as an increasingly essential application, can provide accurate responses to user queries. ensuring that users obtain relevant information and make decisions promptly. The deep learning-based approaches have achieved satisfactory QA results by leveraging the neural network models . However, these approaches require numerous parameters, which increases the workload of tuning model parameters. To address this problem, we propose BT-CKBQA, a practical and highly efficient approach incorporating B M25 and T emplate-based predicate mapping for CKBQA . Besides, a concept lattice based approach is proposed for summarizing the knowledge base, which can largely improve the execution efficiency of QA with little loss of performance. Concretely, BT-CKBQA leverages the BM25 algorithm and custom dictionary to detect the subject of a question sentence. A template-based predicate generation approach is then proposed to generate candidate predicates. Finally, a ranking approach is provided with the joint consideration of character similarity and semantic similarity for predicate mapping. Extensive experiments are conducted over the NLPCC-ICCPOL 2016 and 2018 KBQA datasets, and the experimental results demonstrate the superiority of the proposed approach over the compared baselines. Particularly, the averaged F1-score result of BT-CKBQA for mention detection is up to 98.25\%, which outperforms the best method currently available in the literature. For question answering, the proposed approach achieves superior results than most baselines with the F1-score value of 82.68\%. Compared to state-of-the-art baselines, the execution efficiency and performance of QA per unit time can be improved with up to 56.39\% and 44.06\% gains, respectively. The experimental results for the diversification of questions indicate that the proposed approach performs better for diversified questions than domain-specific questions. The case study over a constructed COVID-19 knowledge base illustrates the effectiveness and practicability of BT-CKBQA.},
  archive      = {J_DKE},
  author       = {Erhe Yang and Fei Hao and Jiaxing Shang and Xiaoliang Chen and Doo-Soon Park},
  doi          = {10.1016/j.datak.2023.102204},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102204},
  shortjournal = {Data Knowl. Eng.},
  title        = {BT-CKBQA: An efficient approach for chinese knowledge base question answering},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified hierarchical-attention network model for legal
judgment predictions. <em>DKE</em>, <em>147</em>, 102203. (<a
href="https://doi.org/10.1016/j.datak.2023.102203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of Artificial Intelligence in Legal Research has reached a high level in simulating human thought processes. Case Pendency is a long-lasting problem in many countries. The judicial system has to be more competent and reliable to provide justice on time for any developing country. Litigants and attorneys devote more time and effort to trial case preparation in the courtroom. The task of decision prediction is to automatically forecast the type of charge, law article, and term of punishment. Most of the earlier works for verdict prediction focused to work on civil law jurisdictions . Some of the challenges in the task are case facts are highly unstructured lengthy documents with a lack of annotations and mainly used machine learning techniques . While most research works ignore the information loss at the encoding stage, our proposed MHAN overcomes the above issue and long-range dependency problem using the attention model over hierarchical encoders with three tiers namely Sentence encoder, word encoder, and character encoder. To avoid information loss, a brand-new judgment prediction framework called MHAN is developed in this study effort. It is built on a modified Hierarchical-Attention network and a specially designed domain-specific word embedding model. Additionally, it emphasizes the feature extraction phase by joining features obtained using MHAN with an improved cosine similarity feature. Finally, a hybrid Self Improved RNN is employed to provide the projected results. Furthermore, the proposed model is trained on 10 types of real-time criminal cases from the Madras High Court of India and Supreme Court of India. It has outperformed prior methods in terms of verdict prediction. By applying different variations of the deep learning model and ablation tests, the proposed model achieves consistent results over baseline models .},
  archive      = {J_DKE},
  author       = {G. Sukanya and J. Priyadarshini},
  doi          = {10.1016/j.datak.2023.102203},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102203},
  shortjournal = {Data Knowl. Eng.},
  title        = {Modified hierarchical-attention network model for legal judgment predictions},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RINX: A system for information and knowledge extraction from
resumes. <em>DKE</em>, <em>147</em>, 102202. (<a
href="https://doi.org/10.1016/j.datak.2023.102202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A resume is a detailed source of information about the candidate which summarizes the personal details, education, career history, project experience, certifications, trainings, awards, and any other achievements. For large organizations or job portals which receive thousands of resumes for recruitment or profile creation, it is not possible to manually go through each resume and identify the important information. Hence, there is a need for a system that automatically extracts the information of interest from the resumes. Such automatic extraction of information from resumes is very challenging because resumes are unstructured documents with a wide range of variations in terms of format, style, and contents. In this paper, we describe RINX ( R esume IN formation e X traction) which is an end-to-end system for automatic extraction of information from resumes. RINX heavily utilizes traditional approaches like linguistic patterns and gazettes for information extraction. RINX also complements these traditional approaches with state-of-the-art machine learning and deep learning based techniques. We further describe a few knowledge extraction techniques as well as several real-life use-cases based on the information extracted from a large repository of resumes.},
  archive      = {J_DKE},
  author       = {Girish K. Palshikar and Sachin Pawar and Anindita Sinha Banerjee and Rajiv Srivastava and Nitin Ramrakhiyani and Sangameshwar Patil and Devavrat Thosar and Jyoti Bhat and Ankita Jain and Swapnil Hingmire and Saheb Chaurasia and Payodhi Mandloi and Durgesh Chalavadi},
  doi          = {10.1016/j.datak.2023.102202},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102202},
  shortjournal = {Data Knowl. Eng.},
  title        = {RINX: A system for information and knowledge extraction from resumes},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PoliViews: A comprehensive and modular approach to the
conceptual modeling of genomic data. <em>DKE</em>, <em>147</em>, 102201.
(<a href="https://doi.org/10.1016/j.datak.2023.102201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human genome complexity is captured by many signals, representing for instance DNA variations, the expression of gene activity, or DNA’s structural rearrangements; a rich set of data types and formats is used to record these signals. Conceptual models can support the description and explanation of the genome’s elaborate structure and behavior. Among others, the Conceptual Schema of the Human Genome (CSG) provides a concept-oriented, top-down representation of the genome behavior, which is independent of data formats . The Genomic Conceptual Model (GCM) provides instead a data-oriented, bottom-up representation, targeting a well-organized, unified description of these formats. In this research, we join the two approaches to achieve PoliViews, a comprehensive model that links (1) a concepts layer , describing genome elements and their conceptual connections, with (2) a data layer , describing datasets derived from genome sequencing with specific technologies. Their dynamic connection is established when specific genomic data types are chosen in the data layer, thereby triggering the selection of a view in the concepts layer. The benefit is mutual: data records can be semantically described by high-level concepts exploiting their links and, in turn, the continuously evolving abstract model can be extended thanks to the input provided by real datasets. PoliViews enables expressing queries that employ a holistic conceptual perspective on the genome, directly translated onto data-oriented terms and organization. Here, we demonstrate the approach by linking two major genomic data types, namely DNA variation and gene expression. For each type, we consider different eminent data sources; we describe their mapping with the corresponding view in the concepts layer, enabling an intra-data-type integration. Then, leveraging on the connections available in the concepts layer, we show how the distinct data types can be interoperated, enabling an inter-data-type integration. The PoliViews approach is shown through several examples of biological interest and can be further extended to any kind of genomic information.},
  archive      = {J_DKE},
  author       = {Anna Bernasconi and Alberto García S. and Stefano Ceri and Oscar Pastor},
  doi          = {10.1016/j.datak.2023.102201},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102201},
  shortjournal = {Data Knowl. Eng.},
  title        = {PoliViews: A comprehensive and modular approach to the conceptual modeling of genomic data},
  volume       = {147},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “mining closed high utility itemsets based on
propositional satisfiability” [data knowl. Eng. 136C (2021) 101927].
<em>DKE</em>, <em>146</em>, 102200. (<a
href="https://doi.org/10.1016/j.datak.2023.102200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Amel Hidouri and Said Jabbour and Badran Raddaoui and Boutheina Ben Yaghlane},
  doi          = {10.1016/j.datak.2023.102200},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102200},
  shortjournal = {Data Knowl. Eng.},
  title        = {Corrigendum to “Mining closed high utility itemsets based on propositional satisfiability” [Data knowl. eng. 136C (2021) 101927]},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards understanding students’ sensemaking of test case
design. <em>DKE</em>, <em>146</em>, 102199. (<a
href="https://doi.org/10.1016/j.datak.2023.102199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software testing is the most used technique for quality assurance in industry. However, in computer science education software testing is still treated as a second-class citizen and students are unable to test their software well enough. One reason for this is that teaching the subject of software testing is difficult as it is a complex intellectual activity for which students need to allocate multiple cognitive resources at the same time. A myriad of primary and secondary studies have tried to solve this problem in education, however still with very limited results. Before we can design interventions to improve our pedagogical approaches, we need to gain more in-depth understanding and recognition of sensemaking as it is happening when students design test cases. An initial exploratory study identified four different sensemaking approaches used by students while creating test models. In this paper we present a follow-up study with 50 students from a large university in Spain. The used methodology was based on the previous study with the improvements that originated from its evaluation. We asked the participants to create a test model based on a description of a test problem using a specialized web-based tool for modeling test cases. We measured how well these models fit the test problem, the sensemaking process that students went through when creating the models, and the students’ perception of the modeling task. The participants received no compensation for their efforts, and we scheduled the experiment during a regular class. Apart from the created models and their metadata, we also collected recordings of the students’ computer screens made during the experiment and used a questionnaire to study their perspectives on the assignment. All the collected textual, graphical, and video data was analyzed using an iterative inductive analysis process to allow new information about the different sensemaking approaches to emerge. We gained better insights into the sensemaking processes of students while modeling test cases for a problem. The results enabled us to refine our previous findings, and we identified new sensemaking approaches. Based on these results, we can further investigate ways to influence the sensemaking process in education, the possible misconceptions that have a negative influence on it, and the desired mental model we want our students to have to design test cases.},
  archive      = {J_DKE},
  author       = {Niels Doorn and Tanja E.J. Vos and Beatriz Marín},
  doi          = {10.1016/j.datak.2023.102199},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102199},
  shortjournal = {Data Knowl. Eng.},
  title        = {Towards understanding students’ sensemaking of test case design},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable machine learning models to analyse maternal
health. <em>DKE</em>, <em>146</em>, 102198. (<a
href="https://doi.org/10.1016/j.datak.2023.102198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maternal health is a significant public health concern for globe and many developing countries. A country like India (with large population), there are considerable disparities in maternal health service utilisation and maternal mortality within and across states. A more than a general healthcare operational policy would suffice, but a precision healthcare strategy would be needed. This article focused on explainable machine learning models that can precisely advise health care intervention policy and medical treatment to an administrative unit rather than a generic policy suggestion for improving maternal health. This study presents an exhaustive list of factors associated with Maternal Mortality Rate (MMR) and a series of explainable AI models. One of models uses CART heuristics to categorise districts (administrative boundaries) into lower and higher MMR classes. Another explainable model, Shapley Additive Explanations (SHAP), used SVM , ANN , boosting, and random forest machine learning models to investigate higher and lower MMR regions. Further, an Explainable Boosting Machine (EBM) also used, and the results are compared for policy suggestions. Some of the ignored features from general social science studies, such as topography and agro-climatic zone characteristics of a particular district, may be crucial in the analysis. Moreover, health infrastructure, insurance, and other factors also influence policymaking. This predictive and explainable work has significant implications for precision healthcare policy design to improve maternal health compared to a broader policy approach.},
  archive      = {J_DKE},
  author       = {Shivshanker Singh Patel},
  doi          = {10.1016/j.datak.2023.102198},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102198},
  shortjournal = {Data Knowl. Eng.},
  title        = {Explainable machine learning models to analyse maternal health},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big data analytics and knowledge discovery. <em>DKE</em>,
<em>146</em>, 102197. (<a
href="https://doi.org/10.1016/j.datak.2023.102197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Matteo Golfarelli and Robert Wrembel},
  doi          = {10.1016/j.datak.2023.102197},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102197},
  shortjournal = {Data Knowl. Eng.},
  title        = {Big data analytics and knowledge discovery},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Better than XML: Towards a lexicographic markup language.
<em>DKE</em>, <em>146</em>, 102196. (<a
href="https://doi.org/10.1016/j.datak.2023.102196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article takes a critical look at how XML is used in lexicography and asks the question, why do dictionary entries often end up looking so complex when encoded in XML? The main reason for the perceived complexity of XML-encoded dictionaries is purely structural markup : XML elements which contain other XML elements instead of human-readable text. The over-abundance of purely structural markup in lexicography is caused by the nature of lexicographic content, much of which is inherently headed . XML has no support for headedness and neither do other commonly used languages such as JSON and YAML. In this article we propose a number of constraints and extensions to XML, JSON and YAML which add support for headedness into these languages.},
  archive      = {J_DKE},
  author       = {Michal Měchura},
  doi          = {10.1016/j.datak.2023.102196},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102196},
  shortjournal = {Data Knowl. Eng.},
  title        = {Better than XML: Towards a lexicographic markup language},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical evaluation of scrum training’s suitability for
the model-driven development of knowledge-intensive software systems.
<em>DKE</em>, <em>146</em>, 102195. (<a
href="https://doi.org/10.1016/j.datak.2023.102195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Product Configuration System (PCS) is a software system that facilitates the sales and production processes of defined customizable products. PCS are specific software developments in the sense that they are knowledge-intensive so that they require models to formalize the complex knowledge inherent to product configurations also leading to dependencies between software functionalities. Scrum is a widely used agile method, but its training has been the subject of little research. Model-driven development implicitly impacts the way development is conducted especially when adopting an agile method as Scrum. This paper, as exploratory research, evaluates Scrum training for PCS projects through a qualitative case study. The goal is to identify the elements that should be focused on within Scrum background training. This research first studies and assesses the training experiences at the case company. Then, it reports on respondents’ opinions about the strengths and challenges of applying Scrum in the mentioned context. The latter is based on multiple data sources: documentation, interviews, participant observations, and workshops. Issues in applicability lead to enhanced training support for learning how to (i) combine Scrum with the model-driven approach inherently required within PCS development, (ii) manage time and effort estimation on the basis of accurate artifacts and (iii) access key employees possessing domain or specific technical knowledge indispensable for pursuing the development.},
  archive      = {J_DKE},
  author       = {Sara Shafiee and Yves Wautelet and Stephan Poelmans and Samedi Heng},
  doi          = {10.1016/j.datak.2023.102195},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102195},
  shortjournal = {Data Knowl. Eng.},
  title        = {An empirical evaluation of scrum training’s suitability for the model-driven development of knowledge-intensive software systems},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated sentimental analysis using heuristic-based
CNN-BiLSTM for e-commerce dataset. <em>DKE</em>, <em>146</em>, 102194.
(<a href="https://doi.org/10.1016/j.datak.2023.102194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sentimental analysis is processed according to structural detection, extraction, quantification, and evaluation of impacts and information present in natural language processing , biometric data , computer-language society, and text analysis. Advanced implementations in data analytics occur, which allows for identifying the underlining trends based on certain efficient computational models . Some of the social and e-commerce platforms consist of numerous reviews regarding the online products that are posted by the end-users. This helps the developers to understand priceless insight during the process of product design. The most significant aim of this work is to develop a Heuristic-based sentimental analysis for E-Commerce data. Here, the pre-processing of input E-Commerce data is performed by blank space removal, stop word removal, and stemming. Here, the statistical features are extracted using Cross Similarity Score (CSS) and Joint Similarity Score (JSS) regarding “positive, negative, and neutral keywords”. In addition, the word2vec features are also extracted. From the two sets of features, “Convolutional Neural Network (CNN) with Bidirectional Long Short Term Memory (BiLSTM)” is separately used that is named as Optimal Hybrid CNN-BiLSTM (OH-CNN-BiLSTM), and performed the hybridization of two models for the sentimental analysis. The designed CNN-BiLSTM technique is ensured better outcomes when processing long text as it provides advantages through the ability of CNN to extract the features and superiority of Bi-LSTM in learning the bidirectional dependencies of the long-term text. The main novelty of this work is to develop an improved Galactic Swarm Optimization (IGSO) algorithm for improving the hybridized model, as it provides faster convergence when getting the accurate solution on a wide range of dimensional space and is also able to solve the optimization issues with multimodal benchmark data. The evaluation of the suggested algorithm is done with various E-Commerce data and secures high accuracy on sentimental analysis and prediction while comparing with the traditional models.},
  archive      = {J_DKE},
  author       = {Dr. N. Ramshankar and Dr. Joe Prathap P.M.},
  doi          = {10.1016/j.datak.2023.102194},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102194},
  shortjournal = {Data Knowl. Eng.},
  title        = {Automated sentimental analysis using heuristic-based CNN-BiLSTM for E-commerce dataset},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new approach to COVID-19 data mining: A deep
spatial–temporal prediction model based on tree structure for traffic
revitalization index. <em>DKE</em>, <em>146</em>, 102193. (<a
href="https://doi.org/10.1016/j.datak.2023.102193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of the COVID-19 epidemic has had a huge impact on a global scale and its impact has covered almost all human industries . The Chinese government enacted a series of policies to restrict the transportation industry in order to slow the spread of the COVID-19 virus in early 2020. With the gradual control of the COVID-19 epidemic and the reduction of confirmed cases, the Chinese transportation industry has gradually recovered. The traffic revitalization index is the main indicator for evaluating the degree of recovery of the urban transportation industry after being affected by the COVID-19 epidemic. The prediction research of traffic revitalization index can help the relevant government departments to know the state of urban traffic from the macro level and formulate relevant policies. Therefore, this study proposes a deep spatial–temporal prediction model based on tree structure for the traffic revitalization index. The model mainly includes spatial convolution module, temporal convolution module and matrix data fusion module. The spatial convolution module builds a tree convolution process based on the tree structure that can contain directional features and hierarchical features of urban nodes. The temporal convolution module constructs a deep network for capturing temporal dependent features of the data in the multi-layer residual structure. The matrix data fusion module can perform multi-scale fusion of COVID-19 epidemic data and traffic revitalization index data to further improve the prediction effect of the model. In this study, experimental comparisons between our model and multiple baseline models are conducted on real datasets. The experimental results show that our model has an average improvement of 21\%, 18\%, and 23\% in MAE , RMSE and MAPE indicators, respectively.},
  archive      = {J_DKE},
  author       = {Zhiqiang Lv and Xiaotong Wang and Zesheng Cheng and Jianbo Li and Haoran Li and Zhihao Xu},
  doi          = {10.1016/j.datak.2023.102193},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102193},
  shortjournal = {Data Knowl. Eng.},
  title        = {A new approach to COVID-19 data mining: A deep spatial–temporal prediction model based on tree structure for traffic revitalization index},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Logical big data integration and near real-time data
analytics. <em>DKE</em>, <em>146</em>, 102185. (<a
href="https://doi.org/10.1016/j.datak.2023.102185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of decision-making, there is a growing demand for near real-time data that traditional solutions, like data warehousing based on long-running ETL processes, cannot fully meet. On the other hand, existing logical data integration solutions are challenging because users must focus on data location and distribution details rather than on data analytics and decision-making. EasyBDI is an open-source system that provides logical integration of data and high-level business-oriented abstractions. It uses schema matching, integration, and mapping techniques, to automatically identify partitioned data and propose a global schema. Users can then specify star schemas based on global entities and submit analytical queries to retrieve data from distributed data sources without knowing the organization and other technical details of the underlying systems. This work presents the algorithms and methods for global schema creation and query execution . Experimental results show that the overhead imposed by logical integration layers is relatively small compared to the execution times of distributed queries.},
  archive      = {J_DKE},
  author       = {Bruno Silva and José Moreira and Rogério Luís de C. Costa},
  doi          = {10.1016/j.datak.2023.102185},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102185},
  shortjournal = {Data Knowl. Eng.},
  title        = {Logical big data integration and near real-time data analytics},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing the convolution-based knowledge graph embeddings
by increasing dimension-wise interactions. <em>DKE</em>, <em>146</em>,
102184. (<a href="https://doi.org/10.1016/j.datak.2023.102184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding learns distributed low-dimensional representations for the elements in knowledge graphs, so that knowledge can be conveniently integrated into various tasks and smart systems. Recently, convolutional neural network has been introduced to embedding technique and obtained impressive achievements in link prediction task. ConvKB, a recently proposed method, captured the global dimension-wise interactions in facts with the convolutional filters . However, ConvKB failed to learn the local interactions between the entity and relation embedding. Moreover, rich interactions among feature maps are neglected in the existing convolutional embedding models. In this paper, based on ConvKB, we propose ConvD which models the local relationships in facts and integrates the cross-channel information based on the dimension-wise interactions to further improve the performance. From the experimental results, ConvD obtains scores that are 96\% and 5\% better than ConvKB on MRR and Hits@10 in the link prediction task. Furthermore, ConvD surpassed state-of-the-art baselines on WN18RR and achieved competitive results on FB15k-237 respectively.},
  archive      = {J_DKE},
  author       = {Fengyuan Lu and Jie Zhou and Xinli Huang},
  doi          = {10.1016/j.datak.2023.102184},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102184},
  shortjournal = {Data Knowl. Eng.},
  title        = {Enhancing the convolution-based knowledge graph embeddings by increasing dimension-wise interactions},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for investigating the dynamics of user and
community sentiments in a social platform. <em>DKE</em>, <em>146</em>,
102183. (<a href="https://doi.org/10.1016/j.datak.2023.102183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social platforms are the preferred medium for many people to express their opinions on many topics. This has led many professionals from various fields (marketing, politics, research and development, etc.) to demand increasingly advanced approaches capable of analyzing the evolution of user or community sentiments on particular topics. In this paper, we want to make a contribution to addressing this issue. Specifically, we propose a model and a framework to analyze the dynamics of user and community sentiments in a social platform. In particular, our framework currently focuses on three activities, namely: (i) finding users capable of creating and maintaining a community that reflects their sentiment on a topic; (ii) studying how a user or community sentiment on a topic evolves over time; and (iii) investigating the cross-contamination between a user community and its neighborhood. We tested our framework by means of an extensive experimental campaign that we describe in the paper. Our framework is extremely scalable, and further activities can be easily implemented in it in the near future.},
  archive      = {J_DKE},
  author       = {Gianluca Bonifazi and Francesco Cauteruccio and Enrico Corradini and Michele Marchetti and Giorgio Terracina and Domenico Ursino and Luca Virgili},
  doi          = {10.1016/j.datak.2023.102183},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102183},
  shortjournal = {Data Knowl. Eng.},
  title        = {A framework for investigating the dynamics of user and community sentiments in a social platform},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trustworthy journalism through AI. <em>DKE</em>,
<em>146</em>, 102182. (<a
href="https://doi.org/10.1016/j.datak.2023.102182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality journalism has become more important than ever due to the need for quality and trustworthy media outlets that can provide accurate information to the public and help to address and counterbalance the wide and rapid spread of disinformation. At the same time, quality journalism is under pressure due to loss of revenue and competition from alternative information providers. This vision paper discusses how recent advances in Artificial Intelligence (AI), and in Machine Learning (ML) in particular, can be harnessed to support efficient production of high-quality journalism. From a news consumer perspective, the key parameter here concerns the degree of trust that is engendered by quality news production. For this reason, the paper will discuss how AI techniques can be applied to all aspects of news, at all stages of its production cycle, to increase trust.},
  archive      = {J_DKE},
  author       = {Andreas L Opdahl and Bjørnar Tessem and Duc-Tien Dang-Nguyen and Enrico Motta and Vinay Setty and Eivind Throndsen and Are Tverberg and Christoph Trattner},
  doi          = {10.1016/j.datak.2023.102182},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102182},
  shortjournal = {Data Knowl. Eng.},
  title        = {Trustworthy journalism through AI},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A design of movie script generation based on natural
language processing by optimized ensemble deep learning with heuristic
algorithm. <em>DKE</em>, <em>146</em>, 102150. (<a
href="https://doi.org/10.1016/j.datak.2023.102150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Movies offer users a huge range of visual information like attractive stories. The traditional approaches have demonstrated that knowing about movie stories via only visual information is complicated. The natural data of graphical texts play a significant task in portraying information in various domains like entertainment, education, communication, etc. Similarly, script generation by considering the earlier dialogue is more complex owing to the inherent nature of the text. Text identification in natural data considers script identification that needs localization of text. Especially for natural scene data, script generation is a complicated task due to different background/foreground components. The texts comprise varied textures, variations in size, orientation, colors, and fonts. It is essential to have a system that produces scripts or stories from a storyline automatically. However, this research is not emerging nowadays. While considering the dialogue systems would also help drive dialogues through a dialogue plan. Hence, a new movie script generation model is suggested through processing the movie text data. Initially, the text data is collected related to a different movie that consists of characters, scenes, genre, location, etc. Secondly, the data pre-processing is carried out to enhance data quality. Further, the significant features are extracted from the pre-processed data through Term Frequency-Inverse Document Frequency (TFIDF) and word2vector. The deep features are extracted to get the noteworthy features using a Deep Belief Network (DBN) from RBN layers. Finally, the deep features are given to the Ensemble-based Movie Scrip Generation (EMCG), where the Optimized hybrid script generation process using ensemble learning is performed by Bidirectional Long Short-Term Memory (Bi-LSTM), Generative Pre-Trained Transformer version 3 (GPT3), and GPT Neo X models, where the parameters of deep learning algorithms are optimized using the Adaptively Improved Cat and Mouse-based Optimizer (AI-CMO) algorithm. Here, the outcomes are attained through taking averaging among the classified outcomes. The standard performance measures are used for evaluating the effectiveness of the proposed method.},
  archive      = {J_DKE},
  author       = {R. Dharaniya and Dr. J. Indumathi and V. Kaliraj},
  doi          = {10.1016/j.datak.2023.102150},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102150},
  shortjournal = {Data Knowl. Eng.},
  title        = {A design of movie script generation based on natural language processing by optimized ensemble deep learning with heuristic algorithm},
  volume       = {146},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly detection with correlation laws. <em>DKE</em>,
<em>145</em>, 102181. (<a
href="https://doi.org/10.1016/j.datak.2023.102181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets from different domains usually contain data defined over a wide set of attributes among which various degrees of correlation exist. The identification of data objects not complying with these hidden correlations is a formidable task. Moreover, often attributes may play different roles in applications. Specifically, some features can be perceived as independent variables which are responsible for the definition of a context in which a dependent variable exhibits anomalous behaving values. Hence, in this work we focus on the detection of data objects showing an anomalous behavior on a subset of attributes, called behavioral, w.r.t. some other ones, called contextual. As a main contribution, we design a model to describe the correlation laws hidden in data distributions over pairs of behavioral–contextual attributes. We introduce a probability measure aimed at scoring subsequently observed objects based on how much their behavior deviates from the detected correlation laws. We test our method on both synthetic and real dataset to demonstrate its effectiveness and show its ability in outperforming some competitors. Moreover, we discuss a case study in the field of gene expression data analysis to prove that it can provide a valuable contribution when dealing with those scenarios in which the features are much more abundant than the samples available for the analysis.},
  archive      = {J_DKE},
  author       = {Fabrizio Angiulli and Fabio Fassetti and Cristina Serrao},
  doi          = {10.1016/j.datak.2023.102181},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102181},
  shortjournal = {Data Knowl. Eng.},
  title        = {Anomaly detection with correlation laws},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards automatic privacy-preserving record linkage: A
transfer learning based classification step. <em>DKE</em>, <em>145</em>,
102180. (<a href="https://doi.org/10.1016/j.datak.2023.102180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-Preserving Record Linkage (PPRL) intends to identify records that match the same real-world entities across disparate data sources while preserving the privacy of the individual entities. To identify matching records across different data sources and still preserve the privacy of the information, PPRL needs to consider several restrictions due to privacy limitations. For instance, PPRL is executed over anonymized (or encrypted) data to avoid re-identification. Moreover, the classification step of PPRL does not have access to labeled information (indicating if a pair of records is a match) and an oracle (specialist) to label a few instances. These limitations make it hard to employ automatic classification techniques . Most PPRL techniques use a simple threshold (defined by a specialist) to define whether a pair of records represent the same real-world entity or not. To overcome these problems, we present a Transfer Learning-based unsupervised classification step to PPRL, which leverages the information available in public (or synthetic) datasets to train accurate classifiers in a privacy-preserving context. We evaluate our approach using real-world and synthetic data, and the results demonstrate that our unsupervised classification step is able to overcome the most used classification strategies in PPRL.},
  archive      = {J_DKE},
  author       = {Thiago Nóbrega and Carlos Eduardo S. Pires and Dimas Cassimiro Nascimento and Leandro Balby Marinho},
  doi          = {10.1016/j.datak.2023.102180},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102180},
  shortjournal = {Data Knowl. Eng.},
  title        = {Towards automatic privacy-preserving record linkage: A transfer learning based classification step},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Similarity matrix enhanced collaborative filtering for
e-government recommendation. <em>DKE</em>, <em>145</em>, 102179. (<a
href="https://doi.org/10.1016/j.datak.2023.102179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized e-government recommendation aims to predict the user’s next action in a near future based on his or her historical behavior data. Similarity measures and distances are widely used for powering personalized recommendations. The classical similarity-based models in the recommender systems are known as K-Nearest Neighbor Collaborative Filtering (KNN-based CF). Most prior work adopts a new similarity formula to obtain the k-nearest neighbors for the user and/or item. This is a good try but falls short of solving the problem of expensive computation and introducing the order of users’ interaction sequences. In this paper, we propose SACF, a similarity matrix enhanced the e-government recommendation model that takes advantage of pairwise learning to introduce the order of uses’ interaction sequences and quickly obtain the similarity matrix using matrix factorization . In other words, the similarity matrix can be presented as the production of two low-rank feature matrices. Therefore, it is possible to effectively estimate the similarity between two users even with few co-rated items. Experimental results on a real-world e-government dataset show that the proposed approach improves the performance significantly compared with other counterparts.},
  archive      = {J_DKE},
  author       = {Ninghua Sun and Qiangqiang Luo and Longya Ran and Peng Jia},
  doi          = {10.1016/j.datak.2023.102179},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102179},
  shortjournal = {Data Knowl. Eng.},
  title        = {Similarity matrix enhanced collaborative filtering for e-government recommendation},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In memoriam — professor aditya ghose. <em>DKE</em>,
<em>145</em>, 102178. (<a
href="https://doi.org/10.1016/j.datak.2023.102178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Joerg Evermann and Jennifer Horkoff and Jeffrey Parsons and Vitor Souza},
  doi          = {10.1016/j.datak.2023.102178},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102178},
  shortjournal = {Data Knowl. Eng.},
  title        = {In memoriam — professor aditya ghose},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preface | data &amp; knowledge engineering - volume 145.
<em>DKE</em>, <em>145</em>, 102177. (<a
href="https://doi.org/10.1016/j.datak.2023.102177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Aditya Ghose ( Guest Editors ) and Jennifer Horkoff and Vítor E. Silva Souza and Jeff Parsons and Joerg Evermann},
  doi          = {10.1016/j.datak.2023.102177},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102177},
  shortjournal = {Data Knowl. Eng.},
  title        = {Preface | data &amp; knowledge engineering - volume 145},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging relevant summarized information and multi-layer
classification to generalize the detection of misleading headlines.
<em>DKE</em>, <em>145</em>, 102176. (<a
href="https://doi.org/10.1016/j.datak.2023.102176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disinformation is an important problem facing society nowadays. Given the rapid and easy access to information, news stories quickly go viral, the vast majority of which are misleading and with no prospect of verification. Specifically, the headline of a correctly designed news item must correspond to a summary of the main information of that news item and it should be neutral. However, many headlines circulating on the Internet use false or distorted information, seeking to confuse or mislead the reader. Misleading headlines indicate a dissonance between the headline and the content of the news story. From a computational perspective, this problem is being tackled as a Stance Detection problem between the headline and the body text of the news item. This paper contributes to the fight against the spread of misleading information by presenting a generic and flexible multi-level hierarchical classification. The approach is based on two stages that enable the detection of the stance between the news headline and the body text. The proposed architecture, called HeadlineStanceChecker+ uses the headline and only the essential information of the news item (not the full body text) as inputs. To extract this essential information, different summarization approaches (extractive and abstractive) are analyzed in order to determine the most relevant information for the task. The experimentation has been carried out using the Fake News Challenge (FNC-1) dataset. A 94.49\% accuracy was obtained using extractive summaries, which were more helpful than abstractive ones. HeadlineStanceChecker+ improves the accuracy results of existing state-of-the-art systems. In conclusion, using automatic extractive summaries together with the two-stage generic architecture is an effective solution to the problem.},
  archive      = {J_DKE},
  author       = {Robiert Sepúlveda-Torres and Marta Vicente and Estela Saquete and Elena Lloret and Manuel Palomar},
  doi          = {10.1016/j.datak.2023.102176},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102176},
  shortjournal = {Data Knowl. Eng.},
  title        = {Leveraging relevant summarized information and multi-layer classification to generalize the detection of misleading headlines},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal event log sanitization for privacy-preserving
process mining. <em>DKE</em>, <em>145</em>, 102175. (<a
href="https://doi.org/10.1016/j.datak.2023.102175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event logs that originate from information systems enable comprehensive analysis of business processes. These logs serve as the starting point for the discovery of process models or the analysis of conformance of a log with a given specification. However, logs potentially contain personal information about individuals involved in process execution. In this paper, we therefore address the risk of privacy attacks on event logs. Specifically, we rely on group-based privacy guarantees instead of noise insertion in order to enable anonymization without adding new behaviour to the log. To this end, we propose two new algorithms for event log sanitization that provide privacy guarantees in terms of k-anonymity for the behavioural perspective of a process and t-closeness for sensitive information associated with events. The algorithms thereby avoid the disclosure of employee identities, prevent the identification of employee membership in the log, and preclude the characterization of employees based on sensitive attributes. Our algorithms overcome the limitations of an existing, greedy algorithm , providing users with a trade-off between computational complexity and the utility of the sanitized event log for downstream analysis. Our Experiments demonstrate that sanitization with our algorithms generates event logs of higher utility compared to the state of the art.},
  archive      = {J_DKE},
  author       = {Stephan A. Fahrenkrog-Petersen and Han van der Aa and Matthias Weidlich},
  doi          = {10.1016/j.datak.2023.102175},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102175},
  shortjournal = {Data Knowl. Eng.},
  title        = {Optimal event log sanitization for privacy-preserving process mining},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AStar: A modeling language for document-oriented geospatial
data warehouses. <em>DKE</em>, <em>145</em>, 102174. (<a
href="https://doi.org/10.1016/j.datak.2023.102174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Geospatial Data Warehouse (GDW) is an extension of a traditional Data Warehouse that includes geospatial data in the decision-making processes. Several studies have proposed the use of document-oriented databases in a GDW as an alternative to relational databases . This is due to the ability of non-relational databases to scale horizontally, allowing for the storage and processing of large volumes of data. In this context, modeling the manner in which facts and dimensions are structured is important in order to understand, maintain, and evolve the Document-oriented GDW (DGDW) through visual analysis. However, to the best of our knowledge, there are no modeling languages that support the design of aggregated data as facts and dimensions, that can be represented as referenced or embedded documents, partitioned into one or more collections. To overcome this lack, we propose Aggregate Star (AStar), a Domain-Specific Modeling Language for designing DGDW logical schemas . AStar is defined from a concrete syntax (graphical notation), an abstract syntax (metamodel), and static semantics (well-formedness rules). In order to describe the semantics of the concepts defined in AStar, translational semantics map the graphical notation to the metamodel and the respective code, to define the schema in MongoDB (using JSON Schema). We evaluate the graphical notation using Physics of Notations (PoN), which provides a set of principles for designing cognitively effective visual notations. This evaluation revealed that AStar is in accordance with seven of the nine PoN principles, an adequate level of cognitive effectiveness. As a proof of concept , the metamodel and well-formedness rules were implemented in a prototype of Computer-Assisted Software Engineering tool, called AStarCASE. In its current version, AStarCASE can be used to design DGDW logical schemas and to generate their corresponding code in the form of JSON Schemas.},
  archive      = {J_DKE},
  author       = {Marcio Ferro and Edson Silva and Robson Fidalgo},
  doi          = {10.1016/j.datak.2023.102174},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102174},
  shortjournal = {Data Knowl. Eng.},
  title        = {AStar: A modeling language for document-oriented geospatial data warehouses},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decision tree post-pruning without loss of accuracy using
the SAT-PP algorithm with an empirical evaluation on clinical data.
<em>DKE</em>, <em>145</em>, 102173. (<a
href="https://doi.org/10.1016/j.datak.2023.102173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A decision tree (DT) is one of the most popular and efficient techniques in data mining. Specifically, in the clinical domain, DTs have been widely used thanks to their relatively easy explainable nature, efficient computation time, and relatively accurate predictions. However, some DT constriction algorithms may produce a large tree size structure which is difficult to understand and often leads to misclassification of data in the testing process due to poor generalization. Post pruning (PP) algorithms have been introduced to reduce the size of the tree structure with a minor (or not at all) decrease in the accuracy of classification while trying to improve the model’s generalization. In this paper, we propose a new Boolean satisfiability (SAT) based PP algorithm called SAT-PP. Our algorithm reduces the tree size while preserving the accuracy of the unpruned tree. We implemented our algorithm on a medical-related classification data sets since in medical-related tasks we emphatically try to avoid decreasing the model’s performance when better training is not an option. Namely, in the case of medical-related tasks, one may prefer an unpruned DT model to a pruned DT model with worse performance. Indeed, we empirically obtained that the SAT-PP algorithm produce the same accuracy and F 1 F1 score as the DT model without PP while statistically significantly reducing the model size and as a result computation time (6.8\%). In addition, we compared the proposed algorithm with other PP algorithms and found similar generalization capabilities.},
  archive      = {J_DKE},
  author       = {Teddy Lazebnik and Svetlana Bunimovich-Mendrazitsky},
  doi          = {10.1016/j.datak.2023.102173},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102173},
  shortjournal = {Data Knowl. Eng.},
  title        = {Decision tree post-pruning without loss of accuracy using the SAT-PP algorithm with an empirical evaluation on clinical data},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to “explicit and implicit oriented aspect-based
sentiment analysis with optimal feature selection and deep learning for
demonetization in india” data knowl. Eng. 142 (2022) 102092.
<em>DKE</em>, <em>145</em>, 102158. (<a
href="https://doi.org/10.1016/j.datak.2023.102158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {K. Ananthajothi and K. Karthikayani and R. Prabha},
  doi          = {10.1016/j.datak.2023.102158},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102158},
  shortjournal = {Data Knowl. Eng.},
  title        = {Corrigendum to “Explicit and implicit oriented aspect-based sentiment analysis with optimal feature selection and deep learning for demonetization in india” data knowl. eng. 142 (2022) 102092},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conceptually-grounded mapping patterns for virtual knowledge
graphs. <em>DKE</em>, <em>145</em>, 102157. (<a
href="https://doi.org/10.1016/j.datak.2023.102157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Knowledge Graphs (VKGs) constitute one of the most promising paradigms for integrating and accessing legacy data sources. A critical bottleneck in the integration process involves the definition, validation, and maintenance of mapping assertions that link data sources to a domain ontology . To support the management of mappings throughout their entire lifecycle, we identify a comprehensive catalog of sophisticated mapping patterns that emerge when linking databases to ontologies. To do so, we build on well-established methodologies and patterns studied in data management, data analysis, and conceptual modeling . These are extended and refined through the analysis of concrete VKG benchmarks and real-world use cases, and considering the inherent impedance mismatch between data sources and ontologies. We validate our catalog on the considered VKG scenarios, showing that it covers the vast majority of mappings present therein.},
  archive      = {J_DKE},
  author       = {Diego Calvanese and Avigdor Gal and Davide Lanti and Marco Montali and Alessandro Mosca and Roee Shraga},
  doi          = {10.1016/j.datak.2023.102157},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102157},
  shortjournal = {Data Knowl. Eng.},
  title        = {Conceptually-grounded mapping patterns for virtual knowledge graphs},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rating scale preferences for accurate recommendations.
<em>DKE</em>, <em>145</em>, 102156. (<a
href="https://doi.org/10.1016/j.datak.2023.102156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems allow users to express preferences for items using a rating scale. However, employing this scale differs from one user to another, and sometimes it seems that each user has a personal scale. Hence it is vital to capture the user preferences for the rating scale for better recommendations. Previous work tried to weight either the similarity value or the individual ratings of items. However, weighting the rating scale itself to reflect personal user interests is more effective. This reduces the problem space by directly targeting the tool used to express user preferences, not individual ratings for items. This paper proposes different approaches for generating a personalized rating scale that captures the user preferences reflected by the statistical properties of his profile. Some approaches consider the features of the active user, while others also consider the attributes of the training user. The experimental results disclose different user behavior patterns for the rating scale. They show that the system performance is improved so much by introducing weights to the rating scale with 70\% improvement for some schemes.},
  archive      = {J_DKE},
  author       = {Mohammad Yahya H. Al-Shamri},
  doi          = {10.1016/j.datak.2023.102156},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102156},
  shortjournal = {Data Knowl. Eng.},
  title        = {Rating scale preferences for accurate recommendations},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neurofuzzy semantic similarity measurement. <em>DKE</em>,
<em>145</em>, 102155. (<a
href="https://doi.org/10.1016/j.datak.2023.102155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically identifying the degree of semantic similarity between two small pieces of text has grown in importance recently. Its impact on various computer-related domains and recent breakthroughs in neural computation has increased the opportunities for better solutions to be developed. This work contributes a neurofuzzy approach for semantic textual similarity that uses neural networks and fuzzy logics. The idea is to combine the capabilities of the deep neural models for working with text with the ones from fuzzy logic for aggregating numerical data. The results of our experiments suggest that such an approach can accurately determine semantic similarity.},
  archive      = {J_DKE},
  author       = {Jorge Martinez-Gil and Riad Mokadem and Josef Küng and Abdelkader Hameurlain},
  doi          = {10.1016/j.datak.2023.102155},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102155},
  shortjournal = {Data Knowl. Eng.},
  title        = {Neurofuzzy semantic similarity measurement},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DLBench+: A benchmark for quantitative and qualitative data
lake assessment. <em>DKE</em>, <em>145</em>, 102154. (<a
href="https://doi.org/10.1016/j.datak.2023.102154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, the concept of data lake has become trendy for data storage and analysis. Thus, several approaches have been proposed to build data lake systems. However, such proposals are difficult to evaluate as there are no commonly shared criteria for comparing data lake systems. Thus, we introduce in this paper DLBench+, a benchmark to evaluate and compare data lake implementations that support textual and/or tabular contents. More concretely, we propose a data model made of both textual and CSV documents, a workload model composed of a set of various tasks, as well as a set of performance-based metrics, all relevant to the context of data lakes. Beyond a purely quantitative assessment , we also propose a methodology to qualitatively evaluate data lake systems through the assessment of user experience . As a proof of concept , we use DLBench+ to evaluate an open source data lake system we developed.},
  archive      = {J_DKE},
  author       = {Pegdwendé N. Sawadogo and Jérôme Darmont},
  doi          = {10.1016/j.datak.2023.102154},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102154},
  shortjournal = {Data Knowl. Eng.},
  title        = {DLBench+: A benchmark for quantitative and qualitative data lake assessment},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bounding box representation of co-location instances for
chebyshev and manhattan metrics. <em>DKE</em>, <em>145</em>, 102153. (<a
href="https://doi.org/10.1016/j.datak.2023.102153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-location Pattern Mining (CPM) is the task of discovering sets of spatial features (object types) whose instances are frequently located close to each other in space. Popular co-location discovery methods consist of iteratively: (1) generating co-location candidates, (2) determining instances of these candidates and calculating a measure of potential interestingness, and (3) determining the set of co-locations based on that measure. In this paper, we focus on the second step, as it is the most time-consuming element of CPM. We assume that the distance function is either the Chebyshev or the Manhattan metric. We provide an instance identification method that is characterized by a lower complexity than the state-of-the-art approach. In particular, (1) we introduce a new representation of co-location instances based on bounding boxes, (2) we formulate and prove several theorems regarding such a representation that can improve instances identification step, (3) we provide a novel algorithm that uses the above-mentioned theorems, and (4) we analyze its complexity. To verify our approach, we performed a series of experiments using two real-world datasets.},
  archive      = {J_DKE},
  author       = {W. Andrzejewski and P. Boinski},
  doi          = {10.1016/j.datak.2023.102153},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102153},
  shortjournal = {Data Knowl. Eng.},
  title        = {Bounding box representation of co-location instances for chebyshev and manhattan metrics},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ISO/IEC 25012-based methodology for managing data quality
requirements in the development of information systems: Towards data
quality by design. <em>DKE</em>, <em>145</em>, 102152. (<a
href="https://doi.org/10.1016/j.datak.2023.102152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users should trust the data that are managed by software applications constituting the Information Systems (IS). This means that organizations should ensure an appropriate level of quality of the data they manage in their IS. Therefore, the requirement for the adequate level of quality of data to be managed by IS must be an essential requirement for every organization. In order to meet this requirement, some features and elements related to data quality (DQ) should be taken into account from the initial stages of software development, attending the “ data quality by design ” principle. The DQ concept is considered to be multidimensional and largely context dependent. For this reason, the management of specific requirements is a difficult task. Thus, the main objective of this paper is to present a methodology for Project Management of Data Quality Requirements Specification called DAQUAVORD aimed at eliciting DQ requirements arising from different users’ viewpoints. These specific requirements should serve as typical requirements, both functional and non-functional, at the time of the development of IS that takes Data Quality into account by default leading to smarter and collaborative development.},
  archive      = {J_DKE},
  author       = {César Guerra-García and Anastasija Nikiforova and Samantha Jiménez and Héctor G. Perez-Gonzalez and Marco Ramírez-Torres and Luis Ontañon-García},
  doi          = {10.1016/j.datak.2023.102152},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102152},
  shortjournal = {Data Knowl. Eng.},
  title        = {ISO/IEC 25012-based methodology for managing data quality requirements in the development of information systems: Towards data quality by design},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid optimization using lion and dragonfly for enhanced
resource allocation in fifth-generation networks. <em>DKE</em>,
<em>145</em>, 102151. (<a
href="https://doi.org/10.1016/j.datak.2023.102151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“5G (Fifth Generation) network promises Enhanced Mobile Communications (EMC), Massive Machine Type Communications (MMTC) and Ultra-Reliable Low Latency ygCommunications (URLLC)”. For enhancing communication, resource allocation is a crucial factor that guarantees the partitioning of channels into resource blocks, which were allocated to diverse users. Different methods have been introduced for improving resource scheduling however, they do not reach the optimum solution. This work aims to scrutinize the resource allocation problems in the “hybrid multi-carrier non-orthogonal multiple access (MC-NOMA) systems” that comprise the OMA and NOMA modes. It is used to achieve the EE and SE trade-off with minimal rate requirements of users. In addition, the total degree of freedom includes “user clustering, power allocation , choice of Multiple Access (MA) modes, and subcarrier assignment as well”. Accordingly, the optimization concept is integrated that offers the most noteworthy way of resolving multi-objective issues. In this context, resource allocation with power allotment is considered a vital factor, which is determined as a single-objective issue. Thereby this paper ensures the optimal allocation of power and subcarriers by a novel algorithm termed as Dragon Levy-based Lion Cub Generation Model (DL-LCG). Finally, the presented model is evaluated over existing models regarding cost analysis},
  archive      = {J_DKE},
  author       = {Bamila Virgin Louis A. and Arul Dalton G.},
  doi          = {10.1016/j.datak.2023.102151},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102151},
  shortjournal = {Data Knowl. Eng.},
  title        = {Hybrid optimization using lion and dragonfly for enhanced resource allocation in fifth-generation networks},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing NoSQL databases based on multiple requirement
views. <em>DKE</em>, <em>145</em>, 102149. (<a
href="https://doi.org/10.1016/j.datak.2023.102149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, new data characteristics led to the development of new database management systems named NoSQL. As opposed to the mature Relational databases , design methods for the new databases receive little attention and mainly consider the data-related requirements. In this paper, we present methods for designing two types of NoSQL databases – Document and Graph databases – that consider not only the data-related but also functional-related requirements. We empirically evaluate the methods and apply the design methods to two leading database systems. We found that the databases designed with the consideration of functional requirements perform better, with respect to time of execution and database I/O operations, compared to when designed without considering them.},
  archive      = {J_DKE},
  author       = {Noa Roy-Hubara and Arnon Sturm and Peretz Shoval},
  doi          = {10.1016/j.datak.2023.102149},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102149},
  shortjournal = {Data Knowl. Eng.},
  title        = {Designing NoSQL databases based on multiple requirement views},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consolidating economic exchange ontologies for financial
reporting standard setting. <em>DKE</em>, <em>145</em>, 102148. (<a
href="https://doi.org/10.1016/j.datak.2023.102148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several UFO grounded economic exchange ontologies have been developed in the last decade, notably COFRIS, OntoREA, REA2, and ATE. An important question is whether they have reached a level to support financial reporting standard setters. This article first describes the foundational assumptions for exchange conceptualization and consolidates the latest developments in COFRIS - a core ontology for financial reporting information systems, within the most recent versions of the UFO theories and the OntoUML tool. The ontology is evaluated based on competency questions . Furthermore, it is confronted with the conceptual frameworks and standards for accounting and financial reporting and compared with other UFO grounded exchange ontologies. The results show both the maturity level of current exchange ontologies and their applicability to standard setting.},
  archive      = {J_DKE},
  author       = {Ivars Blums and Hans Weigand},
  doi          = {10.1016/j.datak.2023.102148},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102148},
  shortjournal = {Data Knowl. Eng.},
  title        = {Consolidating economic exchange ontologies for financial reporting standard setting},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep hyper optimization approach for disease classification
using artificial intelligence. <em>DKE</em>, <em>145</em>, 102147. (<a
href="https://doi.org/10.1016/j.datak.2023.102147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease classification using Artificial Intelligence (AI) is one of the emerging areas for medical professionals to diagnose the disease. There are common diseases like breast cancer, hepatitis, thyroid and heart attack are faced by most of the people that produce severe health problems. An Artificial Neural Network (ANN) is a part of AI used to identify the nonlinear relationship among the features for better prediction. However, there are some common problems like under fitting, overfitting, increased elapsed time and vanishing gradient that occur during the analysis and prediction which reduces the performance of the model. So, there is a need for complex structure recognition without overfitting and under fitting. The present study suggests the Deep Hyper Optimization (DHO) technique to reduce the elapsed time of execution. It is used to fine tune the weight, bias and identify the optimal number of hyper parameters in the hidden layer. As a second opinion to medical professionals, various state of art classification models are used such as Logistic Regression , Decision Tree , Random Forest , K Nearest Neighbor and Gradient Boosting algorithms. The performance of state of art models are observed and compared with Deep Hyper Optimization (DHO) technique. It chooses the best hyper parameter for classification based on the highest probability based on the divide and conquer approach . The proposed model is tested for four different datasets and the performance of model is observed based on the accuracy, elapsed time, precision, recall, and F1 score, FPR, FNR , TNR and Area under Curve (AUC).},
  archive      = {J_DKE},
  author       = {P. Dhivya and A. Bazilabanu},
  doi          = {10.1016/j.datak.2023.102147},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102147},
  shortjournal = {Data Knowl. Eng.},
  title        = {Deep hyper optimization approach for disease classification using artificial intelligence},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Process model forecasting and change exploration using time
series analysis of event sequence data. <em>DKE</em>, <em>145</em>,
102145. (<a href="https://doi.org/10.1016/j.datak.2023.102145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process analytics is a collection of data-driven techniques for, among others, making predictions for individual process instances or overall process models. At the instance level, various novel techniques have been recently devised, tackling analytical tasks such as next activity, remaining time, or outcome prediction. However, there is a notable void regarding predictions at the process model level. It is the ambition of this article to fill this gap. More specifically, we develop a technique to forecast the entire process model from historical event data. A forecasted model is a will-be process model representing a probable description of the overall process for a given period in the future. Such a forecast helps, for instance, to anticipate and prepare for the consequences of upcoming process drifts and emerging bottlenecks. Our technique builds on a representation of event data as multiple time series , each capturing the evolution of a behavioural aspect of the process model, such that corresponding time series forecasting techniques can be applied. Our implementation demonstrates the feasibility of process model forecasting using real-world event data. A user study using our Process Change Exploration tool confirms the usefulness and ease of use of the produced process model forecasts.},
  archive      = {J_DKE},
  author       = {Johannes De Smedt and Anton Yeshchenko and Artem Polyvyanyy and Jochen De Weerdt and Jan Mendling},
  doi          = {10.1016/j.datak.2023.102145},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102145},
  shortjournal = {Data Knowl. Eng.},
  title        = {Process model forecasting and change exploration using time series analysis of event sequence data},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Outranking relations based multi-criteria recommender system
for analysis of health risk using multi-objective feature selection
approach. <em>DKE</em>, <em>145</em>, 102144. (<a
href="https://doi.org/10.1016/j.datak.2023.102144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recommender system filters out important information from a large pool of information to set some important decisions in terms of recommendation. It has had an impact in almost every domain, including health, where it can extract information from massive amounts of digital data on patients to provide a better understanding of their clinical status and predict diseases. It is found that most of the real-world problems are dealing with multiple (conflicting) criteria, and handling those criteria in decision making is a challenging task too. In this context, this paper has proposed a health recommender system based on multi-criteria ranking and a multi-objective feature selection approach. Certain risk factors for the development of cervical cancer in women have been recommended by the proposed system, as well as some models for reliable prediction of this disease. The performance of the proposed system is tested and analysed by implementing it on the Cervical Risk Classification dataset with the three different ranking algorithms of Multi-Criteria Decision (MCDM) Making. It is found that the MOORA (Multi-objective Optimisation on the Basis of Ratio Analysis) MCDM algorithm outperforms the other two algorithms in terms of Precision@N, Recall@N, F1-score@N and Mean Reciprocal Rank (MRR)@N. The performance of the proposed system is also verified using four other benchmarking disease datasets, and statistical Anova and Wilcoxon tests have been done to validate the results.},
  archive      = {J_DKE},
  author       = {Madhusree Kuanr and Puspanjali Mohapatra},
  doi          = {10.1016/j.datak.2023.102144},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102144},
  shortjournal = {Data Knowl. Eng.},
  title        = {Outranking relations based multi-criteria recommender system for analysis of health risk using multi-objective feature selection approach},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pattern accumulated compression method for trajectories
constrained by urban road networks. <em>DKE</em>, <em>145</em>, 102143.
(<a href="https://doi.org/10.1016/j.datak.2023.102143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an edge computing framework, a moving object in an urban road network can collect both its own and others’ trajectories to answer spatial–temporal queries. How to store the ever increasing amount of trajectories with limited storage space is a great challenge. We observe that the traces of moving objects in urban road networks usually exhibit great repetition in terms of spatial and velocity distributions . So we propose two strategies, Pattern Accumulated Compression based on Average Velocity (PAC-AV) and Pattern Accumulated Compression based on Representative Velocity (PAC-RV), to compress trajectories online on the mobile devices . The former mainly exploits the path pattern to compress trajectories while the latter exploits both the path pattern and the velocity pattern to compress trajectories. Both PAC-AV and PAC-RV compress the trajectories by two stages. First, we extract the spatial as well as velocity components according to a velocity deviation threshold. Second, we compress the components into a series of identifiers leveraging a Path Pattern Dictionary (PPD) and a Velocity Pattern Dictionaries (VPD). Our method not only greatly cuts down the storage space but also directly supports the velocity-related queries on the compressed trajectories. Experimental results demonstrate the effectiveness and efficiency of our method.},
  archive      = {J_DKE},
  author       = {Jingyu Han and Wei Lu and Kang Ge and Man Zhu and Wei Chen and Yang Liu and Fan Wu},
  doi          = {10.1016/j.datak.2023.102143},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102143},
  shortjournal = {Data Knowl. Eng.},
  title        = {A pattern accumulated compression method for trajectories constrained by urban road networks},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An enhanced attentive implicit relation embedding for social
recommendation. <em>DKE</em>, <em>145</em>, 102142. (<a
href="https://doi.org/10.1016/j.datak.2023.102142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, recommendation system incorporating social networks has attracted a lot of research attention and is widely used to understand user preferences regarding social relations. Besides, graph neural networks(GNNs) have proven to learn graph-structured data effectively. Thus, integrating GNN into social recommendations has become one challenge. Many approaches encode both the user–item interaction and social relations to learn user preference. However, implicit relations are also crucial to understanding the features of users and items. Thus, we propose the attentive implicit relation embedding for social recommendation(SR-AIR), which models the user–item interaction and social networks, utilizing a graph attention mechanism on implicit relations of users and items. We evaluate our framework on two real-world datasets and demonstrate that our framework performs the best compared with state-of-the-art baselines.},
  archive      = {J_DKE},
  author       = {Xintao Ma and Liyan Dong and Yuequn Wang and Yongli Li and Zhen Liu and Hao Zhang},
  doi          = {10.1016/j.datak.2023.102142},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102142},
  shortjournal = {Data Knowl. Eng.},
  title        = {An enhanced attentive implicit relation embedding for social recommendation},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating multiple conceptual models from behavior-driven
development scenarios. <em>DKE</em>, <em>145</em>, 102141. (<a
href="https://doi.org/10.1016/j.datak.2023.102141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have proposed that generating conceptual models automatically from user stories might be useful for agile software development . It is, however, unclear from the state-of-the-art what a consistent and complementary set of models to generate is, how these models can be generated such that relationships and dependencies in a set of related user stories are unveiled, and why these models are useful in agile software development projects. In this paper, we address these questions through a Design Science research study. First, we define four stylized versions of Unified Modeling Language (UML) diagrams (i.e., use case diagram, class diagram , activity diagram, state machine diagram) that will be the target of the model generation. Although these stylized UML diagrams have a reduced abstract syntax , they offer different perspectives on the software system in focus with potential usefulness for requirements and software engineering . Second, we develop an automated model generation approach based on different design artifacts including a Natural Language Processing (NLP) tool that implements our approach. Key to our solution is the use of the Behavior-Driven Development (BDD) scenario template to document user stories. Using an example set of BDD scenarios as source of the model generation, we demonstrate the feasibility of our approach via the NLP tool that implements our approach. Third, we conduct an empirical study with experts in agile software development involving the researcher-guided interactive use of our tool to explore the use of the generated models. This study shows the perceived usefulness of the models that our tool can generate and identifies different uses and benefits of the models for requirements analysis, system design, software implementation, and testing in projects that employ agile methods.},
  archive      = {J_DKE},
  author       = {Abhimanyu Gupta and Geert Poels and Palash Bera},
  doi          = {10.1016/j.datak.2023.102141},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102141},
  shortjournal = {Data Knowl. Eng.},
  title        = {Generating multiple conceptual models from behavior-driven development scenarios},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach for credit-based resource aware load
balancing algorithm (CB-RALB-SA) for scheduling jobs in cloud computing.
<em>DKE</em>, <em>145</em>, 102138. (<a
href="https://doi.org/10.1016/j.datak.2022.102138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cloud computing has gained popularity, mainly because of its utility and relevance to current technological trends. It is an arrangement that is highly customizable and encapsulated for providing better computational services to its clients worldwide. In the cloud, computing scheduling plays a pivotal role in optimizing resources. A better scheduling algorithm should be efficient and impartial, reducing the makespan time with proper resource utilization. However, most scheduling algorithms customarily lead to less resource utilization, termed load imbalance. The analysis of the existing papers exhibits better Makespan time but cannot guarantee the load-balanced mapping of jobs with proper resource utilization. Therefore, to eliminate the shortcomings of the prevalent/existing algorithms and enhance the performance, CB-RALB-SA, Credit-based Resource Aware Load Balancing scheduling algorithm has been rendered. The proposed work ensures a balanced distribution of tasks based on the capabilities of the resources, which eventually proves sustainable improvement against the existing scheduling algorithms. Therefore, a novel Credit Based Resource Aware Load Balancing Scheduling algorithm (CB-RALB-SA) is proposed. The tasks weighted by the credit-based scheduling algorithm are then mapped to the resources considering each resource’s load and computing capability using FILL and SPILL functions of Resource Aware and Load using Honey bee optimization heuristic algorithm . With the experimental evaluations and results, it has been proved that the proposed approach provides 48.5\% better in Processing Time and 16.90\% better results in makespan time than the Existing CBSA-LB algorithm. Thus, it improves the processor’s efficiency while uplifting the whole system’s performance and has saved memory allocated to tasks and RAM .},
  archive      = {J_DKE},
  author       = {Abhikriti Narwal and Sunita Dhingra},
  doi          = {10.1016/j.datak.2022.102138},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102138},
  shortjournal = {Data Knowl. Eng.},
  title        = {A novel approach for credit-based resource aware load balancing algorithm (CB-RALB-SA) for scheduling jobs in cloud computing},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SimBio: Adopting particle swarm optimization for
ontology-based biomedical term similarity assessment. <em>DKE</em>,
<em>145</em>, 102137. (<a
href="https://doi.org/10.1016/j.datak.2022.102137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the semantic similarity between pairs of terms plays a vital role within a myriad of shared data applications, such as data integration and ontology evolution. A first step towards building such applications is to determine which terms are semantically similar to each other. One feasible way to compute the similarity of two terms is to assess their word similarity by exploiting different knowledge resources, e.g., ontologies or domain corpora. Recently, information-theoretic approaches have shown promising results by computing the information content of concepts from the knowledge provided by ontologies. In these methods, the Most Informative Common Subsumers M I C S MICS s of two concepts play an important role in determining their similarity. While intuitive, surprisingly, these approaches often do not mirror human judgement well. In this paper, we investigate the effect of choosing a suitable subsumer, called Consensus Common Subsumer (CCS) among all common ancestors of two concepts, on the quality of the term similarity assessment. We cast this issue as an optimization problem by adopting the Particle Swarm Optimization algorithm as one of the most capable optimization approaches. An empirical evaluation based on well-established biomedical benchmarks and ontologies illustrates the accuracy of the proposed approach compared to state-of-the-art approaches.},
  archive      = {J_DKE},
  author       = {Samira Babalou and Alsayed Algergawy and Birgitta König-Ries},
  doi          = {10.1016/j.datak.2022.102137},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102137},
  shortjournal = {Data Knowl. Eng.},
  title        = {SimBio: Adopting particle swarm optimization for ontology-based biomedical term similarity assessment},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic schema construction of electrical graph data
platform based on multi-source relational data models. <em>DKE</em>,
<em>145</em>, 102129. (<a
href="https://doi.org/10.1016/j.datak.2022.102129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data storage and management in power systems usually adopt relational databases . However, the relational database requires ample storage space and has low data retrieval and query efficiency. An electrical graph data platform can describe the complicated relationships between concepts and entities involved in power systems with the form of an association graph, which provides a better ability to organize, manage, and apply massive amounts of information. Since the construction of the top-level ontology model or schema for a specific field graph data platform is cumbersome, complex, and generally requires lots of association analysis and expert system intervention, it is insufficiently automated, time-consuming, and unable to cope with large-scale electric power knowledge. This paper proposes a method for automatically constructing the schema of an electrical graph data platform, which uses the diverse table structure information from the relational database and SQL language descriptions to extract ontologies to form the ontology candidate set automatically. Then the method utilizes ontology clustering and disambiguation to initial an ontology graph model and automatically update ontology and relationship expressions. Meanwhile, the model layering is used to construct a hierarchical model based on different business needs, and the schema optimization is applied according to expert comments.},
  archive      = {J_DKE},
  author       = {Yachen Tang and Xingping Wu and Chunlei Zhou and Guangxin Zhu and Jinwei Song and Guangyi Liu and Zhihong Li},
  doi          = {10.1016/j.datak.2022.102129},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102129},
  shortjournal = {Data Knowl. Eng.},
  title        = {Automatic schema construction of electrical graph data platform based on multi-source relational data models},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective and secured authentication and sharing of data
with dynamic groups in cloud. <em>DKE</em>, <em>145</em>, 102125. (<a
href="https://doi.org/10.1016/j.datak.2022.102125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing framework facilitates users to save data in the cloud server for using desired and scalable services. The data sharing in the cloud facilitates several contributors to generously distribute group data. However, how to facilitate data security in group and how to effective outsourced data are major issues in the existing methods. The aim of this research is to devise a technique for privacy preservation with data sharing in the cloud. The method includes nine phases, like set-up phase, user-registration phase, key generation phase, revocation phase, file generation, authentication , encryption, file access and verification, and decryption. In setup, the group manager performs the initialization of the system. In user registration, the registration of group members is done with group manager. In the key generation phase, a common key is generated amongst group members. In the user revocation phase, the user revocation is done with the group manager. In the file encryption phase, the member encrypts the data file. In the file generation phase, the data file are accumulated in the cloud. The file access phase helps to access the file from the cloud. Here, the authentication phase helps to access the cloud and the decryption phase helps to verify authorized users using the signature. The proposed privacy-based group data sharing outperformed with the highest detection rate of 95\%, smallest computation cost of 719 s and smallest memory of 56991123 bytes.},
  archive      = {J_DKE},
  author       = {Dorababu Sudarsa and Nagaraja Rao A. and Sivakumar A.P.},
  doi          = {10.1016/j.datak.2022.102125},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102125},
  shortjournal = {Data Knowl. Eng.},
  title        = {An effective and secured authentication and sharing of data with dynamic groups in cloud},
  volume       = {145},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retraction notice to “pollen risk levels prediction from
multi-source historical data” [data knowl. Eng. 142 (2022) 102096].
<em>DKE</em>, <em>144</em>, 102146. (<a
href="https://doi.org/10.1016/j.datak.2023.102146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Esso-Ridah Bleza and Valérie Monbet and Pierre-François Marteau},
  doi          = {10.1016/j.datak.2023.102146},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102146},
  shortjournal = {Data Knowl. Eng.},
  title        = {Retraction notice to “Pollen risk levels prediction from multi-source historical data” [Data knowl. eng. 142 (2022) 102096]},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional neural network-based high-precision and speed
detection system on CIDDS-001. <em>DKE</em>, <em>144</em>, 102130. (<a
href="https://doi.org/10.1016/j.datak.2022.102130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing interconnection of complex infrastructures gives very advanced communication functionalities, which gives a massive increase in connected devices and an associated flow volume. Cloud computing is constantly threatened by sophisticated attacks, which poses challenges for a security system. The Cloud will obsolete existing detection procedures against cyber-attacks where they would not be adapted accordingly. Intrusion detection is a classification problem wherein various machine learning and data mining techniques are applied to classify the network data into normal and attack traffic. Therefore, the proposal of new rapid and effective detection approaches is an absolute necessity. In this work, a proposed framework is a network anomaly detection system based on Deep Learning . The analysis carried out was based on the hyper-parameters of the layers of our model. This proposed model is a combination of two techniques; namely, a reduction of dimensions based on the approach of the main components and the second is based on a dense supervised neural network based on convolution neural network (CNN) for a multi-classification of normal and intrusive events from a recent data-set Coburg Network Intrusion Detection data-set (CIDDS-001). The experiments carried out show that the very precise choice of hyper-parameters gives better results. By running the proposed CNN model , it is capable of detecting attacks with an accuracy of 99.13\% and an execution time of 12 s.},
  archive      = {J_DKE},
  author       = {Mohamed_Amine Daoud and Youcef Dahmani and Mebarek Bendaoud and Abdelkader Ouared and Hasan Ahmed},
  doi          = {10.1016/j.datak.2022.102130},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102130},
  shortjournal = {Data Knowl. Eng.},
  title        = {Convolutional neural network-based high-precision and speed detection system on CIDDS-001},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HyperBit: A temporal graph store for fast answering queries.
<em>DKE</em>, <em>144</em>, 102128. (<a
href="https://doi.org/10.1016/j.datak.2022.102128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relationships or interactions among entities interactions often have occurrence time. So, temporal graph is becoming a popular model to represent temporal data. Temporal graph is generally much larger than corresponding non-temporal graph because an non-temporal edge may have many corresponding temporal edges. It raises challenges for querying temporal graphs. Here, we present HyperBit, a temporal graph store which can answer temporal queries efficiently. HyperBit models temporal labeled graph as a series of updates or log records on graph. Then we design an efficient partition storage for log. Since it is costly to answer temporal queries using logs due to full scan of log, we propose an optimal algorithm to build some snapshots to speedup query processing . So, HyperBit can answer temporal queries by applying log records on the snapshot close to the time in query. HyperBit employs SPARQL instead of a new language to describe temporal queries. Thus, HyperBit can process non-temporal queries on temporal/non-temporal graphs. Extensive experiments show that HyperBit significantly outperforms RDF-3x, Jena-TDB in terms of update speed while it has a compact storage. When querying static graphs, HyperBit also outperforms RDF-3X, Jena-TDB by a wide margin and is on par with TripleBit. For temporal queries, HyperBit can easily handle billion graphs, maintaining linear time growth so that has excellent scalability.},
  archive      = {J_DKE},
  author       = {Shaoqi Zang and Sheng Han and Pingpeng Yuan and Xuanhua Shi and Hai Jin},
  doi          = {10.1016/j.datak.2022.102128},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102128},
  shortjournal = {Data Knowl. Eng.},
  title        = {HyperBit: A temporal graph store for fast answering queries},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metaheuristic enabled intelligent model for stock market
prediction via integrating volatility spillover: India and its asian and
european counterparts. <em>DKE</em>, <em>144</em>, 102127. (<a
href="https://doi.org/10.1016/j.datak.2022.102127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the price of a stock market changes often owing to a variety of factors. As a result, making an accurate stock price prediction is a difficult process. Hence, this research work proposes a novel intellectual stock market prediction model that incorporates the volatility spillover over Indian and its Asian countries. This intellectual model mainly involves two phases like data library construction and stock market prediction. For stock market prediction, a Neural Network (NN) model is employed and this model intake the data of calculated indicators in the data library and makes the prediction of Indian market. To attain more precise prediction, the NN weight is optimally chosen via novel hybrid algorithm namely Fly Updated Whale Optimization Algorithm (FU-WOA) that is the hybridization of WOA and Firefly Algorithm (FF). At last, the suggested model performance is exploited by comparing other conventional models in the view of various metrics. Especially, the computational cost of the proposed hybrid FU-WOA–NN model is 38.12\%, 15.96\%, 15.52\%, 41.22\%, 16.07\% and 16.33\% better than existing LM-NN, FF-NN, GWO-NN, WOA-NN, PCA as well as ARIMA methods respectively.},
  archive      = {J_DKE},
  author       = {Deepak Kumar Tripathi and Saurabh Chadha and Ankita Tripathi},
  doi          = {10.1016/j.datak.2022.102127},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102127},
  shortjournal = {Data Knowl. Eng.},
  title        = {Metaheuristic enabled intelligent model for stock market prediction via integrating volatility spillover: India and its asian and european counterparts},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing item-based collaborative filtering by users’
similarities injection and low-quality data handling. <em>DKE</em>,
<em>144</em>, 102126. (<a
href="https://doi.org/10.1016/j.datak.2022.102126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing amount of the commercial items (movies, music, books, cars, etc.) produced each day by companies, it becomes very difficult for customers to find the suitable products satisfying their needs. Generally, Recommendation Systems (RSs) were used to fit this necessary requirement by solving the problem of information overload, specially on the web. Indeed, RS are designed to provide relevant resources to a client using certain information about users and resources. To the best of our knowledge, RS remains providing modest performances in many domains. In this context, we proposed a new model named CSWMC that combines two different techniques: item-based and user-based Collaborative Filtering. In fact, our proposed algorithm starts with the estimation of the suitable number of the user’s neighbors that offers to the Recommender System the optimal efficiency. Then, the system integrates this knowledge about users in the ‘Mean Centered’ aggregation method. Also, we proposed a simple method for handling the cold start and data sparsity problems that used mean value of the training datasets. The proposed models were validated through an experimental study on three standards datasets and compared with six well-known models. The obtained results demonstrated that our proposed model (in its two versions: with and without cold start handling) outperforms all the other models in terms of three evaluation metrics : RMSE , MAE and R 2 R2 .},
  archive      = {J_DKE},
  author       = {Fethi Fkih},
  doi          = {10.1016/j.datak.2022.102126},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102126},
  shortjournal = {Data Knowl. Eng.},
  title        = {Enhancing item-based collaborative filtering by users’ similarities injection and low-quality data handling},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-heuristic searched-ensemble learning for fake news
detection with optimal weighted feature selection approach.
<em>DKE</em>, <em>144</em>, 102124. (<a
href="https://doi.org/10.1016/j.datak.2022.102124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, fake news has turned into a major problem because of the negative impact that it creates on society. Social media allows people to spread information on the internet with slight investigations and to add fewer filters than the actual content. Nowadays, the false news in the internet community is unsure, and it creates a wrong impression among the users. Detecting false news has become a critical task based on shared content. To tackle the false news growth in social media, various automatic detection schemes were evaluated. The “Natural Language Processing (NLP)” method also gives a prominent solution for false news detection. The main intention of this paper is to design and introduce an innovative false news recognition method using Meta-heuristic Searched-Ensemble Learning (MS-EL). Further, the selected features are extracted by the “Term Frequency-Inverse Document Frequency (TF-IDF)” and also Word2vec features. Here, the extracted selected features are integrated with the Hybrid Squirrel–Dragonfly Search Optimization (HS-DSO) is used to optimize the weighted feature selection approach with the fitness function of solving data variance and correlation. The proposed MS-EL is adopted in the classification part, having three sets of classifiers, Long Short-Term Memory (LSTM), Support Vector Machine (SVM), and Deep Neural Network (DNN). Here, the ensemble classifier is enhanced by the same HS-DSO that shows the parameter tuning with a high convergence rate. From the experimental outcomes, the accuracy of HS-DSO-MS-EL is 22\% higher than BMO-MS-EL, 24\% higher than SP-BMO-MS-EL, 30\% higher than SSA-MS-EL, and 29\% higher than DA-MS-EL. Thus, the experimental analysis with standard datasets establishes that the introduced fake news detection method has gained higher accuracy than the existing models.},
  archive      = {J_DKE},
  author       = {S. Hannah Nithya ( Research Scholar ) and Dr. Arun Sahayadhas ( Professor )},
  doi          = {10.1016/j.datak.2022.102124},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102124},
  shortjournal = {Data Knowl. Eng.},
  title        = {Meta-heuristic searched-ensemble learning for fake news detection with optimal weighted feature selection approach},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep attention based optimized bi-LSTM for improving
geospatial data ontology. <em>DKE</em>, <em>144</em>, 102123. (<a
href="https://doi.org/10.1016/j.datak.2022.102123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the geospatial semantic information of remote sensing (RS) has attracted attention due to its various applications. This paper introduces a model for ontology based geospatial data integration using novel deep learning techniques. Here, we use a semantic web technology to establish the spatial ontology of risk knowledge with deep learning (DL), namely deep attention based bidirectional search and rescue LSTM for analysis. This approach takes into consideration of the study which presents the technique driven by the spatial ontology which minimizes the cost of modelling. The classification results from DL model enhances the performance of the ontology module. In this paper, ontological reasoning and DL model are jointly used for increase the module efficiency. The implementation of the proposed scheme is implemented on MATLAB 2020a. The performance of the implemented scheme is compared against the existing models like U-Net, Semantic referee and collaboratively boosting framework (CBF). The Overall accuracy (OA) of the system is found to be 0.923 on UCM dataset. Thus, the developed spatial ontologies provide the semantic foundation to achieve a semantic knowledge of geospatial data understandings.},
  archive      = {J_DKE},
  author       = {Palaniappan Sambandam and D. Yuvaraj and P. Padmakumari and Subbiah Swaminathan},
  doi          = {10.1016/j.datak.2022.102123},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102123},
  shortjournal = {Data Knowl. Eng.},
  title        = {Deep attention based optimized bi-LSTM for improving geospatial data ontology},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain based securing medical records in big data
analytics. <em>DKE</em>, <em>144</em>, 102122. (<a
href="https://doi.org/10.1016/j.datak.2022.102122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The patient privacy is danger while medical records and data are transmitted or share beyond secure big data. This is because violations push them to the margins and they begin to avoid fully revealing their stages. This kind of stages contains negative impact in scientific investigate. To overcome this issue, Secure Block Chain System for Managing and Sharing Electronic Medical Records in Big Data Field is proposed. In this manuscript, a Cryptographic Hash Generator (CHG) technique based Secured and Trusted Data storage and transmission using Block Chain (BC) in Hadoop Distributed File System (HDFS). Initially, the Big data collected from the health care center is partitioned into sensitive and insensitive data. Block chain system utilizes an asymmetric cryptography for validating transactions authentication . Here, the user key is created through secured bitwise cryptographic hash generator (CHG) while there is required to fetch the newly record for usage. In block chain system, when a user seeking data from a healthcare application have forward a request to CHG. The message is send back to the user with a secret key for confirmation. The key can be decrypted or even denied access if only a valid user allows the user to link to this cluster. Only sensitive data were selected to the process of encryption for the process of encryption, this CHG technique employs the Discrete Shearlet Transform (DST) for encrypting the data, and the data’s are warehoused in the block chain to upgrade the level of security. And the insensitive data are put directly on the Hadoop Distributed File System. During the verification process , CHG is utilized for creating the request forward through the user. The operator creates the purpose of remote key to create the block (request) and signing the request using transaction private key, then forward to request queuing. To validate a request, the request from the queue is supplied first and an Improved Grey wolf Optimization algorithm (IGWO) is utilized to determine the optimal request that is fetch through the consensus node for initiating the process of validation. After accepting the user’s request, access is given to the user associated with input or requested data, then the verified request is set to broadcast. The proposed method is executed in JAVA and Hadoop platform. Experimental results show that the proposed BC-CHG-DST-IGWOA shows better performances of higher Efficiency 20.14\%, 31.25\%, 24.33\%, 14.69\%, lower time 16.12\%, 15.09\%, 21.36\%, 46.26\% compared with the existing methods, such as medical records managing and securing blockchain based system supported by genetic algorithm with discrete wavelet transform (BC-SMR-BD-GA-DWT), DQN-based optimization framework to secure shared blockchain systems (BC-SMR-BD-DQNSB), Hyper ledger blockchain enabled secure medical record management along deep learning-based diagnosis model (BC-SMR-BD-HBESDM-DLD), Secure attribute-based signature scheme with multiple authorities for blockchain in electronic health records system (BC-SMR-BD-MA-ABS) respectively.},
  archive      = {J_DKE},
  author       = {V. Santhana Marichamy and V. Natarajan},
  doi          = {10.1016/j.datak.2022.102122},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102122},
  shortjournal = {Data Knowl. Eng.},
  title        = {Blockchain based securing medical records in big data analytics},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized speaker change detection approach for speaker
segmentation towards speaker diarization based on deep learning.
<em>DKE</em>, <em>144</em>, 102121. (<a
href="https://doi.org/10.1016/j.datak.2022.102121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speaker diarization is the partitioning of an audio source stream into homogeneous segments according to the speaker’s identity. It can improve the readability of an automatic speech transcription by segmenting the audio stream into speaker turns and identifying the speaker’s true identity when used in combination with speaker recognition systems . Generally, the automatic speaker diarization is done based on two phases, like the transformation of audio segments into feature representation and the clustering. In this paper, clustering along with a hybrid optimization technique is carried out for performing the speaker diarization. For that, the extracted features from the audio signal is processed under speech activity prediction in order to identify the speak segments. The diarization process is done by Deep Embedded Clustering (DEC) in which the constants are trained by the developed Fractional Anticorona Whale Optimization Algorithm (FrACWOA). The FrACWOA is a hybrid optimization technique, which is designed by adapting the concept of fractional theory, precaution behaviour of COVID-19 and hunting performance of whales. DEC performs the diarization, which concurrently learns the representation of features as well as cluster assignments with neural networks . Using a mapping from the information space to a lower-dimensional feature space, DEC repeatedly discovers the most effective solution for a clustering objective. On the basis of testing accuracy, diarization error, false discovery rate (FDR), false negative rate (FNR), and false positive rate (FPR) of 0.902, 0.627, 0.276, 0.117, and 0.118, respectively, the developed FrACWOA+DEC algorithm performed much better with six speakers using the EenaduPrathidwani dataset. Comparing the accuracy of the proposed method to existing approaches such as Active learning, DE+K-means, LSTM , MCGAN, ANN-ABC-LA, and ACWOA+DFC, the accuracy of the proposed method is 12.97\%, 10.31\%, 9.75\%, 7.53\%, 4.32\%, and 2.106\% higher when using 6 speakers.},
  archive      = {J_DKE},
  author       = {VijayKumar K. and Rajeswara Rao R.},
  doi          = {10.1016/j.datak.2022.102121},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102121},
  shortjournal = {Data Knowl. Eng.},
  title        = {Optimized speaker change detection approach for speaker segmentation towards speaker diarization based on deep learning},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimized deep learning-based prediction model for chiller
performance prediction. <em>DKE</em>, <em>144</em>, 102120. (<a
href="https://doi.org/10.1016/j.datak.2022.102120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of primary energy consumption and global climate change are encouraged air conditioning chillers to conventional air-conditioning methods. In the last previous couple of decades, several absorption technologies are introduced for absorption cooling systems and solar cooling to achieve higher chiller performance. In this paper, an efficient and optimal chiller performance prediction mechanism, named Spider monkey Bat Algorithm (SMBat)-based Generative adversarial network (SMBat-based GAN), is proposed. It is exploited to predict the chiller performance from time-series data. The proposed SMBat algorithm integrates the Spider Monkey Optimization (SMO) and the Bat algorithm. The GAN classifier is used to predict the chiller performance using the time series data based on the fitness function. In addition, extraction of the feature is achieved using the chilled water supply temperature, chilled water return temperature, condenser water return temperature, chiller water flow, cooling capacity, and power utilization . The features are selected by the wrapper method for selecting the appropriate features for better chiller performance prediction. Then these features are forwarded to the prediction module, where the prediction is done based on the proposed SMBat-based GAN. Finally, the experimental analysis exhibits that the proposed model offered better performance based on the metrics, such as Mean Square Error (MSE), R2-score, and Mean Absolute Error (MAE) when considering four datasets. The developed SMBat-based GAN attained better results for dataset-3 with a minimal MAE of 0.008, a minimal MSE of 0.0001, and a maximal R2-score of 0.999. The R2-score of the proposed method is 5.905\%, 0.4\%, 0.2\%, and 1\% higher when compared to the existing approaches, namely, regression model, ANN+ Bat, ANN, and Multi obj GA+NN.},
  archive      = {J_DKE},
  author       = {Tamilarasan Sathesh and Yang-Cheng Shih},
  doi          = {10.1016/j.datak.2022.102120},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102120},
  shortjournal = {Data Knowl. Eng.},
  title        = {Optimized deep learning-based prediction model for chiller performance prediction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A constructive deep convolutional network model for
analyzing video-to-image sequences. <em>DKE</em>, <em>144</em>, 102119.
(<a href="https://doi.org/10.1016/j.datak.2022.102119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new technique to add sign language as a special and real-time feature. In addition to gestures, sign language also uses grammatical rules, linguistic frameworks, etc. We require a method that can extract meaningful sentences from non-manual features. Utilizing techniques from computer graphics , neurologic music therapy, and NN-based image/video formation, this is accomplished. Our goal is to use this to process dynamic images for output generation and real-time classification. Current CNN-based techniques operate by taking the entire video as input, dividing it into layers for the classifier to work on, and then combining and providing the output to the user. Here, Convolutional Deep VGG-16 (CDVGG-16) classifiers adopted for sign feature learning , which is iteratively trained and tested. Their architecture consists of blocks, where each block is composed of 2D Convolution and Max Pooling layers. We prefer VGG-16 over VGG-19 in order to improve feature extraction and decrease overfitting. The captured sign language video processing with some pre-processing and classification steps leads to the extraction of sign language feature space assisting deaf people. As a positive approach, the Sign Language Recognition (SLR) modeling framework with deep learning is introduced as it includes various notable advantages. There are three essential phases for provisioning the SLR framework: (1) data acquisition, (2) image pre-processing, and (3) CDVGG-16 classifier. The simulation is done with the MATLAB 2020a environment with various performance metrics like accuracy, precision, F-measure, recall and running time. The promising results show that real-world SLR applications can be formed using the suggested SLR paradigm. The proposed CDVGG-16 gives 95.6\% prediction accuracy, 85.6\% precision, 99.9\% recall, 96.8\% F-measure and 0.125 s running time. For improved performance, our proposed method employs normalization and examines sequential image frames. The results and accuracy indicate that it is a suitable method for video-image classification, as expected, and that deep layers and data augmentation can be used to address any overfitting and underfitting issues. Instead of being a literal translation frame by frame, the filter size is adjusted to ensure that output is translated appropriately given the context.},
  archive      = {J_DKE},
  author       = {Guntupalli Manoj Kumar and A. Pandian},
  doi          = {10.1016/j.datak.2022.102119},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102119},
  shortjournal = {Data Knowl. Eng.},
  title        = {A constructive deep convolutional network model for analyzing video-to-image sequences},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Survey of fake news detection using machine intelligence
approach. <em>DKE</em>, <em>144</em>, 102118. (<a
href="https://doi.org/10.1016/j.datak.2022.102118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the extensive spreading of all information through digital platforms , it is of maximal importance that each people get to differentiate between them. Fake news is a vast problem in our society we cannot predict which news is fake or real without having knowledge or proof of that particular news. This has become a supreme problem, so we decided to create a solution to this problem. Thus, we built a small model which helps in detecting fake news, where we are dealing with some articles which have been collected from the internet. We have labeled each of them as either fake or true. We have trained our dataset using these articles and have used different machine learning algorithms like Passive Aggressive Classifier, Naïve Bayes, Logistic Regression , Decision Tree , Long short term memory (LSTM), and Bidirectional Encoder Representations from Transformers (BERT) to compare the results. Our experimental result has achieved 99.6\% accuracy from Decision Tree algorithm and obtained 99.8\% recall from LSTM for detection of fake news. Passive Aggressive Classifier performs excellent on a large data set.},
  archive      = {J_DKE},
  author       = {Aishika Pal and Pranav and Moumita Pradhan},
  doi          = {10.1016/j.datak.2022.102118},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102118},
  shortjournal = {Data Knowl. Eng.},
  title        = {Survey of fake news detection using machine intelligence approach},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A conceptual model supporting decision-making for test
automation in agile-based software development. <em>DKE</em>,
<em>144</em>, 102111. (<a
href="https://doi.org/10.1016/j.datak.2022.102111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of agile development notion, software test automation is widely adopted by software organizations to reduce the testing time and costs during the software development process. However, it is essential to carefully implement the test automation to ensure the project’s success. In the literature, several studies have indicated that decision-making is crucial for the adoption of test automation. For deciding about test automation adoption, several factors, including the skill level of testers, test tool, and development process, need to be considered since automated testing performance heavily depends on the mentioned factors and their interactions. Thus, it is of vital importance to ascertain the factors that influence test automation in the context of Agile-based Software Development (ASD). To accomplish this, we perform a systematic literature review and a questionnaire-based survey. In total, 21 factors are identified that significantly impact test automation in the ASD context. Additionally, the identified factors are categorized into five main classes. Following the identified factors’ assessment, this work proposes a conceptual model that supports agile practitioners in deciding about the adoption of test automation in the context of ASD. Finally, this work validates the proposed model to assess its practical applicability. The validation of the proposed model is accomplished by using an expert-based validation approach. The attained results show that the proposed model supports agile practitioners in estimating the possibility of success when implementing test automation in the ASD context.},
  archive      = {J_DKE},
  author       = {Shimza Butt and Saif Ur Rehman Khan and Shahid Hussain and Wen-Li Wang},
  doi          = {10.1016/j.datak.2022.102111},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102111},
  shortjournal = {Data Knowl. Eng.},
  title        = {A conceptual model supporting decision-making for test automation in agile-based software development},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ontology-based semantic retrieval of documents using
word2vec model. <em>DKE</em>, <em>144</em>, 102110. (<a
href="https://doi.org/10.1016/j.datak.2022.102110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic retrieval of engineering knowledge is crucial in various engineering activities, such as process development and product model planning. To make a solution for this issue, previously, a few word-based semantic enabling existing approaches such as Lexical Retrieval Model with Semantic Residual Embedding’s (LR-SRE), Document Retrieval Model through Semantic Linking (DR-MTS), Semantic Term Matching in Axiomatic Approaches to Information Retrieval (STM-IR) and Hybrid Ontology for Semantic Information Retrieval Model using Keyword Matching Indexing System (HOS-IR) were utilized. But, they all have shown less query-based accuracy results than the required value. In our proposed Information Retrieval (IR) design, the semantic knowledge-based retrieval scheme has been implemented. For query, entered by a user and processed for finding the dominated word. Word is then compared for its similarity equations, and similarity values are then computed to give output. Highly similar values are obtained as the class value. From class, the respective clusters are selected. Then, documents in that cluster are retrieved and ranked according to the relevance of the user. To support the accuracy level performance of this IR system , a word2vec model has been employed with the benefits of Horse Herd Optimization (HHO), which helps to extract vectors as features for classification. These results are stored as a .csv file for further retrieval. By implementing the proposed IR-word2vec model, the results showed that it outperforms other existing techniques by improved similarity index and accuracy for query results in an execution time of 1.7 s.},
  archive      = {J_DKE},
  author       = {Anil Sharma and Suresh Kumar},
  doi          = {10.1016/j.datak.2022.102110},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102110},
  shortjournal = {Data Knowl. Eng.},
  title        = {Ontology-based semantic retrieval of documents using word2vec model},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event detection with multi-order edge-aware graph
convolution networks. <em>DKE</em>, <em>143</em>, 102109. (<a
href="https://doi.org/10.1016/j.datak.2022.102109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event detection, specifically, identifying the type of events in a piece of text, is a crucial task in information extraction. Previous event detection task research proved that a syntactic graph based on a dependency tree can be integrated into a graph convolutional neural network to better capture the context of a sentence. However, most of the existing studies rely only on first-order syntactic relations and usually ignore dependence label information. In this paper, we propose a multi-order edge-aware graph convolution network (MEA-GCN) based on multi-order grammar and typed dependent label information. We use the BERT representation and the multi-head attention update mechanism to generate a variety of multi-order syntactic graph representations ; consequently, our model can automatically learn dependent information and improve the representation ability of the syntactic relations. The experimental results on the widely used ACE 2005 and TAC KBP 2015 show that our model achieves significant improvement over the competitive baseline methods .},
  archive      = {J_DKE},
  author       = {Jing Wang and Kexin Wang and Yong Yao and Hui Zhao},
  doi          = {10.1016/j.datak.2022.102109},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102109},
  shortjournal = {Data Knowl. Eng.},
  title        = {Event detection with multi-order edge-aware graph convolution networks},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GIRE: Gender-inclusive requirements engineering.
<em>DKE</em>, <em>143</em>, 102108. (<a
href="https://doi.org/10.1016/j.datak.2022.102108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gender inclusion is fundamental to a prosperous society, but inequality and exclusion persist in various sectors of it. One of them is the ICT field, which is still struggling to represent the diversity of those it serves. The lack of diversity and power imbalance in software development affects the produced systems, that, instead of advancing gender inclusion, create new barriers in achieving it. Although considered neutral, software does not equally serve everyone who depends on it, favoring characteristics that are statistically more observed in those that are represented during development. As software development teams are predominantly male, it is not surprising that existing systems favor characteristics that are statistically more observed in men over characteristics observed in other genders. As technologies rapidly evolve and revolutionize the way we live, addressing this problem becomes urgent to ensure that these systems benefit everyone, regardless of their gender. As a first step towards this goal, we performed a systematic mapping study on gender issues in software engineering whose results indicated that gender impacts development and systems, but there are limited approaches for addressing it in Requirements Engineering . This study served as the foundation for proposing a conceptual model for gender-inclusive requirements. Its main objective is to facilitate discussion and analysis of gender and related concepts in the elicitation process to include them in the specification of requirements. In this paper, we extend this work by illustrating the concepts with an example, by presenting a process for using the knowledge of the model and a prototype tool that implements it, and by discussing an evaluation with 31 participants of the conceptual model’s usefulness, difficulty of understanding, strengths and weaknesses, use and recommendation, and finally, its components. The results were positive as both novices and experts in conceptual modeling considered the model useful, provided comprehensive feedback on its strengths but also suggestions for improvement, and most answered positively to the questions about whether they would use and recommend it.},
  archive      = {J_DKE},
  author       = {Inês Nunes and Ana Moreira and João Araujo},
  doi          = {10.1016/j.datak.2022.102108},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102108},
  shortjournal = {Data Knowl. Eng.},
  title        = {GIRE: Gender-inclusive requirements engineering},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A social and technical sustainability requirements
catalogue. <em>DKE</em>, <em>143</em>, 102107. (<a
href="https://doi.org/10.1016/j.datak.2022.102107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Climate change calls for action from all sectors of our global economy, including ICT. Therefore, it is important to change the way we develop software to address the challenges posed by sustainability. Our goal is to contribute with a reusable sustainability requirements catalogue that helps developers be aware of sustainability-related properties worth considering during software development. The information for this catalogue was gathered via a systematic mapping study , whose results were synthesised in feature models and then modelled using iStar for a more expressive and configurable representation. A qualitative evaluation of the catalogue’s readability, interest, utility, and usefulness by 50 participants from the domain, showed that around 79\% of the respondents found the catalogue “Good” or “Very Good”. However, more than 5\% of the expert participants found weaknesses regarding most of the evaluated questions and around 25\% are neutral in their overall evaluation. This led us to evolve the initial version of the catalogue for the social and technical dimensions of sustainability to improve its completeness and usefulness. This is achieved by aligning the information gathered in the systematic mapping study with the well-established quality model of the ISO/IEC 25010:2011, as we expect most of the experts are familiar with those qualities and respective hierarchies. During this process, we found information that led us to propose two additional qualities that were not covered by the ISO standard: fairness and legislation. We applied this evolved version of the catalogue to the U-Bike project comparing the requirements elicited without the catalogue with those identified using the catalogue. The result suggests that new sustainability requirements were worth considering from a sustainability point of view, supporting the usefulness of the catalogue.},
  archive      = {J_DKE},
  author       = {Ana Moreira and João Araújo and Catarina Gralha and Miguel Goulão and Isabel Sofia Brito and Diogo Albuquerque},
  doi          = {10.1016/j.datak.2022.102107},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102107},
  shortjournal = {Data Knowl. Eng.},
  title        = {A social and technical sustainability requirements catalogue},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning and sentiment analysis of bahraini
dialects sequential text data using multilingual deep learning approach.
<em>DKE</em>, <em>143</em>, 102106. (<a
href="https://doi.org/10.1016/j.datak.2022.102106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is a crucial Natural Language Processing task to analyze the user’s emotions and opinions towards events, entities, services, or products. Arabic NLP faces numerous challenges, some of which include: (1) the scarcity of resources, especially in modern standard Arabic and Arabic dialects, particularly the Bahraini one; (2) lack of multilingual deep learning models; and (3) insufficient transfer learning studies on Arabic dialects in general and Bahraini dialects specifically. This research aims to create a balanced dataset of Bahraini dialects that covers product reviews by translating English Amazon product reviews to modern standard Arabic, which were then converted to Bahraini dialects. Another aim of this research is to provide a reusable multilingual deep learning long short term memory model to analyze the parallel dataset of English, modern standard Arabic, and Bahraini dialects, which differ in linguistic properties . Many experiments were conducted using train-validate-test split and k-fold cross-validation to evaluate the model performance using accuracy, F1 score, and AUC metrics. The model runs average accuracy on all datasets ranging from 96.72\% to 97.04\%, 97.91\% to 97.93\% in F1 score, while in AUC was 98.46\% to 98.7\% when utilizing an augmentation technique. Moreover, a pre-trained Long Short Term Memory model was created to exploit and transfer the knowledge gained from analyzing the product reviews in Bahraini dialects to perform sentiment analysis on a small dataset of movie comments in the same dialects. The Pre-trained model performance was 96.97\% accuracy, 96.65\% F1 score, and 97.94\% AUC.},
  archive      = {J_DKE},
  author       = {Thuraya M. Omran and Baraa T. Sharef and Crina Grosan and Yongmin Li},
  doi          = {10.1016/j.datak.2022.102106},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102106},
  shortjournal = {Data Knowl. Eng.},
  title        = {Transfer learning and sentiment analysis of bahraini dialects sequential text data using multilingual deep learning approach},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-layer hybrid (MLH) balancing technique: A combined
approach to remove data imbalance. <em>DKE</em>, <em>143</em>, 102105.
(<a href="https://doi.org/10.1016/j.datak.2022.102105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data is one of the most important elements currently for business decisions as well as for scientific research. However, data imbalance is a critical issue that affects the outcome of business decisions or the performance of a model as the decision would be biased towards the majority class (MaC). Existing data balancing techniques have a major drawback: these create new artificial samples randomly which create outliers and hamper the potentiality of the original dataset. In this paper, we propose a Multi-Layer Hybrid (MLH) Balancing Scheme which combines three oversampling techniques in two layers. By combining the characteristics of ADASYN, SVM-SMOTE, and SMOTE+ENN with our data processing techniques, our scheme gives a distributed, noise-free output. It also creates new data points within the range of the original dataset, which keeps the originality of the new data points. Thus, the generated dataset is much suitable for machine learning models to achieve results with higher accuracy for highly imbalanced data . Experimental results on datasets with an imbalance ratio of up to 59 show that our proposed scheme can effectively generate a balanced dataset. We apply the resultant dataset to Random Forest and Artificial Neural Network algorithms; comparison with existing techniques shows that our scheme gives better results.},
  archive      = {J_DKE},
  author       = {Muhammad Tanveer Islam and Hossen A. Mustafa},
  doi          = {10.1016/j.datak.2022.102105},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102105},
  shortjournal = {Data Knowl. Eng.},
  title        = {Multi-layer hybrid (MLH) balancing technique: A combined approach to remove data imbalance},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive collaborative exploration using incomplete
contexts. <em>DKE</em>, <em>143</em>, 102104. (<a
href="https://doi.org/10.1016/j.datak.2022.102104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A well-known knowledge acquisition method in the field of Formal Concept Analysis (FCA) is attribute exploration . It is used to reveal dependencies in a set of attributes with help of a domain expert. In most applications no single expert is capable (time- and knowledge-wise) of exploring the knowledge domain alone. However, there is up to now no theory that models the interaction of multiple experts for the task of attribute exploration with incomplete knowledge . To this end, we develop a theoretical framework that allows multiple experts to explore a domain together. We use a representation of incomplete knowledge as three-valued contexts. We then adapt the corresponding version of attribute exploration to fit the setting of multiple experts. We suggest formalizations for key components like expert knowledge, interaction and collaboration strategy. In particular, we define an order that allows to compare the results of different exploration strategies on the same task with respect to their information completeness. Furthermore, we discuss other ways of comparing collaboration strategies and suggest avenues for future research.},
  archive      = {J_DKE},
  author       = {Maximilian Felde and Gerd Stumme},
  doi          = {10.1016/j.datak.2022.102104},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102104},
  shortjournal = {Data Knowl. Eng.},
  title        = {Interactive collaborative exploration using incomplete contexts},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An automated multi-web platform voting framework to predict
misleading information proliferated during COVID-19 outbreak using
ensemble method. <em>DKE</em>, <em>143</em>, 102103. (<a
href="https://doi.org/10.1016/j.datak.2022.102103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spreading of misleading information on social web platforms has fuelled massive panic and confusion among the public regarding the Corona disease, the detection of which is of paramount importance. Previous studies mainly relied on a specific web platform to collect crucial evidence to detect fake content. The analysis identifies that retrieving clues from two or more different sources/web platforms gives more reliable prediction and confidence concerning a specific claim. This study proposed a novel multi-web platform voting framework that incorporates 4 sets of novel features: content, linguistic, similarity, and sentiments. The features have been gathered from each web-platforms to validate the news. To validate the fact/claim, a unique source platform is designed to collect relevant clues/headlines from two web platforms (YouTube, Google) based on specific queries and extracted features concerning each clue/headline. The proposed idea is to incorporate a unique platform to assist researchers in gathering relevant and vital evidence from diverse web platforms. After evaluation and validation, it has been identified that the built model is quite intelligent, gives promising results, and effectively predicts misleading information. The model correctly detected about 98\% of the COVID misinformation on the constraint Covid-19 fake news dataset. Furthermore, it is observed that it is efficient to gather clues from multiple web platforms for more reliable predictions to validate the news. The suggested work depicts numerous practical applications for health policy-makers and practitioners that could be useful in safeguarding and implicating awareness among society from misleading information dissemination during this pandemic.},
  archive      = {J_DKE},
  author       = {Deepika Varshney and Dinesh Kumar Vishwakarma},
  doi          = {10.1016/j.datak.2022.102103},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {102103},
  shortjournal = {Data Knowl. Eng.},
  title        = {An automated multi-web platform voting framework to predict misleading information proliferated during COVID-19 outbreak using ensemble method},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
