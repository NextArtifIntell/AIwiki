<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr---781">PR - 781</h2>
<ul>
<li><details>
<summary>
(2023). Expansion window local alignment weighted network for
fine-grained sketch-based image retrieval. <em>PR</em>, <em>144</em>,
109892. (<a href="https://doi.org/10.1016/j.patcog.2023.109892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) is a worthwhile task, which can be useful in many scenarios like recommendation systems, receiving a great deal of attention. In this study, we analyze challenges faced in FG-SBIR and propose a novel Expansion Window Local Alignment Weighted Network (EWLAW-Net). Specifically, it contains two main components: the Expansion Window Local Alignment module (EWLA) and the Local Weighted Fusion module (LWF). The EWLA module adopts an expansion window mechanism to align local features extracted from the backbone with the same semantic meaning between photos and sketches. The LWF module assigns weights to each local feature of the sketch after evaluating their importance and fuses them to calculate the similarity between the sketch and photos for retrieval. Experiments are conducted on five datasets and the results demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Zi-Chao Zhang and Zhen-Yu Xie and Zhen-Duo Chen and Yu-Wei Zhan and Xin Luo and Xin-Shun Xu},
  doi          = {10.1016/j.patcog.2023.109892},
  journal      = {Pattern Recognition},
  pages        = {109892},
  shortjournal = {Pattern Recognition},
  title        = {Expansion window local alignment weighted network for fine-grained sketch-based image retrieval},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motional foreground attention-based video crowd counting.
<em>PR</em>, <em>144</em>, 109891. (<a
href="https://doi.org/10.1016/j.patcog.2023.109891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem of video crowd counting. Compared with single image crowd counting, video provides gradual spatial and temporal variation information that would help to strengthen the robustness of crowd counting. Therefore, it is critical to make full use of neighboring frames both in feature extraction and final prediction for current frame’s estimation. Based on the above observations, we propose a motional foreground attention-based video crowd counting method. Specifically, we first leverage an foreground estimation module based on ConvNeXt to extract the motional features from bidirectional frame differences and output a foreground estimation map . Then the motional features combined with the static features of current frame are sent into feature fusion network, where foreground estimation map is transformed as attention weights for crowd number estimation. Three new indoor video datasets are manually annotated. The proposed method achieves state-of-the-art performance on all indoor and outdoor video datasets.},
  archive      = {J_PR},
  author       = {Miaogen Ling and Tianhang Pan and Yi Ren and Ke Wang and Xin Geng},
  doi          = {10.1016/j.patcog.2023.109891},
  journal      = {Pattern Recognition},
  pages        = {109891},
  shortjournal = {Pattern Recognition},
  title        = {Motional foreground attention-based video crowd counting},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crisis event summary generative model based on hierarchical
multimodal fusion. <em>PR</em>, <em>144</em>, 109890. (<a
href="https://doi.org/10.1016/j.patcog.2023.109890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to quickly obtain information about crisis events on social media such as Twitter and Weibo is crucial for follow-up rescue work and the promotion of postdisaster reconstruction. Therefore, it is very important to obtain useful information through multimodal summary generation technology. The current technology for generating crisis event summaries is mainly affected by unimodal bias and disregards the diversity of information in text and images. To solve these problems, this paper proposes a hierarchical multimodal crisis event summary generation model based on the modal alignment premise and hierarchical thinking. First, the visual context vector and text context vector are obtained, and then the hierarchical multimodal pointer model is employed to generate the text summary. Thus, the modal deviation is solved. Second, to select high-quality images, this paper proposes a dynamic selection strategy, which to some extent considers the requirements of the high correlation between text and images and the diversity of crisis information. Last, the experimental results based on the crisis event data in the MSMO dataset show that the proposed model achieves good performance in the summary generation and image selection of crisis events.},
  archive      = {J_PR},
  author       = {Jing Wang and Shuo Yang and Hui Zhao},
  doi          = {10.1016/j.patcog.2023.109890},
  journal      = {Pattern Recognition},
  pages        = {109890},
  shortjournal = {Pattern Recognition},
  title        = {Crisis event summary generative model based on hierarchical multimodal fusion},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gaussian kernel for kendall’s space of m-d shapes.
<em>PR</em>, <em>144</em>, 109887. (<a
href="https://doi.org/10.1016/j.patcog.2023.109887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop an approach to exploit kernel methods with data lying on the m m -D Kendall shape space. When data arise in a finite-dimensional curved Riemannian manifold, as in this case, the usual Euclidean computer vision and machine learning algorithms must be treated carefully. A good approach is to use positive definite kernels on manifolds to embed the manifold with its corresponding metric in a high-dimensional reproducing kernel Hilbert space , where it is possible to utilize algorithms developed for linear spaces. Different Gaussian kernels can be found in the literature on the 2-D Kendall shape space to perform this embedding. The main novelty of this work is to provide a Gaussian kernel for the m m -D Kendall shape space. This new Kernel coincides in the case m = 2 m=2 with the Gaussian kernels most widely used in the Kendall planar shape space and allows to define an embedding of the m m -D Kendall shape space into a reproducible kernel Hilbert space . As far as we know, the complexity of the m m -D Kendall shape space has meant that this embedding has not been addressed in the literature until now. This methodology will be tested on a machine learning problem with a simulated and a real data set .},
  archive      = {J_PR},
  author       = {Vicent Gimeno i Garcia and Ximo Gual-Arnau and M. Victoria Ibáñez and Amelia Simó},
  doi          = {10.1016/j.patcog.2023.109887},
  journal      = {Pattern Recognition},
  pages        = {109887},
  shortjournal = {Pattern Recognition},
  title        = {A gaussian kernel for kendall’s space of m-D shapes},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ARAI-MVSNet: A multi-view stereo depth estimation network
with adaptive depth range and depth interval. <em>PR</em>, <em>144</em>,
109885. (<a href="https://doi.org/10.1016/j.patcog.2023.109885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-View Stereo (MVS) is a fundamental problem in geometric computer vision which aims to reconstruct a scene using multi-view images with known camera parameters. However, the mainstream approaches represent the scene with a fixed all-pixel depth range and equal depth interval partition, which will result in inadequate utilization of depth planes and imprecise depth estimation. In this paper, we present a novel multi-stage coarse-to-fine framework to achieve adaptive all-pixel depth range and depth interval. We predict a coarse depth map in the first stage, then an Adaptive Depth Range Prediction module is proposed in the second stage to zoom in the scene by leveraging the reference image and the obtained depth map in the first stage and predict a more accurate all-pixel depth range for the following stages. In the third and fourth stages, we propose an Adaptive Depth Interval Adjustment module to achieve adaptive variable interval partition for pixel-wise depth range. The depth interval distribution in this module is normalized by Z-score, which can allocate dense depth hypothesis planes around the potential ground truth depth value and vice versa to achieve more accurate depth estimation. Extensive experiments on four widely used benchmark datasets (DTU, TnT, BlendedMVS, ETH 3D) demonstrate that our model achieves state-of-the-art performance and yields competitive generalization ability . Particularly, our method achieves the highest Acc and Overall on the DTU dataset, while attaining the highest Recall and F 1 F1 -score on the Tanks and Temples intermediate and advanced dataset. Moreover, our method also achieves the lowest e 1 e1 and e 3 e3 on the BlendedMVS dataset and the highest Acc and F 1 F1 -score on the ETH 3D dataset, surpassing all listed methods. Project website: https://github.com/zs670980918/ARAI-MVSNet},
  archive      = {J_PR},
  author       = {Song Zhang and Wenjia Xu and Zhiwei Wei and Lili Zhang and Yang Wang and Junyi Liu},
  doi          = {10.1016/j.patcog.2023.109885},
  journal      = {Pattern Recognition},
  pages        = {109885},
  shortjournal = {Pattern Recognition},
  title        = {ARAI-MVSNet: A multi-view stereo depth estimation network with adaptive depth range and depth interval},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic similarity distance: Towards better text-image
consistency metric in text-to-image generation. <em>PR</em>,
<em>144</em>, 109883. (<a
href="https://doi.org/10.1016/j.patcog.2023.109883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality images from text remains a challenge in visual-language understanding, with text-image consistency being a major concern. Particularly, the most popular metric R R -precision may not accurately reflect the text-image consistency, leading to misleading semantics in generated images. Albeit its significance, designing a better text-image consistency metric surprisingly remains under-explored in the community. In this paper, we make a further step forward to develop a novel CLIP-based metric, Semantic Similarity Distance ( S S D SSD ), which is both theoretically founded from a distributional viewpoint and empirically verified on benchmark datasets. We also introduce Parallel Deep Fusion Generative Adversarial Networks (PDF-GAN), which use two novel components to mitigate inconsistent semantics and bridge the text-image semantic gap . A series of experiments indicate that, under the guidance of S S D SSD , our developed PDF-GAN can induce remarkable enhancements in the consistency between texts and images while preserving acceptable image quality over the CUB and COCO datasets.},
  archive      = {J_PR},
  author       = {Zhaorui Tan and Xi Yang and Zihan Ye and Qiufeng Wang and Yuyao Yan and Anh Nguyen and Kaizhu Huang},
  doi          = {10.1016/j.patcog.2023.109883},
  journal      = {Pattern Recognition},
  pages        = {109883},
  shortjournal = {Pattern Recognition},
  title        = {Semantic similarity distance: Towards better text-image consistency metric in text-to-image generation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated adaptive reweighting for medical image
classification. <em>PR</em>, <em>144</em>, 109880. (<a
href="https://doi.org/10.1016/j.patcog.2023.109880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical data sharing across institutes is crucial to large-scale multi-center studies and the development of real-world AI applications but suffers from serious privacy issues. A promising solution to address this challenge is federated learning , which typically aggregates a global model from heterogeneous data spread across numerous clients without exchanging data. However, the traditional federated learning algorithm ( i.e. , FedAvg) merely aggregates the locally distributed models according to the amount of data on each client and lacks the consideration of data heterogeneity . In this paper, we propose a novel Federated Adaptive Reweighting (FedAR) algorithm for medical image classification . FedAR employs a flexible re-weighting scheme that can balance adaptively the contributions of the amount of data and the performance of the local model on each client to the weight of that client. Specifically, we allow the amount of local data to contribute more to the weight of each client in the early training stage and let the performance of the local model play a more important role in the late stage. We have evaluated the proposed FedAR algorithm against the locally trained model, globally trained baseline, and two existing federated learning algorithms on the ISIC2018 dataset and Chest X-ray14 dataset under the settings with a variable number of clients. Our results suggest that FedAR is an effective federated learning algorithm that substantially outperforms existing federated learning approaches.},
  archive      = {J_PR},
  author       = {Benteng Ma and Yu Feng and Geng Chen and Changyang Li and Yong Xia},
  doi          = {10.1016/j.patcog.2023.109880},
  journal      = {Pattern Recognition},
  pages        = {109880},
  shortjournal = {Pattern Recognition},
  title        = {Federated adaptive reweighting for medical image classification},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of interlayer textural relationships to
discriminate the benignancy/malignancy of brain tumors. <em>PR</em>,
<em>144</em>, 109879. (<a
href="https://doi.org/10.1016/j.patcog.2023.109879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computer-aided diagnosis system is a popular tool to predict the risk factors of brain tumors. However, the existing techniques are unable to provide high detection accuracy due to the inability of capturing the hidden features of brain tumors. In this view, the present work decomposes the MR modality images into different layers using non-sub-sampled shearlet transformation (NSST). Following this, the proposed detection pipeline employs adaptive pulse-coupled neural network (A-PCNN) module and a fuzzy c-means (FCM) clustering algorithm for enhancement and segmentation of suspicious regions respectively. Interlayer textural relationships of tumors are estimated in terms of the layer-wise difference of the gray-level cooccurrence matrix (GLCM), similarity indices, and entropy distributions. Thenceforth, those feature coefficients are coded with binary sequences to express the textural homogeneity/randomness of tumors. Finally, these handcrafted multi-scale feature quantifiers are fed to some standard classifiers such as the k-nearest neighbor (kNN) technique, the linear square support vector machine (LS-SVM), and the decision tree (DT) for discriminating the state of benignancy/malignancy of tumors. Experimental results ensure that the proposed detection model shows superior performance compared to existing methods.},
  archive      = {J_PR},
  author       = {Poulomi Das and Arpita Das},
  doi          = {10.1016/j.patcog.2023.109879},
  journal      = {Pattern Recognition},
  pages        = {109879},
  shortjournal = {Pattern Recognition},
  title        = {Estimation of interlayer textural relationships to discriminate the benignancy/malignancy of brain tumors},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Patch-guided point matching for point cloud registration
with low overlap. <em>PR</em>, <em>144</em>, 109876. (<a
href="https://doi.org/10.1016/j.patcog.2023.109876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is a classic and fundamental problem. Existing point cloud registration methods obtain correspondence point pairs by calculating the correlation between point features. However, the instability of point features makes the outlier rate of corresponding point pairs high, resulting in poor matching results, especially when facing low overlap point clouds. An obvious fact is that patch matching has higher reliability than point matching. To this end, we propose a patch-guided point cloud registration network . Specifically, we perform fusion on patches and points at both the feature and result levels to achieve the guidance of patch to point matching and improve the accuracy of predicted point pairs. At the feature level, we propose a Matching Pyramid Network (MPN) for multi-level patch/point matching. The core of the MPN is an attention-based cross-layer context aggregation (CCA) module, which is used for the fusion of matching features between upper and lower layers. At the result level, we design a matching consistency judgment module to ensure that the point pairs are consistent in the matching of each layer, which greatly reduces the outlier ratio . Based on the above design, the corresponding point pairs predicted by our network have a high inlier ratio, which makes our method perform well in the face of low overlapping point clouds. Extensive experimental results show that our method outperforms other existing methods for indoor and outdoor datasets.},
  archive      = {J_PR},
  author       = {Tianming Zhao and Linfeng Li and Tian Tian and Jiayi Ma and Jinwen Tian},
  doi          = {10.1016/j.patcog.2023.109876},
  journal      = {Pattern Recognition},
  pages        = {109876},
  shortjournal = {Pattern Recognition},
  title        = {Patch-guided point matching for point cloud registration with low overlap},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Memory efficient data-free distillation for continual
learning. <em>PR</em>, <em>144</em>, 109875. (<a
href="https://doi.org/10.1016/j.patcog.2023.109875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks suffer from the catastrophic forgetting phenomenon when trained on sequential tasks in continual learning, especially when data from previous tasks are unavailable. To mitigate catastrophic forgetting, various methods either store data from previous tasks, which may raise privacy concerns, or require large memory storage. Particularly, the distillation-based methods mitigate catastrophic forgetting by using proxy datasets. However, proxy datasets may not match the distributions of the original datasets of previous tasks. To address these problems in a setting where the full training data of previous tasks are unavailable and memory resources are limited, we propose a novel data-free distillation method. Our method encodes knowledge of previous tasks into network parameter gradients by Taylor expansion , deducing a regularizer relying on gradients in network training loss. To improve memory efficiency, we design an approach to compressing the gradients in the regularizer. Moreover, we theoretically analyze the approximation error of our method. Experimental results on multiple datasets demonstrate that our proposed method outperforms the existing approaches in continual learning.},
  archive      = {J_PR},
  author       = {Xiaorong Li and Shipeng Wang and Jian Sun and Zongben Xu},
  doi          = {10.1016/j.patcog.2023.109875},
  journal      = {Pattern Recognition},
  pages        = {109875},
  shortjournal = {Pattern Recognition},
  title        = {Memory efficient data-free distillation for continual learning},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BLoG: Bootstrapped graph representation learning with local
and global regularization for recommendation. <em>PR</em>, <em>144</em>,
109874. (<a href="https://doi.org/10.1016/j.patcog.2023.109874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of online information, the significant application value of recommender systems has received considerable attention. Since user–item interactions can naturally fit into graph structure data , graph neural networks (GNNs), by virtue of their strong ability in graph representation learning , have become the new state-of-the-art approach to recommender systems . Recently, GNN-based contrastive self-supervised learning (SSL) methods have received careful attention due to their superiority over graph-based recommendation under the typical supervised learning paradigm. However, to achieve state-of-the-art performance, GNN-based recommendation with SSL often needs a huge amount of negative examples and the model’s performance is heavily dependent on complex data augmentations . Also, the information interaction among various augmented views is often performed under a single perspective (e.g., structure/feature space or node/graph level). In this paper, we propose a novel bootstrapped graph representation learning with local and global regularization for recommendation, i.e., BLoG, which constructs positive/negative pairs based on the aggregated node features by referring to two alternate views of the original user–item graph structure. In particular, BLoG learns user–item representations by encoding two augmented versions of a user–item bipartite graph using two separate encoders: an online encoder and a target encoder. To facilitate the information interaction between these two distinct graph encoders, we introduce local and global regularization for recommendation, where a graph structural contrastive loss and a node-level semantic loss are defined for local regularization while a graph-level contrastive loss is used for global regularization. An alternative optimization approach is used to train the online encoder and the target encoder. Experimental studies on three benchmark datasets demonstrate that BLoG achieves better recommendation accuracy than several existing baselines.},
  archive      = {J_PR},
  author       = {Ming Li and Lin Zhang and Lixin Cui and Lu Bai and Zhao Li and Xindong Wu},
  doi          = {10.1016/j.patcog.2023.109874},
  journal      = {Pattern Recognition},
  pages        = {109874},
  shortjournal = {Pattern Recognition},
  title        = {BLoG: Bootstrapped graph representation learning with local and global regularization for recommendation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse kernel k-means for high-dimensional data.
<em>PR</em>, <em>144</em>, 109873. (<a
href="https://doi.org/10.1016/j.patcog.2023.109873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kernel k k -means method usually loses its power when clustering high-dimensional data, due to a large number of irrelevant features. We propose a novel sparse kernel k k -means clustering (SKKM) to extend the advantages of kernel k k -means to the high-dimensional cases. We assign each feature a 0 0 - 1 1 indicator and optimize an equivalent kernel k k -means loss function while penalizing the sum of the indicators. An alternating minimization algorithm is proposed to estimate both the class labels and the feature indicators. We prove the consistency of both clustering and feature selection of the proposed method. In addition, we apply the proposed framework to the normalized cut. In the numerical experiments, we demonstrate that the proposed method provides better/comparable performance compared to the existing high-dimensional clustering methods .},
  archive      = {J_PR},
  author       = {Xin Guan and Yoshikazu Terada},
  doi          = {10.1016/j.patcog.2023.109873},
  journal      = {Pattern Recognition},
  pages        = {109873},
  shortjournal = {Pattern Recognition},
  title        = {Sparse kernel k-means for high-dimensional data},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online portfolio selection with predictive instantaneous
risk assessment. <em>PR</em>, <em>144</em>, 109872. (<a
href="https://doi.org/10.1016/j.patcog.2023.109872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online portfolio selection (OPS) has received increasing attention from machine learning and quantitative finance communities. Despite their effectiveness, the pioneering OPS methods have several key limitations. First, price predictions are usually based on predetermined trends, which is inadequate for a fast-changing market patterns. Second, each asset is treated individually, ignoring the pervading relevance among the assets. Third, the risk terms are usually missing or inappropriate in optimizations. This paper proposes a novel OPS method, namely, the online low-dimension ensemble method , to overcome the limitations. Motivated by the stylized facts for the co-movements of assets, the financial market is regarded as a high-dimensional dynamical system (HDS), and a large number of low-dimensional subsystems (LDSs) are randomly generated from the HDS to extract the correlation information among the assets. The assets’ price predictions are first made using these LDSs and then aggregated to formulate the final prediction using ensemble learning techniques. Thanks to the particular merits brought by our predicting scheme, we also develop a novel high-dimensional covariance matrix estimation/prediction method for short-term data, efficiently assessing the instantaneous risk of the projected portfolios. Compared with state-of-the-art methods, our approach obtains more accurate predictions as the correlation information is fully exploited. With the predictive instantaneous risk assessment, a more appropriate optimization problem is proposed, substantially improving the OPS setting and leading to significantly better investment performance. Therefore, this study develops a flexible and promising approach to learning fast-changing market patterns and demonstrates that the high-dimensional feature of the market is a crucial information source for financial modeling with short-term data rather than a barrier in the conventional sense. Extensive experiments on real-world datasets are conducted to illustrate our method further.},
  archive      = {J_PR},
  author       = {Wenzhi Xi and Zhanfeng Li and Xinyuan Song and Hanwen Ning},
  doi          = {10.1016/j.patcog.2023.109872},
  journal      = {Pattern Recognition},
  pages        = {109872},
  shortjournal = {Pattern Recognition},
  title        = {Online portfolio selection with predictive instantaneous risk assessment},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two phase cooperative learning for supervised dimensionality
reduction. <em>PR</em>, <em>144</em>, 109871. (<a
href="https://doi.org/10.1016/j.patcog.2023.109871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous minimization of the reconstruction and classification error is a hard non convex problem , especially when a non-linear mapping is utilized. To overcome this obstacle, motivated by the widespread success of Cooperative Neural Networks , an innovative supervised dimensionality reduction framework is proposed, based on a cooperative two phase optimization strategy . Specifically, the proposed framework that requires minimal parameter adjustment consists of an autoencoder for dimensionality reduction and a separator network for separability assessment of the embedding. This scheme results in meaningful and discriminable codes, which are optimized for the classification task and are exploitable by any trainable classifier. The experimental results showed that the proposed methodology achieved competitive results against the state-of-the-art competing methods, while being much more efficient in terms of parameter count. Finally, it was empirically justified that the proposed methodology introduces advanced behavioural explainability, while enabling applicability for image generation tasks.},
  archive      = {J_PR},
  author       = {Ioannis A. Nellas and Sotiris K. Tasoulis and Spiros V. Georgakopoulos and Vassilis P. Plagianakos},
  doi          = {10.1016/j.patcog.2023.109871},
  journal      = {Pattern Recognition},
  pages        = {109871},
  shortjournal = {Pattern Recognition},
  title        = {Two phase cooperative learning for supervised dimensionality reduction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous network representation learning based on role
feature extraction. <em>PR</em>, <em>144</em>, 109870. (<a
href="https://doi.org/10.1016/j.patcog.2023.109870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since most of the real-world networks are heterogeneous, existing methods cannot characterize the roles of nodes in heterogeneous networks . The neighborhood structure of nodes in heterogeneous networks largely determines the node roles, and the basic statistical features of nodes describe the topology of nodes to some extent, so extracting structural features from the adjacency matrix of networks is crucial for role-oriented network representation learning(structural equivalence). Therefore, in this paper, we propose a heterogeneous network representation learning model based on role feature extraction, called HRFE(Heterogeneous Network Representation Learning for Role Feature Extraction). Firstly, we perform feature extraction for each node in the heterogeneous network to obtain a high-dimensional feature matrix, then perform role discovery using non-negative matrix decomposition techniques to obtain a role-based node representation, and finally verify the effectiveness of the model HRFE through experiments on a large number of real datasets.},
  archive      = {J_PR},
  author       = {Yueheng Sun and Mengyu Jia and Chang Liu and Minglai Shao},
  doi          = {10.1016/j.patcog.2023.109870},
  journal      = {Pattern Recognition},
  pages        = {109870},
  shortjournal = {Pattern Recognition},
  title        = {Heterogeneous network representation learning based on role feature extraction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute subspaces for zero-shot learning. <em>PR</em>,
<em>144</em>, 109869. (<a
href="https://doi.org/10.1016/j.patcog.2023.109869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen categories without corresponding training samples, which is a practical yet challenging task in computer vision and pattern recognition community. Current state-of-the-art locality-based ZSL methods aim to learn the explicit locality of discriminative attributes, which may suffer from insufficient class-level attribute supervision. In this paper, we introduce an Attribute Subspace learning method for ZSL (AS-ZSL) to learn implicit attribute composition, which is more general than attribute localization with only class-level attribute supervision. AS-ZSL exploits subspace representations that can effectively capture the intrinsic composition of high-dimensional image features and the diversity within attribute appearance. Furthermore, we develop a subspace distance based triplet loss to improve the distinguishability of the attribute subspace representation. Attribute subspace learning module is only needed for the training phase to jointly learn discriminative global features. This leads to a compact inference phase. Furthermore, the proposed AS-ZSL can be naturally extended to adapt to the transductive ZSL setting using a novel self-supervised training strategy. Extensive experimental results on several widely used ZSL datasets, i.e. , CUB, AwA2, and SUN, demonstrate the advantage of AS-ZSL compared with the state-of-the-art under different ZSL settings.},
  archive      = {J_PR},
  author       = {Lei Zhou and Yang Liu and Xiao Bai and Na Li and Xiaohan Yu and Jun Zhou and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2023.109869},
  journal      = {Pattern Recognition},
  pages        = {109869},
  shortjournal = {Pattern Recognition},
  title        = {Attribute subspaces for zero-shot learning},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underwater object classification combining SAS and
transferred optical-to-SAS imagery. <em>PR</em>, <em>144</em>, 109868.
(<a href="https://doi.org/10.1016/j.patcog.2023.109868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining synthetic aperture sonar (SAS) imagery with optical images for underwater object classification has the potential to overcome challenges such as water clarity, the stability of the optical image analysis platform, and strong reflections from the seabed for sonar-based classification. In this work, we propose this type of multi-modal combination to discriminate between man-made targets and objects such as rocks or litter. We offer a novel classification algorithm that overcomes the problem of intensity and object formation differences between the two modalities. To this end, we develop a novel set of geometrical shape descriptors that takes into account the geometrical relation between the object’s shadow and highlight. Results from 7,052 pairs of SAS and optical images collected during several sea experiments show improved classification performance compared to the state-of-the-art for better discrimination between different types of underwater objects. For reproducability, we share our database.},
  archive      = {J_PR},
  author       = {Avi Abu and Roee Diamant},
  doi          = {10.1016/j.patcog.2023.109868},
  journal      = {Pattern Recognition},
  pages        = {109868},
  shortjournal = {Pattern Recognition},
  title        = {Underwater object classification combining SAS and transferred optical-to-SAS imagery},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive filters in graph convolutional neural networks.
<em>PR</em>, <em>144</em>, 109867. (<a
href="https://doi.org/10.1016/j.patcog.2023.109867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few years, the availability of an increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (ConvGNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions . Due to the higher flexibility in exploring and exploiting the graph structure of data, there is recently an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks literature over the years. This paper presents a novel method to adapt the behaviour of a ConvGNN to the input performing spatial convolution on graphs using input-specific filters, which are dynamically generated from nodes feature vectors. The experimental assessment confirms the capabilities of the proposed approach, achieving satisfying results using a low number of filters.},
  archive      = {J_PR},
  author       = {Andrea Apicella and Francesco Isgrò and Andrea Pollastro and Roberto Prevete},
  doi          = {10.1016/j.patcog.2023.109867},
  journal      = {Pattern Recognition},
  pages        = {109867},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive filters in graph convolutional neural networks},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-driven diagnosis: A multi-task approach for
segmenting stroke and bell’s palsy. <em>PR</em>, <em>144</em>, 109866.
(<a href="https://doi.org/10.1016/j.patcog.2023.109866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strong efforts have been undertaken to enhance the diagnosis and identification of diseases that cause facial paralysis, such as Bell&#39;s palsy and stroke, because of their detrimental social effects. Stroke is one of the most serious and potentially fatal conditions among the major cardiovascular disorders. We are introducing a deep-learning-based method for early diagnosis of facial paralysis diseases such as stroke and Bell&#39;s palsy. Recognizing the costs associated with traditional diagnostic techniques like magnetic resonance tomography (MRI) and computed tomography (CT) scan images, our model employs a multi-task network, integrating face parsing, facial asymmetry parsing, and category enhancement. Spatial inconsistencies are addressed via a depth-map estimation module that leverages an instance-specific kernel approach. To clarify the boundaries of facial components, we use category edge detection with a foreground attention module, generating generic geometric structures and detailed semantic cues. Our model is trained on two datasets, comprising individuals with regular smiles and those with one-sided facial weakness. This cost-effective, easily accessible solution can streamline the diagnostic process, minimizing data gaps, and reducing needless rescreening and intervention costs.},
  archive      = {J_PR},
  author       = {Sabina Umirzakova and Shabir Ahmad and Sevara Mardieva and Shakhnoza Muksimova and Taeg Keun Whangbo},
  doi          = {10.1016/j.patcog.2023.109866},
  journal      = {Pattern Recognition},
  pages        = {109866},
  shortjournal = {Pattern Recognition},
  title        = {Deep learning-driven diagnosis: A multi-task approach for segmenting stroke and bell&#39;s palsy},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Audio-driven talking face generation with diverse yet
realistic facial animations. <em>PR</em>, <em>144</em>, 109865. (<a
href="https://doi.org/10.1016/j.patcog.2023.109865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-driven talking face generation, which aims to synthesize talking faces with realistic facial animations (including accurate lip movements, vivid facial expression details and natural head poses) corresponding to the audio, has achieved rapid progress in recent years. However, most existing work focuses on generating lip movements only without handling the closely correlated facial expressions, which degrades the realism of the generated faces greatly. This paper presents DIRFA, a novel method that can generate talking faces with diverse yet realistic facial animations from the same driving audio. To accommodate fair variation of plausible facial animations for the same audio, we design a transformer-based probabilistic mapping network that can model the variational facial animation distribution conditioned upon the input audio and autoregressively convert the audio signals into a facial animation sequence. In addition, we introduce a temporally-biased mask into the mapping network, which allows to model the temporal dependency of facial animations and produce temporally smooth facial animation sequence. With the generated facial animation sequence and a source image, photo-realistic talking faces can be synthesized with a generic generation network. Extensive experiments show that DIRFA can generate talking faces with realistic facial animations effectively.},
  archive      = {J_PR},
  author       = {Rongliang Wu and Yingchen Yu and Fangneng Zhan and Jiahui Zhang and Xiaoqin Zhang and Shijian Lu},
  doi          = {10.1016/j.patcog.2023.109865},
  journal      = {Pattern Recognition},
  pages        = {109865},
  shortjournal = {Pattern Recognition},
  title        = {Audio-driven talking face generation with diverse yet realistic facial animations},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel method of human identification based on dental
impression image. <em>PR</em>, <em>144</em>, 109864. (<a
href="https://doi.org/10.1016/j.patcog.2023.109864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale natural disasters and special criminal cases, surface features of bodies, such as faces and fingerprints, are easily destroyed. Teeth possess strong high-temperature resistance, corrosion resistance, and high hardness, which can compensate for the shortcomings of the aforementioned situations. This paper proposes an identification method based on the aggregated features of multi-scale dental impression images. Firstly, a method exploiting the adaptive object detection method based on YOLOv8 is proposed to segment toothprints. Next, a novel geometric feature named calibrated offset distance is extracted, combined with the SIFT feature method, to extract multi-scale and multi-dimensional features from the global toothprint, local toothprints, and single-tooth prints. Finally, all features are aggregated to enhance the descriptive ability and robustness. Experimental results indicate that the method proposed in this paper demonstrates good identification performance.},
  archive      = {J_PR},
  author       = {Jiafa Mao and Lixin Wang and Ning Wang and Yahong Hu and Weigou Sheng},
  doi          = {10.1016/j.patcog.2023.109864},
  journal      = {Pattern Recognition},
  pages        = {109864},
  shortjournal = {Pattern Recognition},
  title        = {A novel method of human identification based on dental impression image},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot forgery detection via guided adversarial
interpolation. <em>PR</em>, <em>144</em>, 109863. (<a
href="https://doi.org/10.1016/j.patcog.2023.109863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in face manipulation models has led to a critical issue in society—the synthesis of realistic visual media. With the emergence of new forgery approaches at an unprecedented rate, existing forgery detection methods suffer from significant performance drops when applied to unseen novel forgery approaches. In this work, we address the few-shot forgery detection problem by (1) designing a comprehensive benchmark based on coverage analysis among various forgery approaches, and (2) proposing Guided Adversarial Interpolation (GAI) . Our key insight is that there exist transferable distribution characteristics between majority and minority forgery classes. 1 Specifically, we enhance the discriminative ability against novel forgery approaches via adversarially interpolating the forgery artifacts of the minority samples to the majority samples under the guidance of a teacher network. Unlike the standard re-balancing method which usually results in over-fitting to minority classes, our method simultaneously takes account of the diversity of majority information as well as the significance of minority information. Extensive experiments demonstrate that our GAI achieves state-of-the-art performances on the established few-shot forgery detection benchmark. Notably, our method is also validated to be robust to choices of majority and minority forgery approaches.},
  archive      = {J_PR},
  author       = {Haonan Qiu and Siyu Chen and Bei Gan and Kun Wang and Huafeng Shi and Jing Shao and Ziwei Liu},
  doi          = {10.1016/j.patcog.2023.109863},
  journal      = {Pattern Recognition},
  pages        = {109863},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot forgery detection via guided adversarial interpolation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modal transformer for RGB-d semantic segmentation of
production workshop objects. <em>PR</em>, <em>144</em>, 109862. (<a
href="https://doi.org/10.1016/j.patcog.2023.109862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene understanding in a production workshop is an important technology to improve its intelligence level, semantic segmentation of production workshop objects is an effective method for realizing scene understanding. Since the varieties of information of production workshop, making full use of the complementary information of RGB image and depth image can effectively improve the semantic segmentation accuracy of production workshop objects. Aiming at solving the multi-scale and real-time problems of segmenting the production workshop objects, this paper proposes Cross-Modal Transformer (CMFormer), a Transformer-based cross-modal semantic segmentation model. Its key feature correction and feature fusion parts are composed of the Multi-Scale Channel Attention Correction(MS-CAC) module and the Global Feature Aggregation(GFA) module. By improving Multi-Head Self-Attention(MHSA) in Transformer, we design Cross-Modal Multi-Head Self-Attention(CM-MHSA) to build long-range interaction between RGB image and depth image, and further design the MS-CAC module and the GFA module on the basis of the CM-MHSA module, to achieve cross-modal information interaction in the channel and spatial dimensions. Among them, the MS-CAC module enriches the multi-scale features of each channel and achieve more accurate channel attention correction between the two modals; the GFA module interacts with RGB feature and depth feature in the spatial dimension and fuses global and local features at the same time. In the experiments on the NYU Depth v2 dataset, the CMFormer reached 68.00\% MPA(Mean Pixel Accuracy) and 55.75\% mIoU(Mean Intersection over Union), achieves the state-of-the-art results. While in the experiments on the Scene Objects for Production workshop dataset(SOP), t he CMFormer achieves 96.74\% MPA, 92.98\% mIoU and 43 FPS(Frames Per Second), which has high precision and good real-time performance. Code is available at: https://github.com/FutureIAI/CMFormer},
  archive      = {J_PR},
  author       = {Qingjun Ru and Guangzhu Chen and Tingyu Zuo and Xiaojuan Liao},
  doi          = {10.1016/j.patcog.2023.109862},
  journal      = {Pattern Recognition},
  pages        = {109862},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal transformer for RGB-D semantic segmentation of production workshop objects},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A weakly supervised inpainting-based learning method for
lung CT image segmentation. <em>PR</em>, <em>144</em>, 109861. (<a
href="https://doi.org/10.1016/j.patcog.2023.109861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, various fully supervised learning methods are successfully applied for lung CT image segmentation . However, pixel-wise annotations are extremely expert-demanding and labor-intensive, but the performance of unsupervised learning methods are failed to meet the demands of practical applications. To achieve a reasonable trade-off between the performance and label dependency, a novel weakly supervised inpainting-based learning method is introduced, in which only bounding box labels are required for accurate segmentation. Specifically, lesion regions are first detected by an object detection network, then we crop them out of the input image and recover the missing holes to normal regions using a progressive CT inpainting network (PCIN). Finally, a post-processing method is designed to get the accurate segmentation mask from the difference image of input and recovered images. In addition, real information (i.e., number, location and size) of the bounding boxes of lesions from the dataset guides us to make the training dataset for PCIN. We apply a multi-scale supervised strategy to train PCIN for a progressive and stable inpainting. Moreover, to remove the visual artifacts resulted from the invalid features of missing holes, an initial patch generation network (IPGN) is proposed for holes initialization with generated pseudo healthy image patches. Experiments on the public COVID-19 dataset demonstrate that PCIN is outstanding in lung CT images inpainting , and the performance of our proposed weakly supervised method is comparable to fully supervised methods.},
  archive      = {J_PR},
  author       = {Fangfang Lu and Zhihao Zhang and Tianxiang Liu and Chi Tang and Hualin Bai and Guangtao Zhai and Jingjing Chen and Xiaoxin Wu},
  doi          = {10.1016/j.patcog.2023.109861},
  journal      = {Pattern Recognition},
  pages        = {109861},
  shortjournal = {Pattern Recognition},
  title        = {A weakly supervised inpainting-based learning method for lung CT image segmentation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view clustering via efficient representation learning
with anchors. <em>PR</em>, <em>144</em>, 109860. (<a
href="https://doi.org/10.1016/j.patcog.2023.109860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view spectral clustering has gained considerable attention due to its potential to enhance clustering performance. Although many methods have shown promising results, they often suffer from high time complexity and are not suitable for large-scale datasets. On the other hand, anchor-based methods are well-known for their efficiency. These methods typically learn the similarity relationship between instances and anchors and then convert it into the similarity relationship between instances, involving a considerable number of calculations. To address this issue, we propose a novel method called Multi-view clustering via Efficient Representation LearnIng with aNchors (MERLIN) in this paper. Instead of learning the instance–instance relationship, MERLIN approaches the clustering problem from the perspective of representation learning. Specifically, MERLIN selects the same anchors for different views and utilizes these anchors to learn a consensus representation that integrates information from all views. Additionally, MERLIN adaptively learns weights for different views to fully exploit the complementary information among multiple views. In comparison with seven state-of-the-art baseline methods across five datasets, MERLIN demonstrates both efficiency and effectiveness in handling multi-view datasets and is suitable for handling large-scale datasets.},
  archive      = {J_PR},
  author       = {Xiao Yu and Hui Liu and Yan Zhang and Shanbao Sun and Caiming Zhang},
  doi          = {10.1016/j.patcog.2023.109860},
  journal      = {Pattern Recognition},
  pages        = {109860},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view clustering via efficient representation learning with anchors},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based pattern recognition on spectral reduced graphs.
<em>PR</em>, <em>144</em>, 109859. (<a
href="https://doi.org/10.1016/j.patcog.2023.109859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based pattern recognition – in particular in conjunction with large graphs – is often computationally expensive. This hampers, or makes it at least challenging, to employ graph-based representations for real-world data. To address this issue, we propose a method for reducing the size of the underlying graphs to their most important substructures using spectral graph clustering . The proposed method partitions the nodes of the graphs into clusters and then merges each cluster into supernodes . The motivation of this procedure is to reduce the computational cost of any graph comparison algorithm while maintaining the accuracy of the final classification. To assess the benefits and limitations of our method, we conduct thorough experiments on nine real-world datasets with different levels of graph reductions. The classification is obtained by four different graph classifiers (viz. a KNN based on graph edit distance, two SVMs based on a shortest path graph and a Weisfeiler–Lehman graph kernel, as well as a graph neural network). The results indicate that we can reduce computation time by up to two orders of magnitude without substantially degrading the classification accuracy .},
  archive      = {J_PR},
  author       = {Anthony Gillioz and Kaspar Riesen},
  doi          = {10.1016/j.patcog.2023.109859},
  journal      = {Pattern Recognition},
  pages        = {109859},
  shortjournal = {Pattern Recognition},
  title        = {Graph-based pattern recognition on spectral reduced graphs},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Graph matching for knowledge graph alignment using
edge-coloring propagation. <em>PR</em>, <em>144</em>, 109851. (<a
href="https://doi.org/10.1016/j.patcog.2023.109851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) is a kind of structured human knowledge of modeling the relationships between real-world entities. High quality KG is of crucial importance for many knowledge-based applications, e.g. , question answering, recommender systems , etc. This paper studies the problem of entity alignment in KGs to promote knowledge fusion. Existing methods model the semantic representation of entities by using graph structural information or attribute information of the KG and then align the entities across different domains by calculating the distances between entities’ embeddings. However, these methods only consider the node-to-node similarity in the alignment procedure while the edge-to-edge similarity is ignored. Our research hypothesis is that the graph edge alignment information is critical in entity alignment. We reformulate the knowledge entity alignment as a quadratic assignment problem (QAP) by adding relation alignment under the one-to-one mapping constraint. To solve the notorious QAP in a large-scale heterogeneous graph like KG, we propose a model, dual neighborhood consensus network (DNCN), which approximately decomposes the QAP into two small-scale linear assignment problems, i.e. , entity alignment and relation alignment. After that, an edge-coloring propagation method is proposed to refine the coarse entity alignment result using the relation correspondence. Theoretical proof shows that this method can guarantee the isomorphism between local sub-graphs. The performance of DNCN is evaluated using the DBP15K and DWY100K benchmarks. Experimental results show that DNCN achieves the best performance on the DBP15K benchmark, and is computationally efficient. Ablation studies verify the importance of graph edge alignment information.},
  archive      = {J_PR},
  author       = {Yuxuan Zhang and Yuanxiang Li and Xian Wei and Yongsheng Yang and Lei Liu and Yi Lu Murphey},
  doi          = {10.1016/j.patcog.2023.109851},
  journal      = {Pattern Recognition},
  pages        = {109851},
  shortjournal = {Pattern Recognition},
  title        = {Graph matching for knowledge graph alignment using edge-coloring propagation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counting-based visual question answering with serial
cascaded attention deep learning. <em>PR</em>, <em>144</em>, 109850. (<a
href="https://doi.org/10.1016/j.patcog.2023.109850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The counting-based questions play a major part in Visual Question Answering (VQA), the most challenging factor is counting the different objects present in the images. Recently more attention is paid to design a model of count-aided VQA. Based on the questions, the VQA system responds with appropriate answers. Yet, the complex questions are necessitating in the system with answers. The earlier models are still facing the challenging problems of counting the various objects within the images as the models become futile to select the features and lack fine-grained representation. In order to sustain the image representation, this paper proposes a new model for VQA using the heuristic approach of serial cascaded deep learning methods. Initially, the standard data regarding images and text data are gathered and fed to the pre-processing process. Consequently, the feature extraction is done on both the image and the text data. Here, the deep features from images are taken using Visual Geometry Group 16 (VGG16) and the text features are extracted using Text Convolutional Neural Network (TCNN). Then, the optimal weighted fused features are obtained, where the weights used for getting the necessary features are tuned via the Improved Tuna Swarm Optimization (ITSO) algorithm. Finally, the counting answers are retrieved based on the given queries, which is carried out via Serial Cascaded Recurrent Neural Network with Attention Mechanism-based Long Short-Term Memory (SCRAM-LSTM). The performance is examined with divergent metrics compared with conventional models. Hence, the findings reveal that it offers superior performance in estimating the appropriate answers. Therefore, the proposed work is widely used for such potential applications as helping blind or visually impaired people to get information, integrating with image retrieval systems, and also for search engines. Especially, it is utilized for the vision and language systems.},
  archive      = {J_PR},
  author       = {Tesfayee MeshuWelde and Lejian Liao},
  doi          = {10.1016/j.patcog.2023.109850},
  journal      = {Pattern Recognition},
  pages        = {109850},
  shortjournal = {Pattern Recognition},
  title        = {Counting-based visual question answering with serial cascaded attention deep learning},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few pixels attacks with generative model. <em>PR</em>,
<em>144</em>, 109849. (<a
href="https://doi.org/10.1016/j.patcog.2023.109849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks have attracted much attention in recent years, and a number of works have demonstrated the effectiveness of attacks on the entire image at perturbation generation. However, in practice, specially designed perturbation of the entire image is impractical. Some work has crafted adversarial samples with a few scrambled pixels by advanced search, e.g., SparseFool, OnePixel, etc., but they take more time to find such pixels that can be perturbed. Therefore, to construct the adversarial samples with few pixels perturbed in an end-to-end way, we propose a new framework, in which a dual-decoder VAE for perturbations finding is designed. To make adversarial learning more effective, we proposed a new version of the adversarial loss by considering the generalization. To evaluate the sophistication of the proposed framework, we compared it with more than a dozen existing related attack methods. The effectiveness and efficiency of the proposed framework are verified from the extensive experimental results. The validity of the model structure is also validated by the ablation study.},
  archive      = {J_PR},
  author       = {Yang Li and Quan Pan and Zhaowen Feng and Erik Cambria},
  doi          = {10.1016/j.patcog.2023.109849},
  journal      = {Pattern Recognition},
  pages        = {109849},
  shortjournal = {Pattern Recognition},
  title        = {Few pixels attacks with generative model},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Encoder–decoder cycle for visual question answering based on
perception-action cycle. <em>PR</em>, <em>144</em>, 109848. (<a
href="https://doi.org/10.1016/j.patcog.2023.109848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel encoder–decoder cycle (EDC) framework inspired by the human learning process called the perception-action cycle to tackle challenging problems such as visual question answering (VQA) and visual relationship detection (VRD). EDC considers the understanding of the visual features of an image as perception and the act of answering the question regarding that image as an action. In the perception-action cycle, information is primarily collected from the environment and then passed to sensory structures in the brain to form an understanding of the environment. Acquired knowledge is then passed to motor structures to perform an action on the environment. Next, sensory structures perceive the altered environment and improve their understanding of the surrounding world. This process of understanding the environment, performing an action correspondingly, and then re-evaluating the initial understanding occurs cyclically in human life. EDC initially mimics this mechanism of introspection by comprehending and refining visual features to acquire the proper knowledge for answering the question. Subsequently, it decodes visual and language features into answer features, feeding them back cyclically to the encoder. In the VRD task, EDC decodes visual features to generate predicate features. We evaluate the proposed framework on the TDIUC, VQA 2.0, and VRD datasets, which outperforms the state-of-the-art models on the TDIUC and VRD datasets.},
  archive      = {J_PR},
  author       = {Safaa Abdullahi Moallim Mohamud and Amin Jalali and Minho Lee},
  doi          = {10.1016/j.patcog.2023.109848},
  journal      = {Pattern Recognition},
  pages        = {109848},
  shortjournal = {Pattern Recognition},
  title        = {Encoder–decoder cycle for visual question answering based on perception-action cycle},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdvMask: A sparse adversarial attack-based data augmentation
method for image classification. <em>PR</em>, <em>144</em>, 109847. (<a
href="https://doi.org/10.1016/j.patcog.2023.109847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has been an essential technique for improving the generalization ability of deep neural networks in image classification tasks. However, intensive changes in appearance and different degrees of occlusion in images are the key factors that severely affect the generalization ability of image classification models. Therefore, in order to enhance the generalization performance and robustness of deep models, data augmentation approaches by providing models with more diverse training data in various scenarios are widely applied. Although many existing data augmentation methods simulate occlusion in the augmented images to enhance the generalization of models, these methods randomly delete some areas in images without considering the semantic information of images. In this work, we propose a novel data augmentation method named AdvMask for image classification based on sparse adversarial attack techniques. AdvMask first identifies the key points that have the greatest influence on the classification results via a proposed end-to-end sparse adversarial attack module. During the data augmentation process, AdvMask efficiently generates diverse augmented data with structured occlusions based on the key points. By doing so, AdvMask can force deep models to seek other relevant content while the most discriminative content is hidden. Extensive experimental results on various benchmark datasets and deep models demonstrate that our proposed method can effectively improve the generalization performance of deep models and significantly outperforms previous data augmentation methods. Code for reproducing our results is available at https://github.com/Jackbrocp/AdvMask .},
  archive      = {J_PR},
  author       = {Suorong Yang and Jinqiao Li and Tianyue Zhang and Jian Zhao and Furao Shen},
  doi          = {10.1016/j.patcog.2023.109847},
  journal      = {Pattern Recognition},
  pages        = {109847},
  shortjournal = {Pattern Recognition},
  title        = {AdvMask: A sparse adversarial attack-based data augmentation method for image classification},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DTEC: Decision tree-based evidential clustering for
interpretable partition of uncertain data. <em>PR</em>, <em>144</em>,
109846. (<a href="https://doi.org/10.1016/j.patcog.2023.109846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the evidential clustering has been developed as a promising clustering framework for uncertain data, which generalizes those hard, fuzzy, possibilistic and rough clustering. However, the resulting cluster assignments are less interpretable in terms of human cognition, which limits its applications in those security, privacy or ethic related fields. In this study, the unsupervised decision tree model is introduced into the evidential clustering framework to improve the interpretability of the evidential partition. A Decision Tree-based Evidential Clustering (DTEC) algorithm is developed to build an unsupervised evidential decision tree, which uses the paths from the root node to leaf nodes to achieve the interpretability of each cluster. The proposed algorithm is composed of three procedures, i.e., cutting-point selection, node evidential splitting, and cluster adjustment, in which the first two procedures are carried out iteratively to build a preliminary unsupervised decision tree and the last procedure is designed to adjust the preliminary decision tree if the number of clusters is available. Both synthetic and real datasets are used to evaluate the performance of the proposed algorithm, and the experimental results demonstrate the good performance of the proposal compared with some representative fuzzy, evidential or decision tree-based clustering algorithms.},
  archive      = {J_PR},
  author       = {Lianmeng Jiao and Haoyu Yang and Feng Wang and Zhun-ga Liu and Quan Pan},
  doi          = {10.1016/j.patcog.2023.109846},
  journal      = {Pattern Recognition},
  pages        = {109846},
  shortjournal = {Pattern Recognition},
  title        = {DTEC: Decision tree-based evidential clustering for interpretable partition of uncertain data},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-imitation guided goal-conditioned reinforcement
learning. <em>PR</em>, <em>144</em>, 109845. (<a
href="https://doi.org/10.1016/j.patcog.2023.109845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goal-conditioned reinforcement learning (GCRL) aims to control agents to reach desired goals, which poses a significant challenge due to task-specific variations in configurations. However, current GCRL methods suffer from limitations in sample efficiency and the need for substantial training data. While existing self-imitation-based GCRL approaches can improve sample efficiency, their scalability to large-scale tasks is limited. In this paper, we propose integrating self-imitation learning with goal-conditioned RL methods into a compatible and reasonable framework. Specifically, we introduce a novel target action value function to aggregate self-imitation learning and goal-conditioned reinforcement learning . The designed target value effectively combines these two policy training mechanisms to accomplish specific tasks. Moreover, we theoretically demonstrate that our approach can learn a superior policy compared to both self-imitation learning and goal-conditioned reinforcement learning. Additionally, experimental results showcase the stability and effectiveness of our method compared to existing approaches in various challenging robotic control tasks.},
  archive      = {J_PR},
  author       = {Yao Li and YuHui Wang and XiaoYang Tan},
  doi          = {10.1016/j.patcog.2023.109845},
  journal      = {Pattern Recognition},
  pages        = {109845},
  shortjournal = {Pattern Recognition},
  title        = {Self-imitation guided goal-conditioned reinforcement learning},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep fidelity in DNN watermarking: A study of backdoor
watermarking for classification models. <em>PR</em>, <em>144</em>,
109844. (<a href="https://doi.org/10.1016/j.patcog.2023.109844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model’s original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement . To address this challenge, we propose the concept of deep fidelity , which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark .},
  archive      = {J_PR},
  author       = {Guang Hua and Andrew Beng Jin Teoh},
  doi          = {10.1016/j.patcog.2023.109844},
  journal      = {Pattern Recognition},
  pages        = {109844},
  shortjournal = {Pattern Recognition},
  title        = {Deep fidelity in DNN watermarking: A study of backdoor watermarking for classification models},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive generative network with recursive-loop for 3D
point cloud generalized zero-shot classification. <em>PR</em>,
<em>144</em>, 109843. (<a
href="https://doi.org/10.1016/j.patcog.2023.109843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Zero-Shot Learning (GZSL) aims to recognize objects from both seen and unseen categories by transferring semantic knowledge and merely utilizing seen class data for training. Recent feature generation methods in the 2D image domain have made great progress. However, very little is known about its usefulness in 3D point cloud zero-shot learning. This work aims to facilitate research on 3D point cloud generalized zero-shot learning. Different from previous works, we focus on synthesizing the more high-level discriminative point cloud features. To this end, we design a representation enhancement strategy to generate the features. Specifically, we propose a C ontrastive G enerative Network with R ecursive- L oop, termed as CGRL , which can be leveraged to enlarge the inter-class distances and narrow the intra-class gaps. By applying the contrastive representations to the generative model in a recursive-loop form, it can provide the self-guidance for the generator recurrently, which can help yield more discriminative features and train a better classifier. To validate the effectiveness of the proposed method, extensive experiments are conducted on three benchmarks, including ModelNet40, McGill, and ScanObjectNN. Experimental evaluations demonstrate the superiority of our approach and it can outperform the state-of-the-arts by a large margin. Code is available at https://github.com/photon-git/CGRL},
  archive      = {J_PR},
  author       = {Yun Hao and Yukun Su and Guosheng Lin and Hanjing Su and Qingyao Wu},
  doi          = {10.1016/j.patcog.2023.109843},
  journal      = {Pattern Recognition},
  pages        = {109843},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive generative network with recursive-loop for 3D point cloud generalized zero-shot classification},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-grained unsupervised domain adaptation approach for
semantic segmentation. <em>PR</em>, <em>144</em>, 109841. (<a
href="https://doi.org/10.1016/j.patcog.2023.109841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When transferring knowledge between different datasets, domain mismatch greatly hinders model’s performance. So domain adaption has been brought up to tackle the problem. Traditional methods focusing either on global or local alignment play a limited role in improving model’s performance. In this paper, we propose a multi-grained unsupervised domain adaptation approach (Muda) for semantic segmentation . Muda aims to enforce multi-grained semantic consistency between domains by aligning domains at both global and category level. Specifically, coarse-grained adaptation uses global adversarial learning on an image translation model and a main segmentation model , which respectively attempts to eliminate appearance differences and to get similar segmentation maps from two domains. While fine-grained adaptation employs an auxiliary model to adapt category information to refine pseudo labels of target data. Experiments and ablation studies are conducted on two synthetic-to-real benchmarks: GTA5 → → Cityscapes and SYNTHIA → → Cityscapes, which show that our model outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Luyang Li and Tai Ma and Yue Lu and Qingli Li and Lianghua He and Ying Wen},
  doi          = {10.1016/j.patcog.2023.109841},
  journal      = {Pattern Recognition},
  pages        = {109841},
  shortjournal = {Pattern Recognition},
  title        = {A multi-grained unsupervised domain adaptation approach for semantic segmentation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure-preserving image translation for multi-source
medical image domain adaptation. <em>PR</em>, <em>144</em>, 109840. (<a
href="https://doi.org/10.1016/j.patcog.2023.109840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is an important task for medical image analysis to improve generalization on datasets collected from diverse institutes using different scanners and protocols. For images with visible domain shift, using image translation models is an intuitive and effective way to perform domain adaptation, but the structure of the generated image may often be distorted when large content discrepancies between domains exist; resulting in poor downstream task performance. To address this, we propose a novel image translation model that disentangles structure and texture to only transfer the latter by using mutual information and texture co-occurrence losses. We translate source domain images to the target domain and employ the generated results as augmented samples for domain adaptation segmentation training. We evaluate our method on three public segmentation datasets: MMWHS, Fundus, and Prostate datasets acquired from diverse institutes. Experimental results show that a segmentation model trained using the augmented images from our approach outperforms state-of-the-art domain adaptation, image translation, and domain generalization methods.},
  archive      = {J_PR},
  author       = {Myeongkyun Kang and Philip Chikontwe and Dongkyu Won and Miguel Luna and Sang Hyun Park},
  doi          = {10.1016/j.patcog.2023.109840},
  journal      = {Pattern Recognition},
  pages        = {109840},
  shortjournal = {Pattern Recognition},
  title        = {Structure-preserving image translation for multi-source medical image domain adaptation},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised update summarization of news events.
<em>PR</em>, <em>144</em>, 109839. (<a
href="https://doi.org/10.1016/j.patcog.2023.109839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A long-running event represents a continuous stream of information on a given topic, such as natural disasters, stock market updates, or even ongoing customer relationship. These news stories include hundreds of individual, time-dependent texts. Simultaneously, new technologies have profoundly transformed the way we consume information. The need to obtain quick, relevant, and digest updates continuously has become a crucial issue and creates new challenges for the task of automatic document summarization. To that end, we introduce an innovative unsupervised method based on two competing sequence-to-sequence models to produce short updated summaries. The proposed architecture relies on several parameters to balance the outputs from the two autoencoders . This relation enables the overall model to correlate generated summaries with relevant information coming from both current and previous news iterations. Depending on the model configuration , we are then able to control the novelty or the consistency of terms included in generated summaries. We evaluate our method on a modified version of the TREC 2013, 2014, and 2015 datasets to track continuous events from a single source. We not only achieve state-of-the-art performance similar to other more complex unsupervised sentence compression approaches, but also influence the information included in the model in the summaries.},
  archive      = {J_PR},
  author       = {Florian Carichon and Florent Fettu and Gilles Caporossi},
  doi          = {10.1016/j.patcog.2023.109839},
  journal      = {Pattern Recognition},
  pages        = {109839},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised update summarization of news events},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EEG-based classification combining bayesian convolutional
neural networks with recurrence plot for motor movement/imagery.
<em>PR</em>, <em>144</em>, 109838. (<a
href="https://doi.org/10.1016/j.patcog.2023.109838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG)-based Motor imagery (MI) is a key topic in the brain-computer interface (BCI). The EEG-based real execution and motor imagery multi-class classification tasks are also crucial, but only a few kinds of literature research it. In addition, classification accuracy still has room for improvement, and the inter-individual variability problems in BCI applications need to be solved. To address these issues, we developed a novel model (RP-BCNNs) that combines the recurrence plot (RP) and Bayesian Convolutional Neural Networks (BCNNs). First, we employ an RP computation for preprocessed EEG signals of each channel and merge all RPs of all channels into one based on the weighted average method. Then, we feed the RP features into BCNNs to classify 2-class, 3-class, 4-class, and 5-class on real/imaginary movements classification tasks. The results show that the RP-BCNNs model outperforms the state-of-the-art methods, achieving average accuracies of 92.86\%, 94.12\%, 91.37\%, 92.61\% for real movements and 94.07\%, 93.77\%, 90.54\%, 91.85\% for imaginary movements. Our findings suggest that combining complex network methods with deep learning can improve the classification performance of EEG-based BCI systems (e.g., motor imagery, emotion recognition, and epileptic seizure classification).},
  archive      = {J_PR},
  author       = {Wenqie Huang and Guanghui Yan and Wenwen Chang and Yuchan Zhang and Yueting Yuan},
  doi          = {10.1016/j.patcog.2023.109838},
  journal      = {Pattern Recognition},
  pages        = {109838},
  shortjournal = {Pattern Recognition},
  title        = {EEG-based classification combining bayesian convolutional neural networks with recurrence plot for motor movement/imagery},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning consistent region features for lifelong person
re-identification. <em>PR</em>, <em>144</em>, 109837. (<a
href="https://doi.org/10.1016/j.patcog.2023.109837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lifelong person re-identification (LRe-ID) model retrieves a person across multiple cameras in continuous data streams and learns new coming datasets incrementally. However, there are two well-known challenges: catastrophic forgetting and generalization loss, which arise due to parameter updates and domain shifts. While there has been encouraging progress in balancing these challenges, few existing methods have addressed them from a unified feature perspective. Inspired by the Complementary Learning Systems theory, an effective framework is proposed to share consistent features and extract discriminative features for each sample. Specifically, this framework consists of Property Region Features , Feature Adaption and Feature Perspicacity . Property Region Features are the parametric representation of consistent region features, Feature Adaption and Feature Perspicacity are responsible for diversity features generation and discriminative features extraction, respectively. Moreover, a cascade knowledge distillation structure is introduced to guarantee Property Feature consistency, and correspondingly, a weighted distillation loss function is designed to prevent generalization loss on current domains caused by overlapping historical knowledge. Extensive experiments conducted on twelve Re-ID datasets, including both rehearsal and no-rehearsal settings, clearly validate the superiority of our method over state-of-the-art competitors, with significantly improved performance. The source code will be released on https://github.com/whisperH/ConRFL/ .},
  archive      = {J_PR},
  author       = {Jinze Huang and Xiaohan Yu and Dong An and Yaoguang Wei and Xiao Bai and Jin Zheng and Chen Wang and Jun Zhou},
  doi          = {10.1016/j.patcog.2023.109837},
  journal      = {Pattern Recognition},
  pages        = {109837},
  shortjournal = {Pattern Recognition},
  title        = {Learning consistent region features for lifelong person re-identification},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep multi-view spectral clustering via ensemble.
<em>PR</em>, <em>144</em>, 109836. (<a
href="https://doi.org/10.1016/j.patcog.2023.109836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based methods have achieved great success in multi-view clustering. However, existing graph-based models generally utilize shallow and linear embedding functions to obtain the common spectral embedding for clustering assignments. In addition, the fusion similarity graphs from multiple views are generally obtained by a simple weighted-sum rule. To this end, we propose a novel deep multi-view spectral clustering via ensemble model (DMCE), which applies ensemble clustering to fuse the similarity graphs from different views. On this basis, we employ the graph auto-encoder to learn the common spectral embedding, which can be regarded as the indicator matrix directly. Moreover, a unified optimization framework is designed to update the variables in the proposed DMCE, which consists of graph reconstruction loss, orthogonal loss, and graph contrastive learning loss. Extensive experiments on six real-world benchmark datasets have demonstrated the effectiveness of our model compared with the state-of-the-art multi-view clustering methods .},
  archive      = {J_PR},
  author       = {Mingyu Zhao and Weidong Yang and Feiping Nie},
  doi          = {10.1016/j.patcog.2023.109836},
  journal      = {Pattern Recognition},
  pages        = {109836},
  shortjournal = {Pattern Recognition},
  title        = {Deep multi-view spectral clustering via ensemble},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamics-aware loss for learning with label noise.
<em>PR</em>, <em>144</em>, 109835. (<a
href="https://doi.org/10.1016/j.patcog.2023.109835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss functions which reconcile fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamics of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn beneficial patterns, then gradually overfit harmful label noise, DAL strengthens the fitting ability initially, then gradually improves robustness. Moreover, at the later stage, to further reduce the negative impact of label noise and combat underfitting simultaneously, we let DNNs put more emphasis on easy examples than hard ones and introduce a bootstrapping term. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.},
  archive      = {J_PR},
  author       = {Xiu-Chuan Li and Xiaobo Xia and Fei Zhu and Tongliang Liu and Xu-Yao Zhang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2023.109835},
  journal      = {Pattern Recognition},
  pages        = {109835},
  shortjournal = {Pattern Recognition},
  title        = {Dynamics-aware loss for learning with label noise},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical multimodal transformers for multipage DocVQA.
<em>PR</em>, <em>144</em>, 109834. (<a
href="https://doi.org/10.1016/j.patcog.2023.109834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing work on DocVQA only considers single-page documents. However, in real applications documents are mostly composed of multiple pages that should be processed altogether. In this work, we propose a new multimodal hierarchical method Hi-VT5, that overcomes the limitations of current methods to process long multipage documents. In contrast to previous hierarchical methods that focus on different semantic granularity (He et al., 2021) or different subtasks (Zhou et al., 2022) used in image classification . Our method is a hierarchical transformer architecture where the encoder learns to summarize the most relevant information of every page and then, the decoder uses this summarized representation to generate the final answer, following a bottom-up approach. Moreover, due to the lack of multipage DocVQA datasets, we also introduce MP-DocVQA, an extension of SP-DocVQA where questions are posed over multipage documents instead of single pages. Through extensive experimentation, we demonstrate that Hi-VT5 is able, in a single stage, to answer the questions and provide the page that contains the answer, which can be used as a kind of explainability measure.},
  archive      = {J_PR},
  author       = {Rubèn Tito and Dimosthenis Karatzas and Ernest Valveny},
  doi          = {10.1016/j.patcog.2023.109834},
  journal      = {Pattern Recognition},
  pages        = {109834},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical multimodal transformers for multipage DocVQA},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph clustering network with structure embedding enhanced.
<em>PR</em>, <em>144</em>, 109833. (<a
href="https://doi.org/10.1016/j.patcog.2023.109833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep clustering utilizing Graph Neural Networks has shown good performance in the graph clustering . However, the structure information of graph was underused in existing deep clustering methods . Particularly, the lack of concern on mining different types structure information simultaneously. To tackle with the problem, this paper proposes a G raph C lustering Network with S tructure E mbedding E nhanced (GC-SEE) which extracts nodes importance-based and attributes importance-based structure information via a feature attention fusion graph convolution module and a graph attention encoder module respectively. Additionally, it captures different orders-based structure information through multi-scale feature fusion . Finally, a self-supervised learning module has been designed to integrate different types structure information and guide the updates of the GC-SEE. The comprehensive experiments on benchmark datasets commonly used demonstrate the superiority of the GC-SEE. The results showcase the effectiveness of the GC-SEE in exploiting multiple types of structure for deep clustering.},
  archive      = {J_PR},
  author       = {Shifei Ding and Benyu Wu and Xiao Xu and Lili Guo and Ling Ding},
  doi          = {10.1016/j.patcog.2023.109833},
  journal      = {Pattern Recognition},
  pages        = {109833},
  shortjournal = {Pattern Recognition},
  title        = {Graph clustering network with structure embedding enhanced},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperspectral image destriping and denoising from a task
decomposition view. <em>PR</em>, <em>144</em>, 109832. (<a
href="https://doi.org/10.1016/j.patcog.2023.109832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized mathematical model for HSI denoising or destriping lacks stability and uniqueness properties, failing to accurately portray the distribution and effects of stripes. Solutions following such a model would inevitably result in excessive destriping of strip-free areas, leading to the loss of texture detail. To remedy the above deficiencies, we reformulate the destriping task and introduce a novel solution from the task decomposition view. It is broken down into auxiliary sub-tasks involving stripe mask detection, stripe intensity estimation, and HSI restoration, which greatly reduces the difficulty of solving such an ill-posed problem. Based on this, we adopt a sequential multi-task learning framework and propose a stripes location-dependent restoration network, termed SLDR, which integrates the distribution and intensity features of stripes to achieve accurate destriping and high-fidelity restoration. Furthermore, we design a stripe attribute-aware estimator and a weighted total variation loss function to capture the unique properties of stripes and adaptively adjust the restoration weights of striped and non-striped regions. Extensive evaluation and comprehensive ablation studies on synthetic and practical scenes show the effectiveness and superiority of our model and architecture.},
  archive      = {J_PR},
  author       = {Erting Pan and Yong Ma and Xiaoguang Mei and Jun Huang and Qihai Chen and Jiayi Ma},
  doi          = {10.1016/j.patcog.2023.109832},
  journal      = {Pattern Recognition},
  pages        = {109832},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral image destriping and denoising from a task decomposition view},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised transfer learning with hierarchical
self-regularization. <em>PR</em>, <em>144</em>, 109831. (<a
href="https://doi.org/10.1016/j.patcog.2023.109831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both semi-supervised learning and transfer learning aim at lowering the annotation burden for training models. However, such two tasks are usually studied separately, i.e. most semi-supervised learning algorithms train models from scratch while transfer learning assumes pre-trained models as the initialization. In this work, we focus on a previously-less-concerned setting that further reduces the annotation efforts through incorporating both semi-supervised and transfer learning, where specifically a pre-trained source model is used as the initialization of semi-supervised learning. As those powerful pre-trained models are ubiquitously available nowadays and can considerably benefit various down-streaming tasks, such a setting is relevant to real-world applications yet challenging to design effective algorithms. Aiming at enabling transfer learning under semi-supervised settings, we propose a hierarchical self-regularization mechanism to exploit unlabeled samples more effectively, where a novel self-regularizer has been introduced to incorporate both individual-level and population-level regularization terms. The former term employs self-distillation to regularize learned deep features for each individual sample, and the latter one enforces self-consistency on feature distributions between labeled and unlabeled samples . Samples involved in both regularizers are weighted by an adaptive strategy, where self-regularization effects of both terms are adaptively controlled by the confidence of every sample. To validate our algorithm, exhaustive experiments have been conducted on diverse datasets such as CIFAR-10 for general object recognition, CUB-200-2011/MIT-indoor-67 for fine-grained classification and MURA for medical image classification . Compared with state-of-the-art semi-supervised learning methods including Pseudo Label , Mean Teacher , MixMatch and FixMatch , our algorithm demonstrates two advantages: first of all, the proposed approach adopts a new point of view to tackle problems caused by inadequate supervision and achieves very competitive results; then, it is complementary to these state-of-the-art methods and thus can be combined with them to get additional improvements. Furthermore, our method can also be applied to fully supervised transfer learning and self-supervised learning. We have published our code at https://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning .},
  archive      = {J_PR},
  author       = {Xingjian Li and Abulikemu Abuduweili and Humphrey Shi and Pengkun Yang and Dejing Dou and Haoyi Xiong and Chengzhong Xu},
  doi          = {10.1016/j.patcog.2023.109831},
  journal      = {Pattern Recognition},
  pages        = {109831},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised transfer learning with hierarchical self-regularization},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CrowdMLP: Weakly-supervised crowd counting via
multi-granularity MLP. <em>PR</em>, <em>144</em>, 109830. (<a
href="https://doi.org/10.1016/j.patcog.2023.109830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, state-of-the-art crowd counting algorithms rely excessively on location-level annotations, which are burdensome to acquire. When only weak supervisory signals at the count level are available, it is arduous and error-prone to regress total counts due to the lack of explicit spatial constraints. To address this issue, we propose a novel and efficient counter, CrowdMLP, which explores the modelling of global dependencies of embeddings and regresses total counts by designing a multi-granularity MLP regressor . Specifically, a locally-focused pre-trained frontend is used to extract crude feature maps with intrinsic spatial cues, preventing the model from collapsing into trivial outcomes. The crude embeddings, along with the raw crowd scenes, are tokenized at different granularity levels. Next, the multi-granularity MLP mixes tokens at the dimensions of cardinality, channel, and spatial for mining global information. We also propose an effective proxy task called Split-Counting to overcome the limited samples and the lack of spatial hints in a self-supervised manner. Extensive experiments demonstrate that CrowdMLP significantly outperforms existing weakly-supervised counting algorithms and performs better than state-of-the-art location-level supervised approaches.},
  archive      = {J_PR},
  author       = {Mingjie Wang and Jun Zhou and Hao Cai and Minglun Gong},
  doi          = {10.1016/j.patcog.2023.109830},
  journal      = {Pattern Recognition},
  pages        = {109830},
  shortjournal = {Pattern Recognition},
  title        = {CrowdMLP: Weakly-supervised crowd counting via multi-granularity MLP},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compositional clustering: Applications to multi-label object
recognition and speaker identification. <em>PR</em>, <em>144</em>,
109829. (<a href="https://doi.org/10.1016/j.patcog.2023.109829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a novel clustering task in which clusters can have compositional relationships, e.g., one cluster contains images of rectangles, one contains images of circles, and a third (compositional) cluster contains images with both objects. In contrast to hierarchical clustering in which a parent cluster represents the intersection of properties of the child clusters, our problem is about finding compositional clusters that represent the union of the properties of the constituent clusters. This task is motivated by recently developed few-shot learning and embedding models (Alfassy et al., 2019; Li et al., 2021) that can distinguish the label sets , not just the individual labels, assigned to the examples. We propose three new algorithms – Compositional Affinity Propagation (CAP), Compositional k k -means (CKM), and Greedy Compositional Reassignment (GCR) – that can partition examples into coherent groups and infer the compositional structure among them. We show promising results, compared to popular algorithms such as Gaussian mixtures, Fuzzy c c -means, and Agglomerative Clustering, on the OmniGlot and LibriSpeech datasets. Our work has applications to open-world multi-label object recognition and speaker identification &amp; diarization with simultaneous speech from multiple speakers.},
  archive      = {J_PR},
  author       = {Zeqian Li and Xinlu He and Jacob Whitehill},
  doi          = {10.1016/j.patcog.2023.109829},
  journal      = {Pattern Recognition},
  pages        = {109829},
  shortjournal = {Pattern Recognition},
  title        = {Compositional clustering: Applications to multi-label object recognition and speaker identification},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting the transferability of adversarial examples via
source-agnostic adversarial feature inducing method. <em>PR</em>,
<em>144</em>, 109828. (<a
href="https://doi.org/10.1016/j.patcog.2023.109828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though deep neural networks (DNNs) have revealed their extraordinary performance in the fields of computer vision , it is evident that the vulnerability of DNNs to adversarial attacks with crafted human-imperceptible perturbations. Most existing adversarial attacks draw their attention to invading target deep task models by enhancing input-diagnostic features via image rotation , warp, or transformation to improve adversarial transferability. Such manners pay close concentration to operation on original inputs regardless of the properties from different source information. Research has inspired us to consider utilizing source-agnostic information and integrating generated features with raw inputs to enrich adversarial properties. For such needs, we propose a simple and flexible adversarial attack method with source-agnostic Feature Inducing Method (FIM) for improving the transferability of adversarial examples (AEs). FIM first focuses on generating perturbed features by imitating diverse patterns from multi-domain sources. Instead of exploiting the original inputs’ diversity, such proposed work gains the various properties by random feature imitation referring to different source distributions. By optimizing the generated features with norm bounds, FIM then integrates original inputs with imitative features. Such manner can diverse row positive class-general features, which reduce the capability of class-specific patterns on cross-model transferability. Based on the crafted property, FIM employs the adaptive gradient-based strategy on such information to generate perturbations, which helps to decrease probability dropping into local optimal when searching for the decision boundary of source and target models. We conduct detailed experiments to evaluate the performance of our proposed approach with existing baselines on three public datasets. The experimental results reveal the better performance of the proposed works on fooling source and target task models leading to a considerable margin in most adversarial scenarios. We further investigate adversarial attacks on adversarial defense models (with adversarial training and trades). Such a proposed attack strategy achieves better attack quality by a margin over 3.00\% on CIFAR10 and reduces the robust accuracy of adversarially trained models by a large margin near 9.00\% on MNIST. Furthermore, we exploit the performance of the proposed attack strategy applied to feature-level adversarial domains and conduct evaluations to demonstrate its adversarial feasibility in integrating with various attack mechanisms, which gains better adversarial effectiveness over 20.00\% than the base attacks on studied deep task models.},
  archive      = {J_PR},
  author       = {Yatie Xiao and Jizhe Zhou and Kongyang Chen and Zhenbang Liu},
  doi          = {10.1016/j.patcog.2023.109828},
  journal      = {Pattern Recognition},
  pages        = {109828},
  shortjournal = {Pattern Recognition},
  title        = {Revisiting the transferability of adversarial examples via source-agnostic adversarial feature inducing method},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature fusion network for long-tailed visual recognition.
<em>PR</em>, <em>144</em>, 109827. (<a
href="https://doi.org/10.1016/j.patcog.2023.109827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved remarkable success in recent years; however, deep learning methods face significant challenges on long-tailed datasets, which are prevalent in real-world scenarios. In a long-tailed dataset, there are many more samples in the head classes than in the tail classes, and this class imbalance makes it difficult to learn a good feature representation for both head and tail classes simultaneously, particularly when using a single-stage method. Although the existing two-stage methods can alleviate the problem of single-stage methods not performing well on the tail classes by classifier retraining in the second stage, this does not resolve the problem of insufficient learning of head and tail features. Thus, in this paper, we propose a two-stage feature fusion network (FFN). The proposed FFN addresses this issue using one network for the head classes and another network for the tail classes, each of which is trained with a different loss function. This allows the feature learning module to effectively distinguish between the head and tail classes in the embedding space. The classifier learning module fuses the features obtained from the feature learning module, and the classifier is fine-tuned to classify the input images. Different from traditional two-stage methods, the proposed utilizes different loss functions for the head and tail classes; thus, the classifier can achieve balanced results between the head and tail classes. We conduct extensive experiments on three benchmark datasets comparing the proposed FFN with six state-of-the-art methods including two baseline methods , the experimental results demonstrate that the FFN achieves significant improvement on all three benchmark datasets. The code is publicly available at https://github.com/zxsong999/Feature-Fusion-Network.pytorch .},
  archive      = {J_PR},
  author       = {Xuesong Zhou and Junhai Zhai and Yang Cao},
  doi          = {10.1016/j.patcog.2023.109827},
  journal      = {Pattern Recognition},
  pages        = {109827},
  shortjournal = {Pattern Recognition},
  title        = {Feature fusion network for long-tailed visual recognition},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AC2AS: Activation consistency coupled ANN-SNN framework for
fast and memory-efficient SNN training. <em>PR</em>, <em>144</em>,
109826. (<a href="https://doi.org/10.1016/j.patcog.2023.109826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks are efficient computation models for low-power environments. Spike-based BP algorithms and ANN-to-SNN (ANN2SNN) conversions are successful techniques for SNN training. Nevertheless, the spike-base BP training is slow and requires large memory costs, while ANN2SNN needs many inference steps to obtain good performance. In this paper, we propose an A ctivation C onsistency C oupled A NN- S NN (A C 2 C2 AS) framework to train the SNN in a fast and memory-efficient way. The A C 2 C2 AS consists of two components: (a) a weight-shared architecture between ANN and SNN and (b) spiking mapping units. Firstly, the architecture trains the weight-shared parameters on the ANN branch, resulting in fast training and low memory costs for SNN. Secondly, the spiking mapping units are designed to ensure that the activation values of the ANN are the spiking features. As a result, the activation consistency is guaranteed, and the classification error of the SNN can be optimized by training the ANN branch. Besides, we design an adaptive threshold adjustment (ATA) algorithm to decrease the firing of noisy spikes. Experiment results show that our A C 2 C2 AS-based models perform well on the benchmark datasets (CIFAR10, CIFAR100, and Tiny-ImageNet). Moreover, the A C 2 C2 AS achieves comparable accuracy under 0 . 625 × 0.625× time steps, 0 . 377 × 0.377× training time, 0 . 27 × 0.27× GPU memory costs, and 0 . 33 × 0.33× spike activities of the Spike-based BP model. The code is available at https://github.com/TJXTT/AC2ASNN .},
  archive      = {J_PR},
  author       = {Jianxiong Tang and Jian-Huang Lai and Xiaohua Xie and Lingxiao Yang and Wei-Shi Zheng},
  doi          = {10.1016/j.patcog.2023.109826},
  journal      = {Pattern Recognition},
  pages        = {109826},
  shortjournal = {Pattern Recognition},
  title        = {AC2AS: Activation consistency coupled ANN-SNN framework for fast and memory-efficient SNN training},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive local adversarial attacks on 3D point clouds.
<em>PR</em>, <em>144</em>, 109825. (<a
href="https://doi.org/10.1016/j.patcog.2023.109825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern artificial intelligence systems rely heavily on deep learning techniques. However, deep neural networks are easily disturbed by adversarial objects. Adversarial examples are beneficial to improve the robustness of the 3D neural network model and enhance the stability of the artificial intelligence system. At present, most 3D adversarial attack methods perturb the entire point cloud to generate adversarial examples, which results in high perturbation costs and low operability in the physical world. In this paper, we propose an adaptive local adversarial attack method (AL-Adv) on 3D point clouds to generate adversarial point clouds. First, we analyze the vulnerability of the 3D network model and extract the salient regions of the input point cloud, namely the vulnerable regions. Second, we propose an adaptive gradient attack algorithm that targets salient regions . The proposed attack algorithm adaptively assigns different disturbances in different directions of the three-dimensional coordinates of the point cloud. Experimental results show that our proposed AL-Adv method achieves a higher attack success rate than the global attack method. Specifically, the adversarial examples generated by AL-Adv demonstrate good imperceptibility and small generation costs.},
  archive      = {J_PR},
  author       = {Shijun Zheng and Weiquan Liu and Siqi Shen and Yu Zang and Chenglu Wen and Ming Cheng and Cheng Wang},
  doi          = {10.1016/j.patcog.2023.109825},
  journal      = {Pattern Recognition},
  pages        = {109825},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive local adversarial attacks on 3D point clouds},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A balanced random learning strategy for CNN based landsat
image segmentation under imbalanced and noisy labels. <em>PR</em>,
<em>144</em>, 109824. (<a
href="https://doi.org/10.1016/j.patcog.2023.109824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landsat image segmentation is important for obtaining large-scale land cover maps. The accuracy of CNN-based Landsat image segmentation highly depends on the quantity and quality of the training samples. However, enough accurate labels for Landsat images are difficult to access. Fortunately, traditional classifier induced segmentation results can be considered as an alternative, although they are noisy and unbalanced to a certain extent. To resist noisy labels and alleviate the impact of imbalanced samples, this paper proposes a confidence interval based balanced random learning strategy. Firstly, a confidence interval-based mask is employed to control the random learning rate of the network from the entire noisy training set. Then, the multi-layer feature maps of CNN are fully utilized to compensate for the information loss in random learning, in which down-sampled labels are used to decrease the uncertainty brought by up-sampling CNN feature maps. In addition, considering the corruption of noisy labels on different classes, a balanced random learning with different confidence levels is performed on each class to further improve the learning ability of CNN. Experimental results on two widely used backbones, namely VGGNet and ResNet , demonstrate that the proposed balanced random learning strategy can effectively improve the performance of CNN under imbalanced and noisy labels, which can be improved by 3.41\%.},
  archive      = {J_PR},
  author       = {Xuemei Zhao and Yong Cheng and Luo Liang and Haijian Wang and Xingyu Gao and Jun Wu},
  doi          = {10.1016/j.patcog.2023.109824},
  journal      = {Pattern Recognition},
  pages        = {109824},
  shortjournal = {Pattern Recognition},
  title        = {A balanced random learning strategy for CNN based landsat image segmentation under imbalanced and noisy labels},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reflectance re-weighted retinex model for non-uniform and
low-light image enhancement. <em>PR</em>, <em>144</em>, 109823. (<a
href="https://doi.org/10.1016/j.patcog.2023.109823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image enhancement is a fundamental low-level task of significant importance that can directly affect high-level image processing tasks. Although various methods have been proposed to enhance images, the effectiveness of current methods deteriorates significantly under non-uniform lighting. Since the brightness may vary dramatically in different regions of real-world photos, current methods hardly achieve a good balance between enhancing low-light regions and retaining normal-light regions in the same image. Consequently, either the low-light regions are under-enhanced or the normal-light regions are over-enhanced, while at the same time, color distortion and artifacts are frequently found. To overcome this shortcoming, we propose a robust Retinex-based model with reflectance map re-weighting that can improve the brightness level of the low-light image and re-balance the brightness concurrently. We introduce an alternating scheme to solve our proposed model, in which the illumination map, reflectance map , and weighting map are updated iteratively. By utilizing the regularization terms, the noise is well-suppressed during the process. An initialization scheme for the weighting map is also proposed to make our model adaptable to a wide range of light conditions. To the best of our knowledge, we are the first to propose a variational model with an explicitly constructed re-weighting prior and the associated weighing map concept for the reflectance map. It can estimate the reflectance map, suppress noise, and re-balance the brightness simultaneously. A series of experimental results on a variety of popular datasets demonstrate the efficacy of our method and its superiority in enhancing real low-light images when compared to other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Fan Jia and Hok Shing Wong and Tiange Wang and Tieyong Zeng},
  doi          = {10.1016/j.patcog.2023.109823},
  journal      = {Pattern Recognition},
  pages        = {109823},
  shortjournal = {Pattern Recognition},
  title        = {A reflectance re-weighted retinex model for non-uniform and low-light image enhancement},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unpaired image super-resolution using a lightweight
invertible neural network. <em>PR</em>, <em>144</em>, 109822. (<a
href="https://doi.org/10.1016/j.patcog.2023.109822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unpaired image super-resolution (SR) has recently attracted considerable attention in the unsupervised SR community. In contrast to supervised SR, existing unpaired SR methods inevitably resort to the generative adversarial network (GAN) to explore data distribution on the given HR and unpaired LR dataset. Nevertheless, predominant strategies often strive for sophisticated network structures or training pipelines, making them intractable to apply in real-world scenarios. In this work, a lightweight invertible neural network (INN) is proposed for unpaired SR to alleviate this limitation. Specifically, we regard image degradation and SR as a pair of mutually-inverse tasks and replace the two generators in one-stage GAN with INN. Due to the information lossless nature of INN, it is impossible to generate noise in vain during image degradation. We thus design a simple noise injection network to induce realistic noise, thereby simulating real LR images . To further maintain the stability and realism of the noise, we propose to extract the noise prior from the real-world LR image. With extracted noise prior as input, our noise injection network can narrow the gap between the generated noise and the real one, thereby encouraging the degraded images to match the real-world LR domain. Extensive experiments demonstrate that our method achieves comparable performance with other SOTA methods in quantitative and qualitative evaluations while enjoying faster speed and much smaller parameters.},
  archive      = {J_PR},
  author       = {Huan Liu and Mingwen Shao and Yuanjian Qiao and Yecong Wan and Deyu Meng},
  doi          = {10.1016/j.patcog.2023.109822},
  journal      = {Pattern Recognition},
  pages        = {109822},
  shortjournal = {Pattern Recognition},
  title        = {Unpaired image super-resolution using a lightweight invertible neural network},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ANAS: Asymptotic NAS for large-scale proxyless search and
multi-task transfer learning. <em>PR</em>, <em>144</em>, 109821. (<a
href="https://doi.org/10.1016/j.patcog.2023.109821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) is an emerging solution to design a lightweight network for researchers to obtain a trade-off between accuracy and speed, releasing researchers from tedious repeated trials. However, the main shortcoming of NAS is its high and unstable memory consumption of the search work, especially for large-scale tasks. In this study, the proposed Asymptotic Neural Architecture Search network (ANAS) achieved a proxyless search for large-scale tasks with economic and stable memory consumption. Instead of proxy search like other NAS algorithms, ANAS achieved the large-scale proxyless search that directly learns deep neural network architecture for target task. ANAS reduced the peak value of memory consumption by an asymptotic method , and kept the memory consumption stable by the linkage change of a series of key indexes. A pruning operation and efficient candidate operations decreased the total memory consumption. Finally, ANAS achieved a good trade-off between accuracy and speed for classification tasks on CIFAR-10, CIFAR-100, and ImageNet datasets. Besides, except for the classification task, it achieved excellent multi-task transfer learning ability for implementing the segmentation task on CamVid and Cityscapes. ANAS reached 22.8\% test errs with 5 M parameter on ImageNet, and 72.9 mIoU (mean Intersection over Union) with 119.9 FPS (Frames Per Second) on Cityscapes dataset.},
  archive      = {J_PR},
  author       = {Bangquan Xie and Zongming Yang and Liang Yang and Ruifa Luo and Jun Lu and Ailin Wei and Xiaoxiong Weng and Bing Li},
  doi          = {10.1016/j.patcog.2023.109821},
  journal      = {Pattern Recognition},
  pages        = {109821},
  shortjournal = {Pattern Recognition},
  title        = {ANAS: Asymptotic NAS for large-scale proxyless search and multi-task transfer learning},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighborhood overlap-aware heterogeneous hypergraph neural
network for link prediction. <em>PR</em>, <em>144</em>, 109818. (<a
href="https://doi.org/10.1016/j.patcog.2023.109818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real world, a large number of networks are heterogeneous, containing different types of semantics and connections. Existing studies typically only consider lower-order pairwise relations rather than higher-order group interactions. Furthermore, they tend to focus more on node attributes rather than graph structural information. This results models failing to maintain graph topology effectively, which reduces the effectiveness on link prediction. To address these limitations, we propose N N eighborhood O O verlap-aware H H eterogeneous hypergraph neural network (NOH) that learns useful structural information from the heterogeneous graph and estimates overlapped neighborhood for link prediction. Our model fuses the heterogeneity of graphs with structural information so that the model maintains both lower-order pairwise relations and higher-order complex semantics. Our extensive experiments on four real-world datasets show that NOH consistently achieves state-of-the-art performance on link prediction.},
  archive      = {J_PR},
  author       = {Yifan Lu and Mengzhou Gao and Huan Liu and Zehao Liu and Wei Yu and Xiaoming Li and Pengfei Jiao},
  doi          = {10.1016/j.patcog.2023.109818},
  journal      = {Pattern Recognition},
  pages        = {109818},
  shortjournal = {Pattern Recognition},
  title        = {Neighborhood overlap-aware heterogeneous hypergraph neural network for link prediction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust table structure recognition with dynamic queries
enhanced detection transformer. <em>PR</em>, <em>144</em>, 109817. (<a
href="https://doi.org/10.1016/j.patcog.2023.109817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognize the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks ; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW, FinTabNet, and cTDaR TrackB2-Modern. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.},
  archive      = {J_PR},
  author       = {Jiawei Wang and Weihong Lin and Chixiang Ma and Mingze Li and Zheng Sun and Lei Sun and Qiang Huo},
  doi          = {10.1016/j.patcog.2023.109817},
  journal      = {Pattern Recognition},
  pages        = {109817},
  shortjournal = {Pattern Recognition},
  title        = {Robust table structure recognition with dynamic queries enhanced detection transformer},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Paired contrastive feature for highly reliable offline
signature verification. <em>PR</em>, <em>144</em>, 109816. (<a
href="https://doi.org/10.1016/j.patcog.2023.109816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signature verification requires high reliability. Especially in the writer-independent scenario with the skilled-forgery-only condition, achieving high reliability is challenging but very important. In this paper, we propose to apply two machine learning frameworks, learning with rejection and top-rank learning, to this task because they can suppress ambiguous results and thus give only reliable verification results. Since those frameworks accept a single input, we transform a pair of genuine and query signatures into a single feature vector, called Paired Contrastive Feature (PCF). PCF internally represents similarity (or discrepancy) between the two paired signatures; thus, reliable machine learning frameworks can make reliable decisions using PCF. Through experiments on three public signature datasets in the offline skilled-forgery-only writer-independent scenario, we evaluate and validate the effectiveness and reliability of the proposed models by comparing their performance with a state-of-the-art model.},
  archive      = {J_PR},
  author       = {Xiaotong ji and Daiki Suehiro and Seiichi Uchida},
  doi          = {10.1016/j.patcog.2023.109816},
  journal      = {Pattern Recognition},
  pages        = {109816},
  shortjournal = {Pattern Recognition},
  title        = {Paired contrastive feature for highly reliable offline signature verification},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain few-shot classification via class-shared and
class-specific dictionaries. <em>PR</em>, <em>144</em>, 109811. (<a
href="https://doi.org/10.1016/j.patcog.2023.109811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Cross-Domain Few-Shot Classification, researchers mainly utilize models which trained with source domain tasks to adapt to the target domain with very few samples, thus causing serious class-difference-caused domain differences. Although researchers have proposed methods to minimize the domain differences, the existing methods have the following drawbacks: 1) most models do not utilize the common knowledge between the source and target domains, and 2) require additional labeled samples from the target domain for finetuning or domain alignment, which is hard to obtain in reality. To address the problem mentioned above, we propose a class-shared and class-specific dictionaries (CSCSD) learning method. To make better utilization of the common knowledge, we apply a class-shared dictionary which is learned to represent the generality of source and target domain. Moreover, class-specific dictionaries are applied to represent the class-specific knowledge that can’t be represented in the class-shared dictionary. Furthermore, unlike most other models, our CSCSD does not require additional target domain samples to meta-train or finetune. With the dictionaries, CSCSD can obtain more distinguishable collaborative representations of samples with the origin representations extracted with the model. To evaluate the effectiveness of CSCSD, we utilize larger datasets, e.g., MiniImageNet and TieredImageNet as source domains and fine-grained datasets, e.g., CUB, Cars, Places, and Plantae as target domains. With our CSCSD, the Cross-Domain Few-Shot accuracy exceeds most domain adaptive Few-Shot which utilizes additional training set in target domains.},
  archive      = {J_PR},
  author       = {Renjie Xu and Lei Xing and Baodi Liu and Dapeng Tao and Weijia Cao and Weifeng Liu},
  doi          = {10.1016/j.patcog.2023.109811},
  journal      = {Pattern Recognition},
  pages        = {109811},
  shortjournal = {Pattern Recognition},
  title        = {Cross-domain few-shot classification via class-shared and class-specific dictionaries},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast stereo matching network based on temporal attention
and 2D convolution. <em>PR</em>, <em>144</em>, 109808. (<a
href="https://doi.org/10.1016/j.patcog.2023.109808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a fast stereo matching network based on temporal attention and 2D convolution (TANet). Due to the high similarity of the disparity between consecutive frames in an image sequence, we propose a temporal attention (TA) module that uses the disparity map of the previous frame to guide the disparity search range in the current frame, thus significantly improving the efficiency of disparity calculation in the cost volume module. Additionally, we propose a hierarchical cost construction and 2D convolution aggregation module that constructs a pyramid cost volume by fusing edge cues to establish detail constraints. This overcomes the problem of difficult convergence caused by information loss when replacing 3D convolution with 2D convolution. Experimental results show that the TA module effectively optimizes the cost volume and, together with 2D convolution, improves the computational speed. Compared with state-of-the-art algorithms, TANet achieves a speedup of nearly 4x, with a running time of 0.061s, and reduces the parameter count by nearly half while decreasing accuracy by 1.1\%. Code is available at https://github.com/Y0uchenZ/TANet.},
  archive      = {J_PR},
  author       = {Youchen Zhao and Hua Zhong and Boyuan Jia and Haixiong Li},
  doi          = {10.1016/j.patcog.2023.109808},
  journal      = {Pattern Recognition},
  pages        = {109808},
  shortjournal = {Pattern Recognition},
  title        = {A fast stereo matching network based on temporal attention and 2D convolution},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving point cloud classification and segmentation via
parametric veronese mapping. <em>PR</em>, <em>144</em>, 109784. (<a
href="https://doi.org/10.1016/j.patcog.2023.109784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based 3D point cloud classification and segmentation has achieved remarkable success. Existing methods are usually implemented in the original space with 3D coordinates as inputs. However, we find that point networks taking only information of first-order coordinates hardly learn geometric features of higher order, such as point cloud normals or poses. In this study, we propose to map the input point clouds into a non-linear space to facilitate networks learning and leveraging high-order features. Firstly, we design the Parametric Veronese Mapping (PVM) function which automatically learns to map point clouds into a non-linear space. As a result, the mapped point clouds are enriched with high-order elements and maintain the basic point set properties as in the original 3D space. We can then exploit existing networks to learn high-order features from mapped point clouds. Secondly, we contribute a two-stage transformation learning module that modifies the previous one-stage module to better leverage high-order features for aligning point clouds in the projective space . Finally, an interaction module is designed to learn more discriminative features by aggregating information from both the original and projective space . Extensive experiments demonstrate that our method successfully improves the ability of most existing networks to learn high-order features and thus contributing to more accurate classification and segmentation. Moreover, the resulting models show stronger robustness to affine transformations and real-world perturbations.},
  archive      = {J_PR},
  author       = {Ruibin Wang and Xianghua Ying and Bowei Xing and Xin Tong and Taiyan Chen and Jinfa Yang and Yongjie Shi},
  doi          = {10.1016/j.patcog.2023.109784},
  journal      = {Pattern Recognition},
  pages        = {109784},
  shortjournal = {Pattern Recognition},
  title        = {Improving point cloud classification and segmentation via parametric veronese mapping},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on deep learning-based image forgery detection.
<em>PR</em>, <em>144</em>, 109778. (<a
href="https://doi.org/10.1016/j.patcog.2023.109778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image is known as one of the communication tools between humans. With the development and availability of digital devices such as cameras and cell phones, taking images has become easy anywhere. Images are used in many medical, forensic medicine, and judiciary applications. Sometimes images are used as evidence, so the authenticity and reliability of digital images are increasingly important. Some people manipulate images by adding or deleting parts of an image, which makes the image invalid. Therefore, image forgery detection and localization are important. The development of image editing tools has made this issue an important problem in the field of computer vision . In recent years, many different algorithms have been proposed to detect forgery in the image and pixel levels . All these algorithms are categorized into two main methods: traditional and deep-learning methods. The deep learning method is one of the important branches of artificial intelligence science. This method has become one of the most popular methods in most computer vision problems due to the automatic identification and prediction process and robustness against geometric transformations and post-processing operations. In this study, a comprehensive review of image forgery types, benchmark datasets, evaluation metrics in forgery detection, traditional forgery detection methods, discovering the weaknesses and limitations of traditional methods, forgery detection with deep learning methods, and the performance of this method is presented. According to the expansion of deep-learning methods and their successful performance in most computer vision problems, our main focus in this study is forgery detection based on deep-learning methods. This survey can be helpful for a researcher to obtain a deep background in the forgery detection field.},
  archive      = {J_PR},
  author       = {Fatemeh Zare Mehrjardi and Ali Mohammad Latif and Mohsen Sardari Zarchi and Razieh Sheikhpour},
  doi          = {10.1016/j.patcog.2023.109778},
  journal      = {Pattern Recognition},
  pages        = {109778},
  shortjournal = {Pattern Recognition},
  title        = {A survey on deep learning-based image forgery detection},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uniform misclassification loss for unbiased model
prediction. <em>PR</em>, <em>144</em>, 109689. (<a
href="https://doi.org/10.1016/j.patcog.2023.109689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning algorithms have achieved tremendous success over the past few years. However, the biased behavior of deep models, where the models favor/disfavor certain demographic subgroups, is a major concern in the deep learning community. Several adverse consequences of biased predictions have been observed in the past. One solution to alleviate the problem is to train deep models for fair outcomes. Therefore, in this research, we propose a novel loss function, termed as Uniform Misclassification Loss (UML) to train deep models for unbiased outcomes. The proposed UML function penalizes the model for the worst-performing subgroup for mitigating bias and enhancing the overall model performance. The proposed loss function is also effective while training with imbalanced data as well. Further, a metric, Joint Performance Disparity Measure (JPD) is introduced to jointly measure the overall model performance and the bias in model prediction. Multiple experiments have been performed on four publicly available datasets for facial attribute prediction and comparisons are performed with existing bias mitigation algorithms. Experimental results are reported using performance and bias evaluation metrics . The proposed loss function outperforms existing bias mitigation algorithms that showcase its effectiveness in obtaining unbiased outcomes and improved performance.},
  archive      = {J_PR},
  author       = {Puspita Majumdar and Mayank Vatsa and Richa Singh},
  doi          = {10.1016/j.patcog.2023.109689},
  journal      = {Pattern Recognition},
  pages        = {109689},
  shortjournal = {Pattern Recognition},
  title        = {Uniform misclassification loss for unbiased model prediction},
  volume       = {144},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive deep support vector data description.
<em>PR</em>, <em>143</em>, 109820. (<a
href="https://doi.org/10.1016/j.patcog.2023.109820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In comparison with support vector data description (SVDD), deep SVDD (DSVDD) is more suitable for dealing with large-scale data sets. DSVDD uses mapping network to replace the role of kernel mapping in SVDD. Moreover, the objective of DSVDD is to simultaneously learn the optimal connection weights of mapping network and the minimum volume of hypersphere. To further improve the performance of DSVDD for tackling large-scale data sets and obtain the discriminative features of the given samples in a self-supervised learning manner, contrastive DSVDD (CDSVDD) is proposed in this study. In the pre-training phase of CDSVDD, the contrastive loss and the rotation prediction loss are jointly minimized to achieve the optimal feature representations. Furthermore, the learned feature representations are utilized to determine the hypersphere center. In the training phase of CDSVDD, the distances between the obtained feature representations and the hypersphere center together with the contrastive loss are simultaneously minimized to derive the optimal network connection weights, the minimum volume of hypersphere and the optimal feature representations. In addition, CDSVDD can efficiently solve the hypersphere collapse problem of DSVDD. The ablation study on CDSVDD verifies that compared with the case of determining the hypersphere center by the feature representations of the original samples, the hypersphere center determined by the feature representations of the augmented samples makes CDSVDD achieve better hypersphere boundary and more compact feature representations. Experimental results on the four benchmark data sets demonstrate that the proposed CDSVDD acquires better detection performance in comparison with its six pertinent methods.},
  archive      = {J_PR},
  author       = {Hong-Jie Xing and Ping-Ping Zhang},
  doi          = {10.1016/j.patcog.2023.109820},
  journal      = {Pattern Recognition},
  pages        = {109820},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive deep support vector data description},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Haar wavelet downsampling: A simple but effective
downsampling module for semantic segmentation. <em>PR</em>,
<em>143</em>, 109819. (<a
href="https://doi.org/10.1016/j.patcog.2023.109819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Downsampling operations such as max pooling or strided convolution are ubiquitously utilized in Convolutional Neural Networks (CNNs) to aggregate local features , enlarge receptive field, and minimize computational overhead. However, for a semantic segmentation task, pooling features over the local neighbourhood may result in the loss of important spatial information, which is conducive for pixel-wise predictions. To address this issue, we introduce a simple yet effective pooling operation called the Haar Wavelet-based Downsampling (HWD) module. This module can be easily integrated into CNNs to enhance the performance of semantic segmentation models. The core idea of HWD is to apply Haar wavelet transform for reducing the spatial resolution of feature maps while preserving as much information as possible. Furthermore, to investigate the benefits of HWD, we propose a novel metric, named as feature entropy index (FEI), which measures the degree of information uncertainty after downsampling in CNNs. Specifically, the FEI can be used to indicate the ability of downsampling methods to preserve essential information in semantic segmentation. Our comprehensive experiments demonstrate that the proposed HWD module could (1) effectively improve the segmentation performance across different modality image datasets with various CNN architectures, and (2) efficiently reduce information uncertainty compared to the conventional downsampling methods. Our implementation are available at https://github.com/apple1986/HWD.},
  archive      = {J_PR},
  author       = {Guoping Xu and Wentao Liao and Xuan Zhang and Chang Li and Xinwei He and Xinglong Wu},
  doi          = {10.1016/j.patcog.2023.109819},
  journal      = {Pattern Recognition},
  pages        = {109819},
  shortjournal = {Pattern Recognition},
  title        = {Haar wavelet downsampling: A simple but effective downsampling module for semantic segmentation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixed data clustering based on a number of similar features.
<em>PR</em>, <em>143</em>, 109815. (<a
href="https://doi.org/10.1016/j.patcog.2023.109815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the degree of similarity measurement is one of the challenges of mixed data clustering . In this article, it has been tried to design a more efficient method by innovating in three important parts of clustering. In the part of the general method, for assigning data objects to the cluster, in addition to the distance, attention is paid to the “number of similar features”. Compared to assigning each object to a cluster, in cases where the distances are equal or close, the cluster center with the highest number of features similar to the given objects will be appropriate. This method is more accurate than the Hamming distance . To determine the cluster centers, instead of random selection, a more suitable object is identified with a distance-based method. In accuracy in three datasets, the proposed algorithm has performed at least two percent better than the other algorithms.},
  archive      = {J_PR},
  author       = {Hamid Rezaei and Negin Daneshpour},
  doi          = {10.1016/j.patcog.2023.109815},
  journal      = {Pattern Recognition},
  pages        = {109815},
  shortjournal = {Pattern Recognition},
  title        = {Mixed data clustering based on a number of similar features},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DITAN: A deep-learning domain agnostic framework for
detection and interpretation of temporally-based multivariate ANomalies.
<em>PR</em>, <em>143</em>, 109814. (<a
href="https://doi.org/10.1016/j.patcog.2023.109814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present DITAN, a novel unsupervised domain-agnostic framework for detecting and interpreting temporal-based anomalies. It is based on an encoder-decoder architecture with both implicit/explicit attention and adjustable layers/units for predicting normality as regular patterns in sequential data. A two-stage thresholding methodology with built-in pruning is used to detect anomalies , while root cause and similarities are interpreted in data and units space. Our approach is designed to intersect the 9 fundamental characteristics extracted from the union of related works. We demonstrate the DITAN modules on real-world datasets of 6 multivariate time series contaminated by point and contextual temporal-based anomalies at a varying duration. Experiments show a dominant predictability power of DITAN against the originally proposed models. DITAN is able to determine critical regions and thus identify anomalous events similarly well. Informative similarities between anomalous records are interpreted, since almost all similarities in units space are also verified in data space.},
  archive      = {J_PR},
  author       = {Michail Giannoulis and Andrew Harris and Vincent Barra},
  doi          = {10.1016/j.patcog.2023.109814},
  journal      = {Pattern Recognition},
  pages        = {109814},
  shortjournal = {Pattern Recognition},
  title        = {DITAN: A deep-learning domain agnostic framework for detection and interpretation of temporally-based multivariate ANomalies},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-adaptive soft-recoding strategy for performance
improvement of error-correcting output codes. <em>PR</em>, <em>143</em>,
109813. (<a href="https://doi.org/10.1016/j.patcog.2023.109813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technique of error-correcting output codes (ECOC) has been proven to be of high discriminative ability in many classification applications. However, most algorithms on the ECOC were designed based on the binary or ternary codes (referred to as the hard codes), which might fail to precisely correct errors in dealing with tough tasks. In this study, a Soft-Recoding strategy based on a self-adaptive algorithm is proposed, which replaces the traditional hard codes with the real-value elements to better fit the output distribution of the base learners. This is achieved by minimizing the ratio of two distances: the distance of the output vector to the ground-truth class, and the average distance of the output vector to the remaining classes. Extensive experiments using five different hard ECOC algorithms and the corresponding softened versions on twenty datasets with diversified numbers of features and classes confirm the effectiveness of our Soft-Recoding strategy in promoting the performance of the original ECOC algorithms. Our source code and additional results are available at: github.com/MLDMXM2017/SA-soft-recoding.},
  archive      = {J_PR},
  author       = {Guangyi Lin and Jie Gao and Nan Zeng and Yong Xu and Kunhong Liu and Beizhan Wang and Junfeng Yao and Qingqiang Wu},
  doi          = {10.1016/j.patcog.2023.109813},
  journal      = {Pattern Recognition},
  pages        = {109813},
  shortjournal = {Pattern Recognition},
  title        = {A self-adaptive soft-recoding strategy for performance improvement of error-correcting output codes},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guided deep embedded clustering regularization for
multifeature medical signal classification. <em>PR</em>, <em>143</em>,
109812. (<a href="https://doi.org/10.1016/j.patcog.2023.109812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical signal classification often focuses on one representation (raw signal or time frequency). In that context, recent works have shown the value of exploiting different representations simultaneously. We propose a regularized end-to-end trained model for classification in a medical context exploiting both the raw signal and a time-frequency representation (TFR). First, a 2D convolutional neural network (CNN) encoder and a 1D CNN-transformer encoder start by extracting embedded representations from the TFR and the raw signal, respectively. Then, the obtained embeddings are fused to form a common latent space that is used for classification. We propose to guide the training of each encoder by applying two iterated losses. Moreover, we propose to regularize the fused common latent space using deep embedded clustering. Extensive experiments on three medical datasets and ablation studies show the adaptability and good performance of our method for medical signal classification. Our method makes it possible to improve the classification performance from 4\% 4\% to 12\% 12\% MCC on a transcranial Doppler dataset, when compared with single-feature counterparts, while giving more stable models . The code is available at: https://github.com/gdec-submission/gdec/ .},
  archive      = {J_PR},
  author       = {Yamil Vindas and Emmanuel Roux and Blaise Kévin Guépié and Marilys Almar and Philippe Delachartre},
  doi          = {10.1016/j.patcog.2023.109812},
  journal      = {Pattern Recognition},
  pages        = {109812},
  shortjournal = {Pattern Recognition},
  title        = {Guided deep embedded clustering regularization for multifeature medical signal classification},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational bayesian deep network for blind poisson
denoising. <em>PR</em>, <em>143</em>, 109810. (<a
href="https://doi.org/10.1016/j.patcog.2023.109810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based approaches have recently achieved considerable results in Poisson denoising under low-light conditions. However, most existing methods mainly focus on the network architecture design, which lacks physical interpretability and thus unsuitable for blind denoising in real environments with unknown levels of noises. To address this issue, we propose a variational Bayesian deep network for blind Poisson denoising (VBDNet). We mainly consider an approximate posterior form for the noise variance in a variational Bayesian framework and utilize a neural network to parameterize the variance of Poisson noise. For network design, VBDNet is divided into two sub-networks. The noise estimation sub-network is responsible for the Bayesian inference. This network improves the blind denoising ability of the subsequent denoising sub-network by learning Poisson noise characteristics under different noise levels in the training process. A network of U-Net structures implements the denoising sub-network for noise removal. By combining the advantage of Bayesian inference (noise estimation sub-network) and deep learning (denoising sub-network), VBDNet outperforms other state-of-the-art methods on both synthetic and natural data. The code and details are available at https://github.com/HLImg/VBDNet .},
  archive      = {J_PR},
  author       = {Hao Liang and Rui Liu and Zhongyuan Wang and Jiayi Ma and Xin Tian},
  doi          = {10.1016/j.patcog.2023.109810},
  journal      = {Pattern Recognition},
  pages        = {109810},
  shortjournal = {Pattern Recognition},
  title        = {Variational bayesian deep network for blind poisson denoising},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information-diffused graph tracking with linear complexity.
<em>PR</em>, <em>143</em>, 109809. (<a
href="https://doi.org/10.1016/j.patcog.2023.109809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mainstream tracking approaches have achieved remarkable performance by adopting transformer structures. However, transformer structures’ inherent design of dot-product with softmax normalization incurs quadratic computation complexity regarding sequence length. This issue is further complicated when vision tasks employ softmax attention, as sequence length scales with the square of images’ sizes. Even though sparse attention and low-rank decomposition can alleviate over-inflated computation, it is still laborious to balance trackers’ accuracy, computation cost, and inference speed. To tackle the above problems, we propose an Information-Diffused Graph tracking pipeline with linear complexity (IDGtrack). As the feature constraint relationship in the physical world is an important cue for vision tasks, graph modules are constructed with information-diffused adjacency matrices to substitute softmax attention, which is not only efficient for linear computations but also maintains the non-negativity and global distribution of the attention matrix. Distinct from traditional linear attention methods exclusive to self-attention, a self-integrated and cross-context graph module with linear complexity is explored where a complete bipartite graph is established between the target and search region, facilitating a comprehensive perception of local and background information. Extensive experiments are conducted on public tracking benchmarks, demonstrating that our method achieves state-of-the-art (SOTA) performance with 111 FPS on GPU RTX3090.},
  archive      = {J_PR},
  author       = {Zhixing Wang and Jinzhen Yao and Chuanming Tang and Jianlin Zhang and Qiliang Bao and Zhenming Peng},
  doi          = {10.1016/j.patcog.2023.109809},
  journal      = {Pattern Recognition},
  pages        = {109809},
  shortjournal = {Pattern Recognition},
  title        = {Information-diffused graph tracking with linear complexity},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Music mobility patterns: How songs propagate around the
world through spotify. <em>PR</em>, <em>143</em>, 109807. (<a
href="https://doi.org/10.1016/j.patcog.2023.109807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, music streaming services allow users to get instant access to an unprecedented amount of music of any type. This entails that songs, including potential new hits, can be discovered by listeners in any part of the globe and their propagation can be tracked through these streaming services. In this context, the present work focuses on recognizing the mobility patterns that songs follow in such a propagation among different countries of the world. To this end, this work defines a novel mechanism to uncover such mobility patterns of music from a directed-graph structure where nodes are countries and each edge reflects a frequent propagation of songs between pairs of countries. The resulting patterns reflect strong correlations with the migratory flows and the cultural and social similarities among regions. For instance, a propagation pattern was observed among North European countries with a good command of English. From such patterns, potential predictors for anticipating the mobility of a song are discussed. The results of this work can be beneficial for record companies and artists in their marketing campaigns for album releases and the organization of concerts and festivals.},
  archive      = {J_PR},
  author       = {Fernando Terroso-Saenz and Jesús Soto and Andres Muñoz},
  doi          = {10.1016/j.patcog.2023.109807},
  journal      = {Pattern Recognition},
  pages        = {109807},
  shortjournal = {Pattern Recognition},
  title        = {Music mobility patterns: How songs propagate around the world through spotify},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KD-former: Kinematic and dynamic coupled transformer network
for 3D human motion prediction. <em>PR</em>, <em>143</em>, 109806. (<a
href="https://doi.org/10.1016/j.patcog.2023.109806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have made remarkable progress on 3D human motion prediction by describing motion with kinematic knowledge. However, kinematics only considers the 3D positions or rotations of human skeletons, failing to reveal the physical characteristics of human motion. Motion dynamics reflects the forces between joints , explicitly encoding the skeleton topology, whereas rarely exploited in motion prediction. In this paper, we propose the K inematic and D ynamic coupled trans Former (KD-Former), which incorporates dynamics with kinematics, to learn powerful features for high-fidelity motion prediction. Specifically, We first formulate a reduced-order dynamic model of human body to calculate the forces of all joints . Then we construct a non-autoregressive encoder-decoder framework based on the transformer structure. The encoder involves a kinematic encoder and a dynamic encoder, which are respectively responsible for extracting the kinematic and dynamic features for given history sequences via a spatial transformer and a temporal transformer. Future query sequences are decoded in parallel in the decoder by leveraging the encoded kinematic and dynamic information of history sequences. Experiments on Human3.6M and CMU MoCap benchmarks verify the effectiveness and superiority of our method. Code will be available at: https://github.com/wslh852/KD-Former.git .},
  archive      = {J_PR},
  author       = {Ju Dai and Hao Li and Rui Zeng and Junxuan Bai and Feng Zhou and Junjun Pan},
  doi          = {10.1016/j.patcog.2023.109806},
  journal      = {Pattern Recognition},
  pages        = {109806},
  shortjournal = {Pattern Recognition},
  title        = {KD-former: Kinematic and dynamic coupled transformer network for 3D human motion prediction},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast deep autoencoder for federated learning. <em>PR</em>,
<em>143</em>, 109805. (<a
href="https://doi.org/10.1016/j.patcog.2023.109805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel, fast and privacy preserving implementation of deep autoencoders. DAEF (Deep AutoEncoder for Federated learning), unlike traditional neural networks , trains a deep autoencoder network in a non-iterative way, which drastically reduces training time. Training can be performed incrementally, in parallel and distributed and, thanks to its mathematical formulation, the information to be exchanged does not endanger the privacy of the training data. The method has been evaluated and compared with other state-of-the-art autoencoders, showing interesting results in terms of accuracy, speed and use of available resources. This makes DAEF a valid method for edge computing and federated learning, in addition to other classic machine learning scenarios.},
  archive      = {J_PR},
  author       = {David Novoa-Paradela and Oscar Fontenla-Romero and Bertha Guijarro-Berdiñas},
  doi          = {10.1016/j.patcog.2023.109805},
  journal      = {Pattern Recognition},
  pages        = {109805},
  shortjournal = {Pattern Recognition},
  title        = {Fast deep autoencoder for federated learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained DTW preserving shapelets for explainable
time-series clustering. <em>PR</em>, <em>143</em>, 109804. (<a
href="https://doi.org/10.1016/j.patcog.2023.109804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of time series is becoming ever more popular due to the proliferation of sensors. A well-known similarity measure for time-series is Dynamic Time Warping (DTW), which does not respect the axioms of a metric. These, however, can be reintroduced through Learning DTW-Preserving Shapelets (LDPS). This article extends LDPS and presents constrained DTW-preserving shapelets (CDPS). CDPS directs the time-series representation to captures the user’s interpretation of the data by considering a limited amount of user knowledge in the from of must-link- cannot link constraints . Subsequently, unconstrained algorithms can be used to generate a clustering that respects the constraints without explicit knowledge of them. Out-of-sample data can be transformed into this space, overcoming the limitations of traditional transductive constrained-clustering algorithms. Furthermore, several Shapelet Cluster Explanation (SCE) approaches are proposed that explain the clustering and can simplify the representation while preserving clustering performance. State-of-the-art performance is demonstrated on multiple time-series datasets and an open-source implementation will be made publicly available upon acceptance.},
  archive      = {J_PR},
  author       = {Hussein El Amouri and Thomas Lampert and Pierre Gançarski and Clément Mallet},
  doi          = {10.1016/j.patcog.2023.109804},
  journal      = {Pattern Recognition},
  pages        = {109804},
  shortjournal = {Pattern Recognition},
  title        = {Constrained DTW preserving shapelets for explainable time-series clustering},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimality in high-dimensional tensor discriminant analysis.
<em>PR</em>, <em>143</em>, 109803. (<a
href="https://doi.org/10.1016/j.patcog.2023.109803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor discriminant analysis is an important topic in tensor data analysis. However, given the many proposals for tensor discriminant analysis methods, there lacks a systematic theoretical study, especially concerning optimality. We fill this gap by providing the minimax lower bounds for the estimation and prediction errors under the tensor discriminant analysis model coupled with the sparsity assumption. We further show that one existing high-dimensional tensor discriminant analysis estimator has matching upper bounds, and is thus optimal. Our results apply to tensors with arbitrary orders and ultra-high dimensions. If one focuses on one-way tensors (i.e., vectors), our results further provide strong theoretical justifications for several popular sparse linear discriminant analysis methods. Numerical studies are also presented to support our theoretical results.},
  archive      = {J_PR},
  author       = {Keqian Min and Qing Mai and Junge Li},
  doi          = {10.1016/j.patcog.2023.109803},
  journal      = {Pattern Recognition},
  pages        = {109803},
  shortjournal = {Pattern Recognition},
  title        = {Optimality in high-dimensional tensor discriminant analysis},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RADAM: Texture recognition through randomized aggregated
encoding of deep activation maps. <em>PR</em>, <em>143</em>, 109802. (<a
href="https://doi.org/10.1016/j.patcog.2023.109802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture analysis is a classical yet challenging task in computer vision for which deep neural networks are actively being applied. Most approaches are based on building feature aggregation modules around a pre-trained backbone and then fine-tuning the new architecture on specific texture recognition tasks. Here we propose a new method named R andom encoding of A ggregated D eep A ctivation M aps (RADAM) which extracts rich texture representations without ever changing the backbone. The technique consists of encoding the output at different depths of a pre-trained deep convolutional network using a Randomized Autoencoder (RAE). The RAE is trained locally to each image using a closed-form solution, and its decoder weights are used to compose a 1-dimensional texture representation that is fed into a linear SVM . This means that no fine-tuning or backpropagation is needed for the backbone. We explore RADAM on several texture benchmarks and achieve state-of-the-art results with different computational budgets. Our results suggest that pre-trained backbones may not require additional fine-tuning for texture recognition if their learned representations are better encoded.},
  archive      = {J_PR},
  author       = {Leonardo Scabini and Kallil M. Zielinski and Lucas C. Ribas and Wesley N. Gonçalves and Bernard De Baets and Odemir M. Bruno},
  doi          = {10.1016/j.patcog.2023.109802},
  journal      = {Pattern Recognition},
  pages        = {109802},
  shortjournal = {Pattern Recognition},
  title        = {RADAM: Texture recognition through randomized aggregated encoding of deep activation maps},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Construction of a feature enhancement network for small
object detection. <em>PR</em>, <em>143</em>, 109801. (<a
href="https://doi.org/10.1016/j.patcog.2023.109801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited by the size, location, number of samples and other factors of the small object itself, the small object is usually insufficient, which degrades the performance of the small object detection algorithms . To address this issue, we construct a novel Feature Enhancement Network (FENet) to improve the performance of small object detection. Firstly, an improved data augmentation method based on collision detection and spatial context extension (CDCI) is proposed to effectively expand the possibility of small object detection. Then, based on the idea of Granular Computing , a multi-granular deformable convolution network is constructed to acquire the offset feature representation at the different granularity levels. Finally, we design a high-resolution block (HR block) and build High-Resolution Block-based Feature Pyramid by parallel embedding HR block in FPN (HR-FPN) to make full use different granularity and resolution features. By above strategies, FENet can acquire sufficient feature information of small objects. In this paper, we firstly applied the multi-granularity deformable convolution to feature extraction of small objects. Meanwhile, a new feature fusion module is constructed by optimizing feature pyramid to maintain the detailed features and enrich the semantic information of small objects. Experiments show that FENet achieves excellent performance compared with performance of other methods when applied to the publicly available COCO dataset, VisDrone dataset and TinyPerson dataset. The code is available at https://github.com/cowarder/FENet .},
  archive      = {J_PR},
  author       = {Hongyun Zhang and Miao Li and Duoqian Miao and Witold Pedrycz and Zhaoguo Wang and Minghui Jiang},
  doi          = {10.1016/j.patcog.2023.109801},
  journal      = {Pattern Recognition},
  pages        = {109801},
  shortjournal = {Pattern Recognition},
  title        = {Construction of a feature enhancement network for small object detection},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MKConv: Multidimensional feature representation for point
cloud analysis. <em>PR</em>, <em>143</em>, 109800. (<a
href="https://doi.org/10.1016/j.patcog.2023.109800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the remarkable success of deep learning , an optimal convolution operation on point clouds remains elusive owing to their irregular data structure . Existing methods mainly focus on designing an effective continuous kernel function that can handle an arbitrary point in continuous space. Various approaches exhibiting high performance have been proposed, but we observe that the standard pointwise feature is represented by 1D channels and can become more informative when its representation involves additional spatial feature dimensions. In this paper, we present Multidimensional Kernel Convolution (MKConv), a novel convolution operator that learns to transform the point feature representation from a vector to a multidimensional matrix. Unlike standard point convolution, MKConv proceeds via two steps. (i) It first activates the spatial dimensions of local feature representation by exploiting multidimensional kernel weights. These spatially expanded features can represent their embedded information through spatial correlation as well as channel correlation in feature space, carrying more detailed local structure information. (ii) Then, discrete convolutions are applied to the multidimensional features which can be regarded as a grid-structured matrix. In this way, we can utilize the discrete convolutions for point cloud data without voxelization that suffers from information loss. Furthermore, we propose a spatial attention module, Multidimensional Local Attention (MLA), to provide comprehensive structure awareness within the local point set by reweighting the spatial feature dimensions. We demonstrate that MKConv has excellent applicability to point cloud processing tasks including object classification, object part segmentation, and scene semantic segmentation with superior results.},
  archive      = {J_PR},
  author       = {Sungmin Woo and Dogyoon Lee and Sangwon Hwang and Woo Jin Kim and Sangyoun Lee},
  doi          = {10.1016/j.patcog.2023.109800},
  journal      = {Pattern Recognition},
  pages        = {109800},
  shortjournal = {Pattern Recognition},
  title        = {MKConv: Multidimensional feature representation for point cloud analysis},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph classification via discriminative edge feature
learning. <em>PR</em>, <em>143</em>, 109799. (<a
href="https://doi.org/10.1016/j.patcog.2023.109799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral graph convolutional neural networks (GCNNs) have been producing encouraging results in graph classification tasks . However, most spectral GCNNs utilize fixed graphs when aggregating node features while omitting edge feature learning and failing to get an optimal graph structure. Moreover, many existing graph datasets do not provide initialized edge features, further restraining the ability of learning edge features via spectral GCNNs. In this paper, we try to address this issue by designing an edge feature scheme and an add-on layer between every two stacked graph convolution layers in spectral GCNN. Both are lightweight while effective in filling the gap between edge feature learning and performance enhancement of graph classification. The edge feature scheme makes edge features adapt to node representations at different spectral graph convolution layers. The add-on layer helps adjust the edge features to an optimal graph structure. To test the effectiveness of our method, we take Euclidean positions as initial node features and extract graphs with semantic information from point cloud objects. The node features of our extracted graphs are more scalable for edge feature learning than most existing graph datasets (in one-hot encoded label format). Three new graph datasets are constructed based on ModelNet40, ModelNet10 and ShapeNet Part datasets. Experimental results show that our method outperforms state-of-the-art graph classification methods on the new datasets. Our code and the constructed graph datasets will be released to the community.},
  archive      = {J_PR},
  author       = {Yang Yi and Xuequan Lu and Shang Gao and Antonio Robles-Kelly and Yuejie Zhang},
  doi          = {10.1016/j.patcog.2023.109799},
  journal      = {Pattern Recognition},
  pages        = {109799},
  shortjournal = {Pattern Recognition},
  title        = {Graph classification via discriminative edge feature learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring transformers for behavioural biometrics: A case
study in gait recognition. <em>PR</em>, <em>143</em>, 109798. (<a
href="https://doi.org/10.1016/j.patcog.2023.109798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics on mobile devices has attracted a lot of attention in recent years as it is considered a user-friendly authentication method. This interest has also been motivated by the success of Deep Learning (DL). Architectures based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have established convenience for the task, improving the performance and robustness in comparison to traditional machine learning techniques. However, some aspects must still be revisited and improved. To the best of our knowledge, this is the first article that explores and proposes a novel gait biometric recognition systems based on Transformers, which currently obtain state-of-the-art performance in many applications. Several state-of-the-art architectures (Vanilla, Informer, Autoformer, Block-Recurrent Transformer, and THAT) are considered in the experimental framework. In addition, new Transformer configurations are proposed to further increase the performance. Experiments are carried out using the two popular public databases: whuGAIT and OU-ISIR. The results achieved prove the high ability of the proposed Transformer, outperforming state-of-the-art CNN and RNN architectures.},
  archive      = {J_PR},
  author       = {Paula Delgado-Santos and Ruben Tolosana and Richard Guest and Farzin Deravi and Ruben Vera-Rodriguez},
  doi          = {10.1016/j.patcog.2023.109798},
  journal      = {Pattern Recognition},
  pages        = {109798},
  shortjournal = {Pattern Recognition},
  title        = {Exploring transformers for behavioural biometrics: A case study in gait recognition},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One shot learning with class partitioning and cross
validation voting (CP-CVV). <em>PR</em>, <em>143</em>, 109797. (<a
href="https://doi.org/10.1016/j.patcog.2023.109797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One Shot Learning includes all those techniques that make it possible to classify images using a single image per category. One of its possible applications is the identification of food products. For a grocery store, it is interesting to record a single image of each product and be able to recognise it again from other images, such as photos taken by customers. Within deep learning , Siamese neural networks are able to verify whether two images belong to the same category or not. In this paper, a new Siamese network training technique, called CP-CVV, is presented. It uses the combination of different models trained with different classes. The separation of validation classes has been done in such a way that each of the combined models is different in order to avoid overfitting with respect to the validation. Unlike normal training, the test images belong to classes that have not previously been used in training, allowing the model to work on new categories, of which only one image exists. Different backbones have been evaluated in the Siamese composition, but also the integration of multiple models with different backbones. The results show that the model improves on previous works and allows the classification problem to be solved, an additional step towards the use of Siamese networks. To the best of our knowledge, there is no existing work that has proposed integrating Siamese neural networks using a class-based validation set separation technique so as to be better at generalising for unknown classes. Additionally, we have applied Cross-Validation-Voting with ConvNeXt to improve the existing classification results of a well-known Grocery Store Dataset.},
  archive      = {J_PR},
  author       = {Jaime Duque-Domingo and Roberto Medina Aparicio and Luis Miguel González Rodrigo},
  doi          = {10.1016/j.patcog.2023.109797},
  journal      = {Pattern Recognition},
  pages        = {109797},
  shortjournal = {Pattern Recognition},
  title        = {One shot learning with class partitioning and cross validation voting (CP-CVV)},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). APUNet: Attention-guided upsampling network for sparse and
non-uniform point cloud. <em>PR</em>, <em>143</em>, 109796. (<a
href="https://doi.org/10.1016/j.patcog.2023.109796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud upsampling is a basic low-level task, that is important for improving the quality of a point cloud. However, existing point cloud upsampling methods perform poorly on sparse and non-uniform point clouds, due to that they fail to fully model the relationship between points. To address this issue, in this paper, we propose an attention-guided network called APUNet to exploit the correlation between points, which can perform unsampling for sparse and non-uniform point cloud. In particular, we first propose a feature extraction unit, DisTransformer, which can effectively model the relationship between points by introducing a distance prior to the attention mechanism . We also design a point cloud feature extraction network based on DisTransformer. By computing the correlation between patches and the correlation between points, we fuse the global and local features to better model the correlation of the whole object. Furthermore, we propose a feature prediction module based on attention mechanisms that avoids generating clustered points by transforming the point cloud expansion task into a point cloud prediction task. Qualitative and quantitative experiments reveal the superiority of our method compared to the current state-of-the-art methods. Compared with other point cloud upsampling methods, APUNet can much better upsample non-uniform and extremely sparse point clouds.},
  archive      = {J_PR},
  author       = {Tianming Zhao and Linfeng Li and Tian Tian and Jiayi Ma and Jinwen Tian},
  doi          = {10.1016/j.patcog.2023.109796},
  journal      = {Pattern Recognition},
  pages        = {109796},
  shortjournal = {Pattern Recognition},
  title        = {APUNet: Attention-guided upsampling network for sparse and non-uniform point cloud},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Hyperspectral anomaly detection based on variational
background inference and generative adversarial network. <em>PR</em>,
<em>143</em>, 109795. (<a
href="https://doi.org/10.1016/j.patcog.2023.109795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral anomaly detection is aimed at detecting targets with significant spectral differences from their surroundings. Recently, deep generative models have been applied to anomaly detections, while the existing generative adversarial network (GAN)-based methods have difficulty in accurately modeling the background and achieving spectrum reconstruction. In this article, a hyperspectral anomaly detection network based on variational background inference and generative adversarial framework (VBIGAN-AD) is proposed. The proposed VBIGAN model can learn the background distribution characteristics of HSIs and enhance the detection performance by the use of reconstruction errors. Specifically, the VBIGAN framework consists of sample and latent GANs, which establishes the relationship between data samples and latent samples through two sub-networks to capture the data distribution. Furthermore, the variational inference method is introduced and the hyperspectral background distribution can be converged to a multivariate normal distribution. To accurately learn the background distribution characteristics and reconstruct the background spectra, the coupling loss is conducted by enforcing feature match in the two discriminators on the basis of composite loss, and the results show that the additional loss can promote the detection performance. As a result, the reconstruction errors generated by the VBIGAN-AD method is utilized to detect abnormal targets. The experiments conducted on five datasets proved the robustness and applicability of the proposed VBIGAN-AD method.},
  archive      = {J_PR},
  author       = {Zhiwei Wang and Xue Wang and Kun Tan and Bo Han and Jianwei Ding and Zhaoxian Liu},
  doi          = {10.1016/j.patcog.2023.109795},
  journal      = {Pattern Recognition},
  pages        = {109795},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral anomaly detection based on variational background inference and generative adversarial network},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based learning of nonlinear physiological interactions
for classification of emotions. <em>PR</em>, <em>143</em>, 109794. (<a
href="https://doi.org/10.1016/j.patcog.2023.109794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition has been drawing the attention of researchers and practitioners in recent years. While various research studies show successful applications to recognize and distinguish emotions based on physiological responses using machine learning techniques , less research to date is focused on the network properties of physiological interactions under different emotional states. To this end, we propose a multi-modal graph learning framework to quantify the interactions among physiological systems and present representative networks associated with emotional states. More specifically, we introduce a novel information-theoretic-based time delay stability to quantify complex interactions between physiological modalities. We test our quantification approach on three publicly available benchmark databases for emotion recognition and demonstrate the comparative performances of measuring the interactions of physiological systems in response to emotional states. Finally, we present the visualization of multi-modal physiological network topology , which may be useful for emotional interpretations in practice.},
  archive      = {J_PR},
  author       = {Huiyu Huang and Miaolin Fan and Chun-An Chou},
  doi          = {10.1016/j.patcog.2023.109794},
  journal      = {Pattern Recognition},
  pages        = {109794},
  shortjournal = {Pattern Recognition},
  title        = {Graph-based learning of nonlinear physiological interactions for classification of emotions},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FGBC: Flexible graph-based balanced classifier for
class-imbalanced semi-supervised learning. <em>PR</em>, <em>143</em>,
109793. (<a href="https://doi.org/10.1016/j.patcog.2023.109793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) has witnessed resounding success in many standard class-balanced benchmark datasets. However, real-world data often exhibit class-imbalanced distributions, which poses significant challenges for existing SSL algorithms. In general, fully supervised models trained on a class-imbalanced dataset are biased toward the majority classes, and this issue becomes more severe for class-imbalanced semi-supervised learning (CISSL) conditions. To address this issue, we put forward a novel CISSL framework dubbed FGBC by introducing a flexible graph-based balanced classifier with three innovations. Specifically, because the propagation of label information becomes difficult for tail classes, we propose a graph-based classifier head attached to the representation layer of the existing SSL framework for efficient pseudo-label propagation. Then, by considering that the learning status of different classes in CISSL may vary, we introduce a flexible threshold adjustment in pseudo-labeling to further select balanced samples to participate in training. Furthermore, to alleviate the risk of overfitting tail classes, we devised a class-aware feature MixUp (CFM) augmentation algorithm, which can further enhance the features of each class by considering their class sizes. Experimental results demonstrate that FGBC achieves state-of-the-art performance on datasets from CIFAR-10/100, SVHN and Small ImageNet-127 under various levels of CISSL conditions.},
  archive      = {J_PR},
  author       = {Xiangyuan Kong and Xiang Wei and Xiaoyu Liu and Jingjie Wang and Weiwei Xing and Wei Lu},
  doi          = {10.1016/j.patcog.2023.109793},
  journal      = {Pattern Recognition},
  pages        = {109793},
  shortjournal = {Pattern Recognition},
  title        = {FGBC: Flexible graph-based balanced classifier for class-imbalanced semi-supervised learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adapt-infomap: Face clustering with adaptive graph
refinement in infomap. <em>PR</em>, <em>143</em>, 109792. (<a
href="https://doi.org/10.1016/j.patcog.2023.109792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face clustering is a critical task in computer vision due to the increasing number of applications such as augmented reality or photo album management. The primary challenge in this task arises from the imperfections in image feature representations. Given image features extracted from an existing pre-trained representation model, it remains an unresolved problem that how to leverage the inherent characteristics of similarities among unlabelled images to improve the clustering performance. In order to solve face clustering in an unsupervised manner , we develop an effective and robust framework named as Adapt-Infomap. First, we reformulate face clustering as a process of non-overlapping community detection. Specially, Adapt-Infomap achieves face clustering by minimizing the entropy of information flows (also known as the map equation) on an affinity graph of images. Since the affinity graph of images might contain noisy edges, we develop an outlier detection strategy in Adapt-Infomap to adaptively refine the affinity graph. Experiments with ablation studies demonstrate that Adapt-Infomap significantly outperforms existing methods and achieves new state-of-the-arts on three popular large-scale datasets for face clustering, e.g. , an absolute improvement of more than 10\% 10\% and 3\% 3\% comparing with prior unsupervised and supervised methods respectively in terms of average of Pairwise F-score.},
  archive      = {J_PR},
  author       = {Xiaotian Yu and Yifan Yang and Aibo Wang and Ling Xing and Haokui Zhang and Hanling Yi and Guangming Lu and Xiaoyu Wang},
  doi          = {10.1016/j.patcog.2023.109792},
  journal      = {Pattern Recognition},
  pages        = {109792},
  shortjournal = {Pattern Recognition},
  title        = {Adapt-infomap: Face clustering with adaptive graph refinement in infomap},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face age synthesis: A review on datasets, methods, and open
research areas. <em>PR</em>, <em>143</em>, 109791. (<a
href="https://doi.org/10.1016/j.patcog.2023.109791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face age synthesis is the determination of how a person looks in the future or the past by reconstructing their facial image. Determining the change in the human face over the years is a critical process for cross-age face recognition systems in forensic issues such as finding missing people and fugitive criminals. Therefore, it is a subject that has attracted attention in recent years. With the implementation of deep learning methods, better quality and photo-realistic images began to be produced. However, researchers continue to improve both aging accuracy and identity preservation requirements. We group the studies in the literature under two categories: classical methods and deep learning methods. We review both categories in the methods used, evaluation methods, and databases.},
  archive      = {J_PR},
  author       = {Ayşe Kale and Oğuz Altun},
  doi          = {10.1016/j.patcog.2023.109791},
  journal      = {Pattern Recognition},
  pages        = {109791},
  shortjournal = {Pattern Recognition},
  title        = {Face age synthesis: A review on datasets, methods, and open research areas},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge transfer evolutionary search for lightweight
neural architecture with dynamic inference. <em>PR</em>, <em>143</em>,
109790. (<a href="https://doi.org/10.1016/j.patcog.2023.109790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relying on the availability of massive labeled samples, most neural architecture search (NAS) methods focus on searching large and complex models; and adopt fixed structures and parameters at the inference stage. Few approaches automatically design lightweight networks for label-limited tasks and further consider the inference differences between inputs. To address these issues, we introduce evolutionary computation (EC) and attention mechanism and propose a knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference, then verify it using synthetic aperture radar (SAR) images. SAR image classification is a typical label-limited task due to the inherent imaging mechanism of SAR. We design the EC-based architecture search and attention-based dynamic inference for SAR image scene classification. Specifically, we build a SAR-tailored search space, explore topology pruning-based mutation operators to search lightweight architectures, and further design a dynamic Ridgelet convolution capable of adaptive reasoning to enhance the representation ability of searched lightweight networks. Moreover, we propose a knowledge transfer training strategy and hybrid evaluation criteria to ensure searching quickly and robustly. Experimental results show that the proposed method can search for superior neural architectures, thus improving the classification performance of SAR images.},
  archive      = {J_PR},
  author       = {Xiaoxue Qian and Fang Liu and Licheng Jiao and Xiangrong Zhang and Xinyan Huang and Shuo Li and Puhua Chen and Xu Liu},
  doi          = {10.1016/j.patcog.2023.109790},
  journal      = {Pattern Recognition},
  pages        = {109790},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge transfer evolutionary search for lightweight neural architecture with dynamic inference},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AGMN: Association graph-based graph matching network for
coronary artery semantic labeling on invasive coronary angiograms.
<em>PR</em>, <em>143</em>, 109789. (<a
href="https://doi.org/10.1016/j.patcog.2023.109789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic labeling of coronary arterial segments in invasive coronary angiography (ICA) is important for automated assessment and report generation of coronary artery stenosis in computer-aided coronary artery disease (CAD) diagnosis. However, separating and identifying individual coronary arterial segments is challenging because morphological similarities of different branches on the coronary arterial tree and human-to-human variabilities exist. Inspired by the training procedure of interventional cardiologists for interpreting the structure of coronary arteries, we propose an association graph-based graph matching network (AGMN) for coronary arterial semantic labeling. We first extract the vascular tree from invasive coronary angiography (ICA) and convert it into multiple individual graphs. Then, an association graph is constructed from two individual graphs where each vertex represents the relationship between two arterial segments. Thus, we convert the arterial segment labeling task into a vertex classification task; ultimately, the semantic artery labeling becomes equivalent to identifying the artery-to-artery correspondence on graphs. More specifically, the AGMN extracts the vertex features by the embedding module using the association graph , aggregates the features from adjacent vertices and edges by graph convolution network, and decodes the features to generate the semantic mappings between arteries. By learning the mapping of arterial branches between two individual graphs, the unlabeled arterial segments are classified by the labeled segments to achieve semantic labeling. A dataset containing 263 ICAs was employed to train and validate the proposed model, and a five-fold cross-validation scheme was performed. Our AGMN model achieved an average accuracy of 0.8264, an average precision of 0.8276, an average recall of 0.8264, and an average F1-score of 0.8262, which significantly outperformed existing coronary artery semantic labeling methods. In conclusion, we have developed and validated a new algorithm with high accuracy, interpretability, and robustness for coronary artery semantic labeling on ICAs.},
  archive      = {J_PR},
  author       = {Chen Zhao and Zhihui Xu and Jingfeng Jiang and Michele Esposito and Drew Pienta and Guang-Uei Hung and Weihua Zhou},
  doi          = {10.1016/j.patcog.2023.109789},
  journal      = {Pattern Recognition},
  pages        = {109789},
  shortjournal = {Pattern Recognition},
  title        = {AGMN: Association graph-based graph matching network for coronary artery semantic labeling on invasive coronary angiograms},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrared small target segmentation networks: A survey.
<em>PR</em>, <em>143</em>, 109788. (<a
href="https://doi.org/10.1016/j.patcog.2023.109788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks , 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.},
  archive      = {J_PR},
  author       = {Renke Kou and Chunping Wang and Zhenming Peng and Zhihe Zhao and Yaohong Chen and Jinhui Han and Fuyu Huang and Ying Yu and Qiang Fu},
  doi          = {10.1016/j.patcog.2023.109788},
  journal      = {Pattern Recognition},
  pages        = {109788},
  shortjournal = {Pattern Recognition},
  title        = {Infrared small target segmentation networks: A survey},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional independence induced unsupervised domain
adaptation. <em>PR</em>, <em>143</em>, 109787. (<a
href="https://doi.org/10.1016/j.patcog.2023.109787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning domain-adaptive features is important to tackle the dataset bias problem, where data distributions in the labeled source domain and the unlabeled target domain can be different. The critical issue is to identify and then reduce the redundant information including class-irrelevant and domain-specific features. In this paper, a conditional independence induced unsupervised domain adaptation (CIDA) method is proposed to tackle the challenges. It aims to find the low-dimensional and transferable feature representation of each observation, namely the latent variable in the domain-adaptive subspace. Technically, two mutual information terms are optimized at the same time. One is the mutual information between the latent variable and the class label, and the other is the mutual information between the latent variable and the domain label. Note that the key module can be approximately reformulated as a conditional independence/dependence based optimization problem , and thus, it has a probabilistic interpretation with the Gaussian process. Temporary labels of the target samples and the model parameters are alternatively optimized. The objective function can be incorporated with deep network architectures, and the algorithm is implemented iteratively in an end-to-end manner. Extensive experiments are conducted on several benchmark datasets, and the results show effectiveness of CIDA.},
  archive      = {J_PR},
  author       = {Xiao-Lin Xu and Geng-Xin Xu and Chuan-Xian Ren and Dao-Qing Dai and Hong Yan},
  doi          = {10.1016/j.patcog.2023.109787},
  journal      = {Pattern Recognition},
  pages        = {109787},
  shortjournal = {Pattern Recognition},
  title        = {Conditional independence induced unsupervised domain adaptation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved polar complex exponential transform for robust
local image description. <em>PR</em>, <em>143</em>, 109786. (<a
href="https://doi.org/10.1016/j.patcog.2023.109786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image description via robust local descriptors plays a vital role in a large number of image representation and matching applications. In this paper, we propose a novel distinctive local image descriptor that is based on the phase and amplitude information of Polar Complex Exponential Transform (PCET). The proposed descriptor, called IPCET (Improved PCET), is robust to the common photometric transformations (e.g., illumination, noise, JPEG compression, and blur) and geometric transformations (e.g., scaling, rotation, translation, and significant affine distortion). We perform extensive experiments to compare our IPCET descriptor with six most cutting-edge region descriptors (i.e., SIFT , Zernike Moment, GLOH, PCA-SIFT, SURF, and ORB). Experimental results demonstrate that our IPCET descriptor outperforms cutting-edge moment-based descriptors.},
  archive      = {J_PR},
  author       = {Zhanlong Yang and Linzhi Yang and Geng Chen and Pew-Thian Yap},
  doi          = {10.1016/j.patcog.2023.109786},
  journal      = {Pattern Recognition},
  pages        = {109786},
  shortjournal = {Pattern Recognition},
  title        = {Improved polar complex exponential transform for robust local image description},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSINet: Mining scale information from digital surface models
for semantic segmentation of aerial images. <em>PR</em>, <em>143</em>,
109785. (<a href="https://doi.org/10.1016/j.patcog.2023.109785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with other kinds of images, aerial images have more obvious object scale distinction and larger resolution, which results in that the whole scale information of aerial images can hardly be explored. To address this difficulty, we develop a novel network based on the digital surface models (DSMs) of aerial images in this paper. The proposed network termed MSINet can efficiently mine scale information through the DSMs from two aspects. Firstly, we propose an interpolation pyramid algorithm to encode the scale information from the DSMs and hence provide a scale prior information to the normal segmentation network . The interpolation pyramid algorithm implements interpolation operations with different scales on the DSMs and detects the pixel value change after the interpolation operations. Objects with different scales will express diverse changes, which provides useful information to encode their scale information. Secondly, aiming to address the problem that the DSMs contain a large amount of noise in the boundary part, a spatial information enhancement module and a mutual-guidance module are developed in this paper. These two modules can fix the misleading guidance information caused by the noise in the boundary part of the DSMs and hence achieve more accurate scale information inserting. The extensive experimental results prove that our methods can outperform other competitors in terms of qualitative and quantitative performance.},
  archive      = {J_PR},
  author       = {Chengli Peng and Haifeng Li and Chao Tao and Yansheng Li and Jiayi Ma},
  doi          = {10.1016/j.patcog.2023.109785},
  journal      = {Pattern Recognition},
  pages        = {109785},
  shortjournal = {Pattern Recognition},
  title        = {MSINet: Mining scale information from digital surface models for semantic segmentation of aerial images},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient EM algorithm for two-layer mixture model of
gaussian process functional regressions. <em>PR</em>, <em>143</em>,
109783. (<a href="https://doi.org/10.1016/j.patcog.2023.109783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixture of Gaussian processes is effective for regression, but it cannot handle the non-stationary curve clustering problem well. The two-layer mixture of Gaussian process functional regressions (TMGPFR) model was established to deal with this problem. In this paper, we first propose the classification EM (CEM) algorithm to solve that the optimization algorithm is inefficient for TMGPFRs, and then propose the deterministic annealing CEM algorithm for TMGPFRs to overcome the local maximum problem of the CEM algorithm. Lastly, experiments are conducted on synthetic and real-world data sets, and the results show that our proposed algorithms are more effective than the compared algorithms on curve clustering and regression.},
  archive      = {J_PR},
  author       = {Di Wu and Yurong Xie and Zhe Qiang},
  doi          = {10.1016/j.patcog.2023.109783},
  journal      = {Pattern Recognition},
  pages        = {109783},
  shortjournal = {Pattern Recognition},
  title        = {An efficient EM algorithm for two-layer mixture model of gaussian process functional regressions},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An encoded histogram of ridge bifurcations and contours for
fingerprint presentation attack detection. <em>PR</em>, <em>143</em>,
109782. (<a href="https://doi.org/10.1016/j.patcog.2023.109782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the exponential growth of internet technologies has made personal authentication an integral part of security applications. The fingerprint-based biometric systems are essentially used to safeguard the users&#39; privacy and confidentiality. However, such systems are prone to spoof attacks by artificial replicas of the fingerprints. This paper presents an improved feature extractor called BiRi-PAD (Encoded Histogram of Ridge Bifurcations and Contours for fingerprint Presentation Attack Detection) that aims to enhance the accuracy of live fingerprint detection. The feature extraction process of the proposed BiRi-PAD consists of four steps. First, the fingerprint image undergoes a process of extracting 2-channel ridge contour maps (2-RC maps). The first channel of 2-RC maps consists of ridge contours extracted by a set of derivative filters in the spatial domain whereas the second channel of 2-RC maps consists of the ridge contours extracted by the maximum moments based on phase congruency in the frequency domain. Further, minutiae-based feature information i.e., ridge bifurcations are extracted by the minimum moments based on phase congruency covariance. Moreover, a fusion equation is proposed to integrate 2-RC maps and bifurcations into a single feature map. Second, an improved Comprehensive Local Phase Quantization (CLPQ) based on the well-known feature descriptor Rotation Invariant Local Phase Quantization ( L P Q r i LPQri ) is proposed to extract the phase information of ridges. CLPQ extracts the orientation of the ridges by using the complex parts of the significant frequency components of L P Q r i LPQri and monogenic filters. Third, the proposed BiRi-PAD quantizes the 2-RC maps into pre-determined intervals. Finally, both 2-RC maps and CLPQ features are integrated to generate a feature vector of a single fingerprint image . Performance evaluations of BiRi-PAD are conducted on three publicly available benchmarks from the LivDet competition, namely LivDet 2013, 2011, and 2015. Experimental evaluations demonstrate that the proposed BiRi-PAD achieves significant reductions in average rates compared to state-of-the-art techniques of fingerprint liveness detection. Specifically, on LivDet 2013, LivDet 2011, and LivDet 2015, the average rates are reduced to 1.92\%, 4.39\%, and 4.55\%, respectively.},
  archive      = {J_PR},
  author       = {Rubab Mehboob and Hassan Dawood and Hussain Dawood},
  doi          = {10.1016/j.patcog.2023.109782},
  journal      = {Pattern Recognition},
  pages        = {109782},
  shortjournal = {Pattern Recognition},
  title        = {An encoded histogram of ridge bifurcations and contours for fingerprint presentation attack detection},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional pseudo-supervised contrast for data-free
knowledge distillation. <em>PR</em>, <em>143</em>, 109781. (<a
href="https://doi.org/10.1016/j.patcog.2023.109781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-free knowledge distillation (DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation (CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git},
  archive      = {J_PR},
  author       = {Renrong Shao and Wei Zhang and Jun Wang},
  doi          = {10.1016/j.patcog.2023.109781},
  journal      = {Pattern Recognition},
  pages        = {109781},
  shortjournal = {Pattern Recognition},
  title        = {Conditional pseudo-supervised contrast for data-free knowledge distillation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-free quantization via mixed-precision compensation
without fine-tuning. <em>PR</em>, <em>143</em>, 109780. (<a
href="https://doi.org/10.1016/j.patcog.2023.109780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network quantization is a very promising solution in the field of model compression , but its resulting accuracy highly depends on a training/fine-tuning process and requires the original data. This not only brings heavy computation and time costs but also is not conducive to privacy and sensitive information protection. Therefore, a few recent works are starting to focus on data-free quantization. However, data-free quantization does not perform well while dealing with ultra-low precision quantization. Although researchers utilize generative methods of synthetic data to address this problem partially, data synthesis needs to take a lot of computation and time. In this paper, we propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process. By assuming the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, we mathematically formulate the reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model. Based on our formulation, we theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps. Since DF-MPC does not require any original/synthetic data, it is a more efficient method to approximate the full-precision model. Experimentally, our DF-MPC is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.},
  archive      = {J_PR},
  author       = {Jun Chen and Shipeng Bai and Tianxin Huang and Mengmeng Wang and Guanzhong Tian and Yong Liu},
  doi          = {10.1016/j.patcog.2023.109780},
  journal      = {Pattern Recognition},
  pages        = {109780},
  shortjournal = {Pattern Recognition},
  title        = {Data-free quantization via mixed-precision compensation without fine-tuning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-proxy feature learning for robust fine-grained visual
recognition. <em>PR</em>, <em>143</em>, 109779. (<a
href="https://doi.org/10.1016/j.patcog.2023.109779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual representation for fine-grained visual recognition can be learned by mandatorily enforcing all samples of the same category into a uniform representation. This strict training objective performs well under closed-set setting but is not applicable to data in the wild containing noisy annotations and long-tailed distributions, e.g., it may lead to a feature space biased to head categories. This paper tackles this challenge by pursuing a more balanced and discriminative feature space by first retaining intra-class variances to isolate noises, then eliminating intra-class variances to improve the visual recognition performance. We propose the Compact Memory Updater to maintain a memory bank, which memorizes proxy features to represent multiple typical appearances of each category in the training set. The Proxy-based Feature Enhancement hence leverages proxy features to ensure samples of the same category have similar features. Iteratively running those two modules boosts the robustness and discriminative power of the learnt representation, hence facilitates various fine-grained visual recognition tasks including person re-identification (re-id), image classification and retrieval. Extensive experiments on noisy and long-tailed training sets show this Multi-Proxy Feature Learning (MPFL) framework achieves promising performance. For instance on a training set with 90\% one-shot categories, MPFL outperforms the recent long-tailed person re-id method LEAP-AF by 16.9\% in rank-1 accuracy.},
  archive      = {J_PR},
  author       = {Shunan Mao and Yaowei Wang and Xiaoyu Wang and Shiliang Zhang},
  doi          = {10.1016/j.patcog.2023.109779},
  journal      = {Pattern Recognition},
  pages        = {109779},
  shortjournal = {Pattern Recognition},
  title        = {Multi-proxy feature learning for robust fine-grained visual recognition},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autoencoders for a manifold learning problem with a jacobian
rank constraint. <em>PR</em>, <em>143</em>, 109777. (<a
href="https://doi.org/10.1016/j.patcog.2023.109777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate the manifold learning problem as the problem of finding an operator that maps any point to a close neighbor that lies on a “hidden” k k -dimensional manifold. We call this operator the correcting function. Under this formulation, autoencoders can be viewed as a tool to approximate the correcting function. Given an autoencoder whose Jacobian has rank k k , we deduce from the classical Constant Rank Theorem that its range has a structure of a k k -dimensional manifold. A k k -dimensionality of the range can be forced by the architecture of an autoencoder (by fixing the dimension of the code space), or alternatively, by an additional constraint that the rank of the autoencoder mapping is not greater than k k . This constraint is included in the objective function as a new term, namely a squared Ky-Fan k k -antinorm of the Jacobian function. We claim that this constraint is a factor that effectively reduces the dimension of the range of an autoencoder, additionally to the reduction defined by the architecture. We also add a new curvature term into the objective. To conclude, we experimentally compare our approach with the CAE+H method on synthetic and real-world datasets.},
  archive      = {J_PR},
  author       = {Rustem Takhanov and Y. Sultan Abylkairov and Maxat Tezekbayev},
  doi          = {10.1016/j.patcog.2023.109777},
  journal      = {Pattern Recognition},
  pages        = {109777},
  shortjournal = {Pattern Recognition},
  title        = {Autoencoders for a manifold learning problem with a jacobian rank constraint},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of tumor in one single ultrasound image via a
novel multi-view learning strategy. <em>PR</em>, <em>143</em>, 109776.
(<a href="https://doi.org/10.1016/j.patcog.2023.109776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis (CAD) technology has been widely used in the early diagnosis of breast cancer. Nowadays, most of the existing breast ultrasound classification methods need to crop a tumor-centered image (TCI) on each image as the input of the system. These methods ignore the fact that the tumor as well as its surrounding tissues can actually be viewed from multiple aspects, and it is difficult to extract multi-resolution information applying only a single view image. In addition, the current methods do not effectively extract fine-grained features, and subtle details play an important role in breast classification. In our research, we propose a novel strategy to generate multi-resolution TCIs in a single ultrasound image , resulting in a multi-data-input learning task. Hence, a conventional single image based learning task is converted into a multi-view learning task, and an improved combined style fusion method suitable for a deep network is proposed, which integrates the advantage of the decision-based and feature-based methods to fuse the information of different views. At the same time, we first attempt to introduce the fine-grained classification method into breast classifications and capture the pairwise correlation between feature channels at each position to extract subtle information. The comparative experimental results show that our method can effectively improve the classification performance and achieves the best results in five metrics.},
  archive      = {J_PR},
  author       = {Yaozhong Luo and Qinghua Huang and Longzhong Liu},
  doi          = {10.1016/j.patcog.2023.109776},
  journal      = {Pattern Recognition},
  pages        = {109776},
  shortjournal = {Pattern Recognition},
  title        = {Classification of tumor in one single ultrasound image via a novel multi-view learning strategy},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning pixel-adaptive weights for portrait photo
retouching. <em>PR</em>, <em>143</em>, 109775. (<a
href="https://doi.org/10.1016/j.patcog.2023.109775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lookup table-based methods achieve promising retouching performance by learning image-adaptive weights to combine 3-dimensional lookup tables (3D LUTs) and conducting pixel-to-pixel color transformation. However, this paradigm ignores the local context cues and applies the same transformation to portrait pixels and background pixels that exhibit the same raw RGB values. In contrast, an expert usually conducts different operations to adjust the color temperatures, tones of portrait regions, and background regions. This inspires us to model local context cues to improve the retouching quality explicitly.Thus, the center pixel of an image patch is first retouched by predicting pixel-adaptive lookup table weights. To modulate the influence of neighboring pixels , as neighboring pixels exhibit different affinities to the center pixel, a local attention mask is estimated. Then, the quality of the local attention mask is further improved by applying supervision, which is based on the affinity map calculated by the ground-truth portrait mask. For group-level consistency, we propose to directly constrain the variance of mean color components in the Lab space. Extensive experiments on the PPR10K dataset demonstrate the effectiveness of the proposed method, the retouching performance on high-resolution photos is improved by over 0.5dB in terms of PSNR, and the group-level inconsistency is reduced by 2.1.},
  archive      = {J_PR},
  author       = {Binglu Wang and Chengzhe Lu and Dawei Yan and Yongqiang Zhao and Ning Li and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.109775},
  journal      = {Pattern Recognition},
  pages        = {109775},
  shortjournal = {Pattern Recognition},
  title        = {Learning pixel-adaptive weights for portrait photo retouching},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). WSDS-GAN: A weak-strong dual supervised learning method for
underwater image enhancement. <em>PR</em>, <em>143</em>, 109774. (<a
href="https://doi.org/10.1016/j.patcog.2023.109774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Image Enhancement (UIE) is a crucial preprocessing step for underwater vision tasks. Addressing the challenge of training supervised deep learning models on large, diverse datasets while learning the intrinsic degradation factors of underwater images is essential for improving model generalization performance . In this paper, we propose a Weak-Strong Dual Supervised Generative Adversarial Network (WSDS-GAN) for UIE. During the first weakly supervised learning phase, unpaired images, consisting of degraded underwater images and clear in-air images, are used to train the model with the goal of recovering color, brightness, and content. In the second strongly supervised learning phase, a limited number of paired images are fed into the model to further train the image detail recovery generator. Comprehensive experiments on public datasets and self-photographed images demonstrate the effectiveness of our proposed method over existing state-of-the-art methods, both qualitatively and quantitatively. Additionally, we show that our method significantly enhances image details to support subsequent underwater vision tasks.},
  archive      = {J_PR},
  author       = {Qiong Liu and Qi Zhang and Wei Liu and Wenbai Chen and Xinwang Liu and Xiangke Wang},
  doi          = {10.1016/j.patcog.2023.109774},
  journal      = {Pattern Recognition},
  pages        = {109774},
  shortjournal = {Pattern Recognition},
  title        = {WSDS-GAN: A weak-strong dual supervised learning method for underwater image enhancement},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crop classification based on multi-temporal PolSAR images
with a single tensor network. <em>PR</em>, <em>143</em>, 109773. (<a
href="https://doi.org/10.1016/j.patcog.2023.109773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable discrimination of crop categories is a significant data source for agricultural monitoring and food security evaluation research. The convolutional neural network (CNN) model is one of the most popular classifiers for crop discrimination based on polarimetric synthetic aperture radar (PolSAR) data. However, it is better to avoid directly using large amounts of raw features that extracted from PolSAR data for CNN models because of the “dimension disaster” problem caused by multiple periods and various feature extraction schemes. Consequently, an extra feature compression model has to be incorporated to mitigate the “dimension disaster” problem. However, ill coupling of the two models may result in degraded classification performance, thus the combination of two models has to be further optimized. In this paper, we propose a novel single tensor affine transformation network (TATN) for crop classification using multi-temporal PolSAR data, where the input sample of the network is a higher order tensor formed by raw features, and the hidden layers of the network adopt the tensor affine transformation rather than convolution to extract discriminative features for classification. Since the tensor affine transformation preserves the structural information of the original input tensor samples, the TATN is expected to achieve a higher crop classification accuracy . Moreover, the TATN holds less amount of parameters than most of deep learning models, which enables to avoid the extra feature compression procedure. The experimental results validate the merits of our model.},
  archive      = {J_PR},
  author       = {Wei-Tao Zhang and Lu Liu and Yv Bai and Yi-Bang Li and Jiao Guo},
  doi          = {10.1016/j.patcog.2023.109773},
  journal      = {Pattern Recognition},
  pages        = {109773},
  shortjournal = {Pattern Recognition},
  title        = {Crop classification based on multi-temporal PolSAR images with a single tensor network},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tri-HGNN: Learning triple policies fused hierarchical graph
neural networks for pedestrian trajectory prediction. <em>PR</em>,
<em>143</em>, 109772. (<a
href="https://doi.org/10.1016/j.patcog.2023.109772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex and dynamic urban traffic scenarios, the accurate trajectory prediction of surrounding pedestrians with interactive behaviors plays a vital role in the self-driving system. Intrinsic factors and extrinsic factors will inevitably influence the pedestrians trajectory. Intrinsic factors such as pedestrians diversified intentions bring rich and diverse multi-modal future possibilities. Besides, extrinsic factors affecting the future trajectory are accompanied by context semantics such as interactions among pedestrians. However, most of the existing methods discuss two problems (interaction and intention) separately. Considering both two factors impact the trajectory of pedestrians, a Triple Policies Fused Hierarchical Graph Neural Networks (Tri-HGNN) is proposed to model spatial and temporal interactions and intentions among the whole scene of pedestrians at each time step and predict the multiple future trajectories. Tri-HGNN contains three different policies: (i) Extrinsic-level policy is used to extract spatial nodes embedding from the interaction graph of pedestrian trajectories by using the Graph Attention Network . (ii) Intrinsic-level policy adopts the Graph Convolutional Network to infer the human intention for more accurate prediction. Moreover, human intention is influenced by the intrinsic interaction generated among pedestrians, so we fuse the interaction features to grasp the influence of the extrinsic interaction. (iii) Basic-level policy then integrates the heuristic information obtained from other two policies and concatenates it with historical trajectories to make multiple predictions through Temporal Convolution Network. Experimental results show that our model improves performance compared with state-of-the-art methods on the ETH/UCY and SDD benchmarks.},
  archive      = {J_PR},
  author       = {Wenjun Zhu and Yanghong Liu and Peng Wang and Mengyi Zhang and Tian Wang and Yang Yi},
  doi          = {10.1016/j.patcog.2023.109772},
  journal      = {Pattern Recognition},
  pages        = {109772},
  shortjournal = {Pattern Recognition},
  title        = {Tri-HGNN: Learning triple policies fused hierarchical graph neural networks for pedestrian trajectory prediction},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain embedding transfer for unequal RGB-d image
recognition. <em>PR</em>, <em>143</em>, 109771. (<a
href="https://doi.org/10.1016/j.patcog.2023.109771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent unsupervised domain adaptation (UDA) approaches concentrate on single RGB source to single RGB target task. They have to face the real-world scenario, where the source domain can be collected from multiple modalities, e.g. , RGB data and depth data. Our work focuses on a more practical and challenging scenario which recognizes RGB images by learning from RGB-D data under the label inequality scenario. We are confronted with three challenges: multiple modalities in the source domain, domain shifting problem and unequal label numbers. To address the aforementioned settings, a novel method, referred to as Domain depth Embedding Transfer (DdET) is proposed, which takes advantage of the depth data in the source domain and handles the domain distribution mismatch under label inequality scenario simultaneously. We conduct comprehensive experiments on five cross domain image classification tasks and observe that DdET can perform favorably against state-of-the-art methods, especially under label inequality scenario.},
  archive      = {J_PR},
  author       = {Ziyun Cai and Xiao-Yuan Jing and Ling Shao},
  doi          = {10.1016/j.patcog.2023.109771},
  journal      = {Pattern Recognition},
  pages        = {109771},
  shortjournal = {Pattern Recognition},
  title        = {Domain embedding transfer for unequal RGB-D image recognition},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Higher-order memory guided temporal random walk for dynamic
heterogeneous network embedding. <em>PR</em>, <em>143</em>, 109766. (<a
href="https://doi.org/10.1016/j.patcog.2023.109766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding (NE) aims at learning node embeddings via structure-based sampling. However, there are complex patterns in network structure (heterogeneity, higher-order dependence, dynamics) in the real world. The existing methods suffer from high dependence and constraints on manually designed higher-order structures and loss of fine-grained temporal information. To solve the above challenges, we propose a novel higher-order memory guided temporal random walk for dynamic heterogeneous network embedding (HoMo-DyHNE). The proposed model is a two-stage architecture consisting of a meta-structure-independent random walk algorithm namely HoMo-TRW with transition vectors and higher-order memory, and a Hawkes-based featured Skip-gram (HFSG) incorporating a multivariate Hawkes point process to measure the history-current association intensity. Extensive experiments demonstrate the superior effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Cheng Ji and Tao Zhao and Qingyun Sun and Xingcheng Fu and Jianxin Li},
  doi          = {10.1016/j.patcog.2023.109766},
  journal      = {Pattern Recognition},
  pages        = {109766},
  shortjournal = {Pattern Recognition},
  title        = {Higher-order memory guided temporal random walk for dynamic heterogeneous network embedding},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video anomaly detection with NTCN-ML: A novel TCN for
multi-instance learning. <em>PR</em>, <em>143</em>, 109765. (<a
href="https://doi.org/10.1016/j.patcog.2023.109765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key challenge in video anomaly detection is the identification of rare abnormal patterns in the positive instances as they exhibit only a small variation compared to normal patterns, and they are largely biased by the dominant negative instances. To address this issue, we propose a weakly supervised video anomaly detection model called NTCN-ML - Novel Temporal Convolutional Network Multi-Instance Learning Model. The NTCN-ML model extracts temporal representations of video data to construct a time-series pattern to optimize the multi-instance learning process. The model examines the correlation between positive and negative samples in the multi-instance learning process to balance the feature association between rare positive and negative instances. The video anomaly detection with the NTCN-ML model achieved 95.3\% and 85.1\% accuracy for UCF-Crime and ShanghaiTech datasets, respectively, and outperformed the baseline models .},
  archive      = {J_PR},
  author       = {Wenhao Shao and Ruliang Xiao and Praboda Rajapaksha and Mengzhu Wang and Noel Crespi and Zhigang Luo and Roberto Minerva},
  doi          = {10.1016/j.patcog.2023.109765},
  journal      = {Pattern Recognition},
  pages        = {109765},
  shortjournal = {Pattern Recognition},
  title        = {Video anomaly detection with NTCN-ML: A novel TCN for multi-instance learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto-attention mechanism for multi-view deep embedding
clustering. <em>PR</em>, <em>143</em>, 109764. (<a
href="https://doi.org/10.1016/j.patcog.2023.109764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In several fields, deep learning has achieved tremendous success. Multi-view learning is a workable method for handling data from several sources. For clustering multi-view data, deep learning and multi-view learning are excellent options. However, a persistent challenge is a need for the current deep learning approach to independently drive divergent neural networks for different perspectives while working with multi-view data. The current methods use the number of viewpoints to calculate neural network statistics. Consequently, as the number of views rises, it results in a considerable calculation. Furthermore, they vainly try to unite various viewpoints at the training. Incorporating a triple fusion technique, this research suggests an innovative multi-view deep embedding clustering (MDEC) model. The suggested model can jointly acquire the specific knowledge in each view as well as the information fragment of the collective views. The main goal of the MDEC is to lower the errors made when learning the features of each view and correlating data from many views. To address the optimization problem , the MDEC model advises a suitable iterative updating approach. In testing modern deep learning and non-deep learning algorithms, the experimental study on small and large-scale multi-view data shows encouraging results for the MDEC model. In multi-view clustering, this work demonstrates the benefit of the deep learning-based approach over the non-ones. However, future work will address a variety of issues related to MDEC including the speed.},
  archive      = {J_PR},
  author       = {Bassoma Diallo and Jie Hu and Tianrui Li and Ghufran Ahmad Khan and Xinyan Liang and Hongjun Wang},
  doi          = {10.1016/j.patcog.2023.109764},
  journal      = {Pattern Recognition},
  pages        = {109764},
  shortjournal = {Pattern Recognition},
  title        = {Auto-attention mechanism for multi-view deep embedding clustering},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Switching clusters’ synchronization for discrete space-time
complex dynamical networks via boundary feedback controls. <em>PR</em>,
<em>143</em>, 109763. (<a
href="https://doi.org/10.1016/j.patcog.2023.109763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike the existing literatures that consider only discrete-time networks, this paper explores the double effects of both discrete time and discrete spatial diffusions in a switching complex dynamical networks. By means of the knowledge of clusters controls, a clusters synchronous frame of space-time discrete switching complex networks with boundary feedback controller is newly proposed and established. With the helps of some indispensable vector-valued sequence inequalities and Lyapunov function with switching signals and clusters’ information, the boundary feedback controllers are designed to synchronize space-time discrete switching complex networks coupled with nodes’ states or spatial diffusions in the form of clusters. Additionally, a realizable computer algorithm is given to make the derived results of this paper easier to enforce. The current work is pioneering in consideration of discrete spatial diffusions and provides a theoretical and practical basis for future research in this regard.},
  archive      = {J_PR},
  author       = {Tianwei Zhang and Zhouhong Li},
  doi          = {10.1016/j.patcog.2023.109763},
  journal      = {Pattern Recognition},
  pages        = {109763},
  shortjournal = {Pattern Recognition},
  title        = {Switching clusters’ synchronization for discrete space-time complex dynamical networks via boundary feedback controls},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An enhanced noise-tolerant hashing for drone object
detection. <em>PR</em>, <em>143</em>, 109762. (<a
href="https://doi.org/10.1016/j.patcog.2023.109762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drone, a.k.a. Unmanned aerial vehicle (UAV), has been pervasively applied in geological hazard monitoring, smart agriculture, and urban planning in the past decade. In this work, we fuse multiple attributes into a noise-tolerant hashing framework that can detect objects from drone pictures extremely fast. Our method can intrinsically and flexibly encode various topological structures from each target object, based on which multi-scale objects can be discovered in a view- and altitude-invariant way. Moreover, by leveraging l F lF and l 1 l1 norms collaboratively, the calculated hash codes are robust to low quality drone pictures and noisy semantic labels . More specifically, for each drone-borne picture, we extract visually/semantically salient object parts inside it. To characterize their topological structure, we construct a graphlet by linking the spatially adjacent object patches into a small graph. Subsequently, a binary matrix factorization (MF) is designed to hierarchically exploit the semantics of these graphlets, wherein three attributes: i) deep binary hash codes learning, ii) contaminated pictures/labels denoising , and iii) adaptive data graph updating are seamlessly incorporated. Such multi-attribute binary MF can be solved iteratively, and in turn each graphlet is transformed into the binary hash codes. Finally, the hash codes corresponding to graphlets within each drone photo are utilized for ranking-based object discovery. Comprehensive experiments on the DAC-SDC, MOHR, and our self-compiled data set have demonstrated the competitively speed and accuracy of our method. As a byproduct, we employ an elaborately-designed FPGA architecture to calculate our hash codes. On average, a 57 frames per second (fps) object detection speed is achieved on 4K drone videos (without temporal modeling).},
  archive      = {J_PR},
  author       = {Luming Zhang and Guifeng Wang and Ming Chen and Fuji Ren and Ling Shao},
  doi          = {10.1016/j.patcog.2023.109762},
  journal      = {Pattern Recognition},
  pages        = {109762},
  shortjournal = {Pattern Recognition},
  title        = {An enhanced noise-tolerant hashing for drone object detection},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning discriminative feature representation with
pixel-level supervision for forest smoke recognition. <em>PR</em>,
<em>143</em>, 109761. (<a
href="https://doi.org/10.1016/j.patcog.2023.109761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing vision-based smoke recognition methods still face the issues of low detection rates and high false alarm rates in complex scenes. One reason is that they label light smoke and heavy smoke as the same value, which ignores the differences in multiple attribute information involved in the smoke imaging process. To solve this issue, this paper presents a pixel-level supervision neural network (PSNet) to learn discriminative feature representations for forest smoke recognition. First, the pixel-level supervision information, including the background component, smoke component, fusion ratio, and class information, is cooperatively considered to effectively guide the model training process. To avoid negative transfer caused by the asynchronous optimization of shared layer parameters and achieve synchronous minimization of each loss term, a regularization term based on the smoke imaging principle and a weight dynamic updating method are proposed to balance the weight coefficients of different loss terms. Second, a detail-difference-aware module (DDAM) based on a detail-difference-aware block (DDAB) and a spatial attention block (SAB) is proposed to distinguish smoke and smoke-like targets by fusing xy-shared convolution and z-shared convolution, which adaptively allocates the weights over different positions to prioritize the most informative visual elements in the spatial domain. Third, an attention-based feature separation module (AFSM) is proposed to relieve mutual interference in extracting background features and smoke features by designing component interaction attention (CIA), background component attention (BCA), smoke component attention (SCA), and enhanced residual blocks (ERBs), which can guide the interaction and separation process of background information and smoke information to enhance the discriminative spatial features and suppress interference features. ERB effectively eliminates noise and enhances smoke edge information based on median filters. Finally, to further enhance the feature representation capability, a multiconnection aggregation method (MCAM) is proposed by fully aggregating local and global features simultaneously. Extensive experiments show that our method achieves better performance than existing smoke recognition methods. Extensive experiments show that our PSNet achieves better performance than existing smoke recognition methods. For smoke recognition, our PSNet achieves a 96.95\% detection rate, 3.02\% false alarm rate, and 0.9694 F1-score. The average calculation time for each image is only 0.0195. For smoke component separation, our PSNet also achieves 0.0014 on evaluation criteria mean square error between predicted smoke component images and labelled smoke component images. These key experimental results are better than those of previous methods.},
  archive      = {J_PR},
  author       = {Huanjie Tao and Qianyue Duan and Minghao Lu and Zhenwu Hu},
  doi          = {10.1016/j.patcog.2023.109761},
  journal      = {Pattern Recognition},
  pages        = {109761},
  shortjournal = {Pattern Recognition},
  title        = {Learning discriminative feature representation with pixel-level supervision for forest smoke recognition},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention‐guided evolutionary attack with elastic‐net
regularization on face recognition. <em>PR</em>, <em>143</em>, 109760.
(<a href="https://doi.org/10.1016/j.patcog.2023.109760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, face recognition has achieved promising results along with the development of advanced Deep Neural Networks (DNNs). The existing face recognition systems are vulnerable to adversarial examples , which brings potential security risks. Evolutionary Attack (EA) has been successfully used to fool face recognition by inducing a minimum perturbation to a face image with few queries. However, EA employs the global information of face images but ignores their local characteristics. In addition, restricting the ℓ 2 ℓ2 -norm of adversarial perturbations hinders the diversity of adversarial perturbations. To solve the above problems, we propose Attention-guided Evolutionary Attack with Elastic-Net Regularization (ERAEA) for attacking face recognition. ERAEA extracts local facial characteristics by attention mechanism , effectively improving the attack effect and image perception quality. In particular, ERAEA adopts an attention mechanism to guide evolutionary direction, which operates on the covariance matrix as it contains crucial information about the evolutionary path. Furthermore, we design an adaptive elastic-net regularization to diversify the adversarial perturbation, accelerating the optimization performance . Extensive experiments obtained on three benchmarks demonstrate that our proposed method achieves better perturbation norm than the state-of-the-art methods with limited queries on face recognition and generates adversarial face images with higher perceptual quality . Besides, ERAEA requires fewer queries to achieve a fixed adversarial perturbation norm.},
  archive      = {J_PR},
  author       = {Cong Hu and Yuanbo Li and Zhenhua Feng and Xiaojun Wu},
  doi          = {10.1016/j.patcog.2023.109760},
  journal      = {Pattern Recognition},
  pages        = {109760},
  shortjournal = {Pattern Recognition},
  title        = {Attention‐guided evolutionary attack with elastic‐net regularization on face recognition},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal-relational hypergraph tri-attention networks for
stock trend prediction. <em>PR</em>, <em>143</em>, 109759. (<a
href="https://doi.org/10.1016/j.patcog.2023.109759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the future price trends of stocks is a challenging yet intriguing problem given its critical role to help investors make profitable decisions. In this paper, we present a collaborative temporal-relational modeling framework for end-to-end stock trend prediction. Different from existing studies relying on the pairwise correlations between stocks, we argue that stocks are naturally connected as a collective group, and introduce two heterogeneous hypergraphs to separately characterize the stock group-wise relationships of industry-belonging and fund-holding. A novel hypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph convolutional networks with a hierarchical organization of intra-hyperedge, inter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN adaptively determines the importance of nodes, hyperedges, and hypergraphs during the information propagation among stocks, so that the potential synergies between stock movements can be fully exploited. Experimental evaluation and investment simulation on real-world stock data demonstrate the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Chaoran Cui and Xiaojie Li and Chunyun Zhang and Weili Guan and Meng Wang},
  doi          = {10.1016/j.patcog.2023.109759},
  journal      = {Pattern Recognition},
  pages        = {109759},
  shortjournal = {Pattern Recognition},
  title        = {Temporal-relational hypergraph tri-attention networks for stock trend prediction},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A discriminative SPD feature learning approach on riemannian
manifolds for EEG classification. <em>PR</em>, <em>143</em>, 109751. (<a
href="https://doi.org/10.1016/j.patcog.2023.109751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance matrix learning methods have become popular for many classification tasks owing to their ability to capture interesting structures in non-linear data while respecting the Riemannian geometry of the underlying symmetric positive definite (SPD) manifolds. Several deep learning architectures applied to these matrix learning methods have recently been proposed in classification tasks by learning discriminative Euclidean-based embeddings. In this paper, we propose a new Riemannian-based deep learning network to generate more discriminative features for electroencephalogram (EEG) classification. Our key innovation lies in learning the Riemannian barycenter for each class within a Riemannian geometric space . The proposed model normalizes the distribution of SPD matrices and learns the center of each class to penalize the distances between the matrix and the corresponding class centers. As a result, our framework can further simultaneously reduce the intra-class distances, enlarge the inter-class distances for the learned features, and consistently outperform other state-of-the-art methods on three widely used EEG datasets and the data from our stress-induced experiment in virtual reality. Experimental results demonstrate the superiority of the proposed framework for learning the non-stationary nature of EEG signals due to the robustness of the covariance descriptor and the benefits of considering the barycenters on the Riemannian geometry.},
  archive      = {J_PR},
  author       = {Byung Hyung Kim and Jin Woo Choi and Honggu Lee and Sungho Jo},
  doi          = {10.1016/j.patcog.2023.109751},
  journal      = {Pattern Recognition},
  pages        = {109751},
  shortjournal = {Pattern Recognition},
  title        = {A discriminative SPD feature learning approach on riemannian manifolds for EEG classification},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Source-free and black-box domain adaptation via
distributionally adversarial training. <em>PR</em>, <em>143</em>,
109750. (<a href="https://doi.org/10.1016/j.patcog.2023.109750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free unsupervised domain adaptation is one class of practical deep learning methods which generalize in the target domain without transferring data from source domain. However, existing source-free domain adaptation methods rely on source model transferring. In many data-critical scenarios, the transferred source models may suffer from membership inference attacks and expose private data. In this paper, we aim to overcome a more practical and challenging setting where the source models cannot be transferred to the target domain. The source models are considered as queryable black-box models which only output hard labels. We use public third-party data to probe the source model and obtain supervision information, dispensing with transferring source model. To fill the gap between third-party data and target data, we further propose Distributionally Adversarial Training (DAT) to align the distribution of third-party data with target data, gain more informative query results and improve the data efficiency. We call this new framework Black-box Probe Domain Adaptation (BPDA) which adopts query mechanism and DAT to probe and refine supervision information. Experimental results on several domain adaptation datasets demonstrate the practicability and data efficiency of BPDA in query-only and source-free unsupervised domain adaptation .},
  archive      = {J_PR},
  author       = {Yucheng Shi and Kunhong Wu and Yahong Han and Yunfeng Shao and Bingshuai Li and Fei Wu},
  doi          = {10.1016/j.patcog.2023.109750},
  journal      = {Pattern Recognition},
  pages        = {109750},
  shortjournal = {Pattern Recognition},
  title        = {Source-free and black-box domain adaptation via distributionally adversarial training},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian kernel fuzzy c-means with width parameter
computation and regularization. <em>PR</em>, <em>143</em>, 109749. (<a
href="https://doi.org/10.1016/j.patcog.2023.109749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional Gaussian kernel fuzzy c-means clustering algorithms require selecting the width hyper-parameter, which is data-dependent and fixed for the entire execution. Not only that, but these parameters are the same for every dataset variable. Therefore, the variables have the same importance in the clustering task , including irrelevant variables. This paper proposes a Gaussian kernel fuzzy c-means with kernelization of the metric and automated computation of width parameters. These width parameters change at each iteration of the algorithm and vary from each variable and from each cluster. Thus, this algorithm can re-scale the variables differently, thus highlighting those that are relevant to the clustering task . Fuzzy clustering algorithms with regularization have become popular due to their high performance in large-scale data clustering , robustness for initialization, and low computational complexity . Because the width parameters of the variables can also be controlled by entropy, this paper also proposes Gaussian kernel fuzzy c-means algorithms with kernelization of the metric and automated computation of width parameters through entropy regularization. To demonstrate their usefulness, the proposed algorithms are compared with the conventional KFCM-K algorithm and previous algorithms that automatically compute the width parameter of the Gaussian kernel.},
  archive      = {J_PR},
  author       = {Eduardo C． Simões and Francisco de A. T． de Carvalho},
  doi          = {10.1016/j.patcog.2023.109749},
  journal      = {Pattern Recognition},
  pages        = {109749},
  shortjournal = {Pattern Recognition},
  title        = {Gaussian kernel fuzzy c-means with width parameter computation and regularization},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring multivariate generalized gamma manifold for color
texture retrieval. <em>PR</em>, <em>143</em>, 109748. (<a
href="https://doi.org/10.1016/j.patcog.2023.109748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a novel method for color-textured image retrieval on a Multivariate Generalized Gamma Distribution manifold (MG Γ Γ D). Thanks to the Gaussian copula theory, we define the expression of MG Γ Γ D, which efficiently models the statistical dependence structure between dual-tree complex wavelet transform (DTCWT) of the color components. The major contribution of this paper is to provide a geometric perspective to the MG Γ Γ D by treating it as a Riemannian manifold while proposing the geodesic distance (GD) as a measure of Riemannian similarity on it. Based on information geometry tools, we conduct a geometrical study of the MG Γ Γ D manifold, allowing us to derive two suitable approximations of the GD. The experiments are performed on five well-known color texture databases, considering the content-based image retrieval (CBIR) framework and using the RGB color space. The obtained results demonstrate the efficiency of the geometric interpretation through the proposed GD as a natural and intuitive similarity measure on the studied statistical manifold.},
  archive      = {J_PR},
  author       = {Zakariae Abbad and Ahmed Drissi El Maliani and Said Ouatik El Alaoui and Mohammed El Hassouni and Mohamed Tahar Kadaoui Abbassi},
  doi          = {10.1016/j.patcog.2023.109748},
  journal      = {Pattern Recognition},
  pages        = {109748},
  shortjournal = {Pattern Recognition},
  title        = {Exploring multivariate generalized gamma manifold for color texture retrieval},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep forest auto-encoder for resource-centric attributes
graph embedding. <em>PR</em>, <em>143</em>, 109747. (<a
href="https://doi.org/10.1016/j.patcog.2023.109747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding is an important technique used for representing graph structure data that preserves intrinsic features in a low-dimensional space suitable for graph-based applications. Graphs containing node attributes and weighted links are commonly employed to model various real-world problems and issues in computer science. In recent years, a hot research topic has been the exploitation of diverse information, including node attributes and topological semantic information, in graph embedding. However, due to limitations in deep learning based on neural networks , such information has not been fully utilized nor adequately integrated in existing models, leaving graph embedding unsatisfactory, especially for large resource graphs (e.g., knowledge graphs and task interaction graphs). In this study, we introduce a resource-centric graph embedding approach based on deep random forests learning, which reconstructs graphs using a deep autoencoder to achieve high effectiveness. To accomplish this, our approach employs three key components. The first component is a preprocessor driven by graph similarity , alongside modularity and self-attention modules, to comprehensively integrate graph representation . The second component utilizes local graph information structures to enhance the raw graph. Finally, we integrate diverse information using multi-grained scanning and dual-level cascade forests in the deep learning extractor and generator, ultimately producing the final graph embedding. Experimental results on seven real-world scenarios show that our approach outperforms state-of-the-art embedding methods.},
  archive      = {J_PR},
  author       = {Yan Ding and Yujuan Zhai and Ming Hu and Jia Zhao},
  doi          = {10.1016/j.patcog.2023.109747},
  journal      = {Pattern Recognition},
  pages        = {109747},
  shortjournal = {Pattern Recognition},
  title        = {Deep forest auto-encoder for resource-centric attributes graph embedding},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reward shaping with hierarchical graph topology.
<em>PR</em>, <em>143</em>, 109746. (<a
href="https://doi.org/10.1016/j.patcog.2023.109746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reward shaping using GCNs is a popular research area in reinforcement learning . However, it is difficult to shape potential functions for complicated tasks. In this paper, we develop Reward Shaping with Hierarchical Graph Topology (HGT). HGT propagates information about the rewards through the message passing mechanism, which can be used as potential functions for reward shaping. We describe reinforcement learning by a probability graph model. Then we generate a underlying graph with each state is a node and edges represent transition probabilities between states. In order to prominently shape potential functions for complex environments, HGT divides the underlying graph constructed from states into multiple subgraphs. Since these subgraphs provide a representation of multiple logical relationships between states in the Markov decision process , the aggregation process rich correlation information between nodes, which makes the propagated messages more powerful. When compared to cutting-edge RL techniques, HGT achieves faster learning rates in experiments on Atari and Mujoco tasks.},
  archive      = {J_PR},
  author       = {Jianghui Sang and Yongli Wang and Weiping Ding and Zaki Ahmadkhan and Lin Xu},
  doi          = {10.1016/j.patcog.2023.109746},
  journal      = {Pattern Recognition},
  pages        = {109746},
  shortjournal = {Pattern Recognition},
  title        = {Reward shaping with hierarchical graph topology},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Uncovering hidden vulnerabilities in convolutional neural
networks through graph-based adversarial robustness evaluation.
<em>PR</em>, <em>143</em>, 109745. (<a
href="https://doi.org/10.1016/j.patcog.2023.109745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are widely used for image classification , but their vulnerability to adversarial attacks poses challenges to their reliability and security. However, current adversarial robustness (AR) measures lack a theoretical foundation, limiting the insight into the decision process. To address this issue, we propose a new AR evaluation framework based on Graph of Patterns (GoPs) models and graph distance algorithms. Our approach provides a fine-grained analysis of AR from three perspectives, providing targeted insight into the vulnerability of CNNs. Compared to current standards, our approach is theoretically grounded and allows fine-tuning of model components without repeated attempts and validation. Our experimental results demonstrate its effectiveness in uncovering hidden vulnerabilities in CNNs and providing actionable approaches to improve their AR. Our GoPs modeling approach and graph distance algorithms can be extended to apply to other graph machine learning tasks such as Metric Learning on multi-relational graphs. Overall, our framework represents significant progress in AR evaluation, providing a more interpretable, targeted, and efficient approach to assess CNN robustness in complex graph-based systems.},
  archive      = {J_PR},
  author       = {Ke Wang and Zicong Chen and Xilin Dang and Xuan Fan and Xuming Han and Chien-Ming Chen and Weiping Ding and Siu-Ming Yiu and Jian Weng},
  doi          = {10.1016/j.patcog.2023.109745},
  journal      = {Pattern Recognition},
  pages        = {109745},
  shortjournal = {Pattern Recognition},
  title        = {Uncovering hidden vulnerabilities in convolutional neural networks through graph-based adversarial robustness evaluation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tri-objective optimization-based cascade ensemble pruning
for deep forest. <em>PR</em>, <em>143</em>, 109744. (<a
href="https://doi.org/10.1016/j.patcog.2023.109744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep forest is a new multi-layer ensemble model, where the high time costs and storage requirements inhibit its large-scale application. However, current deep forest pruning methods used to alleviate these drawbacks do not consider its cascade coupling characteristics. Therefore, we propose a tri-objective optimization-based cascade ensemble pruning (TOOCEP) algorithm for it. Concretely, we first present a tri-objective optimization-based single-layer pruning (TOOSLP) method to prune its single-layer by simultaneously optimizing three objectives, namely accuracy, independent diversity, and coupled diversity. Particularly, the coupled diversity is designed for deep forest to deal with the coupling relationships between its adjacent layers. Then, we perform TOOSLP in a cascade framework to prune the deep forest layer-by-layer. Experimental results on 15 UCI datasets show that TOOCEP outperforms several state-of-the-art methods in accuracy and pruned rate, which significantly reduces the storage space and accelerate the prediction speed of deep forest.},
  archive      = {J_PR},
  author       = {Junzhong Ji and Junwei Li},
  doi          = {10.1016/j.patcog.2023.109744},
  journal      = {Pattern Recognition},
  pages        = {109744},
  shortjournal = {Pattern Recognition},
  title        = {Tri-objective optimization-based cascade ensemble pruning for deep forest},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMPR: Single-stage multi-person pose regression.
<em>PR</em>, <em>143</em>, 109743. (<a
href="https://doi.org/10.1016/j.patcog.2023.109743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-person pose estimators can be roughly divided into two-stage approaches (top-down and bottom-up approaches) and one-stage approaches. The two-stage methods either suffer high computational redundancy for additional person detectors or group keypoints heuristically after predicting all the instance-free keypoints . The recently proposed single-stage methods do not rely on the above two extra stages but have lower performance than the latest bottom-up approaches. In this work, a novel single-stage multi-person pose regression, termed SMPR, is presented. It follows the paradigm of dense prediction and predicts instance-aware keypoints from every location. Besides feature aggregation, we propose better strategies to define positive pose hypotheses for training which all play an important role in dense pose estimation. The network also learns the scores of estimated poses. The pose scoring strategy further improves the pose estimation performance by prioritizing superior poses during non-maximum suppression (NMS). We show that our method not only outperforms existing single-stage methods but also be competitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on the COCO test-dev pose benchmark. The code is available at https://github.com/cmdi-dlut/SMPR .},
  archive      = {J_PR},
  author       = {Huixin Miao and Junqi Lin and Junjie Cao and Xiaoguang He and Zhixun Su and Risheng Liu},
  doi          = {10.1016/j.patcog.2023.109743},
  journal      = {Pattern Recognition},
  pages        = {109743},
  shortjournal = {Pattern Recognition},
  title        = {SMPR: Single-stage multi-person pose regression},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kinship verification using multi-level dictionary pair
learning for multiple resolution images. <em>PR</em>, <em>143</em>,
109742. (<a href="https://doi.org/10.1016/j.patcog.2023.109742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship verification using facial images is gaining substantial attention by computer vision researchers. The real challenge in kinship verification is to effectively represent the discriminative features to ease the differences between kinship image pairs. Further, existing kinship methods only focus on a single resolution, and ignore the variability of resolutions in practical scenarios. To address these issues, we propose a multi-level dictionary pair learning (MLDPL) method to learn dictionary pairs by incorporating multiple resolution images for kinship verification. We learn dictionary pairs jointly by transforming discriminative features of image pairs into different coding coefficients in the same space, thereby reducing the differences between them. Further, multiple resolution images are incorporated into dictionary pair learning to effectively deal with resolution variations in kinship verification. Extensive experiments are performed on different kinship datasets to validate the efficacy of proposed MLDPL method. Experimental results show that MLDPL achieves competitive performance on all kinship datasets.},
  archive      = {J_PR},
  author       = {Aarti Goyal and Toshanlal Meenpal},
  doi          = {10.1016/j.patcog.2023.109742},
  journal      = {Pattern Recognition},
  pages        = {109742},
  shortjournal = {Pattern Recognition},
  title        = {Kinship verification using multi-level dictionary pair learning for multiple resolution images},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Fully context-aware image inpainting with a learned
semantic pyramid. <em>PR</em>, <em>143</em>, 109741. (<a
href="https://doi.org/10.1016/j.patcog.2023.109741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoring reasonable and realistic content for arbitrary missing regions in images is an important yet challenging task. Although recent image inpainting models have made significant progress in generating vivid visual details, they can still lead to texture blurring or structural distortions due to contextual ambiguity when dealing with more complex scenes. To address this issue, we propose the Semantic Pyramid Network (SPN) motivated by the idea that learning multi-scale semantic priors from specific pretext tasks can greatly benefit the recovery of locally missing content in images. SPN consists of two components. First, it distills semantic priors from a pretext model into a multi-scale feature pyramid, achieving a consistent understanding of the global context and local structures. Within the prior learner, we present an optional module for variational inference to realize probabilistic image inpainting driven by various learned priors. The second component of SPN is a fully context-aware image generator, which adaptively and progressively refines low-level visual representations at multiple scales with the (stochastic) prior pyramid. We train the prior learner and the image generator as a unified model without any post-processing. Our approach achieves the state of the art on multiple datasets, including Places2, Paris StreetView, CelebA, and CelebA-HQ, under both deterministic and probabilistic inpainting setups.},
  archive      = {J_PR},
  author       = {Wendong Zhang and Yunbo Wang and Bingbing Ni and Xiaokang Yang},
  doi          = {10.1016/j.patcog.2023.109741},
  journal      = {Pattern Recognition},
  pages        = {109741},
  shortjournal = {Pattern Recognition},
  title        = {Fully context-aware image inpainting with a learned semantic pyramid},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven single image deraining: A comprehensive review
and new perspectives. <em>PR</em>, <em>143</em>, 109740. (<a
href="https://doi.org/10.1016/j.patcog.2023.109740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {S ingle I mage D eraining (SID) aims at recovering the rain-free background from an image degraded by rain streaks. For the powerful fitting ability of deep neural networks and massive training data, data-driven deep SID methods have obtained significant improvement over traditional model/prior-based ones. Current studies usually focus on improving the deraining performance by proposing different categories of deraining networks, while neglecting the interpretation of the solving process. As a result, the generalization ability may still be limited in real-world scenarios, and the deraining results also cannot effectively improve the performance of subsequent high-level tasks (e.g., object detection). To explore these issues, we in this paper re-examine the three important factors (i.e., data, rain model and network architecture ) for the SID problem, and specifically analyze them by proposing new and more reasonable criteria (i.e., general vs. specific, synthetical vs. mathematical, black-box vs. white-box ). We also study the relationship of the three factors from a new perspective of data, and reveal two different solving paradigms ( explicit vs. implicit ) for the SID task. We further discuss the current mainstream data-driven SID methods from five aspects, i.e., training strategy, network pipeline, domain knowledge, data preprocessing , and objective function, and some useful conclusions are summarized by statistics. Besides, we profoundly studied one of the three factors, i.e., data, and measured the performance of current methods on different datasets through extensive experiments to reveal the effectiveness of SID data. Finally, with the comprehensive review and in-depth analysis, we draw some valuable conclusions and suggestions for future research.},
  archive      = {J_PR},
  author       = {Zhao Zhang and Yanyan Wei and Haijun Zhang and Yi Yang and Shuicheng Yan and Meng Wang},
  doi          = {10.1016/j.patcog.2023.109740},
  journal      = {Pattern Recognition},
  pages        = {109740},
  shortjournal = {Pattern Recognition},
  title        = {Data-driven single image deraining: A comprehensive review and new perspectives},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedCL: Federated contrastive learning for multi-center
medical image classification. <em>PR</em>, <em>143</em>, 109739. (<a
href="https://doi.org/10.1016/j.patcog.2023.109739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning , which allows distributed medical institutions to train a shared deep learning model with privacy protection, has become increasingly popular recently. However, in practical application, due to data heterogeneity between different hospitals, the performance of the model will be degraded in the training process. In this paper, we propose a federated contrastive learning (FedCL) approach. FedCL integrates the idea of contrastive learning into the federated learning framework. Specifically, it combines the local model and the global model for contrastive learning, so that the local model gradually approaches the global model with the increase of communication rounds, which improves the generalization ability of the model. We validate our method on two public datasets. Extensive experiments show that our method is superior to other federated learning algorithms in medical image classification .},
  archive      = {J_PR},
  author       = {Zhenbing Liu and Fengfeng Wu and Yumeng Wang and Mengyu Yang and Xipeng Pan},
  doi          = {10.1016/j.patcog.2023.109739},
  journal      = {Pattern Recognition},
  pages        = {109739},
  shortjournal = {Pattern Recognition},
  title        = {FedCL: Federated contrastive learning for multi-center medical image classification},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating topology beyond descriptions for zero-shot
learning. <em>PR</em>, <em>143</em>, 109738. (<a
href="https://doi.org/10.1016/j.patcog.2023.109738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to discriminate object categories through the identification of their attributes and has received much attention for its capability to predict unseen categories without collecting training data. Recently, excellent works have been devoted to optimizing the model inference by mining the topology among categories/attributes, which proves that the topology learning is beneficial and important for ZSL. However, existing works focus almost exclusively on the construction of semantic topological knowledge with textual descriptions, which, though effective, still suffer from two deficiencies: first, the semantic gap between modalities makes it difficult for the category attributes to accurately describe the corresponding visual characters, resulting in the topology constructed in the semantic modality being distorted in the visual modality ; second, it is difficult for one to enumerate all the attributes hidden in images, resulting in an incomplete topology mined only from the defined attributes. Therefore, we propose a C ross-Modality T opology P ropagation M atcher (CTPM) to construct a more complete topology system by collaborative mining of topological knowledge in both the visual and semantic modalities. We stand at the dataset level to construct sample-based visual topological knowledge based on the global image features to preserve the integrity of visual information. Meanwhile, we exploit the matching relationship between visual and semantic modalities to make topological knowledge propagate effectively across modalities, and fully enjoy the benefits of multi-modality topological knowledge in category/attribute reasoning. We validate the effectiveness of our CTPM through extensive experiments and achieve state-of-the-art performance on four ZSL datasets.},
  archive      = {J_PR},
  author       = {Ziyi Chen and Yutong Gao and Congyan Lang and Lili Wei and Yidong Li and Hongzhe Liu and Fayao Liu},
  doi          = {10.1016/j.patcog.2023.109738},
  journal      = {Pattern Recognition},
  pages        = {109738},
  shortjournal = {Pattern Recognition},
  title        = {Integrating topology beyond descriptions for zero-shot learning},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep representation learning for domain generalization with
information bottleneck principle. <em>PR</em>, <em>143</em>, 109737. (<a
href="https://doi.org/10.1016/j.patcog.2023.109737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep neural networks have achieved superior performance on many classical tasks, they deteriorate in real applications due to the unpredictable distribution shift. Domain generalization (DG) focuses on improving the generalization ability of the predictive model in unseen domains by training on multiple available source domains. All these domains share the same categories but commonly obey different distributions. In this paper, we establish a new theoretical framework for domain generalization from the perspective of the information bottleneck (IB) principle, which links representation learning in DG with domain-invariant representation learning and maximizing feature entropy (MFE). Based on the theoretical framework, we provide a feasible solution by class-wise instance discrimination combined with inter-dimension decorrelation and intra-dimension uniformity to learn the desired representation for domain generalization, which achieves excellent performance on multiple datasets without knowing domain labels. Extensive experiments show that the proposed regularization rule (MFE) can improve invariance-based DG methods consistently. Moreover, as an extreme case of domain generalization, we also show that MFE is promising to improve adversarial robustness.},
  archive      = {J_PR},
  author       = {Jiao Zhang and Xu-Yao Zhang and Chuang Wang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2023.109737},
  journal      = {Pattern Recognition},
  pages        = {109737},
  shortjournal = {Pattern Recognition},
  title        = {Deep representation learning for domain generalization with information bottleneck principle},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-particle reconstruction in cryo-EM based on
three-dimensional weighted nuclear norm minimization. <em>PR</em>,
<em>143</em>, 109736. (<a
href="https://doi.org/10.1016/j.patcog.2023.109736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-particle reconstruction (SPR) in cryogenic electron microscopy (cryo-EM) aims at aligning and averaging two-dimensional micrographs to reconstruct a three-dimensional particle. How to reconstruct micrographs from heavy noise is a crucial point for achieving better micrograph quality, and thus many methods focus on noise removal. However, new problems such as over-smoothing often occur in their results due to failure in handling heavy noise well. This paper proposes a three-dimensional weighted nuclear norm minimization (3DWNNM) model for SPR in the cryo-EM task to address these issues. Specifically, we design a minimization solver based on the forward-backward splitting algorithm to tackle our model efficiently. Under certain conditions, this solution has an energy-decaying feature and performs exceptionally well in reconstruction. Numerical experiments fully demonstrate the effectiveness and the robustness of the proposed method.},
  archive      = {J_PR},
  author       = {Chaoyan Huang and Tingting Wu and Juncheng Li and Bin Dong and Tieyong Zeng},
  doi          = {10.1016/j.patcog.2023.109736},
  journal      = {Pattern Recognition},
  pages        = {109736},
  shortjournal = {Pattern Recognition},
  title        = {Single-particle reconstruction in cryo-EM based on three-dimensional weighted nuclear norm minimization},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient probability intervals for classification using
inductive venn predictors. <em>PR</em>, <em>143</em>, 109734. (<a
href="https://doi.org/10.1016/j.patcog.2023.109734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning enabled components are frequently used by autonomous systems and it is common for deep neural networks to be integrated in such systems for their ability to learn complex, non-linear data patterns and make accurate predictions in dynamic environments. However, their large number of parameters and their use as black boxes introduce risks as the confidence in each prediction is unknown and output values like softmax scores are not usually well-calibrated. Different frameworks have been proposed to compute accurate confidence measures along with the predictions but at the same time introduce a number of limitations like execution time overhead or inability to be used with high-dimensional data. In this paper, we use the Inductive Venn Predictors framework for computing probability intervals regarding the correctness of each prediction in real-time. We propose taxonomies based on distance metric learning to compute informative probability intervals in applications involving high-dimensional inputs. By assigning pseudo-labels to unlabeled input data during system deployment we further improve the efficiency of the computed probability intervals. Empirical evaluation on image classification and botnet attacks detection in Internet-of-Things (IoT) applications demonstrates improved accuracy and calibration. The proposed method is computationally efficient, and therefore, can be used in real-time. The code is available at https://github.com/dboursinos/Efficient-Probability-Intervals-Classification-Inductive-Venn-Predictors .},
  archive      = {J_PR},
  author       = {Dimitrios Boursinos and Xenofon Koutsoukos},
  doi          = {10.1016/j.patcog.2023.109734},
  journal      = {Pattern Recognition},
  pages        = {109734},
  shortjournal = {Pattern Recognition},
  title        = {Efficient probability intervals for classification using inductive venn predictors},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-order interaction feature selection for classification
learning: A robust knowledge metric perspective. <em>PR</em>,
<em>143</em>, 109733. (<a
href="https://doi.org/10.1016/j.patcog.2023.109733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important learning task in data mining and knowledge discovery. Nevertheless, the fuzziness , uncertainty, and noise presented by the data greatly complicate the construction of learning models. Moreover, most works focus on exploring low-order correlations between variables using low-dimensional mutual information, without paying attention to high-order interaction for multiple variables, resulting in the loss of some potentially important dependency information. Driven by these two issues, a robust knowledge metric approach is invented to perceive and excavate the latent information hidden in interaction. In this study, firstly, a robust fuzzy granularity space is constructed from different granular structures induced by different features, and the robust fuzzy uncertainty measures (RFUMs) are successively devised. Then, RFUMs are used to measure pair-wise, three-order, and even higher-order interaction dependencies among features. Further, a constrained high-order interaction evaluation function inspired by the N-gram language model is formulated, and a corresponding high-order interaction feature selection algorithm with RFUMs ( HIFS-RFUMs ) is designed. Next, comparative experiments with seven representative algorithms on twenty datasets illustrate its effectiveness. In addition, ablation experiments are conducted on the high-order interaction feature selection algorithm with fuzzy uncertainty measures ( HIFS -FUMs) and the relative reduction algorithm with RFUMs (R2- RFUMs ), which demonstrate the robustness of the metric and the effectiveness for mining high-order interactive features, respectively.},
  archive      = {J_PR},
  author       = {Jihong Wan and Hongmei Chen and Tianrui Li and Min Li and Xiaoling Yang},
  doi          = {10.1016/j.patcog.2023.109733},
  journal      = {Pattern Recognition},
  pages        = {109733},
  shortjournal = {Pattern Recognition},
  title        = {High-order interaction feature selection for classification learning: A robust knowledge metric perspective},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Momentum contrast transformer for COVID-19 diagnosis with
knowledge distillation. <em>PR</em>, <em>143</em>, 109732. (<a
href="https://doi.org/10.1016/j.patcog.2023.109732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent diagnosis has been widely studied in diagnosing novel corona virus disease (COVID-19). Existing deep models typically do not make full use of the global features such as large areas of ground glass opacities, and the local features such as local bronchiolectasis from the COVID-19 chest CT images, leading to unsatisfying recognition accuracy. To address this challenge, this paper proposes a novel method to diagnose COVID-19 using momentum contrast and knowledge distillation , termed MCT-KD . Our method takes advantage of Vision Transformer to design a momentum contrastive learning task to effectively extract global features from COVID-19 chest CT images. Moreover, in transfer and fine-tuning process, we integrate the locality of convolution into Vision Transformer via special knowledge distillation . These strategies enable the final Vision Transformer simultaneously focuses on global and local features from COVID-19 chest CT images. In addition, momentum contrastive learning is self-supervised learning, solving the problem that Vision Transformer is challenging to train on small datasets. Extensive experiments confirm the effectiveness of the proposed MCT-KD. In particular, our MCT-KD is able to achieve 87.43\% and 96.94\% accuracy on two publicly available datasets, respectively.},
  archive      = {J_PR},
  author       = {Aimei Dong and Jian Liu and Guodong Zhang and Zhonghe Wei and Yi Zhai and Guohua Lv},
  doi          = {10.1016/j.patcog.2023.109732},
  journal      = {Pattern Recognition},
  pages        = {109732},
  shortjournal = {Pattern Recognition},
  title        = {Momentum contrast transformer for COVID-19 diagnosis with knowledge distillation},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resampling approach for one-class classification.
<em>PR</em>, <em>143</em>, 109731. (<a
href="https://doi.org/10.1016/j.patcog.2023.109731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of a classification model depends significantly on the degree to which the support of each data class overlaps. Successfully distinguishing between classes is difficult if the support is similar. In the one-class classification (OCC) problem, wherein the data comprise only a single class, the classifier performance is significantly degraded if the population support of each class is similar. In this study, we propose a resampling algorithm that enhances classifier performance by utilizing the macro information that is most easily obtainable in these two problem situations. The algorithm aims to improve classifier performance by reprocessing the given data into data with mitigated class imbalance through raking and sampling techniques. This performance improvement is demonstrated by comparing representative classifiers used in the existing OCC problem with traditional binary classifier models, which are unavailable on a single-class dataset.},
  archive      = {J_PR},
  author       = {Hae-Hwan Lee and Seunghwan Park and Jongho Im},
  doi          = {10.1016/j.patcog.2023.109731},
  journal      = {Pattern Recognition},
  pages        = {109731},
  shortjournal = {Pattern Recognition},
  title        = {Resampling approach for one-class classification},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching based on variance minimization of component
distances using edges of free-form surfaces. <em>PR</em>, <em>143</em>,
109729. (<a href="https://doi.org/10.1016/j.patcog.2023.109729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The basis for guidance in the field of automated robot processing is modeling by visual scanning. Matching algorithms are the real link that can be made between the ideal model and the subsequent robot processing. The matching algorithm that meets the machining requirements plays a pivotal role in the entire process, which provides the exact location of design models and measurement data. In order to meet the requirement of making the machining allowance uniform, a fine registration method which considers the variance minimization of the normal and tangential distances between the edge neighbors of the scattered point cloud is proposed. The edge neighbors are achieved by local growing of edge seeds based on the minimization of energy of supervoxel, which can represent the model more accurate than edge points extracted directly. The edge neighbor points are used to participate in the calculation, and the objective function of minimizing the variance of the two-way distance with the introduction of weight coefficients is proposed to constrain the iterative process. The effect of the distance in two directions on the result is analyzed, to determine the appropriate weight coefficients so that the matching calculation converges quickly and accurately. In comparison with other classical and state-of-the-art matching methods, the method in this paper performs well in terms of solution efficiency and accuracy of results. Moreover, the ability of this paper’s method to resist Gaussian noise is investigated, and it is found that this paper’s method has good robustness when σ σ is less than 1 for Gaussian noise . Ultimately, a uniformly distributed residual model is obtained to provide a visually guided basis for subsequent machining.},
  archive      = {J_PR},
  author       = {Jingyu Sun and Yadong Gong and Jibin Zhao and Huan Zhang and Liya Jin},
  doi          = {10.1016/j.patcog.2023.109729},
  journal      = {Pattern Recognition},
  pages        = {109729},
  shortjournal = {Pattern Recognition},
  title        = {Matching based on variance minimization of component distances using edges of free-form surfaces},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trigonometric projection statistics histograms for 3D local
feature representation and shape description. <em>PR</em>, <em>143</em>,
109727. (<a href="https://doi.org/10.1016/j.patcog.2023.109727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation as a significant approach to three-dimensional (3D) shape description has been widely employed in computer vision. However, most existing methods are suffering from the emerging challenges for descriptiveness, robustness and efficiency. This paper presents a novel feature descriptor named trigonometric projection statistics histograms (TPSH). By constructing the repeatable local reference frame based on a multi-attribute weighting strategy, TPSH can address many prevailing nuisances such as noise, occlusion and varying resolution. The trigonometric projection mechanism is originally proposed for TPSH generation, which combines two perspective views to encode both spatial distribution and geometrical measurements from local shape into statistics histograms. The experimental evaluation on public datasets proves that TPSH outperforms state-of-the-art methods in descriptiveness and robustness while maintaining storage compactness and computational efficiency. It is demonstrated that TPSH can not only be suited for 3D object recognition and shape registration, but also generalized across various acquisition devices, data modalities and application scenarios.},
  archive      = {J_PR},
  author       = {Xingsheng Liu and Anhu Li and Jianfeng Sun and Zhiyong Lu},
  doi          = {10.1016/j.patcog.2023.109727},
  journal      = {Pattern Recognition},
  pages        = {109727},
  shortjournal = {Pattern Recognition},
  title        = {Trigonometric projection statistics histograms for 3D local feature representation and shape description},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling global distribution for federated learning with
label distribution skew. <em>PR</em>, <em>143</em>, 109724. (<a
href="https://doi.org/10.1016/j.patcog.2023.109724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning achieves joint training of deep models by connecting decentralized datasources, which can significantly mitigate the risk of privacy leakage . However, in a more general case, the distributions of labels among clients are different, called “label distribution skew”. Directly applying conventional federated learning without consideration of label distribution skew issue significantly hurts the performance of the global model. To this end, we propose a novel federated learning method, named FedMGD, to alleviate the performance degradation caused by the label distribution skew issue. It introduces a global Generative Adversarial Network to model the global data distribution without access to local datasets, so the global model can be trained using the global information of data distribution without privacy leakage . The experimental results demonstrate that our proposed method significantly outperforms the state-of-the-art on several public benchmarks. Code is available at https://www.github.com/Sheng-T/FedMGD .},
  archive      = {J_PR},
  author       = {Tao Sheng and Chengchao Shen and Yuan Liu and Yeyu Ou and Zhe Qu and Yixiong Liang and Jianxin Wang},
  doi          = {10.1016/j.patcog.2023.109724},
  journal      = {Pattern Recognition},
  pages        = {109724},
  shortjournal = {Pattern Recognition},
  title        = {Modeling global distribution for federated learning with label distribution skew},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preferred vector machine for forest fire detection.
<em>PR</em>, <em>143</em>, 109722. (<a
href="https://doi.org/10.1016/j.patcog.2023.109722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning-based fire detection/recognition is very popular in forest-monitoring systems. However, without considering the prior knowledge, e.g., equal attention on both classes of the fire and non-fire samples, fire miss-detected phenomena frequently appeared in the current methods. In this work, considering model’s interpretability and the limited data for model-training, we propose a novel pixel-precision method, termed as PreVM (Preferred Vector Machine). To guarantee high fire detection rate under precise control, a new L 0 L0 norm constraint is introduced to the fire class. Computationally, instead of the traditional L 1 L1 re-weighted techniques in L 0 L0 norm approximation , this L 0 L0 constraint can be converted into linear inequality and incorporated into the process of parameter selection. To further speed up model-training and reduce error warning rate, we also present a kernel-based L 1 L1 norm PreVM ( L 1 L1 -PreVM). Theoretically, we firstly prove the existence of dual representation for the general L p Lp ( p ≥ p≥ 1) norm regularization problems in RKHS (Reproducing Kernel Hilbert Space). Then, we provide a mathematical evidence for L 1 L1 norm kernelization to conquer the case when feature samples do not appear in pairs. The work also includes an extensive experimentation on the real forest fire images and videos. Compared with the-state-of-art methods, the results show that our PreVM is capable of simultaneously achieving higher fire detection rates and lower error warning rates, and L 1 L1 -PreVM is also superior in real-time detection.},
  archive      = {J_PR},
  author       = {Xubing Yang and Zhichun Hua and Li Zhang and Xijian Fan and Fuquan Zhang and Qiaolin Ye and Liyong Fu},
  doi          = {10.1016/j.patcog.2023.109722},
  journal      = {Pattern Recognition},
  pages        = {109722},
  shortjournal = {Pattern Recognition},
  title        = {Preferred vector machine for forest fire detection},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing the class-imbalance and class-overlap problems by
a metaheuristic-based under-sampling approach. <em>PR</em>,
<em>143</em>, 109721. (<a
href="https://doi.org/10.1016/j.patcog.2023.109721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of imbalanced class distribution in real-world datasets severely impairs the performance of classification algorithms. The learning task becomes more complicated and challenging when there is also the class-overlap problem in imbalanced data. This research tackles these problems by presenting an under-sampling approach based on a metaheuristic method in which the under-sampling problem is mapped into an optimization problem . The proposed approach aims to select an optimal subset of the majority samples to handle the imbalanced and the class-overlap problems simultaneously while avoiding the excessive elimination of majority samples, especially in overlapped regions. The quality of the generated solutions is evaluated by a classifier and optimized in an evolutionary process. Unlike most existing under-sampling methods, the majority samples are not removed only from the overlapped regions; the classifier performance determines the desired regions for eliminating the majority samples. Extensive experiments conducted on 66 synthetic and 24 real-world datasets with different imbalance ratios and overlapping degrees and two large high-dimensional datasets show a significant performance improvement from the proposed method compared to the competitors.},
  archive      = {J_PR},
  author       = {Paria Soltanzadeh and M. Reza Feizi-Derakhshi and Mahdi Hashemzadeh},
  doi          = {10.1016/j.patcog.2023.109721},
  journal      = {Pattern Recognition},
  pages        = {109721},
  shortjournal = {Pattern Recognition},
  title        = {Addressing the class-imbalance and class-overlap problems by a metaheuristic-based under-sampling approach},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic scene deblurring with continuous cross-layer
attention transmission. <em>PR</em>, <em>143</em>, 109719. (<a
href="https://doi.org/10.1016/j.patcog.2023.109719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep convolutional neural networks (CNNs) using attention mechanism have achieved great success for dynamic scene deblurring. In most of these networks, only the features refined by the attention maps can be passed to the next layer and the attention maps of different layers are separated from each other, which does not make full use of the attention information from different layers in the CNN. To address this problem, we introduce a new continuous cross-layer attention transmission (CCLAT) mechanism that can exploit hierarchical attention information from all the convolutional layers . Based on the CCLAT mechanism, we use a very simple attention module to construct a novel residual dense attention fusion block (RDAFB). In RDAFB, the attention maps inferred from the outputs of the preceding RDAFB and each layer are directly connected to the subsequent ones, leading to a CCLAT mechanism. Taking RDAFB as the building block , we design an effective architecture for dynamic scene deblurring named RDAFNet. The experiments on benchmark datasets show that the proposed model outperforms the state-of-the-art deblurring approaches, and demonstrate the effectiveness of CCLAT mechanism. The source code is available on: https://github.com/xjmz6/RDAFNet.},
  archive      = {J_PR},
  author       = {Xia Hua and Mingxin Li and Junxiong Fei and Jianguo Liu and Yu Shi and Hanyu Hong},
  doi          = {10.1016/j.patcog.2023.109719},
  journal      = {Pattern Recognition},
  pages        = {109719},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic scene deblurring with continuous cross-layer attention transmission},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal transport based pyramid graph kernel for autism
spectrum disorder diagnosis. <em>PR</em>, <em>143</em>, 109716. (<a
href="https://doi.org/10.1016/j.patcog.2023.109716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain network, which characterizes the functional and structural interactions of brain regions with graph theory, has been widely utilized to diagnose brain diseases, such as autism spectrum disorder (ASD). It is a challenge to measure the network (or graph) similarity in brain network analysis . Graph kernel (i.e., kernel defined on graphs) offers an efficient tool for measuring the similarity of paired brain networks and yields the excellent classification performance in brain disease diagnosis. However, most of the existing graph kernels neglected the hierarchical architecture information of brain networks. To address this problem, in this paper, we propose an optimal transport based pyramid graph kernel for measuring brain network similarity and then apply it to brain disease classification. The main idea is to transform brain networks into pyramid structures, which reflect the hierarchical architecture information of the brain network with multi-resolution histograms. The optimal transport distance in pyramid structures is calculated for measuring transport costs between paired brain networks. Finally, the optimal transport based pyramid graph kernel is computed based on this optimal transport distance. To evaluate the effectiveness of the proposed optimal transport based pyramid graph kernel, the extensive experiments are performed in functional magnetic resonance imaging data of brain disease from the Autism Brain Imaging Data Exchange database. The experimental results show that our proposed optimal transport based pyramid graph kernel outperforms the state-of-the-art methods in ASD classification tasks .},
  archive      = {J_PR},
  author       = {Kai Ma and Shuo Huang and Peng Wan and Daoqiang Zhang},
  doi          = {10.1016/j.patcog.2023.109716},
  journal      = {Pattern Recognition},
  pages        = {109716},
  shortjournal = {Pattern Recognition},
  title        = {Optimal transport based pyramid graph kernel for autism spectrum disorder diagnosis},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Longitudinal prediction of postnatal brain magnetic
resonance images via a metamorphic generative adversarial network.
<em>PR</em>, <em>143</em>, 109715. (<a
href="https://doi.org/10.1016/j.patcog.2023.109715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing scans are inevitable in longitudinal studies due to either subject dropouts or failed scans. In this paper, we propose a deep learning framework to predict missing scans from acquired scans, catering to longitudinal infant studies. Prediction of infant brain MRI is challenging owing to the rapid contrast and structural changes particularly during the first year of life. We introduce a trustworthy metamorphic generative adversarial network (MGAN) for translating infant brain MRI from one time point to another. MGAN has three key features: (i) Image translation leveraging spatial and frequency information for detail-preserving mapping; (ii) Quality-guided learning strategy that focuses attention on challenging regions. (iii) Multi-scale hybrid loss function that improves translation of image contents. Experimental results indicate that MGAN outperforms existing GANs by accurately predicting both tissue contrasts and anatomical details.},
  archive      = {J_PR},
  author       = {Yunzhi Huang and Sahar Ahmad and Luyi Han and Shuai Wang and Zhengwang Wu and Weili Lin and Gang Li and Li Wang and Pew-Thian Yap},
  doi          = {10.1016/j.patcog.2023.109715},
  journal      = {Pattern Recognition},
  pages        = {109715},
  shortjournal = {Pattern Recognition},
  title        = {Longitudinal prediction of postnatal brain magnetic resonance images via a metamorphic generative adversarial network},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inter-layer transition in neural architecture search.
<em>PR</em>, <em>143</em>, 109697. (<a
href="https://doi.org/10.1016/j.patcog.2023.109697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) attracts much research attention, contributing to its ability to identify better architectures than manually-designed ones. Recently, differential neural architecture search methods have been widely used due to their impressive effectiveness and performance. They represent the network architecture as a repetitive proxy-directed acyclic graph (DAG) and optimize the network weights and architecture weights alternatively in a differential manner. However, existing methods model the architecture weights on each edge (i.e., a layer in the network) as statistically independent variables, ignoring the dependency between edges in DAG induced by their directed topological connections. In this paper, we make the first attempt to investigate such a dependency or relationship by proposing a novel inter-layer transition NAS method. It casts the architecture optimization into a sequential decision process where the dependency between the architecture weights of connected edges is explicitly modeled. Specifically, edges are divided into inner and outer groups according to whether or not their predecessor edges are in the same cell. While the architecture weights of outer edges are optimized independently, those of inner edges are derived sequentially based on the architecture weights of their predecessor edges and the learnable transition matrices in an attentive probability transition manner. Experiments on five benchmark classification datasets, four searching spaces, and NAS-Bench-201 confirm the value of modeling inter-layer dependency and demonstrate the proposed method outperforms other methods.},
  archive      = {J_PR},
  author       = {Benteng Ma and Jing Zhang and Yong Xia and Dacheng Tao},
  doi          = {10.1016/j.patcog.2023.109697},
  journal      = {Pattern Recognition},
  pages        = {109697},
  shortjournal = {Pattern Recognition},
  title        = {Inter-layer transition in neural architecture search},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An active foveated gaze prediction algorithm based on a
bayesian ideal observer. <em>PR</em>, <em>143</em>, 109694. (<a
href="https://doi.org/10.1016/j.patcog.2023.109694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting human eye movements is a crucial task for understanding human behavior and has numerous applications in machine vision. Most current models for predicting eye movements are data-driven and require large datasets of recorded eye movements, which can be expensive and time-consuming to collect. In this paper, we present a novel theory-based model for predicting eye movements in a foveated visual system that maximizes information gain at each fixation. Our model uses a region-proposal network and eccentricity-based max pooling to account for the loss of detail in peripheral vision . We apply our model to predict human fixations in a visual search task for objects in real-world scenes. Unlike data-driven models, our model does not require training on large eye movement datasets and can generalize to any set of natural images and targets. We evaluate the generalization capability of our model by demonstrating its results on two publicly available visual search datasets, Ehinger and COCO-search18, without any further training on those datasets. Our model outperforms or performs comparably to data-driven models that are directly trained on human eye movement datasets.},
  archive      = {J_PR},
  author       = {Shima Rashidi and Weilun Xu and Dian Lin and Andrew Turpin and Lars Kulik and Krista Ehinger},
  doi          = {10.1016/j.patcog.2023.109694},
  journal      = {Pattern Recognition},
  pages        = {109694},
  shortjournal = {Pattern Recognition},
  title        = {An active foveated gaze prediction algorithm based on a bayesian ideal observer},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Local nonlinear dimensionality reduction via preserving the
geometric structure of data. <em>PR</em>, <em>143</em>, 109663. (<a
href="https://doi.org/10.1016/j.patcog.2023.109663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction has many applications in data visualization and machine learning . Existing methods can be classified into global ones and local ones. The global methods usually learn the linear relationship in data, while the local ones learn the manifold intrinsic geometry structure, which has a significant impact on pattern recognition. However, most of existing local methods obtain an embedding with eigenvalue or singular value decomposition , where the computational complexities are very high in a large amount of high-dimensional data. In this paper, we propose a local nonlinear dimensionality reduction method named Vec2vec , which employs a neural network with only one hidden layer to reduce the computational complexity. We first build a neighborhood similarity graph from the input matrix, and then define the context of data points with the random walk properties in the graph. Finally, we train the neural network with the context of data points to learn the embedding of the matrix. We conduct extensive experiments of data classification and clustering on nine image and text datasets to evaluate the performance of our method. Experimental results show that Vec2vec is better than several state-of-the-art dimensionality reduction methods, except that it is equivalent to UMAP on data clustering tasks in the statistical hypothesis tests, but Vec2vec needs less computational time than UMAP in high-dimensional data. Furthermore, we propose a more lightweight method named Approximate Vec2vec (AVec2vec) with little performance degradation , which employs an approximate method to build the neighborhood similarity graph. AVec2vec is still better than some state-of-the-art local dimensionality reduction methods and competitive with UMAP on data classification and clustering tasks in the statistical hypothesis tests.},
  archive      = {J_PR},
  author       = {Xiang Wang and Junxing Zhu and Zichen Xu and Kaijun Ren and Xinwang Liu and Fengyun Wang},
  doi          = {10.1016/j.patcog.2023.109663},
  journal      = {Pattern Recognition},
  pages        = {109663},
  shortjournal = {Pattern Recognition},
  title        = {Local nonlinear dimensionality reduction via preserving the geometric structure of data},
  volume       = {143},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lie group kernel learning method for medical image
classification. <em>PR</em>, <em>142</em>, 109735. (<a
href="https://doi.org/10.1016/j.patcog.2023.109735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image classification is a basic step in medical image analysis and has been an essential task in computer-aided diagnosis. Existing classification methods are proved to be effective in conventional image classification tasks, but they often achieve a suboptimal performance when applied to medical images characterizing by complex nonlinear variation. Aiming at this challenge, this paper proposes a Lie group kernel learning method for medical image classification by combining Lie group theory, kernel functions , SVM and KNN classifiers . The method represents each image with a Lie group feature descriptor constructed from low-level features and builds a SVM classifier from the training images. Geodesic distances between categorical pivots and each testing image are calculated with Lie group kernel functions to select either the SVM or a KNN classifier to do the classification. The proposed method is applied to three medical image datasets and the results demonstrate the efficacy of the method.},
  archive      = {J_PR},
  author       = {Li Liu and Haocheng Sun and Fanzhang Li},
  doi          = {10.1016/j.patcog.2023.109735},
  journal      = {Pattern Recognition},
  pages        = {109735},
  shortjournal = {Pattern Recognition},
  title        = {A lie group kernel learning method for medical image classification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Β-random walk: Collaborative sampling and weighting
mechanisms based on a single parameter for node embeddings. <em>PR</em>,
<em>142</em>, 109730. (<a
href="https://doi.org/10.1016/j.patcog.2023.109730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph embedding transforms a graph into vector representations to facilitate subsequent graph-analytic tasks. Existing graph embedding methods ignore efficient node sampling and intelligent node weighting, leading to a weak node representation. This paper introduces the β β -random walk model with two main contributions. Firstly, the traditional random walk sampling reveals instability. Thus, we associate a parameter β β with each node to balance and stabilize the sampling process, producing high-efficient trajectories. Secondly, we design a weighting mechanism that incorporates these trajectories to generate accurate representations. The designed mechanism models the behavior of each node contextually at each episode, considering the current state and the previous weights to produce the next episode’s weights. The parameter β β optimizes the node weights by simulating multiple high-order proximity walks from each node. This approach provides summarized insights about each node’s behavior and its neighbors’ context, which enables a consistent discovery of prominent paths variation in the graph. Experimental results demonstrate that the β β -random walk outperforms the state-of-the-art baselines in handling small and large graphs.},
  archive      = {J_PR},
  author       = {Badr Hirchoua and Saloua El Motaki},
  doi          = {10.1016/j.patcog.2023.109730},
  journal      = {Pattern Recognition},
  pages        = {109730},
  shortjournal = {Pattern Recognition},
  title        = {β-random walk: Collaborative sampling and weighting mechanisms based on a single parameter for node embeddings},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking the unpretentious u-net for medical ultrasound
image segmentation. <em>PR</em>, <em>142</em>, 109728. (<a
href="https://doi.org/10.1016/j.patcog.2023.109728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast tumor segmentation from ultrasound images is one of the key steps that help us characterize and localize tumor regions. However, variable tumor morphology, blurred boundaries, and similar intensity distributions bring challenges for radiologists to segment breast tumors manually. During clinical diagnosis, there are higher demands on the segmentation accuracy and efficiency of breast ultrasound images, so there is an urgent need for an automated method to improve the segmentation accuracy as a technical tool to assist diagnosis. Inspired by the U-net and its many variations, this paper proposed an unpretentious nested U-net (NU-net) for accurate and efficient breast tumor segmentation. The key idea is to utilize U-nets with different depths and shared weights to achieve robust characterization of breast tumors. Specifically, we first utilize the deeper U-net (fifteen layers) as the backbone network to extract more sufficient breast tumor features. Then, we developed a multi-output U-net to be taken as the bond between the encoder and the decoder to enhance the network adaptability for breast tumors with different scales. Finally, the short-connection based on multi-step down-sampling is used to enhance the correlation of long-range information of encoded features. Extensive experimental results with fifteen state-of-the-art segmentation methods on three public breast ultrasound datasets demonstrate that our method has a more competitive segmentation performance on breast tumors. Furthermore, the robustness of our approach is further illustrated by the segmentation of renal ultrasound images. The source code is publicly available on https://github.com/CGPxy/NU-net .},
  archive      = {J_PR},
  author       = {Gongping Chen and Lei Li and Jianxun Zhang and Yu Dai},
  doi          = {10.1016/j.patcog.2023.109728},
  journal      = {Pattern Recognition},
  pages        = {109728},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking the unpretentious U-net for medical ultrasound image segmentation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attentional prototype inference for few-shot segmentation.
<em>PR</em>, <em>142</em>, 109726. (<a
href="https://doi.org/10.1016/j.patcog.2023.109726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to address few-shot segmentation. While existing prototype-based methods have achieved considerable success, they suffer from uncertainty and ambiguity caused by limited labeled examples. In this work, we propose attentional prototype inference (API), a probabilistic latent variable framework for few-shot segmentation. We define a global latent variable to represent the prototype of each object category, which we model as a probabilistic distribution. The probabilistic modeling of the prototype enhances the model’s generalization ability by handling the inherent uncertainty caused by limited data and intra-class variations of objects. To further enhance the model, we introduce a local latent variable to represent the attention map of each query image, which enables the model to attend to foreground objects while suppressing the background. The optimization of the proposed model is formulated as a variational Bayesian inference problem, which is established by amortized inference networks. We conduct extensive experiments on four benchmarks, where our proposal obtains at least competitive and often better performance than state-of-the-art prototype-based methods. We also provide comprehensive analyses and ablation studies to gain insight into the effectiveness of our method for few-shot segmentation.},
  archive      = {J_PR},
  author       = {Haoliang Sun and Xiankai Lu and Haochen Wang and Yilong Yin and Xiantong Zhen and Cees G.M. Snoek and Ling Shao},
  doi          = {10.1016/j.patcog.2023.109726},
  journal      = {Pattern Recognition},
  pages        = {109726},
  shortjournal = {Pattern Recognition},
  title        = {Attentional prototype inference for few-shot segmentation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpreting vulnerabilities of multi-instance learning to
adversarial perturbations. <em>PR</em>, <em>142</em>, 109725. (<a
href="https://doi.org/10.1016/j.patcog.2023.109725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-instance learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection , text classification , etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instance, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerabilities of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Furthermore, through simulations, we have demonstrated the efficacy of the proposed algorithms in fooling state-of-the-art MIL approaches, such that these models make incorrect predictions regarding the label assigned to the bag. Finally, we have discussed, through experiments, about taking care of these kind of adversarial perturbations through a simple strategy. Source codes are available at https://github.com/InkiInki/MI-UAP .},
  archive      = {J_PR},
  author       = {Yu-Xuan Zhang and Hua Meng and Xue-Mei Cao and Zhengchun Zhou and Mei Yang and Avik Ranjan Adhikary},
  doi          = {10.1016/j.patcog.2023.109725},
  journal      = {Pattern Recognition},
  pages        = {109725},
  shortjournal = {Pattern Recognition},
  title        = {Interpreting vulnerabilities of multi-instance learning to adversarial perturbations},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel classification method combining phase-field and DNN.
<em>PR</em>, <em>142</em>, 109723. (<a
href="https://doi.org/10.1016/j.patcog.2023.109723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel classification method. Firstly, we use the deep neural network (DNN) to classify the training set. After several iterations, we obtain the output vector Y Y . The component of the largest value in vector Y Y is represented as the label being classified, which we take as the output value. Because we chose the sigmoid function as our activation function , the output value is between 0 and 1. Therefore, the output value can represents the probability of the classified label by the DNN. Depending on the distribution of output values, we set tolerance values ( T o l Tol ) that categorize similar output values as the same label in the DNN. If the output value is lower than T o l Tol , we consider it categorically anomalous. Subsequently, we use the Phase-Field model to classify these anomalies and obtain better classification results . As this classification method combines Phase-Field model and DNN, we named it Phase-Field-DNN. In the numerical experiment using MNIST handwritten digit data set as experimental data, the classification accuracy of Phase-Field-DNN model is higher than that of Phase-Field model and DNN model through the analysis of the classification results of binary classification and multi-classification problems with this data. In addition, the model we proposed is used to classify the normal and abnormal brain MRIs, and the classification results are compared with those of others. After comparison, we find that our proposed model achieve the best classification results.},
  archive      = {J_PR},
  author       = {Jian Wang and Ziwei Han and Wenjing Jiang and Junseok Kim},
  doi          = {10.1016/j.patcog.2023.109723},
  journal      = {Pattern Recognition},
  pages        = {109723},
  shortjournal = {Pattern Recognition},
  title        = {A novel classification method combining phase-field and DNN},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalization capacity of multi-class SVM based on
markovian resampling. <em>PR</em>, <em>142</em>, 109720. (<a
href="https://doi.org/10.1016/j.patcog.2023.109720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalization performance of “All-in-one” Multi-class SVM (AIO-MSVM) based on uniformly ergodic Markovian chain (u.e.M.c.) samples is considered. We establish the fast learning rate of AIO-MSVM algorithm with u.e.M.c. samples and prove that AIO-MSVM algorithm with u.e.M.c. samples is consistent. We also propose a novel AIO-MSVM algorithm based on q q -times Markovian resampling (AIO-MSVM-MR), and show the numerical investigation on the learning performance of AIO-MSVM-MR based on public datasets. The experimental studies indicate that compared to the classical AIO-MSVM algorithm and other MSVM algorithms, the proposed AIO-MSVM-MR algorithm has not only smaller misclassification rate, but also less sampling and training total time. We present some discussions on the case of unbalanced training samples, the choices of q q and two technical parameters, and present some explanations on the learning performance of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Zijie Dong and Chen Xu and Jie Xu and Bin Zou and Jingjing Zeng and Yuan Yan Tang},
  doi          = {10.1016/j.patcog.2023.109720},
  journal      = {Pattern Recognition},
  pages        = {109720},
  shortjournal = {Pattern Recognition},
  title        = {Generalization capacity of multi-class SVM based on markovian resampling},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous local clustering and unsupervised feature
selection via strong space constraint. <em>PR</em>, <em>142</em>,
109718. (<a href="https://doi.org/10.1016/j.patcog.2023.109718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fashion method applied in machine learning tasks. However, high dimensional data brings many obstacles for clustering approaches . To address such a problem, the unsupervised feature selection (UFS) method can be incorporated into clustering to reduce dimensionality. In general, most of the UFS methods adopt ℓ 2 , 1 ℓ2,1 -norm for subspace sparsity learning. However, its sparsity highly relies on the setting of trade-off parameter, which may lead to instability of ranking results and the difficulty in obtaining the optimal solution of projection matrix . In this paper, we propose to directly learn an absolutely row-sparsity subspace via the ℓ 2 , 0 ℓ2,0 -norm constraint, called Sparse constraint and Local learning for Unsupervised Feature Selection (SLUFS). It is an ideal sparse subspace constraint which can overcome the drawbacks of the ℓ 2 , 1 ℓ2,1 -norm. However, optimizing the ℓ 2 , 0 ℓ2,0 -norm constraint is an NP-hard problem, and at present, only some approximate solutions can be given, but the convergence can not be guaranteed. To tackle this challenge, we design a novel alternative iterative algorithm to directly optimize the ℓ 2 , 0 ℓ2,0 -norm based model. Most importantly, our strategy can obtain a closed-form solution with strict convergence guarantee. Comprehensive experiments are conducted on several real-world datasets to evaluate the performance of SLUFS with comparison to several related state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zheng Wang and Qi Li and Haifeng Zhao and Feiping Nie},
  doi          = {10.1016/j.patcog.2023.109718},
  journal      = {Pattern Recognition},
  pages        = {109718},
  shortjournal = {Pattern Recognition},
  title        = {Simultaneous local clustering and unsupervised feature selection via strong space constraint},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Region-aware RGB and near-infrared image fusion.
<em>PR</em>, <em>142</em>, 109717. (<a
href="https://doi.org/10.1016/j.patcog.2023.109717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a region-aware fusion method, called RaIF, for RGB and near-infrared (NIR) outdoor scenery image fusion. The method is motivated by the observation that current fusion approaches produce gray appearance in overexposed sky regions and distortion in vegetation regions. RaIF generates the region probability maps by exploiting their specific characteristics in the visible and NIR spectra. It recovers the overexposed sky regions by employing the intrinsic channel correlation between RGB and NIR images, and enhances the vegetation regions in an adjustable manner. RaIF formulates image fusion problem as a gradient-domain optimization problem with luminance and chromaticity regularizations . Experimental results validate the superiority of RaIF that produces fused images with improved appearance in the sky and vegetation regions, and achieves the state-of-the-art performance quantitatively and qualitatively. Furthermore, RaIF can act as a refinement module that improves the fusion results of current deep learning based approaches. It is also capable of recovering specular highlight regions other than sky overexposure .},
  archive      = {J_PR},
  author       = {Jiacheng Ying and Can Tong and Zehua Sheng and Bowen Yao and Si-Yuan Cao and Heng Yu and Hui-Liang Shen},
  doi          = {10.1016/j.patcog.2023.109717},
  journal      = {Pattern Recognition},
  pages        = {109717},
  shortjournal = {Pattern Recognition},
  title        = {Region-aware RGB and near-infrared image fusion},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SSP-net: Scalable sequential pyramid networks for real-time
3D human pose regression. <em>PR</em>, <em>142</em>, 109714. (<a
href="https://doi.org/10.1016/j.patcog.2023.109714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a highly scalable convolutional neural networks , end-to-end trainable, for real-time 3D human pose regression from still RGB images . We call this approach Scalable Sequential Pyramid Networks (SSP-Net) as it is trained with refined supervision at multiple scales in a sequential manner. Our network requires a single training procedure and is capable of producing its best predictions at 120 frames per second (FPS), or acceptable predictions at more than 200 FPS when cut at test time. We show that the proposed regression approach is invariant to the size of feature maps, allowing our method to perform multi-resolution intermediate supervisions and reaching results comparable to the state-of-the-art with very low resolution feature maps. We demonstrate the accuracy and the effectiveness of our method by providing extensive experiments on two of the most important publicly available datasets for 3D pose estimation, Human3.6M and MPI-INF-3DHP. Additionally, we provide relevant insights about our decisions on the network architecture and show its flexibility to meet the best precision-speed compromise.},
  archive      = {J_PR},
  author       = {Diogo Carbonera Luvizon and Hedi Tabia and David Picard},
  doi          = {10.1016/j.patcog.2023.109714},
  journal      = {Pattern Recognition},
  pages        = {109714},
  shortjournal = {Pattern Recognition},
  title        = {SSP-net: Scalable sequential pyramid networks for real-time 3D human pose regression},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A multi-modal transformer network for action detection.
<em>PR</em>, <em>142</em>, 109713. (<a
href="https://doi.org/10.1016/j.patcog.2023.109713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel multi-modal transformer network for detecting actions in untrimmed videos. To enrich the action features, our transformer network utilizes a new multi-modal attention mechanism that computes the correlations between different spatial and motion modalities combinations. Exploring such correlations for actions has not been attempted previously. To use the motion and spatial modality more effectively, we suggest an algorithm that corrects the motion distortion caused by camera movement. Such motion distortion, common in untrimmed videos, severely reduces the expressive power of motion features such as optical flow fields . Our proposed algorithm outperforms the state-of-the-art methods on two public benchmarks, THUMOS14 and ActivityNet. We also conducted comparative experiments on our new instructional activity dataset, including a large set of challenging classroom videos captured from elementary schools.},
  archive      = {J_PR},
  author       = {Matthew Korban and Peter Youngs and Scott T. Acton},
  doi          = {10.1016/j.patcog.2023.109713},
  journal      = {Pattern Recognition},
  pages        = {109713},
  shortjournal = {Pattern Recognition},
  title        = {A multi-modal transformer network for action detection},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global- and local-aware feature augmentation with semantic
orthogonality for few-shot image classification. <em>PR</em>,
<em>142</em>, 109702. (<a
href="https://doi.org/10.1016/j.patcog.2023.109702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As for few-shot image classification , recently, some works revisit the standard transfer learning paradigm, i.e., pre-training and fine-tuning, and have achieved some success. However, we find that this kind of methods heavily relies on a naive image-level data augmentation (e.g., cropping and flipping) at the fine-tuning stage, which will easily suffer from the overfitting problem because of the limited-data regime. To tackle this issue, in this paper, we attempt to perform a novel feature-level semantic augmentation at the fine-tuning stage and propose a Global- and Local-aware Feature Augmentation method (GLFA) from both the channel- and spatial-wise perspectives. In addition, at the pre-training stage, we further propose a Semantic Orthogonal Learning Framework (SOLF) to make the learned feature channels more independently, orthogonal and diverse. Extensive experiments demonstrate that the proposed method can obtain significant performance improvements over the state of the arts. Code is available at https://github.com/onlyyao/GLFA-SOLF.},
  archive      = {J_PR},
  author       = {Boyao Shi and Wenbin Li and Jing Huo and Pengfei Zhu and Lei Wang and Yang Gao},
  doi          = {10.1016/j.patcog.2023.109702},
  journal      = {Pattern Recognition},
  pages        = {109702},
  shortjournal = {Pattern Recognition},
  title        = {Global- and local-aware feature augmentation with semantic orthogonality for few-shot image classification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Features kept generative adversarial network data
augmentation strategy for hyperspectral image classification.
<em>PR</em>, <em>142</em>, 109701. (<a
href="https://doi.org/10.1016/j.patcog.2023.109701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant breakthroughs have been achieved in hyperspectral image (HSI) processing using deep learning techniques, including classification, object detection, and anomaly detection . However, the practical application of deep learning in HSI processing is limited by challenges such as small-sample size and sample imbalance issues. To mitigate these limitations, we propose a novel data augmentation strategy called Feature-Preserving Generative Adversarial Network Data Augmentation (FPGANDA). What sets our data augmentation strategy apart from existing generative model-based approaches is that we preserve the main spectral bands of HSI data using a newly designed band selection method. Additionally, our proposed generative model generates synthetic spectral bands, which are combined with the real spectral bands using a mixture strategy to create augmented data. This approach ensures that the augmented data retain the main features of the original data while also incorporating diverse features from the generated data. We evaluate our method on three different HSI datasets, comparing it with state-of-the-art techniques. Experimental results demonstrate that our proposed method significantly improves classification performance in most scenes and exhibits remarkable compatibility.},
  archive      = {J_PR},
  author       = {Mingyang Zhang and Zhaoyang Wang and Xiangyu Wang and Maoguo Gong and Yue Wu and Hao Li},
  doi          = {10.1016/j.patcog.2023.109701},
  journal      = {Pattern Recognition},
  pages        = {109701},
  shortjournal = {Pattern Recognition},
  title        = {Features kept generative adversarial network data augmentation strategy for hyperspectral image classification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physical model and image translation fused network for
single-image dehazing. <em>PR</em>, <em>142</em>, 109700. (<a
href="https://doi.org/10.1016/j.patcog.2023.109700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visibility and contrast of images captured in adverse weather such as haze or fog degrade dramatically, which further hinders the accomplishment of high-level computer vision tasks such as object detection and semantic segmentation in these conditions. Many methods have been proposed to solve image dehazing problem by using image translation networks or physical model embedding in CNNs. However, the physical model cannot effectively describe the hazy generation process in complex scenes and estimating the model parameters with only a hazy image is an ill-posed problem. Image translation-based methods may lead to artefacts or colour shifts in the recovered results without the guidance or constraints of physical model information. In this paper, an end-to-end physical model and image translation fused network is proposed to generate realistic haze-free images. Since the transmission map can express the haze distribution in the scene, the proposed method adopts an encoder with a multiscale residual block to extract hazy image features, and two separate decoders to recover a clear image and to estimate the transmission map. The multiscale features of the transmission map and image translation are fused to guide the decode processes with a conditional attention feature fusion block, which is composed of sequential channelwise and spatialwise attention. Moreover, a multitask and multiscale deep supervision mechanism is adopted to enhance the feature fusion and recover more image details. The algorithm can efficiently fuse the physical model information and the hazy image translation to address the problem existent in the methods only based on physical model embedding or direct image translation. Experimental results on the visual quality enhancement of hazy images and semantic segmentation tasks in hazy scenes demonstrate that our model can efficiently recover haze-free images, while performing on par with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yan Zhao Su and Chuan He and Zhi Gao Cui and Ai Hua Li and Nian Wang},
  doi          = {10.1016/j.patcog.2023.109700},
  journal      = {Pattern Recognition},
  pages        = {109700},
  shortjournal = {Pattern Recognition},
  title        = {Physical model and image translation fused network for single-image dehazing},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperspectral image denoising via spectral noise
distribution bootstrap. <em>PR</em>, <em>142</em>, 109699. (<a
href="https://doi.org/10.1016/j.patcog.2023.109699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image (HSI) denoising is an ill-posed problem, leading to integrating proper prior knowledge about hyperspectral noise is critical to developing an efficient denoising method. Most existing methods share a common assumption that all bands have equal noise intensity. However, such assumption runs counter to the practical HSIs, leading to unpleasant denoising results. To tackle this, we intend to investigate the intrinsic properties of real HSI noise in the spectral dimension and construct a novel denoising framework bootstrapping by spectral noise distribution N ^ N^ , termed N ^ N^ -Net. On the one hand, we develop dense and sparse recurrent calculations, exploiting intrinsic properties of HSI noise ( i.e. , diversity, dense dependency, and global sparsity) to estimate spectral noise distribution. On the other hand, having the estimated spectral noise distribution, we develop a bootstrap mechanism with a repetitive emphasis on its guidance for subsequent spatial noise separation and clean HSI recovery, ensuring a more delicate denoising effect. In particular, we verify that the proposed denoising framework can achieve promising denoising performances due to the merit of spectral noise distribution bootstrapping, which also promotes new insights for future related research. The code is avaliable at https://github.com/EtPan/N-Net .},
  archive      = {J_PR},
  author       = {Erting Pan and Yong Ma and Xiaoguang Mei and Fan Fan and Jiayi Ma},
  doi          = {10.1016/j.patcog.2023.109699},
  journal      = {Pattern Recognition},
  pages        = {109699},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral image denoising via spectral noise distribution bootstrap},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dataset agnostic document object detection. <em>PR</em>,
<em>142</em>, 109698. (<a
href="https://doi.org/10.1016/j.patcog.2023.109698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Localizing document objects such as tables, figures, and equations is a primary step for extracting information from document images . We propose a novel end-to-end trainable deep network, termed Document Object Localization Network ( doln et), for detecting various objects present in the document images . The proposed network is a multi-stage extension of Mask r-cnn with a dual backbone having deformable convolution for detecting document objects with high detection accuracy at a higher IoU threshold. We also empirically evaluate the proposed doln et on the publicly available benchmark datasets. The proposed DOLNet achieves state-of-the-art performance for most of the bench-mark datasets under various existing experimental environments. Our solution has three important properties: (i) a single trained model doln et ‡ ‡ that performs well across all the popular benchmark datasets, (ii) reports excellent performances across multiple, including with higher IoU thresholds, and (iii) consistently demonstrate the superior quantitative performance by following the same protocol of the recent works for each of the benchmarks.},
  archive      = {J_PR},
  author       = {Ajoy Mondal and Madhav Agarwal and C.V. Jawahar},
  doi          = {10.1016/j.patcog.2023.109698},
  journal      = {Pattern Recognition},
  pages        = {109698},
  shortjournal = {Pattern Recognition},
  title        = {Dataset agnostic document object detection},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep image compression using scene text quality assessment.
<em>PR</em>, <em>142</em>, 109696. (<a
href="https://doi.org/10.1016/j.patcog.2023.109696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compression is a fundamental technology for Internet communication engineering. However, a high compression rate with general methods may degrade images, resulting in unreadable texts. In this paper, we propose an image compression method for maintaining text quality. We developed a scene text image quality assessment model to assess text quality in compressed images. The assessment model iteratively searches for the best-compressed image holding high-quality text. Objective and subjective results showed that the proposed method was superior to existing methods. Furthermore, the proposed assessment model outperformed other deep-learning regression models.},
  archive      = {J_PR},
  author       = {Shohei Uchigasaki and Tomo Miyazaki and Shinichiro Omachi},
  doi          = {10.1016/j.patcog.2023.109696},
  journal      = {Pattern Recognition},
  pages        = {109696},
  shortjournal = {Pattern Recognition},
  title        = {Deep image compression using scene text quality assessment},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end page-level assessment of handwritten text
recognition. <em>PR</em>, <em>142</em>, 109695. (<a
href="https://doi.org/10.1016/j.patcog.2023.109695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of Handwritten Text Recognition (HTR) systems has traditionally used metrics based on the edit distance between HTR and ground truth (GT) transcripts, at both the character and word levels. This is very adequate when the experimental protocol assumes that both GT and HTR text lines are the same, which allows edit distances to be independently computed to each given line. Driven by recent advances in pattern recognition, HTR systems increasingly face the end-to-end page-level transcription of a document, where the precision of locating the different text lines and their corresponding reading order (RO) play a key role. In such a case, the standard metrics do not take into account the inconsistencies that might appear. In this paper, the problem of evaluating HTR systems at the page level is introduced in detail. We analyse the convenience of using a two-fold evaluation, where the transcription accuracy and the RO goodness are considered separately. Different alternatives are proposed, analysed and empirically compared both through partially simulated and through real, full end-to-end experiments. Results support the validity of the proposed two-fold evaluation approach. An important conclusion is that such an evaluation can be adequately achieved by just two simple and well-known metrics: the Word Error Rate (WER), that takes transcription sequentiality into account, and the here re-formulated Bag of Words Word Error Rate (bWER), that ignores order. While the latter directly and very accurately assess intrinsic word recognition errors, the difference between both metrics ( Δ Δ WER) gracefully correlates with the Normalised Spearman’s Foot Rule Distance (NSFD), a metric which explicitly measures RO errors associated with layout analysis flaws. To arrive to these conclusions, we have introduced another metric called Hungarian Word Word Rate (hWER), based on a here proposed regularised version of the Hungarian Algorithm . This metric is shown to be always almost identical to bWER and both bWER and hWER are also almost identical to WER whenever HTR transcripts and GT references are guarantee to be in the same RO.},
  archive      = {J_PR},
  author       = {Enrique Vidal and Alejandro H. Toselli and Antonio Ríos-Vila and Jorge Calvo-Zaragoza},
  doi          = {10.1016/j.patcog.2023.109695},
  journal      = {Pattern Recognition},
  pages        = {109695},
  shortjournal = {Pattern Recognition},
  title        = {End-to-end page-level assessment of handwritten text recognition},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete analytical objects in the body-centered cubic grid.
<em>PR</em>, <em>142</em>, 109693. (<a
href="https://doi.org/10.1016/j.patcog.2023.109693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a characterization of discrete analytical spheres, planes and lines in the body-centered cubic (BCC) grid, both in the Cartesian and in the recently proposed alternative compact coordinate system, in which each integer triplet addresses some voxel in the grid. We define spheres and planes through double Diophantine inequalities and investigate their relevant topological features , such as functionality or the interrelation between the thickness of the objects and their connectivity and separation properties. We define lines as the intersection of planes. The number of the planes (up to six) is equal to the number of the pairs of faces of a BCC voxel that are parallel to the line.},
  archive      = {J_PR},
  author       = {Lidija Čomić and Gaëlle Largeteau-Skapin and Rita Zrour and Ranita Biswas and Eric Andres},
  doi          = {10.1016/j.patcog.2023.109693},
  journal      = {Pattern Recognition},
  pages        = {109693},
  shortjournal = {Pattern Recognition},
  title        = {Discrete analytical objects in the body-centered cubic grid},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-paced principal component analysis. <em>PR</em>,
<em>142</em>, 109692. (<a
href="https://doi.org/10.1016/j.patcog.2023.109692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal Component Analysis (PCA) has been widely used for dimensionality reduction and feature extraction. Robust PCA (RPCA), under different robust distance metrics, such as ℓ 1 ℓ1 -norm and ℓ 2 , p ℓ2,p -norm, can deal with noise or outliers to some extent. However, real-world data may display structures that can not be fully captured by these simple functions. In addition, existing methods treat complex and simple samples equally. By contrast, a learning pattern typically adopted by human beings is to learn from simple to complex and less to more. Based on this principle, we propose a novel method called Self-paced PCA (SPCA) to further reduce the effect of noise and outliers. Notably, the complexity of each sample is calculated at the beginning of each iteration in order to integrate samples from simple to more complex into training. Based on an alternating optimization, SPCA finds an optimal projection matrix and filters out outliers iteratively. Theoretical analysis is presented to show the rationality of SPCA. Extensive experiments on popular data sets demonstrate that the proposed method can improve the state-of-the-art results considerably.},
  archive      = {J_PR},
  author       = {Zhao Kang and Hongfei Liu and Jiangxin Li and Xiaofeng Zhu and Ling Tian},
  doi          = {10.1016/j.patcog.2023.109692},
  journal      = {Pattern Recognition},
  pages        = {109692},
  shortjournal = {Pattern Recognition},
  title        = {Self-paced principal component analysis},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale correlation module for video-based facial
expression recognition in the wild. <em>PR</em>, <em>142</em>, 109691.
(<a href="https://doi.org/10.1016/j.patcog.2023.109691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of facial muscle movements (e.g., mouth opening) is crucial for facial expression recognition (FER). However, extracting these facial motion features is challenging for a deep-learning recognition system for the following reasons: (1) without explicit labels of motion for training, there is no guarantee that convolutional neural networks (CNNs) can extract motions effectively; (2) compared to human action recognition (e.g., the object moving from left to right), some facial motions (e.g., raising eyebrows) are more subtle and thus harder to extract; and (3) the use of optical flow to extract motion features is time-consuming when using a commonly-used camera. In this work, we propose a Multi-Scale Correlation Module (MSCM) together with an adaptive fusion. Firstly, large as well as small facial motions are extracted by MSCM and encoded by CNNs. Then, an adaptive fusion module is used to aggregate motion features. With these modules, our recognition network is able to model both subtle and large motion features for video-based FER with only the RGB image frames as input. Experiments on two datasets, AFEW and DFEW, show that the network achieves state-of-art performances on the benchmarks.},
  archive      = {J_PR},
  author       = {Tankun Li and Kwok-Leung Chan and Tardi Tjahjadi},
  doi          = {10.1016/j.patcog.2023.109691},
  journal      = {Pattern Recognition},
  pages        = {109691},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale correlation module for video-based facial expression recognition in the wild},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Extending version-space theory to multi-label active
learning with imbalanced data. <em>PR</em>, <em>142</em>, 109690. (<a
href="https://doi.org/10.1016/j.patcog.2023.109690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Version space, defined as the subset of the hypothesis space consistent with the training samples, is an important concept in supervised learning. It has been successfully applied for evaluating the informativeness of unlabeled samples in traditional single-label active learning. Specifically, the most inconsistent samples among the version space members can reduce the size of the version space as fast as possible, these samples are given high priority for domain expert annotation, thereby the learner can construct a high-performance classifier by labeling as few samples as possible. We point out that the concept of version space has not been extended to multi-label environments yet, which hinders its application in multi-label active learning. This paper makes an attempt to extend the version space theory from single-label scenario to multi-label scenario, builds up a spatial structure for the multi-label version space, generalizes it from finite case to infinite case, puts forward a simplified representation for it and accordingly proposes a new multi-label active learning algorithm. Moreover, considering the imbalance issue in multi-label data, the algorithm is further improved by allocating different annotation numbers to the labels. Experimental comparisons verify the feasibility and effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Ran Wang and Shuyue Chen and Yu Yu},
  doi          = {10.1016/j.patcog.2023.109690},
  journal      = {Pattern Recognition},
  pages        = {109690},
  shortjournal = {Pattern Recognition},
  title        = {Extending version-space theory to multi-label active learning with imbalanced data},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new methodology in constructing no-reference focus quality
assessment metrics. <em>PR</em>, <em>142</em>, 109688. (<a
href="https://doi.org/10.1016/j.patcog.2023.109688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a new methodology which converts a full-reference focus quality assessment metric into a no-reference one. The methodology consists of three hypotheses which describe the relationship in focus quality between the original image and its variants. Using the proposed methodology, two no-reference metrics were constructed. The first used Brenner Gradient and the second used a full-reference metric proposed by ourselves. Evaluation was conducted on a public dataset and our own proposed dataset. Comparing with other no-reference metrics, our second one exhibited best performance on both datasets, with calculation time comparable to some fastest metrics considered.},
  archive      = {J_PR},
  author       = {Jie Song and Mengjun Liu},
  doi          = {10.1016/j.patcog.2023.109688},
  journal      = {Pattern Recognition},
  pages        = {109688},
  shortjournal = {Pattern Recognition},
  title        = {A new methodology in constructing no-reference focus quality assessment metrics},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Triplet teaching graph contrastive networks with
self-evolving adaptive augmentation. <em>PR</em>, <em>142</em>, 109687.
(<a href="https://doi.org/10.1016/j.patcog.2023.109687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised graph contrastive learning has recently emerged as the solution to the crisis of label information scarcity for graph data in the real world. However, from the general paradigm of graph contrastive learning , most of the existing methods are still flawed in the design and use of augmented views and the design of contrastive targets. Therefore, the works on how to generate reasonable augmented views and utilize them canonically and how to construct efficient and comprehensive contrastive objectives are very meaningful. Based on the teaching concept, this paper proposes a new triplet teaching graph contrastive network with self-evolving adaptive augmentation. Firstly, after carefully analyzing the internal relationships between different augmented perspectives, we present a triple teaching graph neural network framework based on the improved triplet idea. It creates contrastive objectives depending on different contrastive angle levels, providing thorough guidance for graph encoders. Secondly, a self-evolving adaptive graph augmentation scheme based on topology and feature information is proposed. It is worth mentioning that with the continuous deepening of the training process, the scheme can utilize the learnable self-attention mechanism to constantly supply our network framework with an increasing number of reliable augmented views as input. Finally, when designing the contrastive objectives, we introduce a stochastic hybrid module to mine the unexploited information, which opportunely complements the contrastive sample space formed by our network framework. Furthermore, extensive experiments on multiple real-world node classification datasets demonstrate that our model can generate better-quality node embedding for downstream tasks. The implementation of this paper is available at https://github.com/PaperMiao/T-GCSA .},
  archive      = {J_PR},
  author       = {Jiaxing Miao and Feilong Cao and Ming Li and Bing Yang and Hailiang Ye},
  doi          = {10.1016/j.patcog.2023.109687},
  journal      = {Pattern Recognition},
  pages        = {109687},
  shortjournal = {Pattern Recognition},
  title        = {Triplet teaching graph contrastive networks with self-evolving adaptive augmentation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighborhood-based credibility anchor learning for universal
domain adaptation. <em>PR</em>, <em>142</em>, 109686. (<a
href="https://doi.org/10.1016/j.patcog.2023.109686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Universal domain adaptation (UniDA) aims to transfer knowledge from the source domain to the target domain in the presence of distribution shift and class mismatch. Most existing works design threshold-relied methods to reject target private classes by carefully-proposed uncertainty scoring functions which are very sensitive to thresholds. To overcome this problem, a few threshold-free methods are proposed but ignore the neighborhood structure information of the target domain, leading to poor performance. In this paper, we propose Neighborhood-based Credibility Anchor Learning (NCAL), a new threshold-free framework that fully mines the neighborhood structure information to explore better target representations. NCAL contains three key components: a class anchor learning module to learn target class distribution, a credibility-weighted conditional adversarial module to learn class-invariant features of common classes, and an open-set neighborhood clustering module to learn well-clustered features. Extensive experiments demonstrate that our method outperforms the state-of-the-art.},
  archive      = {J_PR},
  author       = {Wan Su and Zhongyi Han and Rundong He and Benzheng Wei and Xueying He and Yilong Yin},
  doi          = {10.1016/j.patcog.2023.109686},
  journal      = {Pattern Recognition},
  pages        = {109686},
  shortjournal = {Pattern Recognition},
  title        = {Neighborhood-based credibility anchor learning for universal domain adaptation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning an artificial neural network to discover
bit-quad-based formulas to compute basic object properties. <em>PR</em>,
<em>142</em>, 109685. (<a
href="https://doi.org/10.1016/j.patcog.2023.109685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape analysis requires estimating object properties in many applications, including optical character recognition , tumor classification, skin cancer recognition, leaf and plant identification, and cell analysis. In particular, bit-quad-based linear expressions proposed by Gray and Duda for calculating the area and perimeter of binary images are widely used in the literature. Nevertheless, these formulas require computing 14 or 15 bit-quad patterns out of 16 possible, becoming critical in applications with limited computing resources. Hence, this paper introduces a method based on a single-layer artificial neural network (ANN) to discover new expressions to calculate the area and perimeter with fewer bit-quads than the original formulas without losing measuring accuracy. Besides, an iterative elimination process removes irrelevant bit-quads whose corresponding weights approach zero. After that, an inductive analysis from observing the learned weights provides interpretable formulas for the area and perimeter. Furthermore, the proposed approach is also applied to find bit-quad-based formulas for directly computing the contact perimeter property, whose original formula requires precomputing Gray’s area and perimeter of the object. In addition, aiming to show our method’s versatility in other applications, we address a real-world problem to discover a bit-quad-based formula to distinguish between two classes of loss of bone density caused by hyperthyroidism and aging in rats. The experimental results show that the proposed approach reduces by approximately half the bit-quads needed to calculate the area and perimeter of Gray and Duda. Likewise, the number of bit-quads to compute the contact perimeter is reduced from nine to six. Besides, the estimated value on test sets by all the found formulas is the same as their original counterparts. On the other hand, the classification of loss of bone density type using the found bit-quad-based formula reaches an accuracy of 100\% in the test set. Therefore, the proposed method is an alternative to finding linear expressions with few bit-quads to measure basic object properties.},
  archive      = {J_PR},
  author       = {Fernando Arce and Wilfrido Gómez-Flores and Uriel Escalona and Humberto Sossa},
  doi          = {10.1016/j.patcog.2023.109685},
  journal      = {Pattern Recognition},
  pages        = {109685},
  shortjournal = {Pattern Recognition},
  title        = {Learning an artificial neural network to discover bit-quad-based formulas to compute basic object properties},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Truncated attention-aware proposal networks with multi-scale
dilation for temporal action detection. <em>PR</em>, <em>142</em>,
109684. (<a href="https://doi.org/10.1016/j.patcog.2023.109684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting actions temporally in untrimmed videos is very challenging, and it accomplishes action classification and localization simultaneously. Capturing the relations among action proposals (i.e., candidate video segments) is of vital importance. While there have been several attempts to encode such relations, they neglect the adverse effects of those irrelevant or negative relations among proposals. Besides, there is a crucial fact that action durations are flexible in videos, which has not been well explored. For the former, we develop a truncated attention mechanism that learns positive proposal relations by dynamically adjusting edge weights of proposal nodes in a graph, and construct the proposal network model using graph convolution networks to suppress disadvantageous relations of proposal pairs by truncating negative attention scores . For the latter, we devise a light multi-scale dilation module shared by all proposals to handle different action durations by enlarging temporal receptive field, thus capturing temporal context to increase the representation capacity of proposals. Unifying these considerations, we present the Multi-scale Dilation based Truncated Attention Proposal Network (MD-TAPN) model for temporal action detection. Our model achieves state-of-the-art performances of detecting actions on two benchmark databases, and especially it outperforms the most competitive method by a significant gain of 3.6\% mAP at tIoU0.5 on THUMOS14.},
  archive      = {J_PR},
  author       = {Ping Li and Jiachen Cao and Li Yuan and Qinghao Ye and Xianghua Xu},
  doi          = {10.1016/j.patcog.2023.109684},
  journal      = {Pattern Recognition},
  pages        = {109684},
  shortjournal = {Pattern Recognition},
  title        = {Truncated attention-aware proposal networks with multi-scale dilation for temporal action detection},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Determining the trustworthiness of DNNs in classification
tasks using generalized feature-based confidence metric. <em>PR</em>,
<em>142</em>, 109683. (<a
href="https://doi.org/10.1016/j.patcog.2023.109683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the confidence of Deep Neural Networks in predictions is crucial for building reliable and robust systems. However, it has received minor attention among other areas related to Deep Learning . The confidence of DNNs in predictions is highly correlated with their ability in feature extraction. Consequently, a more robust feature extractor in DNNs leads to a more confident and trustworthy model. In this study, a method is designed in order to determine the trustworthiness of DNNs based on the quality of their feature extraction components. The concept of feature quality is defined based on the models’ confidence in predictions. In a situation where two DNNs have approximately the same accuracy, the superior model has more confidence in its predictions. Hence, it is less influenced by overfitting, making it more robust and reliable in unseen and noisy environments . Determining such a model is not always possible with the well-known accuracy metric. Accordingly, a novel metric named Generalized Feature-Based Confidence Metric is proposed, which is capable of profoundly evaluating the models’ confidence in predictions. It analyzes layer-by-layer feature vectors generated by DNNs and evaluates their quality. Altogether, these utilities boost assessing and comparing different models with varying widths and depths, improving them, and picking the best one. The practicality of the proposed method and metric is investigated through four significantly diverse case studies and empirically proved. Three of them are reputable benchmarking datasets , namely, CIFAR-10, CIFAR-100, and Fashion-MNIST. Moreover, a new high-quality dataset for the Hand Rubbing problem (made by the authors) is used to analyze the proposed method’s performance in a real-world application. Overall, the proposed metric is able to distinguish between different models from about 1\% to 8\% in terms of confidence in predictions where the models possess almost the same accuracy (0.5\% difference or lower).},
  archive      = {J_PR},
  author       = {Mohammad Amin Haghpanah and Mehdi Tale Masouleh and Ahmad Kalhor},
  doi          = {10.1016/j.patcog.2023.109683},
  journal      = {Pattern Recognition},
  pages        = {109683},
  shortjournal = {Pattern Recognition},
  title        = {Determining the trustworthiness of DNNs in classification tasks using generalized feature-based confidence metric},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GlobalAP: Global average precision optimization for person
re-identification. <em>PR</em>, <em>142</em>, 109682. (<a
href="https://doi.org/10.1016/j.patcog.2023.109682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Average Precision (AP) measures the overall performance on the Person Re-Identification (ReID) task. Optimizing AP using all instances in the training set is accordingly an excellent choice for learning a discriminative ReID model. However, exploiting this method directly is unacceptable in practice due to the high cost of computation on the entire dataset. To this end, this paper proposes an effective and easy-to-use approach called GlobalAP that optimizes AP globally at negligible computational cost. More specifically, GlobalAP adopts a memory module to acquire the embedding features of all instances in the training set. To reduce the required computational complexity , GlobalAP utilizes only a few instances with high similarities to the query, to compute AP; this is because we observe that only these instances significantly affect AP and model optimization. Moreover, we propose to gradually increase the difficulty of GlobalAP to further encourage intra-class compactness and inter-class separability. Ultimately, GlobalAP can globally optimize AP and dramatically boost the model performance at negligible computational cost. We evaluate GlobalAP on six large-scale ReID datasets. Experimental results show that GlobalAP exhibits obvious advantages in terms of both computational efficiency and ReID accuracy.},
  archive      = {J_PR},
  author       = {Yifei Liu and Yaling Liang and Pengfei Wang and Ziheng Chen and Changxing Ding},
  doi          = {10.1016/j.patcog.2023.109682},
  journal      = {Pattern Recognition},
  pages        = {109682},
  shortjournal = {Pattern Recognition},
  title        = {GlobalAP: Global average precision optimization for person re-identification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FBN: Federated bert network with client-server architecture
for cross-lingual signature verification. <em>PR</em>, <em>142</em>,
109681. (<a href="https://doi.org/10.1016/j.patcog.2023.109681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online signature verification has a great challenge due to the poor performance of deep learning techniques on cross-lingual datasets under privacy constraints. In this paper, we propose a novel Federated Bert Network (FBN) by embedding the Bidirectional Encoder Representations from Transformers (Bert) into a Federated Learning (FL) framework with client-server architecture. A new Length Alignment Algorithm is employed to unify the signature pairs’ sequence length, and the input representations are fed into the different clients to complete the independent learning of local-models. In addition, the server (coordinator) uses the improved Federated Average Algorithm with Reward-Punishment Mechanism (FedAvgRP) to aggregate these local-models and further generate a global-model. After multiple iterations, the optimal model can be obtained and cross-tested on four datasets (SVC 2004, MCYT-330, BioecurID, and Ours) with skilled forged (random forged) EERs of 7.65\% (4.76\%), 10.73\% (8.46\%), 10.09\% (7.13\%), and 8.28\% (5.74\%), respectively, far higher than that of the independent learning of state-of-the-art methods. Compared with the domain adaptation and improved FL models, our FBN model performs best in random and skilled forgery scenarios. Moreover, the FedAvgRP algorithm helps our model maintain high performance in the face of data attacks.},
  archive      = {J_PR},
  author       = {Liyang Xie and Zhongcheng Wu and Xian Zhang and Yong Li},
  doi          = {10.1016/j.patcog.2023.109681},
  journal      = {Pattern Recognition},
  pages        = {109681},
  shortjournal = {Pattern Recognition},
  title        = {FBN: Federated bert network with client-server architecture for cross-lingual signature verification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NRPose: Towards noise resistance for multi-person pose
estimation. <em>PR</em>, <em>142</em>, 109680. (<a
href="https://doi.org/10.1016/j.patcog.2023.109680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high signal-to-noise ratio is one of the main challenges of multi-person pose estimation (MPE) and receives little attention. In this work, we find that MPE suffers from two types of noise: aleatoric noise and epistemic noise. The former represents the noise inherent in the observations, such as the background. The latter indicates the noise brought by the priori hypotheses, such as the inappropriate keypoint relations. Both of them reduce the saliency of information available for keypoint localization . We propose the noise-resistance pose estimation (NRPose) that integrates keypoint-oriented region proposal module (KRPM) and pose-aware sparse relation module (PSRM). To mitigate aleatoric noise, KRPM generates keypoint-level RoIs by circumscribing semantically significant regions. To reduce epistemic noise, PSRM filters out the noisy relations dynamically by modeling the noise propagation and keypoint interaction. NRPose outperforms the state-of-the-art methods by at least 1.0 AP on COCO and OCHuman dataset.},
  archive      = {J_PR},
  author       = {Jianhang He and Junyao Sun and Qiong Liu and Shaowu Peng},
  doi          = {10.1016/j.patcog.2023.109680},
  journal      = {Pattern Recognition},
  pages        = {109680},
  shortjournal = {Pattern Recognition},
  title        = {NRPose: Towards noise resistance for multi-person pose estimation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-semantic hypergraph neural network for effective
few-shot learning. <em>PR</em>, <em>142</em>, 109677. (<a
href="https://doi.org/10.1016/j.patcog.2023.109677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Graph-based Few-Shot Learning (FSL) methods exhibit good generalization by mining relations among few samples with Graph Neural Networks . However, most Graph-based FSL methods consider only binary relations and ignore the multi-semantic information of the global context knowledge. We propose a framework of Multi-Semantic Hypergraph for FSL (MSH-FSL) to explore complex latent high-order multi-semantic relations among the few samples. By mining the complex relationship structure of multi-node and multi-semantics, more refined feature representation can be learned, which yields better classification robustness. Specifically, we first construct a novel Multi-Semantic Hypergraph by obtaining associated instances with different semantic features via orthogonal mapping. With the constructed hypergraph, we then develop the Hyergraph Neural Network along with a novel multi-generation hypergraph message passing so as to better leverage the complex latent semantic relations among samples. Finally, after a number of generations, the hyper-node representations embedded in the learned hypergraph become more accurate for obtaining few-shot prediction. In the 5-way 1-shot task of ResNet-12 on mini-Imagenet dataset, the multi-semantic hypergraph outperforms single-semantic graph by 3.1\%, and with the proposed semantic-distribution message passing, the improvement can further reach 6.1\%.},
  archive      = {J_PR},
  author       = {Hao Chen and Linyan Li and Fuyuan Hu and Fan Lyu and Liuqing Zhao and Kaizhu Huang and Wei Feng and Zhenping Xia},
  doi          = {10.1016/j.patcog.2023.109677},
  journal      = {Pattern Recognition},
  pages        = {109677},
  shortjournal = {Pattern Recognition},
  title        = {Multi-semantic hypergraph neural network for effective few-shot learning},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust unsupervised feature selection via data relationship
learning. <em>PR</em>, <em>142</em>, 109676. (<a
href="https://doi.org/10.1016/j.patcog.2023.109676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection robust to many outliers is a challenging task. The crucial difficulty is learning a robust subspace, which preserves local structure. The most common solution is to reduce fitting error by applying different robust norms. However, there are three shortcomings. Firstly, they are not robust enough when outliers distributed both randomly and concentratedly are widely present. Secondly, outlier removal is not considered. Thirdly, it is not easy to understand and choose an euclidean distance threshold that decides a sample as an outlier in different scenarios. The first two shortcomings make previous methods fail to achieve their expected learning results, and the third one increases the application difficulty in different fields. To address these issues, a robust unsupervised feature selection via data relationship learning (RUFSDR) is proposed in this paper. Specifically, scores representing the data’s importance will be learned and assigned to each sample. Inliers will be given different positive scores. Outliers will be given 0 such that a subspace, which preserves the local structure better, can be learned without prior knowledge about the distance threshold. The experiments conducted on various datasets with several scenarios show the superiority of RUFSDR.},
  archive      = {J_PR},
  author       = {Pei Huang and Zhaoming Kong and Mengying Xie and Xiaowei Yang},
  doi          = {10.1016/j.patcog.2023.109676},
  journal      = {Pattern Recognition},
  pages        = {109676},
  shortjournal = {Pattern Recognition},
  title        = {Robust unsupervised feature selection via data relationship learning},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GARNet: Global-aware multi-view 3D reconstruction network
and the cost-performance tradeoff. <em>PR</em>, <em>142</em>, 109674.
(<a href="https://doi.org/10.1016/j.patcog.2023.109674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning technology has made great progress in multi-view 3D reconstruction tasks. At present, the mainstream solutions adopt different ways to fusion the features from several views. Among them, attention-based aggregation function performs relatively well and stably, however, it still has an obvious shortcoming the strong independence of each view during predicting the weights for merging leads to a lack of adaption of the global state. In this paper, we propose a global-aware attention-based fusion approach that builds the correlation between each branch and the global feature to provide a comprehensive foundation for weights inference. On the basis of this, we design a complete reconstruction algorithm . Experiments on ShapeNet verify that our method outperforms existing SOTA methods. Furthermore, we propose a view-reduction method based on maximizing diversity and discuss the cost-performance tradeoff of our model to achieve a better performance when facing a heavy input amount and limited computational cost.},
  archive      = {J_PR},
  author       = {Zhenwei Zhu and Liying Yang and Xuxin Lin and Lin Yang and Yanyan Liang},
  doi          = {10.1016/j.patcog.2023.109674},
  journal      = {Pattern Recognition},
  pages        = {109674},
  shortjournal = {Pattern Recognition},
  title        = {GARNet: Global-aware multi-view 3D reconstruction network and the cost-performance tradeoff},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalization of the shortest path approach for superpixel
segmentation of omnidirectional images. <em>PR</em>, <em>142</em>,
109673. (<a href="https://doi.org/10.1016/j.patcog.2023.109673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing use of image capture devices using wide angles and the need for fast and accurate image analysis in computer vision , there is a demand for dedicated under-representation methods. Most decomposition methods segment an image into a small number of irregular homogeneous regions , called superpixels . Nevertheless, these approaches are generally designed to process natural 2D planar images, i.e. , captured with a 90 o o angle view without distortion. In this work, we present SphSPS, a new general decomposition method (for Spherical Shortest Path-based Superpixels) 1 , that is dedicated to wide 360 o o omnidirectional or spherical images . The produced superpixels respect both the geometry of the 3D spherical acquisition space, and the boundaries of image objects. To fastly extract relevant clustering features , we generalize the shortest path approach between a pixel and a superpixel center. We demonstrate that considering the geometry of the acquisition space to compute the shortest path enables to jointly improve the segmentation accuracy and the shape regularity of superpixels. To evaluate this regularity property, we propose a generalization of a standard 2D regularity metric to the spherical space, addressing the limitations of the only existing spherical compactness measure. Finally, SphSPS is validated on reference 360 o o images from the PSD (Panorama Segmentation Dataset) and also on synthetic road omnidirectional images . Our method significantly outperforms both planar and spherical state-of-the-art approaches in terms of segmentation accuracy, robustness to noise and regularity, providing a very interesting tool for superpixel-based applications on 360 o images.},
  archive      = {J_PR},
  author       = {Rémi Giraud and Rodrigo Borba Pinheiro and Yannick Berthoumieu},
  doi          = {10.1016/j.patcog.2023.109673},
  journal      = {Pattern Recognition},
  pages        = {109673},
  shortjournal = {Pattern Recognition},
  title        = {Generalization of the shortest path approach for superpixel segmentation of omnidirectional images},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-constrained learning. <em>PR</em>, <em>142</em>,
109672. (<a href="https://doi.org/10.1016/j.patcog.2023.109672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a scenario in which we have a huge labeled dataset D D and a limited time to train a given learner using D D . Since we may not be able to use the whole dataset, how should we proceed? We propose TCT , an algorithm for this task, whose design relies on principles from Machine Teaching. We present an experimental study involving 5 different learners and 20 datasets where we show that TCT consistently outperforms alternative teaching/training methods, namely: (1) Training over batches of random samples, until the time limit is reached; (2) The state-of-the-art Machine Teaching algorithm for black-box learners proposed in [Dasgupta et al., ICML 19], and (3) Stochastic Gradient Descent (when applicable). While our work is primarily practical, we also show that a stripped-down version of TCT has provable guarantees.},
  archive      = {J_PR},
  author       = {Sergio Freitas and Eduardo Laber and Pedro Lazera and Marco Molinaro},
  doi          = {10.1016/j.patcog.2023.109672},
  journal      = {Pattern Recognition},
  pages        = {109672},
  shortjournal = {Pattern Recognition},
  title        = {Time-constrained learning},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A decomposition dynamic graph convolutional recurrent
network for traffic forecasting. <em>PR</em>, <em>142</em>, 109670. (<a
href="https://doi.org/10.1016/j.patcog.2023.109670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our daily lives are greatly impacted by traffic conditions, making it essential to have accurate predictions of traffic flow within a road network . Traffic signals used for forecasting are usually generated by sensors along roads, which can be represented as nodes on a graph. These sensors typically produce normal signals representing normal traffic flows and abnormal signals indicating unknown traffic disruptions. Graph convolution networks are widely used for traffic prediction due to their ability to capture correlations between network nodes. However, existing approaches use a predefined or adaptive adjacency matrix that does not accurately reflect real-world relationships between signals. To address this issue, we propose a decomposition dynamic graph convolutional recurrent network (DDGCRN) for traffic forecasting. DDGCRN combines a dynamic graph convolution recurrent network with an RNN-based model that generates dynamic graphs based on time-varying traffic signals, allowing for the extraction of both spatial and temporal features. Additionally, DDGCRN separates abnormal signals from normal traffic signals and models them using a data-driven approach to further improve predictions. Results from our analysis of six real-world datasets demonstrate the superiority of DDGCRN compared to the current state-of-the-art. The source codes are available at: https://github.com/wengwenchao123/DDGCRN .},
  archive      = {J_PR},
  author       = {Wenchao Weng and Jin Fan and Huifeng Wu and Yujie Hu and Hao Tian and Fu Zhu and Jia Wu},
  doi          = {10.1016/j.patcog.2023.109670},
  journal      = {Pattern Recognition},
  pages        = {109670},
  shortjournal = {Pattern Recognition},
  title        = {A decomposition dynamic graph convolutional recurrent network for traffic forecasting},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Person re-identification: A retrospective on domain specific
open challenges and future trends. <em>PR</em>, <em>142</em>, 109669.
(<a href="https://doi.org/10.1016/j.patcog.2023.109669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-Identification (Re-ID) is a critical aspect of visual surveillance systems, which aims to automatically recognize and locate individuals across a multi-camera network with non-overlapping fields-of-view. Despite significant progress in recent years through the use of deep learning-based approaches, there remain many vision-related challenges, such as occlusion, pose, background clutter, misalignment, scale, viewpoint, low resolution &amp; illumination, and cross-domain generalization across camera modalities, that hinder the accurate identification of individuals. The majority of the proposed approaches directly or indirectly aim to solve one or multiple of these existing challenges. To further advance the development of Re-ID solutions, a comprehensive review of the current approaches is necessary. However, no focused review currently exists that analyses and highlights specific aspects for further development. To fill this gap, we present a systematic challenge-specific literature survey of about 300 papers published between 2015 and 2022, which reviews Re-ID approaches from a solution-oriented perspective. This survey is the first of its kind to provide an in-depth analysis of the different approaches used to address the various challenges in Re-ID. Furthermore, our review highlights several prominent and diverse research trends in the Re-ID domain. These trends offer a visionary perspective regarding ongoing person Re-ID research, and they may eventually lead to the development of practical real-world solutions. We highlighted the AI ethics that must be followed while developing a Re-ID solution, and recently being practiced as well. Another exciting future dimension of person Re-ID research is the long-term Re-ID, which is still under evolution. Overall, our survey aims to serve as a valuable resource for researchers and practitioners working in the field of Re-ID and to inspire the development of innovative and effective Re-ID solutions.},
  archive      = {J_PR},
  author       = {Asmat Zahra and Nazia Perwaiz and Muhammad Shahzad and Muhammad Moazam Fraz},
  doi          = {10.1016/j.patcog.2023.109669},
  journal      = {Pattern Recognition},
  pages        = {109669},
  shortjournal = {Pattern Recognition},
  title        = {Person re-identification: A retrospective on domain specific open challenges and future trends},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge driven weights estimation for large-scale few-shot
image recognition. <em>PR</em>, <em>142</em>, 109668. (<a
href="https://doi.org/10.1016/j.patcog.2023.109668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the topic of large-scale few-shot image recognition with semantic-visual relational knowledge-based transfer learning . Compared with classical few-shot learning, which is defined as a k k -way ( k k denotes the number of categories, usually 5/10-way) classification problem, large-scale few-shot recognition contains more categories (100-way +) with few samples per category and is easier to overfit. A promising direction of large-scale few-shot learning is transferring prior relevant semantic/visual knowledge from outside data to accelerate the convergence on limited positives. Inspired by this, we propose a novel Knowledge Driven Weights Estimation framework. Specifically, the framework leverages semantic and visual relations between new few-shot and existed many-shot categories to transfer knowledge trained on many-shot datasets (e.g., ImageNet-1000). We show that the transferred knowledge provides a good initialization for novel few-shot categories leading to faster convergence speed and higher performance than random/imprinting initialization. Experimental results on additional un-seen ImageNet categories (other than the 1000 categories) with few positives show that our method is effective on large-scale few-shot recognition.},
  archive      = {J_PR},
  author       = {Jingjing Chen and Linhai Zhuo and Zhipeng Wei and Hao Zhang and Huazhu Fu and Yu-Gang Jiang},
  doi          = {10.1016/j.patcog.2023.109668},
  journal      = {Pattern Recognition},
  pages        = {109668},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge driven weights estimation for large-scale few-shot image recognition},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EX-ViT: A novel explainable vision transformer for weakly
supervised semantic segmentation. <em>PR</em>, <em>142</em>, 109666. (<a
href="https://doi.org/10.1016/j.patcog.2023.109666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently vision transformer models have become prominent models for a multitude of vision tasks. These models, however, are usually opaque with weak feature interpretability, making their predictions inaccessible to the users. While there has been a surge of interest in the development of post-hoc solutions that explain model decisions, these methods can not be broadly applied to different transformer architectures, as rules for interpretability have to change accordingly based on the heterogeneity of data and model structures. Moreover, there is no method currently built for an intrinsically interpretable transformer, which is able to explain its reasoning process and provide a faithful explanation. To close these crucial gaps, we propose a novel vision transformer dubbed the eXplainable Vision Transformer (eX-ViT), an intrinsically interpretable transformer model that is able to jointly discover robust interpretable features and perform the prediction. Specifically, eX-ViT is composed of the Explainable Multi-Head Attention (E-MHA) module, the Attribute-guided Explainer (AttE) module with the self-supervised attribute-guided loss. The E-MHA tailors explainable attention weights that are able to learn semantically interpretable representations from tokens in terms of model decisions with noise robustness. Meanwhile, AttE is proposed to encode discriminative attribute features for the target object through diverse attribute discovery, which constitutes faithful evidence for the model predictions. Additionally, we have developed a self-supervised attribute-guided loss for our eX-ViT architecture, which utilizes both the attribute discriminability mechanism and the attribute diversity mechanism to enhance the quality of learned representations. As a result, the proposed eX-ViT model can produce faithful and robust interpretations with a variety of learned attributes. To verify and evaluate our method, we apply the eX-ViT to several weakly supervised semantic segmentation (WSSS) tasks, since these tasks typically rely on accurate visual explanations to extract object localization maps. Particularly, the explanation results obtained via eX-ViT are regarded as pseudo segmentation labels to train WSSS models. Comprehensive simulation results illustrate that our proposed eX-ViT model achieves comparable performance to supervised baselines, while surpassing the accuracy and interpretability of state-of-the-art black-box methods using only image-level labels.},
  archive      = {J_PR},
  author       = {Lu Yu and Wei Xiang and Juan Fang and Yi-Ping Phoebe Chen and Lianhua Chi},
  doi          = {10.1016/j.patcog.2023.109666},
  journal      = {Pattern Recognition},
  pages        = {109666},
  shortjournal = {Pattern Recognition},
  title        = {EX-ViT: A novel explainable vision transformer for weakly supervised semantic segmentation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous cross-modal hashing. <em>PR</em>, <em>142</em>,
109662. (<a href="https://doi.org/10.1016/j.patcog.2023.109662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, multimodal data with new classes arrive continuously in the real world. While advanced cross-modal hashing (CMH) focuses primarily on batch-based data with previously observed classes (ASCs), it disregards the effect of newly arriving classes (ANCs) on hash-code conflicts. In addition, class-level continuous hashing scenarios do not suit themselves well with the generic CMH configuration. To solve the aforementioned issues, we propose a novel framework, called CT-CMH, for the new task of continuous cross-modal hashing. For dealing with ANCs, CMH models require the ability of continuous learning, i.e. they can preserve the knowledge of previously observed data and, more crucially, they can be adapted to unseen data with ANCs. Specifically, we introduce the adaptive weight importance updating (AWIU) mechanism to alleviate the catastrophic forgetting problem of CMH and a new hash-code divergence (HCD) method to eliminate hash-code conflicts between ASCs and ANCs. When CT-CMH is equipped with both AWIU and HCD, it can consistently achieve high retrieval performance . The experiment results and visualization analyses validate the effectiveness of our approach. To the best of our knowledge, we are the first to introduce and implement the task of CCMH for ANCs.},
  archive      = {J_PR},
  author       = {Hao Zheng and Jinbao Wang and Xiantong Zhen and Jingkuan Song and Feng Zheng and Ke Lu and Guo-Jun Qi},
  doi          = {10.1016/j.patcog.2023.109662},
  journal      = {Pattern Recognition},
  pages        = {109662},
  shortjournal = {Pattern Recognition},
  title        = {Continuous cross-modal hashing},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GriT-DBSCAN: A spatial clustering algorithm for very large
databases. <em>PR</em>, <em>142</em>, 109658. (<a
href="https://doi.org/10.1016/j.patcog.2023.109658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DBSCAN is a fundamental spatial clustering algorithm with numerous practical applications. However, a bottleneck of DBSCAN is its O ( n 2 ) O(n2) worst-case time complexity. To address this limitation, we propose a new grid-based algorithm for exact DBSCAN in Euclidean space called GriT-DBSCAN, which is based on the following two techniques. First, we introduce grid tree to organize the non-empty grids for the purpose of efficient non-empty neighboring grids queries. Second, by utilizing the spatial relationships among points, we propose a technique that iteratively prunes unnecessary distance calculations when determining whether the minimum distance between two sets is less than or equal to a certain threshold. We theoretically demonstrate that GriT-DBSCAN has excellent reliability in terms of time complexity. In addition, we obtain two variants of GriT-DBSCAN by incorporating heuristics, or by combining the second technique with an existing algorithm. Experiments are conducted on both synthetic and real-world data sets to evaluate the efficiency of GriT-DBSCAN and its variants. The results show that our algorithms outperform existing algorithms.},
  archive      = {J_PR},
  author       = {Xiaogang Huang and Tiefeng Ma and Conan Liu and Shuangzhe Liu},
  doi          = {10.1016/j.patcog.2023.109658},
  journal      = {Pattern Recognition},
  pages        = {109658},
  shortjournal = {Pattern Recognition},
  title        = {GriT-DBSCAN: A spatial clustering algorithm for very large databases},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical co-clustering with augmented matrices from
external domains. <em>PR</em>, <em>142</em>, 109657. (<a
href="https://doi.org/10.1016/j.patcog.2023.109657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-clustering simultaneously classifies row and column objects of data matrices and is considered to have better accuracy than conventional one-way clustering methods . In the era of big data, extracting classification knowledge about objects from several domains has become increasingly feasible. This study proposes a hierarchical co-clustering with augmented matrices (HICCAM), which co-clusters the row and column objects of a target matrix while utilizing the augmented data matrices of these target objects extracted from the external domains. The algorithm is designed to improve classification accuracy by transferring knowledge in augmented matrices and simultaneously improves cluster interpretability using hierarchical cluster structures. Experiments on document clustering confirmed that HICCAM achieved the highest accuracy among comparison methods with and without external knowledge. Its clusters exhibit hierarchical relationships according to their topics. In addition, we provide the experimental results with multiview synthetic datasets that demonstrate a clustering situation in which HICCAM can be effectively identified.},
  archive      = {J_PR},
  author       = {Kai Sugahara and Kazushi Okamoto},
  doi          = {10.1016/j.patcog.2023.109657},
  journal      = {Pattern Recognition},
  pages        = {109657},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical co-clustering with augmented matrices from external domains},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross domain 2D-3D descriptor matching for unconstrained
6-DOF pose estimation. <em>PR</em>, <em>142</em>, 109655. (<a
href="https://doi.org/10.1016/j.patcog.2023.109655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for cross-domain descriptor matching between 2D and 3D modalities. The 2D-3D matching is applied to localize 2D images in 3D point clouds. Direct cross-domain matching allows our technique to localize images in any type of 3D point cloud without any constraints on the nature or mechanism by which it is obtained. We propose a learning based framework, called Desc-Matcher, to directly match features between the two modalities. A dataset of 2D and 3D features with corresponding locations in images and point clouds is generated to train the Desc-Matcher. To estimate the pose of an image in any 3D cloud, keypoints and feature descriptors are extracted from the query image and the point cloud. The trained Desc-Matcher is then used to match the features from the image and the point cloud. A robust pose estimator is used to predict the location and orientation of the query image from the corresponding positions of the matched 2D and 3D features. We carried out an extensive evaluation of the proposed method for indoor and outdoor scenarios and with different types of point clouds to verify the feasibility of our approach. Experimental results show that the proposed approach can reliably estimate the 6-DOF poses of query cameras in any type of 3D point cloud with high precision. We achieved average median errors of 1.09 c m / 0 . 27 ∘ 1.09cm/0.27∘ and 19 c m / 0 . 39 ∘ 19cm/0.39∘ on the Stanford and Cambridge datasets, respectively.},
  archive      = {J_PR},
  author       = {Uzair Nadeem and Mohammed Bennamoun and Roberto Togneri and Ferdous Sohel and Aref Miri Rekavandi and Farid Boussaid},
  doi          = {10.1016/j.patcog.2023.109655},
  journal      = {Pattern Recognition},
  pages        = {109655},
  shortjournal = {Pattern Recognition},
  title        = {Cross domain 2D-3D descriptor matching for unconstrained 6-DOF pose estimation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). Class-specific and self-learning local manifold structure
for domain adaptation. <em>PR</em>, <em>142</em>, 109654. (<a
href="https://doi.org/10.1016/j.patcog.2023.109654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) is a powerful technology that allows a classifier trained on a well-labeled source domain to be adapted to an unlabeled target domain with different distributions. Existing DA methods aim to enhance feature transferability by reducing distribution distance, and they often rely on preserving the global discriminative (GD) structure to boost feature discriminability . However, strict GD loss may degrade transferability, and poor discriminability may result from wrongly labeled samples. To address these issues, we propose a new approach called class-specific and self-learning local manifold structure (CSSL-LM), to extract more desirable features for better DA effects. Specifically, it draws two data points close with large weight if they are from the same class and close in the original feature space. This approach is more relaxed than GD and thus mitigates the negative effect on transferability. Moreover, CSSL-LM is more robust to wrongly labeled samples than GD since two data points that are wrongly labeled as the same classes have small weight and are not required to be close. Inspired by previous adaptive local manifold learning, we utilize a self-learning mechanism to model CSSL-LM more accurately and reliably, particularly for feature-corrupted samples. Extensive experiments on four DA benchmarks verify that CSSL-LM outperforms some state-of-the-art methods. We also construct DA datasets that are randomly wrongly labeled or feature-corrupted to further evaluate CSSL-LM’s robustness.},
  archive      = {J_PR},
  author       = {Wei Wang and Mengzhu Wang and Xiao Dong and Long Lan and Quannan Zu and Xiang Zhang and Cong Wang},
  doi          = {10.1016/j.patcog.2023.109654},
  journal      = {Pattern Recognition},
  pages        = {109654},
  shortjournal = {Pattern Recognition},
  title        = {Class-specific and self-learning local manifold structure for domain adaptation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive reweighted quaternion sparse learning for data
recovery and classification. <em>PR</em>, <em>142</em>, 109653. (<a
href="https://doi.org/10.1016/j.patcog.2023.109653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation (SR) methods in quaternion space have been attracting increasing interests recently. However, most existing quaternion SR methods adopt the quaternion ℓ 1 ℓ1 norm, which penalizes all the entries of the quaternion sparse vector equally and ignores the differences and significance of different entries. Ideally, the entries with large magnitude should be less penalized while those with small magnitude (such as zero entries) should be more penalized. Therefore, we propose an Adaptive Weighted Quaternion Sparse Representation (AWQSR) method in this paper, which can learn weights for distinct entries of the quaternion sparse entries in an adaptive manner. Due to the noncommutativity of quaternion multiplication , it is difficult to tackle the resulting optimization problem of AWQSR. For this reason, we devise an effective iteratively reweighted optimization algorithm based on quaternion operators. To further improve the classification performance, we also develop a Supervised AWQSR based Classification (SAWQSRC) method by leveraging the label information of training samples to learn discriminative weights. Theoretical analysis of SAWQSRC has also been established to show that SAWQSRC succeeds in classification under appropriate conditions. The experiments on simulated data and real data prove the validity of the proposed methods for quaternion signal recovery and classification.},
  archive      = {J_PR},
  author       = {Cuiming Zou and Kit Ian Kou and Yuan Yan Tang and Hao Deng},
  doi          = {10.1016/j.patcog.2023.109653},
  journal      = {Pattern Recognition},
  pages        = {109653},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive reweighted quaternion sparse learning for data recovery and classification},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid neural-like p systems with evolutionary channels for
multiple brain metastases segmentation. <em>PR</em>, <em>142</em>,
109651. (<a href="https://doi.org/10.1016/j.patcog.2023.109651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural-like P systems are membrane computing models inspired by natural computing. Spiking neurral (SN) P systems, a kind of neural-like P systems, are viewed as third-generation neural network models . Although real neurons have complex structures, classical SN P systems simplify the structures and corresponding mechanisms to stationary two-dimensional graphs and lack related evolution mechanisms on spikes and channels, which limits the real applications of these models. In this paper, we propose a new hybrid SN P system with evolutionary channels (HN P systems), including three new types of rules for dynamically generating or removing one-one and one-many/many-one channels with related evolutions of spikes on the hybrid neuron structures. Two dynamic regulatory factors are also presented on rules to help guide the optimization of the HN P systems automatically. Based on the new P system, a multiple brain metastases (BMs) segmentation model is developed. The experimental results indicate that the proposed models outperform the state-of-the-art methods on the BMs, which have large variations in sizes, positions and shapes, and low contrast with their surroundings. Performances on the head and neck segmentation dataset also verifies the effectiveness of the HN P system.},
  archive      = {J_PR},
  author       = {Jie Xue and Qi Li and Xiyu Liu and Yujie Guo and Jie Lu and Bosheng Song and Pu Huang and Qiong An and Guanzhong Gong and Dengwang Li},
  doi          = {10.1016/j.patcog.2023.109651},
  journal      = {Pattern Recognition},
  pages        = {109651},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid neural-like p systems with evolutionary channels for multiple brain metastases segmentation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-directional broad learning system for the unsupervised
stereo matching method. <em>PR</em>, <em>142</em>, 109648. (<a
href="https://doi.org/10.1016/j.patcog.2023.109648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The supervised stereo matching methods usually rely on ground truth disparity maps as the training labels, this limits its practical application in many situations. In this study, we propose a novel unsupervised stereo matching method based on a multi-directional broad learning system. A multi-directional broad learning system was constructed to generate multiple candidate disparity maps. During the generation of each candidate disparity map, an update criterion is proposed for the disparity value based on the maximum similarity of the inverse mapping region to remove the abnormal disparity values of the training samples. Subsequently, multi-direction consistency verification is performed to further eliminate abnormal disparity values, which are based on the uniqueness principle of disparity truth values at the same location. Finally, an invalid depth redefinition based on a local gravity weight method is introduced to select the appropriate disparity value to fill the invalid pixel positions from their neighborhood, which is calculated based on the local region of the color, matching cost, and geometric spaces in the stereo images . We provide the results of experiments on both indoor and outdoor scenarios to demonstrate the effectiveness and flexibility of our approach, including comparisons with state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhang zihao and Niu Ying and Meng Fanman and Yang Tiejun and Fan Chao and Ren Xiaozhen and Wu Ruiqi and Cao Kun and Wang Haocheng},
  doi          = {10.1016/j.patcog.2023.109648},
  journal      = {Pattern Recognition},
  pages        = {109648},
  shortjournal = {Pattern Recognition},
  title        = {Multi-directional broad learning system for the unsupervised stereo matching method},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatio-temporal modelling with multi-gradient features and
elongated quinary pattern descriptor for dynamic facial expression
recognition. <em>PR</em>, <em>142</em>, 109647. (<a
href="https://doi.org/10.1016/j.patcog.2023.109647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new spatio-temporal modelling approach for Dynamic Facial Expression Recognition (DFER). We first convert the domain of the spatial images in the sequence to the gradient of magnitude and angle images at different orientations. Robust gradient components are developed to deal with difficult types of illuminations, such as darkness, by forming the eight edge responses of the Gaussian mask. To describe the dynamic Facial Expression (FE) changes we extend the Elongated Quinary Pattern (EQP) descriptor to encode separately the anisotropic structure of the uniform patterns from Three Orthogonal Planes (TOP) of each gradient sequence. Then each encoded sequence is divided into a stack of block volumes in the XY, XT and YT planes. For each plane, the co-occurrence of histogram features are calculated from each block volume and concatenated together. Simple three-dimensional histogram features are generated by concatenating the histogram features of all planes. A Multi Classifier System (MCS) based on a multi-class Support Vector Machine (SVM) is adopted to combine all scores for the encoded sequences. The proposed approach is evaluated with the challenging MMI and Oulu-CASIA databases with different set-ups and advantage has been shown in terms of generalisation to different databases, together with robustness against difficult pose variations and illumination changes. In terms of Recognition Accuracy (RA), a comparison is established with DFER methods in the literature. A high recognition rate of 79.23\% is attained in the case of six classes when applied to the MMI database which surpasses all the state-of-the-art results.},
  archive      = {J_PR},
  author       = {S.A.M. Al-Sumaidaee and M.A.M. Abdullah and R.R.O. Al-Nima and S.S. Dlay and J.A. Chambers},
  doi          = {10.1016/j.patcog.2023.109647},
  journal      = {Pattern Recognition},
  pages        = {109647},
  shortjournal = {Pattern Recognition},
  title        = {Spatio-temporal modelling with multi-gradient features and elongated quinary pattern descriptor for dynamic facial expression recognition},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-channel embedding learning model for partially labeled
attributed networks. <em>PR</em>, <em>142</em>, 109644. (<a
href="https://doi.org/10.1016/j.patcog.2023.109644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding is an important fundamental work in many network application tasks, which encodes the input network from the high-dimensional and sparse topological space into a low-dimensional and dense vector space. Recently, there has been a growing interest in embedding learning on Partially Labeled Attributed Networks (PLANs) due to the increasing occurrence of node attributes and partially available category labels in real-world networks. Semi-supervised embedding learning is a standard approach employed in PLANs, utilizing category labels to supervise the learning process. However, the semi-supervised learning procedure can fail when labels are scarce, noisy, or unreliable. Additionally, most existing embedding algorithms have not successfully integrated heterogeneous information, such as labels, attributes, and structure. To address these issues, we develop a new model, the Dual-Channel Network Embedding (DcNE), which integrates different types of network information into embeddings from a mutual information (MI) perspective. Specifically, we construct a dual-channel information propagation framework to encode the input network in semi-supervised and self-supervised learning paradigms in parallel. Furthermore, a redundancy elimination module is implemented to capture and eliminate the redundant information between the two encoders. Finally, we propose a unified optimization model that integrates the two learning paradigms to collaborate effectively. In the experiments, we demonstrate the effectiveness of DcNE in various network analysis tasks using real-world datasets, establishing its superiority over state-of-the-art baselines.},
  archive      = {J_PR},
  author       = {Hangyuan Du and Wenjian Wang and Liang Bai},
  doi          = {10.1016/j.patcog.2023.109644},
  journal      = {Pattern Recognition},
  pages        = {109644},
  shortjournal = {Pattern Recognition},
  title        = {Dual-channel embedding learning model for partially labeled attributed networks},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel cancelable finger vein templates based on LDM and
RetinexGan. <em>PR</em>, <em>142</em>, 109643. (<a
href="https://doi.org/10.1016/j.patcog.2023.109643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new biometric template protection scheme, which can deal with the finger vein biometric security threats, through using the LDM and RetinexGAN model. The RetinexGAN model is mainly used to handle the illumination and low contrast problems effectively, while efficiently extracting discriminative features from the finger vein images. The projection of extracted features into dissimilarity space is done using Local Dissimilarity Map (LDM). LDM is an efficient way for finger vein features representation, which investigates the relationships and correlation inter and intra classes, while effectively coming up with the accidental shifts/rotations caused by the arbitrary position of the finger during image acquisition. The proposed approach is successfully evaluated in terms of non-invertibility, non-linkability, revocability and performances. Experimental results and comparison analysis with the state of arts methods confirm that the proposed framework can achieve promising results.},
  archive      = {J_PR},
  author       = {N. Aherrahrou and H. Tairi},
  doi          = {10.1016/j.patcog.2023.109643},
  journal      = {Pattern Recognition},
  pages        = {109643},
  shortjournal = {Pattern Recognition},
  title        = {A novel cancelable finger vein templates based on LDM and RetinexGan},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A k nearest neighbour ensemble via extended neighbourhood
rule and feature subsets. <em>PR</em>, <em>142</em>, 109641. (<a
href="https://doi.org/10.1016/j.patcog.2023.109641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {k NN based ensemble methods minimise the effect of outliers by identifying a set of data points in the given feature space that are nearest to an unseen observation in order to predict its response by using majority voting. The ordinary ensembles based on k NN find out the k nearest observations in a region (bounded by a sphere) based on a predefined value of k . This scenario, however, might not work in situations where the test observation follows the pattern of the closest data points with the same class that lie on a certain path not contained in the given sphere. This paper proposes a k nearest neighbour ensemble where the neighbours are determined in k steps. Starting from the first nearest observation of the test point, the algorithm identifies a single observation that is closest to the observation at the previous step. At each base learner in the ensemble, this search is extended to k steps on a random bootstrap sample with a random subset of features selected from the feature space. The final predicted class of the test point is determined by using a majority vote in the predicted classes given by all base models. This new ensemble method is applied on 20 benchmark datasets and compared with other classical methods, including k NN based models, in terms of classification accuracy , kappa and Brier score as performance metrics. Boxplots are also utilised to illustrate the difference in the results given by the proposed and other state-of-the-art methods. The proposed method outperformed the considered classical methods in the majority of cases. The proposed method is further assessed through a detailed simulation study.},
  archive      = {J_PR},
  author       = {Amjad Ali and Muhammad Hamraz and Naz Gul and Dost Muhammad Khan and Saeed Aldahmani and Zardad Khan},
  doi          = {10.1016/j.patcog.2023.109641},
  journal      = {Pattern Recognition},
  pages        = {109641},
  shortjournal = {Pattern Recognition},
  title        = {A k nearest neighbour ensemble via extended neighbourhood rule and feature subsets},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint a-SNN: Joint training of artificial and spiking neural
networks via self-distillation and weight factorization. <em>PR</em>,
<em>142</em>, 109639. (<a
href="https://doi.org/10.1016/j.patcog.2023.109639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimic the spiking nature of brain neurons and have received lots of research attention. SNNs deal with binary spikes as their activation and therefore derive extreme energy efficiency on hardware. However, it also leads to an intrinsic obstacle that training SNNs from scratch requires a re-definition of the firing function for computing gradient. Artificial Neural Networks (ANNs), however, are fully differentiable to be trained with gradient descent . In this paper, we propose a joint training framework of ANN and SNN, in which the ANN can guide the SNN’s optimization. This joint framework contains two parts: First, the knowledge inside ANN is distilled to SNN by using multiple branches from the networks. Second, we restrict the parameters of ANN and SNN, where they share partial parameters and learn different singular weights. Extensive experiments over several widely used network structures show that our method consistently outperforms many other state-of-the-art training methods. For example, on the CIFAR100 classification task , the spiking ResNet-18 model trained by our method can reach to 77.39\% top-1 accuracy with only 4 time steps.},
  archive      = {J_PR},
  author       = {Yufei Guo and Weihang Peng and Yuanpei Chen and Liwen Zhang and Xiaode Liu and Xuhui Huang and Zhe Ma},
  doi          = {10.1016/j.patcog.2023.109639},
  journal      = {Pattern Recognition},
  pages        = {109639},
  shortjournal = {Pattern Recognition},
  title        = {Joint A-SNN: Joint training of artificial and spiking neural networks via self-distillation and weight factorization},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Correlated and individual feature learning with
contrast-enhanced MR for malignancy characterization of hepatocellular
carcinoma. <em>PR</em>, <em>142</em>, 109638. (<a
href="https://doi.org/10.1016/j.patcog.2023.109638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malignancy characterization of hepatocellular carcinoma (HCC) is of great importance in patient management and prognosis prediction. In this study, we propose an end-to-end correlated and individual feature learning framework to characterize the malignancy of HCC from Contrast-enhanced MR. From the phases of pre-contrast, arterial and portal venous, our framework simultaneously and explicitly learns both the shareable and phase-specific features that are discriminative to malignancy grades. We evaluate our method on the Contrast enhanced MR of 112 consecutive patients with 117 histologically proven HCCs. Experimental results demonstrate that arterial phase yields better results than portal vein and pre-contrast phase. Furthermore, phase specific components show better discriminant ability than the shareable components. Finally, combining the extracted shareable and individual features components has yielded significantly better performance than traditional feature fusion methods. We also conduct t-SNE analysis and feature scoring analysis to qualitatively assess the effectiveness of the proposed method for malignancy characterization.},
  archive      = {J_PR},
  author       = {Yunling Li and Shangxuan Li and Hanqiu Ju and Tatsuya Harada and Honglai Zhang and Ting Duan and Guangyi Wang and Lijuan Zhang and Lin Gu and Wu Zhou},
  doi          = {10.1016/j.patcog.2023.109638},
  journal      = {Pattern Recognition},
  pages        = {109638},
  shortjournal = {Pattern Recognition},
  title        = {Correlated and individual feature learning with contrast-enhanced MR for malignancy characterization of hepatocellular carcinoma},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust rank-one matrix completion with rank estimation.
<em>PR</em>, <em>142</em>, 109637. (<a
href="https://doi.org/10.1016/j.patcog.2023.109637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix completion aims at estimating the missing entries of a low-rank and incomplete data matrix. It frequently arises in many applications such as computer vision , pattern recognition, recommendation system, and data mining. Most of the existing methods face two problems. Firstly, the data matrix in real world is often disturbed by noise. Noise may change the date structure of the incomplete matrix, thereby degrade the performance of matrix completion algorithms. Secondly, some existing methods need to preset a reasonable rank as input, and the value of rank will affect the performance of the algorithms. Therefore, we proposed a robust rank-one matrix completion method with rank estimation in this paper. To mitigate the influence of noise, we divide the incomplete and noisy data matrix into two parts iteratively: low-rank and sparse parts. Besides, we use a weighted rank-one matrix pursuit algorithm to approximate the low-rank part of the data matrix, and the rank of the matrix can be estimated with the adaptive weight vector . The performance of the proposed method is demonstrated by experiments on both synthetic datasets and image datasets. The experimental results demonstrate the performance of the proposed method with incompleted matrices distrubed by sparse noise.},
  archive      = {J_PR},
  author       = {Ziheng Li and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.109637},
  journal      = {Pattern Recognition},
  pages        = {109637},
  shortjournal = {Pattern Recognition},
  title        = {Robust rank-one matrix completion with rank estimation},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-branch spatio-temporal graph neural networks for
pedestrian trajectory prediction. <em>PR</em>, <em>142</em>, 109633. (<a
href="https://doi.org/10.1016/j.patcog.2023.109633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is an important area in computer vision , with wide applications in autonomous driving , robot path planning , and surveillance systems. The core underlying technique of these applications is pattern recognition. A key challenge in this area is modeling social interactions between pedestrians, such as pedestrian view area and group behaviors. However, although many methods have been proposed to model social interactions, pedestrian view area and group behaviors have not been explored together to account for complex situations. Additionally, most existing studies require additional detectors and manual annotations to handle view area and group interactions, respectively. In this paper, we propose a dual-branch spatio-temporal graph neural network to automatically model view area and grouping together. Specifically, a spatio-temporal graph attention network (STGAT) branch is designed to handle pedestrian view area, and a spatio-temporal graph convolutional network (STGCN) branch is designed to model group interactions. The features of these branches are then fused to provide better feature representations, on which a temporal convolution operation (TCN) is performed for trajectory prediction. Experiments on public standard datasets demonstrate that the proposed method achieves very competitive performance and predicts socially acceptable trajectories in different challenging scenarios.},
  archive      = {J_PR},
  author       = {Xingchen Zhang and Panagiotis Angeloudis and Yiannis Demiris},
  doi          = {10.1016/j.patcog.2023.109633},
  journal      = {Pattern Recognition},
  pages        = {109633},
  shortjournal = {Pattern Recognition},
  title        = {Dual-branch spatio-temporal graph neural networks for pedestrian trajectory prediction},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TAT: Targeted backdoor attacks against visual object
tracking. <em>PR</em>, <em>142</em>, 109629. (<a
href="https://doi.org/10.1016/j.patcog.2023.109629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual object tracking (VOT) is a fundamental computer vision task that aims to track a target in a sequence of video frames. It has been broadly adopted in safety- and security-critical applications, such as self-driving systems and traffic control systems . However, the VOT models (i.e., the trackers) that rely on third-party training resources face a severe threat of backdoor attacks , which refer to the type of the attacks that poison a portion of training data and mislead the tracker to track a wrong target. A surge of research interest has arisen in backdoor attacks in the domain of image classification , as a measure to expose the potential security risks of the classifiers and inspire new defense techniques. Despite the prosperity of the research in backdoor attacks in image classification , there still lacks investigation in backdoor attacks against VOT, due to their unique challenges: first, the architecture of a VOT model is much more complicated than that of an image classifier; second, VOT targets a sequence of video frames rather than individual images. To bridge the gap, we propose a novel and effective targeted backdoor attack approach TAT specifically against VOT tasks. In particular, TAT includes a basic version TAT-BA that can achieve effective and stealthy backdoor attacks against VOT trackers, and an advanced version TAT-DA that can evade two representative defense techniques. Our large-scale experimental evaluation demonstrates the effectiveness and the stealthiness of TAT . Moreover, we also demonstrate the performances of TAT-BA under real-world settings and the abilities of TAT-DA to counter defense techniques. The code will be available at https://github.com/MisakaZipi/TAT .},
  archive      = {J_PR},
  author       = {Ziyi Cheng and Baoyuan Wu and Zhenya Zhang and Jianjun Zhao},
  doi          = {10.1016/j.patcog.2023.109629},
  journal      = {Pattern Recognition},
  pages        = {109629},
  shortjournal = {Pattern Recognition},
  title        = {TAT: Targeted backdoor attacks against visual object tracking},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning customised decision trees for domain-knowledge
constraints. <em>PR</em>, <em>142</em>, 109610. (<a
href="https://doi.org/10.1016/j.patcog.2023.109610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When applied to critical domains, machine learning models usually need to comply with prior knowledge and domain-specific requirements. For example, one may require that a learned decision tree model should be of limited size and fair, so as to be easily interpretable, trusted, and adopted. However, most state-of-the-art models, even on decision trees , only aim to maximising expected accuracy. In this paper, we propose a framework in which a diverse family of prior and domain knowledge can be formalised and imposed as constraints on decision trees . This framework is built upon a newly introduced tree representation that leads to two generic linear programming formulations of the optimal decision tree problem. The first one targets binary features , while the second one handles continuous features without the need for discretisation . We theoretically show how a diverse family of constraints can be formalised in our framework. We validate the framework with constraints on several applications and perform extensive experiments, demonstrating empirical evidence of comparable performance w.r.t. state-of-the-art tree learners.},
  archive      = {J_PR},
  author       = {Géraldin Nanfack and Paul Temple and Benoît Frénay},
  doi          = {10.1016/j.patcog.2023.109610},
  journal      = {Pattern Recognition},
  pages        = {109610},
  shortjournal = {Pattern Recognition},
  title        = {Learning customised decision trees for domain-knowledge constraints},
  volume       = {142},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orthonormal product quantization network for scalable face
image retrieval. <em>PR</em>, <em>141</em>, 109671. (<a
href="https://doi.org/10.1016/j.patcog.2023.109671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep quantization methods provided an efficient solution for large-scale image retrieval. However, the significant intra-class variations, like pose, illumination, and expressions in face images, still pose a challenge. In light of this, face image retrieval requires sufficiently powerful learning metrics, which are absent in current deep quantization works. Moreover, to tackle the growing unseen identities in the query stage, face image retrieval drives more demands regarding model generalization and scalability than general image retrieval tasks. This paper integrates product quantization with orthonormal constraints into an end-to-end deep learning framework to effectively retrieve face images. Specifically, we propose a novel scheme that uses predefined orthonormal vectors as codewords to enhance the quantization informativeness and reduce codewords’ redundancy. A tailored loss function maximizes discriminability among identities in each quantization subspace for both the quantized and original features. An entropy-based regularization term is imposed to reduce the quantization error . Experiments are conducted on four commonly-used face datasets under both seen and unseen identity retrieval settings. Our method outperforms all the compared state-of-the-art under both settings. The proposed orthonormal codewords consistently boost both models’ standard retrieval performance and generalization ability , demonstrating the superiority of our method for scalable face image retrieval.},
  archive      = {J_PR},
  author       = {Ming Zhang and Xuefei Zhe and Hong Yan},
  doi          = {10.1016/j.patcog.2023.109671},
  journal      = {Pattern Recognition},
  pages        = {109671},
  shortjournal = {Pattern Recognition},
  title        = {Orthonormal product quantization network for scalable face image retrieval},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Slide deep reinforcement learning networks: Application for
left ventricle segmentation. <em>PR</em>, <em>141</em>, 109667. (<a
href="https://doi.org/10.1016/j.patcog.2023.109667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic segmentation of the left ventricle (LV) in four-chamber view images is critical for computer-aided cardiac disease diagnosis. The complex structure of the cardiac image and the encoder-decoder networks may cause coarse segmentation results. High-accuracy LV segmentation is still a challenge with existing automatic LV segmentation methods . In this paper, we propose a slide deep reinforcement learning segmentation network for pixelwise LV segmentation. The main architecture of the slide reinforcement learning networks consists of a slider item combined state, a group of morphology transforming actions and an agent network. The specifically designed reinforcement learning state comprises an image item and a slider item, which contains both original image information and network act information. The reinforcement learning actions proposed in this paper enable accurate and fast formulation of the binary segment result for each frame by controlling the length and location of the slider. Additionally, the confidence branch proposed in our experiment provides a continuous frame series environment, and the identification algorithm avoids losing the segmentation target. The segmentation result reveals that the proposed method outperforms FCN , SegNet, U-Net and TransUnet. The IoU improved by 23.01\% 23.01\% , 15.4\% 15.4\% , 11.24\% 11.24\% and 6.9\% 6.9\% . Additionally, we demonstrate how the proposed method can be used as a semisupervised method, which is more convenient for the image annotation process.},
  archive      = {J_PR},
  author       = {Wanjun Zhang and Xiaohua Ding and Yang Liu and Baojun Qiao},
  doi          = {10.1016/j.patcog.2023.109667},
  journal      = {Pattern Recognition},
  pages        = {109667},
  shortjournal = {Pattern Recognition},
  title        = {Slide deep reinforcement learning networks: Application for left ventricle segmentation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature fusion and latent feature learning guided brain
tumor segmentation and missing modality recovery network. <em>PR</em>,
<em>141</em>, 109665. (<a
href="https://doi.org/10.1016/j.patcog.2023.109665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate brain tumor segmentation is an essential step for clinical diagnosis and surgical treatment. Multimodal brain tumor segmentation strongly relies on an effective fusion method and an excellent segmentation network . However, it is common to have some missing MR modalities in clinical scenarios due to image corruption, acquisition protocol, scanner availability and scanning cost, which can heavily decrease the tumor segmentation accuracy , and also cause information loss for down-streaming disease analysis. To address this issue, I propose a novel multimodal feature fusion and latent feature learning guided deep neural network . On the one hand, the proposed network can help to segment brain tumors when one or more modalities are missing. On the other hand, it can retrieve the missing modalities to compensate for incomplete data. The proposed network consists of three key components. First, a Multimodal Feature Fusion Module (MFFM) is proposed to effectively fuse the complementary information from different modalities, consisting of a Cross-Modality Fusion Module (CMFM) and a Multi-Scale Fusion Module (MSFM). Second, a Spatial Consistency-based Latent Feature Learning Module (SC-LFLM) is presented to exploit multimodal latent correlation and extract the relevant features to benefit segmentation. Third, the Multi-Task Learning (MTL) paths are integrated to supervise the segmentation and recover the missing modalities. The proposed method is evaluated on BraTS 2018 dataset, and it can achieve superior segmentation results when one or more modalities are missing, compared with the state-of-the-art methods. Furthermore, the proposed modules can be easily adapted to other multimodal network architectures and research fields.},
  archive      = {J_PR},
  author       = {Tongxue Zhou},
  doi          = {10.1016/j.patcog.2023.109665},
  journal      = {Pattern Recognition},
  pages        = {109665},
  shortjournal = {Pattern Recognition},
  title        = {Feature fusion and latent feature learning guided brain tumor segmentation and missing modality recovery network},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster OreFSDet: A lightweight and effective few-shot object
detector for ore images. <em>PR</em>, <em>141</em>, 109664. (<a
href="https://doi.org/10.1016/j.patcog.2023.109664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the ore particle size detection, obtaining a sizable amount of high-quality ore labeled data is time-consuming and expensive. General object detection methods often suffer from severe over-fitting with scarce labeled data. Despite their ability to eliminate over-fitting, existing few-shot object detectors encounter drawbacks such as slow detection speed and high memory requirements, making them difficult to implement in a real-world deployment scenario . To this end, we propose a lightweight and effective few-shot detector to achieve competitive performance with general object detection with only a few samples for ore images. First, the proposed support feature mining block characterizes the importance of location information in support features. Next, the relationship guidance block makes full use of support features to guide the generation of accurate candidate proposals. Finally, the dual-scale semantic aggregation module retrieves detailed features at different resolutions to contribute with the prediction process. Experimental results show that our method consistently exceeds the few-shot detectors with an excellent performance gap on all metrics. Moreover, our method achieves the smallest model size of 19 MB as well as being competitive at 50 FPS detection speed compared with general object detectors.},
  archive      = {J_PR},
  author       = {Yang Zhang and Le Cheng and Yuting Peng and Chengming Xu and Yanwei Fu and Bo Wu and Guodong Sun},
  doi          = {10.1016/j.patcog.2023.109664},
  journal      = {Pattern Recognition},
  pages        = {109664},
  shortjournal = {Pattern Recognition},
  title        = {Faster OreFSDet: A lightweight and effective few-shot object detector for ore images},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Process-oriented heterogeneous graph learning in GNN-based
ICS anomalous pattern recognition. <em>PR</em>, <em>141</em>, 109661.
(<a href="https://doi.org/10.1016/j.patcog.2023.109661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, massive penetrations targeting an Industrial Control System (ICS) network intend to compromise its core industrial processes. So far, numerous advanced methods have been proposed to detect anomalous patterns in the numeric data streams with respect to the heterogeneous field devices involved in the industrial processes. These methods, despite reporting decent results, usually conduct system-wise detection instead of fine-grained anomalous pattern recognition at the device level. Furthermore, lacking explicit consideration of the exclusive process-related features with respect to each differentiated device, the fitness of their application in specified industrial processes is undermined. To tackle these issues, a GNN-based Attributed Heterogeneous Graph Analyzer (the AHGA) is designed to perform device-wise anomalous pattern detection via in-depth process-oriented associativity learning. The AHGA’s framework is constructed with four building blocks : a graph processor, a feature analyzer, a link inference decoder, and an anomaly detector . Its performance is assessed and compared against multiple link inference and anomaly detection baselines over 2 popular ICS datasets (SWaT and WADI). Comparative results demonstrate the AHGA’s reliability in capturing sophisticated process-oriented relations among heterogeneous devices as well as its effectiveness in boosting the performance of anomalous pattern recognition at device-level granularity .},
  archive      = {J_PR},
  author       = {Shuaiyi L(y)u and Kai Wang and Liren Zhang and Bailing Wang},
  doi          = {10.1016/j.patcog.2023.109661},
  journal      = {Pattern Recognition},
  pages        = {109661},
  shortjournal = {Pattern Recognition},
  title        = {Process-oriented heterogeneous graph learning in GNN-based ICS anomalous pattern recognition},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stage information diffusion for joint depth and
surface normal estimation. <em>PR</em>, <em>141</em>, 109660. (<a
href="https://doi.org/10.1016/j.patcog.2023.109660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth and surface normal estimations are important for 3D geometric perception, which has numerous applications including autonomous vehicles and robots. In this paper, we propose a lightweight Multi-stage Information Diffusion Network (MIDNet) for the simultaneous prediction of depth and surface normals from a single RGB image . To obtain semantic and detail-preserving features, we adopt a high-resolution network as our backbone to learn multi-scale features, which are then fused into shared features for the two tasks. To mutually boost each task, a Cross-Correlation Attention Module (CCAM) is proposed to adaptively integrate information for the prediction of the two tasks in multiple stages, including feature-level information interaction and task-level information interaction. Ablation studies show that the proposed multi-stage information diffusion strategy can boost the performance gain for the two tasks at different levels. Compared to current state-of-the-art methods on the NYU Depth V2, Stanford 2D-3D-Semantic and KITTI datasets, our method achieves superior performance for both monocular depth and surface normal estimation.},
  archive      = {J_PR},
  author       = {Zhiheng Fu and Siyu Hong and Mengyi Liu and Hamid Laga and Mohammed Bennamoun and Farid Boussaid and Yulan Guo},
  doi          = {10.1016/j.patcog.2023.109660},
  journal      = {Pattern Recognition},
  pages        = {109660},
  shortjournal = {Pattern Recognition},
  title        = {Multi-stage information diffusion for joint depth and surface normal estimation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved robustness of vision transformers via prelayernorm
in patch embedding. <em>PR</em>, <em>141</em>, 109659. (<a
href="https://doi.org/10.1016/j.patcog.2023.109659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have recently demonstrated state-of-the-art performance in various vision tasks, replacing convolutional neural networks (CNNs). However, because ViT has a different architectural design than CNN, it may behave differently. To investigate whether ViT has a different performance or robustness, we tested ViT and CNN under various imaging conditions in practical vision tasks. We confirmed that for most image transformations, ViT’s robustness was comparable or even better than that of CNN. However, for contrast enhancement, ViT performed particularly poorly. We show that this is because positional embedding in ViT’s patch embedding can work improperly when the color scale changes. We demonstrate that the use of PreLayerNorm, a modified patch embedding structure, ensures the consistent behavior of ViT. Results demonstrate that ViT with PreLayerNorm exhibited improved robustness in the contrast-varying environments.},
  archive      = {J_PR},
  author       = {Bum Jun Kim and Hyeyeon Choi and Hyeonah Jang and Dong Gu Lee and Wonseok Jeong and Sang Woo Kim},
  doi          = {10.1016/j.patcog.2023.109659},
  journal      = {Pattern Recognition},
  pages        = {109659},
  shortjournal = {Pattern Recognition},
  title        = {Improved robustness of vision transformers via prelayernorm in patch embedding},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smoothing group l1/2 regularized discriminative broad
learning system for classification and regression. <em>PR</em>,
<em>141</em>, 109656. (<a
href="https://doi.org/10.1016/j.patcog.2023.109656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the framework of the smoothing group L 1 / 2 L1/2 regularized discriminative broad learning system for pattern classification and regression. The core idea is to improve the sparseness of the standard broad learning system and improve performance on recognition and generalization. First, the ε ε -dragging technique is introduced into the standard broad learning system to relax regression targets and enlarge distances between categories. Then, we integrate the group L 1 / 2 L1/2 regularization to optimize the network architecture to achieve sparsity . For the original group L 1 / 2 L1/2 regularization, the objective function is non-convex and non-smooth, which is hard for theoretical analysis. Therefore, we propose a simple and effective smoothing technique, i.e.,smoothing group L 1 / 2 L1/2 regularization, which can effectively eliminate the deficiency of the original group L 1 / 2 L1/2 regularization. As a result, the final weights projection matrix has a compact form and shows discriminative power capability. In addition, the alternating direction method of multipliers was adopted to optimize the algorithm. The simulation results show that the proposed algorithm has redundancy control capability and improved performance on recognition and generalization. The simulation results proves the efficiency of the theoretical analysis.},
  archive      = {J_PR},
  author       = {Dengxiu Yu and Qian Kang and Junwei Jin and Zhen Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.109656},
  journal      = {Pattern Recognition},
  pages        = {109656},
  shortjournal = {Pattern Recognition},
  title        = {Smoothing group l1/2 regularized discriminative broad learning system for classification and regression},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge transduction for cross-domain few-shot learning.
<em>PR</em>, <em>141</em>, 109652. (<a
href="https://doi.org/10.1016/j.patcog.2023.109652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Few-Shot Learning (CDFSL) aims to classify new categories from new domains with few samples. It confronts a greater domain shift than Few-Shot Learning (FSL). Based on the transfer learning framework, we propose a Knowledge Transduction method (KT) to alleviate domain shift and achieve few-shot recognition. First, a feature adaptation module based on feed-forward attention is constructed to learn domain-adapted features. The feature adaptation module weakens domain shift by transducing knowledge from an auxiliary dataset to the new dataset. Second, a feature transduction module based on deep sparse representation is developed to gather class semantics from limited support images. The feature transduction module transduces knowledge from support images to query images for few-shot recognition. In addition, a stochastic image augmentation method is proposed for FSL to train a more generalized model through consistency representation learning . Our method achieves competitive accuracy on four CDFSL datasets and four FSL datasets compared to state-of-the-art methods. The source code is available at https://github.com/XDUpfLi/KT .},
  archive      = {J_PR},
  author       = {Pengfang Li and Fang Liu and Licheng Jiao and Shuo Li and Lingling Li and Xu Liu and Xinyan Huang},
  doi          = {10.1016/j.patcog.2023.109652},
  journal      = {Pattern Recognition},
  pages        = {109652},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge transduction for cross-domain few-shot learning},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor train factorization under noisy and incomplete data
with automatic rank estimation. <em>PR</em>, <em>141</em>, 109650. (<a
href="https://doi.org/10.1016/j.patcog.2023.109650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a powerful tool in analyzing multi-dimensional data, tensor train (TT) decomposition shows superior performance compared to other tensor decomposition formats. Existing TT decomposition methods , however, either easily overfit with noise, or require substantial fine-tuning to strike a balance between recovery accuracy and model complexity. To avoid the above shortcomings, this paper treats the TT decomposition in a fully Bayesian perspective , which includes automatic TT rank determination and noise power estimation. Theoretical justification on adopting the Gaussian-product-Gamma priors for inducing sparsity on the slices of the TT cores is provided, thus allowing the model complexity to be automatically determined even when the observed tensor data is noisy and contains many missing values. Furthermore, using the variational inference framework, an effective learning algorithm on the probabilistic model parameters is derived. Simulations on synthetic data demonstrate that the proposed algorithm accurately recovers the underlying TT structure from incomplete noisy observations. Further experiments on image and video data also show its superior performance to other existing TT decomposition algorithms.},
  archive      = {J_PR},
  author       = {Le Xu and Lei Cheng and Ngai Wong and Yik-Chung Wu},
  doi          = {10.1016/j.patcog.2023.109650},
  journal      = {Pattern Recognition},
  pages        = {109650},
  shortjournal = {Pattern Recognition},
  title        = {Tensor train factorization under noisy and incomplete data with automatic rank estimation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep feature annotation by iterative meta-pseudo-labeling on
2D projections. <em>PR</em>, <em>141</em>, 109649. (<a
href="https://doi.org/10.1016/j.patcog.2023.109649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absence of large annotated datasets to train deep neural networks (DNNs) is an issue since manual annotation is time-consuming, expensive, and error-prone. Semi-supervised learning techniques can address the problem propagating pseudo labels from supervised to unsupervised samples. However, they still require training and validation sets with many supervised samples. This work proposes a methodology, namely Deep Feature Annotation (DeepFA), that dismisses the validation set and uses very few supervised samples (e.g., 1\% of the dataset). DeepFA modifies the feature spaces of a DNN along with meta-pseudo-labeling iterations in a 2D non-linear projection space using the most confidently labeled samples of an optimum-path forest semi-supervised classifier. We present a comprehensive study on DeepFA and a new variant that detects the best DNN model for generalization during the pseudo-labeling iterations. We evaluate components of DeepFA on eight datasets, finding the best DeepFA approach and showing that it outperforms self-pseudo-labeling.},
  archive      = {J_PR},
  author       = {Bárbara C. Benato and Alexandru C. Telea and Alexandre X. Falcão},
  doi          = {10.1016/j.patcog.2023.109649},
  journal      = {Pattern Recognition},
  pages        = {109649},
  shortjournal = {Pattern Recognition},
  title        = {Deep feature annotation by iterative meta-pseudo-labeling on 2D projections},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do all roads lead to rome? Studying distance measures in the
context of machine learning. <em>PR</em>, <em>141</em>, 109646. (<a
href="https://doi.org/10.1016/j.patcog.2023.109646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many machine learning and data mining tasks are based on distance measures, so a large amount of literature addresses this aspect somehow. Due to the broad scope of the topic, this paper aims to provide an overview of the use of these measures in the most common machine learning problems, pointing out those aspects to consider to choose the most appropriate measure for a particular task. For this purpose, the most recent works addressing the subject were reviewed and seven of the most commonly used measures were analyzed, investigating in detail their main properties and applications. Different experiments were carried out to study their relationships and compare their performance. The degradation of the results in the presence of noise was also considered, as well as the execution time required by each measure.},
  archive      = {J_PR},
  author       = {Eva Blanco-Mallo and Laura Morán-Fernández and Beatriz Remeseiro and Verónica Bolón-Canedo},
  doi          = {10.1016/j.patcog.2023.109646},
  journal      = {Pattern Recognition},
  pages        = {109646},
  shortjournal = {Pattern Recognition},
  title        = {Do all roads lead to rome? studying distance measures in the context of machine learning},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic-guided de-attention with sharpened triplet marginal
loss for visual place recognition. <em>PR</em>, <em>141</em>, 109645.
(<a href="https://doi.org/10.1016/j.patcog.2023.109645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to Earth-level Street View images from Google Maps, a visual image geo-localization can estimate the coarse location of a query image with a visual place recognition process. However, this can get very challenging when non-static objects change with time, severely degrading image retrieval accuracy. We address the problem of city-scale visual place recognition in complex urban environments crowded with non-static clutters. To this end, we first analyze what clutters degrade similarity matching between the query and database images. Second, we design a self-supervised trainable de-attention module that prevents the network from focusing on non-static objects in an input image. In addition, we propose a novel triplet marginal loss called sharpened triplet marginal loss to make feature descriptors more discriminative. Lastly, due to the lack of geo-tagged public datasets with a high density of non-static objects, we propose a clutter augmentation method to evaluate our approach. The experimental results show that our model has notably improved over the existing attention methods in geo-localization tasks on the public benchmark datasets and on their augmented versions with high population and traffic. Our code is available at https://github.com/ccsmm78/deattention_with_stml_for_vpr .},
  archive      = {J_PR},
  author       = {Seung-Min Choi and Seung-Ik Lee and Jae-Yeong Lee and In So Kweon},
  doi          = {10.1016/j.patcog.2023.109645},
  journal      = {Pattern Recognition},
  pages        = {109645},
  shortjournal = {Pattern Recognition},
  title        = {Semantic-guided de-attention with sharpened triplet marginal loss for visual place recognition},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalization of deep learning models for natural gas
indication in 2D seismic data. <em>PR</em>, <em>141</em>, 109642. (<a
href="https://doi.org/10.1016/j.patcog.2023.109642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods based on Machine Learning and Deep Learning are increasingly popular to help interpret large volumes of data that belong to various areas and seek to fulfill multiple tasks. One of these areas studies seismic data in the search for hydrocarbon reserves, for which Deep Learning models are trained, showing acceptable results for low study data. However, these models present generalization problems. Their performance tends to decrease when used on seismic data from new exploration. This tendency is particularly true for 2D data, which have many features. This work presents a method to improve the generalization of the Deep Learning model for the indication of natural gas in 2D seismic data based on the recommendation of training data and hyperparameter operations of the model. The tests used a database of the Parnaíba basin in northeast Brazil . Experiments showed an increase in the correct indication of natural gas that varies according to the metric 8\% ≤ R e c a l l ≤ 37\% 8\%≤Recall≤37\% , with a fluctuation in the increase of false positives of − 2\% ≤ P r e c i s i o n ≤ 13\% −2\%≤Precision≤13\% . It is an improvement in the generalization of the Deep Learning model of up to 11\% according to the F1 score metric or up to 10\% according to the IoU metric.},
  archive      = {J_PR},
  author       = {Luis Fernando Marin Sepulveda and Marcelo Gattass and Aristofanes Correa Silva and Roberto Quevedo and Diogo Michelon and Carlos Siedschlag and Roberto Ribeiro},
  doi          = {10.1016/j.patcog.2023.109642},
  journal      = {Pattern Recognition},
  pages        = {109642},
  shortjournal = {Pattern Recognition},
  title        = {Generalization of deep learning models for natural gas indication in 2D seismic data},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Presumably correct decision sets. <em>PR</em>, <em>141</em>,
109640. (<a href="https://doi.org/10.1016/j.patcog.2023.109640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents the presumably correct decision sets as a tool to analyze uncertainty in the form of inconsistency in decision systems. As a first step, problem instances are gathered into three regions containing weak members, borderline members, and strong members. This is accomplished by using the membership degrees of instances to their neighborhoods while neglecting their actual labels. As a second step, we derive the presumably correct and incorrect sets by contrasting the decision classes determined by a neighborhood function with the actual decision classes. We extract these sets from either the regions containing strong members or the whole universe, which defines the strict and relaxed versions of our theoretical formalism. These sets allow isolating the instances difficult to handle by machine learning algorithms as they are responsible for inconsistent patterns. The simulations using synthetic and real-world datasets illustrate the advantages of our model compared to rough sets, which is deemed a solid state-of-the-art approach to cope with inconsistency. In particular, it is shown that we can increase the accuracy of selected classifiers up to 36\% by weighting the presumably correct and incorrect instances during the training process.},
  archive      = {J_PR},
  author       = {Gonzalo Nápoles and Isel Grau and Agnieszka Jastrzębska and Yamisleydi Salgueiro},
  doi          = {10.1016/j.patcog.2023.109640},
  journal      = {Pattern Recognition},
  pages        = {109640},
  shortjournal = {Pattern Recognition},
  title        = {Presumably correct decision sets},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BDNet: A BERT-based dual-path network for text-to-image
cross-modal person re-identification. <em>PR</em>, <em>141</em>, 109636.
(<a href="https://doi.org/10.1016/j.patcog.2023.109636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image person re-identification (TI-ReID) aims to provide a descriptive sentence to find a specific person in the gallery. The task is very challenging due to the huge feature differences between both image and text descriptions. Currently, most approaches use the idea of combining global and local features to get more fine-grained features. However, these methods usually acquire local features with the help of human pose or segmentation models , which makes it difficult to use in realistic scenarios due to the introduction of additional models or complex training evaluation strategies. To facilitate practical applications, we propose a BERT-based framework for dual-path TI-ReID. Without the help of additional models, our approach directly employs visual attention in the global feature extraction network to allow the network to adaptively learn to focus on salient local features in image and text descriptions, which enhances the network’s attention to local information through a visual attention mechanism , thus strengthening the global feature representation and effectively improving the global feature representation. In addition, to learn text and image modality invariant feature representations, we propose a convolutional shared network (CSN) to learn image and text features together. To optimize cross-modal feature distances more effectively, we propose a global hybrid modal triplet global metric loss. In addition to combining local metric learning and global metric learning, we also introduce the CMPM loss and CMPC loss to jointly optimize the proposed model. Extensive experiments on the CUHK-PEDES dataset show that the proposed method performs significantly better than the current research results, achieving a Rank-1/mAP accuracy of 66.27\%/ 57.04\%.},
  archive      = {J_PR},
  author       = {Qiang Liu and Xiaohai He and Qizhi Teng and Linbo Qing and Honggang Chen},
  doi          = {10.1016/j.patcog.2023.109636},
  journal      = {Pattern Recognition},
  pages        = {109636},
  shortjournal = {Pattern Recognition},
  title        = {BDNet: A BERT-based dual-path network for text-to-image cross-modal person re-identification},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust federated learning under statistical heterogeneity
via hessian spectral decomposition. <em>PR</em>, <em>141</em>, 109635.
(<a href="https://doi.org/10.1016/j.patcog.2023.109635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a collaborative machine learning paradigm in which a global model is learned via aggregating local ones. Although statistical heterogeneity of the local training data is necessary for the generalisability of the global model, it also introduces local model “drift” that slows down the convergence. Thus, how to optimally aggregate local models in FL remains an open problem. Recognising that training data lends varying evidential credence to different parts of a local model, we propose a novel approach to exploit such evidential asymmetry in FL aggregation in not independent and identically distributed (non-IID) data by applying a unique weight coefficient to each of the local parameter updates. To this end, we measure the parameter-level evidential credence making use of the eigenvalues of the Hessian of the local likelihood function, which are theoretically connected to the observed Fisher information . We employ these eigenpairs to propose a novel aggregation method, which we name FedHess. Our experiments show FedHess achieves smoother and faster convergence to a more accurate global model when compared with popular baselines such as Federated Average (FedAvg), FedProx, SCAFFOLD, Federated Curvature (FedCurv) and FedDF across different types of heterogeneous training data drawn from a number of benchmark datasets.},
  archive      = {J_PR},
  author       = {Adnan Ahmad and Wei Luo and Antonio Robles-Kelly},
  doi          = {10.1016/j.patcog.2023.109635},
  journal      = {Pattern Recognition},
  pages        = {109635},
  shortjournal = {Pattern Recognition},
  title        = {Robust federated learning under statistical heterogeneity via hessian spectral decomposition},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypercomplex context guided interaction modeling for scene
graph generation. <em>PR</em>, <em>141</em>, 109634. (<a
href="https://doi.org/10.1016/j.patcog.2023.109634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitively, humans can consciously and subjectively attend to the interactions between objects, and thus infer reasonable visual relations. However, mainstream approaches of Scene Graph Generation (SGG) strive to alleviate the long-tailed distribution problem with various complicated re-weighting strategies, where a simple concatenation of the refined object features is treated as the final representation of visual relations. In spite of their remarkable progress, such an operation overlooks the importance of interaction on relation recognition. To tackle the problem, this work devises a hyper C omplex- C ontext guided I nteraction M odeling (CCIM for short) plug-in, which can be successfully assimilated by the existing methods for performance improvement . Specifically, we first extract the contextual relation feature determined by the constraint r e l a t i o n ≈ u n i o n ( h e a d , t a i l ) − h e a d o b j e c t − t a i l o b j e c t relation≈union(head,tail)−headobject−tailobject . Then, we encode the features of relations and objects into hypercomplex space, with three imaginary components, to learn more expressive representations for SGG. Next, guided by the context, we can capture the interaction between a head or tail object and their relation through the Hamilton product. We further reinforce the interaction between enhanced hypercomplex-valued representations of the two entities with Quaternion inner product. At last, the concatenation of all components from the learned hypercomplex feature is adopted as our final relation representation. Extensive experiments on the popular benchmark Visual Genome in various existing approaches demonstrate the effectiveness and generalization of our proposed model-agnostic method under comprehensive evaluation metrics .},
  archive      = {J_PR},
  author       = {Zheng Wang and Xing Xu and Yadan Luo and Guoqing Wang and Yang Yang},
  doi          = {10.1016/j.patcog.2023.109634},
  journal      = {Pattern Recognition},
  pages        = {109634},
  shortjournal = {Pattern Recognition},
  title        = {Hypercomplex context guided interaction modeling for scene graph generation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view unsupervised feature selection with tensor robust
principal component analysis and consensus graph learning. <em>PR</em>,
<em>141</em>, 109632. (<a
href="https://doi.org/10.1016/j.patcog.2023.109632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multi-view unsupervised feature selection has attracted much attention due to its efficiency and better interpretability in processing high-dimensional multi-view datasets. Most existing methods rely on the constructed similarity matrices to obtain reliable pseudo labels to guide the feature selection. However, the considerable adverse noise in the raw data inevitably impedes the exploration of true underlying similarity structures. Besides, the inter-view correlations are often ignored during the common representation learning , which limits the effective fusion of the essential information from multiple views. To solve these issues, we design a novel robust multi-view unsupervised feature selection framework. Specifically, our method seeks a set of noise-free view-specific similarity matrices by leveraging tensor robust principal component analysis, where the high-order connections among different views are well exploited through the constructed low-rank tensor. Meanwhile, a high-quality consensus similarity matrix is adaptively learned from the view-specific representations within the same unified framework to capture the shared local structures. To enhance the discriminative ability of the feature selection matrix, we further impose a rank constraint on the consensus similarity matrix to obtain reliable pseudo cluster indicators. We present an efficient optimization algorithm ground on the alternating direction method of multipliers to solve the proposed model. Experimental results on six multi-view datasets confirm the superiority of our method.},
  archive      = {J_PR},
  author       = {Cheng Liang and Lianzhi Wang and Li Liu and Huaxiang Zhang and Fei Guo},
  doi          = {10.1016/j.patcog.2023.109632},
  journal      = {Pattern Recognition},
  pages        = {109632},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view unsupervised feature selection with tensor robust principal component analysis and consensus graph learning},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-hypothesis representation learning for
transformer-based 3D human pose estimation. <em>PR</em>, <em>141</em>,
109631. (<a href="https://doi.org/10.1016/j.patcog.2023.109631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite significant progress, estimating 3D human poses from monocular videos remains a challenging task due to depth ambiguity and self-occlusion. Most existing works attempt to solve both issues by exploiting spatial and temporal relationships. However, those works ignore the fact that it is an inverse problem where multiple feasible solutions ( i.e. , hypotheses) exist. To relieve this limitation, we propose a Multi-Hypothesis Transformer that learns spatio-temporal representations of multiple plausible pose hypotheses. In order to effectively model multi-hypothesis dependencies and build strong relationships across hypothesis features, we introduce a one-to-many-to-one three-stage framework: (i) Generate multiple initial hypothesis representations; (ii) Model self-hypothesis communication, merge multiple hypotheses into a single converged representation and then partition it into several diverged hypotheses; (iii) Learn cross-hypothesis communication and aggregate the multi-hypothesis features to synthesize the final 3D pose. Through the above processes, the final representation is enhanced and the synthesized pose is much more accurate. Extensive experiments show that the proposed method achieves state-of-the-art results on two challenging datasets: Human3.6M and MPI-INF-3DHP. The code and models are available at https://github.com/Vegetebird/MHFormer .},
  archive      = {J_PR},
  author       = {Wenhao Li and Hong Liu and Hao Tang and Pichao Wang},
  doi          = {10.1016/j.patcog.2023.109631},
  journal      = {Pattern Recognition},
  pages        = {109631},
  shortjournal = {Pattern Recognition},
  title        = {Multi-hypothesis representation learning for transformer-based 3D human pose estimation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiamRank: A siamese based visual tracking network with
ranking strategy. <em>PR</em>, <em>141</em>, 109630. (<a
href="https://doi.org/10.1016/j.patcog.2023.109630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual tracking is one of the most fundamental and active research topics in the field of computer vision with industrial applications. It has to solve 2 core problems, namely classification and state estimation. Most of the existing trackers utilize deep networks to extract the features of the object. Especially, Siamese based approaches have prevailed in tracking tasks, which generate labels for both positive and negative samples. However, these approaches introduce ambiguities and inaccurate semantic information at the same time, which may cause failure in classification. To address this problem, we present SiamRank by adding the sequential information of different samples in one image. We apply the proposed network to 2 backbones (AlexNet and GoogLeNet) to testify its general performance. Extensive experiments have been carried out on 7 popular benchmarks, including OTB100, LaSOT, GOT-10 K, TrackingNet, NFS, UAV123 and VOT2019, and our tracker achieves state-of-the-art results. Specifically, on both large-scale TrackingNet dataset and long-time LaSOT dataset, SiamRank surpasses the previous approaches with a relative gain of 10\%, while running at 65 FPS.},
  archive      = {J_PR},
  author       = {Feiyu Meng and Xiaomei Gong and Yi Zhang},
  doi          = {10.1016/j.patcog.2023.109630},
  journal      = {Pattern Recognition},
  pages        = {109630},
  shortjournal = {Pattern Recognition},
  title        = {SiamRank: A siamese based visual tracking network with ranking strategy},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deepfacelab: Integrated, flexible and extensible
face-swapping framework. <em>PR</em>, <em>141</em>, 109628. (<a
href="https://doi.org/10.1016/j.patcog.2023.109628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face swapping has drawn a lot of attention for its compelling performance. However, current deepfake methods suffer the effects of obscure workflow and poor performance. To solve these problems, we present DeepFaceLab, the current dominant deepfake framework for practical face-swapping. It provides the necessary tools as well as an easy-to-use way to conduct high-quality face-swapping. It also offers a flexible and loose coupling structure for people who need to strengthen their pipeline with other features without writing complicated boilerplate code. We detail the principles that drive the implementation of DeepFaceLab and introduce its pipeline. DeepFaceLab could achieve cinema-level results with high fidelity as our supplemental video shows. We also demonstrate the advantage of our system by comparing our approach with other face-swapping methods. Deepfake defense not only requires the research of detection but also requires the efforts of generation methods. As for a popular and practical toolkit, we encourage users to promote harmless deepfake-entertainment content on social media, reminding the public of the existence of deepfake when they are looking for entertainment.},
  archive      = {J_PR},
  author       = {Kunlin Liu and Ivan Perov and Daiheng Gao and Nikolay Chervoniy and Wenbo Zhou and Weiming Zhang},
  doi          = {10.1016/j.patcog.2023.109628},
  journal      = {Pattern Recognition},
  pages        = {109628},
  shortjournal = {Pattern Recognition},
  title        = {Deepfacelab: Integrated, flexible and extensible face-swapping framework},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CPSS-FAT: A consistent positive sample selection for object
detection with full adaptive threshold. <em>PR</em>, <em>141</em>,
109627. (<a href="https://doi.org/10.1016/j.patcog.2023.109627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent CNN-based methods for object detection largely focus on training the backbone of the object detector, neglecting the key part of selecting positive/negative samples. Instead, based on our analyses about the limitations and inconsistency of existing positive sample selection, we propose a novel Consistent Positive Sample Selection (CPSS) method to select the positive samples automatically, which joints Box-IoU and normalized central distance of object-anchor by mutual weighting. Meanwhile, we employ a consistent approach to location quality evaluation for suppressing the false-positive predicted box when inferencing. Furthermore, an auxiliary Full Adaptive Threshold (FAT) post-processing is also adopted according to the objects’ occlusion level to improve the recall ratio. We implement the proposed CPSS-FAT detector using MS COCO 2017 and CityScapes datasets, the comparing results indicate that our approaches are effective and robust to the different objects in an open world. Especially, we achieve 52.2\% A P AP and 43.1\% A R S ARS , outperforming most existing detectors.},
  archive      = {J_PR},
  author       = {Xiaobao Yang and Junsheng Wu and Lang He and Sugang Ma and Zhiqiang Hou and Wei Sun},
  doi          = {10.1016/j.patcog.2023.109627},
  journal      = {Pattern Recognition},
  pages        = {109627},
  shortjournal = {Pattern Recognition},
  title        = {CPSS-FAT: A consistent positive sample selection for object detection with full adaptive threshold},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RPI-CapsuleGAN: Predicting RNA-protein interactions through
an interpretable generative adversarial capsule network. <em>PR</em>,
<em>141</em>, 109626. (<a
href="https://doi.org/10.1016/j.patcog.2023.109626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RNA-protein interactions (RPI) play a crucial regulatory role in cellular physiological processes. The study and prediction of RPIs can be insightful for exploring disease mechanisms and drug target design. Traditional RPI prediction methods relied mainly on tedious and expensive biological experiments, and there is an increasing interest in developing more cost-effective computational methods to predict RPIs. This work proposes an interpretable RPI-CapsuleGAN method for RPI prediction based on a generative adversarial capsule network with a convolutional block attention module. First, RPI-CapsuleGAN extracts and fuses multiple features to characterize RNA and protein sequences. Subsequently, the elastic net feature selection method is used to retain features that are highly informative to RPI prediction. Finally, we introduce a convolutional attention mechanism into the generative adversarial capsule network for the first time in order to construct the RPI prediction framework, which is shown to improve the model feature learning of interpretable and expression ability, and effectively solves the problem of the disappearance of the model spatial structure hierarchy. Based on a five-fold cross-validation test, the prediction accuracy of the RPI-CapsuleGAN method reaches 97.1\%, 88.8\%, 92.5\%, 97.3\%, and 87.8\% for datasets RPI488, RPI369, RPI2241, RPI1807, and RPI1446. The RPI-CapsuleGAN method has higher accuracy than state-of-the-art RPI prediction methods that use the same datasets. In the test dataset NPInter227 constructed in this paper, five groups of test sets are composed of positive samples and five groups of negative samples, the prediction accuracy reaches 97.38\%, 96.48\%, 97.38\%, 97.81\%, and 97.15\%, respectively, outperforming other mainstream deep learning algorithms. In addition, RPI-CapsuleGAN obtained better results for the prediction of independent test datasets. Extensive experiments detailed here show that RPI-CapsuleGAN can provide an efficient, accurate, and stable method for RPI prediction.},
  archive      = {J_PR},
  author       = {Yifei Wang and Xue Wang and Cheng Chen and Hongli Gao and Adil Salhi and Xin Gao and Bin Yu},
  doi          = {10.1016/j.patcog.2023.109626},
  journal      = {Pattern Recognition},
  pages        = {109626},
  shortjournal = {Pattern Recognition},
  title        = {RPI-CapsuleGAN: Predicting RNA-protein interactions through an interpretable generative adversarial capsule network},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view prototype-based disambiguation for partial label
learning. <em>PR</em>, <em>141</em>, 109625. (<a
href="https://doi.org/10.1016/j.patcog.2023.109625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study the multi-view partial label learning (MVPLL) problem, where each instance is depicted by different view features and associated with a set of candidate labels, among which a true label exists but is inaccessible in the training phase. Most existing PLL methods only consider single-view case, which learn view classifier independently and neglect the view correlations, thus can not be applied to solve MVPLL problem. Due to the non-deep framework, traditional MVPLL approach is weak in the representation ability, so its performance is still to be improved. To solve the MVPLL problem, a deep multi-view prototype-based disambiguation approach is proposed in this paper. Specifically, we innovatively employ the deep neural network for multi-view ambiguously-labeled image classification to enhance the representation ability, which makes use of the information fusion between multiple views. To improve the discriminative ability, we propose multi-view prototype-based label disambiguation algorithm. On theoretical aspect, an estimation error bound for view-risk estimator is established, which is shown to be larger than that for fuse-risk estimator. Experiments demonstrate the superiorities of our proposed method in terms of the prediction accuracy.},
  archive      = {J_PR},
  author       = {Shiding Sun and Xiaotong Yu and Yingjie Tian},
  doi          = {10.1016/j.patcog.2023.109625},
  journal      = {Pattern Recognition},
  pages        = {109625},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view prototype-based disambiguation for partial label learning},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tensor completion via convolutional sparse coding with small
samples-based training. <em>PR</em>, <em>141</em>, 109624. (<a
href="https://doi.org/10.1016/j.patcog.2023.109624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor data often suffer from missing value problems due to the complex high-dimensional structure while acquiring them. To complete the missing information, lots of Low-Rank Tensor Completion (LRTC) methods have been proposed, most of which depend on the low-rank property of tensor data. In this way, the low-rank component of the original data could be recovered roughly. However, the shortcoming is that the detailed information can not be fully restored, no matter the Sum of the Nuclear Norm (SNN) nor the Tensor Nuclear Norm (TNN) based methods. On the contrary, in the field of signal processing, Convolutional Sparse Coding (CSC) can provide a good representation of the high-frequency component of the image, which is generally associated with the detail component of the data. To this end, we propose two novel methods, LRTC-CSC-I and LRTC-CSC-II, which adopt CSC as a supplementary regularization for LRTC to capture the high-frequency components. Therefore, the LRTC-CSC methods can not only solve the missing value problem but also recover the details. Moreover, the regularizer CSC can be trained with small samples due to the sparsity characteristic. Extensive experiments show the effectiveness of LRTC-CSC methods, and quantitative evaluation indicates that the performance of our models are superior to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Tianchi Liao and Zhebin Wu and Chuan Chen and Zibin Zheng and Xiongjun Zhang},
  doi          = {10.1016/j.patcog.2023.109624},
  journal      = {Pattern Recognition},
  pages        = {109624},
  shortjournal = {Pattern Recognition},
  title        = {Tensor completion via convolutional sparse coding with small samples-based training},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved differential evolution algorithm for quantifying
fraudulent transactions. <em>PR</em>, <em>141</em>, 109623. (<a
href="https://doi.org/10.1016/j.patcog.2023.109623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of fraudulent credit card transactions is a complex problem mainly due to the following factors: 1) The relative behavior of customers and fraudsters may alter over time. 2) The ratio of legitimate to fraudulent transactions is highly imbalanced, and 3) Investigators examine a small segment of transactions in a reasonable time frame. Researchers have proposed various algorithms to identify potential fraud in a new incoming transaction. However, these approaches require significant human investigator effort and are sometimes misleading. To address this issue, this paper proposes an improved multiobjective differential evolution (DE) algorithm to estimate the distribution of fraudulent transactions in a set of new incoming transactions, referred to as quantifying fraudulent transactions. Our paper has three major novelties. First, we present the problem formulation of cost-based feature selection with maximum quantification ability. Second, we improve the DE by applying effective trial vector generation algorithms to the random control parameter settings to exploit the advantage of individual DE variants. Third, we develop the maximum-relevancy-minimum-redundancy-based Pareto refining operator to enhance the self-learning ability of individuals in Pareto solutions . We compare our approach against four other modifications of DE and five state-of-the-art evolutionary algorithms on real-time credit datasets in streaming and non-streaming frameworks using hyper-volume, two-set coverage, and spread performance metrics.},
  archive      = {J_PR},
  author       = {Deepak Kumar Rakesh and Prasanta K. Jana},
  doi          = {10.1016/j.patcog.2023.109623},
  journal      = {Pattern Recognition},
  pages        = {109623},
  shortjournal = {Pattern Recognition},
  title        = {An improved differential evolution algorithm for quantifying fraudulent transactions},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Adaptive fusion affinity graph with noise-free online
low-rank representation for natural image segmentation. <em>PR</em>,
<em>141</em>, 109611. (<a
href="https://doi.org/10.1016/j.patcog.2023.109611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affinity graph-based segmentation methods have become a major trend in computer vision . The performance of these methods rely on the constructed affinity graph, with particular emphasis on the neighborhood topology and pairwise affinities among superpixels. However, these graph-based methods ignore the noisy data from images, that influence the accuracy of pairwise similarities . Multiscale combinatorial grouping and graph fusion also generate a higher computational complexity . In this paper, we propose an adaptive fusion affinity graph with noise-free low-rank representation in an online manner for natural image segmentation . An input image is first over-segmented into superpixels at different scales and then filtered by an improved kernel density estimation method. Moreover, we select global nodes of these superpixels on the basis of their subspace-preserving presentation, which reveals the feature distribution of superpixels exactly. To reduce time complexity while improving performance, a sparse representation of global nodes based on noise-free online low-rank representation is used to obtain a global graph at each scale. Experimental results on BSD300, BSD500, MSRC, SBD, and PASCAL VOC show the effectiveness of our method in comparison with the state-of-the-art approaches. The code is available at https://github.com/Yangzhangcst/AFA-graph .},
  archive      = {J_PR},
  author       = {Yang Zhang and Moyun Liu and Huiming Zhang and Guodong Sun and Jingwu He},
  doi          = {10.1016/j.patcog.2023.109611},
  journal      = {Pattern Recognition},
  pages        = {109611},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive fusion affinity graph with noise-free online low-rank representation for natural image segmentation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L1-norm discriminant analysis via bhattacharyya error bounds
under laplace distributions. <em>PR</em>, <em>141</em>, 109609. (<a
href="https://doi.org/10.1016/j.patcog.2023.109609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {L1-norm discriminant analysis has been proposed to enhance the robustness of classical LDA in the presence of outliers. This paper develops L1-norm discriminant analysis by exploring Bhattacharyya error bounds under Laplace distributions. Unlike some previous models, we assume that the samples of each class in the projected space follow Laplace distributions. It is the first time that the Bhattacharyya error bound is derived under Laplace distributions. It is interesting to note that this bound has a closed-form expression in a one-dimensional projected space. We relax the Bhattacharyya error bound to achieve another bound that can facilitate the design of a tractable model. Since the parameters of Laplace distributions are estimated by the maximum likelihood estimation , this yields the problem of estimating class centroids in our model. We employ a simple yet effective strategy to estimate class centroids in the original space. To address the small-sample-size problem, we transform the original model into a difference criterion by introducing additional parameters. We achieve an alternative representation of our model and design an effective algorithm to solve it. In addition, we also extend our model to its kernel version. The experiments on a series of data sets are done to demonstrate the effectiveness of our method in dealing with contaminated data.},
  archive      = {J_PR},
  author       = {Zhizheng Liang and Lei Zhang},
  doi          = {10.1016/j.patcog.2023.109609},
  journal      = {Pattern Recognition},
  pages        = {109609},
  shortjournal = {Pattern Recognition},
  title        = {L1-norm discriminant analysis via bhattacharyya error bounds under laplace distributions},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Margin-aware rectified augmentation for long-tailed
recognition. <em>PR</em>, <em>141</em>, 109608. (<a
href="https://doi.org/10.1016/j.patcog.2023.109608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-tailed data distribution is prevalent in real world and it poses great challenge on deep neural network training. In this paper, we propose Margin-aware Rectified Augmentation (MRA) to tackle this problem. Specifically, the MRA consists of two parts. From the data perspective, we analyze that data imbalance will cause the decision boundary be biased, and we propose a novel Margin-aware Rectified mixup (MR-mixup) that adaptively rectifies the biased decision boundary. Furthermore, from the model perspective, we analyze that the imbalance will also lead to consistent ‘gradient suppression’ on minority class logits. Then we propose Reweighted Mutual Learning (RML) that provides extra ‘soft target’ as supervision signal and augments the ‘encouraging gradients’ on the minority classes. We conduct extensive experiments on benchmark datasets CIFAR-LT, ImageNet-LT and iNaturalist18. The results demonstrate that the proposed MRA not only achieves state-of-the-art performance, but also yields a better-calibrated prediction.},
  archive      = {J_PR},
  author       = {Liuyu Xiang and Jungong Han and Guiguang Ding},
  doi          = {10.1016/j.patcog.2023.109608},
  journal      = {Pattern Recognition},
  pages        = {109608},
  shortjournal = {Pattern Recognition},
  title        = {Margin-aware rectified augmentation for long-tailed recognition},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AIDA: Analytic isolation and distance-based anomaly
detection algorithm. <em>PR</em>, <em>141</em>, 109607. (<a
href="https://doi.org/10.1016/j.patcog.2023.109607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many unsupervised anomaly detection algorithms rely on the concept of nearest neighbours to compute the anomaly scores. Such algorithms are popular because there are no assumptions about the data, making them a robust choice for unstructured datasets. However, the number ( k k ) of nearest neighbours, which critically affects the model performance, cannot be tuned in an unsupervised setting. Hence, we propose the new and parameter-free Analytic Isolation and Distance-based Anomaly (AIDA) detection algorithm, that combines the metrics of distance with isolation. Based on AIDA, we also introduce the Tempered Isolation-based eXplanation (TIX) algorithm, which identifies the most relevant features characterizing an outlier, even in large multi-dimensional datasets, improving the overall explainability of the detection mechanism. Both AIDA and TIX are thoroughly tested and compared with state-of-the-art alternatives, proving to be useful additions to the existing set of tools in anomaly detection.},
  archive      = {J_PR},
  author       = {Luis Antonio Souto Arias and Cornelis W. Oosterlee and Pasquale Cirillo},
  doi          = {10.1016/j.patcog.2023.109607},
  journal      = {Pattern Recognition},
  pages        = {109607},
  shortjournal = {Pattern Recognition},
  title        = {AIDA: Analytic isolation and distance-based anomaly detection algorithm},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-level progressive transfer learning for cervical
cancer dose prediction. <em>PR</em>, <em>141</em>, 109606. (<a
href="https://doi.org/10.1016/j.patcog.2023.109606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has accomplished the automation of radiation therapy planning, enhancing its quality and efficiency. However, such progress comes at the cost of a large amount of clinical data. For some low-incidence cancers, i.e., cervical cancer, with limited available data, current data-hungry deep models fail to achieve satisfactory performance. To address this, in this paper, considering that cervical cancer and rectum cancer share the same scanning area and organs at risk (OARs), we resort to transfer learning to transfer the knowledge acquired from rectum cancer (source domain) to cervical cancer (target domain) to perform dose map prediction task. To overcome the possible negative transferring problem, we design a two-phase paradigm to progressively transfer knowledge. In the first phase, we aggregate the data of the two domains by linear interpolation and pre-train an aggregated network with the aggregated data to perceive the target dose distribution beforehand. In the second phase, we elaborately design two modules, i.e., a Feature-level Transfer (FT) Module, and an Image-level Transfer (IT) Module, to selectively transfer knowledge in multi-level. Specifically, the FT module aims to preserve those filters that are more helpful while the IT module tries to highlight those samples with more target-specific knowledge. Extensive experiments proclaim the exemplary performance of our proposed method compared with other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lu Wen and Jianghong Xiao and Jie Zeng and Chen Zu and Xi Wu and Jiliu Zhou and Xingchen Peng and Yan Wang},
  doi          = {10.1016/j.patcog.2023.109606},
  journal      = {Pattern Recognition},
  pages        = {109606},
  shortjournal = {Pattern Recognition},
  title        = {Multi-level progressive transfer learning for cervical cancer dose prediction},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-constrained feature selection in multilabel
classification using an information-theoretic approach. <em>PR</em>,
<em>141</em>, 109605. (<a
href="https://doi.org/10.1016/j.patcog.2023.109605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is one of the key steps in building a predictive model in multi-label classification. However, most of the existing methods do not take into account information about the costs associated with considered features, such as the costs of performing diagnostic medical tests. We consider a problem of cost-constrained multilabel feature selection, which aims to select a feature subset relevant to multiple labels while satisfying a user-specific maximal admissible budget. This approach allows for building a model with high predictive power , for which the cost of making a prediction for a single instance does not exceed the user-specified budget. In this problem, the balance between the feature subset relevance and its cost should be considered concurrently, which is nontrivial in practice because their optimal balance is unknown. In this paper, we propose a novel criterion for cost-constrained multilabel feature selection that combines the relevance and cost of the candidate feature. The relevance measure is derived using the lower bound of the mutual information between the feature subset and label vector. Moreover, we propose an effective method for determining the cost-factor value that controls the trade-off between relevancy and cost. The experimental results on multilabel datasets with various characteristics demonstrate the superiority of the proposed method over conventional methods.},
  archive      = {J_PR},
  author       = {Tomasz Klonecki and Paweł Teisseyre and Jaesung Lee},
  doi          = {10.1016/j.patcog.2023.109605},
  journal      = {Pattern Recognition},
  pages        = {109605},
  shortjournal = {Pattern Recognition},
  title        = {Cost-constrained feature selection in multilabel classification using an information-theoretic approach},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Augmented bilinear network for incremental multi-stock
time-series classification. <em>PR</em>, <em>141</em>, 109604. (<a
href="https://doi.org/10.1016/j.patcog.2023.109604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning models have become dominant in tackling financial time-series analysis problems, overturning conventional machine learning and statistical methods. Most often, a model trained for one market or security cannot be directly applied to another market or security due to differences inherent in the market conditions. In addition, as the market evolves over time, it is necessary to update the existing models or train new ones when new data is made available. This scenario, which is inherent in most financial forecasting applications, naturally raises the following research question: How to efficiently adapt a pre-trained model to a new set of data while retaining performance on the old data, especially when the old data is not accessible? In this paper, we propose a method to efficiently retain the knowledge available in a neural network pre-trained on a set of securities and adapt it to achieve high performance in new ones. In our method, the prior knowledge encoded in a pre-trained neural network is maintained by keeping existing connections fixed, and this knowledge is adjusted for the new securities by a set of augmented connections, which are optimized using the new data. The auxiliary connections are constrained to be of low rank. This not only allows us to rapidly optimize for the new task but also reduces the storage and run-time complexity during the deployment phase. The efficiency of our approach is empirically validated in the stock mid-price movement prediction problem using a large-scale limit order book dataset. Experimental results show that our approach enhances prediction performance as well as reduces the overall number of network parameters.},
  archive      = {J_PR},
  author       = {Mostafa Shabani and Dat Thanh Tran and Juho Kanniainen and Alexandros Iosifidis},
  doi          = {10.1016/j.patcog.2023.109604},
  journal      = {Pattern Recognition},
  pages        = {109604},
  shortjournal = {Pattern Recognition},
  title        = {Augmented bilinear network for incremental multi-stock time-series classification},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi scale pixel attention and feature extraction based
neural network for image denoising. <em>PR</em>, <em>141</em>, 109603.
(<a href="https://doi.org/10.1016/j.patcog.2023.109603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a blind Gaussian denoising network that utilize the features of the input image and its negative for generating denoised output of the same. The proposed network is a dual path model which employs a multi-scale pixel attention (MSPA) block on one path and a multi-scale feature extraction (MSFE) block on another. The concept of using the features of a negative image (that it highlights the low contrast region) in blind Gaussian denoising network is, to the best of our knowledge, a first such attempt. The proposed MSPA and MSFE blocks are designed to focus on the features of the image at multiple scales. The MSPA block focuses on the important features of the negative of the input image whereas the MSFE block focuses on extracting features of the input noisy image . The features of both the images are then combined and a final residual noise is obtained, subtracting which from the input noisy image produces the final denoised result. The proposed network is lightweight and fast, due to the low number of convolutional layers involved, and produces superior results (both quantitatively and qualitatively) when compared with various traditional and learning based blind Gaussian denoising techniques. The code of this paper can be downloaded from https://github.com/RTSIR/NIFBGDNet .},
  archive      = {J_PR},
  author       = {Ramesh Kumar Thakur and Suman Kumar Maji},
  doi          = {10.1016/j.patcog.2023.109603},
  journal      = {Pattern Recognition},
  pages        = {109603},
  shortjournal = {Pattern Recognition},
  title        = {Multi scale pixel attention and feature extraction based neural network for image denoising},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SurroundNet: Towards effective low-light image enhancement.
<em>PR</em>, <em>141</em>, 109602. (<a
href="https://doi.org/10.1016/j.patcog.2023.109602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolution Neural Networks (CNNs) have made substantial progress in the low-light image enhancement task, one critical problem of CNNs is the paradox of model complexity and performance. This paper presents a novel SurroundNet that only involves less than 150 K K parameters (about 80–98 percent size reduction compared to SOTAs) and achieves very competitive performance. The proposed network comprises several Adaptive Retinex Blocks (ARBlock), which can be viewed as a novel extension of Single Scale Retinex in feature space. The core of our ARBlock is an efficient illumination estimation function called Adaptive Surround Function (ASF). It can be regarded as a general form of surround functions and be implemented by convolution layers . In addition, we also introduce a Low-Exposure Denoiser (LED) to smooth the low-light image before the enhancement. We evaluate the proposed method on two real-world low-light datasets. Experimental results demonstrate the superiority of our submitted SurroundNet in both performance and network parameters against State-of-the-Art low-light image enhancement methods. The code is available at https://github.com/ouc-ocean-group/SurroundNet .},
  archive      = {J_PR},
  author       = {Fei Zhou and Xin Sun and Junyu Dong and Xiao Xiang Zhu},
  doi          = {10.1016/j.patcog.2023.109602},
  journal      = {Pattern Recognition},
  pages        = {109602},
  shortjournal = {Pattern Recognition},
  title        = {SurroundNet: Towards effective low-light image enhancement},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Low-rank kernel regression with preserved locality for
multi-class analysis. <em>PR</em>, <em>141</em>, 109601. (<a
href="https://doi.org/10.1016/j.patcog.2023.109601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel ridge regression (KRR) is a kind of efficient supervised algorithm for multi-class analysis. However, limited by the implicit kernel space, current KRR methods have weak abilities to deal with redundant features and hidden local structures. Thus, they may get indifferent results when applied to analyze the data with complicated components. To overcome this weakness and obtain better multi-class regression performance, we propose a new method named low-rank kernel regression with preserved locality (RLRKRR). In this method, data are mapped into an explicit feature space by using the random Fourier feature technique to discover the non-linear relationship between data samples. In addition, during the training of the regression coefficient matrix , the low-rank components of this explicit feature space are simultaneously extracted for reducing the effect of the redundancy. Moreover, the graph regularization is performed on the extracted low-rank components to preserve local structures. Furthermore, the l 2 , p l2,p norm is imposed on the regression error term for relieving the impact of outliers. Based on these strategies, RLRKRR is capable to achieve rewarding results in complicated multi-class data analysis. In the comprehensive experiments conducted on various types of datasets, RLRKRR outperforms several state-of-the-art regression methods in terms of classification accuracy (CA).},
  archive      = {J_PR},
  author       = {Yingxu Wang and Long Chen and Jin Zhou and Tianjun Li and Yufeng Yu},
  doi          = {10.1016/j.patcog.2023.109601},
  journal      = {Pattern Recognition},
  pages        = {109601},
  shortjournal = {Pattern Recognition},
  title        = {Low-rank kernel regression with preserved locality for multi-class analysis},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating participating methods in image analysis
challenges: Lessons from MoNuSAC 2020. <em>PR</em>, <em>141</em>,
109600. (<a href="https://doi.org/10.1016/j.patcog.2023.109600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical image analysis competitions often rank the participants based on a single metric that combines assessments of different aspects of the task at hand. While this is useful for declaring a single winner for a competition, it makes it difficult to assess the strengths and weaknesses of participating algorithms. By involving multiple capabilities (detection, segmentation and classification) and releasing the prediction masks provided by several teams, the MoNuSAC 2020 challenge provides an interesting opportunity to look at what information may be lost by using entangled metrics. We analyse the challenge results based on the “Panoptic Quality” (PQ) used by the organizers, as well as on disentangled metrics that assess the detection, classification and segmentation abilities of the algorithms separately. We show that the PQ hides interesting aspects of the results, and that its sensitivity to small changes in the prediction masks makes it hard to interpret these results and to draw useful insights from them. Our results also demonstrate the necessity to have access, as much as possible, to the raw predictions provided by the participating teams so that challenge results can be more easily analysed and thus more useful to the research community.},
  archive      = {J_PR},
  author       = {Adrien Foucart and Olivier Debeir and Christine Decaestecker},
  doi          = {10.1016/j.patcog.2023.109600},
  journal      = {Pattern Recognition},
  pages        = {109600},
  shortjournal = {Pattern Recognition},
  title        = {Evaluating participating methods in image analysis challenges: Lessons from MoNuSAC 2020},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local multi-scale feature aggregation network for real-time
image dehazing. <em>PR</em>, <em>141</em>, 109599. (<a
href="https://doi.org/10.1016/j.patcog.2023.109599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haze causes visual degradation and obscures image information, which gravely affects the reliability of computer vision tasks in real-time systems. Leveraging an enormous number of learning parameters as the restoration costs, learning-based methods have gained significant success, but they are runtime intensive or memory inefficient. In this paper, we propose a local multi-scale feature aggregation network, called LMFA-Net, which has a lightweight model structure and can be used for real-time dehazing. By learning the local mapping relationship between the clean value of a haze image at a certain point and its surrounding local region, LMFA-Net can directly restore the final haze-free image. In particular, we adopt a novel multi-scale feature extraction sub-network (M-Net) to extract features from different scales. As a lightweight network, LMFA-Net can achieve fast and efficient dehazing. Extensive experiments demonstrate that our proposed LMFA-Net surpasses previous state-of-the-art lightweight dehazing methods in both quantitatively and qualitatively.},
  archive      = {J_PR},
  author       = {Yong Liu and Xiaorong Hou},
  doi          = {10.1016/j.patcog.2023.109599},
  journal      = {Pattern Recognition},
  pages        = {109599},
  shortjournal = {Pattern Recognition},
  title        = {Local multi-scale feature aggregation network for real-time image dehazing},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot classification with task-adaptive semantic feature
learning. <em>PR</em>, <em>141</em>, 109594. (<a
href="https://doi.org/10.1016/j.patcog.2023.109594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification aims to learn a classifier that categorizes objects of unseen classes with limited samples. One general approach is to mine as much information as possible from limited samples. This can be achieved by incorporating data from multiple modalities. However, existing multi-modality methods only use additional modality in support samples while adhering to a single modal in query samples. Such approach could lead to information imbalance between support and query samples, which confounds model generalization from support to query samples. Towards this problem, we propose a task-adaptive semantic feature learning mechanism to incorporate semantic features for both support and query samples. The semantic feature learner is trained episodic-wisely by regressing from the feature vectors of the support samples. It is utilized to predict semantic features for the query samples. Such method maintains a consistent training scheme between support and query samples and enables direct model transfer from support to query data, which significantly improves model generalization. We conduct extensive experiments on four benchmarks in both inductive and transductive settings. Results show that the proposed TasNet outperforms state-of-the-art methods with an improvement of 1\% to 5\% in classification accuracy , demonstrating the superiority of our method. The exhaustive ablation studies further validate the effectiveness of our framework. The code is available at: https://github.com/pmhDL/TasNet},
  archive      = {J_PR},
  author       = {Mei-Hong Pan and Hong-Yi Xin and Chun-Qiu Xia and Hong-Bin Shen},
  doi          = {10.1016/j.patcog.2023.109594},
  journal      = {Pattern Recognition},
  pages        = {109594},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot classification with task-adaptive semantic feature learning},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FontTransformer: Few-shot high-resolution chinese glyph
image synthesis via stacked transformers. <em>PR</em>, <em>141</em>,
109593. (<a href="https://doi.org/10.1016/j.patcog.2023.109593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic generation of high-quality Chinese fonts from a few online training samples is a challenging task, especially when the amount of samples is very small. Existing few-shot font generation methods can only synthesize low-resolution glyph images that often possess incorrect topological structures or/and incomplete strokes. To address the problem, this paper proposes FontTransformer, a novel few-shot learning model, for high-resolution Chinese glyph image synthesis by using stacked Transformers. The key idea is to apply the parallel Transformer to avoid the accumulation of prediction errors and utilize the serial Transformer to enhance the quality of synthesized strokes. Meanwhile, we also design a novel encoding scheme to feed more glyph information and prior knowledge to our model, which further enables the generation of high-resolution and visually-pleasing glyph images. Both qualitative and quantitative experimental results demonstrate the superiority of our method compared to other existing approaches in few-shot Chinese font synthesis task.},
  archive      = {J_PR},
  author       = {Yitian Liu and Zhouhui Lian},
  doi          = {10.1016/j.patcog.2023.109593},
  journal      = {Pattern Recognition},
  pages        = {109593},
  shortjournal = {Pattern Recognition},
  title        = {FontTransformer: Few-shot high-resolution chinese glyph image synthesis via stacked transformers},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Holistic transformer: A joint neural network for trajectory
prediction and decision-making of autonomous vehicles. <em>PR</em>,
<em>141</em>, 109592. (<a
href="https://doi.org/10.1016/j.patcog.2023.109592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory prediction and behavioral decision-making are two important tasks for autonomous vehicles that require a good understanding of the environmental context. Notably, behavioral decisions are better made by referring to the outputs of trajectory predictions. However, most current solutions perform these tasks separately. Therefore, this paper proposes a new joint holistic transformer network that combines multiple cues to predict trajectories and make behavioral decisions simultaneously. To better explore the intrinsic relationships among cues, the network uses existing knowledge and adopts three kinds of attention mechanisms : the sparse multi-head type for reducing noise impact, feature selection sparse type for optimally using partial prior knowledge, and multi-head with sigmoid activation type for optimally using posteriori knowledge. Compared with other trajectory prediction models, the proposed model has a better comprehensive performance and good interpretability. Perceptual noise robustness experiments demonstrate that the proposed model has good noise robustness. Thus, simultaneous trajectory prediction and behavioral decision-making combining multiple cues are accomplished, which reduces computational costs and enhances semantic relationships between scenes and agents.},
  archive      = {J_PR},
  author       = {Hongyu Hu and Qi Wang and Zhengguang Zhang and Zhengyi Li and Zhenhai Gao},
  doi          = {10.1016/j.patcog.2023.109592},
  journal      = {Pattern Recognition},
  pages        = {109592},
  shortjournal = {Pattern Recognition},
  title        = {Holistic transformer: A joint neural network for trajectory prediction and decision-making of autonomous vehicles},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning cross-domain semantic-visual relationships for
transductive zero-shot learning. <em>PR</em>, <em>141</em>, 109591. (<a
href="https://doi.org/10.1016/j.patcog.2023.109591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Learning (ZSL) learns models for recognizing new classes. One of the main challenges in ZSL is the domain discrepancy caused by the category inconsistency between training and testing data. Domain adaptation is the most intuitive way to address this challenge. However, existing domain adaptation techniques cannot be directly applied into ZSL due to the disjoint label space between source and target domains. This work proposes the Transferrable Semantic-Visual Relation (TSVR) approach towards transductive ZSL. TSVR redefines image recognition as predicting the similarity/dissimilarity labels for semantic-visual fusions consisting of class attributes and visual features. After the above transformation, the source and target domains can have the same label space, which hence enables to quantify domain discrepancy. For the redefined problem, the number of similar semantic-visual pairs is significantly smaller than that of dissimilar ones. To this end, we further propose to use Domain-Specific Batch Normalization to align the domain discrepancy.},
  archive      = {J_PR},
  author       = {Fengmao Lv and Jianyang Zhang and Guowu Yang and Lei Feng and Yufeng Yu and Lixin Duan},
  doi          = {10.1016/j.patcog.2023.109591},
  journal      = {Pattern Recognition},
  pages        = {109591},
  shortjournal = {Pattern Recognition},
  title        = {Learning cross-domain semantic-visual relationships for transductive zero-shot learning},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). GGD-GAN: Gradient-guided dual-branch adversarial networks
for relic sketch generation. <em>PR</em>, <em>141</em>, 109586. (<a
href="https://doi.org/10.1016/j.patcog.2023.109586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch reflects the main content and structure of the painted cultural relics, which help researcher understand the original drawing intention and painting skills. Although the existing automatic sketch extraction methods can improve efficiency, most of them are based on edge detection leading the limitations in incomplete details, serious disease noise and blurring. In this paper, a gradient guided dual-branch generative adversarial networks (GANs) is proposed for high-quality relic sketch generation. The sketch generation branch (SGB) and auxiliary gradient-image generation branch (GGB) were designed via two independent GANs for learning different and complement characteristics. We also designed a feature transmission module for transferring context features from SGB to GGB, and a fusion block to realize the gradient guidance from GGB to SGB, forcing SGB to pay more attention to shape, details, and noise suppression . Both branches not only learn different characteristics independently, but also affect and complement each other, which generates rich and clean high-quality sketches. In our experiments, we compared our method with eight state-of-the-art algorithms quantitatively and qualitatively, analysis the effects of the gradient guidance feature transfer, and the generalization of the proposed dual-branch GAN framework. Experiments show the proposed framework is promising in its ability to extract a clear, coherent, and complete sketch of painted cultural relics.},
  archive      = {J_PR},
  author       = {Jun Wang and Erlei Zhang and Shan Cui and Jiaxin Wang and Qunxi Zhang and Jianping Fan and Jinye Peng},
  doi          = {10.1016/j.patcog.2023.109586},
  journal      = {Pattern Recognition},
  pages        = {109586},
  shortjournal = {Pattern Recognition},
  title        = {GGD-GAN: Gradient-guided dual-branch adversarial networks for relic sketch generation},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowd counting from single images using recursive
multi-pathway zooming and foreground enhancement. <em>PR</em>,
<em>141</em>, 109585. (<a
href="https://doi.org/10.1016/j.patcog.2023.109585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is a challenging task due to many challenges such as scale variations and noisy background. To handle these challenges, we propose a novel framework named Multi-Pathway Zooming Network (MZNet) in this paper. The proposed framework recursively optimizes multi-scale features using multiple zooming pathways and progressively enhances the foreground information to improve crowd counting performance. Each zooming pathway comprises two zooming directions, zooming in and zooming out. Convolutional features at different resolutions are propagated to optimize the context information at each specific level. By sequentially integrating and interacting multi-observation information, the optimized features are powerful in handling the scale variation issue, and thus the crowd counting performance can be enhanced. To address the noisy background in many scenarios, we also introduce a new scheme to enhance the foreground information by incorporating a masked input image into the network, which is formed by a mask that element-wise multiplies with the original image. Finally, the context information, incorporated with an output density map, is recursively finetuned in our network to boost the counting performance. Extensive experiments evaluated on challenging benchmark datasets show competitive performances for both crowded and sparse scenarios.},
  archive      = {J_PR},
  author       = {Junjie Ma and Yaping Dai and Zhiyang Jia and Fuchun Sun and Yap-Peng Tan and Jun Liu},
  doi          = {10.1016/j.patcog.2023.109585},
  journal      = {Pattern Recognition},
  pages        = {109585},
  shortjournal = {Pattern Recognition},
  title        = {Crowd counting from single images using recursive multi-pathway zooming and foreground enhancement},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Corrigendum to “FETNet: Feature erasing and transferring
network for scene text removal”: Pattern recognition volume 140 (2023)
109531. <em>PR</em>, <em>141</em>, 109581. (<a
href="https://doi.org/10.1016/j.patcog.2023.109581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Guangtao Lyu and Kun Liu and Anna Zhu and Seiichi Uchida and Brian Kenji Iwana},
  doi          = {10.1016/j.patcog.2023.109581},
  journal      = {Pattern Recognition},
  pages        = {109581},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “FETNet: feature erasing and transferring network for scene text removal”: pattern recognition volume 140 (2023) 109531},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DyGAT: Dynamic stroke classification of online handwritten
documents and sketches. <em>PR</em>, <em>141</em>, 109564. (<a
href="https://doi.org/10.1016/j.patcog.2023.109564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online handwriting is widely used in human-machine interface, education, office automation, and so on. Stroke classification for online handwritten documents and sketches aims to divide strokes into several semantic categories and is a necessary step for document recognition and understanding. Previous methods are essentially static in that they have to wait for the user to finish the whole sketch before making prediction. However, in practice, the more user-friendly way is to make real-time prediction as the user is writing. In this paper, we introduce Dynamic Graph ATtention network (DyGAT) to solve the dynamic stroke classification problem. The core of our method is to formalize a document/sketch into a multi-feature graph, in which nodes represent strokes, edges represent the relationships between strokes, and multiple nodes are applied to one stroke to control the information flow. The proposed method is general and is applicable to online handwritten data of many types. We conduct experiments on popular public datasets to perform sketch semantic segmentation, document layout analysis and diagram recognition, and experimental results show competitive performance. Particularly, the proposed method achieves stroke classification accuracies which are only slightly lower than those of static classification.},
  archive      = {J_PR},
  author       = {Yu-Ting Yang and Yan-Ming Zhang and Xiao-Long Yun and Fei Yin and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2023.109564},
  journal      = {Pattern Recognition},
  pages        = {109564},
  shortjournal = {Pattern Recognition},
  title        = {DyGAT: Dynamic stroke classification of online handwritten documents and sketches},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coloring anime line art videos with transformation region
enhancement network. <em>PR</em>, <em>141</em>, 109562. (<a
href="https://doi.org/10.1016/j.patcog.2023.109562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic colorization of anime line art videos aims to produce color frames given line art frames and reference color images, which is challenging due to various motions and geometric transformations across frame sequences. Existing methods usually utilize the feature maps of reference images directly and treat all the regions in an image equally. However, this may overlook the details of the regions undergoing geometric transformations . To emphasize the regions with significant transformations between the reference and target frames, we propose a Transformation Region Enhancement Network (TRE-Net) to exploit useful reference information and enhance the colorization of key transformation regions with Region Localization Module (RLM) and Feature Enhancement Module (FEM). Specifically, we propose Multi-scale Euclidean Distance Difference (Multi-scale EDD) Maps in RLM which effectively locate geometric transformation regions by contrasting the Euclidean Distance Maps of two line arts and aggregating representations at multiple scales of the network. In addition, FEM is devised to enhance feature learning in the regions with geometric transformation and to ensure proper color alignment. FEM learns locally enhanced features through an attention-gating operation at a low computational cost. With the well-represented key geometric transformation regions, our method exploits the multi-scale reference information well for color alignment, thus produces perceptually pleasing frames. Comprehensive experimental results show that our proposed method is superior to existing methods in terms of the overall quality of colorized anime line art videos.},
  archive      = {J_PR},
  author       = {Ning Wang and Muyao Niu and Zhi Dou and Zhihui Wang and Zhiyong Wang and Zhaoyan Ming and Bin Liu and Haojie Li},
  doi          = {10.1016/j.patcog.2023.109562},
  journal      = {Pattern Recognition},
  pages        = {109562},
  shortjournal = {Pattern Recognition},
  title        = {Coloring anime line art videos with transformation region enhancement network},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Community channel-net: Efficient channel-wise interactions
via community graph topology. <em>PR</em>, <em>141</em>, 109536. (<a
href="https://doi.org/10.1016/j.patcog.2023.109536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The layer-wise structure of deep neural networks (DNNs) isolates the channel interactions in the same layer, which significantly impedes the efficient learning of DNNs. Several existing methods enable channel-wise information exchange via learning channel interdependence in a heuristic and empirical manner. Nevertheless, only informative channels are emphasized while other channels are suppressed in these approaches. This results in a low channel diversity , which impeds the generalization of DNNs. Our work aims to learn channel-wise interdependence and keep the channel diversity concurrently via designing optimal channel interaction patterns. We model the channel interaction pattern from a graph perspective, where the interactions can be regarded as information exchange on the channel graph . Based on this framework, we propose the Community Channel-Net (CC-Net), using a community-based graph topology for channel interaction. Each community contains channels with semantic commonalities, and the inter-community connections are activated among critical channels. With this structured and dynamic topology , the channels from the same community can learn channel interdependence , and those critical channels from distinct communities can gain more diverse features . CC-Net outperforms baselines on image classification tasks over various backbones with fewer computational costs.},
  archive      = {J_PR},
  author       = {Fan Feng and Qi Liu and Zhanglin Peng and Ruimao Zhang and Rosa H.M. Chan},
  doi          = {10.1016/j.patcog.2023.109536},
  journal      = {Pattern Recognition},
  pages        = {109536},
  shortjournal = {Pattern Recognition},
  title        = {Community channel-net: Efficient channel-wise interactions via community graph topology},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An incremental facility location clustering with a new
hybrid constrained pseudometric. <em>PR</em>, <em>141</em>, 109520. (<a
href="https://doi.org/10.1016/j.patcog.2023.109520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Euclidean metric, one of the classical similarity measures applied in clustering algorithms , has drawbacks when applied to spatial clustering. The resulting clusters are spherical and similarly sized, and the edges of objects are considerably smoothed. This paper proposes a novel hybrid constrained pseudometric formed by the linear combination of the Euclidean metric and a pseudometric plus penalty. The pseudometric is used in a new deterministic incremental heuristic facility location algorithm (IHFL). Our method generates larger, isotropic, and partially overlapping clusters of different sizes and spatial densities , better adapting to the surface complexity than the classical non-deterministic clustering. Cluster properties are used to derive new features for supervised/unsupervised learning. Possible applications are the classification of point clouds, their simplification, detection, filtering, and extraction of different structural patterns or sampled objects. Experiments were run on point clouds derived from laser scanning and images.},
  archive      = {J_PR},
  author       = {Tomáš Bayer and Ivana Kolingerová and Markéta Potůčková and Miroslav Čábelka and Eva Štefanová},
  doi          = {10.1016/j.patcog.2023.109520},
  journal      = {Pattern Recognition},
  pages        = {109520},
  shortjournal = {Pattern Recognition},
  title        = {An incremental facility location clustering with a new hybrid constrained pseudometric},
  volume       = {141},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-information of radicals: A new clue for zero-shot
chinese character recognition. <em>PR</em>, <em>140</em>, 109598. (<a
href="https://doi.org/10.1016/j.patcog.2023.109598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot Chinese character recognition (ZSCCR) is an important research topic in Chinese character recognition as it attempts to recognize unseen Chinese characters. As basic components and mid-level representations, radicals are significant for ZSCCR. However, previous methods treat the importance of radicals equally, ignoring the different contributions of radicals in distinguishing characters. In this paper, we propose the self-information of radicals (SIR) to measure the importance of radicals in recognizing Chinese characters. The proposed SIR can be easily adopted by two commonly used radical-based ZSCCR frameworks, i.e., sequence matching based and attribute embedding based. For sequence matching based ZSCCR, we propose a novel Chinese character uncertainty elimination (CUE) framework to alleviate the radical sequence mismatch problem. For attribute embedding based ZSCCR, we propose a novel radical information embedding (RIE) method that can highlight the importance of indispensable radicals and weaken the influence of some unnecessary radicals. We conducted comprehensive experiments on the CASIA-HWDB, ICDAR2013, CTW datasets, and AHCDB datasets to evaluate the proposed method. Experiments show that our proposed methods can achieve superior performance to the state-of-the-art methods, which demonstrate the effectiveness and the high extensibility of the proposed SIR.},
  archive      = {J_PR},
  author       = {Guo-Feng Luo and Da-Han Wang and Xia Du and Hua-Yi Yin and Xu-Yao Zhang and Shunzhi Zhu},
  doi          = {10.1016/j.patcog.2023.109598},
  journal      = {Pattern Recognition},
  pages        = {109598},
  shortjournal = {Pattern Recognition},
  title        = {Self-information of radicals: A new clue for zero-shot chinese character recognition},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Factorized multi-graph matching. <em>PR</em>, <em>140</em>,
109597. (<a href="https://doi.org/10.1016/j.patcog.2023.109597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-graph matching has become a popular yet challenging task in graph theory. There exist two major problems in multi-graph matching, i.e., the cycle-consistency problem, and the high time and space complexity problem. On one hand, the pairwise-based multi-graph matching methods are of low time and space complexity, but in order to keep the cycle-consistency of the matching results, they need additional constraints. Besides, the accuracy of the pairwise-based multi-graph matching is highly dependent on the selected optimization algorithms . On the other hand, the tensor-based multi-graph matching methods can avoid the cycle-consistency problem, while their time and space complexity is extremely high. In this paper, we found the equivalence between the pairwise-based and the tensor-based multi-graph matching methods under some specific circumstances. Based on this finding, we proposed a new multi-graph matching method, which not only avoids the cycle-consistency problem, but also reduces the complexity. In addition, we further improved the proposed method by introducing a lossless factorization of the affinity matrix in the multi-graph matching methods. Synthetic and real data experiments demonstrate the superiority of our method.},
  archive      = {J_PR},
  author       = {Liangliang Zhu and Xinwen Zhu and Xiurui Geng},
  doi          = {10.1016/j.patcog.2023.109597},
  journal      = {Pattern Recognition},
  pages        = {109597},
  shortjournal = {Pattern Recognition},
  title        = {Factorized multi-graph matching},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). GSAL: Geometric structure adversarial learning for robust
medical image segmentation. <em>PR</em>, <em>140</em>, 109596. (<a
href="https://doi.org/10.1016/j.patcog.2023.109596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic medical image segmentation plays a crucial role in clinical diagnosis and treatment. However, it is still a challenging task due to the complex interior characteristics ( e.g. , inconsistent intensity, low contrast, texture heterogeneity) and ambiguous external boundary structures. In this paper, we introduce a novel geometric structure learning mechanism (GSLM) to overcome the limitations of existing segmentation models that lack learning ”focus, path, and difficulty.” The geometric structure in this mechanism is jointly characterized by the skeleton-like structure extracted by the mask distance transform (MDT) and the boundary structure extracted by the mask distance inverse transform (MDIT). Among them, the skeleton-like and boundary pay attention to the trend of interior characteristics consistency and external structure continuity, respectively. With this idea, we design GSAL, a novel end-to-end geometric structure adversarial learning for robust medical image segmentation. GSAL has four components: a geometric structure generator, which yields the geometric structure to learn the most discriminative features that preserve interior characteristics consistency and external boundary structure continuity, skeleton-like and boundary structure discriminators, which enhance and correct the characterization of internal and external geometry to mutually promote the capture of global contextual dependencies, and a geometric structure fusion sub-network, which fuses the two complementary and refined skeleton-like and boundary structures to generate the high-quality segmentation results. The proposed approach has been successfully applied to three different challenging medical image segmentation tasks, including polyp segmentation , COVID-19 lung infection segmentation, and lung nodule segmentation. Extensive experimental results demonstrate that the proposed GSAL achieves favorably against most state-of-the-art methods under different evaluation metrics . The code is available at: https://github.com/DLWK/GSAL .},
  archive      = {J_PR},
  author       = {Kun Wang and Xiaohong Zhang and Yuting Lu and Wei Zhang and Sheng Huang and Dan Yang},
  doi          = {10.1016/j.patcog.2023.109596},
  journal      = {Pattern Recognition},
  pages        = {109596},
  shortjournal = {Pattern Recognition},
  title        = {GSAL: Geometric structure adversarial learning for robust medical image segmentation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantics-enhanced early action detection using dynamic
dilated convolution. <em>PR</em>, <em>140</em>, 109595. (<a
href="https://doi.org/10.1016/j.patcog.2023.109595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new pipeline to perform early action detection from skeleton-based untrimmed videos. Our pipeline includes two new technical components. The first is a new Dynamic Dilated Convolutional Network (DDCN), which supports dynamic temporal sampling and makes feature learning more robust against temporal scale variance in action sequences. The second is a new semantic referencing module, which uses identified objects in the scene and their co-existence relationship with actions to adjust the probabilities of inferred actions. Such semantic guidance can help distinguish many ambiguous actions, which is a core challenge in the early detection of incomplete actions. Our pipeline achieves state-of-the-art performance in early action detection in two widely used skeleton-based untrimmed video benchmarks. The source codes are available at: https://github.com/Powercoder64/DDCN_SRM .},
  archive      = {J_PR},
  author       = {Matthew Korban and Xin Li},
  doi          = {10.1016/j.patcog.2023.109595},
  journal      = {Pattern Recognition},
  pages        = {109595},
  shortjournal = {Pattern Recognition},
  title        = {Semantics-enhanced early action detection using dynamic dilated convolution},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pairwise learning for the partial label ranking problem.
<em>PR</em>, <em>140</em>, 109590. (<a
href="https://doi.org/10.1016/j.patcog.2023.109590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The partial label ranking problem is a particular preference learning scenario that focuses on learning preference models from data, such that they predict a complete ranking with ties defined over the values of the class variable for a given input instance. This work proposes to transform the rankings into preference relations among pairs of class labels and to learn a standard classifier for each of them. This classifier is then used to estimate the probability of each event from the preference relation between the two compared class labels. Finally, the probabilities obtained for each preference comparison are used to compute a preference matrix utilized to solve the corresponding rank aggregation problem and so obtain the ranking among all the class labels. The experimental evaluation shows that the proposed method is ranked ahead of competing algorithms in accuracy while obtaining similar CPU time results.},
  archive      = {J_PR},
  author       = {Juan C. Alfaro and Juan A. Aledo and José A. Gámez},
  doi          = {10.1016/j.patcog.2023.109590},
  journal      = {Pattern Recognition},
  pages        = {109590},
  shortjournal = {Pattern Recognition},
  title        = {Pairwise learning for the partial label ranking problem},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FSConv: Flexible and separable convolution for convolutional
neural networks compression. <em>PR</em>, <em>140</em>, 109589. (<a
href="https://doi.org/10.1016/j.patcog.2023.109589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of limited computation resources, convolutional neural networks (CNNs) are difficult to deploy on mobile devices. To overcome this issue, many methods have successively reduced parameters in CNNs with the idea of removing redundancy among feature maps. We observe similarities between feature maps at the same layer but not complete consistency. Intuitively, the difference between similar feature maps is an essential ingredient for the success of CNNs. Therefore, we propose a flexible and separable convolution (FSConv) in a different perspective to embrace redundancy while requiring less computation, which can implicitly cluster feature maps into different clusters without introducing similarity measurements. Our proposed model extracts intrinsic information from the representative part through ordinary convolution in each cluster and reveals tiny hidden details from the redundant part through groupwise/depthwise convolution. Experimental results demonstrate that FSConv-equipped networks always perform better than previous state-of-the-art CNNs compression algorithms. Code is available at https://github.com/Clarkxielf/FSConv-Flexible-and-Separable-Convolution-for-Convolutional-Neural-Networks-Compression .},
  archive      = {J_PR},
  author       = {Yangyang Zhu and Luofeng Xie and Zhengfeng Xie and Ming Yin and Guofu Yin},
  doi          = {10.1016/j.patcog.2023.109589},
  journal      = {Pattern Recognition},
  pages        = {109589},
  shortjournal = {Pattern Recognition},
  title        = {FSConv: Flexible and separable convolution for convolutional neural networks compression},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MoCA: Incorporating domain pretraining and cross attention
for textbook question answering. <em>PR</em>, <em>140</em>, 109588. (<a
href="https://doi.org/10.1016/j.patcog.2023.109588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textbook Question Answering (TQA) is a complex multimodal task to infer answers given large context descriptions and abundant diagrams. Compared with Visual Question Answering (VQA), TQA contains a large number of uncommon terminologies and various diagram inputs. It brings new challenges to the representation capability of language model for domain-specific spans. Also, it requires the model to take fully advantage of the complementary information of different diagram types, which pushes the multimodal fusion task to a more complex level. To tackle the above issues, we propose a novel model named MoCA, which incorporates M ulti-stage d o main pretraining and C ross-guided multimodal A ttention for the TQA task. Firstly, we introduce a multi-stage domain pretraining module to conduct unsupervised post-pretraining with a span mask strategy and supervised pre-finetune. Especially for domain post-pretraining, we propose a heuristic generation algorithm to employ the terminology corpus. Secondly, to fully consider the rich inputs of context and diagrams, we propose a cross-guided multimodal attention mechanism to update the features of text, question diagram and instructional diagram based on a progressive strategy. Further, a dual gating mechanism is adopted to improve the model ensemble of three background retrievals. The experimental results show the superiority of our model, which outperforms the state-of-the-art methods on the validation and test split respectively. Also, ablation and comparison experiments verify the effectiveness of each module proposed in our model.},
  archive      = {J_PR},
  author       = {Fangzhi Xu and Qika Lin and Jun Liu and Lingling Zhang and Tianzhe Zhao and Qi Chai and Yudai Pan and Yi Huang and Qianying Wang},
  doi          = {10.1016/j.patcog.2023.109588},
  journal      = {Pattern Recognition},
  pages        = {109588},
  shortjournal = {Pattern Recognition},
  title        = {MoCA: Incorporating domain pretraining and cross attention for textbook question answering},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task weighting based on particle filter in deep multi-task
learning with a view to uncertainty and performance. <em>PR</em>,
<em>140</em>, 109587. (<a
href="https://doi.org/10.1016/j.patcog.2023.109587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently multi-task learning (MTL) has been widely used in different applications to build more robust models by sharing knowledge across several related tasks. However, one challenge that arises is the variability in the learning pace of different tasks causing the inefficiency of naively training all tasks. Therefore, it is of great importance to consider some coefficients to balance tasks in the process of learning, but, due to the large search space and the significance of setting them properly, conventional search methods such as grid or random search are no longer effective. In this paper, we propose a learning mechanism for these coefficients based on the high efficiency of the particle filter (PF) algorithm to deal with nonlinear search problems. PF considers each state of the tasks’ coefficients as a particle and recursively converges coefficients to an optimum point. While in most previous works coefficients were evaluated to only increase performance, to address the recent concerns related to applying AI in real-world applications, we also incorporate uncertainty alongside our method to prevent learning coefficients leading to unstable outcomes. This mechanism is independent of the models main learning process and can be easily added to every learning system without changing its training algorithm . Extensive experiments on real-world data sets demonstrate the superiority of the proposed method over the state-of-the-art methods on both performance and uncertainty. We also proved the acceptable performance of the method using Cramer Rao lower bound theory.},
  archive      = {J_PR},
  author       = {Emad Aghajanzadeh and Tahereh Bahraini and Amir Hossein Mehrizi and Hadi Sadoghi Yazdi},
  doi          = {10.1016/j.patcog.2023.109587},
  journal      = {Pattern Recognition},
  pages        = {109587},
  shortjournal = {Pattern Recognition},
  title        = {Task weighting based on particle filter in deep multi-task learning with a view to uncertainty and performance},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underestimation modification for intrinsic dimension
estimation. <em>PR</em>, <em>140</em>, 109580. (<a
href="https://doi.org/10.1016/j.patcog.2023.109580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intrinsic dimension is the dimension of the low-dimensional manifold where the high-dimensional data is located. Accurately estimating the intrinsic dimension of the data set is helpful for data-dimensionality reduction and preprocessing. Due to the unknown spatial distribution of data and the limited sample size of a dataset, estimation methods which only use distance information tend to underestimate the intrinsic dimension of dataset. To reduce the estimation complexity and improve the accuracy, two estimation algorithms based on ID( κ κ ) are proposed, where κ κ is the scaling ratio of the neighborhood radius of the sample point. First, according to the selection criteria of parameter κ κ , an improved algorithm for selecting the optimal scaling ratio κ κ is proposed, which reduces the computational complexity and improves the stability of estimation. Second, using simulation datasets with the same sample size and known intrinsic dimensions, the relationship between the estimated dimension and the true intrinsic dimension is obtained, and an underestimation modification method for intrinsic dimension estimation is proposed. Results of comparative experiments on simulation and real datasets indicate that the underestimation modification algorithm has high estimation accuracy and robustness.},
  archive      = {J_PR},
  author       = {Haiquan Qiu and Youlong Yang and Hua Pan},
  doi          = {10.1016/j.patcog.2023.109580},
  journal      = {Pattern Recognition},
  pages        = {109580},
  shortjournal = {Pattern Recognition},
  title        = {Underestimation modification for intrinsic dimension estimation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RA-YOLOX: Re-parameterization align decoupled head and novel
label assignment scheme based on YOLOX. <em>PR</em>, <em>140</em>,
109579. (<a href="https://doi.org/10.1016/j.patcog.2023.109579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {YOLOX is a state-of-the-art one-stage object detection model for real-time applications that employs a decoupled head and advanced label assignment. Despite its impressive performance, YOLOX has limitations that prevent it from achieving optimal accuracy in real-time settings. To improve these limitations, we propose a new approach called re-parameterization align YOLOX (RA-YOLOX). Our approach employs a novel re-parameterization align decoupled head to align the classification and regression tasks, enhancing the learning of connection information between classification and regression. In addition, we propose a novel label assignment(LA) scheme that effectively defines positive and negative samples and precisely designs loss weight function. Our LA scheme enables the detector to focus on high-quality positive samples and filter out low-quality positive samples during training. We provide three sizes of lite models, namely RA-YOLOX-s, RA-YOLOX-tiny, and RA-YOLOX-nano, all of which outperform YOLOX models of similar size by an average precision of 2.3\%, 1.5\%, and 1.7\%, respectively, on the MS COCO-2017 validation set, demonstrating the efficacy of our approach. Our code is available at github.com/hcmyhc/RA-YOLOX .},
  archive      = {J_PR},
  author       = {Zuopeng Zhao and Chen He and Guangming Zhao and Jie Zhou and Kai Hao},
  doi          = {10.1016/j.patcog.2023.109579},
  journal      = {Pattern Recognition},
  pages        = {109579},
  shortjournal = {Pattern Recognition},
  title        = {RA-YOLOX: Re-parameterization align decoupled head and novel label assignment scheme based on YOLOX},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topic-aware video summarization using multimodal
transformer. <em>PR</em>, <em>140</em>, 109578. (<a
href="https://doi.org/10.1016/j.patcog.2023.109578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization aims to generate a short and compact summary to represent the original video. Existing methods mainly focus on how to extract a general objective synopsis that precisely summaries the video content. However, in real scenarios, a video usually contains rich content with multiple topics and people may cast diverse interests on the visual contents even for the same video. In this paper, we propose a novel topic-aware video summarization task that generates multiple video summaries with different topics. To support the study of this new task, we first build a video benchmark dataset by collecting videos from various types of movies and annotate them with topic labels and frame-level importance scores. Then we propose a multimodal Transformer model for the topic-aware video summarization, which simultaneously predicts topic labels and generates topic-related summaries by adaptively fusing multimodal features extracted from the video. Experimental results show the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Yubo Zhu and Wentian Zhao and Rui Hua and Xinxiao Wu},
  doi          = {10.1016/j.patcog.2023.109578},
  journal      = {Pattern Recognition},
  pages        = {109578},
  shortjournal = {Pattern Recognition},
  title        = {Topic-aware video summarization using multimodal transformer},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regression by re-ranking. <em>PR</em>, <em>140</em>, 109577.
(<a href="https://doi.org/10.1016/j.patcog.2023.109577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several approaches based on regression have been developed in the past few years with the goal of improving prediction results, including the use of ranking strategies. Re-ranking has been exploited and successfully employed in several applications, improving rankings by encoding the manifold structure and redefining distances among elements from a dataset. Despite the promising results observed, re-ranking has not been evaluated in regressions tasks . This paper proposes a novel, generic, and customizable framework entitled Regression by Re-ranking (RbR) , which explores the ability of re-ranking algorithms in determining relevant rankings of objects in prediction tasks. The framework relies on the integration of a base regressor , unsupervised re-ranking learning techniques, and predictions associated with nearest neighbours weighted according to their ranking positions. The RbR framework was evaluated under a rigorous experimental protocol and presented significant results in improving the prediction when compared to state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Filipe Marcel Fernandes Gonçalves and Daniel Carlos Guimarães Pedronette and Ricardo da Silva Torres},
  doi          = {10.1016/j.patcog.2023.109577},
  journal      = {Pattern Recognition},
  pages        = {109577},
  shortjournal = {Pattern Recognition},
  title        = {Regression by re-ranking},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A noising-denoising framework for point cloud upsampling via
normalizing flows. <em>PR</em>, <em>140</em>, 109569. (<a
href="https://doi.org/10.1016/j.patcog.2023.109569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud upsampling aims to generate dense and uniform point cloud from the sparse input point cloud. One challenge is how to flexibly upsample the sparse point cloud in arbitrary ratios, even without the given supervised high resolution point cloud. To address this challenge, we propose a noising-denoising framework, dubbed ND-PUFlow, for arbitrary 3D point cloud upsampling (3DPU) in supervised and self-supervised settings. It consists of two stages, i.e., dense noisy points generation and noisy points denoising via continuous normalizing flows (CNFs). In the first stage, noisy points are generated by adding noise to the input points. In the second stage, CNFs move each noisy point to the underlying surface, forming a dense and clean point cloud. Extensive experiments show that our method is competitive in both supervised and self-supervised settings, and in most cases, it achieves the best performance on benchmark datasets for 3DPU.},
  archive      = {J_PR},
  author       = {Xin Hu and Xin Wei and Jian Sun},
  doi          = {10.1016/j.patcog.2023.109569},
  journal      = {Pattern Recognition},
  pages        = {109569},
  shortjournal = {Pattern Recognition},
  title        = {A noising-denoising framework for point cloud upsampling via normalizing flows},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Distributional and spatial-temporal robust representation
learning for transportation activity recognition. <em>PR</em>,
<em>140</em>, 109568. (<a
href="https://doi.org/10.1016/j.patcog.2023.109568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation activity recognition (TAR) provides valuable support for intelligent transportation applications, such as urban transportation planning, driving behavior analysis, and traffic prediction . There are many advantages of movable sensor-based TAR, and the key challenge is to capture salient features from segmented data for representing diverse patterns of activity. Although existing methods based on statistical information are efficient, they usually rely on domain knowledge to construct high-quality features manually. Likewise, the methods based on spatial-temporal relationships achieve good performance but fail to extract statistical features. The features extracted by these two methods have proven to be crucial for the classification of activity. How to combine them to acquire a more robust representation remains an open question. In this work, we introduce a novel parallel model named Distributional and Spatial-Temporal Robust Representation (DSTRR), which combines automatic learning of statistical, spatial, and temporal features into a unified framework. This model leads to three optimized subnets and thus obtains a robust representation specific to TAR. Extensive experiments performed on three public datasets show that DSTRR is a state-of-the-art method compared with the baseline methods . The results of ablation study and visualization not only demonstrate the effectiveness of each component in DSTRR, but also show the model remains robust to a wide range of parameter variations.},
  archive      = {J_PR},
  author       = {Jing Liu and Yang Liu and Wei Zhu and Xiaoguang Zhu and Liang Song},
  doi          = {10.1016/j.patcog.2023.109568},
  journal      = {Pattern Recognition},
  pages        = {109568},
  shortjournal = {Pattern Recognition},
  title        = {Distributional and spatial-temporal robust representation learning for transportation activity recognition},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RareAnom: A benchmark video dataset for rare type anomalies.
<em>PR</em>, <em>140</em>, 109567. (<a
href="https://doi.org/10.1016/j.patcog.2023.109567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing video anomaly detection methods and datasets suffer from restricted anomaly categories containing single-source (CCTV) videos recorded in controlled environment, inadequate annotations, and lack of adequate supervision. To mitigate these problems, we introduce a new dataset ( RareAnom ) containing 17 rare types of real-world anomalies (2200 videos) recorded using multiple sources (e.g., CCTV, handheld cameras, dash-cams, and mobile phones) with rich temporal annotations. A new fully unsupervised anomaly detection and classification method has been proposed. It has three stages: training of a 3D Convolution Autoencoder using pseudo-labelled video segments, anomaly detection using latent features, and classification. Unlike the existing datasets, we have benchmarked RareAnom using three levels of supervision: fully, weakly, and unsupervised. It has been compared with UCF-Crime and XD-Violence datasets. The proposed anomaly detection and classification method beats the latest unsupervised methods by 4.49\%, 8.66\%, and 6.77\% on RareAnom, UCF-Crime, and XD-violence datasets, respectively.},
  archive      = {J_PR},
  author       = {Kamalakar Vijay Thakare and Debi Prosad Dogra and Heeseung Choi and Haksub Kim and Ig-Jae Kim},
  doi          = {10.1016/j.patcog.2023.109567},
  journal      = {Pattern Recognition},
  pages        = {109567},
  shortjournal = {Pattern Recognition},
  title        = {RareAnom: A benchmark video dataset for rare type anomalies},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel relation aware wrapper method for feature selection.
<em>PR</em>, <em>140</em>, 109566. (<a
href="https://doi.org/10.1016/j.patcog.2023.109566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection, aiming at eliminating irrelevant and redundant features, is an important data preprocessing technology for downstream tasks, e.g., classification. With the explosive growth of data in various fields, some data are high-dimensional and contain critical and complex hidden relationships, which brings new challenges to feature selection: i) How to find out the underlying available relationships from the data, and ii) how to use the learned relations to better select features? To deal with these challenges, we propose a novel wrapper feature selection method named R e lation Awa r e Fe a ture S election M e thod (ERASE), which can learn and use the underlying sample relations and feature relations for feature selection. Different from existing methods, our method jointly learns sample relationships and feature relationships through a graph of samples and trees of features. Furthermore, it uses the relations to select the optimal feature subset according to the new proposed Relation-based Sequence Floating Selection Strategy. Extensive experimental results on nine datasets from different domains demonstrate that our method achieves the best performance in most cases compared with other feature selection methods, including state-of-the-art wrapper methods.},
  archive      = {J_PR},
  author       = {Zhaogeng Liu and Jielong Yang and Li Wang and Yi Chang},
  doi          = {10.1016/j.patcog.2023.109566},
  journal      = {Pattern Recognition},
  pages        = {109566},
  shortjournal = {Pattern Recognition},
  title        = {A novel relation aware wrapper method for feature selection},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust semi-supervised multi-view graph learning with
sharable and individual structure. <em>PR</em>, <em>140</em>, 109565.
(<a href="https://doi.org/10.1016/j.patcog.2023.109565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of a high-quality multi-view consensus graph is key to graph-based semi-supervised multi-view learning (GSSMvL) methods. However, most existing GSSMvL methods explore sample relationships in the original multi-view feature space, which obtains a contaminated graph that cannot reveal the underlying manifold structure of the samples. Moreover, traditional GSSMvL methods fail to explore the diverse structures of multi-view features, which may lose their complementary information and lead to a suboptimal graph. In this paper, we propose a novel unified robust semi-supervised multi-view graph learning framework based on the sharable and individual structure (RSSMvSI), which can eliminate the influence of noise and exploit the knowledge of multi-view data in a reasonable manner. Specifically, we first learn clean data by manipulating sparse noise with l 2 , 1 l2,1 norm. We then simultaneously explore the sharable and individual self-representation subspace on the learned clean multi-view data. The key point is that noisy data does not participate in subspace learning, which improves the robustness of the proposed method. By constructing the optimal consensus graph with the learned sharable and individual subspace, RSSMvSI can better utilize the complementary information of multi-view data and approximate the manifold structure of samples. To the best of our knowledge, this is the first attempt to learn the self-representation subspace on recovered multi-view clean data. Extensive experiments on various real-world multi-view datasets demonstrate the superiority and robustness against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Wei Guo and Zhe Wang and Wenli Du},
  doi          = {10.1016/j.patcog.2023.109565},
  journal      = {Pattern Recognition},
  pages        = {109565},
  shortjournal = {Pattern Recognition},
  title        = {Robust semi-supervised multi-view graph learning with sharable and individual structure},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RMAML: Riemannian meta-learning with orthogonality
constraints. <em>PR</em>, <em>140</em>, 109563. (<a
href="https://doi.org/10.1016/j.patcog.2023.109563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-learning is the core capability that enables intelligent systems to rapidly generalize their prior experience to learn new tasks. In general, the optimization-based methods formalize the meta-learning as a bi-level optimization problem , that is a nested optimization framework, in which meta-parameters are optimized (or learned) at the outer-level, while the inner-level optimizes the task-specific parameters. In this paper, we introduce RMAML, a meta-learning method that enforces orthogonality constraints to the bi-level optimization problem . We develop a geometry aware framework that generalizes the bi-level optimization problem to the Riemannian (constrained) setting. Using the Riemannian operations such as orthogonal projection , retraction and parallel transport, the bi-level optimization is reformulated so that it respects the Riemannian geometry. Moreover, we observe that a superior stable optimization and an improved generalization ability can be achieved when the parameters and meta-parameters of the method are modeled using a Stiefel Manifold . We empirically show that RMAML can easily reach competitive performances against several state of the art algorithms for few-shot classification and consistently outperforms its Euclidean counterpart, MAML. For example, by using the geometry of the Stiefel manifold to structure the fully-connected layers in a deep neural network , a 7\% increase in single-domain few-shot classification accuracy is achieved. For the cross-domain few-shot learning, RMAML outperforms MAML by up to 9\% of accuracy. Our ablation study also demonstrates the effectiveness of RMAML over MAML in terms of higher accuracy with a reduced number of tasks and (or) inner-level updates.},
  archive      = {J_PR},
  author       = {Hadi Tabealhojeh and Peyman Adibi and Hossein Karshenas and Soumava Kumar Roy and Mehrtash Harandi},
  doi          = {10.1016/j.patcog.2023.109563},
  journal      = {Pattern Recognition},
  pages        = {109563},
  shortjournal = {Pattern Recognition},
  title        = {RMAML: Riemannian meta-learning with orthogonality constraints},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exemplar-free class incremental learning via discriminative
and comparable parallel one-class classifiers. <em>PR</em>,
<em>140</em>, 109561. (<a
href="https://doi.org/10.1016/j.patcog.2023.109561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exemplar-free class incremental learning (IL) requires classification models to learn new-class knowledge incrementally without retaining any old samples. Recently, the IL framework based on parallel one-class classifiers (POC) has demonstrated promising performance. It trains a one-class classifier (OCC) for each category and thus is immune to the catastrophic forgetting problem. However, the single-class training strategy may incur weak discriminability and low comparability between different classifiers in POC. To meet this challenge, we propose a new IL framework, referred to as D iscriminative and C omparable P arallel O ne-class C lassifiers (DCPOC). Instead of ordinary OCCs (e.g., deep SVDD) used in other POC methods, DCPOC adopts variational auto-encoders (VAE) as OCCs because VAEs can be used not only to identify classes for given samples but also to generate pseudo samples for the trained classes. With this advantage, DCPOC trains a new-class VAE in contrast with the old-class VAEs, which benefits the new-class VAE to reconstruct better for new-class samples but worse for old-class pseudo samples, thus enhancing the comparability. Furthermore, DCPOC introduces a hinge reconstruction loss to reinforce the discriminability. We evaluate our method on MNIST, CIFAR10, CIFAR100, Tiny-ImageNet, and ImageNet. The experimental results show that DCPOC achieves state-of-the-art performance on these datasets. 1},
  archive      = {J_PR},
  author       = {Wenju Sun and Qingyong Li and Jing Zhang and Danyu Wang and Wen Wang and YangLi-ao Geng},
  doi          = {10.1016/j.patcog.2023.109561},
  journal      = {Pattern Recognition},
  pages        = {109561},
  shortjournal = {Pattern Recognition},
  title        = {Exemplar-free class incremental learning via discriminative and comparable parallel one-class classifiers},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Improve temporal action proposals using hierarchical
context. <em>PR</em>, <em>140</em>, 109560. (<a
href="https://doi.org/10.1016/j.patcog.2023.109560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal (TAP) aims to generate accurate candidates of action instances in an untrimmed video. It has been proved that contexts are critically important to this task. In this paper, we propose a novel hierarchical context network (HCN) to further explore the snippet-level and proposal-level contexts, which are used to improve the representations of snippets and proposals, respectively. First, we pinpoint that different scales of snippet-level contexts are not equally important for different action instances. To this end, we incorporate a novel gating mechanism into the U-Net structure to capture the content-adaptive snippet-level contexts. Second, to exploit the proposal-level contexts, we propose a task-specific self-attention model with high efficiency. By stacking multiple attention models, we can deeply explore the proposal-level contexts in a wide range. Finally, to leverage both levels of context, we equip HCN with three branches to evaluate proposals from local to global perspectives. Our experiments on the ActivityNet-1.3 and THUMOS14 datasets show that HCN significantly outperforms previous TAP methods. Additionally, further experiments demonstrate that our method can substantially improve the state-of-the-art action detection performance when combined with existing action classifiers.},
  archive      = {J_PR},
  author       = {Qinying Liu and Zilei Wang and Shenghai Rong},
  doi          = {10.1016/j.patcog.2023.109560},
  journal      = {Pattern Recognition},
  pages        = {109560},
  shortjournal = {Pattern Recognition},
  title        = {Improve temporal action proposals using hierarchical context},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying effective trajectory predictions under the
guidance of trajectory anomaly detection model. <em>PR</em>,
<em>140</em>, 109559. (<a
href="https://doi.org/10.1016/j.patcog.2023.109559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory Prediction (TP) is an important research topic in computer vision and robotics fields. Recently, many stochastic TP models have been proposed to deal with this problem and have achieved better performance than the traditional models with deterministic trajectory outputs. However, these stochastic models can generate a number of future trajectories with different qualities. They are lack of self-evaluation ability, that is, to examine the rationality of their prediction results, thus failing to guide users to identify high-quality ones from their candidate results. This hinders them from playing their best in real applications. In this paper, we make up for this defect and propose TPAD, a novel TP evaluation method based on the trajectory Anomaly Detection (AD) technique. In TPAD, we firstly combine the Automated Machine Learning (AutoML) technique and the experience in the AD and TP field to automatically design an effective trajectory AD model. Then, we utilize the learned trajectory AD model to examine the rationality of the predicted trajectories, and screen out good TP results for users. Extensive experimental results demonstrate that TPAD can effectively identify near-optimal prediction results, improving stochastic TP models’ practical application effect.},
  archive      = {J_PR},
  author       = {Chunnan Wang and Chen Liang and Xiang Chen and Hongzhi Wang},
  doi          = {10.1016/j.patcog.2023.109559},
  journal      = {Pattern Recognition},
  pages        = {109559},
  shortjournal = {Pattern Recognition},
  title        = {Identifying effective trajectory predictions under the guidance of trajectory anomaly detection model},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CANet: Contextual information and spatial attention based
network for detecting small defects in manufacturing industry.
<em>PR</em>, <em>140</em>, 109558. (<a
href="https://doi.org/10.1016/j.patcog.2023.109558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the promising development of Automatic Visual Inspection (AVI) in the manufacturing industry, detecting small-sized defects with fewer pixels coverage remains a challenging problem due to its insufficient attention and lack of semantic information. Most exsiting convolutional inspection methods overlook the long-range dependence of context and lack adaptive fusion strategies to exploit heterogeneous features. To address these issues in AVI, this paper proposes a novel contextual information and spatial attention based network (CANet), which consists of two steps, namely CAblock and LaplacianFPN, for effective perception and exploitation of small defect features. Specifically, CAblock extracts semantic information with rich context by encoding spatial long-range dependence and decoding contextual information as channel-specific bias through a Spatial Attention Encoder (SAE) and a Context Block Decoder (CBD), respectively. LaplacianFPN further performs adaptive feature fusion considering both feature consistency and heterogeneity via two parallel branches. As a benchmark, a self-built Engine Surface Defects (ESD) dataset collected in real industry containing 89.70\% small defects is constructed. Experimental results show that CANet achieves mAP-50 improvements of 1.5\% and 4.3\% compared to state-of-the-art methods on NEU-DET and ESD, which demonstrates the effectiveness of the proposed method. The code is now available at https://github.com/xiuqhou/CANet .},
  archive      = {J_PR},
  author       = {Xiuquan Hou and Meiqin Liu and Senlin Zhang and Ping Wei and Badong Chen},
  doi          = {10.1016/j.patcog.2023.109558},
  journal      = {Pattern Recognition},
  pages        = {109558},
  shortjournal = {Pattern Recognition},
  title        = {CANet: Contextual information and spatial attention based network for detecting small defects in manufacturing industry},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time semantic segmentation model using iteratively
shared features in multiple sub-encoders. <em>PR</em>, <em>140</em>,
109557. (<a href="https://doi.org/10.1016/j.patcog.2023.109557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies show a significant growth in semantic segmentation . However, many semantic segmentation models still have a large number of parameters, making them unsuitable for resource-constrained embedded devices. To address this issue, we propose an efficient Shared Feature Reuse Segmentation (SFRSeg) model containing several novelties: a new yet effective shared-branch multiple sub-encoders design, a context mining module and a semantic aggregating module for better context granularity . In particular, our shared-branch approach improves the entire feature hierarchy by sharing the spatial and context knowledge in both shallow and deep branches. After every shared point in each sub-encoder, a proposed cascading context mining (CCM) module is deployed to filter out the noisy spatial details from the feature maps and provides a diverse size of receptive fields for capturing the latent context between multi-scale geometric shapes in the scene. To overcome the gradient vanishing issue at the early stage, we reduce the number of layers in the first sub-encoder and employ a unique multiple sub-encoders design which reprocesses the rich global feature maps through multiple sub-encoders for better feature refinement. Later, the rich semantic features generated by the efficient sub-encoders at different levels are fused by the proposed Hybrid Path Attention Semantic Aggregation (HPA-SA) module that effectively reduces the semantic gap between feature maps at different levels and alleviate the well-known boundary degeneration effect. To make it computationally efficient for resource-constrained embedded devices, a series of lightweight methods such as a lightweight encoder, a squeeze-and-excitation design, separable convolution filters , channel reduction (CR) are carefully exploited. With an exceptional performance on Cityscapes (70.6\% test mIoU) and CamVid (74.7\% test mIoU) data sets, the proposed model is shown to be superior over existing light real-time semantic segmentation models whilst having only 1.6 million parameters.},
  archive      = {J_PR},
  author       = {Tanmay Singha and Duc-Son Pham and Aneesh Krishna},
  doi          = {10.1016/j.patcog.2023.109557},
  journal      = {Pattern Recognition},
  pages        = {109557},
  shortjournal = {Pattern Recognition},
  title        = {A real-time semantic segmentation model using iteratively shared features in multiple sub-encoders},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diluted binary neural network. <em>PR</em>, <em>140</em>,
109556. (<a href="https://doi.org/10.1016/j.patcog.2023.109556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary neural networks (BNNs) are promising on resource-constrained devices because they reduce memory consumption and accelerate inference effectively. However, they are still potential on performance improvement. Prior studies attribute performance degradation of BNNs to limited representation ability and gradient mismatch. In this paper, we find that it also results from the mandatory representation of small full-precision auxiliary weights to large values. To tackle with this issue, we propose an approach dubbed as Diluted Binary Neural Network (DBNN). Besides avoiding mandatory representation effectively, the proposed DBNN also alleviates sign flip problem to a large extent. For activations, we jointly minimize quantization error and maximize information entropy to develop the binarization scheme. Compared with existing sparsity-binarization approaches, DBNN trains network from scratch without other procedures and achieves larger sparsity . Experiments on several datasets with various networks demonstrate the superiority of our approach.},
  archive      = {J_PR},
  author       = {Yuhan Lin and Lingfeng Niu and Yang Xiao and Ruizhi Zhou},
  doi          = {10.1016/j.patcog.2023.109556},
  journal      = {Pattern Recognition},
  pages        = {109556},
  shortjournal = {Pattern Recognition},
  title        = {Diluted binary neural network},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-level feature aggregation network for polyp
segmentation. <em>PR</em>, <em>140</em>, 109555. (<a
href="https://doi.org/10.1016/j.patcog.2023.109555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of polyps from colonoscopy images plays a critical role in the diagnosis and cure of colorectal cancer. Although effectiveness has been achieved in the field of polyp segmentation , there are still several challenges. Polyps often have a diversity of size and shape and have no sharp boundary between polyps and their surrounding. To address these challenges, we propose a novel Cross-level Feature Aggregation Network (CFA-Net) for polyp segmentation. Specifically, we first propose a boundary prediction network to generate boundary-aware features, which are incorporated into the segmentation network using a layer-wise strategy. In particular, we design a two-stream structure based segmentation network , to exploit hierarchical semantic information from cross-level features. Furthermore, a Cross-level Feature Fusion (CFF) module is proposed to integrate the adjacent features from different levels, which can characterize the cross-level and multi-scale information to handle scale variations of polyps. Further, a Boundary Aggregated Module (BAM) is proposed to incorporate boundary information into the segmentation network, which enhances these hierarchical features to generate finer segmentation maps . Quantitative and qualitative experiments on five public datasets demonstrate the effectiveness of our CFA-Net against other state-of-the-art polyp segmentation methods. The source code and segmentation maps will be released at https://github.com/taozh2017/CFANet .},
  archive      = {J_PR},
  author       = {Tao Zhou and Yi Zhou and Kelei He and Chen Gong and Jian Yang and Huazhu Fu and Dinggang Shen},
  doi          = {10.1016/j.patcog.2023.109555},
  journal      = {Pattern Recognition},
  pages        = {109555},
  shortjournal = {Pattern Recognition},
  title        = {Cross-level feature aggregation network for polyp segmentation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rigorous non-disjoint discretization for naive bayes.
<em>PR</em>, <em>140</em>, 109554. (<a
href="https://doi.org/10.1016/j.patcog.2023.109554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Naive Bayes is a classical machine learning algorithm for which discretization is commonly used to transform quantitative attributes into qualitative attributes. Of numerous discretization methods, Non-Disjoint Discretization (NDD) proposes a novel perspective by forming overlapping intervals and always locating a value toward the middle of an interval. However, existing approaches to NDD fail to adequately consider the effect of multiple occurrences of a single value — a commonly occurring circumstance in practice. By necessity, all occurrences of a single value fall within the same interval. As a result, it is often not possible to discretize an attribute into intervals containing equal numbers of training instances. Current methods address this issue in an ad hoc manner, reducing the specificity of the resulting atomic intervals. In this study, we propose a non-disjoint discretization method for NB, called Rigorous Non-Disjoint Discretization (RNDD), that handles multiple occurrences of a single value in a systematic manner. Our extensive experimental results suggest that RNDD significantly outperforms NDD along with all other existing state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Huan Zhang and Liangxiao Jiang and Geoffrey I. Webb},
  doi          = {10.1016/j.patcog.2023.109554},
  journal      = {Pattern Recognition},
  pages        = {109554},
  shortjournal = {Pattern Recognition},
  title        = {Rigorous non-disjoint discretization for naive bayes},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Keep an eye on faces: Robust face detection with
heatmap-assisted spatial attention and scale-aware layer attention.
<em>PR</em>, <em>140</em>, 109553. (<a
href="https://doi.org/10.1016/j.patcog.2023.109553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern anchor-based face detectors learn discriminative features using large-capacity networks and extensive anchor settings. In spite of their promising results, they are not without problems. First, most anchors extract redundant features from the background. As a consequence, the performance improvements are achieved at the expense of a disproportionate computational complexity . Second, the predicted face boxes are only distinguished by a classifier supervised by pre-defined positive, negative and ignored anchors. This strategy may ignore potential contributions from cohorts of anchors labeled negative/ignored during inference simply because of their inferior initialisation, although they can regress well to a target. In other words, true positives and representative features may get filtered out by unreliable confidence scores. To deal with the first concern and achieve more efficient face detection, we propose a Heatmap-assisted Spatial Attention (HSA) module and a Scale-aware Layer Attention (SLA) module to extract informative features using lower computational costs. To be specific, SLA incorporates the information from all the feature pyramid layers, weighted adaptively to remove redundant layers. HSA predicts a reshaped Gaussian heatmap and employs it to facilitate a spatial feature selection by better highlighting facial areas. For more reliable decision-making, we merge the predicted heatmap scores and classification results by voting. Since our heatmap scores are based on the distance to the face centres, they are able to retain all the well-regressed anchors. The experiments obtained on several well-known benchmarks demonstrate the merits of the proposed method.},
  archive      = {J_PR},
  author       = {Lei Ju and Josef Kittler and Muhammad Awais Rana and Wankou Yang and Zhenhua Feng},
  doi          = {10.1016/j.patcog.2023.109553},
  journal      = {Pattern Recognition},
  pages        = {109553},
  shortjournal = {Pattern Recognition},
  title        = {Keep an eye on faces: Robust face detection with heatmap-assisted spatial attention and scale-aware layer attention},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HeadPose-softmax: Head pose adaptive curriculum learning
loss for deep face recognition. <em>PR</em>, <em>140</em>, 109552. (<a
href="https://doi.org/10.1016/j.patcog.2023.109552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has been one of the most popular applications in the field of target detection. Currently, frontal faces can be easily detected, but multi-view face detection remains a difficult task because of various factors such as illumination, various poses, occlusions, and facial expressions. Margin-based loss functions are used to increase the feature margins between different classes, thus enhancing the discriminability of face recognition models, but the performance in face detection in complex scenes (e.g., high pitch angle face detection in surveillance environments) can be significantly degraded. Recently, the idea of a mining-based strategy to emphasize hard samples has been used to achieve good results in multi-view face detection. However, most of the existing methods do not explicitly emphasize samples based on their importance, resulting in the underutilization of hard samples. In this paper, we propose a curriculum learning loss function (HeadPose-Softmax) to classify the difficulty of a sample based on its facial pose, and embed the concept of curriculum learning into the loss function to implement a novel training strategy for deep face recognition. The loss function explicitly emphasizes the importance of the samples according to the different difficulty of each sample, which allows the model to make fuller use of hard samples, focus on learning pose invariant features, and improve the accuracy of the model in multi-view face detection tasks. Specifically, our HeadPose-Softmax dynamically adjusts the relative importance of the hard samples according to the pose angle of the face in the hard samples during the training phase. At each stage, different samples are assigned different importance according to their corresponding difficulty. Extensive experimental results under popular benchmarks show that our HeadPose-Softmax can enhance the accuracy of the model in multi-view face detection and outperform the state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Jifan Yang and Zhongyuan Wang and Baojin Huang and Jinsheng Xiao and Chao Liang and Zhen Han and Hua Zou},
  doi          = {10.1016/j.patcog.2023.109552},
  journal      = {Pattern Recognition},
  pages        = {109552},
  shortjournal = {Pattern Recognition},
  title        = {HeadPose-softmax: Head pose adaptive curriculum learning loss for deep face recognition},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense light field reconstruction based on epipolar focus
spectrum. <em>PR</em>, <em>140</em>, 109551. (<a
href="https://doi.org/10.1016/j.patcog.2023.109551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing light field (LF) representations, such as epipolar plane image (EPI) and sub-aperture images, do not consider the structural characteristics across the views, so they usually require additional disparity and spatial structure cues for follow-up tasks. Besides, they have difficulties dealing with occlusions or large disparity scenes. To this end, this paper proposes a novel Epipolar Focus Spectrum (EFS) representation by rearranging the EPI spectrum. Different from the classical EPI representation where an EPI line corresponds to a specific depth, there is a one-to-one mapping from the EFS line to the view. By exploring the EFS sampling task, the analytical function is derived for constructing a non-aliasing EFS. To demonstrate its effectiveness, we develop a trainable EFS-based pipeline for light field reconstruction, where a dense light field can be reconstructed by compensating the missing EFS lines given a sparse light field, yielding promising results with cross-view consistency, especially in the presence of severe occlusion and large disparity. Experimental results on both synthetic and real-world datasets demonstrate the validity and superiority of the proposed method over SOTA methods.},
  archive      = {J_PR},
  author       = {Yaning Li and Xue Wang and Hao Zhu and Guoqing Zhou and Qing Wang},
  doi          = {10.1016/j.patcog.2023.109551},
  journal      = {Pattern Recognition},
  pages        = {109551},
  shortjournal = {Pattern Recognition},
  title        = {Dense light field reconstruction based on epipolar focus spectrum},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learn from each other to classify better: Cross-layer mutual
attention learning for fine-grained visual classification. <em>PR</em>,
<em>140</em>, 109550. (<a
href="https://doi.org/10.1016/j.patcog.2023.109550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) is valuable yet challenging. The difficulty of FGVC mainly lies in its intrinsic inter-class similarity, intra-class variation, and limited training data. Moreover, with the popularity of deep convolutional neural networks , researchers have mainly used deep, abstract, semantic information for FGVC, while shallow, detailed information has been neglected. This work proposes a cross-layer mutual attention learning network (CMAL-Net) to solve the above problems. Specifically, this work views the shallow to deep layers of CNNs as “experts” knowledgeable about different perspectives. We let each expert give a category prediction and an attention region indicating the found clues. Attention regions are treated as information carriers among experts, bringing three benefits: ( i ) helping the model focus on discriminative regions; ( ii ) providing more training data; ( iii ) allowing experts to learn from each other to improve the overall performance. CMAL-Net achieves state-of-the-art performance on three competitive datasets: FGVC-Aircraft, Stanford Cars, and Food-11. The source code is available at https://github.com/Dichao-Liu/CMAL},
  archive      = {J_PR},
  author       = {Dichao Liu and Longjiao Zhao and Yu Wang and Jien Kato},
  doi          = {10.1016/j.patcog.2023.109550},
  journal      = {Pattern Recognition},
  pages        = {109550},
  shortjournal = {Pattern Recognition},
  title        = {Learn from each other to classify better: Cross-layer mutual attention learning for fine-grained visual classification},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A black-box reversible adversarial example for authorizable
recognition to shared images. <em>PR</em>, <em>140</em>, 109549. (<a
href="https://doi.org/10.1016/j.patcog.2023.109549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared images on the Internet are easily collected, classified, and analyzed by unauthorized commercial companies through Deep Neural Networks (DNNs). The illegal use of these data damages the rights and interests of authorized companies and individuals. How to ensure that network-shared data is legally used by authorized users and not used by unauthorized DNNs has become an urgent problem. Reversible Adversarial Example (RAE) provides an effective solution, which can mislead the classification of unauthorized DNNs and does not affect the authorized users. The existing RAE schemes assumed that we could know the parameters of the target model and thus generate reversible adversarial examples. However, model parameters are often protected to avoid leakage, increasing the difficulty of generating accurate RAEs. In this paper, we first propose a Black-box Reversible Adversarial Example (B-RAE) scheme to generate robust reversible adversarial examples. We aim to protect image privacy while maintaining data usability in real scenarios. Experimental results and analysis have demonstrated that the proposed B-RAE is more effective and robust compared with the existing schemes.},
  archive      = {J_PR},
  author       = {Lizhi Xiong and Yue Wu and Peipeng Yu and Yuhui Zheng},
  doi          = {10.1016/j.patcog.2023.109549},
  journal      = {Pattern Recognition},
  pages        = {109549},
  shortjournal = {Pattern Recognition},
  title        = {A black-box reversible adversarial example for authorizable recognition to shared images},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-attention enhanced representation learning for image-text
matching. <em>PR</em>, <em>140</em>, 109548. (<a
href="https://doi.org/10.1016/j.patcog.2023.109548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-text matching has become a research hotspot in recent years. The key point of image-text matching is to accurately measure the similarity between an image and a sentence. However, most existing methods either focus on the inter-modality similarities between regions in image and words in text or the intra-modality similarities within image regions or words, such that they cannot well exploit detailed correlations between images and texts. Furthermore, existing methods typically train their models using a triplet ranking loss, which relies on the similarity of randomly sampled triples. Since the weights of positive and negative samples are not adjusted, it cannot provide enough gradient information for training, resulting in slow convergence and limited performance. To address the above problems, we propose an image-text matching method named Bi-Attention Enhanced Representation Learning (BAERL). It builds a self-attention learning sub-network to exploit intra-modality correlations within image regions or words and a co-attention learning sub-network to exploit inter-modality correlations between image regions and words. Then, representations obtained by two sub-networks capture holistic correlations between images and texts. In addition, BAERL uses the self-similarity polynomial loss instead of triplet ranking loss to train the model. The self-similarity polynomial loss can adaptively assign appropriate weights to different pairs based on their similarity scores so as to further improve the retrieval performance . Experiments on two benchmark datasets demonstrate the superior performance of the proposed BAERL method over several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yumin Tian and Aqiang Ding and Di Wang and Xuemei Luo and Bo Wan and Yifeng Wang},
  doi          = {10.1016/j.patcog.2023.109548},
  journal      = {Pattern Recognition},
  pages        = {109548},
  shortjournal = {Pattern Recognition},
  title        = {Bi-attention enhanced representation learning for image-text matching},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AA-trans: Core attention aggregating transformer with
information entropy selector for fine-grained visual classification.
<em>PR</em>, <em>140</em>, 109547. (<a
href="https://doi.org/10.1016/j.patcog.2023.109547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of fine-grained visual classification (FGVC) is to distinguish targets from subordinate classifications. Since fine-grained images have the inherent characteristic of large inter-class variances and small intra-class variances, it is considered an extremely difficult task. Most existing approaches adopt CNN-based networks as feature extractors, which causes the extracted discriminative regions to contain most parts of the object in this way, thus failing to locate the really important parts. Recently, the vision transformer (ViT) has demonstrated its power on a wide range of image tasks, which uses an attention mechanism to capture global contextual information to establish a remote dependency on the target and thus extract more powerful features. Nevertheless, the ViT model still focuses more on global coarse-grained information rather than local fine-grained information, which may lead to its undesirable performance in fine-grained image classification . To this end, we redesigned an attention aggregating transformer (AA-Trans) to better capture minor differences among images by improving the ViT structure in this paper. In detail, we propose a core attention aggregator (CAA), which enables better information sharing between each transformer layer. Besides, we further propose an innovative information entropy selector (IES) to guide the network in acquiring discriminative parts of the image precisely. Extensive experiments show that our proposed model structure can achieve a new state-of-the-art performance on several mainstream datasets.},
  archive      = {J_PR},
  author       = {Qi Wang and JianJun Wang and Hongyu Deng and Xue Wu and Yazhou Wang and Gefei Hao},
  doi          = {10.1016/j.patcog.2023.109547},
  journal      = {Pattern Recognition},
  pages        = {109547},
  shortjournal = {Pattern Recognition},
  title        = {AA-trans: Core attention aggregating transformer with information entropy selector for fine-grained visual classification},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised generalizable multi-source person
re-identification: A domain-specific adaptive framework. <em>PR</em>,
<em>140</em>, 109546. (<a
href="https://doi.org/10.1016/j.patcog.2023.109546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) has attracted much attention in person re-identification (ReID) recently. It aims to make a model trained on multiple source domains generalize to an unseen target domain. Although achieving promising progress, existing methods usually need the source domains to be labeled, which could be a significant burden for practical ReID tasks. In this paper, we turn to investigate “unsupervised” domain generalization for ReID, by assuming that no label is available for any source domains. To address this challenging setting, we propose a simple and efficient domain-specific adaptive framework, and realize it with an adaptive normalization module designed upon the batch and instance normalization techniques. In doing so, we successfully yield reliable pseudo-labels to implement training and also enhance the domain generalization capability of the model as required. In addition, we show that our framework can even be applied to improve person ReID under the settings of supervised domain generalization and unsupervised domain adaptation , demonstrating competitive performance with respect to relevant methods. Extensive experimental study on benchmark datasets is conducted to validate the proposed framework. A significance of our work lies in that it shows the potential of unsupervised domain generalization for person ReID and sets a strong baseline for the further research on this topic. The code is available at https://github.com/Qi5Lei/DSAF .},
  archive      = {J_PR},
  author       = {Lei Qi and Jiaqi Liu and Lei Wang and Yinghuan Shi and Xin Geng},
  doi          = {10.1016/j.patcog.2023.109546},
  journal      = {Pattern Recognition},
  pages        = {109546},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised generalizable multi-source person re-identification: A domain-specific adaptive framework},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-tubal-rank tensor recovery with multilayer subspace
prior learning. <em>PR</em>, <em>140</em>, 109545. (<a
href="https://doi.org/10.1016/j.patcog.2023.109545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, low-rank tensor recovery employing the subspace prior information is an emerging topic, which has attracted considerable attention. However, existing studies cannot flexibly and fully utilize the accessible subspace prior information, thereby leading to suboptimal restored performance. Aiming at addressing this issue, based on the tensor singular value decomposition (t-SVD), this article presents a novel strategy that integrates more than two layers of subspace knowledge about columns and rows of target tensor into one unified recovery framework. Specially, we first design a multilayer subspace prior learning scheme, and then apply it to two common low-rank tensor recovery problems, i.e., tensor completion and tensor robust component principal analysis. Crucially, we prove that our approach can achieve exact recovery of tensors under a significantly weaker incoherence assumption than the analogous conditions previously proposed. Furthermore, two efficient algorithms with convergence guarantees based on alternating direction method of multipliers (ADMM) are proposed to solve the corresponding models. The experimental results on synthetic and real tensor data show that the proposed algorithms outperform other state-of-the-art algorithms in terms of both qualitative and quantitative metrics.},
  archive      = {J_PR},
  author       = {Weichao Kong and Feng Zhang and Wenjin Qin and Jianjun Wang},
  doi          = {10.1016/j.patcog.2023.109545},
  journal      = {Pattern Recognition},
  pages        = {109545},
  shortjournal = {Pattern Recognition},
  title        = {Low-tubal-rank tensor recovery with multilayer subspace prior learning},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motif entropy graph kernel. <em>PR</em>, <em>140</em>,
109544. (<a href="https://doi.org/10.1016/j.patcog.2023.109544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph kernels have achieved excellent performance in graph classification tasks . In this paper, we propose a novel deep motif entropy graph kernel for the purpose of graph classification. For better capturing the differences between substructures, we gauge detailed information through a family of K K -layer expansion motifs rooted at each node and combine the Weisfeiler-Lehman algorithm to subdivide motifs, which is further enhanced by motif entropy. Experiments on eight graph-structured datasets demonstrate that our method is able to outperform the state-of-the-art kernel methods for the tasks of graph classification.},
  archive      = {J_PR},
  author       = {Liang Zhang and Longqiang Yi and Yu Liu and Cheng Wang and Da Zhou},
  doi          = {10.1016/j.patcog.2023.109544},
  journal      = {Pattern Recognition},
  pages        = {109544},
  shortjournal = {Pattern Recognition},
  title        = {Motif entropy graph kernel},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning relation-based features for fine-grained image
retrieval. <em>PR</em>, <em>140</em>, 109543. (<a
href="https://doi.org/10.1016/j.patcog.2023.109543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-Grained Image Retrieval (FGIR) is a fundamental yet challenging task that has recently received considerable attention. However, two critical issues remain unresolved. On the one hand, convolutional neural networks (CNNs) trained with image-level labels tend to focus on the most discriminative image patches but overlook the implicit relation among them. On the other hand, existing large models developed for FGIR are computationally expensive and difficult to learn discriminative features . To address these issues without additional object-level annotations or localization sub-networks, we propose a novel unified framework for fine-grained image retrieval. Specifically, a novel Relation-based Convolutional Descriptor Aggregation (RCDA) method for extracting subtle yet discriminative features from fine-grained images is introduced. The RCDA method consists of a local feature generation network and a relation extraction (RE) module that models both explicit information and implicit relations. The explicit information is modeled by computing feature similarities, while the implicit relation is mined via an expectation-maximization algorithm. Moreover, we further leverage the knowledge distillation technique to optimize the parameters of the feature generation network and speed up the fine-tuning procedure by transferring knowledge from a large model to a smaller model. Experimental results on three benchmark datasets (CUB-200-2011, Stanford-Car and FGVC-Aircraft) demonstrate that the proposed method not only achieves a significant improvement over baseline models but also outperforms state-of-the-art methods by a large margin (6.4\%, 1.3\%, 23.2\%, respectively).},
  archive      = {J_PR},
  author       = {Yingying Zhu and Gang Cao and Zhanyuan Yang and Xiufan Lu},
  doi          = {10.1016/j.patcog.2023.109543},
  journal      = {Pattern Recognition},
  pages        = {109543},
  shortjournal = {Pattern Recognition},
  title        = {Learning relation-based features for fine-grained image retrieval},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraphRevisedIE: Multimodal information extraction with
graph-revised network. <em>PR</em>, <em>140</em>, 109542. (<a
href="https://doi.org/10.1016/j.patcog.2023.109542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Key information extraction (KIE) from visually rich documents (VRD) has been a challenging task in document intelligence because of not only the complicated and diverse layouts of VRD that make the model hard to generalize but also the lack of methods to exploit the multimodal features in VRD. In this paper, we propose a light-weight model named GraphRevisedIE that effectively embeds multimodal features such as textual, visual, and layout features from VRD and leverages graph revision and graph convolution to enrich the multimodal embedding with global context. Extensive experiments on multiple real-world datasets show that GraphRevisedIE generalizes to documents of varied layouts and achieves comparable or better performance compared to previous KIE methods. We also publish a business license dataset that contains both real-life and synthesized documents to facilitate research of document KIE.},
  archive      = {J_PR},
  author       = {Panfeng Cao and Jian Wu},
  doi          = {10.1016/j.patcog.2023.109542},
  journal      = {Pattern Recognition},
  pages        = {109542},
  shortjournal = {Pattern Recognition},
  title        = {GraphRevisedIE: Multimodal information extraction with graph-revised network},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ambiguity-aware robust teacher (ART): Enhanced
self-knowledge distillation framework with pruned teacher network.
<em>PR</em>, <em>140</em>, 109541. (<a
href="https://doi.org/10.1016/j.patcog.2023.109541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-knowledge distillation (self-KD) methods, which use a student model itself as the teacher model instead of a large and complex teacher model, are currently a subject of active study. Since most previous self-KD approaches relied on the knowledge of a single teacher model, if the teacher model incorrectly predicted confusing samples, poor-quality knowledge was transferred to the student model. Unfortunately, natural images are often ambiguous for teacher models due to multiple objects, mislabeling, or low quality. In this paper, we propose a novel knowledge distillation framework named ambiguity-aware robust teacher knowledge distillation (ART-KD) that provides refined knowledge, that reflects the ambiguity of the samples with network pruning. Since the pruned teacher model is simply obtained by copying and pruning the teacher model, re-training process is unnecessary in ART-KD. The key insight of ART-KD lies in the predictions of a teacher model and pruned teacher model for ambiguous samples providing different distributions with low similarity. From these two distributions, we obtain a joint distribution considering the ambiguity of the samples as teacher’s knowledge for distillation. We comprehensively evaluate our method on public classification benchmarks, as well as more challenging benchmarks for fine-grained visual recognition (FGVR), achieving much superior performance to state-of-the-art counterparts.},
  archive      = {J_PR},
  author       = {Yucheol Cho and Gyeongdo Ham and Jae-Hyeok Lee and Daeshik Kim},
  doi          = {10.1016/j.patcog.2023.109541},
  journal      = {Pattern Recognition},
  pages        = {109541},
  shortjournal = {Pattern Recognition},
  title        = {Ambiguity-aware robust teacher (ART): Enhanced self-knowledge distillation framework with pruned teacher network},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global spatio-temporal synergistic topology learning for
skeleton-based action recognition. <em>PR</em>, <em>140</em>, 109540.
(<a href="https://doi.org/10.1016/j.patcog.2023.109540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to RGB video-based action recognition, skeleton-based action recognition algorithm has attracted much more attention due to being more lightweight, better generalization and robustness. The extraction of temporal and spatial features is a crucial factor for skeleton-based action recognition. However, existing feature extraction methods suffer from two limitations: (1) the isolated extraction of temporal and spatial feature cannot capture temporal feature connections among non-adjacent joints and (2) convolution-limited perceptual fields cannot capture global temporal features of joints effectively. In this work, we propose a global spatio-temporal synergistic feature learning module (GSTL), which generates global spatio-temporal synergistic topology of joints by spatio-temporal feature fusion . By further combining the GSTL with a temporal modeling unit, we develop a powerful global spatio-temporal synergistic topology learning network (GSTLN), and it achieves competitive performance with fewer parameters on three challenge datasets: NTU RGB + D, NTU RGB + D 120, and NW-UCLA.},
  archive      = {J_PR},
  author       = {Meng Dai and Zhonghua Sun and Tianyi Wang and Jinchao Feng and Kebin Jia},
  doi          = {10.1016/j.patcog.2023.109540},
  journal      = {Pattern Recognition},
  pages        = {109540},
  shortjournal = {Pattern Recognition},
  title        = {Global spatio-temporal synergistic topology learning for skeleton-based action recognition},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical disentangling network for object representation
learning. <em>PR</em>, <em>140</em>, 109539. (<a
href="https://doi.org/10.1016/j.patcog.2023.109539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An object can be described as the combination of primary visual attributes. Disentangling such underlying primitives is the long-term objective of representation learning . It is observed that categories have natural hierarchical characteristics, i.e., any two objects can share some common primitives at a particular category level while possess unique traits at another. However, previous works usually operate in a flat manner (i.e., at a particular level) to disentangle the representations of objects. Even though they may obtain the primitives to constitute objects as the categories at that level, their results are obviously not efficient and complete. In this paper, we propose a Hierarchical Disentangling Network (HDN) to exploit the rich hierarchical characteristics among categories to divide the disentangling process in a coarse-to-fine manner (i.e., level-wise), such that each level only focuses on learning the specific representations and finally the common and unique representations at all levels jointly constitute the raw object. Specifically, HDN is designed based on an encoder-decoder architecture. To simultaneously ensure the level-wise disentanglement and interpretability of the encoded representations, a novel hierarchical Generative Adversarial Network (GAN) is introduced. Quantitative and qualitative evaluations on popular object datasets validate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen},
  doi          = {10.1016/j.patcog.2023.109539},
  journal      = {Pattern Recognition},
  pages        = {109539},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical disentangling network for object representation learning},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint optimization for attention-based generation and
recognition of chinese characters using tree position embedding.
<em>PR</em>, <em>140</em>, 109538. (<a
href="https://doi.org/10.1016/j.patcog.2023.109538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the growing interest in Chinese character generation, creating a nonexistent character remains an open challenge. Radical-based Chinese character generation is still a novel task while radical-based Chinese character recognition is more technologically advanced. To fully utilize the knowledge of recognition task, we first propose an attention-based generator. The generator chooses the most relevant radical to generate each zone with an attention mechanism . Then, we present a joint optimization approach to training generation-recognition models, which can help the generator and recognizer learn from each other effectively. The joint optimization is implemented via contrastive learning and dual learning. Considering the symmetry of the generation and recognition, contrastive learning aims to strengthen the performance of the encoder of recognizer and the decoder of generator. Since the generation and recognition tasks can form a closed loop, dual learning feeds the output from one to another as input. Based on the feedback signals generated during the two tasks, we can iteratively update the two models until convergence. Finally, as our model ignores the order information of a sequence, we exploit position embedding to extend the image representation ability and propose tree position embedding to represent the positional information for tree structure captions of Chinese characters. The experimental results in printed and nature scenes show that the proposed method improves the quality of the generating images and increases the recognition accuracy for Chinese characters.},
  archive      = {J_PR},
  author       = {Mobai Xue and Jun Du and Bin Wang and Bo Ren and Yu Hu},
  doi          = {10.1016/j.patcog.2023.109538},
  journal      = {Pattern Recognition},
  pages        = {109538},
  shortjournal = {Pattern Recognition},
  title        = {Joint optimization for attention-based generation and recognition of chinese characters using tree position embedding},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Line graph contrastive learning for link prediction.
<em>PR</em>, <em>140</em>, 109537. (<a
href="https://doi.org/10.1016/j.patcog.2023.109537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction tasks focus on predicting possible future connections. Most existing researches measure the likelihood of links by different similarity scores on node pairs and predict links between nodes. However, the similarity-based approaches have some challenges in information loss on nodes and generalization ability on similarity indexes. To address the above issues, we propose a Line Graph Contrastive Learning (LGCL) method to obtain rich information with multiple perspectives. LGCL obtains a subgraph view by h h -hop subgraph sampling with target node pairs. After transforming the sampled subgraph into a line graph, the link prediction task is converted into a node classification task , which graph convolution progress can learn edge embeddings from graphs more effectively. Then we design a novel cross-scale contrastive learning framework on the line graph and the subgraph to maximize the mutual information of them, so that fuses the structure and feature information. The experimental results demonstrate that the proposed LGCL outperforms the state-of-the-art methods and has better performance on generalization and robustness.},
  archive      = {J_PR},
  author       = {Zehua Zhang and Shilin Sun and Guixiang Ma and Caiming Zhong},
  doi          = {10.1016/j.patcog.2023.109537},
  journal      = {Pattern Recognition},
  pages        = {109537},
  shortjournal = {Pattern Recognition},
  title        = {Line graph contrastive learning for link prediction},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning on stratified data: Joint estimation
transferred from strata. <em>PR</em>, <em>140</em>, 109535. (<a
href="https://doi.org/10.1016/j.patcog.2023.109535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the target model with the help of auxiliary models from different but possibly related groups. Inspired by transfer learning, we propose a method called joint estimation transferred from strata (JETS). To obtain a sparse solution , JETS constructs a penalized framework combining a term that penalizes the target model and an additional term that penalizes the differences between auxiliary models and the target model. In this way, JETS overcomes the challenge caused by the limited samples in high-dimensional study, and obtains stable and accurate estimates regardless of whether auxiliary samples contain noisy information. We demonstrate that this method enjoys the computational advantage of the traditional methods such as the lasso. During simulations and applications, the proposed method is compared with several existing methods and JETS outperforms others.},
  archive      = {J_PR},
  author       = {Yimiao Gao and Yuehan Yang},
  doi          = {10.1016/j.patcog.2023.109535},
  journal      = {Pattern Recognition},
  pages        = {109535},
  shortjournal = {Pattern Recognition},
  title        = {Transfer learning on stratified data: Joint estimation transferred from strata},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Towards better long-tailed oracle character recognition
with adversarial data augmentation. <em>PR</em>, <em>140</em>, 109534.
(<a href="https://doi.org/10.1016/j.patcog.2023.109534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deciphering oracle bone script is of great significance to the study of ancient Chinese culture as well as archaeology. Although recent studies on oracle character recognition have made substantial progress, they still suffer from the long-tailed data situation that results in a noticeable performance drop on the tail classes. To mitigate this issue, we propose a generative adversarial framework to augment oracle characters in the problematic classes. In this framework, the generator produces synthetic data through convex combinations of all the available samples in the corresponding classes, and is further optimized through adversarial learning with the classifier and simultaneously the discriminator . Meanwhile, we introduce Repatch to generalize samples in the generator. Since tail classes do not have sufficient data for convex combinations , we propose the TailMix mechanism to generate suitable tail class samples from other classes. Experimental results show that our proposed algorithm obtains remarkable performance in oracle character recognition and achieves new state-of-the-art average (total) accuracy with 86.03\% (89.46\%), 86.54\% (93.86\%), 95.22\% (96.17\%) on the three datasets Oracle-AYNU, OBC306 and Oracle-20K, respectively.},
  archive      = {J_PR},
  author       = {Jing Li and Qiu-Feng Wang and Kaizhu Huang and Xi Yang and Rui Zhang and John Y. Goulermas},
  doi          = {10.1016/j.patcog.2023.109534},
  journal      = {Pattern Recognition},
  pages        = {109534},
  shortjournal = {Pattern Recognition},
  title        = {Towards better long-tailed oracle character recognition with adversarial data augmentation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reciprocal normalization for domain adaptation. <em>PR</em>,
<em>140</em>, 109533. (<a
href="https://doi.org/10.1016/j.patcog.2023.109533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batch normalization (BN) is widely used in modern deep neural networks , which has been shown to represent the domain-related knowledge, and thus is ineffective for cross-domain tasks like unsupervised domain adaptation (UDA). Existing BN variant methods aggregate source and target domain knowledge in the same channel in normalization module. However, the misalignment between the features of corresponding channels across domains often leads to a sub-optimal transferability. In this paper, we exploit the cross-domain relation and propose a novel normalization method, Reciprocal Normalization (RN). Specifically, RN first presents a Reciprocal Compensation (RC) module to acquire the compensatory for each channel in both domains based on the cross-domain channel-wise correlation. Then RN develops a Reciprocal Aggregation (RA) module to adaptively aggregate the feature with its cross-domain compensatory components. As an alternative to BN, RN is more suitable for UDA problems and can be easily integrated into popular domain adaptation methods. Experiments show that the proposed RN outperforms existing normalization counterparts by a large margin and helps state-of-the-art adaptation approaches achieve better results. The source code is available on https://github.com/Openning07/reciprocal-normalization-for-DA .},
  archive      = {J_PR},
  author       = {Zhiyong Huang and Kekai Sheng and Ke Li and Jian Liang and Taiping Yao and Weiming Dong and Dengwen Zhou and Xing Sun},
  doi          = {10.1016/j.patcog.2023.109533},
  journal      = {Pattern Recognition},
  pages        = {109533},
  shortjournal = {Pattern Recognition},
  title        = {Reciprocal normalization for domain adaptation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An enhanced vision transformer with wavelet position
embedding for histopathological image classification. <em>PR</em>,
<em>140</em>, 109532. (<a
href="https://doi.org/10.1016/j.patcog.2023.109532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Histopathological image classification is a fundamental task in pathological diagnosis workflow. It remains a huge challenge due to the complexity of histopathological images. Recently, hybrid methods combining convolutional neural networks(CNN) with vision transformers(ViT) are proposed to this field. These methods can well represent the global and local contextual information and achieve excellent classification performances. However, the downsampling operation like max-pooling which ignores the sampling theorem transmits the jagged artifacts into transformer, which would lead to an aliasing phenomenon. It makes the subsequent feature maps focus on the incorrect regions and influences the final classification results . In this work, we propose an enhanced vision transformer with wavelet position embedding to tackle this challenge. In particular, a wavelet position embedding module, which introduces the wave transform into position embedding, is employed to enhance the smoothness of discontinuous feature information by decomposing sequences into amplitude and phase in pathological feature maps. In addition, an external multi-head attention is proposed to replace self-attention in the transformer block with two linear layers. It reduces the cost of computation and excavates potential correlations between different samples. We evaluate the proposed method on three public histopathological classification challenging datasets, and perform a quantitative comparison with previous state-of-the-art methods. The results empirically demonstrate that our method achieves the best accuracy. Furthermore, it has the least parameters and a very low FLOPs . In conclusion, the enhanced vision transformer shows high classification performances and demonstrates significant potential for assisting pathologists in pathological diagnosis.},
  archive      = {J_PR},
  author       = {Meidan Ding and Aiping Qu and Haiqin Zhong and Zhihui Lai and Shuomin Xiao and Penghui He},
  doi          = {10.1016/j.patcog.2023.109532},
  journal      = {Pattern Recognition},
  pages        = {109532},
  shortjournal = {Pattern Recognition},
  title        = {An enhanced vision transformer with wavelet position embedding for histopathological image classification},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). FETNet: Feature erasing and transferring network for scene
text removal. <em>PR</em>, <em>140</em>, 109531. (<a
href="https://doi.org/10.1016/j.patcog.2023.109531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scene text removal (STR) task aims to remove text regions and recover the background smoothly in images for private information protection. Most existing STR methods adopt encoder-decoder-based CNNs, with direct copies of the features in the skip connections. However, the encoded features contain both text texture and structure information. The insufficient utilization of text features hampers the performance of background reconstruction in text removal regions. To tackle these problems, we propose a novel Feature Erasing and Transferring (FET) mechanism to reconfigure the encoded features for STR in this paper. In FET, a Feature Erasing Module (FEM) is designed to erase text features. An attention module is responsible for generating the feature similarity guidance. The Feature Transferring Module (FTM) is introduced to transfer the corresponding features in different layers based on the attention guidance. With this mechanism, a one-stage, end-to-end trainable network called FETNet is constructed for scene text removal. In addition, to facilitate research on both scene text removal and segmentation tasks , we introduce a novel dataset, Flickr-ST, with multi-category annotations. A sufficient number of experiments and ablation studies are conducted on the public datasets and Flickr-ST. Our proposed method achieves state-of-the-art performance using most metrics, with remarkably higher quality scene text removal results.},
  archive      = {J_PR},
  author       = {Guangtao Lyu and Kun Liu and Anna Zhu and Seiichi Uchida and Brian Kenji Iwana},
  doi          = {10.1016/j.patcog.2023.109531},
  journal      = {Pattern Recognition},
  pages        = {109531},
  shortjournal = {Pattern Recognition},
  title        = {FETNet: Feature erasing and transferring network for scene text removal},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloud-VAE: Variational autoencoder with concepts embedded.
<em>PR</em>, <em>140</em>, 109530. (<a
href="https://doi.org/10.1016/j.patcog.2023.109530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational Autoencoder (VAE) has been widely and successfully used in learning coherent latent representation of data. However, the lack of interpretability in the latent space constructed by the VAE under the prior distribution is still an urgent problem. This paper proposes a VAE with understandable concept embedding named Cloud-VAE, which constructs interpretable latent space by disentangling the latent variables and considering their uncertainty based on cloud model. Firstly, cloud model-based clustering algorithm cast initial constraint of latent space into a prior distribution of concept which can be embedded into the latent space of the VAE to disentangle the latent variables. Secondly, reparameterization trick based on forward cloud transformation algorithm is designed to estimate the latent space concept by increasing the randomness of latent variables. Furthermore, variational lower bound of Cloud-VAE is derived to guide the training process to construct concepts of latent space, realizing the mutual mapping between latent space and concept space. Finally, experimental results on 6 benchmark datasets show that Cloud-VAE has good clustering and reconstruction performance, which can explicitly explain the aggregation process of the model and discover more interpretable disentangled representations.},
  archive      = {J_PR},
  author       = {Yue Liu and Zitu Liu and Shuang Li and Zhenyao Yu and Yike Guo and Qun Liu and Guoyin Wang},
  doi          = {10.1016/j.patcog.2023.109530},
  journal      = {Pattern Recognition},
  pages        = {109530},
  shortjournal = {Pattern Recognition},
  title        = {Cloud-VAE: Variational autoencoder with concepts embedded},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-automatic muscle segmentation in MR images using deep
registration-based label propagation. <em>PR</em>, <em>140</em>, 109529.
(<a href="https://doi.org/10.1016/j.patcog.2023.109529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully automated approaches based on convolutional neural networks have shown promising performances on muscle segmentation from magnetic resonance (MR) images, but still rely on an extensive amount of training data to achieve valuable results. Muscle segmentation for pediatric and rare diseases cohorts is therefore still often done manually. Producing dense delineations over 3D volumes remains a time-consuming and tedious task, with significant redundancy between successive slices. In this work, we propose a segmentation method relying on registration-based label propagation, which provides 3D muscle delineations from a limited number of annotated 2D slices. Based on an unsupervised deep registration scheme, our approach ensures the preservation of anatomical structures by penalizing deformation compositions that do not produce consistent segmentation from one annotated slice to another. Evaluation is performed on MR data from lower leg and shoulder joints . Results demonstrate that the proposed semi-automatic multi-label segmentation model outperforms state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {Nathan Decaux and Pierre-Henri Conze and Juliette Ropars and Xinyan He and Frances T. Sheehan and Christelle Pons and Douraied Ben Salem and Sylvain Brochard and François Rousseau},
  doi          = {10.1016/j.patcog.2023.109529},
  journal      = {Pattern Recognition},
  pages        = {109529},
  shortjournal = {Pattern Recognition},
  title        = {Semi-automatic muscle segmentation in MR images using deep registration-based label propagation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continual spatio-temporal graph convolutional networks.
<em>PR</em>, <em>140</em>, 109528. (<a
href="https://doi.org/10.1016/j.patcog.2023.109528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based reasoning over skeleton data has emerged as a promising approach for human action recognition. However, the application of prior graph-based methods, which predominantly employ whole temporal sequences as their input, to the setting of online inference entails considerable computational redundancy. In this paper, we tackle this issue by reformulating the Spatio-Temporal Graph Convolutional Neural Network as a Continual Inference Network, which can perform step-by-step predictions in time without repeat frame processing. To evaluate our method, we create a continual version of ST-GCN, Co ST-GCN, alongside two derived methods with different self-attention mechanisms, Co AGCN and Co S-TR. We investigate weight transfer strategies and architectural modifications for inference acceleration, and perform experiments on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets. Retaining similar predictive accuracy, we observe up to 109 × reduction in time complexity, on-hardware accelerations of 26 × , and reductions in maximum allocated memory of 52\% during online inference.},
  archive      = {J_PR},
  author       = {Lukas Hedegaard and Negar Heidari and Alexandros Iosifidis},
  doi          = {10.1016/j.patcog.2023.109528},
  journal      = {Pattern Recognition},
  pages        = {109528},
  shortjournal = {Pattern Recognition},
  title        = {Continual spatio-temporal graph convolutional networks},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards automatic model compression via a unified two-stage
framework. <em>PR</em>, <em>140</em>, 109527. (<a
href="https://doi.org/10.1016/j.patcog.2023.109527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks have become ubiquitous in various domains. Meanwhile, the problems of massive storage and computation costs have hindered the deployment of these models to real-world applications. This paper proposes a novel and unified two-stage framework for automatic model compression . To determine the compression ratio of each layer, we improve the optimization from two aspects. First, to predict the performance of each compression policy, we propose Dynamic BN , which improves the correlation significantly with little computation overhead. Second, to search for the compression ratio allocation, we propose an efficient and hyperparameter-free solving algorithm based on the proposed Hessian matrix approximation and Knapsack problem reformulation. Moreover, comprehensive experiments and analyses are conducted on the CIFAR-100&amp;ImageNet datasets and various network architectures to demonstrate its performance advantages over existing model compression methods under the quantization-only, pruning-only, and pruning-quantization settings.},
  archive      = {J_PR},
  author       = {Weihan Chen and Peisong Wang and Jian Cheng},
  doi          = {10.1016/j.patcog.2023.109527},
  journal      = {Pattern Recognition},
  pages        = {109527},
  shortjournal = {Pattern Recognition},
  title        = {Towards automatic model compression via a unified two-stage framework},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Crowdmeta: Crowdsourcing truth inference with
meta-knowledge transfer. <em>PR</em>, <em>140</em>, 109525. (<a
href="https://doi.org/10.1016/j.patcog.2023.109525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing provides a fast and low-cost solution to collect annotations for training data in computer vision . However, there are two challenges in crowdsourced image annotation: First, when crowdsourced workers perform annotation tasks in an unfamiliar domain, their accuracy will dramatically decline due to the lack of expertise; Second, the difficulties of tasks may be different due to the noises in images, which is only related to the features of images themselves and will affect the judgment of workers. It is well known that transferring knowledge from relevant domains can form a better representation for training samples, which benefits the estimation of workers’ expertise in truth inference models. However, the existing knowledge transfer processes for crowdsourcing require a considerable number of well-collected samples in source domains. Comprehensively considering the above issues, this paper proposes a novel probabilistic model for crowdsourcing truth inference, which fuses few-shot meta-learning and transfer learning . The proposed model transfers meta-knowledge from the source domain to form better high-level representations of the instances in the target domain. Simultaneously utilizing both high-level representations and instance features, the quality of workers and the difficulty of instances can be better modeled and inferred. Experimental results on a number of datasets show that the proposed model not only outperforms the state-of-the-art models but also significantly reduces the number of instances required in the source domain.},
  archive      = {J_PR},
  author       = {Jing Zhang and Sunyue Xu and Victor S. Sheng},
  doi          = {10.1016/j.patcog.2023.109525},
  journal      = {Pattern Recognition},
  pages        = {109525},
  shortjournal = {Pattern Recognition},
  title        = {Crowdmeta: Crowdsourcing truth inference with meta-knowledge transfer},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature clustering-assisted feature selection with
differential evolution. <em>PR</em>, <em>140</em>, 109523. (<a
href="https://doi.org/10.1016/j.patcog.2023.109523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data collection technologies may produce thousands of or even more features in a single dataset. The high dimensionality of data poses a barrier to determining discriminating features due to the curse of dimensionality. Thanks to the global search ability, many population-based feature selection approaches have been proposed. However, very few studies pay attention on that a feature selection task has multiple optimal feature subsets. To search for multiple optimal feature subsets, we propose a feature clustering-assisted feature selection method. The proposed method employs the knowledge of correlation measures to group features. And, this correlation knowledge is embedded into the encoding method and the search process. A niching-based mutation operator is also used to explore the vicinity of a target individual. The aim is to find different feature subsets with very similar or the same classification performance. In addition, a modification operator is proposed aiming to increase the population diversity to improve the feature selection performance. The experiments on 16 datasets show that the proposed algorithm outperforms other popular feature selection methods in terms of classification accuracy and feature subset size.},
  archive      = {J_PR},
  author       = {Peng Wang and Bing Xue and Jing Liang and Mengjie Zhang},
  doi          = {10.1016/j.patcog.2023.109523},
  journal      = {Pattern Recognition},
  pages        = {109523},
  shortjournal = {Pattern Recognition},
  title        = {Feature clustering-assisted feature selection with differential evolution},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boundary-constrained robust regularization for single image
dehazing. <em>PR</em>, <em>140</em>, 109522. (<a
href="https://doi.org/10.1016/j.patcog.2023.109522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, for single image dehazing, regularization-based schemes improve the initial transmission map iteratively by using a guidance map as a structural prior. We conducted experiments on a large number of hazy images and observed that a constrained transmission map affects the quality of the recovered image. However, regularization-based methods do not constrain the transmission map to its physically valid range during the iterative process. It degrades its robustness to outliers, and consequently, deteriorates the quality of the recovered image. In addition, conventional methods fuse the structural information of the guidance and initial transmission map without considering any structural differences between them. To address these issues, in this paper, we present a robust regularization scheme that constraints the transmission map during its enhancement. In the proposed scheme, a nonconvex energy function is constructed that leverages the mutual structural information of the guidance and transmission map. The nonconvex problem is solved by a majorize-minimization algorithm, and the intermediate transmission maps are constrained through the appropriate lower and upper bounds. The retrieved transmission map has better edge-preserving properties, and ultimately, results in a high-quality haze-free image that has faithful colors and fine details. The proposed scheme is tested on benchmark datasets and results are evaluated through quantitative metrics. The comparative analysis has revealed the effectiveness of the proposed scheme.},
  archive      = {J_PR},
  author       = {Usman Ali and Jeongdan Choi and KyoungWook Min and Young-Kyu Choi and Muhammad Tariq Mahmood},
  doi          = {10.1016/j.patcog.2023.109522},
  journal      = {Pattern Recognition},
  pages        = {109522},
  shortjournal = {Pattern Recognition},
  title        = {Boundary-constrained robust regularization for single image dehazing},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative semi-supervised learning via deep and
dictionary representation for image classification. <em>PR</em>,
<em>140</em>, 109521. (<a
href="https://doi.org/10.1016/j.patcog.2023.109521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised dictionary learning and deep learning have achieved promising performance in the classification task . However, in many real-world applications there usually exist very limited labeled training samples, although abundant unlabeled data is relatively easy to collect. How to effectively exploit the discrimination of unlabeled data is still an open question, hence semi-supervised learning has attracted much attention from wide fields. Semi-supervised deep feature learning has well exploited the feature discrimination from only the discriminative viewpoint, while dictionary representation-based classification has also been applied to semi-supervised learning but with shallow features. In this paper, we propose a novel discriminative semi-supervised learning via deep and dictionary representation (DSSLDDR), which jointly utilizes the discrimination of dictionary representation for data reconstruction and the distinguishing feature of each sample. To exploit the powerful discrimination of dictionary representation, class-specific dictionaries are required to discriminatively reconstruct a sample, with the reconstruction error to predict the sample’s class label. To exploit the semantic information, the deep neural network extracts discriminative features by using multiple nonlinear transformations to generate the powerful descriptor. Then the class-specific dictionary learning and deep network learning are integrated together to conduct more accurate class estimation for unlabeled data and learn a more discriminative classifier, where an entropy regularization is designed to balance and control the class estimation of unlabeled data. Furthermore, we propose the DSSLDDR++, the extension model of DSSLDDR based on consistency/contrastive learning to further improve the accuracy of class estimation for unlabeled data, making a more powerful semi-supervised learning classifier. Extensive experiments on benchmark datasets show the effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Meng Yang and Jie Ling and Jiaming Chen and Mao Feng and Jian Yang},
  doi          = {10.1016/j.patcog.2023.109521},
  journal      = {Pattern Recognition},
  pages        = {109521},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative semi-supervised learning via deep and dictionary representation for image classification},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MEP-3M: A large-scale multi-modal e-commerce product
dataset. <em>PR</em>, <em>140</em>, 109519. (<a
href="https://doi.org/10.1016/j.patcog.2023.109519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The product categories are vital for the E-commerce platforms due to the core applications on automatic product category assignment, personalized product recommendations, etc. In this paper, we construct a large-scale M ulti-modal E -commerce P roducts classification dataset MEP-3M, which is large-scale, hierarchical-categorized, multi-modal, fine-grained, and long-tailed. Statistically, MEP-3M consists of over 3 million products, thus achieves the largest data scale in comparison to the existing E-commerce product datasets. The products in MEP-3M are represented in three modalities: image, textual description, and OCR text, and labeled with tree-like labels. The third level labels are extremely fine-grained. In addition, we exploit four novel practical tasks on this dataset, Product classification, Hierarchical Product Classification, Fine-grained Product Classification, and Product Representation Learning . For each task, we present some image-only, text-only, and multi-modal baseline performances for further researches. The MEP-3M dataset will be released at https://github.com/ChenDelong1999/MEP-3M .},
  archive      = {J_PR},
  author       = {Fan Liu and Delong Chen and Xiaoyu Du and Ruizhuo Gao and Feng Xu},
  doi          = {10.1016/j.patcog.2023.109519},
  journal      = {Pattern Recognition},
  pages        = {109519},
  shortjournal = {Pattern Recognition},
  title        = {MEP-3M: A large-scale multi-modal E-commerce product dataset},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A uniform transformer-based structure for feature fusion and
enhancement for RGB-d saliency detection. <em>PR</em>, <em>140</em>,
109516. (<a href="https://doi.org/10.1016/j.patcog.2023.109516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D saliency detection integrates information from both RGB images and depth maps to improve the prediction of salient regions under challenging conditions . The key to RGB-D saliency detection is to fully mine and fuse information at multiple scales across the two modalities. Previous approaches tend to apply the multi-scale and multi-modal fusion separately via local operations, which fails to capture long-range dependencies. Here we propose a transformer-based structure to address this issue. The proposed architecture is composed of two modules: an Intra-modality Feature Enhancement Module (IFEM) and an Inter-modality Feature Fusion Module (IFFM). IFFM conducts a sufficient feature fusion by integrating features from multiple scales and two modalities over all positions simultaneously. IFEM enhances feature on each scale by selecting and integrating complementary information from other scales within the same modality before IFFM. We show that transformer is a uniform operation which presents great efficacy in both feature fusion and feature enhancement, and simplifies the model design. Extensive experimental results on five benchmark datasets demonstrate that our proposed network performs favorably against most state-of-the-art RGB-D saliency detection methods. Furthermore, our model is efficient for having relatively smaller FLOPs and model size compared with other methods.},
  archive      = {J_PR},
  author       = {Yue Wang and Xu Jia and Lu Zhang and Yuke Li and James H. Elder and Huchuan Lu},
  doi          = {10.1016/j.patcog.2023.109516},
  journal      = {Pattern Recognition},
  pages        = {109516},
  shortjournal = {Pattern Recognition},
  title        = {A uniform transformer-based structure for feature fusion and enhancement for RGB-D saliency detection},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensitivity pruner: Filter-level compression algorithm for
deep neural networks. <em>PR</em>, <em>140</em>, 109508. (<a
href="https://doi.org/10.1016/j.patcog.2023.109508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As neural networks get deeper for better performance, the demand for deployable models on resource-constrained devices also grows. In this work, we propose eliminating less sensitive filters to compress models. The previous method evaluates neuron importance using the connection matrix gradient in a single shot. To mitigate the sampling bias, we integrate this measure into the previously proposed “pruning while fine-tuning” framework. Besides classification errors , we introduce the difference between the learned and the single-shot strategy as the second loss component with a self-adjustive hyper-parameter that balances the training goal between improving accuracy and pruning more filters. Our Sensitivity Pruner (SP) adapts the unstructured pruning saliency metric to structured pruning tasks and enables the strategy to be derived sequentially to accommodate the updating sparsity . Experimental results demonstrate that SP significantly reduces the computational cost and the pruned models give comparable or better performance on CIFAR10, CIFAR100, and ILSVRC-12 datasets.},
  archive      = {J_PR},
  author       = {Suhan Guo and Bilan Lai and Suorong Yang and Jian Zhao and Furao Shen},
  doi          = {10.1016/j.patcog.2023.109508},
  journal      = {Pattern Recognition},
  pages        = {109508},
  shortjournal = {Pattern Recognition},
  title        = {Sensitivity pruner: Filter-level compression algorithm for deep neural networks},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-task semi-supervised crowd counting via global to
local self-correction. <em>PR</em>, <em>140</em>, 109506. (<a
href="https://doi.org/10.1016/j.patcog.2023.109506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel multi-task semi-supervised method. To sufficiently exploit massive unlabeled data , multi-task pseudo-labels and global to local self-correction strategy are proposed. Specifically, labeled images and massive amounts of unlabeled images with proposed multi-task pseudo-labels are leveraged for model optimization. The density level of the whole image is predicted in classification task . The density is estimated in density regression task . The crowd area is segmented out in segmentation task . To suppress incorrect predictions caused by the inevitable noises from some unlabeled data misleading the model, the counting relationship between classification task and density task is exploited to propose the global self-correction strategy, and the semantic consistency between density task and segmentation task is mined to propose the local self-correction strategy. The classification task and segmentation task contribute in generating the final highly refined density map from the density task. Extensive experiments on six benchmark datasets indicate the superiority of our method over the SOTA methods in semi-supervised paradigm.},
  archive      = {J_PR},
  author       = {Jiwei Chen and Zengfu Wang},
  doi          = {10.1016/j.patcog.2023.109506},
  journal      = {Pattern Recognition},
  pages        = {109506},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task semi-supervised crowd counting via global to local self-correction},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aeriform in-action: A novel dataset for human action
recognition in aerial videos. <em>PR</em>, <em>140</em>, 109505. (<a
href="https://doi.org/10.1016/j.patcog.2023.109505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human actions being diverse in nature cannot be generalized, thus making it quite difficult to train a machine to recognize such diversified actions. This challenge is further compounded by the lack of availability of datasets for aerial surveillance, as collecting and annotating a large dataset is a formidable task. This paper aims to solve the problem of data scarcity by introducing a new dataset, Aeriform in-action for recognizing human actions from aerial videos. The proposed dataset consists of 32 high-resolution videos containing 13 action classes with 55,477 frames (without augmentation) and almost 400,000 annotations. It includes complex and aggressive actions such as kicking and punching, as well as drone signaling actions like waving and handshaking. The dataset also includes human-object interactions like carrying and reading. In addition to the dataset, this paper also presents a two-step deep learning framework for recognizing human actions based on the integration of human detection and action recognition module. The action recognition module adopts a modified version of the ResNeXt101 architecture (M-ResNext101) to recognize human actions in aerial videos. The performance of the proposed M-ResNext101 model is compared with 13 other deep learning models, and it outperforms all of them with an accuracy of 76.44\% on the test data. The proposed dataset for human action recognition in aerial videos is available on https://surbhi-31.github.io/Aeriform-in-action/.},
  archive      = {J_PR},
  author       = {Surbhi Kapoor and Akashdeep Sharma and Amandeep Verma and Sarbjeet Singh},
  doi          = {10.1016/j.patcog.2023.109505},
  journal      = {Pattern Recognition},
  pages        = {109505},
  shortjournal = {Pattern Recognition},
  title        = {Aeriform in-action: A novel dataset for human action recognition in aerial videos},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scatter matrix decomposition for jointly sparse learning.
<em>PR</em>, <em>140</em>, 109485. (<a
href="https://doi.org/10.1016/j.patcog.2023.109485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Linear Discriminant Analysis (OLDA) based on generalized Eigen-equation is widely used in the field of computer vision and pattern recognition. However, the performance of OLDA for feature extraction and classification needs to be improved as it lacks sparsity for better interpretation of the features. Moreover, computing the orthogonal sparse projections based on LDA is very difficult and is still unsolved. To solve these problems, in this paper, we propose a method called Jointly Sparse Orthogonal Linear Discriminant Analysis (JSOLDA). Different from the existing OLDA, JSOLDA is proposed from a novel viewpoint of scatter matrix decomposition. Theoretical analysis shows that OLDA can be derived by the constrained scatter matrix decomposition. In addition, by imposing L 2,1 -norm on the penalty term, the proposed JSOLDA can obtain the jointly sparse orthogonal projections to perform feature extraction. We also design an iterative algorithm to obtain the optimal solution. Systematic theoretical analysis between the OLDA and JSOLDA are uncovered. Both of convergence and computational complexity are also discussed. Experimental results on four data sets (i.e., COIL100, USPS, ICADAR2003 and CMU PIE) indicate that JSOLDA outperforms several well-known LDA-based and L 2,1 -norm based methods.},
  archive      = {J_PR},
  author       = {Dongmei Mo and Zhihui Lai and Jie Zhou and Hu Qinghua},
  doi          = {10.1016/j.patcog.2023.109485},
  journal      = {Pattern Recognition},
  pages        = {109485},
  shortjournal = {Pattern Recognition},
  title        = {Scatter matrix decomposition for jointly sparse learning},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multicycle disassembly-based decomposition algorithm to
train multiclass support vector machines. <em>PR</em>, <em>140</em>,
109479. (<a href="https://doi.org/10.1016/j.patcog.2023.109479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employing the classic optimization solver to train a multiclass support vector machine (SVM) requires prohibitive training time as the sample size and number of categories increase. It has been proposed to develop the corresponding decomposition algorithm (DA) as it is efficient for training SVMs. However, the dual problem of multiclass SVM comprises complex constraints that complicate DA design, so no corresponding DA has yet been developed. We propose a multicycle disassembly-based DA (MCD-DA) to efficiently solve the training problem of multiclass SVM. First, a graph model is constructed to re-express the constraints in multiclass SVM. Then, the original complex feasible region is partitioned into several simple sub-feasible regions, and multiple cycle-based disassembly strategies are designed to update the working variables analytically within each specific sub-feasible region. We mathematically verify that MCD-DA can stop within a finite number of cycle disassemblies and reach the τ τ -optimal solution satisfying relaxed Karush–Kuhn–Tucker conditions. Remarkably, MCD-DA as a universal decomposition algorithm can be used to solve many other SVM variants, including C -SVM, v -SVM, and one-class SVM. Experimental results using six UCI datasets demonstrate that MCD-DA outperforms typical optimization algorithms for more sample cases.},
  archive      = {J_PR},
  author       = {Tong Gao and Hao Chen},
  doi          = {10.1016/j.patcog.2023.109479},
  journal      = {Pattern Recognition},
  pages        = {109479},
  shortjournal = {Pattern Recognition},
  title        = {Multicycle disassembly-based decomposition algorithm to train multiclass support vector machines},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Triple-attention interaction network for breast tumor
classification based on multi-modality images. <em>PR</em>,
<em>139</em>, 109526. (<a
href="https://doi.org/10.1016/j.patcog.2023.109526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer can be diagnosed using medical imaging. Classification performance of medical imaging can be improved by multi-modality image fusion. However, existing fusion algorithm fail to consider the importance of modality interactions and cannot fully utilize multi-modality information. Attention mechanisms can effectively explore and combine multi-modality information. Thus, we propose a novel triple-attention interaction network for breast tumor classification based on diffusion-weighted imaging (DWI) and apparent dispersion coefficient (ADC) images. A triple inter-modality interaction mechanism is proposed to fully fuse the multi-modality information. Three modal interactions were performed through the developed inter-modality relation module, channel interaction module, and multi-level attention fusion module to explore the correlation, complementary, and discriminative information, respectively. Additionally, we introduce a novel dual parallel-attention module for the incorporation of spatial and channel attention to improve the discriminative ability of single-modality features. Using these mechanisms, the proposed algorithm can mine and explore useful multi-modality information fully, to improve classification performance. Experimental results demonstrate that our algorithm outperforms other multi-modality fusion algorithm, and extensive ablation studies were conducted to verify the advantages of our algorithm. The area under the receiver operating characteristic curve, accuracy, specificity, and sensitivity were 90.5\%, 89.0\%, 85.6\%, and 92.4\%, respectively.},
  archive      = {J_PR},
  author       = {Xiao Yang and Xiaoming Xi and Kesong Wang and Liangyun Sun and Lingzhao Meng and Xiushan Nie and Lishan Qiao and Yilong Yin},
  doi          = {10.1016/j.patcog.2023.109526},
  journal      = {Pattern Recognition},
  pages        = {109526},
  shortjournal = {Pattern Recognition},
  title        = {Triple-attention interaction network for breast tumor classification based on multi-modality images},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSCA-net: Multi-scale contextual attention network for skin
lesion segmentation. <em>PR</em>, <em>139</em>, 109524. (<a
href="https://doi.org/10.1016/j.patcog.2023.109524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lesion segmentation algorithms automatically outline lesion areas in medical images, facilitating more effective identification and assessment of the clinically relevant features, and improving the efficacy and diagnosis accuracy. However, most fully convolutional network based segmentation methods suffer from spatial and contextual information loss when decreasing image resolution. To overcome this shortcoming, this paper proposes a skin lesion segmentation model, namely, the Multi-Scale Contextual Attention Network (MSCA-Net), which can exploit the multi-scale contextual information in images. Inspired by the skip connection of U-Net, we design a multi-scale bridge (MSB) module which interacts with multi-scale features to effectively fuse the multi-scale contextual information of the encoder and decoder path features. We further propose a global-local channel spatial attention module (GL-CSAM), aiming at capturing global contextual information. In addition, to take full advantage of the multi-scale features of the decoder, we propose a scale-aware deep supervision (SADS) module to achieve hierarchical iterative deep supervision. Comprehensive experimental results on the public dataset of ISIC 2017, ISIC 2018, and PH 2 2 show that our proposed method outperforms other state-of-the-art methods, demonstrating the efficacy of our method in skin lesion segmentation. Our code is available at https://github.com/YonghengSun1997/MSCA-Net .},
  archive      = {J_PR},
  author       = {Yongheng Sun and Duwei Dai and Qianni Zhang and Yaqi Wang and Songhua Xu and Chunfeng Lian},
  doi          = {10.1016/j.patcog.2023.109524},
  journal      = {Pattern Recognition},
  pages        = {109524},
  shortjournal = {Pattern Recognition},
  title        = {MSCA-net: Multi-scale contextual attention network for skin lesion segmentation},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Markov clustering regularized multi-hop graph neural
network. <em>PR</em>, <em>139</em>, 109518. (<a
href="https://doi.org/10.1016/j.patcog.2023.109518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have shown great potential for graph data analysis. In this paper, we focus on multi-hop graph neural networks and aim to extend existing models to a high-order multi-hop form for graph-level representation learning. However, such a directly extending method suffers from two limitations, i.e., computational inefficiency and limited representation ability of the multi-hop neighbor . For the former limitation, we utilize an iteration approach to approximate the power of a complex adjacency matrix to achieve linear computational complexity . For the latter limitation, we introduce the Regularized Markov Clustering (R-MCL) to regularize the flow matrix, i.e., the adjacency matrix , in each iteration step . With these two strategies, we construct Markov Clustering Regularized Multi-hop Graph Neural Network (MCMGN) for graph-level representation learning tasks. Specifically, MCMGN consists of a multi-hop message passing phase and a readout phase, where the multi-hop message passing phase aims to learn multi-hop node embedding , and then the readout phase aggregates multi-hop node representations to generate graph embedding for graph-level representation learning tasks. Extensive experiments on eight graph benchmark datasets strongly demonstrate the effectiveness of Markov Clustering Regularized Multi-hop Graph Neural Network, leading to superior performance on graph classification.},
  archive      = {J_PR},
  author       = {Xiaolong Fan and Maoguo Gong and Yue Wu},
  doi          = {10.1016/j.patcog.2023.109518},
  journal      = {Pattern Recognition},
  pages        = {109518},
  shortjournal = {Pattern Recognition},
  title        = {Markov clustering regularized multi-hop graph neural network},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The impact of isolation kernel on agglomerative hierarchical
clustering algorithms. <em>PR</em>, <em>139</em>, 109517. (<a
href="https://doi.org/10.1016/j.patcog.2023.109517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agglomerative hierarchical clustering (AHC) is one of the popular clustering approaches. AHC generates a dendrogram that provides richer information and insights from a dataset than partitioning clustering. However, a major problem with existing distance-based AHC methods is: it fails to effectively identify adjacent clusters with varied densities, regardless of the cluster extraction methods applied to the resultant dendrogram. This paper aims to reveal the root cause of this issue and provides a solution by using a data-dependent kernel. We analyse the condition under which existing AHC methods fail to effectively extract clusters, and give the reason why the data-dependent kernel is an effective remedy. This leads to a new approach to kernerlise existing hierarchical clustering algorithms including the traditional AHC algorithms, HDBSCAN, GDL, PHA and HC-OT. Our extensive empirical evaluation shows that the recently introduced Isolation Kernel produces a higher quality or purer dendrogram than distance, Gaussian Kernel and adaptive Gaussian Kernel in all the above mentioned AHC algorithms.},
  archive      = {J_PR},
  author       = {Xin Han and Ye Zhu and Kai Ming Ting and Gang Li},
  doi          = {10.1016/j.patcog.2023.109517},
  journal      = {Pattern Recognition},
  pages        = {109517},
  shortjournal = {Pattern Recognition},
  title        = {The impact of isolation kernel on agglomerative hierarchical clustering algorithms},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elaborate multi-task subspace learning with discrete group
constraint. <em>PR</em>, <em>139</em>, 109515. (<a
href="https://doi.org/10.1016/j.patcog.2023.109515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-task learning (MTL), multiple related tasks can be learned simultaneously under the shared information to improve the generalization performance . However, most of MTL methods assume that all the learning tasks are related indeed and appropriate for joint learning. In some real situations, this assumption may not hold and further lead to the problem of negative transfer. Therefore, in this paper, we not only focus on researching the problem of robustly learning the common feature structure shared by tasks, but also discriminate with which tasks one task should share. By combining with the idea of subspace learning, we propose an elaborate multi-task subspace learning model (EMTSL) with discrete group structure constraint, which can cluster the learned tasks into a set of groups. By introducing the Schatten p p -norm instead of trace norm, our model EMTSL can better approximate the low-rank constraint and also avoid the trivial solution . Furthermore, we design an efficient algorithm based on the re-weighted method to solve the proposed model. In addition, the convergence analysis of our algorithm is given in this paper. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our method.},
  archive      = {J_PR},
  author       = {Wei Chang and Feiping Nie and Rong Wang and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.109515},
  journal      = {Pattern Recognition},
  pages        = {109515},
  shortjournal = {Pattern Recognition},
  title        = {Elaborate multi-task subspace learning with discrete group constraint},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Bi-RRNet: Bi-level recurrent refinement network for
camouflaged object detection. <em>PR</em>, <em>139</em>, 109514. (<a
href="https://doi.org/10.1016/j.patcog.2023.109514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a lightweight Bi-level Recurrent Refinement Network (Bi-RRNet) for Camouflaged Object Detection (COD) that consists of a Lower-level RRNet (L-RRN) and an Up-level RRNet (U-RRN) to progressively refine the multi-level context features for precise dense prediction. In particular, the L-RRN recursively refines the deeper layer high-level semantic features with the high-resolution low-level features from the earlier layers in a top-down manner, and the U-RRN progressively polishes the refined features from the L-RRN in a recurrent manner, producing the high-resolution semantic features that are essential to accurate COD. Moreover, we develop a Multi-scale Scene Perception Module (MSPM) that, in order to deal with target appearance variation, first compresses the global scene context information at each layer into a learnable weight vector and then modulates the multi-scale context features produced by a filter bank with various local receptive fields using the learned weights. Meanwhile, we design a Region-Consistency Enhancement Module (RCEM) that makes use of high-level semantic features to direct filtering out the cluttered information in the lower-layer features. This module can highlight the regions of camouflaged objects, maximizing the inter-class contrast between the objects and their surroundings. Extensive experiments on four challenging benchmark datasets, including CHAMELEON, CAMO, COD10K, and NC4K, show that our Bi-RRNet outperforms a variety of state-of-the-art methods in terms of accuracy and model parameters. Our Bi-RRNet, in particular, is lightweight, with 14.95M parameters that are only half the size of the state-of-the-art BSA-Net.},
  archive      = {J_PR},
  author       = {Yan Liu and Kaihua Zhang and Yaqian Zhao and Hu Chen and Qingshan Liu},
  doi          = {10.1016/j.patcog.2023.109514},
  journal      = {Pattern Recognition},
  pages        = {109514},
  shortjournal = {Pattern Recognition},
  title        = {Bi-RRNet: Bi-level recurrent refinement network for camouflaged object detection},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual similarity pre-training and domain difference
encouragement learning for vehicle re-identification in the wild.
<em>PR</em>, <em>139</em>, 109513. (<a
href="https://doi.org/10.1016/j.patcog.2023.109513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing unsupervised domain adaptation (UDA) tasks require extensive annotated wild data in the source domain to be generalized to the target domain. Additionally, the large gap between the source and target domains hinder the clustering performance. Our work concentrates on few-shot UDA task to train a robust Re-ID model from practical vehicle re-identification (Re-ID). That is to say, this task learns discriminative representations from a few labeled source data to unlabeled target data. In this paper, a novel progressive few-shot UDA learning framework for vehicle Re-ID is proposed, which consists of two branches. In source branch, the dual prior model is used to gain the color and IDs in unlabeled source data. A dual constraint label smoothing regularization (DCLSR) loss is designed to supervise extensive unlabeled source data during pre-training phase, which considers color and ID constraints to mine the similarity between unlabeled source data and a few labeled ones. The target branch develops a progressive domain difference encouragement learning method to optimize the cross-domain capability of Re-ID model. The domain difference penalty term (DDPT) is encoded by the feature variations before and after style transfer, which improves clustering results and refines the pseudo label. Comprehensive experimental results verify that the proposed approach outperforms other ones in the practical UDA task.},
  archive      = {J_PR},
  author       = {Qi Wang and Yuling Zhong and Weidong Min and Haoyu Zhao and Di Gai and Qing Han},
  doi          = {10.1016/j.patcog.2023.109513},
  journal      = {Pattern Recognition},
  pages        = {109513},
  shortjournal = {Pattern Recognition},
  title        = {Dual similarity pre-training and domain difference encouragement learning for vehicle re-identification in the wild},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Not all samples are born equal: Towards effective
clean-label backdoor attacks. <em>PR</em>, <em>139</em>, 109512. (<a
href="https://doi.org/10.1016/j.patcog.2023.109512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks. The attacked model behaves normally on benign samples, while its predictions are misled whenever adversary-specified trigger patterns appear. Currently, clean-label backdoor attacks are usually regarded as the most stealthy methods in which adversaries can only poison samples from the target class without modifying their labels. However, these attacks can hardly succeed. In this paper, we reveal that the difficulty of clean-label attacks mainly lies in the antagonistic effects of ‘robust features’ related to the target class contained in poisoned samples. Specifically, robust features tend to be easily learned by victim models and thus undermine the learning of trigger patterns. Based on these understandings, we propose a simple yet effective plug-in method to enhance clean-label backdoor attacks by poisoning ‘hard’ instead of random samples. We adopt three classical difficulty metrics as examples to implement our method. We demonstrate that our method can consistently improve vanilla attacks, based on extensive experiments on benchmark datasets.},
  archive      = {J_PR},
  author       = {Yinghua Gao and Yiming Li and Linghui Zhu and Dongxian Wu and Yong Jiang and Shu-Tao Xia},
  doi          = {10.1016/j.patcog.2023.109512},
  journal      = {Pattern Recognition},
  pages        = {109512},
  shortjournal = {Pattern Recognition},
  title        = {Not all samples are born equal: Towards effective clean-label backdoor attacks},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underwater object detection algorithm based on feature
enhancement and progressive dynamic aggregation strategy. <em>PR</em>,
<em>139</em>, 109511. (<a
href="https://doi.org/10.1016/j.patcog.2023.109511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problems that the conventional object detector is hard to extract features and miss detection of small objects when detecting underwater objects due to the noise of underwater environment and the scale change of objects, this paper designs a novel feature enhancement &amp; progressive dynamic aggregation strategy, and proposes a new underwater object detector based on YOLOv5s. Firstly, a feature enhancement gating module is designed to selectively suppress or enhance multi-level features and reduce the interference of underwater complex environment noise on feature fusion . Then, the adjacent feature fusion mechanism and dynamic fusion module are designed to dynamically learn fusion weights and perform multi-level feature fusion progressively, so as to suppress the conflict information in multi-scale feature fusion and prevent small objects from being submerged by the conflict information. At last, a spatial pyramid pool structure (FMSPP) based on the same size quickly mixed pool layer is proposed, which can make the network obtain stronger description ability of texture and contour features, reduce the parameters, and further improve the generalization ability and classification accuracy. The ablation experiments and multi-method comparison experiments on URPC and DUT-USEG data sets prove the effectiveness of the proposed strategy. Compared with the current mainstream detectors, our detector achieves obvious advantages in detection performance and efficiency.},
  archive      = {J_PR},
  author       = {Xia Hua and Xiaopeng Cui and Xinghua Xu and Shaohua Qiu and Yingjie Liang and Xianqiang Bao and Zhong Li},
  doi          = {10.1016/j.patcog.2023.109511},
  journal      = {Pattern Recognition},
  pages        = {109511},
  shortjournal = {Pattern Recognition},
  title        = {Underwater object detection algorithm based on feature enhancement and progressive dynamic aggregation strategy},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Patch loss: A generic multi-scale perceptual loss for single
image super-resolution. <em>PR</em>, <em>139</em>, 109510. (<a
href="https://doi.org/10.1016/j.patcog.2023.109510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In single image super-resolution (SISR), although PSNR is a key metric for signal fidelity, images with high PSNR do not necessarily render high visual quality. As a result, current perception-driven SISR methods employ perceptual metrics close to the human eye to measure the quality of the generated images. Unfortunately, the perceptual loss and adversarial loss, widely used by the perception-driven SISR methods, still underperform on these non-differentiable perceptual metrics. To this end, we propose a generic multi-scale perceptual loss, i.e., the patch loss, which can be easily plugged into off-the-shelf SISR methods to improve a broad range of perceptual metrics. Specifically, the proposed patch loss minimizes the multi-scale similarity of image patches and enhances the restoration of regions with complex textures and sharp edges via parameter-free adaptive patch-wise attention. Our proposed patch loss introduces more realistic details compared to the perceptual loss and fewer artifacts compared to the adversarial loss.},
  archive      = {J_PR},
  author       = {Tai An and Binjie Mao and Bin Xue and Chunlei Huo and Shiming Xiang and Chunhong Pan},
  doi          = {10.1016/j.patcog.2023.109510},
  journal      = {Pattern Recognition},
  pages        = {109510},
  shortjournal = {Pattern Recognition},
  title        = {Patch loss: A generic multi-scale perceptual loss for single image super-resolution},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FGPNet: A weakly supervised fine-grained 3D point clouds
classification network. <em>PR</em>, <em>139</em>, 109509. (<a
href="https://doi.org/10.1016/j.patcog.2023.109509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point clouds classification has been a hot research topic and received great progress in recent years. However, due to the similar data distributions and subtle differences among various sub-categories in a meta-category, the 3D point clouds classification at a fine-grained level is still very challenging, especially without the annotations of part locations or attributes. In this paper, we propose a novel weakly supervised network for fine-grained 3D point clouds classification, namely FGPNet. Different from the previous supervised fine-grained classification methods that use class labels and other manual annotation information, FGPNet develops a unified framework to address both local geometric details and global spatial structures only using the class labels as input. Specifically, FGPNet firstly employs a context-aware discriminative feature extraction (CDFE) module, which extract contextual contrasted information across differential receptive fields hierarchically, and further capture discriminative local details from point clouds. Subsequently, an SimAM-Capsule Aggregation (SCA) module is introduced to highlight the significant local features and capture their spatial relationships. Quantitative and qualitative experimental results on fine-grained dataset including three categories Airplane, Chair and Car demonstrate that FGPNet outperforms the state-of-the-art methods on fine-grained 3D point clouds classification tasks .},
  archive      = {J_PR},
  author       = {Huihui Shao and Jing Bai and Rusong Wu and Jinzhe Jiang and Hongbo Liang},
  doi          = {10.1016/j.patcog.2023.109509},
  journal      = {Pattern Recognition},
  pages        = {109509},
  shortjournal = {Pattern Recognition},
  title        = {FGPNet: A weakly supervised fine-grained 3D point clouds classification network},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Doubly contrastive representation learning for federated
image recognition. <em>PR</em>, <em>139</em>, 109507. (<a
href="https://doi.org/10.1016/j.patcog.2023.109507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problem of personalized federated learning (FL) with the schema of contrastive learning (CL), which is to implement collaborative pattern classification by many clients. The traditional FL frameworks mostly facilitate the global model for the server and the local models for the clients to be similar, often ignoring the data heterogeneity of the clients. Aiming at achieving better performance in clients, this study introduces a personalized federated contrastive learning model, dubbed PerFCL, by proposing a new approach to doubly contrastive representation learning (DCL). Concretely, PerFCL borrows the DCL scheme, where one CL loss compares the shared parts of local models with the global model and the other CL loss compares the personalized parts of local models with the global model. To encourage the difference between the two parts, we created a double optimization problem composed of maximizing the comparison agreement for the former and minimizing the comparison agreement for the latter. We evaluated the proposed model on three publicly available data sets for federated image classification . Experiment results show that PerFCL benefits from the proposed DCL strategy and performs better than the state-of-the-art federated-learning models.},
  archive      = {J_PR},
  author       = {Yupei Zhang and Yunan Xu and Shuangshuang Wei and Yifei Wang and Yuxin Li and Xuequn Shang},
  doi          = {10.1016/j.patcog.2023.109507},
  journal      = {Pattern Recognition},
  pages        = {109507},
  shortjournal = {Pattern Recognition},
  title        = {Doubly contrastive representation learning for federated image recognition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-dimensional graph neural network for sequential
recommendation. <em>PR</em>, <em>139</em>, 109504. (<a
href="https://doi.org/10.1016/j.patcog.2023.109504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) technology has been widely used in recommendation systems because most information in recommendation systems has a graph structure in nature, and GNNs have advantages in graph representation learning. In sequential recommendation, the relationships between interacting items can be constructed as an isomorphic graph, and (GNNs) can capture high-order information between graph nodes . Many models have used graph-based methods for sequential recommendation, and achieved great success. However, the existing research only considers the number of interactions between items when constructing the item graph. As such, revisions are needed to capture the multi-dimensional transformation relationships between items. Hence, we emphasize the importance of multi-dimensional information, and we propose a C ategory and T ime information integrated G raph N eural N etwork (CTGNN), which combines the item category and interaction time information with a multi-layer graph convolution network to form multi-dimensional fine-grained item representations. In addition, we design a temporal self-attention network to model the dynamic user preference and make the next-item recommendation. Finally, we conduct extensive experiments on three real-world datasets, and the results demonstrate the excellent performance of the proposed model.},
  archive      = {J_PR},
  author       = {Yongjing Hao and Jun Ma and Pengpeng Zhao and Guanfeng Liu and Xuefeng Xian and Lei Zhao and Victor S. Sheng},
  doi          = {10.1016/j.patcog.2023.109504},
  journal      = {Pattern Recognition},
  pages        = {109504},
  shortjournal = {Pattern Recognition},
  title        = {Multi-dimensional graph neural network for sequential recommendation},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A composite network model for face super-resolution with
multi-order head attention facial priors. <em>PR</em>, <em>139</em>,
109503. (<a href="https://doi.org/10.1016/j.patcog.2023.109503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face super-resolution (FSR) aims to reconstruct high-resolution face images from low-resolution (LR) ones. Despite the progress made by deep convolutional neural networks (DCNNs) on FSR, convolutions struggle to relate spatially distant concepts and what is more, all image pixels and prior information (e.g., landmarks and facial component heatmaps) are treated equally regardless of importance, causing inaccuracy and decreasing the quality of face image recovery. To address these issues, in this paper we propose a composite network model for FSR with multi-order head attention facial priors. The proposed model contains a face hallucination transformer (FHT)-based network and a multi-order head attention (MOHA)-based DCNN. The FHT-based network can capture long-range dependencies and gradually increase resolution to achieve efficient and effective inference, while the MOHA-based DCNN exploits detailed and two-dimensional information of LR face images. Moreover, the novel generic submodule of the MOHA-based DCNN, namely Multi-Order Head Attention Network , can accurately model the relationship of facial components between spatial and channel dimensions. The proposed composite network model seamlessly integrates the advantages of DCNNs and transformers to super-resolve LR face images. When compared with state-of-the-art FSR methods on public benchmark datasets, the proposed model shows competitive recovery performance.},
  archive      = {J_PR},
  author       = {Feng Wei and Song Wang and Jucheng Yang and Xiao Sun and Yuan Wang and Yarui Chen},
  doi          = {10.1016/j.patcog.2023.109503},
  journal      = {Pattern Recognition},
  pages        = {109503},
  shortjournal = {Pattern Recognition},
  title        = {A composite network model for face super-resolution with multi-order head attention facial priors},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly-supervised pre-training for 3D human pose estimation
via perspective knowledge. <em>PR</em>, <em>139</em>, 109497. (<a
href="https://doi.org/10.1016/j.patcog.2023.109497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep learning-based 3D pose estimation approaches require plenty of 3D pose annotations. However, existing 3D datasets lack diversity, which limits the performance of current methods and their generalization ability . Although existing methods utilize 2D pose annotations to help 3D pose estimation, they mainly focus on extracting 2D structural constraints from 2D poses, ignoring the 3D information hidden in the images. In this paper, we propose a novel method to extract weak 3D information directly from 2D images without 3D pose supervision. Firstly, we utilize 2D pose annotations and perspective prior knowledge to generate the relative depth of human joints . Then, we collect a 2D pose dataset (MCPC) and generate relative depth labels. Based on MCPC, we propose a weakly-supervised pre-training (WSP) strategy to distinguish the depth relationship between two points in an image. WSP enables the learning of the relative depth of two keypoints on lots of in-the-wild images, which is more capable of predicting depth and generalization ability for 3D human pose estimation. After fine-tuning the pose model on 3D pose datasets, WSP achieves state-of-the-art results on two widely-used benchmarks.},
  archive      = {J_PR},
  author       = {Zhongwei Qiu and Kai Qiu and Jianlong Fu and Dongmei Fu},
  doi          = {10.1016/j.patcog.2023.109497},
  journal      = {Pattern Recognition},
  pages        = {109497},
  shortjournal = {Pattern Recognition},
  title        = {Weakly-supervised pre-training for 3D human pose estimation via perspective knowledge},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint spatial and scale attention network for multi-view
facial expression recognition. <em>PR</em>, <em>139</em>, 109496. (<a
href="https://doi.org/10.1016/j.patcog.2023.109496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view facial expression recognition (FER) is a challenging task because the appearance of an expression varies greatly due to poses. To alleviate the influences of poses, recently developed methods perform pose normalization, learn pose-invariant features, or learn pose-specific FER classifiers. However, these methods usually rely on a prerequisite pose estimator or expressive region detector that is independent of the subsequent expression analysis. Different from existing methods, we propose a joint spatial and scale attention network (SSA-Net) to localize proper regions for simultaneous head pose estimation (HPE) and FER. Specifically, SSA-Net discovers the regions most relevant to the facial expression at hierarchical scales by a spatial attention mechanism, and the most informative scales are selected in a scale attention learning manner to learn the joint pose-invariant and expression-discriminative representations. Then, we employ a dynamically constrained multi-task learning mechanism with a delicately designed constrain regulation to properly and adaptively train the network to optimize the representations, thus achieving accurate multi-view FER. The effectiveness of the proposed SSA-Net is validated on three multi-view datasets (BU-3DFE, Multi-PIE, and KDEF) and three in-the-wild FER datasets (AffectNet, SFEW, and FER2013). Extensive experiments demonstrate that the proposed framework outperforms existing state-of-the-art methods under both within-dataset and cross-dataset settings, with relative accuracy gains of 2.36\%, 1.33\%, 3.11\%, 2.84\%, 15.7\%, and 7.57\%, respectively.},
  archive      = {J_PR},
  author       = {Yuanyuan Liu and Jiyao Peng and Wei Dai and Jiabei Zeng and Shiguang Shan},
  doi          = {10.1016/j.patcog.2023.109496},
  journal      = {Pattern Recognition},
  pages        = {109496},
  shortjournal = {Pattern Recognition},
  title        = {Joint spatial and scale attention network for multi-view facial expression recognition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Class-incremental object detection. <em>PR</em>,
<em>139</em>, 109488. (<a
href="https://doi.org/10.1016/j.patcog.2023.109488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning architectures have shown remarkable results in the object detection task. However, they experience a critical performance drop when they are required to learn new classes incrementally without forgetting old ones. This catastrophic forgetting phenomenon impedes the deployment of artificial intelligence in real word scenarios where systems need to learn new and different representations over time. Recently, many incremental learning methods have been proposed to avoid the catastrophic forgetting problem. However, current state-of-the-art class-incremental learning strategies aim at preserving the knowledge of old classes while learning new ones sequentially, which would encounter other problems as follows: (1) In the process of preserving information of old classes, only a small portion of data in the previous tasks are kept and replayed during training, which inevitably incurs bias that is favorable for the new classes but malicious to the old classes. (2) With the knowledge of previous classes distilled into the new model, a sub-optimal solution for the new task is obtained since the preserving process of previous classes sabotages the training of new classes. To address these issues, termed as Information Asymmetry (IA) , we propose a double-head framework which preserves the knowledge of old classes and learns the knowledge of new classes separately. Specifically, we transfer the knowledge of the previous model to the current learned one for overcoming the catastrophic forgetting problem. Furthermore, considering that IA would introduce impacts on the training of the new model, we propose a Non-Affection mask to distill the knowledge of the interested regions at the feature level. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods on PASCAL VOC and MS COCO datasets.},
  archive      = {J_PR},
  author       = {Na Dong and Yongqiang Zhang and Mingli Ding and Yancheng Bai},
  doi          = {10.1016/j.patcog.2023.109488},
  journal      = {Pattern Recognition},
  pages        = {109488},
  shortjournal = {Pattern Recognition},
  title        = {Class-incremental object detection},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning of scene flow with occlusion
handling through feature masking. <em>PR</em>, <em>139</em>, 109487. (<a
href="https://doi.org/10.1016/j.patcog.2023.109487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we improve the optical flow and depth based on the important observation that they should share the geometric structure of the reference image. We initially propose a feature masking method to reduce the occlusion impact on the optical flow and depth by producing preliminary motion features that share the structure of the reference image. In addition, we propose a deformable decoder that learns the geometrical structure of the reference image in the form of a group of offsets and uses them to adapt the motion features of flow and depth maps, thereby preventing incorrect propagation in occluded regions and providing more structural details in the other regions. Furthermore, we recursively update the optical flow with self-supervised cues learned from the rigid flow and optical flow. Our method achieves a new state-of-the-art result for the optical flow on the KITTI 2015 benchmark with F1 = 11.17\%.},
  archive      = {J_PR},
  author       = {Xuezhi Xiang and Rokia Abdein and Ning Lv and Jie Yang},
  doi          = {10.1016/j.patcog.2023.109487},
  journal      = {Pattern Recognition},
  pages        = {109487},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised learning of scene flow with occlusion handling through feature masking},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic graph convolutional networks by semi-supervised
contrastive learning. <em>PR</em>, <em>139</em>, 109486. (<a
href="https://doi.org/10.1016/j.patcog.2023.109486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional graph convolutional network(GCN) and its variants usually only propagate node information through the topology given by the dataset. However, the given topology can only represent a certain relationship and ignore some correlative feature information between nodes, which may make the graph convolutional networks unable to fully utilize the data information. To address the above issue, a novel model named Dynamic Graph Convolutional Networks by Semi-Supervised Contrastive Learning (DGSCL) is proposed in this paper. First, a feature graph is dynamically constructed from the input node features to exploit the potential correlative feature information between nodes. Then, to ensure a high-quality feature graph, a semi-supervised contrastive learning method is designed to learn discriminative node embeddings , which can iteratively refine the constructed feature graph with the learned node embeddings . Finally, we fuse the node embeddings obtained from the given topology and the dynamic feature graph by two co-attention modules to produce more informative embeddings for the classification task . Through a series of experiments, we demonstrate the competitive performance of our model on seven node classification benchmarks.},
  archive      = {J_PR},
  author       = {Guolin Zhang and Zehui Hu and Guoqiu Wen and Junbo Ma and Xiaofeng Zhu},
  doi          = {10.1016/j.patcog.2023.109486},
  journal      = {Pattern Recognition},
  pages        = {109486},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic graph convolutional networks by semi-supervised contrastive learning},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepActsNet: A deep ensemble framework combining features
from face, hands, and body for action recognition. <em>PR</em>,
<em>139</em>, 109484. (<a
href="https://doi.org/10.1016/j.patcog.2023.109484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition from videos has gained substantial focus due to its wide applications in the field of video understanding . Most of the existing approaches extract human skeleton data from videos to encode actions because of the invariance nature of the skeleton information with respect to lightning conditions and background changes. Despite their success in achieving high recognition accuracy, methods based on limited body joints fail to capture the nuances of subtle body parts which are highly relevant for discriminating similar actions. In this paper, we overcome this limitation by presenting a holistic framework for combining spatial and motion features from the body, face, and hands to develop a novel data representation termed “Deep Actions Stamps (DeepActs)” for video-based action recognition. Compared to the skeleton sequences based on limited body joints, DeepActs encode more effective spatio-temporal features that provide robustness against pose estimation noises and improve action recognition accuracy. We also present “DeepActsNet”, a deep learning based ensemble model which learns convolutional and structural features from Deep Action Stamps for highly accurate action recognition. Experiments on three challenging action recognition datasets (NTU60, NTU120, and SYSU) show that the proposed model produces significant improvements in the action recognition accuracy with less computational cost compared to the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Umar Asif and Deval Mehta and Stefan Von Cavallar and Jianbin Tang and Stefan Harrer},
  doi          = {10.1016/j.patcog.2023.109484},
  journal      = {Pattern Recognition},
  pages        = {109484},
  shortjournal = {Pattern Recognition},
  title        = {DeepActsNet: A deep ensemble framework combining features from face, hands, and body for action recognition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep debiased contrastive hashing. <em>PR</em>,
<em>139</em>, 109483. (<a
href="https://doi.org/10.1016/j.patcog.2023.109483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has achieved great success in multimedia retrieval due to its high computing efficiency and low storage cost. Recently, contrastive-learning-based hashing methods have achieved decent retrieval performance in label-free scenarios by learning distortion-invariant representations with Siamese networks . Their learning principle, i.e ., instance discrimination , maximizes the correlation between self-augmented views and treats all others as negative samples. However, it may learn with false negative samples that are naturally similar, resulting in biased hash learning. To bridge this flaw, we reveal the between-instance similarity of naturally similar samples by exploring the latent structure of the training data. As a result, we propose the D eep D ebiased C ontrastive H ashing ( DDCH ) algorithm, using the neighborhood discovery module to explore the intrinsic similarity relationship that can help contrastive hashing reduce false negatives for superior discriminatory ability . Furthermore, we elucidate the rationale for incorporating the module into the contrastive hashing framework and explain our hashing process from an Expectation-Maximization (EM) perspective. Extensive experimental results on three benchmark image datasets demonstrate that DDCH significantly outperforms the state-of-the-art unsupervised hashing methods for image retrieval.},
  archive      = {J_PR},
  author       = {Rukai Wei and Yu Liu and Jingkuan Song and Yanzhao Xie and Ke Zhou},
  doi          = {10.1016/j.patcog.2023.109483},
  journal      = {Pattern Recognition},
  pages        = {109483},
  shortjournal = {Pattern Recognition},
  title        = {Deep debiased contrastive hashing},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capacitive empirical risk function-based bag-of-words and
pattern classification processes. <em>PR</em>, <em>139</em>, 109482. (<a
href="https://doi.org/10.1016/j.patcog.2023.109482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes capacitive bag-of-words (Cap BoW) modeling and capacitive pattern classification processes for improved generalization ability . The Cap BoW and capacitive pattern classification are realized by introducing the notion of the capacitive empirical risk function (CERF). The CERF is used as a cost function to build a capacitive pattern classification process. The resulted capacitive pattern classification is used in the classification stage of the bag-of-words process, realizing thereby the Cap BoW process. The use of the CERF in building the capacitive pattern classification and Cap BoW processes is demonstrated to achieve simultaneous reduction to the empirical risk function (ERF) and confidence interval, reducing potential overfitting and enhancing the generalization ability of considered models. To have a thorough evaluation, two groups of experiments are carried out. The first group addresses the capacitive pattern classification process for 4 datasets. The second group addresses a more holistic impact of the CERF by using Cap Bow model to build capacitive dual ergodicity limits-based bag-of-words (Cap DEL-BoW) process, which is applied to 5 image datasets. In both groups of experiments, remarkable enhancement is demonstrated with the use of CERF-based pattern classification and Cap BoW processes. Comparison with corresponding conventional non-capacitive models demonstrates the tangible enhancement with the use of CERF-based models.},
  archive      = {J_PR},
  author       = {Ibrahim F. Ghalyan},
  doi          = {10.1016/j.patcog.2023.109482},
  journal      = {Pattern Recognition},
  pages        = {109482},
  shortjournal = {Pattern Recognition},
  title        = {Capacitive empirical risk function-based bag-of-words and pattern classification processes},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object-centric contour-aware data augmentation using
superpixels of varying granularity. <em>PR</em>, <em>139</em>, 109481.
(<a href="https://doi.org/10.1016/j.patcog.2023.109481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regional dropout strategies have demonstrated to be very effective in improving both the performance and the generalization capability of deep learning models. However, when such strategies are performed in a totally random manner, the background noise and label mismatch problems arise. To tackle such problems, existing approaches typically focus on regions with the highest distinctiveness. Yet, there are two main drawbacks of existing approaches: (I) Many existing region-based augmentation methods can only use rectangular regions, resulting in the loss of object contour information; (II) Deterministic selection of the most discriminative regions leads to poor diversification in data augmentation. In fact, a trade-off is needed between diversification and concentration, which can decrease the undesirable noise. In this paper, we propose a novel object-centric contour-aware CutMix data augmentation strategy with arbitrary- shape and size superpixel supports, which is hereafter referred to as OcCaMix for short. It not only captures the most discriminative regions, but also effectively preserves the contour details of the objects. Moreover, it enables the search of natural object parts of different sizes. Extensive experiments on a large number of benchmark datasets show that OcCaMix significantly outperforms state-of-the-art CutMix based data augmentation methods in classification tasks . The source codes and trained models are available at https://github.com/DanielaPlusPlus/OcCaMix .},
  archive      = {J_PR},
  author       = {F. Dornaika and D. Sun and K. Hammoudi and J. Charafeddine and A. Cabani and C. Zhang},
  doi          = {10.1016/j.patcog.2023.109481},
  journal      = {Pattern Recognition},
  pages        = {109481},
  shortjournal = {Pattern Recognition},
  title        = {Object-centric contour-aware data augmentation using superpixels of varying granularity},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on machine learning from few samples. <em>PR</em>,
<em>139</em>, 109480. (<a
href="https://doi.org/10.1016/j.patcog.2023.109480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning , few surveys for few sample learning (FSL) are available. We extensively study almost all papers of FSL spanning from the 2000s to now and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history and current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review their latest advances. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision , natural language processing , audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.},
  archive      = {J_PR},
  author       = {Jiang Lu and Pinghua Gong and Jieping Ye and Jianwei Zhang and Changshui Zhang},
  doi          = {10.1016/j.patcog.2023.109480},
  journal      = {Pattern Recognition},
  pages        = {109480},
  shortjournal = {Pattern Recognition},
  title        = {A survey on machine learning from few samples},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast support vector machine training via three-term
conjugate-like SMO algorithm. <em>PR</em>, <em>139</em>, 109478. (<a
href="https://doi.org/10.1016/j.patcog.2023.109478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is an important class of methods in pattern recognition, and the sequential minimal optimization (SMO) algorithm is one of the most popular methods for training SVM at present. Based on the conjugate sequential minimal optimization algorithm (CSMO), we propose a novel three-term conjugate-like sequential minimal optimization algorithm (TCSMO) for classification and regression tasks . Compared with the CSMO, although the three-term conjugate-like SMO slightly increases the amount of arithmetic operations in each iteration, it significantly reduces the number of iterations required to converge to the specified accuracy and shortens the training time of the SVM. Additionally, we give a convergence proof of the three-term conjugate-like SMO algorithm and four new conjugate parameters. Numerical experiments show that the three-term conjugate-like SMO algorithm performs better numerically in both classification and regression tasks .},
  archive      = {J_PR},
  author       = {Lang Yu and Shengjie Li and Siyi Liu},
  doi          = {10.1016/j.patcog.2023.109478},
  journal      = {Pattern Recognition},
  pages        = {109478},
  shortjournal = {Pattern Recognition},
  title        = {Fast support vector machine training via three-term conjugate-like SMO algorithm},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep generative image priors for semantic face manipulation.
<em>PR</em>, <em>139</em>, 109477. (<a
href="https://doi.org/10.1016/j.patcog.2023.109477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works on generative adversarial networks (GANs) mainly focus on how to synthesize high-fidelity images. In this paper, we present a framework to leverage the knowledge learned by GANs for semantic face manipulation. In particular, we propose to control the semantics of synthesized faces by adapting the latent codes with an attribute prediction model. Moreover, in order to achieve a more accurate estimation of different facial attributes, we propose to pretrain the attribute prediction model by inverting the synthesized face images back to the GAN latent space. As a result, our method explicitly considers the semantics encoded in the latent space of a pretrained GAN and is able to faithfully edit various attributes like eyeglasses, smiling, bald, age, mustache and gender for high-resolution face images. Extensive experiments show that our method has superior performance compared to state of the art for both face attribute prediction and semantic face manipulation.},
  archive      = {J_PR},
  author       = {Xianxu Hou and Linlin Shen and Zhong Ming and Guoping Qiu},
  doi          = {10.1016/j.patcog.2023.109477},
  journal      = {Pattern Recognition},
  pages        = {109477},
  shortjournal = {Pattern Recognition},
  title        = {Deep generative image priors for semantic face manipulation},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TreeNet: Structure preserving multi-class 3D point cloud
completion. <em>PR</em>, <em>139</em>, 109476. (<a
href="https://doi.org/10.1016/j.patcog.2023.109476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating the missing data of 3D object point clouds from partial observations is a challenging task. Existing state-of-the-art learning-based 3D point cloud completion methods tend to use a limited number of categories/classes of training data and regenerate the entire point cloud based on the training datasets. As a result, output 3D point clouds generated by such methods may lose details (i.e. sharp edges and topology changes) due to the lack of multi-class training. These methods also lose the structural and spatial details of partial inputs due to the models do not separate the reconstructed partial input from missing points in the output. In this paper, we propose a novel deep learning network - TreeNet for 3D point cloud completion. TreeNet has two networks in hierarchical tree-based structures: TreeNet-multiclass focuses on multi-class training with a specific class of the completion task on each sub-tree to improve the quality of point cloud output; TreeNet-binary focuses on generating points in missing areas and fully preserving the original partial input. TreeNet-multiclass and TreeNet-binary are both network decoders and can be trained independently. TreeNet decoder is the combination of TreeNet-multiclass and TreeNet-binary and is trained with an encoder from existing methods (i.e. PointNet encoder). We compare the proposed TreeNet with five state-of-the-art learning-based methods on fifty classes of the public Shapenet dataset and unknown classes, which shows that TreeNet provides a significant improvement in the overall quality and exhibits strong generalization to unknown classes that are not trained.},
  archive      = {J_PR},
  author       = {Long Xi and Wen Tang and TaoRuan Wan},
  doi          = {10.1016/j.patcog.2023.109476},
  journal      = {Pattern Recognition},
  pages        = {109476},
  shortjournal = {Pattern Recognition},
  title        = {TreeNet: Structure preserving multi-class 3D point cloud completion},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Localized curvature-based combinatorial subgraph sampling
for large-scale graphs. <em>PR</em>, <em>139</em>, 109475. (<a
href="https://doi.org/10.1016/j.patcog.2023.109475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a subgraph sampling method based on curvature to train large-scale graphs via mini-batch training. Owing to the difficulty in sampling globally optimal subgraphs from large graphs, we sample the subgraphs to minimize the distributional metric with combinatorial sampling. In particular, we define a combinatorial metric that distributionally measures the similarity between an original graph and all possible node and edge combinations of the subgraphs. Further, we prove that the subgraphs sampled using the probability model proportional to the discrete Ricci curvature ( i.e ., Ollivier-Ricci curvatures) of the edges can minimize the proposed metric. Moreover, as accurate calculation of the curvature on a large graph is challenging, we propose to use a localized curvature considering only 3-cycles on the graph, suggesting that this is a sufficiently approximated curvature on a sparse graph. We also show that the probability models of conventional sampling methods are related to coarsely approximated curvatures with no cycles, implying that the curvature is closely related to subgraph sampling. The experimental results confirm the feasibility of integrating the proposed curvature-based sampling method into existing graph neural networks to improve performance.},
  archive      = {J_PR},
  author       = {Dong Wook Shu and Youjin Kim and Junseok Kwon},
  doi          = {10.1016/j.patcog.2023.109475},
  journal      = {Pattern Recognition},
  pages        = {109475},
  shortjournal = {Pattern Recognition},
  title        = {Localized curvature-based combinatorial subgraph sampling for large-scale graphs},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fourier-based augmentation with applications to domain
generalization. <em>PR</em>, <em>139</em>, 109474. (<a
href="https://doi.org/10.1016/j.patcog.2023.109474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When deployed on a new domain different from the training set, deep learning often suffers from severe performance degradation . To combat domain shift, domain adaptation and domain generalization are proposed, where the former aims at transferring knowledge from related source domains to a known target domain, while the latter is more challenging by requiring the model to generalize to unknown target domains. This paper focuses on domain generalization and introduce a novel Fourier-based perspective for it. The main idea comes from the fact that Fourier amplitude component contains low-level statistics while phase component preserves high-level semantics. We thus propose a novel Fourier-based data augmentation strategy called AmpMix by linearly interpolating the amplitudes of two images while keeping their phases unchanged, to highlight the generalizable semantics contained in phase. To make full use of Fourier-augmented samples, we further incorporate consistency training between different augmentation views and devise a Fourier-based framework for three different domain generalization settings. Extensive experiments demonstrate the effectiveness of our Fourier-based method.},
  archive      = {J_PR},
  author       = {Qinwei Xu and Ruipeng Zhang and Ziqing Fan and Yanfeng Wang and Yi-Yan Wu and Ya Zhang},
  doi          = {10.1016/j.patcog.2023.109474},
  journal      = {Pattern Recognition},
  pages        = {109474},
  shortjournal = {Pattern Recognition},
  title        = {Fourier-based augmentation with applications to domain generalization},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible model weighting for one-dependence estimators based
on point-wise independence analysis. <em>PR</em>, <em>139</em>, 109473.
(<a href="https://doi.org/10.1016/j.patcog.2023.109473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that Bayesian network classifiers (BNCs) are powerful tools for knowledge representation and classification, and averaged one-dependence estimators (AODE) is one of the most popular and effective BNCs since it can achieve the tradeoff between bias and variance due to its independence assumptions and ensemble learning strategy. However, unverified independence assumptions may result in biased estimates of probability distribution and then degradation in classification performance. In this paper, we prove theoretically the uncertainty of probability-theoretic independence and propose to measure the independence between attribute values implicated in specific instance. The estimates of conditional probability can be finely tuned based on point-wise independence analysis. Point-wise log likelihood function is then applied as weighting metric for committee members of AODE to improve the estimate of joint probability . Extensive experiments on 36 benchmark datasets show that, compared to other state-of-the-art classifiers, weighted one-dependence estimators using point-wise independence analysis can achieve competitive classification performance in terms of zero-one loss, RMSE , bias-variance decomposition and conditional log likelihood.},
  archive      = {J_PR},
  author       = {He Kong and Limin Wang},
  doi          = {10.1016/j.patcog.2023.109473},
  journal      = {Pattern Recognition},
  pages        = {109473},
  shortjournal = {Pattern Recognition},
  title        = {Flexible model weighting for one-dependence estimators based on point-wise independence analysis},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Sparse feature selection via fast embedding spectral
analysis. <em>PR</em>, <em>139</em>, 109472. (<a
href="https://doi.org/10.1016/j.patcog.2023.109472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection has been a research hotspot in many fields. Models based on graph learning are currently the most popular approaches. However, the sparsity of most models is not strong, and graph learning for pair-sample evaluation takes a lot of time. ℓ 2 , 1 ℓ2,1 -norm regularization is the sparsity strategy adopted in most sparse models at present since the convex function is easy to solve. Nevertheless, the sparsity of ℓ 2 , 1 ℓ2,1 -norm is insufficient, and there exist parameter adjustment problems. ℓ 2 , 0 ℓ2,0 -norm is a better choice, which can strengthen the sparse constraints of the subspace. In this paper, the Sparse feature selection via Fast Embedding Spectral Analysis (SFESA) is proposed. Firstly, an adaptive anchor nearest neighbor graph is constructed to avoid the high time cost of learning pairwise nearest neighbor graphs to a certain extent. The low-dimensional embedding of data manifold structure is maintained by performing spectral analysis for the constructed graph. Secondly, the projected data is approximated to the low-dimensional embedding structure via a regularization term. Finally, ℓ 2 , 0 ℓ2,0 -norm is employed to constrain the projection matrix to enhance the subspace sparsity. Furthermore, a fast iterative algorithm is presented to solve this non-convex optimization problem . Extensive experiments on multiple public datasets show that SFESA can obtain excellent performance in less time.},
  archive      = {J_PR},
  author       = {Jingyu Wang and Hongmei Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.109472},
  journal      = {Pattern Recognition},
  pages        = {109472},
  shortjournal = {Pattern Recognition},
  title        = {Sparse feature selection via fast embedding spectral analysis},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental estimation of low-density separating hyperplanes
for clustering large data sets. <em>PR</em>, <em>139</em>, 109471. (<a
href="https://doi.org/10.1016/j.patcog.2023.109471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient unsupervised method for obtaining low-density hyperplane separators is proposed. The method is based on a modified stochastic gradient descent applied on a convolution of the empirical distribution function with a smoothing kernel. Low-density hyperplanes are motivated by the fact that they avoid intersecting high density regions, and so tend to pass between high density clusters, thus separating them from one another, while keeping the individual clusters intact. Multiple hyperplanes can be combined in a hierarchical model to obtain a complete clustering solution. A simple post-processing of solutions induced by large collections of hyperplanes yields an efficient and accurate clustering method, capable of automatically selecting the number of clusters. Experiments show that the proposed method is highly competitive in terms of both speed and accuracy when compared with relevant benchmarks. Code is available in the form of an R package at https://github.com/DavidHofmeyr/iMDH .},
  archive      = {J_PR},
  author       = {David P. Hofmeyr},
  doi          = {10.1016/j.patcog.2023.109471},
  journal      = {Pattern Recognition},
  pages        = {109471},
  shortjournal = {Pattern Recognition},
  title        = {Incremental estimation of low-density separating hyperplanes for clustering large data sets},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strongly augmented contrastive clustering. <em>PR</em>,
<em>139</em>, 109470. (<a
href="https://doi.org/10.1016/j.patcog.2023.109470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering has attracted increasing attention in recent years due to its capability of joint representation learning and clustering via deep neural networks . In its latest developments, the contrastive learning has emerged as an effective technique to substantially enhance the deep clustering performance. However, the existing contrastive learning based deep clustering algorithms mostly focus on some carefully-designed augmentations (often with limited transformations to preserve the structure), referred to as weak augmentations, but cannot go beyond the weak augmentations to explore the more opportunities in stronger augmentations (with more aggressive transformations or even severe distortions). In this paper, we present an end-to-end deep clustering approach termed S trongly A ugmented C ontrastive C lustering (SACC), which extends the conventional two-augmentation-view paradigm to multiple views and jointly leverages strong and weak augmentations for strengthened deep clustering. Particularly, we utilize a backbone network with triply-shared weights, where a strongly augmented view and two weakly augmented views are incorporated. Based on the representations produced by the backbone, the weak-weak view pair and the strong-weak view pairs are simultaneously exploited for the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector), which, together with the backbone, can be jointly optimized in a purely unsupervised manner . Experimental results on five challenging image datasets have shown the superiority of our SACC approach over the state-of-the-art. The code is available at https://github.com/dengxiaozhi/SACC .},
  archive      = {J_PR},
  author       = {Xiaozhi Deng and Dong Huang and Ding-Hua Chen and Chang-Dong Wang and Jian-Huang Lai},
  doi          = {10.1016/j.patcog.2023.109470},
  journal      = {Pattern Recognition},
  pages        = {109470},
  shortjournal = {Pattern Recognition},
  title        = {Strongly augmented contrastive clustering},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep embedded clustering with distribution consistency
preservation for attributed networks. <em>PR</em>, <em>139</em>, 109469.
(<a href="https://doi.org/10.1016/j.patcog.2023.109469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many complex systems in the real world can be characterized as attributed networks. To mine the potential information in these networks, deep embedded clustering, which obtains node representations and clusters simultaneously, has been given much attention in recent years. Under the assumption of consistency for data in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. However, many existing methods ignore this property, even though they separately encode node representations from network topology and node attributes and cluster nodes on representation vectors learned from one of the views. Therefore, in this study, we propose an end-to-end deep embedded clustering model for attributed networks. It utilizes graph autoencoder and node attribute autoencoder to learn node representations and cluster assignments. In addition, a distribution consistency constraint is introduced to maintain the latent consistency of cluster distributions in two views. Extensive experiments on several datasets demonstrate that the proposed model achieves significantly better or competitive performance compared with the state-of-the-art methods. The source code can be found at https://github.com/Zhengymm/DCP.},
  archive      = {J_PR},
  author       = {Yimei Zheng and Caiyan Jia and Jian Yu and Xuanya Li},
  doi          = {10.1016/j.patcog.2023.109469},
  journal      = {Pattern Recognition},
  pages        = {109469},
  shortjournal = {Pattern Recognition},
  title        = {Deep embedded clustering with distribution consistency preservation for attributed networks},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cycle optimization metric learning for few-shot
classification. <em>PR</em>, <em>139</em>, 109468. (<a
href="https://doi.org/10.1016/j.patcog.2023.109468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric learning methods are widely used in few-shot learning due to their simplicity and effectiveness. Most existing methods directly predict query labels by comparing the similarity between support and query samples. In this paper, we design a cycle optimization metric network for few-shot classification task that optimizes model performance based on loop-prediction of the labels of query samples and support samples. Specifically, we construct a forward network and reverse network based on a geometric algebra Graph Neural Network (GA-GNN). These two networks form the loop prediction from support samples to query samples and then back to support samples, guided by a cycle-consistency loss. We also introduce an optimization module that is able to correct the predicted results of query samples to further improve the network performance. Our extensive experimental results demonstrate that the proposed cycle optimization metric network outperforms existing state-of-the-art few-shot learning methods on classification tasks.},
  archive      = {J_PR},
  author       = {Qifan Liu and Wenming Cao and Zhihai He},
  doi          = {10.1016/j.patcog.2023.109468},
  journal      = {Pattern Recognition},
  pages        = {109468},
  shortjournal = {Pattern Recognition},
  title        = {Cycle optimization metric learning for few-shot classification},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An interpretable channelwise attention mechanism based on
asymmetric and skewed gaussian distribution. <em>PR</em>, <em>139</em>,
109467. (<a href="https://doi.org/10.1016/j.patcog.2023.109467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channelwise attention mechanisms have recently been demonstrated to boost the performance of deep convolutional neural networks (CNNs). The hypothesis on the negative correlation between channelwise responses and their importance levels has been verified. Therefore, according to the shifted means and unbalanced responses that are observed in attention distribution, two empirical hypotheses are proposed in this paper. Then, as an interpretable attention module, bilateral asymmetric skewed Gaussian attention (bi-SGA) is utilized to combine skewed and asymmetric properties into the Gaussian context transformer (GCT), which is the state-of-the-art. Finally, extensive experiments on the different benchmarks validate the rationality of the hypotheses. The proposed bi-SGA improves the overall performance of GCT attention mechanisms with only two extra parameters to be learned. Moreover, our attention mechanism also provides a perspective on analyzing the channelwise importance levels in deep neural networks in an interpretable and logical manner.},
  archive      = {J_PR},
  author       = {Cheng Chen and Bo Li},
  doi          = {10.1016/j.patcog.2023.109467},
  journal      = {Pattern Recognition},
  pages        = {109467},
  shortjournal = {Pattern Recognition},
  title        = {An interpretable channelwise attention mechanism based on asymmetric and skewed gaussian distribution},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial pan-sharpening attacks for object detection in
remote sensing. <em>PR</em>, <em>139</em>, 109466. (<a
href="https://doi.org/10.1016/j.patcog.2023.109466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pan-sharpening, as one of the most commonly used techniques in remote sensing systems, aims to fuse texture-rich PAN images and multi-spectral MS images to obtain texture-rich MS images . With the development of deep learning , CNN based Pan-sharpening methods have received more and more attention in recent years. Since Pan-sharpening technique can integrate the complementary information of Pan and MS images, researchers usually apply object detectors on these pan-sharpened images to achieve reliable detection results. However, recent studies have shown that Deep Learning-based object detection methods are vulnerable to adversarial examples , i.e., adding imperceptible noise to clean images can fool well-trained deep neural networks . It is interesting to combine the pan-sharpening technique with adversarial examples to attack object detectors in remote sensing. In this paper, we propose a framework to generate adversarial pan-sharpened images. Specifically, we propose a two-stream network to generate the pan-sharpened images, and then utilize the shape loss and label loss to perform the attack task. To guarantee the quality of pan-sharpened images, a perceptual loss is utilized to balance spectral preservation and attacking performance. Experimental results demonstrate that the proposed method can generate effective adversarial pan-sharpened images that maintain a high success rate for white-box attacks and achieve transferability for black-box attacks.},
  archive      = {J_PR},
  author       = {Xingxing Wei and Maoxun Yuan},
  doi          = {10.1016/j.patcog.2023.109466},
  journal      = {Pattern Recognition},
  pages        = {109466},
  shortjournal = {Pattern Recognition},
  title        = {Adversarial pan-sharpening attacks for object detection in remote sensing},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Corrigendum to “toward a blind image quality evaluator in
the wild by learning beyond human opinion scores” pattern recognition.
Volume 137 (2023) 109296. <em>PR</em>, <em>139</em>, 109465. (<a
href="https://doi.org/10.1016/j.patcog.2023.109465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Zhihua Wang and Zhi-Ri Tang and Jianguo Zhang and Yuming Fang},
  doi          = {10.1016/j.patcog.2023.109465},
  journal      = {Pattern Recognition},
  pages        = {109465},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to ‘Toward a blind image quality evaluator in the wild by learning beyond human opinion scores’ pattern recognition. volume 137 (2023) 109296},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperspectral subpixel target detection based on interaction
subspace model. <em>PR</em>, <em>139</em>, 109464. (<a
href="https://doi.org/10.1016/j.patcog.2023.109464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we examine the problem of detecting subpixel targets in hyperspectral images . The so-called subpixel target refers to a target that only occupies a part of a pixel due to the low spatial resolution of hyperspectral sensors . Considering that the subpixel target spectrum is not always reliable (e.g., due to spectral variability), an interaction subspace model is designed to deal with this problem. In this subspace model, the second-order interaction terms are introduced to better describe the spectral variability, thereby improving the robustness. Specifically, the subspace model uses a hyperplane in a high-dimensional space to model spectral variability, while traditional models (e.g., the additive model and the replacement model) use a line in the high-dimensional space to model spectral variability. Obviously, the stronger description ability of the hyperplane makes the subspace model more tolerant to the mismatch of the target spectrum. Based on this interaction subspace model, we derive adaptive detectors according to the one-step generalized likelihood ratio test and its two-step variant. Experiments conducted on hyperspectral data demonstrate that the proposed two-step detector exhibits the strongest robustness in cases where the target spectrum is not very reliable.},
  archive      = {J_PR},
  author       = {Shengyin Sun and Jun Liu and Siyu Sun},
  doi          = {10.1016/j.patcog.2023.109464},
  journal      = {Pattern Recognition},
  pages        = {109464},
  shortjournal = {Pattern Recognition},
  title        = {Hyperspectral subpixel target detection based on interaction subspace model},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian asymmetric quantized neural networks. <em>PR</em>,
<em>139</em>, 109463. (<a
href="https://doi.org/10.1016/j.patcog.2023.109463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a robust model compression for neural networks via parameter quantization. Traditionally, quantized neural networks (QNN) were constructed by binary or ternary weights where the weights were deterministic. This paper generalizes QNN in two directions. First, M -ary QNN is developed to adjust the balance between memory storage and model capacity. The representation values and the quantization partitions in M -ary quantization are mutually estimated to enhance the resolution of gradients in neural network training . A flexible quantization with asymmetric partitions is formulated. Second, the variational inference is incorporated to implement the Bayesian asymmetric QNN. The uncertainty of weights is faithfully represented to enhance the robustness of the trained model in presence of heterogeneous data . Importantly, the multiple spike-and-slab prior is proposed to represent the quantization levels in Bayesian asymmetric learning. M -ary quantization is then optimized by maximizing the evidence lower bound of classification network. An adaptive parameter space is built to implement Bayesian quantization and neural representation . The experiments on various image recognition tasks show that M -ary QNN achieves similar performance as the full-precision neural network (FPNN), but the memory cost and the test time are significantly reduced relative to FPNN. The merit of Bayesian M -ary QNN using multiple spike-and-slab prior is investigated.},
  archive      = {J_PR},
  author       = {Jen-Tzung Chien and Su-Ting Chang},
  doi          = {10.1016/j.patcog.2023.109463},
  journal      = {Pattern Recognition},
  pages        = {109463},
  shortjournal = {Pattern Recognition},
  title        = {Bayesian asymmetric quantized neural networks},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Deep collaborative graph hashing for discriminative image
retrieval. <em>PR</em>, <em>139</em>, 109462. (<a
href="https://doi.org/10.1016/j.patcog.2023.109462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most striking success of deep hashing for large-scale image retrieval benefits from its powerful discriminative representation of deep learning and the attractive computational efficiency of compact hash code learning. Most existing deep semantic-preserving hashing regard the available semantic labels as the ground truth for classification or transform them into prevalent pairwise similarities . However, such strategies fail to capture the interactive correlations between the visual semantics embedded in images and the given category-level labels. Moreover, they utilize the fixed piecewise or pairwise semantics as the optimization objectives , which suffers from the limited flexibility on semantic representation and adaptive knowledge communication in hash code learning. In this paper, we propose a novel Deep Collaborative Graph Hashing (DCGH), which collectively considers multi-level semantic embeddings, latent common space construction, and intrinsic structure mining in discriminative hash codes learning, for large-scale image retrieval . To the best of our knowledge, this is the first collaborative graph hashing for image retrieval. Specifically, instead of using the conventional single-flow visual network architecture , we design a dual-stream feature encoding network to jointly explore the multi-level semantic information across visual and semantic features . Moreover, a well-established shared latent space is constructed based on space reconstruction to explore the concurrent information and bridge the semantic gap between visual and semantic space. Furthermore, a graph convolutional network is introduced to preserve the latent structural relations in the optimal pairwise similarity-preserving hash codes. The whole learning framework is optimized in an end-to-end fashion. Extensive experiments on different datasets demonstrate that our DCGH can achieve superb image retrieval performance against state-of-the-art supervised hashing methods . The source codes of the proposed DCGH are available at https://github.com/JalinWang/DCGH .},
  archive      = {J_PR},
  author       = {Zheng Zhang and Jianning Wang and Lei Zhu and Yadan Luo and Guangming Lu},
  doi          = {10.1016/j.patcog.2023.109462},
  journal      = {Pattern Recognition},
  pages        = {109462},
  shortjournal = {Pattern Recognition},
  title        = {Deep collaborative graph hashing for discriminative image retrieval},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense extreme inception network for edge detection.
<em>PR</em>, <em>139</em>, 109461. (<a
href="https://doi.org/10.1016/j.patcog.2023.109461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection is the basis of many computer vision applications. State of the art predominantly relies on deep learning with two decisive factors: dataset content and network architecture . Most of the publicly available datasets are not curated for edge detection tasks. Here, we address this limitation. First, we argue that edges, contours and boundaries, despite their overlaps, are three distinct visual features requiring separate benchmark datasets. To this end, we present a new dataset of edges. Second, we propose a novel architecture, termed Dense Extreme Inception Network for Edge Detection (DexiNed), that can be trained from scratch without any pre-trained weights. DexiNed outperforms other algorithms in the presented dataset. It also generalizes well to other datasets without any fine-tuning. The higher quality of DexiNed is also perceptually evident thanks to the sharper and finer edges it outputs.},
  archive      = {J_PR},
  author       = {Xavier Soria and Angel Sappa and Patricio Humanante and Arash Akbarinia},
  doi          = {10.1016/j.patcog.2023.109461},
  journal      = {Pattern Recognition},
  pages        = {109461},
  shortjournal = {Pattern Recognition},
  title        = {Dense extreme inception network for edge detection},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Skeleton estimation of directed acyclic graphs using partial
least squares from correlated data. <em>PR</em>, <em>139</em>, 109460.
(<a href="https://doi.org/10.1016/j.patcog.2023.109460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graphs (DAGs) are directed graphical models that are well known for discovering causal relationships between variables in a high-dimensional setting. When the DAG is not identifiable due to the lack of interventional data, the skeleton can be estimated using observational data , which is formed by removing the direction of the edges in a DAG. In real data analyses , variables are often highly correlated due to some form of clustered sampling, and ignoring this correlation will inflate the standard errors of the parameter estimates in the regression-based DAG structure learning framework. In this work, we propose a two-stage DAG skeleton estimation approach for highly correlated data . First, we propose a novel neighborhood selection method based on sparse partial least squares (PLS) regression, and a cluster-weighted adaptive penalty is imposed on the PLS weight vectors to exploit the local information. In the second stage, the DAG skeleton is estimated by evaluating a set of conditional independence hypotheses. Simulation studies are presented to demonstrate the effectiveness of the proposed method. The algorithm is also tested on publicly available datasets, and we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data under different network structures.},
  archive      = {J_PR},
  author       = {Xiaokang Wang and Shan Lu and Rui Zhou and Huiwen Wang},
  doi          = {10.1016/j.patcog.2023.109460},
  journal      = {Pattern Recognition},
  pages        = {109460},
  shortjournal = {Pattern Recognition},
  title        = {Skeleton estimation of directed acyclic graphs using partial least squares from correlated data},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional invertible image re-scaling. <em>PR</em>,
<em>139</em>, 109459. (<a
href="https://doi.org/10.1016/j.patcog.2023.109459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key issue for image re-scaling is modeling the down-sampling loss. The loss can be approached by a prior distribution through an invertible network in recent works. However, this assumption of independence between images and down-sampling losses is not always satisfied in practice. To address the above problem, we design a module to learn a latent variable representing the down-scaling loss conditional on low-resolution(LR) images according to the information entropy theory. Unlike previous methods, the down-scaling loss is recovered through the proposed module whose inputs are both LR images and predefined distributions. The difference between the high-resolution(HR) image and the low-resolution(LR) image is represented with a latent variable through a lossless invertible neural network. In addition, a conditional entropy loss is proposed to train the invertible neural network to suppress the conditional entropy between LR images and latent variables. It helps to decrease the effect of the latent variable to generate the HR image from LR images during the up-scaling procedure. We evaluate the proposed method on widely used data sets, and the experimental results demonstrate that our proposed method performs favorably against SOTA methods in terms of PSNR and SSIM metrics.},
  archive      = {J_PR},
  author       = {Yufei Zha and Fan Li and Peng Zhang and Wei Huang},
  doi          = {10.1016/j.patcog.2023.109459},
  journal      = {Pattern Recognition},
  pages        = {109459},
  shortjournal = {Pattern Recognition},
  title        = {Conditional invertible image re-scaling},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Where you edit is what you get: Text-guided image editing
with region-based attention. <em>PR</em>, <em>139</em>, 109458. (<a
href="https://doi.org/10.1016/j.patcog.2023.109458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging the abundant knowledge learned from pre-trained multi-modal models like CLIP has recently proved to be effective for text-guided image editing. Though convincing results have been made when combining the image generator StyleGAN with CLIP, most methods need to train separate models for different prompts, and irrelevant regions are often changed after editing due to the lack of spatial disentanglement. We propose a novel framework that can edit different images according to different prompts in one model. Besides, an innovative region-based spatial attention mechanism is adopted to explicitly guarantee the locality of editing. Experiments mainly in the face domain verify the feasibility of our framework and show that when multi-text editing and local editing are accomplishable, our method can complete practical applications like sequential editing and regional style transfer.},
  archive      = {J_PR},
  author       = {Changming Xiao and Qi Yang and Xiaoqiang Xu and Jianwei Zhang and Feng Zhou and Changshui Zhang},
  doi          = {10.1016/j.patcog.2023.109458},
  journal      = {Pattern Recognition},
  pages        = {109458},
  shortjournal = {Pattern Recognition},
  title        = {Where you edit is what you get: Text-guided image editing with region-based attention},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive spatial-temporal surrounding-aware correlation
filter tracking via ensemble learning. <em>PR</em>, <em>139</em>,
109457. (<a href="https://doi.org/10.1016/j.patcog.2023.109457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of computer vision , object trackers based on discriminative correlation filters (DCF) have demonstrated superior performance and accuracy compared to traditional trackers. However, most existing DCF-based trackers are easily affected by various factors, such as cluttered background, illumination variations , occlusions, rotations etc. Therefore, in order to accurately track the target, further investigation into the characteristics of the correlation filter is required. In this study, we propose an adaptive spatial-temporal surrounding-aware correlation filter tracker via the ensemble learning (ASTSAELT) technique. Specifically, the adaptive spatial-temporal regularized correlation filter to remove the boundary effects and temporal degradation is presented. And then, a method of extracting surrounding samples according to the size and shape of the tracking object, designed to preserve the integrity of the object, is proposed. Moreover, our tracker utilizes a multi-expert tracking framework to improve its performance by integrating both handcrafted features and deep convolutional layer features. And then, the update strategy is proposed to measure the reliability of the current tracking result and mitigate model corruption. Finally, numerous experiments on visual tracking benchmarks including OTB2013, OTB2015, TempleColor128, UAV123, UAVDT and DTB70 are implemented to verify the developed method achieves superior performance compared with several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Sathishkumar Moorthy and Young Hoon Joo},
  doi          = {10.1016/j.patcog.2023.109457},
  journal      = {Pattern Recognition},
  pages        = {109457},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive spatial-temporal surrounding-aware correlation filter tracking via ensemble learning},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy granular recurrence plot and quantification analysis:
A novel method for classification. <em>PR</em>, <em>139</em>, 109456.
(<a href="https://doi.org/10.1016/j.patcog.2023.109456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, recurrence plot (RP) and its quantification techniques have become an important research tool in nonlinear analysis . In the existing researches, an RP is directly established on a time series ignoring the influence of noise on data, which will affect our judgement on the dynamic properties of a system. To tackle the problem there, this paper proposes a novel recurrence plot, namely fuzzy granular recurrence plot (FGRP). An FGRP of a time series is built not directly on the time series itself but on its corresponding granular time series which is composed of fuzzy information granules. With specific capability, fuzzy information granules are used as building blocks of an FGRP to achieve high-level, compact and understandable signal models. In order to apply the FGRP method to time series classification tasks , an FGRP based classification model is designed in this paper. Subsequent experiments show that the FGRP of a time series can reduce the effect of noise, and the FGRP based classification model can improve the classification performance.},
  archive      = {J_PR},
  author       = {Qian He and Fusheng Yu and Jiaqi Chang and Chenxi Ouyang},
  doi          = {10.1016/j.patcog.2023.109456},
  journal      = {Pattern Recognition},
  pages        = {109456},
  shortjournal = {Pattern Recognition},
  title        = {Fuzzy granular recurrence plot and quantification analysis: A novel method for classification},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relation-mining self-attention network for skeleton-based
human action recognition. <em>PR</em>, <em>139</em>, 109455. (<a
href="https://doi.org/10.1016/j.patcog.2023.109455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling spatiotemporal global dependencies and dynamics of body joints are crucial to recognizing actions from 3D skeleton sequences. We propose a Relation-mining Self-Attention Network (RSA-Net) for skeleton-based human action recognition . The proposed RSA-Net is motivated by two important observations: (1) body joint relationships can be modeled independently as pairwise and unary to reduce the difficulty of action feature learning . (2) Computing action semantics and position information independently removes noisy correlations over heterogeneous embedding. The proposed RSA-Net contains pairwise self-attention, unary self-attention, and position embedding attention modules. The pairwise self-attention captures the relationship between every two body joints. The unary self-attention learns a general correlation features among one key joint over all other query joints. The position embedding attention module computes the correlation between action semantics and position information independently with separate projection matrices . Extensive evaluations are performed in the NTU-60, NTU-120, and UESTC datasets with CS, CV, CSet, and A-view evaluation benchmarks . The proposed RSA-Net outperforms existing transformer-based approaches and comparable results with state-of-the-art graph ConvNet methods. The source code is available in Github 1 .},
  archive      = {J_PR},
  author       = {Kumie Gedamu and Yanli Ji and LingLing Gao and Yang Yang and Heng Tao Shen},
  doi          = {10.1016/j.patcog.2023.109455},
  journal      = {Pattern Recognition},
  pages        = {109455},
  shortjournal = {Pattern Recognition},
  title        = {Relation-mining self-attention network for skeleton-based human action recognition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). K-sets and k-swaps algorithms for clustering sets.
<em>PR</em>, <em>139</em>, 109454. (<a
href="https://doi.org/10.1016/j.patcog.2023.109454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present two new clustering algorithms called k-sets and k-swaps for data where each object is a set. First, we define the mean of the sets in a cluster, and the distance between a set and the mean. We then derive the k-sets algorithm from the principles of classical k -means so that it repeats the assignment and update steps until convergence. To the best of our knowledge, the proposed algorithm is the first k -means based algorithm for this kind of data. We adopt the idea also into random swap algorithm, which is a wrapper around the k -means that avoids local minima. This variant is called k-swaps . We show by experiments that this algorithm provides more accurate clustering results than k-medoids and other competitive methods.},
  archive      = {J_PR},
  author       = {Mohammad Rezaei and Pasi Fränti},
  doi          = {10.1016/j.patcog.2023.109454},
  journal      = {Pattern Recognition},
  pages        = {109454},
  shortjournal = {Pattern Recognition},
  title        = {K-sets and k-swaps algorithms for clustering sets},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of human-computer interaction (HCI) &amp; natural
habits-based behavioural biometric modalities for user recognition
schemes. <em>PR</em>, <em>139</em>, 109453. (<a
href="https://doi.org/10.1016/j.patcog.2023.109453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) systems is having a profound impact across all aspects of life. Recognising and identifying particular users is central to delivering the personalised experience that citizens want to experience, and that organisations wish to deliver. This article presents a survey of human-computer interaction-based (HCI-based) and natural habits-based behavioural biometrics that can be acquired unobtrusively through smart devices or IoT sensors for user recognition purposes. Robust and usable user recognition is also a security requirement for emerging IoT ecosystems to protect them from adversaries. Typically, it can be specified as a fundamental building block for most types of human-to-things accountability principles and access-control methods. However, end-users are facing numerous security and usability challenges in using currently available knowledge- and token-based recognition ( i.e., authentication and identification ) schemes. To address the limitations of conventional recognition schemes, biometrics , naturally come as a first choice to supporting sophisticated user recognition solutions. We perform a comprehensive review of touch-stroke, swipe, touch signature, hand-movements, voice, gait and footstep behavioural biometrics modalities . This survey analyzes the recent state-of-the-art research of these behavioural biometrics with a goal to identify their attributes and features for generating unique identification signatures. Finally, we present security, privacy, and usability evaluations that can strengthen the designing of robust and usable user recognition schemes for IoT applications.},
  archive      = {J_PR},
  author       = {Sandeep Gupta and Carsten Maple and Bruno Crispo and Kiran Raja and Artsiom Yautsiukhin and Fabio Martinelli},
  doi          = {10.1016/j.patcog.2023.109453},
  journal      = {Pattern Recognition},
  pages        = {109453},
  shortjournal = {Pattern Recognition},
  title        = {A survey of human-computer interaction (HCI) &amp; natural habits-based behavioural biometric modalities for user recognition schemes},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic-based conditional generative adversarial hashing
with pairwise labels. <em>PR</em>, <em>139</em>, 109452. (<a
href="https://doi.org/10.1016/j.patcog.2023.109452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has been widely exploited in recent years due to the rapid growth of image and video data on the web. Benefiting from recent advances in deep learning , deep hashing methods have achieved promising results with supervised information . However, it is usually expensive to collect the supervised information . In order to utilize both labeled and unlabeled data samples, many semi-supervised hashing methods based on Generative Adversarial Networks (GANs) have been proposed. Most of them still need the conditional information, which is usually generated by the pre-trained neural networks or leveraging random binary vectors. One natural question about these methods is that how can we generate a better conditional information given the semantic similarity information? In this paper, we propose a general two-stage conditional GANs hashing framework based on the pairwise label information. Both the labeled and unlabeled data samples are exploited to learn hash codes under our framework. In the first stage, the conditional information is generated via a general Bayesian approach , which has a much lower dimensional representation and maintains the semantic information of original data samples. In the second stage, a semi-supervised approach is presented to learn hash codes based on the conditional information. Both pairwise based cross entropy loss and adversarial loss are introduced to make full use of labeled and unlabeled data samples. Extensive experiments have shown that the propose algorithm outperforms current state-of-the-art methods on three benchmark image datasets, which demonstrates the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Qi Li and Weining Wang and Yuanyan Tang and Chengzhong Xu and Zhenan Sun},
  doi          = {10.1016/j.patcog.2023.109452},
  journal      = {Pattern Recognition},
  pages        = {109452},
  shortjournal = {Pattern Recognition},
  title        = {Semantic-based conditional generative adversarial hashing with pairwise labels},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). YOGA: Deep object detection in the wild with lightweight
feature learning and multiscale attention. <em>PR</em>, <em>139</em>,
109451. (<a href="https://doi.org/10.1016/j.patcog.2023.109451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce YOGA, a deep learning based yet lightweight object detection model that can operate on low-end edge devices while still achieving competitive accuracy. The YOGA architecture consists of a two-phase feature learning pipeline with a cheap linear transformation , which learns feature maps using only half of the convolution filters required by conventional convolutional neural networks . In addition, it performs multi-scale feature fusion in its neck using an attention mechanism instead of the naive concatenation used by conventional detectors. YOGA is a flexible model that can be easily scaled up or down by several orders of magnitude to fit a broad range of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets with over 10 state-of-the-art object detectors. The results show that YOGA strikes the best trade-off between model size and accuracy (up to 22\% increase of AP and 23–34\% reduction of parameters and FLOPs), making it an ideal choice for deployment in the wild on low-end edge devices. This is further affirmed by our hardware implementation and evaluation on NVIDIA Jetson Nano.},
  archive      = {J_PR},
  author       = {Raja Sunkara and Tie Luo},
  doi          = {10.1016/j.patcog.2023.109451},
  journal      = {Pattern Recognition},
  pages        = {109451},
  shortjournal = {Pattern Recognition},
  title        = {YOGA: Deep object detection in the wild with lightweight feature learning and multiscale attention},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative subspace learning via optimization on
riemannian manifold. <em>PR</em>, <em>139</em>, 109450. (<a
href="https://doi.org/10.1016/j.patcog.2023.109450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discriminative subspace learning is an important problem in machine learning , which aims to find the maximum separable decision subspace. Traditional Euclidean-based methods usually use Fisher discriminant criterion for finding an optimal linear mapping from a high-dimensional data space to a lower-dimensional subspace, which hardly guarantee a quadratic rate of global convergence and suffers from the singularity problem. Here, we propose the manifold optimization-based discriminant analysis (MODA) which is constructed by using the latent subspace alignment and the geometry of objective function with orthogonality constraint. MODA is solved by using Riemannian version of trust-region algorithm. Experimental results on various image datasets and electroencephalogram (EEG) datasets show that MODA achieves the best separability and is significantly superior to the competing algorithms. Especially for the time series of EEG signals, the accuracy of MODA is 20–30\% higher than existing algorithms. The code for MODA is available at https://github.com/ncclabsustech/MODA-algorithm .},
  archive      = {J_PR},
  author       = {Wanguang Yin and Zhengming Ma and Quanying Liu},
  doi          = {10.1016/j.patcog.2023.109450},
  journal      = {Pattern Recognition},
  pages        = {109450},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative subspace learning via optimization on riemannian manifold},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ECM-EFS: An ensemble feature selection based on enhanced
co-association matrix. <em>PR</em>, <em>139</em>, 109449. (<a
href="https://doi.org/10.1016/j.patcog.2023.109449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, feature selection faces a huge challenge that no single feature selection method can effectively deal with various data sets for all real cases. Ensemble learning is a potential promising solution to address this problem. We propose an ensemble feature selection method based on enhanced co-association matrix (ECM-EFS). Positive-co-association matrix (PCM), negative-co-association matrix (NCM), and relative-co-association matrix (RCM) are first introduced to discover the relationship among features by ensembling the results in multiple feature selection methods. To further produce a more stable feature selection result, “Feature Kernel” is also introduced and used as a starting point for feature selection. Comparative experiments with four state-of-the-art methods have confirmed that the ECM-EFS can provide more robust results. Moreover, compared with traditional ensemble feature selection methods, our method can compensate information loss and reduce computational cost significantly.},
  archive      = {J_PR},
  author       = {Ting Wu and Yihang Hao and Bo Yang and Lizhi Peng},
  doi          = {10.1016/j.patcog.2023.109449},
  journal      = {Pattern Recognition},
  pages        = {109449},
  shortjournal = {Pattern Recognition},
  title        = {ECM-EFS: An ensemble feature selection based on enhanced co-association matrix},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-channel graph contrastive learning for self-supervised
graph-level representation learning. <em>PR</em>, <em>139</em>, 109448.
(<a href="https://doi.org/10.1016/j.patcog.2023.109448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised graph-level representation learning aims to learn discriminative representations for subgraphs or entire graphs without human-curated labels. Recently, graph contrastive learning (GCL) methods have revolutionized this field and achieved state-of-the-art results in various downstream tasks. Nonetheless, current GCL models are mostly based on simple node-level information aggregation operations and fail to reveal various substructures from input graphs. Moreover, to perform graph-graph contrastive training, they often involve well-designed graph augmentation, which is expensive and requires extensive expert efforts. Here, we propose a novel GCL framework, namely DualGCL, for self-supervised graph-level representation learning . For fine-grained local information incorporation, we first present an adaptive hierarchical aggregation process with a differentiable Transformer-based aggregator . Then, to efficiently learn graph-level discriminative representations, we introduce a dual-channel contrastive learning process in a multi-granularity and augmentation-free contrasting mode. When tested empirically on six popular graph classification benchmarks, our DualGCL achieves better or comparable performance than various strong baselines.},
  archive      = {J_PR},
  author       = {Zhenfei Luo and Yixiang Dong and Qinghua Zheng and Huan Liu and Minnan Luo},
  doi          = {10.1016/j.patcog.2023.109448},
  journal      = {Pattern Recognition},
  pages        = {109448},
  shortjournal = {Pattern Recognition},
  title        = {Dual-channel graph contrastive learning for self-supervised graph-level representation learning},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MEQA: Manifold embedding quality assessment via anisotropic
scaling and kolmogorov-smirnov test. <em>PR</em>, <em>139</em>, 109447.
(<a href="https://doi.org/10.1016/j.patcog.2023.109447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manifold learning methods unfold the manifold structures and embed them in a lower-dimensional space. The quality of such an embedding should be measured both qualitatively and quantitatively. The proposed manifold embedding quality assessment (MEQA) method does so by taking into account of local and global structure preservation as both are important traits of an embedding. To measure the local structure preservation MEQA uses two transformations. Initially anisotropic scaling, rotation and translation are incorporated to measure the closeness between the original and the embedded data points. In the next stage, rigid transformation is incorporated to quantify the previous transformation which involved anisotropic scaling. For quantifying the global structure preservation, the Kolmogorov-Smirnov test is applied in a distributed manner over each dimension and averaged over them. To establish the superiority of MEQA we conducted several studies over standard synthetic and real-life datasets across separate existing feature extraction techniques.},
  archive      = {J_PR},
  author       = {Subhadip Boral and Mainak Sarkar and Ashish Ghosh},
  doi          = {10.1016/j.patcog.2023.109447},
  journal      = {Pattern Recognition},
  pages        = {109447},
  shortjournal = {Pattern Recognition},
  title        = {MEQA: Manifold embedding quality assessment via anisotropic scaling and kolmogorov-smirnov test},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bi-level metric learning framework via self-paced learning
weighting. <em>PR</em>, <em>139</em>, 109446. (<a
href="https://doi.org/10.1016/j.patcog.2023.109446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance metric learning (DML) has achieved great success in many real-world applications. However, most existing DML models characterize the quality of tuples on the tuple level while ignoring the anchor level. Therefore, the models are less accurate to portray the quality of tuples and tend to be over-fitting when anchors are noisy samples. In this paper, we devise a bi-level metric learning framework (BMLF), which characterizes the quality of tuples more finely on both levels, enhancing the generalization performance of the DML model. Furthermore, we present an implementation of BMLF based on a self-paced learning regular term and design the corresponding optimization algorithm . By weighing tuples on the anchor level and training the model using tuples with higher weights preferentially, the side effect of low-quality noisy samples will be alleviated. We empirically demonstrate that the effectiveness and robustness of the proposed method outperform the state-of-the-art methods on several benchmark datasets.},
  archive      = {J_PR},
  author       = {Jing Yan and Wei Wei and Xinyao Guo and Chuangyin Dang and Jiye Liang},
  doi          = {10.1016/j.patcog.2023.109446},
  journal      = {Pattern Recognition},
  pages        = {109446},
  shortjournal = {Pattern Recognition},
  title        = {A bi-level metric learning framework via self-paced learning weighting},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gradient optimization and manifold preserving based binary
neural network for point cloud. <em>PR</em>, <em>139</em>, 109445. (<a
href="https://doi.org/10.1016/j.patcog.2023.109445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With significant progress of deep learning on 3D point cloud, the demand for deployment of point cloud neural network on the edge devices is growing. Binary neural network, a type of quantization compression method, with extreme low bit and fast inference speed, attracts more attention. It is more challenging, but has greater potentiality. Most of the researches on binary networks focus on images rather than point cloud. Considering the particularity of point cloud neural network, this paper presents a novel binarization framework, which includes two main contributions. Firstly, a gradient optimization method is proposed to overcome the shortcomings of Straight Through Estimator (STE) commonly used in the back propagation of binary network training. Secondly, based on the analysis of manifold distortion caused by the binary convolution and pooling operations, we propose an optimized scaling recovery method to restore manifold for the convoluted feature, and also, a pooling correction method to improve the pooled feature&#39;s fidelity. Manifold distortion leads to the severe feature homogeneity problem, which brings trouble in generating features with sufficient discrimination for classification and segmentation. The manifold preserving optimizations are designed to introduce minimum extra parameters to balance the accuracy with the computation and storage consumption. Experiments show that the proposed method outperforms state-of-the-art in accuracy with ignored overhead, and also has good scalability.},
  archive      = {J_PR},
  author       = {Zhi Zhao and Ke Xu and Yanxin Ma and Jianwei Wan},
  doi          = {10.1016/j.patcog.2023.109445},
  journal      = {Pattern Recognition},
  pages        = {109445},
  shortjournal = {Pattern Recognition},
  title        = {A gradient optimization and manifold preserving based binary neural network for point cloud},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Segmentation of 3D point cloud data representing full human
body geometry: A review. <em>PR</em>, <em>139</em>, 109444. (<a
href="https://doi.org/10.1016/j.patcog.2023.109444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to present a review of the segmentation techniques of the 3D data representing human body in the form of point clouds. The techniques discussed are divided into three subgroups: 2D contour approaches, topological techniques, and machine learning heuristics. These subgroups are then reviewed regarding the following aspects: computation time, accuracy, reliability, dependency on human pose, and segment count. The authors emphasize an analysis of these algorithms with respect to their exploitation in the segmentation of 3D data varying in time, as well as further improvement of the applications in anthropometry. The conclusion reached is that machine learning approach tends to be the most suitable solution for future 4D applications. Another foreseeable direction of development in the field of segmentation algorithms is the classification of the points on the borders between segments and maintaining fluent and consistent edges of the segments between the subsequent frames.},
  archive      = {J_PR},
  author       = {Damian Krawczyk and Robert Sitnik},
  doi          = {10.1016/j.patcog.2023.109444},
  journal      = {Pattern Recognition},
  pages        = {109444},
  shortjournal = {Pattern Recognition},
  title        = {Segmentation of 3D point cloud data representing full human body geometry: A review},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid token transformer for deep face recognition.
<em>PR</em>, <em>139</em>, 109443. (<a
href="https://doi.org/10.1016/j.patcog.2023.109443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolutional Neural Networks have achieved remarkable successes in face recognition, they still suffer a critical limitation on capturing long range relations among facial regions. The recent vision transformers can naturally alleviate this problem, by learning global token dependencies. However, They are insufficient to discover high-level facial semantics since tokens in these transformers are based on small and fixed regions. To tackle such difficulty, we propose a novel Hybrid tOken Transformer (HOTformer) module to identify key facial semantics for effective recognition with cooperation of atomic and holistic tokens. Specifically, atomic tokens are generated from small fixed-size regions that can learn fine-grained core representation. Alternatively, holistic tokens are constructed from big adaptively-learned regions that can capture coarse-grained contextual representation. Furthermore, our HOTformer is a plug-and-play module. By hierarchically inserting it into convolutional networks, we can build a concise HOTformer-Net that achieves a preferable computation like CNN while boosting accuracy like transformer.},
  archive      = {J_PR},
  author       = {Weicong Su and Yali Wang and Kunchang Li and Peng Gao and Yu Qiao},
  doi          = {10.1016/j.patcog.2023.109443},
  journal      = {Pattern Recognition},
  pages        = {109443},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid token transformer for deep face recognition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic dual graph networks for textbook question answering.
<em>PR</em>, <em>139</em>, 109441. (<a
href="https://doi.org/10.1016/j.patcog.2023.109441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textbook Question Answering (TQA) is a complex task oriented to multi-modal context, which requires reasoning on a diagram and a long essay to get the correct answer. There are mainly two related issues for the task. First, diagrams are mostly abstract expressions of real world and some constituents with similar appearance may have different semantics, which makes it difficult to understand them effectively. Secondly, a long essay contains abundant and useful information for question answering, which shows that it is vital to extract the relevant information from the abundant text and then perform reasoning on it. To address the two issues, we propose a new model, Dynamic Dual Graph Networks (DDGNet), which performs question-guided multi-step reasoning on the dynamic directed Diagram Graph Network (DGN) for the diagram and Textual Graph Network (TGN) for the most related paragraph extracted from a long essay. Specifically, DGN combines text features with positional features of text boxes in the diagram as the node feature to avoid the ambiguity of visual features for the abstract constituents and help express explicit semantics. TGN uses the representation of each sentence in the most related paragraph as the node feature to learn the contextualized interaction of the useful information in the graph reasoning process. For the reasoning strategy, we propose a question-guided multi-step graph reasoning method to update both DGN and TGN dynamically under the question guidance in every step. Experimental results show that our proposed model outperforms the baselines on the TQA dataset. Moreover, extensive ablation studies are also conducted to analyze the effectiveness of our proposed model.},
  archive      = {J_PR},
  author       = {Yaxian Wang and Jun Liu and Jie Ma and Hongwei Zeng and Lingling Zhang and Junjun Li},
  doi          = {10.1016/j.patcog.2023.109441},
  journal      = {Pattern Recognition},
  pages        = {109441},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic dual graph networks for textbook question answering},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stage stacked temporal convolution neural networks
(MS-s-TCNs) for biosignal segmentation and anomaly localization.
<em>PR</em>, <em>139</em>, 109440. (<a
href="https://doi.org/10.1016/j.patcog.2023.109440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the computer vision domain, temporal convolution networks (TCN) have gained traction due to their lightweight, robust architectures for sequence-to-sequence prediction tasks. With that insight, in this study, we propose a novel deep learning architecture for biosignal segmentation and anomaly localization based on TCNs, named the multi-stage stacked TCN, which employs multiple TCN modules with varying dilation factors. More precisely, for each stage, our architecture uses TCN modules with multiple dilation factors, and we use convolution-based fusion to combine predictions returned from each stage. Furthermore, aiming smoothed predictions, we introduce a novel loss function based on the first-order derivative. To demonstrate the robustness of our architecture, we evaluate our model on five different tasks related to three 1D biosignal modalities (heart sounds, lung sounds and electrocardiogram). Our proposed framework achieves state-of-the-art performance for all tasks, significantly outperforming the respective state-of-the-art models having F1 score gains up to ≈ 9 ≈9\%. Furthermore, the framework demonstrates competitive performance gains compared to traditional multi-stage TCN models with similar configurations yielding F1 score gains up to ≈ 5 ≈5\%. Our model is also interpretable. Using neural conductance, we demonstrate the effectiveness of having TCNs with varying dilation factors. Our visualizations show that the model benefits from feature maps captured at multiple dilation factors, and the information is effectively propagated through the network such that the final stage produces the most accurate result.},
  archive      = {J_PR},
  author       = {Theekshana Dissanayake and Tharindu Fernando and Simon Denman and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.patcog.2023.109440},
  journal      = {Pattern Recognition},
  pages        = {109440},
  shortjournal = {Pattern Recognition},
  title        = {Multi-stage stacked temporal convolution neural networks (MS-S-TCNs) for biosignal segmentation and anomaly localization},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel-based feature aggregation framework in point cloud
networks. <em>PR</em>, <em>139</em>, 109439. (<a
href="https://doi.org/10.1016/j.patcog.2023.109439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various effective deep networks have been developed for analysis of 3D point clouds. One key step in these networks is to aggregate the features of orderless points into a compact representation for the cloud. As a typical order-invariant aggregation method, max-pooling has been widely applied. However, while enjoying simplicity and high efficiency, max-pooling does not fully exploit the feature information since it not only ignores the non-maximum elements in each feature dimension but also neglects the interactions between different dimensions. These drawbacks of max-pooling motivate us to explore advanced feature aggregation methods for 3D point cloud analysis . The desired advanced method should be capable of modeling richer information between the point features than max-pooling, and, at the same time, it can readily replace max-pooling without the need to modify other parts of the existing network architecture . To this end, this paper proposes a novel kernel-based feature aggregation framework for 3D point cloud analysis for the first time. The proposed method effectively considers all the elements in each dimension and models the nonlinear interactions between feature dimensions as complementary information to max-pooling. In addition, it is a plug-in module that can be integrated to many common networks as a replacement of max-pooling. Comprehensive experiments are conducted to demonstrate the consistently superior performance and high generality of the proposed method over max-pooling. Specifically, the proposed kernel-based feature aggregation framework consistently improves the max-pooling with three representative backbones of PointNet, DGCNN and PCT across four 3D point cloud based analysis tasks, including supervised 3D object classification, 3D part segmentation, indoor semantic segmentation and one additional unsupervised place retrieval task. Especially, it shows remarkable performance improvement over max-pooling in the unsupervised retrieval task, demonstrating its advantage in forming 3D point cloud representation.},
  archive      = {J_PR},
  author       = {Jianjia Zhang and Zhenxi Zhang and Lei Wang and Luping Zhou and Xiaocai Zhang and Mengting Liu and Weiwen Wu},
  doi          = {10.1016/j.patcog.2023.109439},
  journal      = {Pattern Recognition},
  pages        = {109439},
  shortjournal = {Pattern Recognition},
  title        = {Kernel-based feature aggregation framework in point cloud networks},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Attention reweighted sparse subspace clustering.
<em>PR</em>, <em>139</em>, 109438. (<a
href="https://doi.org/10.1016/j.patcog.2023.109438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering has attracted much attention in many applications of computer vision and pattern recognition. Spectral clustering based methods, such as sparse subspace clustering (SSC) and low-rank representation (LRR), have become popular due to their theoretical guarantees and impressive performance. However, many state-of-the-art subspace clustering methods specify the mean square error (MSE) criterion as the loss function, which is sensitive to outliers and complex noises in reality. These methods have poor performance when the data are corrupted by complex noise. In this paper, we propose a robust sparse subspace clustering method, termed Attention Reweighted SSC (ARSSC), by paying less attention to the corrupted entries (adaptively assigning small weights to the corrupted entries in each data point). To reduce the extra bias in estimation introduced by ℓ 1 ℓ1 regularization , we also utilize non-convex penalties to overcome the overpenalized problem. In addition, we provide theoretical guarantees for ARSSC and theoretically show that our method gives a subspace-preserving affinity matrix under appropriate conditions. To solve the ARSSC optimization problem , we devise an optimization algorithm using an Alternating Direction Method of Multipliers (ADMM) method. Experiments on real-world databases validate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Libin Wang and Yulong Wang and Hao Deng and Hong Chen},
  doi          = {10.1016/j.patcog.2023.109438},
  journal      = {Pattern Recognition},
  pages        = {109438},
  shortjournal = {Pattern Recognition},
  title        = {Attention reweighted sparse subspace clustering},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the diversity and invariance in yourself for
visual pre-training task. <em>PR</em>, <em>139</em>, 109437. (<a
href="https://doi.org/10.1016/j.patcog.2023.109437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, self-supervised learning methods have achieved remarkable success in the visual pre-training task. By simply pulling the different augmented views of each image together or other novel mechanisms, they can learn much unsupervised knowledge and significantly improve the transfer performance of pre-training models. However, these works still cannot avoid the representation collapse problem, i.e. , they only focus on limited regions or the extracted features on totally different regions inside each image are nearly the same. Generally, this problem makes the pre-training models cannot sufficiently describe the multi-grained information inside images, which further limits the upper bound of their transfer performance. To alleviate this issue, this paper introduces a simple but effective mechanism, called E xploring the D iversity and I nvariance in Y ourself (E-DIY). By simply pushing the most different regions inside each augmented view away, E-DIY can preserve the diversity of extracted region-level features. By pulling the most similar regions from different augmented views of the same image together, E-DIY can ensure the robustness of extracted region-level features. Benefiting from the above diversity and invariance exploring mechanism, E-DIY better extracts the multi-grained visual information inside each image compared with previous self-supervised learning approaches. Extensive experiments on various downstream tasks have demonstrated the superiority of our method, e.g. , there is 1.9\% 1.9\% improvement (compared with the recent state-of-the-art method, BYOL) on A P 50 AP50 metric of detection task on VOC.},
  archive      = {J_PR},
  author       = {Longhui Wei and Lingxi Xie and Wengang Zhou and Houqiang Li and Qi Tian},
  doi          = {10.1016/j.patcog.2023.109437},
  journal      = {Pattern Recognition},
  pages        = {109437},
  shortjournal = {Pattern Recognition},
  title        = {Exploring the diversity and invariance in yourself for visual pre-training task},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent dueling q-learning with mean field and value
decomposition. <em>PR</em>, <em>139</em>, 109436. (<a
href="https://doi.org/10.1016/j.patcog.2023.109436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A great deal of multi agent reinforcement learning(MARL) work has investigated how multiple agents effectively accomplish cooperative tasks utilizing value function decomposition methods. However, existing value decomposition methods can only handle cooperative tasks with shared reward, due to these methods factorize the value function from a global perspective. To tackle the competitive tasks and mixed cooperative-competitive tasks with differing individual reward setting, we design the Multi-agent Dueling Q-learning (MDQ) method based on mean-filed theory and individual value decomposition. Specifically, we integrate the mean-field theory with the value decomposition to factorize the value function at the individual level, which can deal with mixed cooperative-competitive tasks. Besides, we take a dueling network architecture to distinguish which states are valuable, eliminating the need to learn the impact of each action on each state, therefore enabling efficient learning and leading to better policy evaluation. The proposed method MDQ is applicable not only to cooperative tasks with shared rewards setting, but also to mixed cooperative-competitive tasks with individualized reward settings. In this end, it is flexible and generically applicable enough to most multi-agent tasks. Empirical experiments on various mixed cooperative-competitive tasks demonstrate that MDQ significantly outperforms existing multi agent reinforcement learning methods.},
  archive      = {J_PR},
  author       = {Shifei Ding and Wei Du and Ling Ding and Lili Guo and Jian Zhang and Bo An},
  doi          = {10.1016/j.patcog.2023.109436},
  journal      = {Pattern Recognition},
  pages        = {109436},
  shortjournal = {Pattern Recognition},
  title        = {Multi-agent dueling Q-learning with mean field and value decomposition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intensity mixture and band-adaptive detail fusion for
pansharpening. <em>PR</em>, <em>139</em>, 109434. (<a
href="https://doi.org/10.1016/j.patcog.2023.109434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pansharpening aims to sharpen a low-resolution multispectral (MS) image through a high-resolution single-channel panchromatic (PAN) image to obtain a high-resolution multi-spectral (HRMS) image. However, low correlation between the PAN and MS images, as well as the inaccurate detail injection for each band of MS image are the key problems causing spectral and spatial distortions in pansharpening. To address these issues, a new pansharpening method based on the intensity mixture and band-adaptive detail fusion is proposed. To obtain a mixed-intensity image ( T ) that has a high correlation with the MS image and maintain the gradient information of the PAN image, the intensity mixture model is constructed by establishing the intensity and gradient constraints between T and the source images. As it is hard to obtain a proper degradation filter in the model, a filter estimation algorithm is designed by the distribution alignment. To inject the details that match the point spread function of the sensor, a band-adaptive detail fusion algorithm is presented to fuse the details extracted from T with those from the MS image for each band. Furthermore, as there are far fewer details in the MS image than in T , a detail enhancement algorithm is proposed to enhance the details proportionally. The final HRMS image is obtained by injecting the fused details into the upsampled MS image. Extensive experiments show that the proposed method can efficiently achieve the best results in fusion quality compared to state-of-the-art methods. The code is availabe at https://github.com/yotick/IMBD .},
  archive      = {J_PR},
  author       = {Hangyuan Lu and Yong Yang and Shuying Huang and Xiaolong Chen and Hongfu Su and Wei Tu},
  doi          = {10.1016/j.patcog.2023.109434},
  journal      = {Pattern Recognition},
  pages        = {109434},
  shortjournal = {Pattern Recognition},
  title        = {Intensity mixture and band-adaptive detail fusion for pansharpening},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to’ HCFNN: High-order coverage function neural
network for image classification’ pattern recognition. Volume 131(2022)
108873. <em>PR</em>, <em>139</em>, 109433. (<a
href="https://doi.org/10.1016/j.patcog.2023.109433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Xin Ning and Weijuan Tian and Zaiyang Yu and Weijun Li and Xiao Bai and Yuebao Wang},
  doi          = {10.1016/j.patcog.2023.109433},
  journal      = {Pattern Recognition},
  pages        = {109433},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to’ HCFNN: High-order coverage function neural network for image classification’ pattern recognition. volume 131(2022) 108873},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying the preferential direction of the model gradient
in adversarial training with projected gradient descent. <em>PR</em>,
<em>139</em>, 109430. (<a
href="https://doi.org/10.1016/j.patcog.2023.109430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training , especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction . However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this alignment increases the robustness of models.},
  archive      = {J_PR},
  author       = {Ricardo Bigolin Lanfredi and Joyce D. Schroeder and Tolga Tasdizen},
  doi          = {10.1016/j.patcog.2023.109430},
  journal      = {Pattern Recognition},
  pages        = {109430},
  shortjournal = {Pattern Recognition},
  title        = {Quantifying the preferential direction of the model gradient in adversarial training with projected gradient descent},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatio-temporal hard attention learning for skeleton-based
activity recognition. <em>PR</em>, <em>139</em>, 109428. (<a
href="https://doi.org/10.1016/j.patcog.2023.109428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of skeleton data for activity recognition has become prevalent due to its advantages over RGB data. A skeleton video includes frames showing two- or three-dimensional coordinates of human body joints . For recognizing an activity, not all the video frames are informative, and only a few key frames can well represent an activity. Moreover, not all joints participate in every activity; i.e., the key joints may vary across frames and activities. In this paper, we propose a novel framework for finding temporal and spatial attentions in a cooperative manner for activity recognition. The proposed method, which is called STH-DRL, consists of a temporal agent and a spatial agent. The temporal agent is responsible for finding the key frames, i.e., temporal hard attention finding, and the spatial agent attempts to find the key joints, i.e., spatial hard attention finding. We formulate the search problems as Markov decision processes and train both agents through interacting with each other using deep reinforcement learning. Experimental results on three widely used activity recognition benchmark datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Bahareh Nikpour and Narges Armanfard},
  doi          = {10.1016/j.patcog.2023.109428},
  journal      = {Pattern Recognition},
  pages        = {109428},
  shortjournal = {Pattern Recognition},
  title        = {Spatio-temporal hard attention learning for skeleton-based activity recognition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Complementary adversarial mechanisms for weakly-supervised
temporal action localization. <em>PR</em>, <em>139</em>, 109426. (<a
href="https://doi.org/10.1016/j.patcog.2023.109426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised Temporal Action Localization (WTAL) aims to locate the start and end boundaries of action instances and recognize their corresponding categories. Classical methods mainly rely on random erasure mechanisms, attention mechanisms , or imposing loss constraints. Despite their great progress, there are still two challenges of incomplete positioning and context confusion. Therefore, we propose a framework with complementary adversarial mechanisms to address these issues. In the adversarial learning stage, for an input snippet, we roughly determine its proper duration, by matching it with the specified multi-scaled anchors based on CAS score loss; then, it undergoes a frame-level iterative regression to precisely figure out its boundary, which can reject none closely related frames merged in, and ensures no overlapping between different action proposals. Subsequently, the GCN module explicitly enhances the feature representation for this fine localized snippet, aiming to strengthen the exclusiveness between different action snippets. Afterward, our complementary learning module calculates the similarity between the original input video V g Vg and the video V r Vr reconstructed with the above localization refined snippets, aiming to ensure no closely relevant frames missing, this checking mechanism works as feedback to guide the regression module for more accurate localization regression. Finally, each refined snippet undergoes multi-instance learning to obtain its classification score, and the top-k strategy is used to aggregate temporally adjacent snippets based on their content similarity, which can avoid fragmentation of an action proposal. This method is tested on datasets of THUMOS14 and ActivityNet1.2, and their average accuracy is 64.68\% and 32.94\% respectively, its comparisons with other articles prove its effectiveness.},
  archive      = {J_PR},
  author       = {Chuanxu Wang and Jing Wang and Peng Liu},
  doi          = {10.1016/j.patcog.2023.109426},
  journal      = {Pattern Recognition},
  pages        = {109426},
  shortjournal = {Pattern Recognition},
  title        = {Complementary adversarial mechanisms for weakly-supervised temporal action localization},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-contact PPG signal and heart rate estimation with
multi-hierarchical convolutional network. <em>PR</em>, <em>139</em>,
109421. (<a href="https://doi.org/10.1016/j.patcog.2023.109421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heartbeat rhythm and heart rate (HR) are important physiological parameters of the human body. This study presents an efficient multi-hierarchical spatio-temporal convolutional network that can quickly estimate remote physiological (rPPG) signal and HR from face video clips. First, the facial color distribution characteristics are extracted using a low-level face feature generation (LFFG) module. Then, the three-dimensional (3D) spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are used to strengthen the spatio-temporal correlation of multi-channel features. In the MHFF , sparse optical flow is used to capture the tiny motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is used to extract the estimated rPPG signal. The heart rate estimation results show that the proposed network overperforms the state-of-the-art methods on three datasets, 1) UBFC-RPPG, 2) COHFACE, 3) our dataset, with the mean absolute error (MAE) of 2.15, 5.57, 1.75 beats per minute (bpm) respectively.},
  archive      = {J_PR},
  author       = {Bin Li and Panpan Zhang and Jinye Peng and Hong Fu},
  doi          = {10.1016/j.patcog.2023.109421},
  journal      = {Pattern Recognition},
  pages        = {109421},
  shortjournal = {Pattern Recognition},
  title        = {Non-contact PPG signal and heart rate estimation with multi-hierarchical convolutional network},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VLCDoC: Vision-language contrastive pre-training model for
cross-modal document classification. <em>PR</em>, <em>139</em>, 109419.
(<a href="https://doi.org/10.1016/j.patcog.2023.109419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning from document data has achieved great success lately as it allows to pre-train semantically meaningful features as a prior into a learnable downstream task. In this paper, we approach the document classification problem by learning cross-modal representations through language and vision cues, considering intra- and inter-modality relationships. Instead of merging features from different modalities into a joint representation space, the proposed method exploits high-level interactions and learns relevant semantic information from effective attention flows within and across modalities. The proposed learning objective is devised between intra- and inter-modality alignment tasks, where the similarity distribution per task is computed by contracting positive sample pairs while simultaneously contrasting negative ones in the joint representation space. Extensive experiments on public benchmark datasets demonstrate the effectiveness and the generality of our model both on low-scale and large-scale datasets.},
  archive      = {J_PR},
  author       = {Souhail Bakkali and Zuheng Ming and Mickael Coustaty and Marçal Rusiñol and Oriol Ramos Terrades},
  doi          = {10.1016/j.patcog.2023.109419},
  journal      = {Pattern Recognition},
  pages        = {109419},
  shortjournal = {Pattern Recognition},
  title        = {VLCDoC: Vision-language contrastive pre-training model for cross-modal document classification},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density peaks clustering algorithm based on fuzzy and
weighted shared neighbor for uneven density datasets. <em>PR</em>,
<em>139</em>, 109406. (<a
href="https://doi.org/10.1016/j.patcog.2023.109406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uneven density data refers to data with a certain difference in sample density between clusters. The local density of density peaks clustering algorithm (DPC) does not consider the effect of sample density difference between clusters of uneven density data, which may lead to wrong selection of cluster centers; the algorithm allocation strategy makes it easy to incorrectly allocate samples originally belonging to sparse clusters to dense clusters, which reduces clustering efficiency. In this study, we proposed the density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets (DPC-FWSN). First, a nearest neighbor fuzzy kernel function is obtained by combining K-nearest neighbor and fuzzy neighborhood. Then, local density is redefined by the nearest neighbor fuzzy kernel function. The local density can better characterize the distribution characteristics of the sample by balancing the contribution of sample density in dense and sparse areas, in order to avoid the situation that the sparse cluster does not have a cluster center. Finally, the allocation strategy for weighted shared neighbor similarity is proposed to optimize the sample allocation at the boundary of the sparse cluster. Experiments are performed on IDPC-FA, FKNN-DPC, FNDPC, DPCSA and DPC for uneven density datasets, complex morphologies datasets and real datasets. The clustering results demonstrate that DPC-FWSN effectively handles datasets with uneven density distribution.},
  archive      = {J_PR},
  author       = {Jia Zhao and Gang Wang and Jeng-Shyang Pan and Tanghuai Fan and Ivan Lee},
  doi          = {10.1016/j.patcog.2023.109406},
  journal      = {Pattern Recognition},
  pages        = {109406},
  shortjournal = {Pattern Recognition},
  title        = {Density peaks clustering algorithm based on fuzzy and weighted shared neighbor for uneven density datasets},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective and adaptable k-means algorithm for big data
cluster analysis. <em>PR</em>, <em>139</em>, 109404. (<a
href="https://doi.org/10.1016/j.patcog.2023.109404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tradition K K -means clustering algorithm is easy to fall into local optimum, poor clustering effect on large capacity data and uneven distribution of clustering centroids . To solve these problems, a novel k k -means clustering algorithm based on Lévy flight trajectory (Lk-means) is proposed in the paper. In the iterative process of LK-means algorithm, Lévy flight is used to search new positions to avoid premature convergence in clustering. It is also applied to increase the diversity of the cluster, strengthen the global search ability of K K -means algorithm, and avoid falling into the local optimal value too early. Nevertheless, the complexity of hybrid algorithm is not increased in the process of Lévy flight optimization. To verify the data clustering effect of LK-means algorithm, experiments are conducted to compare it with the k k -means algorithm, XK-means algorithm, DDKmeans algorithm and Canopyk-means algorithm on 10 open source data sets . The results show that LK-means algorithm has better search results and more evenly distributed cluster centroids , which greatly improves the global search ability, big data processing ability and uneven distribution centroids of cluster of K K -means algorithm.},
  archive      = {J_PR},
  author       = {Haize Hu and Jianxun Liu and Xiangping Zhang and Mengge Fang},
  doi          = {10.1016/j.patcog.2023.109404},
  journal      = {Pattern Recognition},
  pages        = {109404},
  shortjournal = {Pattern Recognition},
  title        = {An effective and adaptable K-means algorithm for big data cluster analysis},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bottom-up 2D pose estimation via dual anatomical centers for
small-scale persons. <em>PR</em>, <em>139</em>, 109403. (<a
href="https://doi.org/10.1016/j.patcog.2023.109403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-person 2D pose estimation, the bottom-up methods simultaneously predict poses for all persons, and unlike the top-down methods, do not rely on human detection. However, the SOTA bottom-up methods’ accuracy is still inferior compared to the existing top-down methods. This is due to the predicted human poses being regressed based on the inconsistent human bounding box center and the lack of human-scale normalization, leading to the predicted human poses being inaccurate and small-scale persons being missed. To push the envelope of the bottom-up pose estimation, we firstly propose multi-scale training to enhance the network to handle scale variation with single-scale testing, particularly for small-scale persons. Secondly, we introduce dual anatomical centers (i.e., head and body), where we can predict the human poses more accurately and reliably, especially for small-scale persons. Moreover, existing bottom-up methods use multi-scale testing to boost the accuracy of pose estimation at the price of multiple additional forward passes, which weakens the efficiency of bottom-up methods, the core strength compared to top-down methods. By contrast, our multi-scale training enables the model to predict high-quality poses in a single forward pass (i.e., single-scale testing). Our method achieves 38.4\% improvement on bounding box precision and 39.1\% improvement on bounding box recall over the state of the art (SOTA) on the challenging small-scale persons subset of COCO. For the human pose AP evaluation, we achieve a new SOTA (71.0 AP) on the COCO test-dev set with the single-scale testing. We also achieve the top performance (40.3 AP) on the OCHuman dataset in cross-dataset evaluation.},
  archive      = {J_PR},
  author       = {Yu Cheng and Yihao Ai and Bo Wang and Xinchao Wang and Robby T. Tan},
  doi          = {10.1016/j.patcog.2023.109403},
  journal      = {Pattern Recognition},
  pages        = {109403},
  shortjournal = {Pattern Recognition},
  title        = {Bottom-up 2D pose estimation via dual anatomical centers for small-scale persons},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BabyNet: Reconstructing 3D faces of babies from uncalibrated
photographs. <em>PR</em>, <em>139</em>, 109367. (<a
href="https://doi.org/10.1016/j.patcog.2023.109367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a 3D face reconstruction system that aims at recovering the 3D facial geometry of babies from uncalibrated photographs, BabyNet. Since the 3D facial geometry of babies differs substantially from that of adults, baby-specific facial reconstruction systems are needed. BabyNet consists of two stages: 1) a 3D graph convolutional autoencoder learns a latent space of the baby 3D facial shape; and 2) a 2D encoder that maps photographs to the 3D latent space based on representative features extracted using transfer learning. In this way, using the pre-trained 3D decoder, we can recover a 3D face from 2D images. We evaluate BabyNet and show that 1) methods based on adult datasets cannot model the 3D facial geometry of babies, which proves the need for a baby-specific method, and 2) BabyNet outperforms classical model-fitting methods even when a baby-specific 3D morphable model, such as BabyFM, is used.},
  archive      = {J_PR},
  author       = {Araceli Morales and Antonia Alomar and Antonio R. Porras and Marius George Linguraru and Gemma Piella and Federico M. Sukno},
  doi          = {10.1016/j.patcog.2023.109367},
  journal      = {Pattern Recognition},
  pages        = {109367},
  shortjournal = {Pattern Recognition},
  title        = {BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Efficient large-scale oblique image matching based on
cascade hashing and match data scheduling. <em>PR</em>, <em>138</em>,
109442. (<a href="https://doi.org/10.1016/j.patcog.2023.109442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we design an efficient large-scale oblique image matching method. First, to reduce the number of redundant transmissions of match data, we propose a novel three-level buffer data scheduling (TLBDS) algorithm that considers the adjacency between images for match data scheduling from disk to graphics memory. Second, we adopt the epipolar constraint to filter the initial candidate points of cascade hashing matching, thereby significantly increasing the robustness of matching feature points. Comprehensive experiments are conducted on three oblique image datasets to test the efficiency and effectiveness of the proposed method. The experimental results show that our method can complete a match pair within 2.50 ∼ ∼ 2.64 ms, which not only is much faster than two open benchmark pipelines (i.e., OpenMVG and COLMAP) by 20.4 ∼ ∼ 97.0 times but also have higher efficiency than two state-of-the-art commercial software (i.e., Agisoft Metashape and Pix4Dmapper) by 10.4 ∼ ∼ 50.0 times.},
  archive      = {J_PR},
  author       = {Qiyuan Zhang and Shunyi Zheng and Ce Zhang and Xiqi Wang and Rui Li},
  doi          = {10.1016/j.patcog.2023.109442},
  journal      = {Pattern Recognition},
  pages        = {109442},
  shortjournal = {Pattern Recognition},
  title        = {Efficient large-scale oblique image matching based on cascade hashing and match data scheduling},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting transferability of physical attack against
detectors by redistributing separable attention. <em>PR</em>,
<em>138</em>, 109435. (<a
href="https://doi.org/10.1016/j.patcog.2023.109435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on attack transferability is of great importance as it can guide how to conduct an adversarial attack without knowing any information about target models. However, it remains challenging for adversarial examples to maintain a good attack transferability performance, especially for the black-box attack implemented in the physical world. To enhance black-box transferability of physical attacks on object detectors, we present a novel adversarial learning method to produce adversarial patches by redistributing separable attention maps. Concretely, we first develop smoothed multilayer attention maps by introducing serial composite transformations, which could suppress model-specific noise on the one hand, and cover objects to be concealed at various resolutions on the other hand. Besides, our method resorts to a scalable mask to separate object attention from the background and adjust their distribution with a novel loss function. Extensive experiments show that our approach outperforms state-of-the-art methods in both the digital space and the physical world. Our code is available at https://github.com/zhangyu13a/transPhyAtt .},
  archive      = {J_PR},
  author       = {Yu Zhang and Zhiqiang Gong and Yichuang Zhang and Kangcheng Bin and Yongqian Li and Jiahao Qi and Hao Wen and Ping Zhong},
  doi          = {10.1016/j.patcog.2023.109435},
  journal      = {Pattern Recognition},
  pages        = {109435},
  shortjournal = {Pattern Recognition},
  title        = {Boosting transferability of physical attack against detectors by redistributing separable attention},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D medical image segmentation using parallel transformers.
<em>PR</em>, <em>138</em>, 109432. (<a
href="https://doi.org/10.1016/j.patcog.2023.109432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent 3D medical image segmentation methods adopt convolutional neural networks (CNNs) that rely on deep feature representation and achieve adequate performance. However, due to the convolutional architectures having limited receptive fields, they cannot explicitly model the long-range dependencies in the medical image. Recently, Transformer can benefit from global dependencies using self-attention mechanisms and learn highly expressive representations. Some works were designed based on the Transformers, but the existing Transformers suffer from extreme computational and memories, and they cannot take full advantage of the powerful feature representations in 3D medical image segmentation. In this paper, we aim to connect the different resolution streams in parallel and propose a novel network, named Trans former based H igh R esolution Net work (TransHRNet), with an Effective Transformer (EffTrans) block, which has sufficient feature representation even at high feature resolutions . Given a 3D image, the encoder first utilizes CNN to extract the feature representations to capture the local information, and then the different feature maps are reshaped elaborately for tokens that are fed into each Transformer stream in parallel to learn the global information and repeatedly exchange the information across streams. Unfortunately, the proposed framework based on the standard Transformer needs a huge amount of computation, thus we introduce a deep and effective Transformer to deliver better performance with fewer parameters. The proposed TransHRNet is evaluated on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV) dataset that consists of 11 major human organs and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks . Experimental results show that it performs better than the convolutional and other related Transformer-based methods on the 3D multi-organ segmentation tasks . Code is available at https://github.com/duweidai/TransHRNet .},
  archive      = {J_PR},
  author       = {Qingsen Yan and Shengqiang Liu and Songhua Xu and Caixia Dong and Zongfang Li and Javen Qinfeng Shi and Yanning Zhang and Duwei Dai},
  doi          = {10.1016/j.patcog.2023.109432},
  journal      = {Pattern Recognition},
  pages        = {109432},
  shortjournal = {Pattern Recognition},
  title        = {3D medical image segmentation using parallel transformers},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrared and visible image fusion based on multi-state
contextual hidden markov model. <em>PR</em>, <em>138</em>, 109431. (<a
href="https://doi.org/10.1016/j.patcog.2023.109431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel multi-state contextual hidden Markov model (MCHMM) in the non-subsampled Shearlet transform (NSST) domain for image fusion. The traditional two-state hidden Markov model divides the multi-scale coefficients only into large and small states, which can lead to an inaccurate statistical model and reduce the quality of the fusion result. Our method improves upon this by developing a multi-state model and a soft context variable to provide a fine-grained representation of the high-frequency subbands, resulting in improved fusion results. Additionally, the fusion of low-frequency subbands is performed on the difference of regional energy to ensure visual quality. Our experimental results on several datasets demonstrate that the proposed method outperforms other fusion methods in both subjective and objective evaluations.},
  archive      = {J_PR},
  author       = {Xiaoqing Luo and Yuting Jiang and Anqi Wang and Juan Wang and Zhancheng Zhang and Xiao-Jun Wu},
  doi          = {10.1016/j.patcog.2023.109431},
  journal      = {Pattern Recognition},
  pages        = {109431},
  shortjournal = {Pattern Recognition},
  title        = {Infrared and visible image fusion based on multi-state contextual hidden markov model},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aggregated pyramid gating network for human pose estimation
without pre-training. <em>PR</em>, <em>138</em>, 109429. (<a
href="https://doi.org/10.1016/j.patcog.2023.109429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a comprehensive aggregated residual gating structure, the Pyramid GAting Network (PGA-Net) for human pose estimation which can select, distill, and fuse semantic level and natural level information from multiple scales. In comparison, through utilizing multi-scale features, most existing state-of-the-art pose estimation methods are still limited in three aspects. First, multi-scale features contain massively redundant information, which is unfortunately not distilled by most existing approaches. Second, preferring deeper network structures to extract strong semantic features , the conventional methods often ignore original texture information fusion. Third, to attain a good parameter initialization, the current methods heavily rely on pre-training, which is very time-consuming or even unavailable. While better coping with the above problems, our proposed PGA-Net distills high-level semantic features and replenishes low-level original information to reinforce module representation capability. Meanwhile, PGA-Net demonstrates notable training stability and superior performance even without pre-training. Extensive experiments demonstrate that our method consistently outperforms previous approaches even without pre-training, enabling thus an end-to-end model training from scratch. In COCO benchmark, PGA-Net consistently achieves over 3\% improvements than the baseline (without pre-training) under various model configurations . 1},
  archive      = {J_PR},
  author       = {Chenru Jiang and Kaizhu Huang and Shufei Zhang and Xinheng Wang and Jimin Xiao and Yannis Goulermas},
  doi          = {10.1016/j.patcog.2023.109429},
  journal      = {Pattern Recognition},
  pages        = {109429},
  shortjournal = {Pattern Recognition},
  title        = {Aggregated pyramid gating network for human pose estimation without pre-training},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric algebra-based multiview interaction networks for
3D human motion prediction. <em>PR</em>, <em>138</em>, 109427. (<a
href="https://doi.org/10.1016/j.patcog.2023.109427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D skeleton-based human motion prediction is an essential and challenging task for human-machine interactions, which aims to forecasts future poses given a history of their previous motions. Recent works based on Graph Neural Networks (GCNs) show promising performance for motion prediction due to the powerful ability of feature aggregation of GCNs. However, with the deep and multi-stage GCN model deployment , its feature extraction mechanism tends to result in feature similarity over all joints , and degrade the prediction performance. In addition, such a graph structure in recent works was still insufficient to process the high dimensional structural data in Euclidean space when inference through multi-layer networks. To solve the problem, we propose a novel Geometric Algebra-based Multi-view Interaction network (GA-MIN), which captures and aggregates motion features from two interactions: 1) global-interaction, which refactors various spectrum dependencies using geometric algebra-based structure, and 2) self-interaction, which leverage self-attention mechanism to capture compact representations . Extensive experiments are conducted on three public datasets: Human3.6M, CMU Mocap, and 3DPW, which prove that the proposed GA-MIN outperforms state-of-the-art methods on 3D Mean Per Joint Position Error (MPJPE) and Mean Angle Error (MAE) on average.},
  archive      = {J_PR},
  author       = {Jianqi Zhong and Wenming Cao},
  doi          = {10.1016/j.patcog.2023.109427},
  journal      = {Pattern Recognition},
  pages        = {109427},
  shortjournal = {Pattern Recognition},
  title        = {Geometric algebra-based multiview interaction networks for 3D human motion prediction},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NPDN-3D: A 3D neural partial differential network for
spatiotemporal prediction. <em>PR</em>, <em>138</em>, 109425. (<a
href="https://doi.org/10.1016/j.patcog.2023.109425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network-based methods have been widely applied to spatiotemporal prediction tasks, such as video prediction and weather forecasting. However, most existing works are designed for prediction in 2D space, and 3D prediction has not been extensively studied. In this paper, we propose to leverage 3D partial differential equations (PDEs) for spatiotemporal prediction in 3D space, and further develop a novel 3D neural partial differential network. This is inspired by that 3D PDEs can model both horizontal and vertical information interactions by various partial derivatives. Moreover, they can also formulate physical knowledge by equations. To integrate 3D PDEs in neural networks, we first develop the theory of approximating 3D partial derivatives by 3D convolutions, and further present an effective strategy to utilize the theory in practice. Then based on the theory and strategy, we propose a novel 3D Neural Partial Differential Network for prediction, named NPDN-3D. Specifically, NPDN-3D consists of two pivotal modules: (1) a neural partial differential module for capturing low-order spatiotemporal dynamics. This module is the key for prediction, where the dynamics are formulated by commonly-used low-order 3D PDEs. (2) A residual module for capturing the remaining non-low-order dynamics. This module performs as an extensible plug-in to enhance the expressiveness of our model. Extensive experiments on two simulated datasets and two real datasets show that our method not only achieves better prediction but also learns the correct PDEs.},
  archive      = {J_PR},
  author       = {Xu Huang and Shanshan Feng and Yunming Ye and Xutao Li and Bowen Zhang and Shidong Chen},
  doi          = {10.1016/j.patcog.2023.109425},
  journal      = {Pattern Recognition},
  pages        = {109425},
  shortjournal = {Pattern Recognition},
  title        = {NPDN-3D: A 3D neural partial differential network for spatiotemporal prediction},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ThumbDet: One thumbnail image is enough for object
detection. <em>PR</em>, <em>138</em>, 109424. (<a
href="https://doi.org/10.1016/j.patcog.2023.109424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision fields have witnessed great success thanks to deep convolutional neural networks (CNNs). However, state-of-the-art methods often benefit from large models and datasets, which introduce heavy parameters and computational requirements. Deploying such large models in real-world applications is very difficult because of the limited computing resources. Although many researchers focus on designing efficient block structures to compress model parameters, they ignore that the role of large-scale input images is also an important factor for algorithm efficiency. Reducing input resolution is a useful method to boost runtime efficiency, however, traditional interpolation methods assume a fixed degradation criterion that greatly hurts performance. To solve the above problems, in this paper, we propose a novel framework named ThumbDet for reducing model computation while maintaining detection accuracy. In our framework, we first design an image down-sampling module to learn a small-scale image that looks realistic and contains discriminative properties. Furthermore, we propose a distillation-boost supervision strategy to maintain the detection performance of small-scaled images as the original-size inputs. Extensive experiments conducted on a standard object detection dataset MS COCO demonstrate the effectiveness of the proposed method when using very low-resolution images ( i.e. 4 × 4× down-sampling) as inputs. In particular, ThumbDet achieves satisfactory detection performance ( i.e. 32.3\% in mAP) while drastically reducing computation and memory requirements ( i.e. speed up of 1.26 × × ), outperforming the traditional interpolation methods ( e.g. bicubic) by +3.2\% absolutely in terms of mAP.},
  archive      = {J_PR},
  author       = {Yongqiang Zhang and Yin Zhang and Rui Tian and Zian Zhang and Yancheng Bai and Wangmeng Zuo and Mingli Ding},
  doi          = {10.1016/j.patcog.2023.109424},
  journal      = {Pattern Recognition},
  pages        = {109424},
  shortjournal = {Pattern Recognition},
  title        = {ThumbDet: One thumbnail image is enough for object detection},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic graph structure learning for multivariate time
series forecasting. <em>PR</em>, <em>138</em>, 109423. (<a
href="https://doi.org/10.1016/j.patcog.2023.109423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting is a challenging task because the dynamic spatio-temporal dependencies between variables are a combination of multiple unknown association patterns. Existing graph neural networks typically model multivariate relationships with a predefined spatial graph or a learned fixed adjacency graph, which fails to handle the aforementioned challenges. In this study, we decompose association patterns into stable long-term and dynamic short-term patterns and propose a novel framework, named the static and dynamic graph learning network (SDGL), for modeling unknown patterns. Our approach infers two types of graph structures, from the data simultaneously: static and dynamic graphs. A static graph is developed to capture the fixed long-term pattern via node embedding , and we leverage graph regularity to control its learning direction. Dynamic graphs, which are time-varying matrices based on changing node-level features, are used to model dynamic dependencies over the short term. To effectively capture local dynamic patterns, we integrate the learned long-term pattern as an inductive bias. Experiments on six benchmark datasets show the state-of-the-art performance of our method. Analysis of the learned graphs reveals that the model succeeds in modeling dynamic spatio-temporal dependencies.},
  archive      = {J_PR},
  author       = {Zhuo Lin Li and Gao Wei Zhang and Jie Yu and Ling Yu Xu},
  doi          = {10.1016/j.patcog.2023.109423},
  journal      = {Pattern Recognition},
  pages        = {109423},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic graph structure learning for multivariate time series forecasting},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BEST: Building evidences from scattered templates for
accurate contactless palmprint recognition. <em>PR</em>, <em>138</em>,
109422. (<a href="https://doi.org/10.1016/j.patcog.2023.109422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contactless palmprint identification offers significantly improved hygiene and user convenience, making it highly attractive for a range of civilian applications, especially during the current pandemic. However, the accurate recognition of contactless palmprint images can be highly challenging, attributed to the significant variations in the intra-class similarity and limitations of conventional palmprint feature descriptors under involuntary or contactless imaging variations. State-of-the-art completely contactless palmprint matching algorithms in the literature cannot adequately address these challenges and are not sufficiently accurate and fast enough for such real-world applications. This paper proposes a novel approach that adaptively locates the local palmprint regions with high similarities between their corresponding feature representations or templates to address these challenges. We consider spatial localization of such highly similar feature representations from multiple local regions and consolidate them to generate a more reliable match score. This paper presents reproducible and comparative experimental results, using within-database, cross-database, and cross-sensor performance evaluation, on four publicly available contactless palmprint datasets, including a sizeable contactless palmprint database from 600 different subjects. The proposed method achieves outperforming results compared with three state-of-the-art deep learning-based methods and five widely used conventional methods. In addition, the proposed method is also significantly faster than all state-of-the-art baseline methods .},
  archive      = {J_PR},
  author       = {Feng Yulin and Ajay Kumar},
  doi          = {10.1016/j.patcog.2023.109422},
  journal      = {Pattern Recognition},
  pages        = {109422},
  shortjournal = {Pattern Recognition},
  title        = {BEST: Building evidences from scattered templates for accurate contactless palmprint recognition},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards local visual modeling for image captioning.
<em>PR</em>, <em>138</em>, 109420. (<a
href="https://doi.org/10.1016/j.patcog.2023.109420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the local visual modeling with grid features for image captioning, which is critical for generating accurate and detailed captions. To achieve this target, we propose a Locality-Sensitive Transformer Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention (LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer interaction in Transformer via modeling the relationship between each grid and its neighbors. It reduces the difficulty of local object recognition during captioning. LSF is used for inter-layer information fusion, which aggregates the information of different encoder layers for cross-layer semantical complementarity. With these two novel designs, the proposed LSTNet can model the local visual information of grid features to improve the captioning quality. To validate LSTNet, we conduct extensive experiments on the competitive MS-COCO benchmark. The experimental results show that LSTNet is not only capable of local visual modeling, but also outperforms a bunch of state-of-the-art captioning models on offline and online testings, i.e., 134.8 CIDEr and 136.3 CIDEr, respectively. Besides, the generalization of LSTNet is also verified on the Flickr8k and Flickr30k datasets. The source code is available on GitHub: https://www.github.com/xmu-xiaoma666/LSTNet .},
  archive      = {J_PR},
  author       = {Yiwei Ma and Jiayi Ji and Xiaoshuai Sun and Yiyi Zhou and Rongrong Ji},
  doi          = {10.1016/j.patcog.2023.109420},
  journal      = {Pattern Recognition},
  pages        = {109420},
  shortjournal = {Pattern Recognition},
  title        = {Towards local visual modeling for image captioning},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mitigate the classification ambiguity via
localization-classification sequence in object detection. <em>PR</em>,
<em>138</em>, 109418. (<a
href="https://doi.org/10.1016/j.patcog.2023.109418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In anchor-based detectors, the confidence scores and label-assignment results for the classification task are determined by the unrefined anchors rather than the final-refined boxes, which causes classification ambiguity due to the lack of correlation between the classification and localization tasks. In this paper, we investigate the classification ambiguity thoroughly via extensive experiments, and present the localization-classification sequence detector (LCSDet) that performs localization and classification in order, bridging the gap between them. To achieve this, the refinement-aware (RA) classification branch and RA assignment are proposed in LCSDet. In inference, the RA classification branch rectifies the feature misalignment and directly classifies the refined anchors. During training, the RA assignment tackles the training instability, narrows the location-quality gap and assigns the refined anchors to ground-truth objects. Comprehensive experiments indicate that the LCSDet can effectively mitigate the classification ambiguity and achieve stable improvement across different baselines.},
  archive      = {J_PR},
  author       = {Chang Liu and Shaorong Xie and Xiaomao Li and Jiantao Gao and Weiping Xiao and Baojie Fan and Yan Peng},
  doi          = {10.1016/j.patcog.2023.109418},
  journal      = {Pattern Recognition},
  pages        = {109418},
  shortjournal = {Pattern Recognition},
  title        = {Mitigate the classification ambiguity via localization-classification sequence in object detection},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Timid semi–supervised learning for face expression analysis.
<em>PR</em>, <em>138</em>, 109417. (<a
href="https://doi.org/10.1016/j.patcog.2023.109417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years, semi–supervised learning has been proposed as a strategy with high potential for improving machine learning capabilities. Face expression recognition may highly benefit from such a technique, as accurate labeling is both difficult and costly, whereas millions of unlabeled images with human faces are available on the Internet, but without annotations. In this paper we evaluate the benefits of semi–supervised learning in the practical scenarios of face expression analysis. Our conclusion is that better performance is indeed achievable, but by methods that put a distinct emphasis on the diversity of exploring patterns in the unlabeled data domain. The evaluation is carried on multiple tasks such as detecting Action Units on EmotioNet, assessing Action Units intensity on the spontaneous DISFA database and, respectively, recognizing expressions on static images acquired in the wild, from the RAF-DB and FER+ databases. We show that, in these scenarios, a so–called timid semi–supervised learner is more robust and achieves higher performance than standard, confident semi–supervised learners.},
  archive      = {J_PR},
  author       = {Mihai Badea and Corneliu Florea and Andrei Racoviţeanu and Laura Florea and Constantin Vertan},
  doi          = {10.1016/j.patcog.2023.109417},
  journal      = {Pattern Recognition},
  pages        = {109417},
  shortjournal = {Pattern Recognition},
  title        = {Timid semi–supervised learning for face expression analysis},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cycle-object consistency for image-to-image domain
adaptation. <em>PR</em>, <em>138</em>, 109416. (<a
href="https://doi.org/10.1016/j.patcog.2023.109416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in generative adversarial networks (GANs) have been proven effective in performing domain adaptation for object detectors through data augmentation . While GANs are exceptionally successful, those methods that can preserve objects well in the image-to-image translation task usually require an auxiliary task, such as semantic segmentation to prevent the image content from being too distorted. However, pixel-level annotations are difficult to obtain in practice. Alternatively, instance-aware image-translation model treats object instances and background separately. Yet, it requires object detectors at test time, assuming that off-the-shelf detectors work well in both domains. In this work, we present AugGAN-Det, which introduces Cycle-object Consistency (CoCo) loss to generate instance-aware translated images across complex domains. The object detector of the target domain is directly leveraged in generator training and guides the preserved objects in the translated images to carry target-domain appearances. Compared to previous models, which e.g., require pixel-level semantic segmentation to force the latent distribution to be object-preserving, this work only needs bounding box annotations which are significantly easier to acquire. Next, as to the instance-aware GAN models, our model, AugGAN-Det, internalizes global and object style-transfer without explicitly aligning the instance features. Most importantly, a detector is not required at test time. Experimental results demonstrate that our model outperforms recent object-preserving and instance-level models and achieves state-of-the-art detection accuracy and visual perceptual quality .},
  archive      = {J_PR},
  author       = {Che-Tsung Lin and Jie-Long Kew and Chee Seng Chan and Shang-Hong Lai and Christopher Zach},
  doi          = {10.1016/j.patcog.2023.109416},
  journal      = {Pattern Recognition},
  pages        = {109416},
  shortjournal = {Pattern Recognition},
  title        = {Cycle-object consistency for image-to-image domain adaptation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate light field depth estimation under occlusion.
<em>PR</em>, <em>138</em>, 109415. (<a
href="https://doi.org/10.1016/j.patcog.2023.109415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epipolar plane images (EPIs) have advantages in light field depth estimation, but the current EPI-based methods only use one aspect of the information related to the line or its surroundings in EPIs. Moreover, most current methods merely extract the depth map of the target view, ignoring the depth information from other views. To fully utilize the available information, we first introduce a novel data cost by comprehensively utilizing the characteristics of pixel consistency on the line and region difference around the line in EPIs to improve the robustness against occlusion and noise. Then, we put forward a multi-view depth integration strategy that copes with weak texture and occlusion areas. Finally, an edging preserving filter is applied to further refine the depth map. Experiments on synthetic and real light field datasets show that the proposed method outperforms the state-of-the-art light field depth estimation algorithms, especially in the presence of occluded pixels.},
  archive      = {J_PR},
  author       = {Yuxuan Liu and Mitko Aleksandrov and Zhihua Hu and Yan Meng and Li Zhang and Sisi Zlatanova and Haibin Ai and Pengjie Tao},
  doi          = {10.1016/j.patcog.2023.109415},
  journal      = {Pattern Recognition},
  pages        = {109415},
  shortjournal = {Pattern Recognition},
  title        = {Accurate light field depth estimation under occlusion},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust weighted co-clustering with global and local
discrimination. <em>PR</em>, <em>138</em>, 109405. (<a
href="https://doi.org/10.1016/j.patcog.2023.109405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past few decades, the clustering problem has made considerable progress, and co-clustering algorithms have attracted more attention. Compared with one-side clustering, co-clustering not only groups samples according to the distribution of features but also groups features according to the distribution of samples at the same time. This duality helps to explore the structural information of data, such as genes and texts. In this paper, a new co-clustering algorithm is proposed to simultaneously consider feature weights, data noise, local manifolds, and global scatter, named robust weighted co-clustering with global and local discrimination. Furthermore, an alternate update rule is put forward to optimize objective, theoretically proven to converge. Then, the algorithm’s duality, robustness, and effectiveness have been verified on synthetic, corrupted, and real datasets, respectively. The runtime and parameter sensitivity of the algorithm are also analyzed. Finally, sufficient experiments clarify the competitiveness of our algorithm compared to other ones.},
  archive      = {J_PR},
  author       = {Zhoumin Lu and Shiping Wang and Genggeng Liu and Feiping Nie},
  doi          = {10.1016/j.patcog.2023.109405},
  journal      = {Pattern Recognition},
  pages        = {109405},
  shortjournal = {Pattern Recognition},
  title        = {Robust weighted co-clustering with global and local discrimination},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Cross-task and cross-domain SAR target recognition: A
meta-transfer learning approach. <em>PR</em>, <em>138</em>, 109402. (<a
href="https://doi.org/10.1016/j.patcog.2023.109402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta learning and transfer learning offer promising solutions to the problem of requiring large amounts of data in deep learning approaches for synthetic aperture radar (SAR) target recognition. To improve their performance further, we propose a novel Me ta- tra nsfer learning approach for cross-task and cross-domain SAR target recognition (MetraSAR). In the meta training phase, we train a robust meta learner with the human-like ability to master new knowledge quickly across tasks and domains. By designing the weighted classification loss with class weights, we conduct hard class mining that forces the meta learner to grow stronger. In addition to the external knowledge transfer across different tasks, we achieve the internal transfer across domains by using the domain confusion loss with a domain discriminator . To balance the two designed loss terms, we adopt the multi-gradient descent algorithm to optimize the meta learner adaptively. In the meta testing phase, the trained robust meta learner is transferred to solve the new task with few shot samples and a quick generalization. Extensive experiments on the moving and stationary target acquisition and recognition (MSTAR) dataset validate that MetraSAR has better performance than conventional SAR target recognition methods.},
  archive      = {J_PR},
  author       = {Yukun Zhang and Xiansheng Guo and Henry Leung and Lin Li},
  doi          = {10.1016/j.patcog.2023.109402},
  journal      = {Pattern Recognition},
  pages        = {109402},
  shortjournal = {Pattern Recognition},
  title        = {Cross-task and cross-domain SAR target recognition: A meta-transfer learning approach},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Poisson PCA for matrix count data. <em>PR</em>,
<em>138</em>, 109401. (<a
href="https://doi.org/10.1016/j.patcog.2023.109401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a dimension reduction framework for data consisting of matrices of counts. Our model is based on the assumption of existence of a small amount of independent normal latent variables that drive the dependency structure of the observed data, and can be seen as the exact discrete analogue of a contaminated low-rank matrix normal model. We derive estimators for the model parameters and establish their limiting normality. An extension of a recent proposal from the literature is used to estimate the latent dimension of the model. The method is shown to outperform both its vectorization-based competitors and matrix methods assuming the continuity of the data distribution in analysing simulated data and real world abundance data.},
  archive      = {J_PR},
  author       = {Joni Virta and Andreas Artemiou},
  doi          = {10.1016/j.patcog.2023.109401},
  journal      = {Pattern Recognition},
  pages        = {109401},
  shortjournal = {Pattern Recognition},
  title        = {Poisson PCA for matrix count data},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from multiple annotators for medical image
segmentation. <em>PR</em>, <em>138</em>, 109400. (<a
href="https://doi.org/10.1016/j.patcog.2023.109400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised machine learning methods have been widely developed for segmentation tasks in recent years. However, the quality of labels has high impact on the predictive performance of these algorithms. This issue is particularly acute in the medical image domain, where both the cost of annotation and the inter-observer variability are high. Different human experts contribute estimates of the ”actual” segmentation labels in a typical label acquisition process, influenced by their personal biases and competency levels. The performance of automatic segmentation algorithms is limited when these noisy labels are used as the expert consensus label. In this work, we use two coupled CNNs to jointly learn, from purely noisy observations alone, the reliability of individual annotators and the expert consensus label distributions. The separation of the two is achieved by maximally describing the annotator’s “unreliable behavior” (we call it “maximally unreliable”) while achieving high fidelity with the noisy training data. We first create a toy segmentation dataset using MNIST and investigate the properties of the proposed algorithm. We then use three public medical imaging segmentation datasets to demonstrate our method’s efficacy, including both simulated (where necessary) and real-world annotations: 1) ISBI2015 (multiple-sclerosis lesions); 2) BraTS (brain tumors); 3) LIDC-IDRI (lung abnormalities). Finally, we create a real-world multiple sclerosis lesion dataset (QSMSC at UCL: Queen Square Multiple Sclerosis Center at UCL, UK) with manual segmentations from 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label). In all datasets, our method consistently outperforms competing methods and relevant baselines, especially when the number of annotations is small and the amount of disagreement is large. The studies also reveal that the system is capable of capturing the complicated spatial characteristics of annotators’ mistakes.},
  archive      = {J_PR},
  author       = {Le Zhang and Ryutaro Tanno and Moucheng Xu and Yawen Huang and Kevin Bronik and Chen Jin and Joseph Jacob and Yefeng Zheng and Ling Shao and Olga Ciccarelli and Frederik Barkhof and Daniel C. Alexander},
  doi          = {10.1016/j.patcog.2023.109400},
  journal      = {Pattern Recognition},
  pages        = {109400},
  shortjournal = {Pattern Recognition},
  title        = {Learning from multiple annotators for medical image segmentation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MUNet: Motion uncertainty-aware semi-supervised video object
segmentation. <em>PR</em>, <em>138</em>, 109399. (<a
href="https://doi.org/10.1016/j.patcog.2023.109399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of semi-supervised video object segmentation (VOS) has been greatly advanced and state-of-the-art performance has been made by dense matching-based methods. The recent methods leverage space-time memory (STM) networks and learn to retrieve relevant information from all available sources, where the past frames with object masks form an external memory and the current frame as the query is segmented using the mask information in the memory. However, when forming the memory and performing matching, these methods only exploit the appearance information while ignoring the motion information. In this paper, we advocate for the return of the motion information and propose a motion uncertainty-aware framework (MUNet) for semi-supervised VOS. First, we propose an implicit method to learn the spatial correspondences between neighboring frames, building upon a correlation cost volume. To handle the challenging cases of occlusion and textureless regions during constructing dense correspondences, we incorporate the uncertainty in dense matching and achieve motion uncertainty-aware feature representation. Second, we introduce a motion-aware spatial attention module to effectively fuse the motion features with the semantic features . Comprehensive experiments on challenging benchmarks show that using a small amount of data and combining it with powerful motion information can bring a significant performance boost . We achieve 76.5\% 76.5\% J &amp; F J&amp;#x26;F only using DAVIS17 for training 2 , which significantly outperforms the SOTA methods under the low-data protocol. The code and supplementary materials will be available at https://npucvr.github.io/MUNet .},
  archive      = {J_PR},
  author       = {Jiadai Sun and Yuxin Mao and Yuchao Dai and Yiran Zhong and Jianyuan Wang},
  doi          = {10.1016/j.patcog.2023.109399},
  journal      = {Pattern Recognition},
  pages        = {109399},
  shortjournal = {Pattern Recognition},
  title        = {MUNet: Motion uncertainty-aware semi-supervised video object segmentation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human-related anomalous event detection via memory-augmented
wasserstein generative adversarial network with gradient penalty.
<em>PR</em>, <em>138</em>, 109398. (<a
href="https://doi.org/10.1016/j.patcog.2023.109398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely detection of human-related anomaly in surveillance videos is a challenging task. Generally, the irregular human motion and action patterns can be regarded as abnormal human-related events. In this paper, we utilize the skeleton trajectories to learn the regularities of human motion and action in videos for anomaly detection . The skeleton trajectories are decomposed into global and local feature sequences, which are utilized to provide human motion and action information, respectively. Then, the global and local sequences are modeled as two separate sub-processes with our proposed Memory-augmented Wasserstein Generative Adversarial Network with Gradient Penalty (MemWGAN-GP). In each sub-process, the pre-trained MemWGAN-GP is employed to predict future feature sequences from corresponding input past sequences and reconstruct the input sequences simultaneously. The predicted and reconstructed feature sequences are compared with their groundtruth to identify anomalous sequences. The MemWGAN-GP integrates the autoencoder with a WGAN model to boost the reconstruction and prediction ability of the autoencoder . Besides, a memory module is employed in MemWGAN-GP to overcome high capacity of the autoencoder for anomalies reconstruction and prediction. Experimental results on four challenging datasets demonstrate advantages of the proposed method over other state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Nanjun Li and Faliang Chang and Chunsheng Liu},
  doi          = {10.1016/j.patcog.2023.109398},
  journal      = {Pattern Recognition},
  pages        = {109398},
  shortjournal = {Pattern Recognition},
  title        = {Human-related anomalous event detection via memory-augmented wasserstein generative adversarial network with gradient penalty},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human-centered deep compositional model for handling
occlusions. <em>PR</em>, <em>138</em>, 109397. (<a
href="https://doi.org/10.1016/j.patcog.2023.109397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their powerful discriminative abilities, Convolutional Neural Networks (CNNs) lack the properties of generative models . This leads to a decreased performance in environments where objects are poorly visible. Solving such a problem by adding more training samples can quickly lead to a combinatorial explosion, therefore the underlying architecture has to be changed instead. This work proposes a Human-Centered Deep Compositional model (HCDC) that combines low-level visual discrimination of a CNN and the high-level reasoning of a Hierarchical Compositional model (HCM). Defined as a transparent model, it can be optimized to real-world environments by adding compactly encoded domain knowledge from human studies and physical laws. The new FridgeNetv2 dataset and a mixture of publicly available datasets are used as a benchmark. The experimental results show the proposed model is explainable, has higher discriminative and generative power, and better handles the occlusion than the current state-of-the-art Mask-RCNN in instance segmentation tasks.},
  archive      = {J_PR},
  author       = {Gregor Koporec and Janez Perš},
  doi          = {10.1016/j.patcog.2023.109397},
  journal      = {Pattern Recognition},
  pages        = {109397},
  shortjournal = {Pattern Recognition},
  title        = {Human-centered deep compositional model for handling occlusions},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessing polygonal approximations: A new measurement and a
comparative study. <em>PR</em>, <em>138</em>, 109396. (<a
href="https://doi.org/10.1016/j.patcog.2023.109396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two proposals related to the evaluation of polygonal approximations are presented in this document. First, a new measurement, called normalized compression ratio and adjustment error ( NCA NCA ), to provide a fair evaluation of the performance of the polygonal approximations of 2D closed curves is proposed. Second, a new methodology for evaluation of measurements for assessing polygonal approximations is also proposed. This methodology is based on the optimal quality curve concept, which can characterize the performance of the measurements. A simple visual analysis of the optimal quality curve allows possible drawbacks or weaknesses of the measurement to be detected. The new evaluation methodology is used to compare the performance of the proposed NCA NCA and the most popular measurements, such as Rosin’s Merit, FOM FOM or versions of FOM FOM . Experiments show that NCA NCA obtains the best results and, therefore, may be used to fairly evaluate the performance of polygonal approximations.},
  archive      = {J_PR},
  author       = {Nicolás Luis Fernández-García and Luis Del-Moral Martínez and Ángel Carmona-Poyato and Francisco José Madrid-Cuevas and Rafael Medina-Carnicer},
  doi          = {10.1016/j.patcog.2023.109396},
  journal      = {Pattern Recognition},
  pages        = {109396},
  shortjournal = {Pattern Recognition},
  title        = {Assessing polygonal approximations: A new measurement and a comparative study},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global and local structure preserving nonnegative subspace
clustering. <em>PR</em>, <em>138</em>, 109388. (<a
href="https://doi.org/10.1016/j.patcog.2023.109388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most subspace clustering methods construct the similarity matrix based on self-expressive property and apply the spectral relaxation on the similarity matrix to get the final clusters. Despite the advantages of this framework, it has two limitations that are easily ignored. Firstly, the original self-expressive model only considers the global structure of data, and the ubiquitous local structure among data is not paid enough attention. Secondly, spectral relaxation is naturally suitable for 2-way clustering tasks , but when dealing with multi-way clustering tasks , the assignment of cluster members becomes indirect and requires additional steps. To overcome these problems, this paper proposes a global and local structure preserving nonnegative subspace clustering method, which learns data similarities and cluster indicators in a mutually enhanced way within a unified framework. Besides, the model is extended to kernel space to strengthen its capability of dealing with nonlinear data structures . For optimizing the objective function of the method, multiplicative updating rules based on nonnegative Lagrangian relaxation are developed, and the convergence is guaranteed in theory. Abundant experiments have shown that the proposed model is better than many advanced clustering methods in most cases.},
  archive      = {J_PR},
  author       = {Hongjie Jia and Dongxia Zhu and Longxia Huang and Qirong Mao and Liangjun Wang and Heping Song},
  doi          = {10.1016/j.patcog.2023.109388},
  journal      = {Pattern Recognition},
  pages        = {109388},
  shortjournal = {Pattern Recognition},
  title        = {Global and local structure preserving nonnegative subspace clustering},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elucidating robust learning with uncertainty-aware
corruption pattern estimation. <em>PR</em>, <em>138</em>, 109387. (<a
href="https://doi.org/10.1016/j.patcog.2023.109387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust learning methods aim to learn a clean target distribution from noisy and corrupted training data where a specific corruption pattern is often assumed a priori. Our proposed method can not only successfully learn the clean target distribution from a dirty dataset but also can estimate the underlying noise pattern. To this end, we leverage a mixture-of-experts model that can distinguish two different types of predictive uncertainty, aleatoric and epistemic uncertainty. We show that the ability to estimate the uncertainty plays a significant role in elucidating the corruption patterns as these two objectives are tightly intertwined. We also present a novel validation scheme for evaluating the performance of the corruption pattern estimation. Our proposed method is extensively assessed in terms of both robustness and corruption pattern estimation in the computer vision domain. Code has been made publicly available at https://github.com/jeongeun980906/Uncertainty-Aware-Robust-Learning .},
  archive      = {J_PR},
  author       = {Jeongeun Park and Seungyoun Shin and Sangheum Hwang and Sungjoon Choi},
  doi          = {10.1016/j.patcog.2023.109387},
  journal      = {Pattern Recognition},
  pages        = {109387},
  shortjournal = {Pattern Recognition},
  title        = {Elucidating robust learning with uncertainty-aware corruption pattern estimation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RACL: A robust adaptive contrastive learning method for
conversational satisfaction prediction. <em>PR</em>, <em>138</em>,
109386. (<a href="https://doi.org/10.1016/j.patcog.2023.109386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Gang Chen and Xiangge Li and Shuaiyong Xiao and Chenghong Zhang and Xianghua Lu},
  doi          = {10.1016/j.patcog.2023.109386},
  journal      = {Pattern Recognition},
  pages        = {109386},
  shortjournal = {Pattern Recognition},
  title        = {RACL: A robust adaptive contrastive learning method for conversational satisfaction prediction},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From anomaly detection to open set recognition: Bridging the
gap. <em>PR</em>, <em>138</em>, 109385. (<a
href="https://doi.org/10.1016/j.patcog.2023.109385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classifiers that return compact acceptance regions are crucial for the success in anomaly detection and open set recognition settings since we have to determine and reject the anomalies and samples coming from the unknown classes. This paper introduces novel methods that approximate the class acceptance regions with compact hypersphere models for anomaly detection and open set recognition. As opposed to the other deep hypersphere classifiers, we treat the hypersphere centers as learnable parameters and update them based on the changing deep feature representations. In addition, we propose novel loss terms that are more robust to the noisy labels within the outlier exposure and background datasets. The proposed methods bear similarity to the deep distance metric learning classifiers using the triplet loss function with the exception that the anchors are set to the hypersphere centers which are updated dynamically. The experimental results show that the proposed methods achieve the state-of-the-art accuracies on the majority of the tested datasets in the context of anomaly detection and open set recognition.},
  archive      = {J_PR},
  author       = {Hakan Cevikalp and Bedirhan Uzun and Yusuf Salk and Hasan Saribas and Okan Köpüklü},
  doi          = {10.1016/j.patcog.2023.109385},
  journal      = {Pattern Recognition},
  pages        = {109385},
  shortjournal = {Pattern Recognition},
  title        = {From anomaly detection to open set recognition: Bridging the gap},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to reduce change detection to semantic segmentation.
<em>PR</em>, <em>138</em>, 109384. (<a
href="https://doi.org/10.1016/j.patcog.2023.109384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) aims to identify changes that occur in an image pair taken different times. Prior methods devise specific networks from scratch to predict change masks in pixel-level, and struggle with general segmentation problems. In this paper, we propose a new paradigm that reduces CD to semantic segmentation which means tailoring an existing and powerful semantic segmentation network to solve CD. This new paradigm conveniently enjoys the mainstream semantic segmentation techniques to deal with general segmentation problems in CD. Hence we can concentrate on studying how to detect changes. We propose a novel and importance insight that different change types exist in CD and they should be learned separately. Based on it, we devise a module named MTF to extract the change information and fuse temporal features. MTF enjoys high interpretability and reveals the essential characteristic of CD. And most segmentation networks can be adapted to solve the CD problems with our MTF module. Finally, we propose C-3PO, a network to detect changes at pixel-level. C-3PO achieves state-of-the-art performance without bells and whistles. It is simple but effective and can be considered as a new baseline in this field. Our code for C-3PO is available at https://github.com/DoctorKey/C-3PO .},
  archive      = {J_PR},
  author       = {Guo-Hua Wang and Bin-Bin Gao and Chengjie Wang},
  doi          = {10.1016/j.patcog.2023.109384},
  journal      = {Pattern Recognition},
  pages        = {109384},
  shortjournal = {Pattern Recognition},
  title        = {How to reduce change detection to semantic segmentation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SATS: Self-attention transfer for continual semantic
segmentation. <em>PR</em>, <em>138</em>, 109383. (<a
href="https://doi.org/10.1016/j.patcog.2023.109383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continually learning to segment more and more types of image regions is a desired capability for many intelligent systems. However, such continual semantic segmentation exhibits catastrophic forgetting issues similar to those of continual classification learning . Unlike the existing knowledge distillation strategies for alleviating this problem, transferring a new type of information, namely, the relationships between elements (e.g., pixels) within each image that can capture both within-class and between-class knowledge, is proposed in this study. Such information can be effectively obtained from self-attention maps in a Transformer-style segmentation model . Considering that pixels belonging to the same class in each image typically share similar visual properties, a class-specific region pooling operator is novelly applied to provide reliable relationship information for knowledge transfer. Extensive evaluations on multiple public benchmarks reveal that the proposed self-attention transfer method can effectively alleviate the catastrophic forgetting issue. Furthermore, flexible combinations of the proposed method with widely adopted strategies considerably outperform state-of-the-art solutions.},
  archive      = {J_PR},
  author       = {Yiqiao Qiu and Yixing Shen and Zhuohao Sun and Yanchong Zheng and Xiaobin Chang and Weishi Zheng and Ruixuan Wang},
  doi          = {10.1016/j.patcog.2023.109383},
  journal      = {Pattern Recognition},
  pages        = {109383},
  shortjournal = {Pattern Recognition},
  title        = {SATS: Self-attention transfer for continual semantic segmentation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Defense against adversarial attacks with efficient
frequency-adaptive compression and reconstruction. <em>PR</em>,
<em>138</em>, 109382. (<a
href="https://doi.org/10.1016/j.patcog.2023.109382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of deep neural networks exposes themselves to adversarial attacks in the real world drawn from closed-set and open-set, which poses great threats to their application in safety-critical systems. Since adversarial attacks tend to mislead an original model by adding small perturbations into clean images, an intuitive idea of defensing adversarial attacks is eliminating perturbations as much as possible to mitigate attacking effects. However, such elimination-based strategies unfortunately fail to achieve satisfactory robustness. Aiming to investigate the intrinsic reasons for this phenomenon, systematic experiments are carried out in this paper to indicate that even a 20\% residual perturbation can still preserve and exhibit attacking effects as strong as a full one. Our study also indicates that there are strong correlations between perturbations and legitimate images. Thus, breaking the correlation across multiple bands is more effective in mitigating attacking effects. Based on these findings, this paper proposes an efficient defense strategy called “Frequency-Adaptive Compression and rEconstruction (FACE)” to improve the robustness of the model to adversarial attacks. Specifically, low-frequency bands containing semantic information are compressed by a down-sampling operation, while the channel width of high-frequency bands is squeezed and further compressed by adding noise before the Tanh activating function. Meanwhile, attachment spaces of perturbations are also squeezed to the extent as much as possible. Finally, a clean output is obtained by upsampling together with expanded reconstruction. Experiments are extensively conducted on widely used datasets to demonstrate the effectiveness of the proposed method. For closed-set attacks, FACE outperforms the STOA elimination-based methods on ImageNet, achieving a 27.9\% improvement. For the MNIST open-set attacks, it not only reduces the success rate of targeted attack by a large margin (from 100\% to 24.7\%), but also mitigates attacking effects with an FPR-95 value of 0.3.},
  archive      = {J_PR},
  author       = {Zhong-Han Niu and Yu-Bin Yang},
  doi          = {10.1016/j.patcog.2023.109382},
  journal      = {Pattern Recognition},
  pages        = {109382},
  shortjournal = {Pattern Recognition},
  title        = {Defense against adversarial attacks with efficient frequency-adaptive compression and reconstruction},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep metric learning for few-shot image classification: A
review of recent developments. <em>PR</em>, <em>138</em>, 109381. (<a
href="https://doi.org/10.1016/j.patcog.2023.109381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot image classification is a challenging problem that aims to achieve the human level of recognition based only on a small number of training images. One main solution to few-shot image classification is deep metric learning. These methods, by classifying unseen samples according to their distances to few seen samples in an embedding space learned by powerful deep neural networks , can avoid overfitting to few training images in few-shot image classification and have achieved the state-of-the-art performance. In this paper, we provide an up-to-date review of deep metric learning methods for few-shot image classification from 2018 to 2022 and categorize them into three groups according to three stages of metric learning, namely learning feature embeddings, learning class representations, and learning distance measures. Under this taxonomy, we identify the trends of transitioning from learning task-agnostic features to task-specific features, from simple computation of prototypes to computing task-dependent prototypes or learning prototypes, from using analytical distance or similarity measures to learning similarities through convolutional or graph neural networks . Finally, we discuss the current challenges and future directions of few-shot deep metric learning from the perspectives of effectiveness, optimization and applicability, and summarize their applications to real-world computer vision tasks.},
  archive      = {J_PR},
  author       = {Xiaoxu Li and Xiaochen Yang and Zhanyu Ma and Jing-Hao Xue},
  doi          = {10.1016/j.patcog.2023.109381},
  journal      = {Pattern Recognition},
  pages        = {109381},
  shortjournal = {Pattern Recognition},
  title        = {Deep metric learning for few-shot image classification: A review of recent developments},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust spherical principal curves. <em>PR</em>,
<em>138</em>, 109380. (<a
href="https://doi.org/10.1016/j.patcog.2023.109380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal curves are a nonlinear generalization of principal components and go through the mean of data lying in Euclidean space. In this paper, we propose L 1 L1 -type and Huber-type principal curves through the median of data to robustify the principal curves for a dataset that may contain outliers. We further investigate the stationarity of the proposed robust principal curves on S 2 S2 . Results from numerical experiments on S 2 S2 and S 4 S4 , including real data analysis, manifest promising empirical features of the proposed method.},
  archive      = {J_PR},
  author       = {Jongmin Lee and Hee-Seok Oh},
  doi          = {10.1016/j.patcog.2023.109380},
  journal      = {Pattern Recognition},
  pages        = {109380},
  shortjournal = {Pattern Recognition},
  title        = {Robust spherical principal curves},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving pseudo labels with intra-class similarity for
unsupervised domain adaptation. <em>PR</em>, <em>138</em>, 109379. (<a
href="https://doi.org/10.1016/j.patcog.2023.109379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) transfers knowledge from a label-rich source domain to a different but related fully-unlabeled target domain. To address the problem of domain shift, more and more UDA methods adopt pseudo labels of the target samples to improve the generalization ability on the target domain. However, inaccurate pseudo labels of the target samples may yield suboptimal performance with error accumulation during the optimization process. Moreover, once the pseudo labels are generated, how to remedy the generated pseudo labels is far from explored. In this paper, we propose a novel approach to improve the accuracy of the pseudo labels in the target domain. It first generates coarse pseudo labels by a conventional UDA method. Then, it iteratively exploits the intra-class similarity of the target samples for improving the generated coarse pseudo labels, and aligns the source and target domains with the improved pseudo labels. The accuracy improvement of the pseudo labels is made by first deleting dissimilar samples, and then using spanning trees to eliminate the samples with the wrong pseudo labels in the intra-class samples. We have applied the proposed approach to several conventional UDA methods as an additional term. Experimental results demonstrate that the proposed method can boost the accuracy of the pseudo labels and further lead to more discriminative and domain invariant features than the conventional baselines.},
  archive      = {J_PR},
  author       = {Jie Wang and Xiao-Lei Zhang},
  doi          = {10.1016/j.patcog.2023.109379},
  journal      = {Pattern Recognition},
  pages        = {109379},
  shortjournal = {Pattern Recognition},
  title        = {Improving pseudo labels with intra-class similarity for unsupervised domain adaptation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MFSJMI: Multi-label feature selection considering join
mutual information and interaction weight. <em>PR</em>, <em>138</em>,
109378. (<a href="https://doi.org/10.1016/j.patcog.2023.109378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection captures a reliable and informative feature subset from high-dimensional multi-label data, which plays an important role in pattern recognition. In conventional information-theoretical based multi-label feature selection methods, the high-order feature relevance between feature and label set is evaluated using low-order mutual information. However, existing methods do not establish the theoretical basis for the low-order approximation . To fill this gap, we first identify two underlying assumptions based on high-order label distribution: Label Independence Assumption (LIA) and Paired-label Independence Assumption (PIA). Second, we systematically analyze the strengths and weaknesses of two assumptions and introduce joint mutual information to satisfy more realistic label distribution. Furthermore, by decomposing joint mutual information, an interaction weight is proposed to consider multiple label correlations. Finally, a new method considering join mutual information and interaction weight is proposed. Comprehensive experiments demonstrate the effectiveness of the proposed method on various evaluation metrics .},
  archive      = {J_PR},
  author       = {Ping Zhang and Guixia Liu and Jiazhi Song},
  doi          = {10.1016/j.patcog.2023.109378},
  journal      = {Pattern Recognition},
  pages        = {109378},
  shortjournal = {Pattern Recognition},
  title        = {MFSJMI: Multi-label feature selection considering join mutual information and interaction weight},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascaded feature fusion with multi-level self-attention
mechanism for object detection. <em>PR</em>, <em>138</em>, 109377. (<a
href="https://doi.org/10.1016/j.patcog.2023.109377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has been a challenging task due to the complexity and diversity of objects. The emergence of self-attention mechanism provides a new clue for feature fusion in object detection task. Most existing self-attention mechanisms focus on extracting the correlation between global and local information in space or among channels, however it remains problematic issues of how to effectively fuse all those features. To address the above problems, we propose a Pooling and Global feature Fusion Self-attention Mechanism (PGFSM) to capture multi-level correlations among a variety of features, so as to perform cascaded aggregations upon them. PGFSM consists of three parts: Spatial Self-attention Pooling Fusion Module (SSPFM), Channel Self-attention Pooling Fusion Module (CSPFM), and Spatial and Channel Global Self-attention Fusion Module (SCGSFM). SSPFM and CSPFM respectively carried out in space and channel, extract the global maximum pooling and global average pooling self-attention features; SCGSFM extracts the spatial and channel fused characteristic relationship in the global. Finally, the three fused feature relations are added on the original feature to achieve an enhanced trait representation. In test, our PGFSM is embedded into YOLOv4, YOLOv5 , and EfficientDet network respectively, and evaluated in PASCAL VOC and MS COCO datasets. The experiment results show that the feature fusion self-attention mechanism improves the performance of object detection compared to each original framework and also the state-of-the-art modules, which proves the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Chuanxu Wang and Huiru Wang},
  doi          = {10.1016/j.patcog.2023.109377},
  journal      = {Pattern Recognition},
  pages        = {109377},
  shortjournal = {Pattern Recognition},
  title        = {Cascaded feature fusion with multi-level self-attention mechanism for object detection},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Independent vector analysis: Model, applications,
challenges. <em>PR</em>, <em>138</em>, 109376. (<a
href="https://doi.org/10.1016/j.patcog.2023.109376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper overviews an appealing unsupervised learning method named independent vector analysis (IVA) for its promising applications, such as in audio/speech signal separation, medical signal processing, remote sensing, video/image processing, wireless communication processing, and so on. As a useful data-driven technique in blind source separation (BSS) field, IVA has played an increasingly vital role in dealing with the problems of convolutive mixture separation, multivariate latent variable analysis and multivariate data fusion. IVA extends the conventional independent component analysis (ICA) to multidimensional components, which can result in more available information utilization. Compared with ICA mechanism, IVA is not only to utilize the statistical independence of multivariate signals but also the statistical inner dependency of each multivariate signal. With this generalization, IVA can manipulate some prominent ill-pose issues faced in sensor receiving models and has the advantage to overcome the inherent random permutation ambiguity problem in joint BSS. Motivated by the flexible and versatile technology superiorities of IVA, this paper concentrates on reviewing the IVA model in details, associated methods briefly, and its potential applications as well as prospects. Moreover, some significant open problems about IVA challenges are also discussed in this paper.},
  archive      = {J_PR},
  author       = {Zhongqiang Luo},
  doi          = {10.1016/j.patcog.2023.109376},
  journal      = {Pattern Recognition},
  pages        = {109376},
  shortjournal = {Pattern Recognition},
  title        = {Independent vector analysis: Model, applications, challenges},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time series clustering with an EM algorithm for mixtures of
linear gaussian state space models. <em>PR</em>, <em>138</em>, 109375.
(<a href="https://doi.org/10.1016/j.patcog.2023.109375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the task of clustering a set of individual time series while modeling each cluster, that is, model-based time series clustering. The task requires a parametric model with sufficient flexibility to describe the dynamics in various time series. To address this problem, we propose a novel model-based time series clustering method with mixtures of linear Gaussian state space models, which have high flexibility. The proposed method uses a new expectation-maximization algorithm for the mixture model to estimate the model parameters, and determines the number of clusters using the Bayesian information criterion. Experiments on a simulated dataset demonstrate the effectiveness of the method in clustering, parameter estimation, and model selection. The method is applied to real datasets commonly used to evaluate time series clustering methods. Results showed that the proposed method produces clustering results that are as accurate or more accurate than those obtained using previous methods.},
  archive      = {J_PR},
  author       = {Ryohei Umatani and Takashi Imai and Kaoru Kawamoto and Shutaro Kunimasa},
  doi          = {10.1016/j.patcog.2023.109375},
  journal      = {Pattern Recognition},
  pages        = {109375},
  shortjournal = {Pattern Recognition},
  title        = {Time series clustering with an EM algorithm for mixtures of linear gaussian state space models},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weight-guided class complementing for long-tailed image
recognition. <em>PR</em>, <em>138</em>, 109374. (<a
href="https://doi.org/10.1016/j.patcog.2023.109374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data are often long-tailed distributed and have plenty classes. This characteristic leads to a significant performance drop for various models. One reason behind that is the gradient shift caused by unsampled classes in each training iteration. In this paper, we propose a W eight- G uided C lass C omplementing framework to address this issue. Specifically, this framework first complements the unsampled classes in each training iteration by using a dynamic updated data slot. Then, considering the over-fitting issue caused by class complementing, we utilize the classifier weights as learned knowledge and encourage the model to discover more class specific characteristics. Finally, we design a weight refining scheme to deal with the long-tailed bias existing in classifier weights. Experimental results show that our framework can be implemented upon different existing approaches effectively, achieving consistent improvements on various benchmarks with new state-of-the-art performances. Codes will be released.},
  archive      = {J_PR},
  author       = {Xinqiao Zhao and Jimin Xiao and Siyue Yu and Hui Li and Bingfeng Zhang},
  doi          = {10.1016/j.patcog.2023.109374},
  journal      = {Pattern Recognition},
  pages        = {109374},
  shortjournal = {Pattern Recognition},
  title        = {Weight-guided class complementing for long-tailed image recognition},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A graph model-based multiscale feature fitting method for
unsupervised anomaly detection. <em>PR</em>, <em>138</em>, 109373. (<a
href="https://doi.org/10.1016/j.patcog.2023.109373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection and localization without prior knowledge is a challenging problem in industrial manufacturing due to the complexity and variety of anomaly types. Most of the existing methods have achieved considerable anomaly detection performance based on the distance between normal features and abnormal features. However, when the defect area is hard to distinguish from the background or the defect area is small, the distance between normal and abnormal features will be too close to detect anomaly areas. In addition, existing methods do not consider the influences of features in different layers with different anomaly sizes. In this paper, a graph model-based multiscale feature fitting method is proposed for unsupervised anomaly detection. Specifically, we build a graph model based on the K K nearest neighbors of an anchor image. The feature fitting and anomaly scores of the anchor images in the graph vertices are calculated next. Finally, a weighted multiscale anomaly map matching method is proposed to detect and locate the anomaly regions of test images. Compared with the state-of-the-art methods, our proposed method achieves competitive improvement in anomaly detection and localization on the MVTec AD dataset, the two KolektorSDD datasets, and the mSTC dataset.},
  archive      = {J_PR},
  author       = {Fanghui Zhang and Shichao Kan and Damin Zhang and Yigang Cen and Linna Zhang and Vladimir Mladenovic},
  doi          = {10.1016/j.patcog.2023.109373},
  journal      = {Pattern Recognition},
  pages        = {109373},
  shortjournal = {Pattern Recognition},
  title        = {A graph model-based multiscale feature fitting method for unsupervised anomaly detection},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Under the hood of transformer networks for trajectory
forecasting. <em>PR</em>, <em>138</em>, 109372. (<a
href="https://doi.org/10.1016/j.patcog.2023.109372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer Networks have established themselves as the de-facto state-of-the-art for trajectory forecasting but there is currently no systematic study on their capability to model the motion patterns of people, without interactions with other individuals nor the social context. There is abundant literature on LSTMs, CNNs and GANs on this subject. However methods adopting Transformer techniques achieve great performances by complex models and a clear analysis of their adoption as plain sequence models is missing. This paper proposes the first in-depth study of Transformer Networks (TF) and the Bidirectional Transformers (BERT) for the forecasting of the individual motion of people, without bells and whistles. We conduct an exhaustive evaluation of the input/output representations, problem formulations and sequence modelling, including a novel analysis of their capability to predict multi-modal futures. Out of comparative evaluation on the ETH+UCY benchmark, both TF and BERT are top performers in predicting individual motions and remain within a narrow margin wrt more complex techniques, including both social interactions and scene contexts. Source code will be released for all conducted experiments.},
  archive      = {J_PR},
  author       = {Luca Franco and Leonardo Placidi and Francesco Giuliari and Irtiza Hasan and Marco Cristani and Fabio Galasso},
  doi          = {10.1016/j.patcog.2023.109372},
  journal      = {Pattern Recognition},
  pages        = {109372},
  shortjournal = {Pattern Recognition},
  title        = {Under the hood of transformer networks for trajectory forecasting},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Capturing the few-shot class distribution: Transductive
distribution optimization. <em>PR</em>, <em>138</em>, 109371. (<a
href="https://doi.org/10.1016/j.patcog.2023.109371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is challenging since only a few labeled samples are available for training a learning model. To alleviate the data limitation problem in few-shot learning, several works try to generate samples or features by learning a model or distribution. But complex models and biased estimation of class distribution hamper their interpretability and generalization ability , respectively. In this work, we propose a generation based Transductive Distribution Optimization (TDO) method, which introduces neither extra parameters nor complex models. We use a few labeled samples and some high-confident unlabeled samples of the target set to capture the distributions of the few-shot classes, and then generate sufficient samples from them to augment the labeled inputs. Our method can work with most pre-trained feature extractors and outperforms state-of-the-art methods with a simple linear classifier . The visualization of the generated samples shows that our method can capture an accurate distribution even though the few labeled samples deviate from the ground-truth distribution.},
  archive      = {J_PR},
  author       = {Xinyue Liu and Ligang Liu and Han Liu and Xiaotong Zhang},
  doi          = {10.1016/j.patcog.2023.109371},
  journal      = {Pattern Recognition},
  pages        = {109371},
  shortjournal = {Pattern Recognition},
  title        = {Capturing the few-shot class distribution: Transductive distribution optimization},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning efficient facial landmark model for human
attractiveness analysis. <em>PR</em>, <em>138</em>, 109370. (<a
href="https://doi.org/10.1016/j.patcog.2023.109370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing geometric features on facial attractiveness analysis only focus on the ratios and distances, which is incomplete to represent all the information of a face. In this paper, we introduce a new category of feature, i.e., the angle features, to describe the angle of different organs such as the chin and eyes, which help boost the analysis performance in experiment. In addition, existing facial beauty analysis papers usually apply existing landmark models and extract their own different geometric feature sets on the landmarks. On the one hand, the geometric features are quite chaotic between different papers. On the other hand, most of the landmarks in the existing landmark model are useless for geometric feature extraction which wastes a lot of computational resources. To tackle these issues, we suggest to define a common geometric feature set and learn a special landmark model for attractiveness analysis. Specially, we collect all the available geometric features from the previous jobs and introduce a genetic feature selection algorithm to select the most effective geometric features. Furthermore, we introduce a special landmark model which exactly covers all the extracted geometric features. The experiments show that our method with the introduced angle features and the common feature set can outperform state-of-art facial beauty estimation methods with geometric features.},
  archive      = {J_PR},
  author       = {Tianhao Peng and Mu Li and Fangmei Chen and Yong Xu and David Zhang},
  doi          = {10.1016/j.patcog.2023.109370},
  journal      = {Pattern Recognition},
  pages        = {109370},
  shortjournal = {Pattern Recognition},
  title        = {Learning efficient facial landmark model for human attractiveness analysis},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised person re-identification via multi-domain joint
learning. <em>PR</em>, <em>138</em>, 109369. (<a
href="https://doi.org/10.1016/j.patcog.2023.109369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have achieved impressive progress in the task of person re-identification. However, how to generalize a learned model from the source domain to the target domain remains a long-standing challenge. Inspired by the fact that the enrichment of data diversity and the utilization of miscellaneous semantic features can lead to better generalization ability , we design a model that integrates a novel data augmentation method with a multi-label assignment strategy to achieve semantic features decoupling in the source domain. The pre-trained model is employed to extract several kinds of semantic features from the target dataset, and each kind of semantic features is regarded as a specific domain. We then cluster features of each domain and exploit the connection between different clustering results to perform self-distillation for generating more reliable pseudo labels. Finally, the obtained pseudo labels are used to fine-tune the pre-trained model to achieve model transfer from the source domain to the target one. Extensive experiments demonstrate that our approach outperforms some state-of-the-art methods by a clear margin and even surpass some supervised methods. Source code is available at: https://www.github.com/flychen321/MDJL .},
  archive      = {J_PR},
  author       = {Feng Chen and Nian Wang and Jun Tang and Pu Yan and Jun Yu},
  doi          = {10.1016/j.patcog.2023.109369},
  journal      = {Pattern Recognition},
  pages        = {109369},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised person re-identification via multi-domain joint learning},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Expression snippet transformer for robust video-based
facial expression recognition. <em>PR</em>, <em>138</em>, 109368. (<a
href="https://doi.org/10.1016/j.patcog.2023.109368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Transformer can be powerful for modeling visual relations and describing complicated patterns, it could still perform unsatisfactorily for video-based facial expression recognition, since the expression movements in a video can be too small to reflect meaningful spatial-temporal relations. To this end, we propose to decompose the modeling of expression movements of a video into the modeling of a series of expression snippets, each of which contains a few frames, and then boost the Transformer’s ability for intra-snippet and inter-snippet visual modeling, respectively, obtaining the Expression snippet Transformer (EST). For intra-snippet modeling, we devise an attention-augmented snippet feature extractor to enhance the encoding of subtle facial movements of each snippet. For inter-snippet modeling, we introduce a shuffled snippet order prediction head and a corresponding loss to improve the modeling of subtle motion changes across subsequent snippets. The EST obtains state-of-the-art performance, demonstrating its superiority to other CNN-based methods. Our code and the trained model are available at https://github.com/DreamMr/EST},
  archive      = {J_PR},
  author       = {Yuanyuan Liu and Wenbin Wang and Chuanxu Feng and Haoyu Zhang and Zhe Chen and Yibing Zhan},
  doi          = {10.1016/j.patcog.2023.109368},
  journal      = {Pattern Recognition},
  pages        = {109368},
  shortjournal = {Pattern Recognition},
  title        = {Expression snippet transformer for robust video-based facial expression recognition},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A single-stage point cloud cleaning network for outlier
removal and denoising. <em>PR</em>, <em>138</em>, 109366. (<a
href="https://doi.org/10.1016/j.patcog.2023.109366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a simple, flexible and effective representation for objects, 3D point cloud has attracted more and more attention in recent years. However, raw point clouds obtained from 3D scanners or image-based reconstruction techniques are often contaminated with noise and outliers, which hinders downstream tasks such as object classification, surface reconstruction, and so on. Therefore, point cloud cleaning, i.e., removing noisy points and outliers from raw point cloud, is a prior step of most geometry processing workflows. The exiting techniques for point cloud cleaning usually include two stages, that is, discarding outliers at first, and then denoising the resulting point cloud. This two-stage process usually requires two different models, which is cumbersome to train and use. To solve this problem, a novel data driven method, named SSPCN (single-stage point cloud cleaning network), is proposed in this paper. SSPCN can simultaneously remove outliers and denoise a point cloud in a single model. Specifically, SSPCN is consisted of adaptive downsampling module, feature compensation module, upsampling module and coordinate reconstruction module. Given a raw point cloud as input, the downsampling module is first used to obtain a prefiltered point cloud subset and learn initial features of the subset. The feature compensation module is then utilized to learn accurate features from initial features. Next, the upsampling module upsamples the features to restore the original size of the point cloud. Last, the coordinate reconstruction module generates a cleaned point cloud from upsampled features. SSPCN is validated both on synthetic and real scanned data. Extensive experiments demonstrate that SSPCN outperforms state-of-the-art point cloud cleaning techniques in terms of quantitative metric and visual quality.},
  archive      = {J_PR},
  author       = {Ying Li and Huankun Sheng},
  doi          = {10.1016/j.patcog.2023.109366},
  journal      = {Pattern Recognition},
  pages        = {109366},
  shortjournal = {Pattern Recognition},
  title        = {A single-stage point cloud cleaning network for outlier removal and denoising},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AFI-GAN: Improving feature interpolation of feature pyramid
networks via adversarial training for object detection. <em>PR</em>,
<em>138</em>, 109365. (<a
href="https://doi.org/10.1016/j.patcog.2023.109365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent convolutional detectors learn strong semantic features by generating and combining multi-scale features via feature interpolation. However, simple interpolation incurs often noisy and blurred features. To resolve this, we propose a novel adversarially-trained interpolator which can substitute for the traditional interpolation effortlessly. In specific, we design AFI-GAN consisting of an AF interpolator and a feature patch discriminator . In addition, we present a progressive adversarial learning and AFI-GAN losses to generate multi-scale features for downstream detection tasks. However, we can also finetune the proposed AFI-GAN with the recent multi-scale detectors without the adversarial learning once a pre-trained AF interpolator is provided. We prove the effectiveness and flexibility of our AF interpolator, and achieve the better box and mask APs by 2.2\% and 1.6\% on average compared to using other interpolation. Moreover, we achieve an impressive detection score of 57.3\% mAP on the MSCOCO dataset. Code is available at https://github.com/inhavl-shlee/AFI-GAN .},
  archive      = {J_PR},
  author       = {Seong-Ho Lee and Seung-Hwan Bae},
  doi          = {10.1016/j.patcog.2023.109365},
  journal      = {Pattern Recognition},
  pages        = {109365},
  shortjournal = {Pattern Recognition},
  title        = {AFI-GAN: Improving feature interpolation of feature pyramid networks via adversarial training for object detection},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MinEnt: Minimum entropy for self-supervised representation
learning. <em>PR</em>, <em>138</em>, 109364. (<a
href="https://doi.org/10.1016/j.patcog.2023.109364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised representation learning is becoming more and more popular due to its superior performance. According to the information entropy theory, the smaller the information entropy of a feature, the more certain it is and the less redundant it is. Based on this, we propose a simple yet effective self-supervised representation learning method via Minimum Entropy (MinEnt). From the perspective of reducing information entropy, our MinEnt takes the output of the projector towards its nearest minimum entropy as the optimization target. The core of our MinEnt consists of three important steps: 1) normalize along the batch dimension to avoid model collapse, 2) compute the nearest minimum entropy to get the target, 3) compute the loss and backpropagate to optimize the network. Our MinEnt can learn efficient representations, even without the need for techniques such as negative sample pairs, predictors, momentum encoders, cross-correlation matrices, etc. Experimental results on four widely used datasets show that our method achieves competitive results in a simple manner.},
  archive      = {J_PR},
  author       = {Shuo Li and Fang Liu and Zehua Hao and Licheng Jiao and Xu Liu and Yuwei Guo},
  doi          = {10.1016/j.patcog.2023.109364},
  journal      = {Pattern Recognition},
  pages        = {109364},
  shortjournal = {Pattern Recognition},
  title        = {MinEnt: Minimum entropy for self-supervised representation learning},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lambertian-based adversarial attacks on deep-learning-based
underwater side-scan sonar image classification. <em>PR</em>,
<em>138</em>, 109363. (<a
href="https://doi.org/10.1016/j.patcog.2023.109363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) are extensively applied to the classification tasks for Side-scan sonar (SSS) images. However, state-of-the-art neural networks are prone to be confused by adversarial attacks that generate a tiny modification of the images, threatening the security of SSS classification. The robustness of CNN to adversarial attacks can be improved by introducing adversarial examples through adversarial training. Practical adversarial examples are often generated from elaborate adversarial attackers. For the underwater scenario of sonar, a specially designed adversarial attack method to weaken SSS image classification can make the research community better understand the weakness of CNN in this scenario and improve the security measures in a well-directed way. Thus, exploring adversarial attack methods for SSS image classification is essential. Nevertheless, the existing adversarial attack methods are designed for optical images, reflecting no physical characteristics of sonar images . To fill this gap and investigate the adversarial attack related to real-world conditions, in this paper, we propose an adversarial attack method named Lambertian Adversarial Sonar Attack (LASA). It initially leverages the Lambertian model to simulate the formation of the SSS image, factoring the image to three parameters, then updates the parameters on the direction of gradients by the chain rule. Finally, the parameters regenerate the adversarial example to fool the classifier. To validate the performance of LASA, we constructed a diversified SSS image dataset containing three categories. On our dataset, LASA reduces the Top-1 accuracy of a well-trained ResNet-101 to 7.31\% ± 0.21 7.31\%±0.21 (one-shot version) and 0.00\% (iterative version), the success rate of targeted attack reaches 97.03 ± 2.24 97.03±2.24 , far beyond the performance of the existing state-of-the-art adversarial attack methods. Meanwhile, we show that the adversarial training using examples generated from LASA makes the classifier more robust. We expect that our methods can be applied as a benchmark of adversarial attacks on SSS images, motivating future research to design novel neural networks or defensive methods to resist real-world adversarial attacks on SSS images.},
  archive      = {J_PR},
  author       = {Qixiang Ma and Longyu Jiang and Wenxue Yu},
  doi          = {10.1016/j.patcog.2023.109363},
  journal      = {Pattern Recognition},
  pages        = {109363},
  shortjournal = {Pattern Recognition},
  title        = {Lambertian-based adversarial attacks on deep-learning-based underwater side-scan sonar image classification},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surface normal and gaussian weight constraints for indoor
depth structure completion. <em>PR</em>, <em>138</em>, 109362. (<a
href="https://doi.org/10.1016/j.patcog.2023.109362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raw depth maps captured by depth sensors generally contain missing contents due to glossy, transparent, and sparsity problems. Recent methods well completed flat regions of raw depth maps; however, ignored the accuracy of depth structures. In this paper, an effective depth structure completion method is developed to infer missing depth structures. First, a raw depth map is divided into flat regions and depth structures based on a structure prediction network. Second, two local features including surface normals and Gaussian weights are extracted from a reference RGB image to impose constraints on flat regions and depth structures, separately. Third, a kernel least-square module is adopted to handle the texture-copy artifacts problem. Finally, an iterative optimization model is developed by embedding the two constraints into a Markov random field. The cost function of the model comprises three terms, which limit data fidelity between completed depth map and raw depth map, smoothness of flat regions, and accuracy of depth structures, respectively. The proposed method is evaluated on four indoor datasets including Matterport3D, RealSense, ScanNet, and NYUv2, and compared with eight recent baselines. Quantitative results demonstrate that RMSE and MAE of completed depth maps are considerably reduced by 22.0\% and 45.3\%, respectively. Visual results show the superiority in completing depth structures and suppressing texture-copy artifacts. Generalization test verify the effectiveness on unseen datasets.},
  archive      = {J_PR},
  author       = {Dongran Ren and Meng Yang and Jiangfan Wu and Nanning Zheng},
  doi          = {10.1016/j.patcog.2023.109362},
  journal      = {Pattern Recognition},
  pages        = {109362},
  shortjournal = {Pattern Recognition},
  title        = {Surface normal and gaussian weight constraints for indoor depth structure completion},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refined edge detection with cascaded and high-resolution
convolutional network. <em>PR</em>, <em>138</em>, 109361. (<a
href="https://doi.org/10.1016/j.patcog.2023.109361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection is represented as one of the most challenging tasks in computer vision , due to the complexity of detecting the edges or boundaries in real-world images that contains objects of different types and scales like trees, building as well as various backgrounds. Edge detection is represented also as a key task for many computer vision applications. Using a set of backbones as well as attention modules, deep-learning-based methods improved the detection of edges compared with traditional methods like Sobel or Canny. However, images of complex scenes still represent a challenge for these methods. Also, the detected edges using the existing approaches suffer from non-refined results with erroneous edges. In this paper, we attempted to overcome these challenges for refined edge detection using a cascaded and high-resolution network named (CHRNet). By maintaining the high resolution of edges during the training process, and conserving the resolution of the edge image during the network stage, sub-blocks are connected at every stage with the output of the previous layer. Also, after each layer, we use batch normalization layer with an active affine parameter as an erosion operation for the homogeneous region in the image. The proposed method is evaluated using the most challenging datasets including BSDS500, NYUD, and Multicue. The obtained results outperform the designed edge detection networks in terms of performance metrics and quality of output images.The code is available at: https://github.com/elharroussomar/chrnet/},
  archive      = {J_PR},
  author       = {Omar Elharrouss and Youssef Hmamouche and Assia Kamal Idrissi and Btissam El Khamlichi and Amal El Fallah-Seghrouchni},
  doi          = {10.1016/j.patcog.2023.109361},
  journal      = {Pattern Recognition},
  pages        = {109361},
  shortjournal = {Pattern Recognition},
  title        = {Refined edge detection with cascaded and high-resolution convolutional network},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Versatile recurrent neural network for wide types of video
restoration. <em>PR</em>, <em>138</em>, 109360. (<a
href="https://doi.org/10.1016/j.patcog.2023.109360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video shooting of natural scenes often suffers from various serious degradation, such as motion blur , impact of atmospheric turbulence , random noise and resolution reduction , etc. Different from the maturity of image restoration research, video restoration is much more complicated so that it lacks effective general method. Here, we present a versatile recurrent neural network (VRNN) to handle wide types of video degradation and generate stable videos with ideal clarity. We complete the design of VRNN through deducing a general video restoration paradigm that reveals the importance of simultaneously utilizing past and future information for restoring current frame. Specifically, we propose a novel RNN cell in which hidden state flows in bidirections, enriching temporal information contained in the extracted features. Furthermore, a feature fusion module involves temporal and spatial attention processing is designed to refine features of neighbouring frames and help reconstruct current frame. Extensive experiments on well-known public datasets (including four different kinds of video restoration tasks, with a total of 35,666 videos and 515,774 frames) show that the proposed VRNN achieves 1–4 dB of PSNR increasing or several times less of computational complexity in all tasks against state-of-the-art methods, manifesting the versatile and efficient ability of proposed VRNN in wide types of video restoration.},
  archive      = {J_PR},
  author       = {Yadong Wang and Xiangzhi Bai},
  doi          = {10.1016/j.patcog.2023.109360},
  journal      = {Pattern Recognition},
  pages        = {109360},
  shortjournal = {Pattern Recognition},
  title        = {Versatile recurrent neural network for wide types of video restoration},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-learning for dynamic tuning of active learning on
stream classification. <em>PR</em>, <em>138</em>, 109359. (<a
href="https://doi.org/10.1016/j.patcog.2023.109359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised data stream learning depends on the incoming sample’s true label to update a classifier’s model. In real life, obtaining the ground truth for each instance is a challenging process; it is highly costly and time consuming. Active Learning has already bridged this gap by finding a reduced set of instances to support the creation of a reliable stream classifier. However, identifying a reduced number of informative instances to support a suitable classifier update and drift adaptation is very tricky. To better adapt to concept drifts using a reduced number of samples, we propose an online tuning of the Uncertainty Sampling threshold using a meta-learning approach. Our approach exploits statistical meta-features from adaptive windows to meta-recommend a suitable threshold to address the trade-off between the number of labelling queries and high accuracy. Experiments exposed that the proposed approach provides the best trade-off between accuracy and query reduction by dynamic tuning the uncertainty threshold using lightweight meta-features.},
  archive      = {J_PR},
  author       = {Vinicius Eiji Martins and Alberto Cano and Sylvio Barbon Junior},
  doi          = {10.1016/j.patcog.2023.109359},
  journal      = {Pattern Recognition},
  pages        = {109359},
  shortjournal = {Pattern Recognition},
  title        = {Meta-learning for dynamic tuning of active learning on stream classification},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing egocentric 3D pose estimation with third person
views. <em>PR</em>, <em>138</em>, 109358. (<a
href="https://doi.org/10.1016/j.patcog.2023.109358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach to enhance the 3D body pose estimation of a person computed from videos captured from a single wearable camera . The main technical contribution consists of leveraging high-level features linking first- and third-views in a joint embedding space. To learn such embedding space we introduce First2Third-Pose , a new paired synchronized dataset of nearly 2000 videos depicting human activities captured from both first- and third-view perspectives. We explicitly consider spatial- and motion-domain features, combined using a semi-Siamese architecture trained in a self-supervised fashion. Experimental results demonstrate that the joint multi-view embedded space learned with our dataset is useful to extract discriminatory features from arbitrary single-view egocentric videos, with no need to perform any sort of domain adaptation or knowledge of camera parameters. An extensive evaluation demonstrates that we achieve significant improvement in egocentric 3D body pose estimation performance on two unconstrained datasets, over three supervised state-of-the-art approaches. The collected dataset and pre-trained model are available for research purposes. 1},
  archive      = {J_PR},
  author       = {Ameya Dhamanaskar and Mariella Dimiccoli and Enric Corona and Albert Pumarola and Francesc Moreno-Noguer},
  doi          = {10.1016/j.patcog.2023.109358},
  journal      = {Pattern Recognition},
  pages        = {109358},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing egocentric 3D pose estimation with third person views},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-dimensional multi-label classification: Towards
encompassing heterogeneous label spaces and multi-label annotations.
<em>PR</em>, <em>138</em>, 109357. (<a
href="https://doi.org/10.1016/j.patcog.2023.109357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional classification framework, the semantics of each object is usually characterized by annotating a single class label from one homogeneous label space. Nonetheless, objects with rich semantics naturally arise in real-world applications whose properties need to be characterized in a more sophisticated manner. In this paper, a new classification framework named Multi-Dimensional Multi-Label (MDML) classification is investigated which models objects with rich semantics by encompassing heterogeneous label spaces and multi-label annotations. Specifically, MDML generalizes the traditional classification framework by assuming a number of heterogeneous label spaces to characterize semantics from different dimensions, where each object is further annotated with multiple class labels from each heterogeneous label space. To learn from MDML training examples, a first attempt named CLIM is proposed based on an augmented stacking strategy. Firstly, CLIM induces a base multi-label predictive model w.r.t. each label space by maximizing the likelihood of the observed multiple class labels. Secondly, the thresholding predictions from all base models are used to augment the original feature space to yield stacked multi-label predictive models. The two-level models are refined alternately via empirical threshold tuning. Experiments on four real-world MDML data sets validate the effectiveness of CLIM in learning from training examples with heterogeneous label spaces and multi-label annotations.},
  archive      = {J_PR},
  author       = {Bin-Bin Jia and Min-Ling Zhang},
  doi          = {10.1016/j.patcog.2023.109357},
  journal      = {Pattern Recognition},
  pages        = {109357},
  shortjournal = {Pattern Recognition},
  title        = {Multi-dimensional multi-label classification: Towards encompassing heterogeneous label spaces and multi-label annotations},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Semi-supervised vector-valued learning: Improved bounds and
algorithms. <em>PR</em>, <em>138</em>, 109356. (<a
href="https://doi.org/10.1016/j.patcog.2023.109356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning . Using local Rademacher complexity and unlabeled data , we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization . Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.},
  archive      = {J_PR},
  author       = {Jian Li and Yong Liu and Weiping Wang},
  doi          = {10.1016/j.patcog.2023.109356},
  journal      = {Pattern Recognition},
  pages        = {109356},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised vector-valued learning: Improved bounds and algorithms},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On a method for detecting periods and repeating patterns in
time series data with autocorrelation and function approximation.
<em>PR</em>, <em>138</em>, 109355. (<a
href="https://doi.org/10.1016/j.patcog.2023.109355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting recurrent patterns in time series data is an important capability. The reason is that repeating patterns on the one hand indicate well defined processes that can be further analyzed once detected and on the other hand are a reliable feature to predict future occurrences and adapt accordingly. The challenge in real data to define a period is that a time series is usually also influenced by non-periodic dynamics and noise. In this work, a mathematical framework is proved to define regular patterns. Their properties are used within a suggested algorithm based on the concept of autocorrelation and function approximation to fit a model capturing the periodic part of the time series. Based on that model and a corresponding autocorrelation, a new score is defined to evaluate how well a hypothesized period fits to the time series. This score is particularly useful in a big data scenario where decisions for periodicity are needed to be taken automatically, which is one of the main achievement of the presented work. The period analysis algorithm is applied to data from two different use cases. The first one is a data center scenario where the information of the periodic pattern is used to create a feature that improves a machine learning framework predicting future resource demands. The feature represents the phase of the repeating pattern. In a second scenario, expression data from mice liver cells are investigated concerning periodic rhythms. A Python implementation of the presented algorithm is provided via a github repository under https://github.com/LauritzR/period-detection .},
  archive      = {J_PR},
  author       = {Tim Breitenbach and Bartosz Wilkusz and Lauritz Rasbach and Patrick Jahnke},
  doi          = {10.1016/j.patcog.2023.109355},
  journal      = {Pattern Recognition},
  pages        = {109355},
  shortjournal = {Pattern Recognition},
  title        = {On a method for detecting periods and repeating patterns in time series data with autocorrelation and function approximation},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Self-structured pyramid network with parallel
spatial-channel attention for change detection in VHR remote sensed
imagery. <em>PR</em>, <em>138</em>, 109354. (<a
href="https://doi.org/10.1016/j.patcog.2023.109354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land cover change detection (CD) in very-high-resolution (VHR) images is still impeded by weak pattern separability and land cover complexity. To address these challenges, a self-structured pyramid network (S 2 2 PNet) with a parallel spatial-channel attention mechanism (PSAM) and a self-structured feature pyramid (SFP) is proposed for a finer annotation of changed land cover. The proposed PSAM refines the features of different levels in dual-branch coordinated by running parallel without mutual influence for a better recognition of varied objects, which can lead to less incorrectly detected land cover. And the SFP integrates the embedded multi-scale features to acquire an improved cognition over multi-scale objects, which can contribute to a more complete annotation over diverse objects. All-round experiments over several widely used open large-scale VHR CD data sets are carried out, which indicate the efficiency and effectiveness of the proposed method. Related comparisons suggest that the proposed method can achieve higher performance over several existing state-of-the-art CD methods . The source codes will be released at https://github.com/HaiXing-1998/S2PNet-CD .},
  archive      = {J_PR},
  author       = {Mingyang Zhang and Hanhong Zheng and Maoguo Gong and Yue Wu and Hao Li and Xiangming Jiang},
  doi          = {10.1016/j.patcog.2023.109354},
  journal      = {Pattern Recognition},
  pages        = {109354},
  shortjournal = {Pattern Recognition},
  title        = {Self-structured pyramid network with parallel spatial-channel attention for change detection in VHR remote sensed imagery},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized multi-aspect distance metric for mixed-type
data clustering. <em>PR</em>, <em>138</em>, 109353. (<a
href="https://doi.org/10.1016/j.patcog.2023.109353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance calculation is straightforward when working with pure categorical or pure numerical data sets. Defining a unified distance to improve the clustering performance for a mixed data set composed of nominal, ordinal, and numerical attributes is very challenging due to the attributes’ different natures. In this study, we proposed a new measure of distance for a mixed-type data set that regards inter-attribute information and intra-attribute information depending on the type of attributes. In this regard, entropy and Jensen–Shannon divergence concepts were used to exploit the inter-attribute information of categorical-categorical and categorical-numerical attributes, respectively. Also, a modified version of Mahalanobis distance was proposed to consider the intra- and inter-attribute information of numerical attributes. We also introduced a unified framework based on mutual information to control attributes’ contribution to distance measurement. The proposed distance in conjunction with spectral clustering was extensively evaluated concerning various categorical, numerical, and mixed-type benchmark data sets, and the results demonstrated the efficacy of the proposed method.},
  archive      = {J_PR},
  author       = {Elahe Mousavi and Mohammadreza Sehhati},
  doi          = {10.1016/j.patcog.2023.109353},
  journal      = {Pattern Recognition},
  pages        = {109353},
  shortjournal = {Pattern Recognition},
  title        = {A generalized multi-aspect distance metric for mixed-type data clustering},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving generalization of double low-rank representation
using schatten-p norm. <em>PR</em>, <em>138</em>, 109352. (<a
href="https://doi.org/10.1016/j.patcog.2023.109352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank representation reveals a highly-informative entailment of sparse matrices, where double low-rank representation (DLRR) presents an effective solution by adopting nuclear norm . However, it is a special constraint of Schatten- p p norm with p = 1 p=1 which equally treats all singular values , deviating from the optimal low-rank representation that considers p = 0 p=0 . Thus, this paper improves the DLRR generalization of DLRR by relaxing p = 1 p=1 into 0 0&amp;lt;p≤1 to tighten the low-rank constraint of the Schatten- p p norm. With such a relaxation, low-rank optimization is then accelerated, resulting in a lower bound on the calculation complexity. Experiments on unsupervised feature extraction and subspace clustering demonstrate that our low-rank optimization taking 0 0&amp;lt;p≤1 achieves a superior performance against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jiaoyan Zhao and Yongsheng Liang and Shuangyan Yi and Qiangqiang Shen and Xiaofeng Cao},
  doi          = {10.1016/j.patcog.2023.109352},
  journal      = {Pattern Recognition},
  pages        = {109352},
  shortjournal = {Pattern Recognition},
  title        = {Improving generalization of double low-rank representation using schatten-p norm},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On a linear fused gromov-wasserstein distance for graph
structured data. <em>PR</em>, <em>138</em>, 109351. (<a
href="https://doi.org/10.1016/j.patcog.2023.109351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework for embedding graph structured data into a vector space, taking into account node features and structures of graphs into the optimal transport (OT) problem. Then we propose a novel distance between two graphs, named LinearFGW , defined as the Euclidean distance between their embeddings. The advantages of the proposed distance are twofold: 1) it takes into account node features and structures of graphs for measuring the dissimilarity between graphs in a kernel-based framework, 2) it is more efficient for computing a kernel matrix than pairwise OT-based distances, particularly fused Gromov-Wasserstein [1], making it possible to deal with large-scale data sets. Our theoretical analysis and experimental results demonstrate that our proposed distance leads to an increase in performance compared to the existing state-of-the-art graph distances when evaluated on graph classification and clustering tasks.},
  archive      = {J_PR},
  author       = {Dai Hai Nguyen and Koji Tsuda},
  doi          = {10.1016/j.patcog.2023.109351},
  journal      = {Pattern Recognition},
  pages        = {109351},
  shortjournal = {Pattern Recognition},
  title        = {On a linear fused gromov-wasserstein distance for graph structured data},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised clustering with assistance from
off-the-shelf classifier. <em>PR</em>, <em>138</em>, 109350. (<a
href="https://doi.org/10.1016/j.patcog.2023.109350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering outperforms conventional clustering by mutually promoting representation learning and cluster assignment. However, most existing deep clustering methods suffer from two major drawbacks. Firstly, most cluster assignment methods are highly dependent on the intermediate target distribution generated by a handcrafted nonlinear mapping function. Secondly, the clustering results can be easily guided towards wrong direction by the misassigned samples in each cluster. The existing deep clustering methods are incapable of discriminating such samples. These facts largely limit the possible performance that deep clustering methods can reach. To address these issues, a novel Self-Supervised Clustering (SSC) framework is constructed, which boosts the clustering performance by classification in an unsupervised manner . Fuzzy theory is used to score the membership of each sample to the clusters in terms of probability in each training epoch, which evaluates the intermediate clustering result certainty of each sample. The most reliable samples can be selected with the help of a sample selection method according to the membership and enhanced by data augmentation method. These augmented data are employed to fine-tune an off-the-shelf deep network classifier with the labels provided by the clustering in a self-supervised way. The classification results of the original dataset are used as the target distribution to guide the training process of the deep clustering model. The proposed framework can efficiently discriminate sample outliers and generate better target distribution with the assistance of the powerful classifier. Extensive experiments indicate that the proposed framework remarkably outperforms state-of-the-art deep clustering methods on four benchmark datasets.},
  archive      = {J_PR},
  author       = {Hanxuan Wang and Na Lu and Huan Luo and Qinyang Liu},
  doi          = {10.1016/j.patcog.2023.109350},
  journal      = {Pattern Recognition},
  pages        = {109350},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised clustering with assistance from off-the-shelf classifier},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-taught multi-view spectral clustering. <em>PR</em>,
<em>138</em>, 109349. (<a
href="https://doi.org/10.1016/j.patcog.2023.109349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By integrating multiple views, i.e., multi-view learning (ML), we can discover the underlying data structures so that the performance of learning tasks can improve. As a basic and important branch of ML, multi-view clustering has achieved great success recently in pattern recognition and machine learning communities. Most existing multi-view spectral clustering methods heavily adopt the relax-and-discretize strategy to obtain discrete cluster labels (clustering results), i.e., using predefined similarity graphs to learn a consensus Laplacian embedding shared by all views for K K -means clustering. However, the above clustering strategy may significantly affect clustering performance since there is information loss between independent steps. In this paper, we establish a novel Self-taught Multi-view Spectral Clustering (SMSC) framework to address the above issue. As the main contributions of this paper, we provide two versions of SMSC based on convex combination and centroid graph fusion schemes. Specifically, a self-taught mechanism is introduced in SMSC, which can effectively feedback the manifold structure induced by Laplacian embedding and the cluster information hidden in the discrete indicator matrix to learn an optimal consensus similarity graph for graph partitioning . The effectiveness of the proposed methods has been evaluated on real-world multi-view datasets, and experimental results show that our methods outperform other state-of-the-art baselines.},
  archive      = {J_PR},
  author       = {Guo Zhong and Chi-Man Pun},
  doi          = {10.1016/j.patcog.2023.109349},
  journal      = {Pattern Recognition},
  pages        = {109349},
  shortjournal = {Pattern Recognition},
  title        = {Self-taught multi-view spectral clustering},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse possibilistic c-means clustering with lasso.
<em>PR</em>, <em>138</em>, 109348. (<a
href="https://doi.org/10.1016/j.patcog.2023.109348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Krishnapuram and Keller first proposed possibilistic c-means (PCM) clustering in 1993. Afterward, PCM was widely studied with various extensions. The PCM algorithm and its extensions always treat feature components under equal importance, but, in real applications, different features may better have different weights. Recently, Yang and Benjamin in 2021 proposed a feature-weighted PCM clustering with feature reduction. Although Yang and Benjamin (2021) can reduce feature dimensions, it still encounters the curse of dimensionality for high dimensional data . One possible way to address this problem is to conduct a sparse clustering technique . In this paper, we further study the PCM clustering by incorporating the idea of sparsity with different feature weights. We propose two approaches that use the PCM clustering with the least absolute shrinkage and selection operator (Lasso). The first one is the sparse PCM subject to a Lasso constraint of feature weights, called S-PCM1. The second is the sparse PCM by adding a Lasso penalty term of feature weights in the objective function, called S-PCM2. We show that S-PCM1 and S-PCM2 are theoretically the same, and both can induce sparsity in features, but they use different procedures in algorithms. Synthetic and real data sets are used to compare S-PCM1 and S-PCM2 with some existing sparsity clustering algorithms . Experimental results and comparisons demonstrate the effectiveness and usefulness of the proposed S-PCM1 and S-PCM2 clustering algorithms .},
  archive      = {J_PR},
  author       = {Miin-Shen Yang and Josephine B.M. Benjamin},
  doi          = {10.1016/j.patcog.2023.109348},
  journal      = {Pattern Recognition},
  pages        = {109348},
  shortjournal = {Pattern Recognition},
  title        = {Sparse possibilistic c-means clustering with lasso},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised class-to-class translation for domain
variations. <em>PR</em>, <em>138</em>, 109346. (<a
href="https://doi.org/10.1016/j.patcog.2023.109346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of image-to-image translation models tend to struggle in varying domain settings. For one varying domain, samples vary significantly in shape and size and have no domain labels. This paper proposes an unsupervised class-to-class translation model based on conditional contrastive learning to tackle the domain variations problem. The initial hypothesis is that the latent modalities of two varying domains are categorizable by style differences of different samples and turn the image-to-image translation problem into class-to-class translation. Firstly, unsupervised semantic clustering is performed for each domain to divide them into multiple classes and then leverage the classification features of different classes to perform class-to-class translation. Two conditional contrastive learning loss functions for each domain are proposed to perform unsupervised semantic clustering and decompose it into multiple classes. Then in the class-to-class translation stage, the classification features of different classes are employed to learn the latent modalities. The proposed model outperforms state-of-the-art baseline methods by employing the latent modalities of different classes. The sample code is available at https://github.com/c1a1o1/ucct .},
  archive      = {J_PR},
  author       = {Zhiyi Cao and Wei Wang and Lina Huo and Shaozhang Niu},
  doi          = {10.1016/j.patcog.2023.109346},
  journal      = {Pattern Recognition},
  pages        = {109346},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised class-to-class translation for domain variations},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiview jointly sparse discriminant common subspace
learning. <em>PR</em>, <em>138</em>, 109342. (<a
href="https://doi.org/10.1016/j.patcog.2023.109342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview data leads to the demand for classifying samples from various views, and the large gap between different views makes the classification task challenging. Recently, researchers have extended linear discriminant analysis (LDA) to multi-view scenarios. However, the extended methods are generally associated with the small-class problem, that is, the projection size is limited by the number of classes. In addition, they are sensitive to variations in images or outliers. To solve these problems, this study proposes a generalized robust multiview discriminant analysis (GRMDA) to obtain a linear transform for each view and for learning multiview jointly sparse discriminant common subspace. GRMDA aims to achieve both maximal between-class and minimal within-class variation for data from multiple views in a common space. Instead of formulating the ratio trace problem, we reformulate GRMDA inspired by maximum margin criterion (MMC) to address the small-class problem. Moreover, the proposed method achieves stronger robustness by reconstructing the within-class and between-class scatter terms from the definition of L 2 , 1 L2,1 norm. Furthermore, GRMDA ensures joint sparsity using the L 2 , 1 L2,1 norm-based regularization term. Additionally, we present an iterative algorithm , convergence proof , and complexity analysis. Experiments on six popular databases, that is, COIL100, USPS/MNIST, Extended Yale Face B, AR, BBCSport, and multiple feature datasets, were conducted to evaluate the performance of GRMDA against the state-of-the-art multiview methods. The experimental results demonstrate that the proposed method can achieve a significant performance with strong robustness and fast convergence.},
  archive      = {J_PR},
  author       = {Yiling Lin and Zhihui Lai and Jie Zhou and Jiajun Wen and Heng Kong},
  doi          = {10.1016/j.patcog.2023.109342},
  journal      = {Pattern Recognition},
  pages        = {109342},
  shortjournal = {Pattern Recognition},
  title        = {Multiview jointly sparse discriminant common subspace learning},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust image clustering via context-aware contrastive graph
learning. <em>PR</em>, <em>138</em>, 109340. (<a
href="https://doi.org/10.1016/j.patcog.2023.109340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolution networks (GCN) have recently become popular for image clustering. However, existing GCN-based image clustering techniques focus on learning image neighbourhoods which leads to poor reasoning on the cluster boundaries. To address this challenge, we propose a supervised image clustering approach based on contrastive graph learning (CGL). Our method generates an influential graph view (IGV) and a topological graph view (TGV) for each class to represent its global context from different viewpoints. These generated graph views are used to reason the inter-cluster relationships and intra-cluster boundaries from the local context of each node in a contrastive manner. Our method considers each class as a fully connected graph to explore its characteristics and strategically generate directional graph views. This enhances the transferability of the proposed approach to handle data with a similar structure. We conduct extensive experiments on open datasets such as LFW, CASIA-WebFace, and CIFAR-10 and show that our method outperforms state-of-the-art including deep GRAph Contrastive rEpresentation learning (GRACE), GraphCL , and Graph Contrastive Clustering (GCC).},
  archive      = {J_PR},
  author       = {Uno Fang and Jianxin Li and Xuequan Lu and Ajmal Mian and Zhaoquan Gu},
  doi          = {10.1016/j.patcog.2023.109340},
  journal      = {Pattern Recognition},
  pages        = {109340},
  shortjournal = {Pattern Recognition},
  title        = {Robust image clustering via context-aware contrastive graph learning},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning visual question answering on controlled semantic
noisy labels. <em>PR</em>, <em>138</em>, 109339. (<a
href="https://doi.org/10.1016/j.patcog.2023.109339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) has made great progress recently due to the increasing ability to understand and encode multi-modal inputs based on deep learning . However, existing VQA models are usually based on assumptions of clean labels, and it is contradictory to real scenarios where labels are expensive and inevitably contain noises. In this paper, we take the lead in addressing this issue by establishing the first benchmark of controlled semantic noisy labels for VQA task, evaluating existing methods, and coming up with corresponding solutions. Specifically, through analyzing human labels of existing VQA datasets, we first design a controlled semantic label noise by imitating human mislabeling behavior, which is more reasonable than conventional random noise. Then, we evaluate several popular VQA models on these new benchmark datasets and show that their performance degrades significantly compared to the original setting. To this end, we propose a Semantic Noisy Label Correction (SNLC) to mitigate impacts of noisy labels, including a Semantic Cross-Entropy (SCE) loss and a Semantic Embedding Contrastive (SEC) loss. Extensive experiments demonstrate the effectiveness of the proposed method SNLC. The proposed approach achieves a stable improvement on several existing models. The source code is available at https://github.com/zchoi/SNLC .},
  archive      = {J_PR},
  author       = {Haonan Zhang and Pengpeng Zeng and Yuxuan Hu and Jin Qian and Jingkuan Song and Lianli Gao},
  doi          = {10.1016/j.patcog.2023.109339},
  journal      = {Pattern Recognition},
  pages        = {109339},
  shortjournal = {Pattern Recognition},
  title        = {Learning visual question answering on controlled semantic noisy labels},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Computation-efficient knowledge distillation via
uncertainty-aware mixup. <em>PR</em>, <em>138</em>, 109338. (<a
href="https://doi.org/10.1016/j.patcog.2023.109338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has emerged as an essential technique not only for model compression , but also other learning tasks such as continual learning. Given the richer application spectrum and potential online usage of KD, knowledge distillation efficiency becomes a pivotal component. In this work, we study this little-explored but important topic. Unlike previous works that focus solely on the accuracy of student network, we attempt to achieve a harder goal – to obtain a performance comparable to conventional KD with a lower computation cost during the transfer. To this end, we present UNcertainty-aware mIXup (UNIX), an effective approach that can reduce transfer cost by 20\% to 30\% and yet maintain comparable or achieve even better student performance than conventional KD. This is made possible via effective uncertainty sampling and a novel adaptive mixup approach that select informative samples dynamically over ample data and compact knowledge in these samples. We show that our approach inherently performs hard sample mining. We demonstrate the applicability of our approach to improve various existing KD approaches by reducing their queries to a teacher network. Extensive experiments are performed on CIFAR100 and ImageNet. Code and model are available at https://github.com/xuguodong03/UNIXKD .},
  archive      = {J_PR},
  author       = {Guodong Xu and Ziwei Liu and Chen Change Loy},
  doi          = {10.1016/j.patcog.2023.109338},
  journal      = {Pattern Recognition},
  pages        = {109338},
  shortjournal = {Pattern Recognition},
  title        = {Computation-efficient knowledge distillation via uncertainty-aware mixup},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond OCR + VQA: Towards end-to-end reading and reasoning
for robust and accurate textvqa. <em>PR</em>, <em>138</em>, 109337. (<a
href="https://doi.org/10.1016/j.patcog.2023.109337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based visual question answering (TextVQA), which answers a visual question by considering both visual contents and scene texts, has attracted increasing attention recently. Most existing methods employ an optical character recognition (OCR) module as a pre-processor to read texts, then combine it with a visual question answering (VQA) framework. However, inaccurate OCR results may lead to cumulative error propagation, and the correlation between text reading and text-based reasoning is not fully exploited. In this work, we integrate OCR into the flow of TextVQA, targeting the mutual reinforcement of OCR and VQA tasks. Specifically, a visually enhanced text embedding module is proposed to predict semantic features from the visual information of texts, by which texts can be reasonably understood even without accurate recognition. Further, two elaborate schemes are developed to leverage contextual information in VQA to modify OCR results. The first scheme is a reading modification module that adaptively selects the answer results according to the contexts. Second, we propose an efficient end-to-end text reading and reasoning network, where the downstream VQA signal contributes to the optimization of text reading. Extensive experiments show that our method outperforms existing alternatives in terms of accuracy and robustness, whether ground truth OCR annotations are used or not.},
  archive      = {J_PR},
  author       = {Gangyan Zeng and Yuan Zhang and Yu Zhou and Xiaomeng Yang and Ning Jiang and Guoqing Zhao and Weiping Wang and Xu-Cheng Yin},
  doi          = {10.1016/j.patcog.2023.109337},
  journal      = {Pattern Recognition},
  pages        = {109337},
  shortjournal = {Pattern Recognition},
  title        = {Beyond OCR + VQA: Towards end-to-end reading and reasoning for robust and accurate textvqa},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust detectors of rotationally symmetric shapes based on
novel semi-shape signatures. <em>PR</em>, <em>138</em>, 109336. (<a
href="https://doi.org/10.1016/j.patcog.2023.109336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient detectors of rotationally symmetric shapes are proposed by introducing a novel concept of semi-shape signatures to overcome the main problem of projection-based approaches for studying the rotationally symmetric properties of an arbitrary binary shape. Indeed, the fact that the projection cues in these conventional approaches are periodical with a period of π π has restricted an applicable exploitation of rotational symmetry detection. To this end, we propose a new concept of the profile of semi-shapes as a shape signature together with a simple yet efficient technique so that the rotational symmetry of the binary shape can be determined by considering the correlation between this signature and its circular shift. Moreover, a new meaningful measure, ranging from 0 to 1, is also introduced to indicate how perfect the rotational symmetry would be. Experimental results of detecting on single/compound shapes have clearly corroborated the competence of our proposal.},
  archive      = {J_PR},
  author       = {Thanh Phuong Nguyen and Thanh Tuan Nguyen},
  doi          = {10.1016/j.patcog.2023.109336},
  journal      = {Pattern Recognition},
  pages        = {109336},
  shortjournal = {Pattern Recognition},
  title        = {Robust detectors of rotationally symmetric shapes based on novel semi-shape signatures},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory-augmented appearance-motion network for video anomaly
detection. <em>PR</em>, <em>138</em>, 109335. (<a
href="https://doi.org/10.1016/j.patcog.2023.109335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection is a promising yet challenging task, where only normal events are observed in the training phase. Without any explicit classification boundary between normal and abnormal events, anomaly detection can be turned into an outlier detection problem by regarding any event that does not conform to the normal patterns as an anomaly. Most of the existing works mainly focus on improving the representation of normal events, while ignore the relationship between normal and abnormal events. Besides, the lack of restrictions on classification boundaries also leads to performance degradation . To address the above problems, we design a novel autoencoder-based Memory-Augmented Appearance-Motion Network (MAAM-Net), which consists of a novel end-to-end network to learn appearance and motion feature of a given input frame, a fused memory module to build a bridge for normal and abnormal events, a well-designed margin-based latent loss to relieve the computation costs, and a pointed Patch-based Stride Convolutional Detection (PSCD) algorithm to eliminate the degradation phenomenon. Specifically, the memory module is embedded between the encoder and decoder, which serves as a sparse dictionary of normal patterns, therefore it can be further employed to reintegrate abnormal events during inference. To further distort the reintegration quality of abnormal events, the margin-based latent loss is leveraged to enforce the memory module to select a sparse set of critical memory items. Last but not least, the simple yet effective detection method focuses on patches rather than the overall frame responses, which can benefit from the distortion of abnormal events. Extensive experiments and ablation studies on three anomaly detection benchmarks, i.e., UCSD Ped2, CUHK Avenue, and ShanghaiTech, demonstrate the effectiveness and efficiency of our proposed MAAM-Net. Notably, we achieve superior AUC performances on UCSD Ped2 (0.977), CHUK Avenue (0.909), and ShanghaiTech (0.713). The code is publicly available at https://github.com/Owen-Tian/MAAM-Net .},
  archive      = {J_PR},
  author       = {Le Wang and Junwen Tian and Sanping Zhou and Haoyue Shi and Gang Hua},
  doi          = {10.1016/j.patcog.2023.109335},
  journal      = {Pattern Recognition},
  pages        = {109335},
  shortjournal = {Pattern Recognition},
  title        = {Memory-augmented appearance-motion network for video anomaly detection},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting outliers from pairwise proximities: Proximity
isolation forests. <em>PR</em>, <em>138</em>, 109334. (<a
href="https://doi.org/10.1016/j.patcog.2023.109334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because outliers are very different from the rest of the data, it is natural to represent outliers by their distances to other objects. Furthermore, there are many scenarios in which only pairwise distances are known, and feature-based outlier detection methods cannot directly be applied. Considering these observations, and given the success of Isolation Forests for (feature-based) outlier detection , we propose Proximity Isolation Forest, a proximity-based extension. The methodology only requires a set of pairwise distances to work, making it suitable for different types of data. Analogously to Isolation Forest, outliers are detected via their early isolation in the trees; to encode the isolation we design nine training strategies, both random and optimized. We thoroughly evaluate the proposed approach on fifteen datasets, successfully assessing its robustness and suitability for the task; additionally we compare favourably to alternative proximity-based methods.},
  archive      = {J_PR},
  author       = {Antonella Mensi and David M.J. Tax and Manuele Bicego},
  doi          = {10.1016/j.patcog.2023.109334},
  journal      = {Pattern Recognition},
  pages        = {109334},
  shortjournal = {Pattern Recognition},
  title        = {Detecting outliers from pairwise proximities: Proximity isolation forests},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly-supervised butterfly detection based on saliency map.
<em>PR</em>, <em>138</em>, 109313. (<a
href="https://doi.org/10.1016/j.patcog.2023.109313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the actual needs for detecting multiple features of butterflies in natural ecosystems, this paper proposes a model of weakly-supervised butterfly detection based on a saliency map (WBD-SM) to enhance the accuracy of butterfly detection in the ecological environment as well as to overcome the difficulty of fine annotation. Our proposed model first extracts the features of different scales using the VGG16 without the fully connected layers as the backbone network. Next, the saliency maps of butterfly images are extracted using the deep supervision network with shortcut connections (DSS) used for the butterfly target location. The class activation maps of butterfly images are derived via the adversarial complementary learning (ACoL) network for butterfly target recognition. Then, the saliency and class activation maps are post-processed with conditional random fields, thereby obtaining the refined saliency maps of butterfly objects. Finally, the locations of the butterflies are acquired based on the saliency maps. Experimental results on the 20 categories of butterfly dataset collected in this paper indicate that the WBD-SM achieves a higher recognition accuracy than that of the VGG16 under different division ratios. At the same time, when the training set and test set are 8:2, our WBD-SM attains a 95.67\% localization accuracy , which is 9.37\% and 11.87\% higher than the results of the DSS and ACoL, respectively. Compared with three state-of-the-art fully-supervised object detection networks, RefineDet, YOLOv3 and single-shot detection (SSD), the detection performance of our WBD-SM is better than RefineDet, and YOLOv3, and is almost the same as SSD.},
  archive      = {J_PR},
  author       = {Ting Zhang and Muhammad Waqas and Yu Fang and Zhaoying Liu and Zahid Halim and Yujian Li and Sheng Chen},
  doi          = {10.1016/j.patcog.2023.109313},
  journal      = {Pattern Recognition},
  pages        = {109313},
  shortjournal = {Pattern Recognition},
  title        = {Weakly-supervised butterfly detection based on saliency map},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wse-MF: A weighting-based student exercise matrix
factorization model. <em>PR</em>, <em>138</em>, 109285. (<a
href="https://doi.org/10.1016/j.patcog.2022.109285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Students who have been taught new ideas need to develop their skills by carrying out further work in their own time. This often consists of a series of exercises which must be completed. While students can choose exercises themselves from online sources, they will learn more quickly and easily if the exercises are specifically tailored to their needs. A good teacher will always aim to do this, but with the large groups of students who typically take advantage of open online courses, it may not be possible. Exercise prediction, working with large-scale matrix data, is a better way to address this challenge, and a key stage within such prediction is to calculate the probability that a student will answer a given question correctly. Therefore, this paper presents a novel approach called Weighting-based Student Exercise Matrix Factorization (Wse-MF) which combines student learning ability and exercise difficulty as prior weights. In order to learn how to complete the matrix, we apply an iterative optimization method that makes the approach practical for large-scale educational deployment. Compared with eight models in cognitive diagnosis and matrix factorization, our research results suggest that Wse-MF significantly outperforms the state-of-the-art on a range of real-world datasets in both prediction quality and time complexity. Moreover, we find that there is an optimal value of the latent factor K K (the inner dimension of the factorization) for each dataset, which is related to the relationship between skills and exercises in that dataset. Similarly, the optimal value of hyperparameter c 0 c0 is linked to the ratio between exercises and students. Taken as a whole, we demonstrate improvements to matrix factorization within the context of educational data.},
  archive      = {J_PR},
  author       = {Xia Sun and Bo Li and Richard Sutcliffe and Zhizezhang Gao and Wenying Kang and Jun Feng},
  doi          = {10.1016/j.patcog.2022.109285},
  journal      = {Pattern Recognition},
  pages        = {109285},
  shortjournal = {Pattern Recognition},
  title        = {Wse-MF: A weighting-based student exercise matrix factorization model},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey of image augmentation techniques for
deep learning. <em>PR</em>, <em>137</em>, 109347. (<a
href="https://doi.org/10.1016/j.patcog.2023.109347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning has achieved satisfactory performance in computer vision , a large volume of images is required. However, collecting images is often expensive and challenging. Many image augmentation algorithms have been proposed to alleviate this issue. Understanding existing algorithms is, therefore, essential for finding suitable and developing novel methods for a given task. In this study, we perform a comprehensive survey of image augmentation for deep learning using a novel informative taxonomy. To examine the basic objective of image augmentation, we introduce challenges in computer vision tasks and vicinity distribution. The algorithms are then classified among three categories: model-free, model-based, and optimizing policy-based. The model-free category employs the methods from image processing , whereas the model-based approach leverages image generation models to synthesize images. In contrast, the optimizing policy-based approach aims to find an optimal combination of operations. Based on this analysis, we believe that our survey enhances the understanding necessary for choosing suitable methods and designing novel algorithms.},
  archive      = {J_PR},
  author       = {Mingle Xu and Sook Yoon and Alvaro Fuentes and Dong Sun Park},
  doi          = {10.1016/j.patcog.2023.109347},
  journal      = {Pattern Recognition},
  pages        = {109347},
  shortjournal = {Pattern Recognition},
  title        = {A comprehensive survey of image augmentation techniques for deep learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frequency learning attention networks based on deep learning
for automatic modulation classification in wireless communication.
<em>PR</em>, <em>137</em>, 109345. (<a
href="https://doi.org/10.1016/j.patcog.2023.109345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have been recently applied in automatic modulation classification task and achieved remarkable success. However, Existing neural networks mainly focus on the purely data-driven architecture design, and fail to explore the hand-crafted feature mechanisms which are particularly significant for radio signal presentation in wireless communication . Inspired by digital signal processing theories, we propose frequency learning attention networks (FLANs) to analyze the radio spectral bias from frequency perspective, based on a multi-spectral attention mechanism for learning-based frequency components selection. FLANs are the general case of classical global average pooling and leverage identical structures of the popular neural networks. Extensive experiments have been conducted to validate the superiority of FLANs for automatic modulation classification over a wide variety of state-of-the-art methods on RADIOML 2018.01A dataset.},
  archive      = {J_PR},
  author       = {Duona Zhang and Yuanyao Lu and Yundong Li and Wenrui Ding and Baochang Zhang and Jing Xiao},
  doi          = {10.1016/j.patcog.2023.109345},
  journal      = {Pattern Recognition},
  pages        = {109345},
  shortjournal = {Pattern Recognition},
  title        = {Frequency learning attention networks based on deep learning for automatic modulation classification in wireless communication},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high dynamic range imaging method for short exposure
multiview images. <em>PR</em>, <em>137</em>, 109344. (<a
href="https://doi.org/10.1016/j.patcog.2023.109344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restoration and enhancement of multiview low dynamic range (MVLDR) images captured in low lighting conditions is a great challenge. The disparity maps are hardly reliable in practical, real-world scenarios and suffers from holes and artifacts due to large baseline and angle deviation among multiple cameras in low lighting conditions. Furthermore, multiple images with some additional information (e.g., ISO/exposure time, etc.) are required for the radiance map and poses the additional challenges of deghosting to encounter motion artifacts. In this paper, we proposed a method to reconstruct multiview high dynamic range (MVHDR) images from MVLDR images without relying on disparity maps . We detect and accurately match the feature points among the involved input views and gather the brightness information from the neighboring viewpoints to optimize an image restoration function based on input exposure gain to finally generate MVHDR images. Our method is very reliable and suitable for a wide baseline among sparse cameras. The proposed method requires only one image per viewpoint without any additional information and outperforms others.},
  archive      = {J_PR},
  author       = {Rizwan Khan and You Yang and Kejun Wu and Atif Mehmood and Zahid Hussain Qaisar and Zhonglong Zheng},
  doi          = {10.1016/j.patcog.2023.109344},
  journal      = {Pattern Recognition},
  pages        = {109344},
  shortjournal = {Pattern Recognition},
  title        = {A high dynamic range imaging method for short exposure multiview images},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank tensor recovery via non-convex regularization,
structured factorization and spatio-temporal characteristics.
<em>PR</em>, <em>137</em>, 109343. (<a
href="https://doi.org/10.1016/j.patcog.2023.109343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the convex low-rank 3rd-order tensor recovery has attracted considerable attention. However, there are some limitations to the convex relaxation approach, which may yield biased estimators. To overcome this disadvantage, we develop a novel non-convex tensor pseudo-norm to replace the weighted sum of the tensor nuclear norm as a tighter rank approximation. Then in tensor robust principle component analysis , we introduce the noise analysis to separate the spare foreground from the dynamic background more accurately. Furthermore, by introducing a spatio-temporal matrix, we can make better use of the inherent spatio-temporal characteristics of the low-rank static background and sparse foreground. Finally, we introduce an incoherent term to constrain the sparse foreground and the dynamic background to improve the separability . Some preliminary numerical examples of color image, video, and face image data sets are presented to illustrate the efficiency of our proposed methods.},
  archive      = {J_PR},
  author       = {Quan Yu and Ming Yang},
  doi          = {10.1016/j.patcog.2023.109343},
  journal      = {Pattern Recognition},
  pages        = {109343},
  shortjournal = {Pattern Recognition},
  title        = {Low-rank tensor recovery via non-convex regularization, structured factorization and spatio-temporal characteristics},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DCSNE: Density-based clustering using graph shared neighbors
and entropy. <em>PR</em>, <em>137</em>, 109341. (<a
href="https://doi.org/10.1016/j.patcog.2023.109341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density-based clustering techniques identify arbitrary shaped clusters in the presence of outliers by capturing the intrinsic distribution of data and separating high and low-density regions based on the neighborhood information. They use global parameters to compute the density distribution of data points, the optimization of which is quite a challenging and time intensive task. The similarity graphs constructed from the data points can easily capture the topology of the density regions without using any user-defined parameters. Moreover, the concept of entropy can be useful to capture the randomness and disorderliness present in highly complex data distributions. Our proposed algorithm makes use of entropy in conjunction with the graph local neighborhood information to compute density distribution of data points. Then, actual clusters are identified using the density distribution and the shared neighbor information between the regions. The experimental results show that the proposed technique outperforms other comparable methods in terms of cluster quality in the presence of noise on diversified gene expression and real datasets.},
  archive      = {J_PR},
  author       = {Rashmi Maheshwari and Sraban Kumar Mohanty and Amaresh Chandra Mishra},
  doi          = {10.1016/j.patcog.2023.109341},
  journal      = {Pattern Recognition},
  pages        = {109341},
  shortjournal = {Pattern Recognition},
  title        = {DCSNE: Density-based clustering using graph shared neighbors and entropy},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Amercing: An intuitive and effective constraint for dynamic
time warping. <em>PR</em>, <em>137</em>, 109333. (<a
href="https://doi.org/10.1016/j.patcog.2023.109333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic Time Warping (DTW) is a time series distance measure that allows non-linear alignments between series. Constraints on the alignments in the form of windows and weights have been introduced because unconstrained DTW is too permissive in its alignments. However, windowing introduces a crude step function, allowing unconstrained flexibility within the window, and none beyond it. While not entailing a step function, a multiplicative weight is relative to the distances between aligned points along a warped path, rather than being a direct function of the amount of warping that is introduced. In this paper, we introduce Amerced Dynamic Time Warping (ADTW), a new, intuitive, DTW variant that penalizes the act of warping by a fixed additive cost. Like windowing and weighting, ADTW constrains the amount of warping. However, it avoids both abrupt discontinuities in the amount of warping allowed and the limitations of a multiplicative penalty. We formally introduce ADTW, prove some of its properties, and discuss its parameterization. We show on a simple example how it can be parameterized to achieve an intuitive outcome, and demonstrate its usefulness on a standard time series classification benchmark. We provide a demonstration application in C++ Herrmann(2021)[1].},
  archive      = {J_PR},
  author       = {Matthieu Herrmann and Geoffrey I. Webb},
  doi          = {10.1016/j.patcog.2023.109333},
  journal      = {Pattern Recognition},
  pages        = {109333},
  shortjournal = {Pattern Recognition},
  title        = {Amercing: An intuitive and effective constraint for dynamic time warping},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-residual unrestricted pruned ultra-faster line detection
for edge devices. <em>PR</em>, <em>137</em>, 109321. (<a
href="https://doi.org/10.1016/j.patcog.2023.109321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line detection with deep learning is a popular visual task that focuses mostly on lane detection . It requires quicker inference speed and lower consumption, especially for high-speed edge device applications. Based on the UFAST, we propose the Non-Residual Unrestricted Pruned Ultra-faster (NRUPU) line detection via a novel model compression method including non-interference structural reconstruction (NISR), shallow channel priority reservation (SCPR) pruning and non-residual equivalent transformation (NRET). NISR is a structure reconstruction scheme allocating residual branches into each layer to solve the cross-layer channel interference in ResNet-18. SCPR pruning directly uses the factors of BN layers to build channel importance evaluation for backbone and designs channel selection method for head based on data distribution consistency, reducing the parameters of each layer independently. Then NRET losslessly converts the multi-branch model to a single-branch one containing only convolution, linear, and relu, which reduces implementation complexity on edge devices. These designs follow the theoretical foundations: the data distribution transformation trend and effect of gradient back-propagation on model learning ability. Compared with previous pruning methods, our method optimizes not only the parameters of the model but also the structure of the model. We train NRUPU in RTX2080Ti and deploy tests on edge devices NVIDIA Jetson Xavier NX (NJXN) and Atlas 200 DK (A2DK). Extensive experiments are conducted on the dataset TuSimple, CULane and our belt dataset with 11,894 data. Results show that NRUPU achieves over 96\% speed increase and over 66\% parameter reduction on all datasets within 0.7\% accuracy loss. The FPS can reach 749, 665 and 783 on RTX2080Ti, 133, 117 and 143 on NJNX, 178, 161 and 183 on A2DK respectively. The code is released at https://anonymous.4open.science/r/NRUPU-29B0 .},
  archive      = {J_PR},
  author       = {Pengpeng Chen and Dongjingdian Liu and Shouwan Gao},
  doi          = {10.1016/j.patcog.2023.109321},
  journal      = {Pattern Recognition},
  pages        = {109321},
  shortjournal = {Pattern Recognition},
  title        = {Non-residual unrestricted pruned ultra-faster line detection for edge devices},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-balanced distillation for object detection.
<em>PR</em>, <em>137</em>, 109320. (<a
href="https://doi.org/10.1016/j.patcog.2023.109320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mainstream object detectors are commonly constituted of two sub-tasks, including classification and regression tasks, implemented by two parallel heads. This classic design paradigm inevitably leads to inconsistent spatial distributions between classification score and localization quality (IOU). Therefore, this paper alleviates this misalignment in the view of knowledge distillation . First, we observe that the massive teacher achieves a higher proportion of harmonious predictions than the lightweight student. Based on this intriguing observation, a novel Harmony Score (HS) is devised to estimate the alignment of classification and regression qualities. HS models the relationship between two sub-tasks and is seen as prior knowledge to promote harmonious predictions for the student. Second, this spatial misalignment will result in inharmonious region selection when distilling features. To alleviate this problem, a novel Task-decoupled Feature Distillation (TFD) is proposed by flexibly balancing the contributions of classification and regression tasks. Eventually, HD and TFD constitute the proposed method, named Task-Balanced Distillation (TBD). Extensive experiments demonstrate the considerable potential and generalization of the proposed method. Notably, when equipped with TBD, the performances of RetinaNet-R18/RetinaNet-R50/Faster-RCNN-R18 can be boosted from 33.2/37.4/34.5 to 37.3/41.2/37.7, outperforming the recent KD-based methods like FRS, FGD, and MGD.},
  archive      = {J_PR},
  author       = {Ruining Tang and Zhenyu Liu and Yangguang Li and Yiguo Song and Hui Liu and Qide Wang and Jing Shao and Guifang Duan and Jianrong Tan},
  doi          = {10.1016/j.patcog.2023.109320},
  journal      = {Pattern Recognition},
  pages        = {109320},
  shortjournal = {Pattern Recognition},
  title        = {Task-balanced distillation for object detection},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Reducing bi-level feature redundancy for unsupervised
domain adaptation. <em>PR</em>, <em>137</em>, 109319. (<a
href="https://doi.org/10.1016/j.patcog.2023.109319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) deals with the problem of transferring knowledge from a labeled source domain to an unlabeled target domain when the two domains have distinct data distributions. Therefore, the purpose of domain adaptation is to mitigate the distribution divergence between the two domains. Many existing UDA methods only use the traditional batch normalization layer, but this may lead to a large number of feature redundancy and lead performance degradation . In this paper, we introduce a novel deep learning paradigm called feature redundancy in UDA to enhance adaptation ability. Specifically, we first show that feature redundancy also exists on unsupervised domain adaptation (UDA), which has been ignored by most previous efforts. We utilize feature similarity as a metric to measure feature redundancy and then analyze the relationship between uniform feature spectrum and minimal feature similarity. Based on this relationship, we intend to reduce cross-domain feature redundancy for UDA by making the distribution of feature spectrum uniforms in a bi-level way. For the first level, we propose a cross-domain batch normalization with the whitening module (xBN) to ensure compact domain-specific features and learn domain-invariant features at the same time. With the domain-specific features from the first level that paves a way, on the second level, we suggest an alternative orthogonal regularizer (OR) that can make the distribution of the feature spectrum more uniform, thus domain-invariant feature redundancy is mitigated. Such a bi-level mechanism greatly reduces the feature redundancy for UDA. To evaluate the efficacy of the proposed bi-level mechanism, we plug those two novel modules ( i.e. , xBN and OR) into convolutional neural networks (CNNs) to form our UDA model and also conduct the corresponding empirical evaluations on five cross-domain object recognition benchmarks including both classical and large-scale image datasets. Experimental results show that the proposed UDA model could achieve state-of-the-art performance both in quantity and quality. Our source codes will be released after publication.},
  archive      = {J_PR},
  author       = {Mengzhu Wang and Shanshan Wang and Wei Wang and Li Shen and Xiang Zhang and Long Lan and Zhigang Luo},
  doi          = {10.1016/j.patcog.2023.109319},
  journal      = {Pattern Recognition},
  pages        = {109319},
  shortjournal = {Pattern Recognition},
  title        = {Reducing bi-level feature redundancy for unsupervised domain adaptation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Arbitrary order total variation for deformable image
registration. <em>PR</em>, <em>137</em>, 109318. (<a
href="https://doi.org/10.1016/j.patcog.2023.109318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate image registration in a variational framework and focus on regularization generality and solver efficiency. We first propose a variational model combining the state-of-the-art sum of absolute differences (SAD) and a new arbitrary order total variation regularization term. The main advantage is that this variational model preserves discontinuities in the resultant deformation while being robust to outlier noise. It is however non-trivial to optimize the model due to its non-convexity, non-differentiabilities, and generality in the derivative order. To tackle these, we propose to first apply linearization to the model to formulate a convex objective function and then break down the resultant convex optimization into several point-wise, closed-form subproblems using a fast, over-relaxed alternating direction method of multipliers (ADMM). With this proposed algorithm, we show that solving higher-order variational formulations is similar to solving their lower-order counterparts. Extensive experiments show that our ADMM is significantly more efficient than both the subgradient and primal-dual algorithms particularly when higher-order derivatives are used, and that our new models outperform state-of-the-art methods based on deep learning and free-form deformation. Our code implemented in both Matlab and Pytorch is publicly available at https://github.com/j-duan/AOTV.},
  archive      = {J_PR},
  author       = {Jinming Duan and Xi Jia and Joseph Bartlett and Wenqi Lu and Zhaowen Qiu},
  doi          = {10.1016/j.patcog.2023.109318},
  journal      = {Pattern Recognition},
  pages        = {109318},
  shortjournal = {Pattern Recognition},
  title        = {Arbitrary order total variation for deformable image registration},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characters as graphs: Interpretable handwritten chinese
character recognition via pyramid graph transformer. <em>PR</em>,
<em>137</em>, 109317. (<a
href="https://doi.org/10.1016/j.patcog.2023.109317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is meaningful but challenging to teach machines to recognize handwritten Chinese characters. However, conventional approaches typically view handwritten Chinese characters as either static images or temporal trajectories, which may ignore the inherent geometric semantics of characters. Instead, here we first propose to represent handwritten characters as skeleton graphs, explicitly considering the natural characteristics of characters (i.e., characters as graphs). Furthermore, we propose a novel Pyramid Graph Transformer (PyGT) to specifically process the graph-structured characters, which fully integrates the advantages of Transformers and graph convolutional networks . Specifically, our PyGT can learn better graph features through (i) capturing the global information from all nodes with graph attention mechanism and (ii) modelling the explicit local adjacency structures of nodes with graph convolutions. Furthermore, the PyGT learns the multi-resolution features by constructing a progressive shrinking pyramid. Compared with existing approaches, it is more interpretable to recognize characters as geometric graphs . Moreover, the proposed method is generic for both online and offline handwritten Chinese character recognition (HCCR), and it also can be feasibly extended to handwritten text recognition. Extensive experiments empirically demonstrate the superiority of PyGT over the prevalent approaches including 2D-CNN, RNN/1D-CNN, and Vision Transformer (ViT) for HCCR. The code is available at https://github.com/ganji15/PyGT-HCCR .},
  archive      = {J_PR},
  author       = {Ji Gan and Yuyan Chen and Bo Hu and Jiaxu Leng and Weiqiang Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2023.109317},
  journal      = {Pattern Recognition},
  pages        = {109317},
  shortjournal = {Pattern Recognition},
  title        = {Characters as graphs: Interpretable handwritten chinese character recognition via pyramid graph transformer},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Theoretical guarantee for crowdsourcing learning with unsure
option. <em>PR</em>, <em>137</em>, 109316. (<a
href="https://doi.org/10.1016/j.patcog.2023.109316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing learning, in which labels are collected from multiple workers through crowdsourcing platforms , has attracted much attention during the past decade. This learning paradigm would reduce the labeling cost since crowdsourcing workers may be non-expert and hence less costly. On the other hand, crowdsourcing learning algorithms also suffer from being misled by incorrect labels introduced by imperfect workers. To control such risks, recently, it has been suggested to provide workers an additional unsure option during the labeling process. Although the benefits of the unsure option have been empirically demonstrated, theoretical analysis is still limited. In this article, a theoretical analysis of crowdsourcing learning with the unsure option is presented. Specifically, an upper bound of minimally sufficient number of crowd labels required for learning a probably approximately correct (PAC) classification model with and without the unsure option are given respectively. Next, a condition under which providing (or not providing) an unsure option to workers is derived. Then, the theoretical results are extended to guide non-identical label options (with or without unsure options) to different workers. Last, several useful applications are proposed based on theoretical results.},
  archive      = {J_PR},
  author       = {Yigong Pan and Ke Tang and Guangzhong Sun},
  doi          = {10.1016/j.patcog.2023.109316},
  journal      = {Pattern Recognition},
  pages        = {109316},
  shortjournal = {Pattern Recognition},
  title        = {Theoretical guarantee for crowdsourcing learning with unsure option},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale attention guided pose transfer. <em>PR</em>,
<em>137</em>, 109315. (<a
href="https://doi.org/10.1016/j.patcog.2023.109315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose transfer refers to the probabilistic image generation of a person with a previously unseen novel pose from another image of that person having a different pose. Due to potential academic and commercial applications, pose transfer has been extensively studied in recent years. Among the various approaches to the problem, attention guided progressive generation is shown to produce state-of-the-art results in most cases. This paper presents an improved network architecture for pose transfer by introducing attention links at every resolution level of the encoder and decoder. By utilizing such dense multi-scale attention guided approach, we are able to achieve significant improvement over the existing methods both visually and analytically. We conclude our findings with extensive qualitative and quantitative comparisons against several existing methods on the DeepFashion dataset. We also show the generality of the proposed network architecture by extending it to multiple application domains, such as semantic reconstruction, virtual try-on and style manipulation. 1},
  archive      = {J_PR},
  author       = {Prasun Roy and Saumik Bhattacharya and Subhankar Ghosh and Umapada Pal},
  doi          = {10.1016/j.patcog.2023.109315},
  journal      = {Pattern Recognition},
  pages        = {109315},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale attention guided pose transfer},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature weighting in DBSCAN using reverse nearest
neighbours. <em>PR</em>, <em>137</em>, 109314. (<a
href="https://doi.org/10.1016/j.patcog.2023.109314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DBSCAN is arguably the most popular density-based clustering algorithm, and it is capable of recovering non-spherical clusters. One of its main weaknesses is that it treats all features equally. In this paper, we propose a density-based clustering algorithm capable of calculating feature weights representing the degree of relevance of each feature, which takes the density structure of the data into account. First, we improve DBSCAN and introduce a new algorithm called DBSCANR. DBSCANR reduces the number of parameters of DBSCAN to one. Then, a new step is introduced to the clustering process of DBSCANR to iteratively update feature weights based on the current partition of data. The feature weights produced by the weighted version of the new clustering algorithm, W-DBSCANR, measure the relevance of variables in a clustering and can be used in feature selection in data mining applications where large and complex real-world data are often involved. Experimental results on both artificial and real-world data have shown that the new algorithms outperformed various DBSCAN type algorithms in recovering clusters in data.},
  archive      = {J_PR},
  author       = {Stiphen Chowdhury and Na Helian and Renato Cordeiro de Amorim},
  doi          = {10.1016/j.patcog.2023.109314},
  journal      = {Pattern Recognition},
  pages        = {109314},
  shortjournal = {Pattern Recognition},
  title        = {Feature weighting in DBSCAN using reverse nearest neighbours},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-efficient and byzantine-robust distributed
learning with statistical guarantee. <em>PR</em>, <em>137</em>, 109312.
(<a href="https://doi.org/10.1016/j.patcog.2023.109312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication efficiency and robustness are two major issues in modern distributed learning frameworks. This is due to the practical situations where some computing nodes may have limited communication power or may behave adversarial behaviors. To address the two issues simultaneously, this paper develops two communication-efficient and robust distributed learning algorithms for convex problems . Our motivation is based on surrogate likelihood framework and the median and trimmed mean operations. Particularly, the proposed algorithms are provably robust against Byzantine failures , and also achieve optimal statistical rates for strong convex losses and convex (non-smooth) penalties. For typical statistical models such as generalized linear models, our results show that statistical errors dominate optimization errors in finite iterations. Simulated and real data experiments are conducted to demonstrate the numerical performance of our algorithms.},
  archive      = {J_PR},
  author       = {Xingcai Zhou and Le Chang and Pengfei Xu and Shaogao Lv},
  doi          = {10.1016/j.patcog.2023.109312},
  journal      = {Pattern Recognition},
  pages        = {109312},
  shortjournal = {Pattern Recognition},
  title        = {Communication-efficient and byzantine-robust distributed learning with statistical guarantee},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient sampling using feature matching and variable
minimal structure size. <em>PR</em>, <em>137</em>, 109311. (<a
href="https://doi.org/10.1016/j.patcog.2023.109311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Greedy search-based guided sampling is a promising research field in model fitting to data with multiple structures in the presence of a large number of outliers. However, these greedy search-based guided sampling algorithms are sensitive to the fixed minimal (acceptable) structure size and the initial model hypothesis: when the fixed minimal structure size is too small, data subsets sampled by these algorithms are not representative. In contrast, when it is too large, data subsets might be contaminated by outliers. Furthermore, these algorithms may fail to obtain an accurate model hypothesis, if the initial model hypothesis is far from the true model. In this paper, we address the above-mentioned two issues by proposing two greedy search-based strategies: one aims to adaptively estimate minimal structure sizes and the other aims to generate effective initial model hypotheses. Specifically, on one hand, to avoid using the fixed minimal structure size, a strategy is proposed to adaptively estimate minimal structure sizes by using previously obtained ones. On the other hand, to reduce the impact of outliers, a strategy is proposed to filter out outliers to obtain a reduced data subset by using a feature matching algorithm . Then, this strategy generates promising initial model hypotheses by using a proximity sampling on the reduced data subset. Finally, an efficient sampling algorithm based on the two proposed greedy search-based strategies is applied to three vision tasks, i.e., fundamental matrix estimation, homography plane detection and 3D motion segmentation . Extensive experimental results demonstrate the effectiveness of the proposed sampling algorithm.},
  archive      = {J_PR},
  author       = {Taotao Lai and Alireza Sadri and Shuyuan Lin and Zuoyong Li and Riqing Chen and Hanzi Wang},
  doi          = {10.1016/j.patcog.2023.109311},
  journal      = {Pattern Recognition},
  pages        = {109311},
  shortjournal = {Pattern Recognition},
  title        = {Efficient sampling using feature matching and variable minimal structure size},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge aggregation networks for class incremental
learning. <em>PR</em>, <em>137</em>, 109310. (<a
href="https://doi.org/10.1016/j.patcog.2023.109310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing class incremental learning methods rely on storing old exemplars to avoid catastrophic forgetting. However, these methods inevitably face the gradient conflict problem, the inherent conflict between new streaming knowledge and existing knowledge in the gradient direction . To alleviate gradient conflict, this paper reuses the previous knowledge and expands the branch to accommodate new concepts instead of fine-tuning the original models. Specifically, this paper designs a novel dual-branch network called Knowledge Aggregation Networks. The previously trained model is frozen as a branch to retain existing knowledge, and a consistent trainable network is constructed as the other branch to learn new concepts. An adaptive feature fusion module is adopted to dynamically balance the two branches’ information during training. Moreover, a model compression stage maintains the dual-branch structure. Extensive experiments on CIFAR-100, ImageNet-Sub, and ImageNet show that our method significantly outperforms the other methods and effectively balances stability and plasticity.},
  archive      = {J_PR},
  author       = {Zhiling Fu and Zhe Wang and Xinlei Xu and Dongdong Li and Hai Yang},
  doi          = {10.1016/j.patcog.2023.109310},
  journal      = {Pattern Recognition},
  pages        = {109310},
  shortjournal = {Pattern Recognition},
  title        = {Knowledge aggregation networks for class incremental learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). ProbSAP: A comprehensive and high-performance system for
student academic performance prediction. <em>PR</em>, <em>137</em>,
109309. (<a href="https://doi.org/10.1016/j.patcog.2023.109309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The student academic performance prediction is becoming an indispensable service in the computer supported intelligent education system. But conventional machine learning-based methods can only exploit the sparse discriminative features of student behaviors in imbalanced academic datasets to predict student academic performance (SAP). Furthermore, there is a lack of imbalanced data processing mechanisms that can efficiently capture student characteristics and achievement. Therefore, we propose a comprehensive and high-performance prediction framework to probe SAP characteristics (ProbSAP) on massive educational data, which can resolve imbalanced data issue and improve academic prediction performance for making course final mark prediction. It consists of three main components: collaborative data processing module for enhancing the data quality, scalable metadata clustering module for alleviating the imbalance of academic features, and XGBoost-enhanced SAP prediction module for academic performance forecasting. The collaborative data processing module integrates multi-dimensional academic data, which sustains a good supply for clustering and modeling in the ProbSAP framework. The comparative evaluation results demonstrate that ProbSAP delivers superior accuracy and efficiency improvement for the course final mark prediction of college students over other state-of-the-art methods such as CNN , SVR, RFR , XGBoost , Catboost-SHAP, and AS-SAN. On average, ProbSAP reduces the mean absolute error (MAE) by 84.76\%, 72.11\%, and 66.49\% compared with XGBoost , Catboost-SHAP, and AS-SAN, respectively. It also leads to a better out-sample fit that minimizes prediction errors between 1\% and 9\% with over 98\% of actual samples.},
  archive      = {J_PR},
  author       = {Xinning Wang and Yuben Zhao and Chong Li and Peng Ren},
  doi          = {10.1016/j.patcog.2023.109309},
  journal      = {Pattern Recognition},
  pages        = {109309},
  shortjournal = {Pattern Recognition},
  title        = {ProbSAP: A comprehensive and high-performance system for student academic performance prediction},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive evaluation framework for deep model
robustness. <em>PR</em>, <em>137</em>, 109308. (<a
href="https://doi.org/10.1016/j.patcog.2023.109308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved remarkable performance across a wide range of applications, while they are vulnerable to adversarial examples , which motivates the evaluation and benchmark of model robustness. However, current evaluations usually use simple metrics to study the performance of defenses, which are far from understanding the limitation and weaknesses of these defense methods. Thus, most proposed defenses are quickly shown to be attacked successfully, which results in the “arm race” phenomenon between attack and defense. To mitigate this problem, we establish a model robustness evaluation framework containing 23 comprehensive and rigorous metrics, which consider two key perspectives of adversarial learning (i.e., data and model). Through neuron coverage and data imperceptibility , we use data-oriented metrics to measure the integrity of test examples; by delving into model structure and behavior, we exploit model-oriented metrics to further evaluate robustness in the adversarial setting . To fully demonstrate the effectiveness of our framework, we conduct large-scale experiments on multiple datasets including CIFAR-10, SVHN, and ImageNet using different models and defenses with our open-source platform. Overall, our paper provides a comprehensive evaluation framework, where researchers could conduct comprehensive and fast evaluations using the open-source toolkit, and the analytical results could inspire deeper understanding and further improvement to the model robustness.},
  archive      = {J_PR},
  author       = {Jun Guo and Wei Bao and Jiakai Wang and Yuqing Ma and Xinghai Gao and Gang Xiao and Aishan Liu and Jian Dong and Xianglong Liu and Wenjun Wu},
  doi          = {10.1016/j.patcog.2023.109308},
  journal      = {Pattern Recognition},
  pages        = {109308},
  shortjournal = {Pattern Recognition},
  title        = {A comprehensive evaluation framework for deep model robustness},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A local tangent plane distance-based approach to 3D point
cloud segmentation via clustering. <em>PR</em>, <em>137</em>, 109307.
(<a href="https://doi.org/10.1016/j.patcog.2023.109307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an effective measure for the planar segmentation problem based on the clustering method . It uses the distance from a point to the local plane as a metric to characterize the relationship between data. As a result, the data points of the coplanar have a high similarity to distinguish each plane. A dissimilarity matrix of the input point cloud can be evaluated, and multidimensional scaling analysis is performed to reconstruct the correlation information between data points in the 3D Euclidean space. The obtained reconstructed point cloud shows the separation between different planes. An adaptive DBSCAN clustering method based on density stratification is developed to perform cluster segmentation on the reconstructed point cloud. Experimental results show that the proposed method can effectively solve the over-segmentation problem, and at the same time provide high segmentation accuracy .},
  archive      = {J_PR},
  author       = {Hui Chen and Tingting Xie and Man Liang and Wanquan Liu and Peter Xiaoping Liu},
  doi          = {10.1016/j.patcog.2023.109307},
  journal      = {Pattern Recognition},
  pages        = {109307},
  shortjournal = {Pattern Recognition},
  title        = {A local tangent plane distance-based approach to 3D point cloud segmentation via clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepSIR: Deep semantic iterative registration for LiDAR
point clouds. <em>PR</em>, <em>137</em>, 109306. (<a
href="https://doi.org/10.1016/j.patcog.2023.109306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes DeepSIR, a novel learning-based iterative registration framework for real-world 3D LiDAR point clouds. Specifically, a front-end semantic feature extraction (Semantic-feat) model is designed to fully explore semantic information in LiDAR data. To highlight the recognized objects of interest, we propose a novel point score that uses semantic and geometric information . To effectively integrate the extracted semantic features, geometric features , and point scores, we introduce an aggregation module to learn a hybrid feature on each point. Meanwhile, our method dynamically explores feature descriptions and optimizes poses through an iterative pipeline. Extensive experiments on outdoor driving datasets demonstrate that our DeepSIR achieves comparable performance to state-of-the-art methods and runs at a much faster speed. The source code will be made publicly available. 1},
  archive      = {J_PR},
  author       = {Qing Li and Cheng Wang and Chenglu Wen and Xin Li},
  doi          = {10.1016/j.patcog.2023.109306},
  journal      = {Pattern Recognition},
  pages        = {109306},
  shortjournal = {Pattern Recognition},
  title        = {DeepSIR: Deep semantic iterative registration for LiDAR point clouds},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Granularity-aware distillation and structure modeling region
proposal network for fine-grained image classification. <em>PR</em>,
<em>137</em>, 109305. (<a
href="https://doi.org/10.1016/j.patcog.2023.109305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) aims to identify objects belonging to multiple sub-categories of the same super-category. The key to solving fine-grained classification problems is to learn discriminative visual feature representation with only subtle differences. Although previous work based on refined feature learning has made great progress, however, high-level semantic features often lack key information for fine-grained visual object nuances. How to efficiently integrate semantic information of different granularities from classification networks is a critical. In this paper, we propose Granularity-aware Distillation and Structure Modeling region Proposal Network(GDSMP-Net). Our solution integrates multi-granularity hierarchical information through a multi-granularity fusion learning strategy to enhance feature representation. In view of the inherent challenges of large intra-class differences in FGVC, a cross-layer self-distillation regularization is proposed to to strengthen the connection between high-level semantics and low-level semantics for robust multi-granularity feature learning. On this basis, we use a weakly supervised method to generate local branches, and the collaborative learning of discriminative semantics and structural semantics based on local regions, facilitating model to perceive contextual information to capture structural interactions between local semantics. Comprehensive experiments show that our method achieves state-of-the-art performance on four widely-used challenging datasets.(CUB-200-2011, Stanford Cars, FGVC-Aircraft and NA-birds).},
  archive      = {J_PR},
  author       = {Xiao Ke and Yuhang Cai and Baitao Chen and Hao Liu and Wenzhong Guo},
  doi          = {10.1016/j.patcog.2023.109305},
  journal      = {Pattern Recognition},
  pages        = {109305},
  shortjournal = {Pattern Recognition},
  title        = {Granularity-aware distillation and structure modeling region proposal network for fine-grained image classification},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detail enhancement-based vehicle re-identification with
orientation-guided re-ranking. <em>PR</em>, <em>137</em>, 109304. (<a
href="https://doi.org/10.1016/j.patcog.2023.109304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification (re-ID) has attracted extensive attention in the field of computer vision owing to the development of smart cities. Two issues remain in the task of vehicle re-ID: minor inter-class differences and extreme orientation variations. To address them, we propose a detailed enhancement-based vehicle re-ID method with orientation-guided re-ranking. In the proposed method, a novel enhancement module using stripe-based partition, is presented to avoid the negative influence of minor inter-class difference. The stripe-based partition subdivides the feature map of middle layers in the network into two stripes, and then the module explores more detailed information for vehicle re-ID. Furthermore, a multilayer feature fusion module is proposed to enhance the feature representation of a vehicle. To address the problem of extreme orientation variation in vehicle re-ID, we propose an orientation-guided re-ranking strategy to re-rank the retrieval result based on the orientation information and query expansion algorithm. This strategy can optimize the ranking of vehicles whose orientation is not similar to the query images in the post-processing stage. Extensive experiments on three public datasets confirm the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Ziruo Sun and Xiushan Nie and Xiaopeng Bi and Shaohua Wang and Yilong Yin},
  doi          = {10.1016/j.patcog.2023.109304},
  journal      = {Pattern Recognition},
  pages        = {109304},
  shortjournal = {Pattern Recognition},
  title        = {Detail enhancement-based vehicle re-identification with orientation-guided re-ranking},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural network for ordinal classification of imbalanced data
by minimizing a bayesian cost. <em>PR</em>, <em>137</em>, 109303. (<a
href="https://doi.org/10.1016/j.patcog.2023.109303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal classification of imbalanced data is a challenging problem that appears in many real world applications . The challenge is to simultaneously consider the order of the classes and the class imbalance, which can notably improve the performance metrics. The Bayesian formulation allows to deal with these two characteristics jointly: It takes into account the prior probability of each class and the decision costs, which can be used to include the imbalance and the ordinal information, respectively. We propose to use the Bayesian formulation to train neural networks, which have shown excellent results in many classification tasks . A loss function is proposed to train networks with a single neuron in the output layer and a threshold based decision rule. The loss is an estimate of the Bayesian classification cost, based on the Parzen windows estimator, which is fitted for a thresholded decision. Experiments with several real datasets show that the proposed method provides competitive results in different scenarios, due to its high flexibility to specify the relative importance of the errors in the classification of patterns of different classes, considering the order and independently of the probability of each class.},
  archive      = {J_PR},
  author       = {Marcelino Lázaro and Aníbal R. Figueiras-Vidal},
  doi          = {10.1016/j.patcog.2023.109303},
  journal      = {Pattern Recognition},
  pages        = {109303},
  shortjournal = {Pattern Recognition},
  title        = {Neural network for ordinal classification of imbalanced data by minimizing a bayesian cost},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Poincaré fréchet mean. <em>PR</em>, <em>137</em>, 109302.
(<a href="https://doi.org/10.1016/j.patcog.2023.109302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing the Fréchet mean from the Euclidean metric is not able to properly capture the geometric characteristics of many non-trivial operations, such as the non-dot inner product and non-Euclidean gradients defined on the manifold. One effective solution is to derive its hyperbolic representations in the Poincaré or hyperboloid model. Our goodness-of-fit testing shows that the Poincaré Fréchet mean achieves much lower χ 2 χ2 power than that of the hyperboloid and typical non-linear kernels with regard to parameter perturbations . However, recent advanced optimization solvers, such as Riemannian gradient descent and minimizing upper bound, may result in imprecise convergences. This paper presents an ( 1 − ϵ 1−ϵ )-approximation approach to search a core-set on the Poincaré model, reducing deviations of the Poincaré Fréchet mean to its optimum. A hierarchical splitting algorithm that implicitly explores the hyperbolic representations for an arbitrary manifold is then presented. Experiments show that the ( 1 − ϵ 1−ϵ ) Poincaré Fréchet mean adopted in hierarchical splitting, achieves better representations than Euclidean, kernel, and Lorentzian Fréchet means in graph and image data.},
  archive      = {J_PR},
  author       = {Xiaofeng Cao},
  doi          = {10.1016/j.patcog.2023.109302},
  journal      = {Pattern Recognition},
  pages        = {109302},
  shortjournal = {Pattern Recognition},
  title        = {Poincaré fréchet mean},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised node classification via fine-grained graph
auxiliary augmentation learning. <em>PR</em>, <em>137</em>, 109301. (<a
href="https://doi.org/10.1016/j.patcog.2023.109301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node classification has become an important research topic in recent years. Since there are always a few training samples, researchers improve the performance by properly leveraging the predictions of unlabeled nodes during training. However, suffering from the model degradation resulted from the accumulative error of pseudo-labels, there is limited improvement. In this paper we present fine-grained G raph A uxiliary a U gmentation (GAU). It trains the primary task together with an automatically created auxiliary task which is a fine-grained node classification task. And an auxiliary augmentation strategy is designed to enlarge the labeled set for the auxiliary task by utilizing the pseudo-labels of the primary task. Comprehensive experiments show that GAU alleviates the sensitivity of the model to the pseudo-label quality, so more unlabeled nodes can participate in the training. From the perspective of co-training, the fine-grained auxiliary task which is trained by much more unlabeled nodes helps to learn better node representations from a different view, thereby boosting the final performance. Extensive experiments verify the superior performance of the GAU on different GNN architectures when compared with other state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Jia Lv and Kaikai Song and Qiang Ye and Guangjian Tian},
  doi          = {10.1016/j.patcog.2023.109301},
  journal      = {Pattern Recognition},
  pages        = {109301},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised node classification via fine-grained graph auxiliary augmentation learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical nearest neighbor descent, in-tree, and
clustering. <em>PR</em>, <em>137</em>, 109300. (<a
href="https://doi.org/10.1016/j.patcog.2023.109300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, we have proposed a physically-inspired graph-theoretical method, called the Nearest Descent (ND), which is capable of organizing a dataset into an in-tree graph structure. Due to some beautiful and effective features, the constructed in-tree proves well-suited for data clustering . Although there exist some undesired edges (i.e., the inter-cluster edges) in this in-tree, those edges are usually very distinguishable, in sharp contrast to the cases in the famous Minimal Spanning Tree (MST). Here, we propose another graph-theoretical method, called the Hierarchical Nearest Neighbor Descent (HNND). Like ND, HNND also organizes a dataset into an in-tree, but in a more efficient way. Consequently, HNND-based clustering (HNND-C) is more efficient than ND-based clustering (ND-C) as well. This is well proved by the experimental results on five high-dimensional and large-size mass cytometry datasets. The experimental results also show that HNND-C achieves overall better performance than some state-of-the-art clustering methods .},
  archive      = {J_PR},
  author       = {Teng Qiu and Yongjie Li},
  doi          = {10.1016/j.patcog.2023.109300},
  journal      = {Pattern Recognition},
  pages        = {109300},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical nearest neighbor descent, in-tree, and clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal unsupervised domain adaptation for semantic
image segmentation. <em>PR</em>, <em>137</em>, 109299. (<a
href="https://doi.org/10.1016/j.patcog.2022.109299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel multi-modal-based Unsupervised Domain Adaptation (UDA) method for semantic segmentation . Recently, depth has proven to be a relevent property for providing geometric cues to enhance the RGB representation. However, existing UDA methods solely process RGB images or additionally cultivate depth-awareness with an auxiliary depth estimation task. We argue that geometric cues that are crucial to semantic segmentation , such as local shape and relative position, are challenging to recover from an auxiliary depth estimation task with mere color (RGB) information. In this paper, we propose a novel multi-modal UDA method named MMADT, which relies on both RGB and depth images as input. In particular, we design a Depth Fusion Block (DFB) to recalibrate depth information and leverage Depth Adversarial Training (DAT) to bridge the depth discrepancy between the source and target domain. Besides, we propose a self-supervised multi-modal depth estimation assistant network named Geo-Assistant (GA) to align the feature space of RGB and depth and shape the sensitivity of our MMADT to depth information. We experimentally observed significant performance improvement in multiple synthetic to real adaptation benchmarks, i.e., SYNTHIA-to-Cityscapes, GTA5-to-Cityscapes and SELMA-to-Cityscapes. Additionally, our multi-modal UDA scheme is easy to port to other UDA methods with a consistent performance boost.},
  archive      = {J_PR},
  author       = {Sijie Hu and Fabien Bonardi and Samia Bouchafa and Désiré Sidibé},
  doi          = {10.1016/j.patcog.2022.109299},
  journal      = {Pattern Recognition},
  pages        = {109299},
  shortjournal = {Pattern Recognition},
  title        = {Multi-modal unsupervised domain adaptation for semantic image segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-strategy contrastive learning framework for weakly
supervised semantic segmentation. <em>PR</em>, <em>137</em>, 109298. (<a
href="https://doi.org/10.1016/j.patcog.2022.109298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) has gained significant popularity as it relies only on weak labels such as image level annotations rather than the pixel level annotations required by supervised semantic segmentation (SSS) methods. Despite drastically reduced annotation costs, typical feature representations learned from WSSS are only representative of some salient parts of objects and less reliable compared to SSS due to the weak guidance during training. In this paper, we propose a novel Multi-Strategy Contrastive Learning (MuSCLe) framework to obtain enhanced feature representations and improve WSSS performance by exploiting similarity and dissimilarity of contrastive sample pairs at image, region, pixel and object boundary levels. Extensive experiments demonstrate the effectiveness of our method and show that MuSCLe outperforms current state-of-the-art methods on the widely used PASCAL VOC 2012 dataset.},
  archive      = {J_PR},
  author       = {Kunhao Yuan and Gerald Schaefer and Yu-Kun Lai and Yifan Wang and Xiyao Liu and Lin Guan and Hui Fang},
  doi          = {10.1016/j.patcog.2022.109298},
  journal      = {Pattern Recognition},
  pages        = {109298},
  shortjournal = {Pattern Recognition},
  title        = {A multi-strategy contrastive learning framework for weakly supervised semantic segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning depth via leveraging semantics: Self-supervised
monocular depth estimation with both implicit and explicit semantic
guidance. <em>PR</em>, <em>137</em>, 109297. (<a
href="https://doi.org/10.1016/j.patcog.2022.109297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation has shown great success in learning depth using only images for supervision. In this paper, we propose to enhance self-supervised depth estimation with semantics and propose a novel learning scheme, which incorporates both implicit and explicit semantic guidances. Specifically, we propose to relate depth distributions to the semantic category information by proposing a Semantic-aware Spatial Feature Modulation (SSFM) scheme, which implicitly modulates the semantic and depth features in a joint learning framework. The modulation parameters are generated from semantic labels to acquire category-level guidance. Meanwhile, a semantic-guided ranking loss is proposed to explicitly constrain the estimated depth borders using the corresponding segmentation labels . To avoid the impact brought by erroneous segmentation labels , both robust sampling strategy and prediction uncertainty weighting are proposed for the ranking loss. Extensive experimental results show that our method produces high-quality depth maps with semantically consistent depth distributions and accurate depth edges, outperforming the state-of-the-art methods by significant margins.},
  archive      = {J_PR},
  author       = {Rui Li and Danna Xue and Shaolin Su and Xiantuo He and Qing Mao and Yu Zhu and Jinqiu Sun and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.109297},
  journal      = {Pattern Recognition},
  pages        = {109297},
  shortjournal = {Pattern Recognition},
  title        = {Learning depth via leveraging semantics: Self-supervised monocular depth estimation with both implicit and explicit semantic guidance},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Toward a blind image quality evaluator in the wild by
learning beyond human opinion scores. <em>PR</em>, <em>137</em>, 109296.
(<a href="https://doi.org/10.1016/j.patcog.2022.109296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, most existing blind image quality assessment (BIQA) models i n t h e w i l d heavily rely on human ratings, which are extraordinarily labor-expensive to collect. Here, we propose an o p i n i o n − f r e e BIQA method that learns from multiple annotators to assess the perceptual quality of images captured in the wild. Specifically, we first synthesize distorted images based on the pristine counterparts. We then randomly assemble a set of image pairs from the synthetic images , and use a group of IQA models to assign pseudo-binary labels for each pair indicating which image has higher quality as the supervisory signal. Based on the newly established pseudo-labeled dataset, we train a deep neural network (DNN)-based BIQA model to rank the perceptual quality, optimized for consistency with the binary rank labels. Since there exists domain shift, e.g., distortion shift and content shift, between the synthetic and in-the-wild images, we leverage two ways to alleviate this issue. First, the simulated distortions should be similar to authentic distortions as much as possible. Second, an unsupervised domain adaptation (UDA) module is further applied to encourage learning domain-invariant features between two domains. Extensive experiments demonstrate the effectiveness of our proposed o p i n i o n − f r e e BIQA model, yielding SOTA performance in terms of correlation with human opinion scores, as well as gMAD competition. Our code is available at: https://github.com/wangzhihua520/OF_BIQA .},
  archive      = {J_PR},
  author       = {Zhihua Wang and Zhi-Ri Tang and Jianguo Zhang and Yuming Fang},
  doi          = {10.1016/j.patcog.2022.109296},
  journal      = {Pattern Recognition},
  pages        = {109296},
  shortjournal = {Pattern Recognition},
  title        = {Toward a blind image quality evaluator in the wild by learning beyond human opinion scores},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TCCFusion: An infrared and visible image fusion method based
on transformer and cross correlation. <em>PR</em>, <em>137</em>, 109295.
(<a href="https://doi.org/10.1016/j.patcog.2022.109295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion aims to obtain a synthetic image that can simultaneously exhibit salient objects and provide abundant texture details. However, existing deep learning-based methods generally depend on convolutional operations, which indeed have good local feature extraction ability, but the restricted receptive field limits its capability in modeling long-range dependencies. To conquer this dilemma, we propose an infrared and visible image fusion method based on Transformer and cross correlation, named TCCFusion. Specifically, we design a local feature extraction branch (LFEB) to preserve local complementary information, in which a dense-shape network is introduced to reuse the information that may be lost during the convolutional operation. To avoid the limitation of the receptive field and to fully extract the global significant information, a global feature extraction branch (GFEB) is devised that consists of three Transformer blocks for long-range relationship construction. In addition, LFEB and GFEB are arranged in a parallel fashion to maintain local and global useful information in a more effective way. Furthermore, we design a cross correlation loss to train the proposed fusion model in an unsupervised manner, with which the fusion result can obtain adequate thermal radiation information in an infrared image and ample texture details in a visible image. Massive experiments on two mainstream datasets illustrate that our TCCFusion outperforms state-of-the-art algorithms not only on visual quality but also on quantitative assessments . Ablation experiments on the network framework and objective function demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Wei Tang and Fazhi He and Yu Liu},
  doi          = {10.1016/j.patcog.2022.109295},
  journal      = {Pattern Recognition},
  pages        = {109295},
  shortjournal = {Pattern Recognition},
  title        = {TCCFusion: An infrared and visible image fusion method based on transformer and cross correlation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent wavelet structure-preserving residual network for
single image deraining. <em>PR</em>, <em>137</em>, 109294. (<a
href="https://doi.org/10.1016/j.patcog.2022.109294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of deep learning and image prior has been widely used in single image deraining since 2017. Recent studies have demonstrated an excellent deraining effect on the high-frequency part of rain images, but less attention was paid to the low-frequency part of rain images. The rain streaks remain in the low-frequency part of rain images, thus limiting the deraining effect. Since the rain streaks in rain images are often mixed with object edges and background scenes, it is challenging to separate rain from them by directly learning the deraining function in the image domain. To solve these problems, we propose a novel Recurrent Wavelet Structure-preserving Residual Network (RWSRNet), which mainly preserves and introduces the low-frequency sub-images of each level into the low-frequency rain removal sub-networks that are greatly different from the state-of-the-art approaches introducing wavelet transform. In addition, we also share the low-frequency structure information to the high-frequency sub-networks through block connection, which further enriches the detailed information, facilitates convergence, and strengthens the ability of our network to remove rain streaks in high frequency. Finally, we fuse the derained low-frequency sub-images of each level through the proposed image weighted blending module and finally reconstruct the low- and high-frequency sub-images into clean images through inverse wavelet transform recursively. The experimental results indicate that the proposed method achieves an excellent deraining effect on both low- and high-frequency parts of rain images and has better performance in low-frequency preservation and high-frequency enhancement in comparison with the state-of-the-art approaches on synthetic and real image datasets.},
  archive      = {J_PR},
  author       = {Wei-Yen Hsu and Wei-Chi Chang},
  doi          = {10.1016/j.patcog.2022.109294},
  journal      = {Pattern Recognition},
  pages        = {109294},
  shortjournal = {Pattern Recognition},
  title        = {Recurrent wavelet structure-preserving residual network for single image deraining},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric-aware dense matching network for 6D pose
estimation of objects from RGB-d images. <em>PR</em>, <em>137</em>,
109293. (<a href="https://doi.org/10.1016/j.patcog.2022.109293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {6D pose estimation for certain targets from RGB-D images is a fundamental problem in computer vision . Current methods emphasize learning the overall expression of the targets, which leads to poor performance under occlusion and truncation conditions. In this paper, we propose using a geometric-aware dense matching network to obtain visible dense correspondences between a RGB-D image and 3D model to address difficult predictions from unseen keypoints . Two geometrical structures are considered for dense matching. (1) The neighbor area of the correspondences is treated as suboptimal matches in addition to the correspondence to reduce the influence of the error caused by ground truth calibration. (2) The distance consistency of the correspondences is leveraged to eliminate the ambiguity from the symmetrical objects. Experiments on LM-O dataset (77.1\% ADD(S)-0.1d) and YCB-V dataset (97.6\% ADD(S)) show the effectiveness and advantages of our proposed method. 1},
  archive      = {J_PR},
  author       = {Chenrui Wu and Long Chen and Shenglong Wang and Han Yang and Junjie Jiang},
  doi          = {10.1016/j.patcog.2022.109293},
  journal      = {Pattern Recognition},
  pages        = {109293},
  shortjournal = {Pattern Recognition},
  title        = {Geometric-aware dense matching network for 6D pose estimation of objects from RGB-D images},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware semi-supervised few shot segmentation.
<em>PR</em>, <em>137</em>, 109292. (<a
href="https://doi.org/10.1016/j.patcog.2022.109292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few shot segmentation (FSS) aims to learn pixel-level classification of a target object in a query image using only a few annotated support samples. This is challenging as it requires modeling appearance variations of target objects and the diverse visual cues between query and support images with limited information. To address this problem, we propose a semi-supervised FSS strategy that leverages additional prototypes from unlabeled images with uncertainty guided pseudo label refinement. To obtain reliable prototypes from unlabeled images, we meta-train a neural network to jointly predict segmentation and estimate the uncertainty of predictions. We employ the uncertainty estimates to exclude predictions with high degrees of uncertainty for pseudo label construction to obtain additional prototypes from the refined pseudo labels. During inference, query segmentation is predicted using prototypes from both support and unlabeled images including low-level features of the query images. Our approach can easily supplement existing approaches without the requirement of additional training when employing unlabeled samples . Extensive experiments on PASCAL- 5 i 5i and COCO- 20 i 20i demonstrate that our model can effectively remove unreliable predictions to refine pseudo labels and significantly improve upon baseline performance.},
  archive      = {J_PR},
  author       = {Soopil Kim and Philip Chikontwe and Sion An and Sang Hyun Park},
  doi          = {10.1016/j.patcog.2022.109292},
  journal      = {Pattern Recognition},
  pages        = {109292},
  shortjournal = {Pattern Recognition},
  title        = {Uncertainty-aware semi-supervised few shot segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid feature enhancement network for few-shot semantic
segmentation. <em>PR</em>, <em>137</em>, 109291. (<a
href="https://doi.org/10.1016/j.patcog.2022.109291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although few-shot semantic segmentation methods have been widely studied in computer vision field, it still has room for improvement. In this work, we propose to enrich the feature representation with texture information and assign adaptive weights to losses. Specially, we incorporate the texture information obtained by texture enhance module with layer&#39;s features on ResNet , and then get a series of hybrid features. The incorporation of texture information enhances the similarity calculation to make the support set guidance more effective. Besides, the proposed adaptive loss makes the network optimize in a better direction. The experiments testify that the proposed method achieves the better results than that of previous methods on few-shot segmentation dataset such as PASCAL-5 i , COCO-20 i and FSS-1000.},
  archive      = {J_PR},
  author       = {Hai Min and Yemao Zhang and Yang Zhao and Wei Jia and Yingke Lei and Chunxiao Fan},
  doi          = {10.1016/j.patcog.2022.109291},
  journal      = {Pattern Recognition},
  pages        = {109291},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid feature enhancement network for few-shot semantic segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real time iris segmentation quality evaluation using
medoids. <em>PR</em>, <em>137</em>, 109290. (<a
href="https://doi.org/10.1016/j.patcog.2022.109290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating an efficient and robust iris segmentation-quality estimation module in iris biometric systems will undoubtedly enhance its performance and competitive advantage. It proffers a real time detection of segmentation errors to forestall their propagation to the subsequent modules. Hence, we propose a novel automatic iris segmentation-quality estimation model using medoids. The performance of the proposed model was empirically evaluated with reference to three published models, using three benchmarked iris datasets and our novel proprietary iris dataset – Biometrics Vision and Computing Iris dataset. The proposed medoids based model was experimentally demonstrated to be effective, robust and relatively efficient in estimating iris segmentation-quality. Specifically, the proposed model recorded the best classification accuracy rate of 95.27\% on one of the datasets. Also, it consistently recorded the least classification error rate across several iris datasets with diverse segmentation-errors, which suggest that the medoids based model is relatively more robust than the examined counterparts.},
  archive      = {J_PR},
  author       = {Ugochi U.C. Ejiogu and Ogechukwu N. Iloanusi},
  doi          = {10.1016/j.patcog.2022.109290},
  journal      = {Pattern Recognition},
  pages        = {109290},
  shortjournal = {Pattern Recognition},
  title        = {Real time iris segmentation quality evaluation using medoids},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight network for smoke semantic segmentation.
<em>PR</em>, <em>137</em>, 109289. (<a
href="https://doi.org/10.1016/j.patcog.2022.109289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To obtain real-time performance on computation limited devices, we propose a lightweight network for smoke segmentation. To enhance the ability of feature encoding, we first propose an Attention Encoding Module (AEM) by designing a Channel Split and Shuffle Attention Module (CSSAM), which can extract powerful features and reduce computations simultaneously. CSSAM adopts Channel split and shuffle to greatly reduce learnable parameters for improving computation speed, and uses attention mechanism to focus on salient objects to enhance the effectiveness of features. In addition, AEM repeatedly stacks CSSAM in different encoding stages to achieve scale invariance. For the middle-level features of encoding stages, we propose a Spatial Enhancement Module (SEM) to boost the representation ability of spatial details. SEM concatenates feature maps produced by average and maximum pooling to achieve dominant and global responses, which are then weighted by the activated output of global average pooling to generate attention features. In the highest level of encoding stages, we present a Channel Attention Module (CAM) to explicitly model interdependency between channels. By reshaping 2D features into 1D features, we use element-wise matrix multiplications to reduce computation complexity for extracting channel-related information. Finally, we design a Feature Fusion Module (FFM) and a Global Coefficient Path (GCP) to fuse the outputs of SEM and CAM in an attention way for further improving robustness of final features. Experiments show that our method is significantly superior to existing state-of-the-art algorithms in smoke datasets, and also obtains excellent results in both synthetic and real smoke datasets. However, our method has less than 1 M network parameters.},
  archive      = {J_PR},
  author       = {Feiniu Yuan and Kang Li and Chunmei Wang and Zhijun Fang},
  doi          = {10.1016/j.patcog.2022.109289},
  journal      = {Pattern Recognition},
  pages        = {109289},
  shortjournal = {Pattern Recognition},
  title        = {A lightweight network for smoke semantic segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 2D image head pose estimation via latent space regression
under occlusion settings. <em>PR</em>, <em>137</em>, 109288. (<a
href="https://doi.org/10.1016/j.patcog.2022.109288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head orientation is a challenging Computer Vision problem that has been extensively researched having a wide variety of applications. However, current state-of-the-art systems still underperform in the presence of occlusions and are unreliable for many task applications in such scenarios. This work proposes a novel deep learning approach for the problem of head pose estimation under occlusions. The strategy is based on latent space regression as a fundamental key to better structure the problem for occluded scenarios. Our model surpasses several state-of-the-art methodologies for occluded HPE, and achieves similar accuracy for non-occluded scenarios. We demonstrate the usefulness of the proposed approach with: (i) two synthetically occluded versions of the BIWI and AFLW2000 datasets, (ii) real-life occlusions of the Pandora dataset, and (iii) a real-life application to human-robot interaction scenarios where face occlusions often occur. Specifically, the autonomous feeding from a robotic arm .},
  archive      = {J_PR},
  author       = {José Celestino and Manuel Marques and Jacinto C. Nascimento and João Paulo Costeira},
  doi          = {10.1016/j.patcog.2022.109288},
  journal      = {Pattern Recognition},
  pages        = {109288},
  shortjournal = {Pattern Recognition},
  title        = {2D image head pose estimation via latent space regression under occlusion settings},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Overcoming weaknesses of density peak clustering using a
data-dependent similarity measure. <em>PR</em>, <em>137</em>, 109287.
(<a href="https://doi.org/10.1016/j.patcog.2022.109287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density Peak Clustering (DPC) is a popular state-of-the-art clustering algorithm , which requires pairwise (dis)similarity of data objects to detect arbitrary shaped clusters. While it is shown to perform well for many applications, DPC remains: (i) not robust for datasets with clusters having different densities, and (ii) sensitive to the change in the units/scales used to represent data. These drawbacks are mainly due to the use of the data-independent similarity measure based on the Euclidean distance . In this paper, we address these issues by proposing an effective data-dependent similarity measure based on Probability Mass , which we call MP-Similarity , and by incorporating it in DPC to create MP-DPC, a data-dependent variant of DPC. We evaluate and compare MP-DPC against diverse baselines using several clustering metrics and datasets. Our experiments demonstrate that: (a) MP-DPC produces better clustering results than DPC using the Euclidean distance and existing data-dependent similarity measures; (b) MP-Similarity coupled with Shared-Nearest-Neighbor-based density metric in DPC further enhances the quality of clustering results ; and (c) unlike DPC with existing data-independent and data-dependent similarity measures, MP-DPC is robust to the change in the units/scales used to represent data. Our findings suggest that MP-Similarity provides a more viable solution for DPC in datasets with unknown distribution or units/scales of features, which is often the case in many real-world applications.},
  archive      = {J_PR},
  author       = {Zafaryab Rasool and Sunil Aryal and Mohamed Reda Bouadjenek and Richard Dazeley},
  doi          = {10.1016/j.patcog.2022.109287},
  journal      = {Pattern Recognition},
  pages        = {109287},
  shortjournal = {Pattern Recognition},
  title        = {Overcoming weaknesses of density peak clustering using a data-dependent similarity measure},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating transferable adversarial examples for speech
classification. <em>PR</em>, <em>137</em>, 109286. (<a
href="https://doi.org/10.1016/j.patcog.2022.109286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the success of deep neural networks , the existence of adversarial attacks has revealed the vulnerability of neural networks in terms of security. Adversarial attacks add subtle noise to the original example, resulting in a false prediction. Although adversarial attacks have been mainly studied in the image domain, a recent line of research has discovered that speech classification systems are also exposed to adversarial attacks. By adding inaudible noise, an adversary can deceive speech classification systems and cause fatal issues in various applications, such as speaker identification and command recognition tasks. However, research on the transferability of audio adversarial examples is still limited. Thus, in this study, we first investigate the transferability of audio adversarial examples with different structures and conditions. Through extensive experiments, we discover that the transferability of audio adversarial examples is related to their noise sensitivity. Based on the analyses, we present a new adversarial attack called noise injected attack that generates highly transferable audio adversarial examples by injecting additive noise during the gradient ascent process. Our experimental results demonstrate that the proposed method outperforms other adversarial attacks in terms of transferability.},
  archive      = {J_PR},
  author       = {Hoki Kim and Jinseong Park and Jaewook Lee},
  doi          = {10.1016/j.patcog.2022.109286},
  journal      = {Pattern Recognition},
  pages        = {109286},
  shortjournal = {Pattern Recognition},
  title        = {Generating transferable adversarial examples for speech classification},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The performance index of convolutional neural network-based
classifiers in class imbalance problem. <em>PR</em>, <em>137</em>,
109284. (<a href="https://doi.org/10.1016/j.patcog.2022.109284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a common problem in many classification domains. This paper provides an evaluation index and one algorithm for this problem based on binary classification . The Model Performance Index ( MPI ) is proposed for assessing classifier performance as a new evaluation metric , considering class imbalance impacts. Based on MPI , we investigate algorithms to estimate ideal classifier performance with a fair distribution (1:1) , referred to as the Ideal Model Performance Algorithm. Experimentally, compared with traditional metrics, MPI is more sensitive. Specifically, it can detect all types of changes in classifier performances, while others might remain at the same levels. Moreover, for the estimation of classifier performances, the algorithm reaches small differences between predictions and the values observed. Generally, for ideal performances, it achieved error rates of 0.060\% - 1.3\% for rare class in four experiments, showing a practical value on estimation and representation on the classifier performances.},
  archive      = {J_PR},
  author       = {Yanchen Liu and King Wai Chiu Lai},
  doi          = {10.1016/j.patcog.2022.109284},
  journal      = {Pattern Recognition},
  pages        = {109284},
  shortjournal = {Pattern Recognition},
  title        = {The performance index of convolutional neural network-based classifiers in class imbalance problem},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rectified euler k-means and beyond. <em>PR</em>,
<em>137</em>, 109283. (<a
href="https://doi.org/10.1016/j.patcog.2022.109283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Euler k k -means (EulerK) first maps data onto the unit hyper-sphere surface of equi-dimensional space via a complex mapping which induces the robust Euler kernel and next employs the popular k k -means. Consequently, besides enjoying the virtues of k k -means such as simplicity and scalability to large data sets, EulerK is also robust to noises and outliers. Although so, the centroids captured by EulerK deviate from the unit hyper-sphere surface and thus in strict distributional sense, actually are outliers. This weird phenomenon also occurs in some generic kernel clustering methods . Intuitively, using such outlier-like centroids should not be quite reasonable but it is still seldom attended. To eliminate the deviation, we propose two R ectified E uler k k -means methods, i.e., REK1 and REK2, which retain the merits of EulerK while acquiring real centroids residing on the mapped space to better characterize the data structures . Specifically, REK1 rectifies EulerK by imposing the constraint on the centroids while REK2 views each centroid as the mapped image from a pre-image in the original space and optimizes these pre-images in Euler kernel induced space. Undoubtedly, our proposed REKs can methodologically be extended to solve problems of such a category. Finally, the experiments validate the effectiveness of REK1 and REK2.},
  archive      = {J_PR},
  author       = {Yunxia Lin and Songcan Chen},
  doi          = {10.1016/j.patcog.2022.109283},
  journal      = {Pattern Recognition},
  pages        = {109283},
  shortjournal = {Pattern Recognition},
  title        = {Rectified euler k-means and beyond},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised semi-supervised nonnegative matrix
factorization for data clustering. <em>PR</em>, <em>137</em>, 109282.
(<a href="https://doi.org/10.1016/j.patcog.2022.109282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised nonnegative matrix factorization exploits the strengths of matrix factorization in successfully learning part-based representation and is also able to achieve high learning performance when facing a scarcity of labeled data and a large amount of unlabeled data . Its major challenge lies in how to learn more discriminative representations from limited labeled data . Furthermore, self-supervised learning has been proven very effective at learning representations from unlabeled data in various learning tasks. Recent research works focus on utilizing the capacity of self-supervised learning to enhance semi-supervised learning. In this paper, we design an effective Self-Supervised Semi-Supervised Nonnegative Matrix Factorization (S 4 NMF) in a semi-supervised clustering setting. The S 4 NMF directly extracts a consensus result from ensembled NMFs with similarity and dissimilarity regularizations . In an iterative process, this self-supervisory information will be fed back to the proposed model to boost semi-supervised learning and form more distinct clusters. The proposed iterative algorithm is used to solve the given problem, which is defined as an optimization problem with a well-formulated objective function. In addition, the theoretical and empirical analyses investigate the convergence of the proposed optimization algorithm . To demonstrate the effectiveness of the proposed model in semi-supervised clustering, we conduct extensive experiments on standard benchmark datasets. The source code for reproducing our results can be found at https://github.com/ChavoshiNejad/S4NMF .},
  archive      = {J_PR},
  author       = {Jovan Chavoshinejad and Seyed Amjad Seyedi and Fardin Akhlaghian Tab and Navid Salahian},
  doi          = {10.1016/j.patcog.2022.109282},
  journal      = {Pattern Recognition},
  pages        = {109282},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised semi-supervised nonnegative matrix factorization for data clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatically weighted binary multi-view clustering via deep
initialization (AW-BMVC). <em>PR</em>, <em>137</em>, 109281. (<a
href="https://doi.org/10.1016/j.patcog.2022.109281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is inherently a process of exploratory data analysis. It has attracted more attention recently because much real-world data consists of multiple representations or views. However, it becomes increasingly problematic when dealing with large and heterogeneous data . It is worth noting that several approaches have been developed to increase computational efficiency, although most of them have some drawbacks: (1) Most existing techniques consider equal or static weights to quantify importance across different views and samples, so common and complementary features cannot be used. (2) The clustering task is performed by arbitrary initialization without caring about the rich structure of the joint discrete representation, and thus poorly executed. In this paper, we propose a novel approach called “Auto-Weighted Binary Multi-View Clustering Via Deep Initialization” for large-scale multi-view clustering based on two main scenarios. First, we consider the distinction between different views based on the importance of samples, and therefore apply a dynamic learning strategy for the automatic weighting of views and samples. Second, in the context of initializing binary clustering, we develop a new CNN feature and use a low-dimensional binary embedding by exploiting the efficient capabilities of Fourier mapping. Moreover, our approach simultaneously learns a joint discrete representation and performs direct clustering using a constrained binary matrix factorization ; the optimization problem is perfectly solved in a unified learning model. Experimental results conducted on several challenging datasets demonstrate the effectiveness and superiority of the proposed approach over state-of-the-art methods in terms of accuracy, normalized mutual information, and purity.},
  archive      = {J_PR},
  author       = {Khamis Houfar and Djamel Samai and Fadi Dornaika and Azeddine Benlamoudi and Khaled Bensid and Abdelmalik Taleb-Ahmed},
  doi          = {10.1016/j.patcog.2022.109281},
  journal      = {Pattern Recognition},
  pages        = {109281},
  shortjournal = {Pattern Recognition},
  title        = {Automatically weighted binary multi-view clustering via deep initialization (AW-BMVC)},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CrossRectify: Leveraging disagreement for semi-supervised
object detection. <em>PR</em>, <em>137</em>, 109280. (<a
href="https://doi.org/10.1016/j.patcog.2022.109280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised object detection has recently achieved substantial progress. As a mainstream solution, the self-labeling-based methods train the detector on both labeled data and unlabeled data with pseudo labels predicted by the detector itself, but their performances are always limited. Through experimental analysis, we reveal the underlying reason is that the detector is misguided by the incorrect pseudo labels predicted by itself (dubbed self-errors). These self-errors can hurt performance even worse than random-errors, and can be neither discerned nor rectified during the self-labeling process. In this paper, we propose an effective detection framework named CrossRectify, to obtain accurate pseudo labels by simultaneously training two detectors with different initial parameters. Specifically, the proposed approach leverages the disagreements between detectors to discern the self-errors and refines the pseudo label quality by the proposed cross-rectifying mechanism. Extensive experiments show that CrossRectify achieves outperforming performances over various detector structures on 2D and 3D detection benchmarks.},
  archive      = {J_PR},
  author       = {Chengcheng Ma and Xingjia Pan and Qixiang Ye and Fan Tang and Weiming Dong and Changsheng Xu},
  doi          = {10.1016/j.patcog.2022.109280},
  journal      = {Pattern Recognition},
  pages        = {109280},
  shortjournal = {Pattern Recognition},
  title        = {CrossRectify: Leveraging disagreement for semi-supervised object detection},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised foreground learning for weakly supervised
localization and detection. <em>PR</em>, <em>137</em>, 109279. (<a
href="https://doi.org/10.1016/j.patcog.2022.109279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep learning models require large amounts of accurately annotated data, which is often difficult to satisfy. Hence, weakly supervised tasks, including weakly supervised object localization (WSOL) and detection (WSOD), have recently received attention in the computer vision community. In this paper, we motivate and propose the weakly supervised foreground learning (WSFL) task by showing that both WSOL and WSOD can be greatly improved if groundtruth foreground masks are available. More importantly, we propose a complete WSFL pipeline with low computational cost, which generates pseudo boxes, learns foreground masks, and does not need any localization annotations. With the help of foreground masks predicted by our WSFL model, we achieve 74.37\% correct localization accuracy on CUB for WSOL, and 55.7\% mean average precision on VOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL model also shows excellent transfer ability.},
  archive      = {J_PR},
  author       = {Chen-Lin Zhang and Yin Li and Jianxin Wu},
  doi          = {10.1016/j.patcog.2022.109279},
  journal      = {Pattern Recognition},
  pages        = {109279},
  shortjournal = {Pattern Recognition},
  title        = {Weakly supervised foreground learning for weakly supervised localization and detection},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object detection based on cortex hierarchical activation in
border sensitive mechanism and classification-GIou joint representation.
<em>PR</em>, <em>137</em>, 109278. (<a
href="https://doi.org/10.1016/j.patcog.2022.109278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By imitating the brain neurons for object perception, the deep networks enable a comprehensive feature characterization in the task of object detection. Considering such a perceptual ability is usually bounded in a box area for feature extraction, the balance of dimension reduction and feature information retaining has been taken into account in more recent studies, especially for the information preservation in the border areas. Motivated by the mechanism of neuron cortex activation, in this work, a novel function based on cortex hierarchical activation is proposed to achieve more effective border sensitive mechanism by joint pooling in backbone networks . In order to avoid the parameter solidification , this strategy is also capable to benefit the feature extraction on the border without unnecessary model re-training. Furthermore, by replacing the square kernel with a designed band shape kernel, more adequate feature description can be obtained on the border via the combination of the strip hierarchical pooling and strip max pooling. With an extension of the proposed activation function on classification-GIoU joint representation, the overall detection accuracy has been further improved. Experimental evaluations on the COCO benchmark datasets have shown that the proposed work has a superior performance in comparison to other state-of-the-art detection approaches.},
  archive      = {J_PR},
  author       = {Yaoye Song and Peng Zhang and Wei Huang and Yufei Zha and Tao You and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.109278},
  journal      = {Pattern Recognition},
  pages        = {109278},
  shortjournal = {Pattern Recognition},
  title        = {Object detection based on cortex hierarchical activation in border sensitive mechanism and classification-GIou joint representation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight semi-supervised network for single image rain
removal. <em>PR</em>, <em>137</em>, 109277. (<a
href="https://doi.org/10.1016/j.patcog.2022.109277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning technologies have shown their advantages in Single Image Rain Removal (SIRR) tasks. However, the derained results of most methods are limited to some challenges. First, due to the lack of real-world rainy/clean image pairs, many methods seriously rely on the labeled synthetic training images and will not effectively remove complex rain streaks in real-world scenarios. Second, most existing SIRR models require high computing power, which considerably limits their real-world applications. To address these issues, we propose a Lightweight Semi-supervised Network (LSNet) for SIRR. Our LSNet utilizes a compact semi-supervised framework to improve generalization ability in real-world rainy images removal. Meanwhile, in our semi-supervised framework, we also design a cascaded sub-network, which progressively removes complex rain streaks via a multi-stage manner. Specially, the multi-stage manner is based on a series of cascaded blocks, where we conduct recursive learning strategy to reduce model parameters. Extensive experimental results demonstrate that our method achieves comparable performance to the state-of-the-arts while has fewer parameters.},
  archive      = {J_PR},
  author       = {Nanfeng Jiang and Jiawei Luo and Junhong Lin and Weiling Chen and Tiesong Zhao},
  doi          = {10.1016/j.patcog.2022.109277},
  journal      = {Pattern Recognition},
  pages        = {109277},
  shortjournal = {Pattern Recognition},
  title        = {Lightweight semi-supervised network for single image rain removal},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep continual hashing with gradient-aware memory for
cross-modal retrieval. <em>PR</em>, <em>137</em>, 109276. (<a
href="https://doi.org/10.1016/j.patcog.2022.109276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing (CMH) has become widely used for large-scale multimedia retrieval . However, most current CMH methods focus on the closed retrieval scenario, not the real-world environments, i.e., complex and changing semantics. When data containing new class objects emerge, the current CMH has to retrain the model on all history training data, not the new data, to accommodate new semantics, but the never-stop upload of data on the Internet makes this impractical. In this paper, we devise a deep hashing method called Continual Cross-Modal Hashing with Gradient Aware Memory (CCMH-GAM) for learning binary codes of multi-label cross-modal data with increasing categories. CCMH-GAM is a two-step hashing architecture, one hashing network learns to hash the increasing semantics of data, i.e., label, into the semantic codes, and other modality-specific hashing networks learn to map data into the corresponding semantic codes. Specifically, to keep the encoding ability for old semantics, a regularization based on accumulating low-storage label-code pairs is designed for the former network. For the modality-specific networks, we propose a memory construction method via approximating the full episodic gradients of all data by some exemplars and derive its fast implementation with the upper bound of approximation error. Based on this memory, we propose a gradient projection method to theoretically improve the probability of old data’s code being unchanged after updating the model. Extensive experiments on three datasets demonstrate that CCMH-GAM can continually learn hash functions and yield state-of-the-art retrieval performance .},
  archive      = {J_PR},
  author       = {Ge Song and Xiaoyang Tan and Ming Yang},
  doi          = {10.1016/j.patcog.2022.109276},
  journal      = {Pattern Recognition},
  pages        = {109276},
  shortjournal = {Pattern Recognition},
  title        = {Deep continual hashing with gradient-aware memory for cross-modal retrieval},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RESKM: A general framework to accelerate large-scale
spectral clustering. <em>PR</em>, <em>137</em>, 109275. (<a
href="https://doi.org/10.1016/j.patcog.2022.109275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral Clustering is an effective preprocessing method in communities for its excellent performance, but its scalability still is a challenge. Many efforts have been made to face this problem, and several solutions are proposed, including Nyström Approximation , Sparse Representation Approximation , etc. However, according to our survey, there is still a large room for improvement. This work thoroughly investigates the factors relevant to large-scale Spectral Clustering and proposes a general framework to accelerate Spectral Clustering by utilizing the Robust and Efficient Spectral k-Means (RESKM). The contributions of RESKM are three folds: (1) a unified framework is proposed for large-scale Spectral Clustering; (2) it consists of four phases, each phase is theoretically analyzed, and the corresponding acceleration is suggested; (3) the majority of the existing large-scale Spectral Clustering methods can be integrated into RESKM and therefore be accelerated. Experiments on datasets with different scalability demonstrate that the robustness and efficiency of RESKM.},
  archive      = {J_PR},
  author       = {Geping Yang and Sucheng Deng and Xiang Chen and Can Chen and Yiyang Yang and Zhiguo Gong and Zhifeng Hao},
  doi          = {10.1016/j.patcog.2022.109275},
  journal      = {Pattern Recognition},
  pages        = {109275},
  shortjournal = {Pattern Recognition},
  title        = {RESKM: A general framework to accelerate large-scale spectral clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypergraph based semi-supervised symmetric nonnegative
matrix factorization for image clustering. <em>PR</em>, <em>137</em>,
109274. (<a href="https://doi.org/10.1016/j.patcog.2022.109274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised symmetric nonnegative matrix factorization (SNMF) has been shown to be a significant method for both linear and nonlinear data clustering applications. Nevertheless, existing SNMF-based methods only adopt a simple graph to construct the similarity matrix , and cannot fully use the limited supervised information for the construction of the similarity matrix . To overcome the drawbacks of previous SNMF-based methods, a new semi-supervised SNMF-based method called hypergraph based semi-supervised SNMF (HSSNMF), is proposed in this paper for image clustering. Specifically, HSSNMF adopts a predefined hypergraph to build a similarity matrix for capturing the high-order relationships of samples. By exploiting a new hypergraph based pairwise constraints propagation (HPCP) algorithm, HSSNMF propagates the pairwise constraints of the limited data points to the entire data points, which can make full use of the limited supervised information and construct a more informative similarity matrix. Using the multiplicative updating algorithm, a discriminative assignment matrix can then be obtained by solving the optimization problem of HSSNMF. Moreover, analyses of the convergence, supervisory information, and computational complexity of HSSNMF are presented. Finally, extensive clustering experiments have been conducted on six real-world image datasets, and the experimental results have demonstrated the superiority of HSSNMF while compared with several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jingxing Yin and Siyuan Peng and Zhijing Yang and Badong Chen and Zhiping Lin},
  doi          = {10.1016/j.patcog.2022.109274},
  journal      = {Pattern Recognition},
  pages        = {109274},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph based semi-supervised symmetric nonnegative matrix factorization for image clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive mutual k-nearest neighbors clustering algorithm
based on maximizing mutual information. <em>PR</em>, <em>137</em>,
109273. (<a href="https://doi.org/10.1016/j.patcog.2022.109273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering based on Mutual K-nearest Neighbors (CMNN) is a classical method of grouping data into different clusters. However, it has two well-known limitations: (1) the clustering results are very much dependent on the parameter k ; (2) CMNN assumes that noise points correspond to clusters of small sizes according to the Mutual K-nearest Neighbors (MKNN) criterion, but some data points in small size clusters are wrongly identified as noises. To address these two issues, we propose an adaptive improved CMNN algorithm (AVCMNN), which consists of two parts: (1) improved CMNN algorithm (abbreviated as VCMNN) and (2) adaptive VCMNN algorithm (abbreviated as AVCMNN). Specifically, the first part is VCMNN algorithm, we first reassign the data points in some small-size clusters by a novel voting strategy because some of them are wrongly identified as noise points, and the clustering results are improved. Then, the second part is AVCMNN, we use maximizing mutual information to construct an objective function to optimize the parameters of the proposed method and finally obtain the better parameters values and clustering results. We conduct extensive experiments on twenty datasets, including six synthetic datasets , ten UCI datasets, and four image datasets. The experimental results show that VCMNN and AVCMNN outperforms three classical algorithms ( i.e. , CMNN, DPC, and DBSCAN) and six state-of-the-art (SOTA) clustering algorithms in most cases.},
  archive      = {J_PR},
  author       = {Yizhang Wang and Wei Pang and Zhixiang Jiao},
  doi          = {10.1016/j.patcog.2022.109273},
  journal      = {Pattern Recognition},
  pages        = {109273},
  shortjournal = {Pattern Recognition},
  title        = {An adaptive mutual K-nearest neighbors clustering algorithm based on maximizing mutual information},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving visual-semantic embeddings by learning
semantically-enhanced hard negatives for cross-modal information
retrieval. <em>PR</em>, <em>137</em>, 109272. (<a
href="https://doi.org/10.1016/j.patcog.2022.109272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Semantic Embedding (VSE) networks aim to extract the semantics of images and their descriptions and embed them into the same latent space for cross-modal information retrieval. Most existing VSE networks are trained by adopting a hard negatives loss function which learns an objective margin between the similarity of relevant and irrelevant image–description embedding pairs. However, the objective margin in the hard negatives loss function is set as a fixed hyperparameter that ignores the semantic differences of the irrelevant image–description pairs. To address the challenge of measuring the optimal similarities between image–description pairs before obtaining the trained VSE networks, this paper presents a novel approach that comprises two main parts: (1) finds the underlying semantics of image descriptions; and (2) proposes a novel semantically-enhanced hard negatives loss function, where the learning objective is dynamically determined based on the optimal similarity scores between irrelevant image–description pairs. Extensive experiments were carried out by integrating the proposed methods into five state-of-the-art VSE networks that were applied to three benchmark datasets for cross-modal information retrieval tasks. The results revealed that the proposed methods achieved the best performance and can also be adopted by existing and future VSE networks.},
  archive      = {J_PR},
  author       = {Yan Gong and Georgina Cosma},
  doi          = {10.1016/j.patcog.2022.109272},
  journal      = {Pattern Recognition},
  pages        = {109272},
  shortjournal = {Pattern Recognition},
  title        = {Improving visual-semantic embeddings by learning semantically-enhanced hard negatives for cross-modal information retrieval},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Riemannian representation learning for multi-source domain
adaptation. <em>PR</em>, <em>137</em>, 109271. (<a
href="https://doi.org/10.1016/j.patcog.2022.109271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Source Domain Adaptation (MSDA) aims at training a classification model that achieves small target error, by leveraging labeled data from multiple source domains and unlabeled data from a target domain. The source and target domains are described by related but different joint distributions, which lie on a Riemannian manifold named the statistical manifold. In this paper, we characterize the joint distribution difference by the Hellinger distance, which bears strong connection to the Riemannian metric defined on the statistical manifold. We show that the target error of a neural network classification model is upper bounded by the average source error of the model and the average Hellinger distance, i.e., the average of multiple Hellinger distances between the source and target joint distributions in the network representation space. Motivated by the error bound, we introduce Riemannian Representation Learning (RRL): An approach that trains the network model by minimizing (i) the average empirical Hellinger distance with respect to the representation function, and (ii) the average empirical source error with respect to the network model. Specifically, we derive the average empirical Hellinger distance by constructing and solving unconstrained convex optimization problems whose global optimal solutions are easy to find. With the network model trained, we expect it to achieve small error in the target domain. Our experimental results on several image datasets demonstrate that the proposed RRL approach is statistically better than the comparison methods.},
  archive      = {J_PR},
  author       = {Sentao Chen and Lin Zheng and Hanrui Wu},
  doi          = {10.1016/j.patcog.2022.109271},
  journal      = {Pattern Recognition},
  pages        = {109271},
  shortjournal = {Pattern Recognition},
  title        = {Riemannian representation learning for multi-source domain adaptation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid routing transformer for zero-shot learning.
<em>PR</em>, <em>137</em>, 109270. (<a
href="https://doi.org/10.1016/j.patcog.2022.109270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to learn models that can recognize unseen image semantics based on the training of data with seen semantics. Recent studies either leverage the global image features or mine discriminative local patch features to associate the extracted visual features to the semantic attributes. However, due to the lack of the necessary top-down guidance and semantic alignment for ensuring the model attend to the real attribute-correlation regions, these methods still encounter a significant semantic gap between the visual modality and the attribute modality, which makes their prediction on unseen semantics unreliable. To solve this problem, this paper establishes a novel transformer encoder-decoder model, called hybrid routing transformer (HRT). In HRT encoder, we embed an active attention, which is constructed by both the bottom-up and the top-down dynamic routing pathways to generate the attribute-aligned visual feature. While in HRT decoder, we use static routing to calculate the correlation among the attribute-aligned visual features, the corresponding attribute semantics, and the class attribute vectors to generate the final class label predictions. This design makes the presented transformer model a hybrid of 1) top-down and bottom-up attention pathways and 2) dynamic and static routing pathways. Comprehensive experiments on three widely-used benchmark datasets, namely CUB, SUN, and AWA2, are conducted. The obtained experimental results demonstrate the effectiveness of the proposed method. Our code is released in https://github.com/KORIYN/HRT .},
  archive      = {J_PR},
  author       = {De Cheng and Gerong Wang and Bo Wang and Qiang Zhang and Jungong Han and Dingwen Zhang},
  doi          = {10.1016/j.patcog.2022.109270},
  journal      = {Pattern Recognition},
  pages        = {109270},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid routing transformer for zero-shot learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to use k-means for big data clustering? <em>PR</em>,
<em>137</em>, 109269. (<a
href="https://doi.org/10.1016/j.patcog.2022.109269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-means plays a vital role in data mining and is the simplest and most widely used algorithm under the Euclidean Minimum Sum-of-Squares Clustering (MSSC) model. However, its performance drastically drops when applied to vast amounts of data. Therefore, it is crucial to improve K-means by scaling it to big data using as few of the following computational resources as possible: data, time, and algorithmic ingredients. We propose a new parallel scheme of using K-means and K-means++ algorithms for big data clustering that satisfies the properties of a “true big data” algorithm and outperforms the classical and recent state-of-the-art MSSC approaches in terms of solution quality and runtime. The new approach naturally implements global search by decomposing the MSSC problem without using additional metaheuristics . This work shows that data decomposition is the basic approach to solve the big data clustering problem. The empirical success of the new algorithm allowed us to challenge the common belief that more data is required to obtain a good clustering solution. Moreover, the present work questions the established trend that more sophisticated hybrid approaches and algorithms are required to obtain a better clustering solution.},
  archive      = {J_PR},
  author       = {Rustam Mussabayev and Nenad Mladenovic and Bassem Jarboui and Ravil Mussabayev},
  doi          = {10.1016/j.patcog.2022.109269},
  journal      = {Pattern Recognition},
  pages        = {109269},
  shortjournal = {Pattern Recognition},
  title        = {How to use K-means for big data clustering?},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Invariance encoding in sliced-wasserstein space for image
classification with limited training data. <em>PR</em>, <em>137</em>,
109268. (<a href="https://doi.org/10.1016/j.patcog.2022.109268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) are broadly considered to be state-of-the-art generic end-to-end image classification systems. However, they are known to underperform when training data are limited and thus require data augmentation strategies that render the method computationally expensive and not always effective. Rather than using a data augmentation strategy to encode invariances as typically done in machine learning , here we propose to mathematically augment a nearest subspace classification model in sliced-Wasserstein space by exploiting certain mathematical properties of the Radon Cumulative Distribution Transform (R-CDT), a recently introduced image transform. We demonstrate that for a particular type of learning problem, our mathematical solution has advantages over data augmentation with deep CNNs in terms of classification accuracy and computational complexity , and is particularly effective under a limited training data setting. The method is simple, effective, computationally efficient, non-iterative, and requires no parameters to be tuned. Python code implementing our method is available at https://github.com/rohdelab/mathematical_augmentation . Our method is integrated as a part of the software package PyTransKit, which is available at https://github.com/rohdelab/PyTransKit .},
  archive      = {J_PR},
  author       = {Mohammad Shifat-E-Rabbi and Yan Zhuang and Shiying Li and Abu Hasnat Mohammad Rubaiyat and Xuwang Yin and Gustavo K. Rohde},
  doi          = {10.1016/j.patcog.2022.109268},
  journal      = {Pattern Recognition},
  pages        = {109268},
  shortjournal = {Pattern Recognition},
  title        = {Invariance encoding in sliced-wasserstein space for image classification with limited training data},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse norm regularized attribute selection for graph neural
networks. <em>PR</em>, <em>137</em>, 109265. (<a
href="https://doi.org/10.1016/j.patcog.2022.109265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been widely used for graph learning tasks. The main aspect of GNN’s layer-wise message passing is conducting attribute/feature propagation on graph. Most existing GNNs generally conduct feature propagation across all feature dimensions. However, in many real applications, attributes usually contain irrelevant and redundant noise. In this case, attribute/feature selection is desired to extract meaningful features and eliminate noisy ones for GNN’s layer-wise propagation. Based on this observation, in this paper, we combine ℓ 2 , 1 / ℓ 1 ℓ2,1/ℓ1 -norm regularized attribute selection and GNNs together and propose a novel Attribute selection guided GNNs (AsGNNs) for graph data representation. AsGNNs aim to adaptively select some desired meaningful features/attributes that best serve GNNs. Moreover, an effective optimization framework has also been derived to train the proposed AsGNNs. The proposed AsGNNs provide a general framework which can incorporate any GNNs to conduct feature selection for layer-wise propagation. In this paper, we implement AsGNNs on both graph convolutional network (GCN) and graph attention network (GAT) and develop AsGCN and AsGAT for graph learning. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed AsGNNs (AsGCN, AsGAT) on semi-supervised learning tasks.},
  archive      = {J_PR},
  author       = {Bo Jiang and Beibei Wang and Bin Luo},
  doi          = {10.1016/j.patcog.2022.109265},
  journal      = {Pattern Recognition},
  pages        = {109265},
  shortjournal = {Pattern Recognition},
  title        = {Sparse norm regularized attribute selection for graph neural networks},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-order similarity learning for multi-view spectral
clustering. <em>PR</em>, <em>137</em>, 109264. (<a
href="https://doi.org/10.1016/j.patcog.2022.109264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the problem of multi-view spectral clustering (MVSC) based on multi-order similarity learning. Unlike the existing methods that focus on direct similarity of pairwise data points without considering the hidden multi-order similarity among different data points, a novel multi-order similarity learning model for MVSC (MOSL) is proposed. Specifically, the first-order similarity (FOS) and second-order similarity (SOS) are learned to excavate the local structure relation and adjacent structure relation of pairwise data points. Afterwards, the third-order similarity (TOS) based on low-rank tensor is learned to excavate the view-specific information and consensus information from multiple views. Moreover, a trace constraint on each affinity graph from multiple views is learned to ensure the strict block diagonal structure of each affinity graph. Extensive experiments on six commonly benchmark datasets show that the proposed method outperforms state-of-the-art methods in most scenarios and is capable of revealing a reliable affinity graph structure concealed in different data points.},
  archive      = {J_PR},
  author       = {Yanying Mei and Zhenwen Ren and Bin Wu and Tao Yang and Yanhua Shao},
  doi          = {10.1016/j.patcog.2022.109264},
  journal      = {Pattern Recognition},
  pages        = {109264},
  shortjournal = {Pattern Recognition},
  title        = {Multi-order similarity learning for multi-view spectral clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual self-attention mechanism for vehicle
re-identification. <em>PR</em>, <em>137</em>, 109258. (<a
href="https://doi.org/10.1016/j.patcog.2022.109258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle re-identification has attracted tremendous attention from computer vision communities for its extensive applications in intelligent transportation and public security, while the high inter-class similarity and the large intra-class difference between vehicles bring out great challenges for re-identification (re-ID). To tackle these challenges, we learn from the self-attention mechanism in Natural Language Processing and propose a dual self-attention module to learn different regional dependencies: static self-attention for selectively refining semantic features and dynamic self-attention (called cross-region attention) for enhancing the spatial awareness of local feature . The static self-attention refines attended pixels within the entire image and salient regions , while the cross-region attention creatively captures the position-related regional dependencies for pixels within the windscreen area. These attention modules capture long-range dependencies and relative position information between different pixels or regions for vehicle feature learning globally and locally, realizing an efficient vehicle feature embedding by concatenating these augmented features for vehicle re-ID. Extensive experiments demonstrate the effectiveness and promising performance of our approach against the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Wenqian Zhu and Zhongyuan Wang and Xiaochen Wang and Ruimin Hu and Huikai Liu and Cheng Liu and Chao Wang and Dengshi Li},
  doi          = {10.1016/j.patcog.2022.109258},
  journal      = {Pattern Recognition},
  pages        = {109258},
  shortjournal = {Pattern Recognition},
  title        = {A dual self-attention mechanism for vehicle re-identification},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Control distance IoU and control distance IoU loss for
better bounding box regression. <em>PR</em>, <em>137</em>, 109256. (<a
href="https://doi.org/10.1016/j.patcog.2022.109256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous improvements in feedback mechanisms have contributed to the great progress in object detection. In this paper, we first present an evaluation-feedback module, which consists of an evaluation system and feedback mechanism. Then we analyze and summarize traditional evaluation-feedback modules. We focus on both the evaluation system and the feedback mechanism, and propose C ontrol D istance IoU and C ontrol D istance IoU loss function (CDIoU and CDIoU loss) without increasing parameters in models, which make significant enhancements on several classical and emerging models. Finally, we propose A utomatic G round T ruth C lustering (AGTC) and F loating L earning R ate D ecay (FLRD) for faster regression in object detection. Experiments show that a coordinated evaluation-feedback module can effectively improve model performance. Both CNN and transformer-based detectors with CDIoU + CDIoU loss, AGTC , and FLRD achieve excellent performances. There are a maximum AP improvement of 2.9\%, an average AP of 1.1\% improvement on MS COCO, a maximum AP improvement of 8.2\%, and an average AP improvement of 3.7\% on Visdrone dataset.},
  archive      = {J_PR},
  author       = {Chen Dong and Miao Duoqian},
  doi          = {10.1016/j.patcog.2022.109256},
  journal      = {Pattern Recognition},
  pages        = {109256},
  shortjournal = {Pattern Recognition},
  title        = {Control distance IoU and control distance IoU loss for better bounding box regression},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RGB-d salient object ranking based on depth stack and truth
stack for complex indoor scenes. <em>PR</em>, <em>137</em>, 109251. (<a
href="https://doi.org/10.1016/j.patcog.2022.109251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D salient object detection has achieved a great development in recent years due to its extensive applications. Previous studies mainly focus on simple scene images with one single object. These models usually become overwhelmed by complex scenes with multiple objects. Moreover, these methods model salient object detection as a binary segmentation problem. However, psychology studies show that humans shift their visual attention from one object to another and rank salient objects, especially in complex indoor scenes. Following the psychological studies, we propose to rank salient objects in RGB-D images of complex indoor scenes. Due to the lack of such data, we first construct a RGB-D salient object ranking dataset containing complex indoor scenes with multiple objects. The saliency ranking of different objects is defined based on the order that an observer notices these objects. The final salient object ranking result is an average across the saliency rankings of 13 observers. This RGB-D salient object ranking dataset is also analyzed with current mainstream RGB-D salient object detection dataset for comparison. Since location information provided by depth images can help to determine the saliency ranking of objects, we further propose an end-to-end network exploiting depth stack and ground truth stack to predict the order of salient objects in complex scenes. The quantitative and qualitative comparisons demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Jingzheng Deng and Jinxia Zhang and Zewen Hu and Liantao Wang and Jiacheng Jiang and Xinchao Zhu and Xinyi Chen and Yin Yuan and Chao Wang},
  doi          = {10.1016/j.patcog.2022.109251},
  journal      = {Pattern Recognition},
  pages        = {109251},
  shortjournal = {Pattern Recognition},
  title        = {RGB-D salient object ranking based on depth stack and truth stack for complex indoor scenes},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pose-driven attention-guided image generation for person
re-identification. <em>PR</em>, <em>137</em>, 109246. (<a
href="https://doi.org/10.1016/j.patcog.2022.109246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) concerns the matching of subject images across different camera views in a multi camera surveillance system. One of the major challenges in person re-ID is pose variations across the camera network, which significantly affects the appearance of a person. Existing development data lack adequate pose variations to carry out effective training of person re-ID systems. To solve this issue, in this paper we propose an end-to-end pose-driven attention-guided generative adversarial network , to generate multiple poses of a person. We propose to attentively learn and transfer the subject pose through an attention mechanism . A semantic-consistency loss is proposed to preserve the semantic information of the person during pose transfer. To ensure fine image details are realistic after pose translation, an appearance discriminator is used while a pose discriminator is used to ensure the pose of the transferred images will exactly be the same as the target pose. We show that by incorporating the proposed approach in a person re-identification framework, realistic pose transferred images and state-of-the-art re-identification results can be achieved.},
  archive      = {J_PR},
  author       = {Amena Khatun and Simon Denman and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.patcog.2022.109246},
  journal      = {Pattern Recognition},
  pages        = {109246},
  shortjournal = {Pattern Recognition},
  title        = {Pose-driven attention-guided image generation for person re-identification},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diverse image inpainting with disentangled uncertainty.
<em>PR</em>, <em>137</em>, 109243. (<a
href="https://doi.org/10.1016/j.patcog.2022.109243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing inpainting methods repair a corrupted image to a single output, which gives people no choice to select the most satisfactory result. However, image inpainting is essentially a multi-modal problem because the inpainted results could have multiple possibilities. To generate both diverse and realistic inpainted results, we propose a diverse image inpainting framework with disentangled uncertainty. We disentangle the uncertainty of the missing region into two aspects: structure and appearance. Correspondingly, we divide the process of diverse image inpainting into two stages: diverse structure inpainting and diverse appearance inpainting. In the first stage, we restore the structure of the missing region, producing diverse complete edge maps. In the second stage, using a complete edge map as the guidance, we fill in diverse appearance information of the missing region. We also design a light-weighted disentangling subnetwork to disentangle structure information and appearance information. Besides, we propose a novel style-based masked residual block to better deal with the uncertainty. Experiments on CelebA-HQ, Paris Street View, and Places2 demonstrate that our method can repair the corrupted image with higher fidelity and diversity than other existing methods.},
  archive      = {J_PR},
  author       = {Wentao Wang and Lu He and Li Niu and Jianfu Zhang and Yue Liu and Haoyu Ling and Liqing Zhang},
  doi          = {10.1016/j.patcog.2022.109243},
  journal      = {Pattern Recognition},
  pages        = {109243},
  shortjournal = {Pattern Recognition},
  title        = {Diverse image inpainting with disentangled uncertainty},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shallow decision trees for explainable k-means clustering.
<em>PR</em>, <em>137</em>, 109239. (<a
href="https://doi.org/10.1016/j.patcog.2022.109239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of recent works have employed decision trees for the construction of explainable partitions that aim to minimize the k k -means cost function. These works, however, largely ignore metrics related to the depths of the leaves in the resulting tree, which is perhaps surprising considering how the explainability of a decision tree depends on these depths. To fill this gap in the literature, we propose an efficient algorithm with a penalty term in its loss function to favor the construction of shallow decision trees – i.e., trees whose leaves are not very deep, which translate to clusters that are defined by a small number of attributes and are therefore easier to explain. In experiments on 16 datasets, our algorithm yields better results than decision-tree clustering algorithms recently presented in the literature, typically achieving lower or equivalent costs with considerably shallower trees.},
  archive      = {J_PR},
  author       = {Eduardo Laber and Lucas Murtinho and Felipe Oliveira},
  doi          = {10.1016/j.patcog.2022.109239},
  journal      = {Pattern Recognition},
  pages        = {109239},
  shortjournal = {Pattern Recognition},
  title        = {Shallow decision trees for explainable k-means clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning a bi-directional discriminative representation for
deep clustering. <em>PR</em>, <em>137</em>, 109237. (<a
href="https://doi.org/10.1016/j.patcog.2022.109237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, deep clustering achieves superior performance by jointly performing representation learning and cluster assignment. Although numerous deep clustering algorithms have emerged, most of them have difficulty learning representations that fit the clustering distribution. To address this issue, we propose a bi-directional discriminative representation learning clustering (BDRC) framework in this paper. In our framework, a dual autoencoder network, a bi-directional mutual information maximization module and a self-supervised cluster prediction module are combined into a joint optimization framework. To learn more cluster-friendly representations, the bi-directional mutual information maximization module is executed on both samples and their nearest neighbors to explore the cluster relationships between samples. In order to improve the stability of the model, a self-supervised cluster prediction module is devised to predict clustering assignments to supervise the autoencoder using the KL-divergence. Moreover, the UMAP is used to find the manifold of the latent representations which can better preserve the global structure. Experiments on some benchmark datasets demonstrate the superiority of the proposed BDRC algorithm.},
  archive      = {J_PR},
  author       = {Yiming Wang and Dongxia Chang and Zhiqiang Fu and Yao Zhao},
  doi          = {10.1016/j.patcog.2022.109237},
  journal      = {Pattern Recognition},
  pages        = {109237},
  shortjournal = {Pattern Recognition},
  title        = {Learning a bi-directional discriminative representation for deep clustering},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid optimization with unconstrained variables on partial
point cloud registration. <em>PR</em>, <em>136</em>, 109267. (<a
href="https://doi.org/10.1016/j.patcog.2022.109267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud registration is a fundamental problem in computer vision (CV) and computer graphics (CG). Recently, a series of learning-based algorithms have been proposed to show the advantages in registration accuracy and inference speed. However, those learning-based methods usually ignore transformations with constrained rotations and translations in registration. In this paper, we propose a novel hybrid optimization method to solve the constrained rotational and translational transformations. A mapping function is introduced to deal with the restrained variables in optimization. Our method achieves superior performance on the Multi-View Partial Point dataset, which won the first place on the registration challenge in ICCV 2021. The method is also validated on the synthetic datasets ModelNet, ICL-NUIM, and the realistic 3DMatch dataset. We demonstrate that the global optimization methods still have great potential research for point cloud registration. The code is available at https://github.com/Dizzy-cell/HOUV .},
  archive      = {J_PR},
  author       = {Yuanjie Yan and Junyi An and Jian Zhao and Furao Shen},
  doi          = {10.1016/j.patcog.2022.109267},
  journal      = {Pattern Recognition},
  pages        = {109267},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid optimization with unconstrained variables on partial point cloud registration},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdaNS: Adaptive negative sampling for unsupervised graph
representation learning. <em>PR</em>, <em>136</em>, 109266. (<a
href="https://doi.org/10.1016/j.patcog.2022.109266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised graph representation learning has attracted considerable attention through effectively encoding graph-structured data without semantic annotations . To accelerate its training, noise contrastive estimation (NCE) samples uniformly negative examples to fit an unnormalized graph model. However, this uniform sampling strategy may easily lead to slow convergence, even the vanishing gradient problem. In this paper, we theoretically show that sampling those hard negatives close to the current anchor can relieve the above difficulties. With this finding, we then propose an Ada ptive N egative S ampling strategy, namely AdaNS, which efficiently samples the hard negatives from the mixing distribution regarding the dimensional elements of the current node representation. Experiments show that our AdaNS sampling strategy applied on top of representative unsupervised models, e.g., DeepWalk, GraphSAGE, can outperform the existing negative sampling strategies in the tasks of node classification and visualization. This also further demonstrates that sampling those hard negatives can bring performance improvements for learning the node representations.},
  archive      = {J_PR},
  author       = {Yu Wang and Liang Hu and Wanfu Gao and Xiaofeng Cao and Yi Chang},
  doi          = {10.1016/j.patcog.2022.109266},
  journal      = {Pattern Recognition},
  pages        = {109266},
  shortjournal = {Pattern Recognition},
  title        = {AdaNS: Adaptive negative sampling for unsupervised graph representation learning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AuxBranch: Binarization residual-aware network design via
auxiliary branch search. <em>PR</em>, <em>136</em>, 109263. (<a
href="https://doi.org/10.1016/j.patcog.2022.109263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While network binarization is a promising method in memory saving and speedup on hardware, it inevitably leads to binarization residual of intermediate features, resulting in performance capability degradation. To alleviate the above issue, we focus on the network topology design scheme to the more suitable network structure for the extreme-low-bit scenario. In this paper, we propose the baseline-auxiliary expanding network design method to compensate for the binarization residual of features via searching for auxiliary branches, denoted as AuxBranch. The intermediate feature maps are reasonably enhanced by combining baseline and auxiliary features, mimicking the corresponding feature output of the full-precision network. In addition, we devise a hybrid performance estimator (PE) with three elements of preliminary accuracy, feature similarity, and computational complexity . The PE jointly performs an efficient architecture search for binarization baseline and enables automatic computation complexity adjustment under diverse constraints. Extensive experiments show that our approach is superior in terms of accuracy and computational performance, and is plug-and-play for different network backbones and binarization policies. Our code is available at https://github.com/VipaiLab/AuxBranch .},
  archive      = {J_PR},
  author       = {Siming Fu and Huanpeng Chu and Lu Yu and Bo Peng and Zheyang Li and Wenming Tan and Haoji Hu},
  doi          = {10.1016/j.patcog.2022.109263},
  journal      = {Pattern Recognition},
  pages        = {109263},
  shortjournal = {Pattern Recognition},
  title        = {AuxBranch: Binarization residual-aware network design via auxiliary branch search},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning generalized visual odometry using position-aware
optical flow and geometric bundle adjustment. <em>PR</em>, <em>136</em>,
109262. (<a href="https://doi.org/10.1016/j.patcog.2022.109262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent visual odometry (VO) methods incorporating geometric algorithm into deep-learning architecture have shown outstanding performance on the challenging monocular VO task. Despite encouraging results are shown, previous methods ignore the requirement of generalization capability under noisy environment and various scenes. To address this challenging issue, this work first proposes a novel optical flow network (PANet). Compared with previous methods that predict optical flow as a direct regression task , our PANet computes optical flow by predicting it into the discrete position space with optical flow probability volume, and then converting it to optical flow. Next, we improve the bundle adjustment module to fit the self-supervised training pipeline by introducing multiple sampling, ego-motion initialization, dynamic damping factor adjustment, and Jacobi matrix weighting. In addition, a novel normalized photometric loss function is advanced to improve the depth estimation accuracy. The experiments show that the proposed system not only achieves comparable performance with other state-of-the-art self-supervised learning-based methods on the KITTI dataset, but also significantly improves the generalization capability compared with geometry-based, learning-based and hybrid VO systems on the noisy KITTI and the challenging outdoor (KAIST) scenes.},
  archive      = {J_PR},
  author       = {Yi-Jun Cao and Xian-Shi Zhang and Fu-Ya Luo and Peng Peng and Chuan Lin and Kai-Fu Yang and Yong-Jie Li},
  doi          = {10.1016/j.patcog.2022.109262},
  journal      = {Pattern Recognition},
  pages        = {109262},
  shortjournal = {Pattern Recognition},
  title        = {Learning generalized visual odometry using position-aware optical flow and geometric bundle adjustment},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). A pyramid input augmented multi-scale CNN for GGO detection
in 3D lung CT images. <em>PR</em>, <em>136</em>, 109261. (<a
href="https://doi.org/10.1016/j.patcog.2022.109261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new convolutional neural network (CNN) with multi-scale processing for detecting ground-glass opacity nodules (GGO) in 3D computed tomography (CT) images, which is referred to as PiaNet for short. PiaNet consists of a feature-extraction module and a prediction module. The former module is constructed by introducing pyramid multi-scale source connections into a contracting-expanding structure. Besides, a new multi-receptive-field convolution block (MRCB) is presented to fuse the convolutions with multiple kernels of varying sizes for capturing features in each scale of information better. The latter module includes a bounding-box regressor and a classifier that are employed to simultaneously recognize GGO nodules and estimate bounding boxes at multiple scales. To train the proposed PiaNet, a two-stage transfer learning strategy is developed. In the first stage, the feature-extraction module is embedded into a classifier network that is trained on a large data set of GGO and non-GGO patches, which are generated by performing data augmentation from a small number of annotated CT scans. In the second stage, the pretrained feature-extraction module is loaded into PiaNet, and then PiaNet is fine-tuned using the annotated CT scans. We evaluate the proposed PiaNet with the LIDC-IDRI dataset. The experimental results demonstrate that our method outperforms state-of-the-art counterparts, including the Subsolid CAD and Aidence systems and CPM-Net and S4ND and GA-SSD methods. PiaNet achieves a sensitivity of 93.6\% with only one false positive per scan.},
  archive      = {J_PR},
  author       = {Weihua Liu and Xiabi Liu and Xiongbiao Luo and Murong Wang and Guanghui Han and Xinming Zhao and Zheng Zhu},
  doi          = {10.1016/j.patcog.2022.109261},
  journal      = {Pattern Recognition},
  pages        = {109261},
  shortjournal = {Pattern Recognition},
  title        = {A pyramid input augmented multi-scale CNN for GGO detection in 3D lung CT images},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Joint depth map super-resolution method via deep
hybrid-cross guidance filter. <em>PR</em>, <em>136</em>, 109260. (<a
href="https://doi.org/10.1016/j.patcog.2022.109260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays color-guided Depth map Super-Resolution (DSR) methods mainly have three thorny problems: (1) joint DSR methods have serious detail and structure loss at very high sampling rate; (2) existing DSR networks have high computational complexity ; (3) color-depth inconsistency makes it hard to fuse dual-modality features. To resolve these problems, we propose a joint hybrid-cross guidance filter method to progressively recover the quality of degraded Low-Resolution (LR) depth maps by exploiting color-depth consistency from multiple perspectives. Specifically, the proposed method leverages pyramid structure to extract multi-scale features from High-Resolution (HR) color image. At each scale, hybrid side window filter block is proposed to achieve high-efficiency color feature extraction after each down-sampling for HR color image. This block is also used to extract depth features from the LR depth map. Meanwhile, we propose a multi-perspective cross-guided fusion filter block to progressively fuse high-quality multi-scale structure information of color image with corresponding enhanced depth features. In this filter block, two kinds of space-aware group-compensation modules are introduced to capture various spatial features from different perspectives. Meanwhile, color-depth cross-attention module is proposed to extract color-depth consistency features for impactful boundary preservation. Comprehensively qualitative and quantitative experimental results have demonstrated that our method can achieve superior performances against a lot of state-of-the-art depth SR approaches in terms of mean absolute deviation and root mean square error on Middlebury, NYU-v2 and RGB-D-D datasets.},
  archive      = {J_PR},
  author       = {Ke Wang and Lijun Zhao and Jinjing Zhang and Jialong Zhang and Anhong Wang and Huihui Bai},
  doi          = {10.1016/j.patcog.2022.109260},
  journal      = {Pattern Recognition},
  pages        = {109260},
  shortjournal = {Pattern Recognition},
  title        = {Joint depth map super-resolution method via deep hybrid-cross guidance filter},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TETFN: A text enhanced transformer fusion network for
multimodal sentiment analysis. <em>PR</em>, <em>136</em>, 109259. (<a
href="https://doi.org/10.1016/j.patcog.2022.109259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA), which aims to recognize sentiment expressed by speakers in videos utilizing textual, visual and acoustic cues, has attracted extensive research attention in recent years. However, textual, visual and acoustic modalities often contribute differently to sentiment analysis. In general, text contains more intuitive sentiment-related information and outperforms nonlinguistic modalities in MSA. Seeking a strategy to take advantage of this property to obtain a fusion representation containing more sentiment-related information and simultaneously preserving inter- and intra-modality relationships becomes a significant challenge. To this end, we propose a novel method named Text Enhanced Transformer Fusion Network (TETFN), which learns text-oriented pairwise cross-modal mappings for obtaining effective unified multimodal representations. In particular, it incorporates textual information in learning sentiment-related nonlinguistic representations through text-based multi-head attention. In addition to preserving consistency information by cross-modal mappings, it also retains the differentiated information among modalities through unimodal label prediction. Furthermore, the vision pre-trained model Vision-Transformer is utilized to extract visual features from the original videos to preserve both global and local information of a human face. Extensive experiments on benchmark datasets CMU-MOSI and CMU-MOSEI demonstrate the superior performance of the proposed TETFN over state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Di Wang and Xutong Guo and Yumin Tian and Jinhui Liu and LiHuo He and Xuemei Luo},
  doi          = {10.1016/j.patcog.2022.109259},
  journal      = {Pattern Recognition},
  pages        = {109259},
  shortjournal = {Pattern Recognition},
  title        = {TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ALVLS: Adaptive local variances-based levelset framework for
medical images segmentation. <em>PR</em>, <em>136</em>, 109257. (<a
href="https://doi.org/10.1016/j.patcog.2022.109257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is a very challenging task, not only because the intensity of the medical image itself is not uniform, but also it may be accompanied by the impact of noise. Although mathematics, computer science, medicine, and other interdisciplinary fields have begun to study the problem of medical image segmentation, and have put forward a variety of segmentation algorithms , there is still much room for further improvement and enhancement. In the process of medical image collection and reconstruction, it is easy to produce intensity inhomogeneity and noises, as well as interference from other tissues, resulting in the difficulty of accurate segmentation. In this paper, we propose the adaptive local variances-based level set (ALVLS) model to segment medical images with intensity inhomogeneity and noises, including cardiac MR images, brain MR images, and breast ultrasound images . According to the variance difference information, the ALVLS model can adjust the effect of the area term adaptively. The local intensity variances are designed to optimize the ability to resist noise, which improves the segmentation accuracy of medical images. We also propose the two-layer level set model for segmenting left ventricles and left epicardium simultaneously. Experimental results for medical images and synthetic images show the desirable performance of the ALVLS model in accuracy, efficiency, and robustness to noise. In medical image competition, the Dice coefficient is used to calculate the similarity between the segmentation result and the ground truth. Thus we do comparisons with other methods and show that the Dice coefficient of the proposed method is higher than other testing methods.},
  archive      = {J_PR},
  author       = {Xiu Shu and Yunyun Yang and Jun Liu and Xiaojun Chang and Boying Wu},
  doi          = {10.1016/j.patcog.2022.109257},
  journal      = {Pattern Recognition},
  pages        = {109257},
  shortjournal = {Pattern Recognition},
  title        = {ALVLS: Adaptive local variances-based levelset framework for medical images segmentation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An ensemble hierarchical clustering algorithm based on
merits at cluster and partition levels. <em>PR</em>, <em>136</em>,
109255. (<a href="https://doi.org/10.1016/j.patcog.2022.109255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering has emerged as a combination of several basic clustering algorithms to achieve high quality final clustering. However, this technique is challenging due to the complexities in primary clusters such as overlapping, vagueness, instability and uncertainty. Typically, ensemble clustering uses all the primary clusters into partitions for consensus, where the merits of a cluster or a partition can be considered to improve the quality of the consensus. In general, the robustness of a partition may be poorly measured, while having some high-quality clusters. Inspired by the evaluation of cluster and partition, this paper proposes an ensemble hierarchical clustering algorithm based on the cluster consensus selection approach. Here, the selection of a subset of primary clusters from partitions based on their merit level is emphasized. Merit level is defined using the development of Normalized Mutual Information measure. Clusters of basic clustering algorithms that satisfy the predefined threshold of this measure are selected to participate in the final consensus. In addition, the consensus of the selected primary clusters to create the final clusters is performed based on the clusters clustering technique . In this technique, the selected primary clusters are re-clustered to create hyper-clusters. Finally, the final clusters are formed by assigning instances to hyper-clusters with the highest similarity. Here, an innovative criterion based on merit and cluster size for defining similarity is presented. The performance of the proposed algorithm has been proven by extensive experiments on real-world datasets from the UCI repository compared to state-of-the-art algorithms such as CPDM, ENMI, IDEA, CFTLC and SSCEN.},
  archive      = {J_PR},
  author       = {Qirui Huang and Rui Gao and Hoda Akhavan},
  doi          = {10.1016/j.patcog.2022.109255},
  journal      = {Pattern Recognition},
  pages        = {109255},
  shortjournal = {Pattern Recognition},
  title        = {An ensemble hierarchical clustering algorithm based on merits at cluster and partition levels},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neurodynamics-driven supervised feature selection.
<em>PR</em>, <em>136</em>, 109254. (<a
href="https://doi.org/10.1016/j.patcog.2022.109254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is an important dimensionality reduction technique in machine learning , pattern recognition, image processing , and data mining. Most existing feature selection methods are greedy in nature thus are prone to sub-optimality. Though some feature selection methods based on global optimization of unsupervised redundancy may potentiate performance improvements, they may or may not be relevant to classification as the information on pairwise features with class labels is missing. In this paper, based on a supervised similarity measure, a biconvex optimization problem is formulated for holistic feature section with a quadratically weighted objective function subject to linear equality and nonnegativity constraints. In addition, an iteratively reweighted convex quadratic program is reformulated. A two-timescale duplex neurodynamic system is applied to solve the formulated biconvex optimization problem and a projection neural network is customized to solve the iteratively reweighted convex optimization problem. Experimental results of the proposed neurodynamics-based supervised feature selection are elaborated in comparison with several existing feature selection methods based on twenty benchmark datasets to substantiate the efficacy and superiority of the neurodynamics-based method for selecting informative features in classification.},
  archive      = {J_PR},
  author       = {Yadi Wang and Jun Wang and Dacheng Tao},
  doi          = {10.1016/j.patcog.2022.109254},
  journal      = {Pattern Recognition},
  pages        = {109254},
  shortjournal = {Pattern Recognition},
  title        = {Neurodynamics-driven supervised feature selection},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Twin SVM for conditional probability estimation in binary
and multiclass classification. <em>PR</em>, <em>136</em>, 109253. (<a
href="https://doi.org/10.1016/j.patcog.2022.109253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we estimate the conditional probability function by presenting a new twin SVM model (CPTWSVM) in binary and multiclass classification problems. The motivation of CPTWSVM is to implement the empirical risk minimization on training data, which is hard to realize in traditional twin SVMs. In each subproblem of CPTWSVM, it measures the empirical risk and outputs the corresponding probability estimate of each class, which eliminates the problems of inconsistent measurement in twin SVMs. Though an additional discriminant objective function is introduced, the optimization problem size of each subproblem is smaller than conditional probability SVM, and is solved by block decomposition algorithm efficiently. In addition, we extend CPTWSVM to multiclass classification by estimating the conditional probability of each class, and maintaining the above properties. Numerical experiments on benchmark and real application datasets demonstrate that CPTWSVM outputs the estimate of probability and the data projection well, resulting in better generalization ability than some leading TWSVMs communities, in terms of binary and multiclass classification.},
  archive      = {J_PR},
  author       = {Yuan-Hai Shao and Xiao-Jing Lv and Ling-Wei Huang and Lan Bai},
  doi          = {10.1016/j.patcog.2022.109253},
  journal      = {Pattern Recognition},
  pages        = {109253},
  shortjournal = {Pattern Recognition},
  title        = {Twin SVM for conditional probability estimation in binary and multiclass classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving handgun detection through a combination of visual
features and body pose-based data. <em>PR</em>, <em>136</em>, 109252.
(<a href="https://doi.org/10.1016/j.patcog.2022.109252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of the presence of dangerous objects such as handguns in Closed-Circuit Television (CCTV) images is vital to reduce the potential damage. In this work, a novel method for automatic detection of handguns in CCTV-like images based on a combination architecture which leverages body pose estimation is proposed. Weapon appearance features along with body pose features are combined to perform robust detection in typical surveillance environments where appearance features alone are not sufficient (e.g., because the handgun may appear too small or dark). Both CNN and recent transformer-based architectures are applied for visual feature extraction. Experiments on multiple datasets show that this approach improves state-of-the-art pose-based handgun detectors. An ablation study is also performed to verify the contribution of the pose processing branch and the false positive filter.},
  archive      = {J_PR},
  author       = {Jesus Ruiz-Santaquiteria and Alberto Velasco-Mata and Noelia Vallez and Oscar Deniz and Gloria Bueno},
  doi          = {10.1016/j.patcog.2022.109252},
  journal      = {Pattern Recognition},
  pages        = {109252},
  shortjournal = {Pattern Recognition},
  title        = {Improving handgun detection through a combination of visual features and body pose-based data},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to restore multiple image degradations
simultaneously. <em>PR</em>, <em>136</em>, 109250. (<a
href="https://doi.org/10.1016/j.patcog.2022.109250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image corruptions are common in the real world, for example images in the wild may come with unknown blur, bias field, noise, or other kinds of non-linear distributional shifts, thus hampering encoding methods and rendering downstream task unreliable. Image upgradation requires a complicated balance between high-level contextualised information and spatial specific details. Existing approaches to solving the problems are designed to focus on single corruption, which unavoidably results in poor performance when the acquisitions suffer from multiple degradations. In this study, we investigate the possibility of handling multiple degradations and enhancing the quality of images via deblurring, bias field correction , and denoising . To tackle the problems with propagating errors caused by independent learning, we propose a unified and scalable framework, which consists of three special decoders. Two decoders learn artifact attention from provided images thereby generating realistic individual artifact and multiple artifacts on single image; the third decoder is trained towards removing artifact on the synthetic image with multiple corruptions thereby generating high quality image. We additionally provide improvements over previous image degradation synthesis approaches by modelling multiple image degradations directly from data observations. We first create a toy MNIST dataset and investigate the properties of the proposed algorithm. We then use brain MRI datasets to demonstrate our method’s robustness, including both simulated (where necessary) and real-world artifacts. In addition, our method can be used for single/or multiple degradation(s) synthesis by implementing the learned degradation operators in a new domain from a given dataset. The code will be released upon acceptance of the paper.},
  archive      = {J_PR},
  author       = {Le Zhang and Kevin Bronik and Bartłomiej W. Papież},
  doi          = {10.1016/j.patcog.2022.109250},
  journal      = {Pattern Recognition},
  pages        = {109250},
  shortjournal = {Pattern Recognition},
  title        = {Learning to restore multiple image degradations simultaneously},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TreEnhance: A tree search method for low-light image
enhancement. <em>PR</em>, <em>136</em>, 109249. (<a
href="https://doi.org/10.1016/j.patcog.2022.109249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present TreEnhance, an automatic method for low-light image enhancement capable of improving the quality of digital images. The method combines tree search theory, and in particular the Monte Carlo Tree Search (MCTS) algorithm, with deep reinforcement learning . Given as input a low-light image, TreEnhance produces as output its enhanced version together with the sequence of image editing operations used to obtain it. During the training phase, the method repeatedly alternates two main phases: a generation phase, where a modified version of MCTS explores the space of image editing operations and selects the most promising sequence, and an optimization phase, where the parameters of a neural network , implementing the enhancement policy, are updated. Two different inference solutions are proposed for the enhancement of new images: one is based on MCTS and is more accurate but more time and memory consuming; the other directly applies the learned policy and is faster but slightly less precise. As a further contribution, we propose a guided search strategy that “reverses” the enhancement procedure that a photo editor applied to a given input image. Unlike other methods from the state of the art, TreEnhance does not pose any constraint on the image resolution and can be used in a variety of scenarios with minimal tuning. We tested the method on two datasets: the Low-Light dataset and the Adobe Five-K dataset obtaining good results from both a qualitative and a quantitative point of view.},
  archive      = {J_PR},
  author       = {Marco Cotogni and Claudio Cusano},
  doi          = {10.1016/j.patcog.2022.109249},
  journal      = {Pattern Recognition},
  pages        = {109249},
  shortjournal = {Pattern Recognition},
  title        = {TreEnhance: A tree search method for low-light image enhancement},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel DAGAN for synthesizing garment images based on
design attribute disentangled representation. <em>PR</em>, <em>136</em>,
109248. (<a href="https://doi.org/10.1016/j.patcog.2022.109248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online costume design, it is vital to preview the design effect rapidly by entangling design attributes from reference images. This paper proposes a novel method, named design attributes generative adversarial network (DAGAN) for synthesizing garment images based on design attribute disentangled representation. The garment style is disentangled into the shape, texture, shadow, and decoration design attributes. The shape mask, repeating texture region, Laplace image gradient, and local logo are leveraged as visual representations for clothing design attributes from reference images. Following the design sequence from global to local, GDA-Net and LDA-Net in DAGAN entangle global and local design attributes, respectively, in the latent space. Then, the desired garment image is synthesized to represent design intentions explicitly. The DA-dataset for clothing design attributes is released. Extensive experiments demonstrate that the DAGAN is robust to various instances of design attributes on Design Attributes dataset (DA-dataset), and that is superior to the cross-domain transfer models in entangling design attributes from reference images.},
  archive      = {J_PR},
  author       = {Naiyu Fang and Lemiao Qiu and Shuyou Zhang and Zili Wang and Kerui Hu and Kang Wang},
  doi          = {10.1016/j.patcog.2022.109248},
  journal      = {Pattern Recognition},
  pages        = {109248},
  shortjournal = {Pattern Recognition},
  title        = {A novel DAGAN for synthesizing garment images based on design attribute disentangled representation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coarse-to-fine feature representation based on deformable
partition attention for melanoma identification. <em>PR</em>,
<em>136</em>, 109247. (<a
href="https://doi.org/10.1016/j.patcog.2022.109247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the histopathological melanoma image diagnosis system, manual identification of super-scale slides with dense cells is tedious, time-consuming, and subjective. To deal with this problem, we propose an automatic identification network based on the deformable partition attention to identify lots of dense slides as an assistant. A coarse-to-fine strategy is adopted in feature representation and qualitative identification to improve the identification accuracy of melanomas and nevi. First of all, because it is difficult to extract features in the lesion area with blurred boundaries and uneven distribution, we develop a deformable partition attention module, which integrates the advantage of the attention mechanism and deformable convolution. The module overcomes the limitation of rectangular convolution and gradually refines the channel and spatial features , which enriches feature representation by combining global and local features . Secondly, to address the problem of difficult convergence and poor recognition rate caused by the excessive non-aligned distance between benign-malignant and benign subcategories, we propose a progressive architecture via a coarse sub-network closely followed by a fine sub-network. Moreover, to further increase the inter-class differences and reduce the intra-class disparities, we propose a joint loss function to mine hard samples, which effectively improves the identification performance. Experimental results on the clinical dataset show that the proposed algorithm has higher sensitivity and specificity and outperforms state-of-the-art deep neural networks .},
  archive      = {J_PR},
  author       = {Dong Zhang and Jing Yang and Shaoyi Du and Hongcheng Han and Yuyan Ge and Longfei Zhu and Ce Li and Meifeng Xu and Nanning Zheng},
  doi          = {10.1016/j.patcog.2022.109247},
  journal      = {Pattern Recognition},
  pages        = {109247},
  shortjournal = {Pattern Recognition},
  title        = {Coarse-to-fine feature representation based on deformable partition attention for melanoma identification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HAMIL: Hierarchical aggregation-based multi-instance
learning for microscopy image classification. <em>PR</em>, <em>136</em>,
109245. (<a href="https://doi.org/10.1016/j.patcog.2022.109245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-instance learning is common for computer vision tasks, especially in biomedical image processing . Traditional methods for multi-instance learning focus on designing feature aggregation methods and multi-instance classifiers, where the aggregation operation is performed either in the feature extraction or learning phase. As deep neural networks (DNNs) achieve great success in image processing via automatic feature learning , certain feature aggregation mechanisms need to be incorporated into common DNN architecture for multi-instance learning. Moreover, flexibility and reliability are crucial considerations to deal with varying quality and number of instances. In this study, we propose a hierarchical aggregation network for multi-instance learning, called HAMIL. The hierarchical aggregation protocol enables feature fusion in a defined order, and the simple convolutional aggregation units lead to an efficient and flexible architecture. We assess the model performance on two microscopy image classification tasks, namely protein subcellular localization using immunofluorescence images and gene annotation using spatial gene expression images. The experimental results show that HAMIL outperforms the state-of-the-art feature aggregation methods and the existing models for addressing these two tasks. The visualization analyses also demonstrate the ability of HAMIL to focus on high-quality instances.},
  archive      = {J_PR},
  author       = {Yang Yang and Yanlun Tu and Houchao Lei and Wei Long},
  doi          = {10.1016/j.patcog.2022.109245},
  journal      = {Pattern Recognition},
  pages        = {109245},
  shortjournal = {Pattern Recognition},
  title        = {HAMIL: Hierarchical aggregation-based multi-instance learning for microscopy image classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DIODE: Dilatable incremental object detection. <em>PR</em>,
<em>136</em>, 109244. (<a
href="https://doi.org/10.1016/j.patcog.2022.109244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accommodate rapid changes in the real world, the cognition system of humans is capable of continually learning concepts. On the contrary, conventional deep learning models lack this capability of preserving previously learned knowledge. When a neural network is fine-tuned to learn new tasks, its performance on previously trained tasks will significantly deteriorate. Many recent works on incremental object detection tackle this problem by introducing advanced regularization. Although these methods have shown promising results, the benefits are often short-lived after the first incremental step. Under multi-step incremental learning, the trade-off between old knowledge preserving and new task learning becomes progressively more severe. Thus, the performance of regularization-based incremental object detectors gradually decays for subsequent learning steps. In this paper, we aim to alleviate this performance decay on multi-step incremental detection tasks by proposing a dilatable incremental object detector (DIODE). For the task-shared parameters, our method adaptively penalizes the changes of important weights for previous tasks. At the same time, the structure of the model is dilated or expanded by a limited number of task-specific parameters to promote new task learning. Extensive experiments on PASCAL VOC and COCO datasets demonstrate substantial improvements over the state-of-the-art methods. Notably, compared with the state-of-the-art methods, our method achieves up to 6.0\% performance improvement by increasing the number of parameters by just 1.2\% for each newly learned task.},
  archive      = {J_PR},
  author       = {Can Peng and Kun Zhao and Sam Maksoud and Tianren Wang and Brian C. Lovell},
  doi          = {10.1016/j.patcog.2022.109244},
  journal      = {Pattern Recognition},
  pages        = {109244},
  shortjournal = {Pattern Recognition},
  title        = {DIODE: Dilatable incremental object detection},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Position-aware and structure embedding networks for deep
graph matching. <em>PR</em>, <em>136</em>, 109242. (<a
href="https://doi.org/10.1016/j.patcog.2022.109242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching refers to the process of establishing node correspondences based on edge-to-edge constraints between graph nodes . This can be formulated as a combinatorial optimization problem under node permutation and pairwise consistency constraints. The main challenge of graph matching is to effectively find the correct match while reducing the ambiguities produced by similar nodes and edges. In this paper, we present a novel end-to-end neural framework that converts graph matching to a linear assignment problem in a high-dimensional space. This is combined with relative position information at the node level, and high-order structural arrangement information at the subgraph level. By capturing the relative position attributes of nodes between different graphs and the subgraph structural arrangement attributes, we can improve the performance of graph matching tasks, and establish reliable node-to-node correspondences. Our method can be generalized to any graph embedding setting, which can be used as components to deal with various graph matching problems answered with deep learning methods . We validate our method on several real-world tasks, by providing ablation studies to evaluate the generalization capability across different categories. We also compare state-of-the-art alternatives to demonstrate performance.},
  archive      = {J_PR},
  author       = {Dongdong Chen and Yuxing Dai and Lichi Zhang and Zhihong Zhang and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2022.109242},
  journal      = {Pattern Recognition},
  pages        = {109242},
  shortjournal = {Pattern Recognition},
  title        = {Position-aware and structure embedding networks for deep graph matching},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new image decomposition approach using pixel-wise analysis
sparsity model. <em>PR</em>, <em>136</em>, 109241. (<a
href="https://doi.org/10.1016/j.patcog.2022.109241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decomposing an image into two ‘simpler’ layers has been widely used in low-level vision tasks, such as image recovery and enhancement. It is an ill-posed problem since the number of unknowns are larger than the input. In this paper, a two-step strategy is introduced, including task-aware priors estimate and a decomposition model . A pixel-wise analysis sparsity model is proposed to regularize the separation layers, which supposes the transformed image generated with analysis operator is sparse. Unlike regularizing all pixels with one penalty weight, we try to estimate each pixel’s sparsity level with task-aware priors and to achieve pixel-wise sparse penalty. Additionally, one separation layer is regularized with both synthesis sparsity model and pixel-wise analysis sparsity model to exploit their complementary mechanisms. Unlike the analysis one utilizing image local features , the synthesis one exploits an over-complete dictionary and non-local similarity cues to provide flexible prior for regularizing the decomposition results. The proposed model is solved by an alternating optimization algorithm . We evaluate it with two applications, Retinex model and rain streaks removal. Extensive experiments on multiple enhancement datasets, many synthetic and real rainy images demonstrate that our method can remove imaging noise during Retinex decomposition, and can produce high fidelity deraining results. It achieves competing performance in terms of quantitative metrics and visual quality compared with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Shuangli Du and Yiguang Liu and Minghua Zhao and Zhenyu Xu and Jie Li and Zhenzhen You},
  doi          = {10.1016/j.patcog.2022.109241},
  journal      = {Pattern Recognition},
  pages        = {109241},
  shortjournal = {Pattern Recognition},
  title        = {A new image decomposition approach using pixel-wise analysis sparsity model},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A proxy learning curve for the bayes classifier.
<em>PR</em>, <em>136</em>, 109240. (<a
href="https://doi.org/10.1016/j.patcog.2022.109240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a theoretical learning curve is derived for the multi-class Bayes classifier. This curve fits general multivariate parametric models of the class-conditional probability density. The derivation uses a proxy approach based on analyzing the convergence of a statistic which is proportional to the posterior probability of the true class. By doing so, the curve depends only on the training set size and on the dimension of the feature vector; it does not depend on the model parameters. Essentially, the learning curve provides an estimate of the reduction in the excess of the probability of error that can be obtained by increasing the training set size. This makes it attractive in order to deal with the practical problems of defining appropriate training set sizes.},
  archive      = {J_PR},
  author       = {Addisson Salazar and Luis Vergara and Enrique Vidal},
  doi          = {10.1016/j.patcog.2022.109240},
  journal      = {Pattern Recognition},
  pages        = {109240},
  shortjournal = {Pattern Recognition},
  title        = {A proxy learning curve for the bayes classifier},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sampling-based density peaks clustering algorithm for
large-scale data. <em>PR</em>, <em>136</em>, 109238. (<a
href="https://doi.org/10.1016/j.patcog.2022.109238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, massive amount of data is generated. How to discover useful information to support decision-making has become one of the focuses of scholar&#39;s research. Clustering is thought to be one of the main means to deal with large-scale data. Density peaks clustering (DPC) is an effective density-based clustering algorithm which is widely applied in numerous fields because of its satisfactory performance. However, the computational complexity of DPC is O ( N 2 ) O(N2) which is not friendly to large-scale data. To solve this issue, a sampling-based density peaks clustering algorithm for large-scale data (SDPC) is proposed. Firstly, a sampling method is used to reduce the distance calculations. Secondly, approximate representatives are identified by an improved TI search strategy which further accelerates the clustering process. Afterwards, the approximate representatives are clustered by DPC. Finally, the remaining points are allocated to the same cluster as its nearest representatives. Experimental results on both synthetic datasets and real-world datasets illustrate that SDPC is more efficient than DPC, while its clustering performance maintains the same level as DPC.},
  archive      = {J_PR},
  author       = {Shifei Ding and Chao Li and Xiao Xu and Ling Ding and Jian Zhang and Lili Guo and Tianhao Shi},
  doi          = {10.1016/j.patcog.2022.109238},
  journal      = {Pattern Recognition},
  pages        = {109238},
  shortjournal = {Pattern Recognition},
  title        = {A sampling-based density peaks clustering algorithm for large-scale data},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compact network embedding for fast node classification.
<em>PR</em>, <em>136</em>, 109236. (<a
href="https://doi.org/10.1016/j.patcog.2022.109236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding has shown promising performance in real-world applications. The network embedding typically lies in a continuous vector space, where storage and computation costs are high, especially in large-scale applications. This paper proposes more compact representation to fulfill the gap. The proposed discrete network embedding (DNE) leverages hash code to represent node in Hamming space. The Hamming similarity between hash codes approximates the ground-truth similarity. The embedding and classifier are jointly learned to improve compactness and discrimination. The proposed multi-class classifier is further constrained to be discrete to expedite classification. In addition, this paper further extends DNE and proposes deep discrete attributed network embedding (DDANE) to learn compact deep embedding from more informative attributed network. From the perspective of generalized signal smoothing, the proposed DDANE trains an improved graph convolutional network autoencoder to effectively leverage node attribute and network structure. Extensive experiments on node classification demonstrate the proposed methods exhibit lower storage and computational complexity than state-of-the-art network embedding methods, and achieve satisfactory accuracy.},
  archive      = {J_PR},
  author       = {Xiaobo Shen and Yew-Soon Ong and Zheng Mao and Shirui Pan and Weiwei Liu and Yuhui Zheng},
  doi          = {10.1016/j.patcog.2022.109236},
  journal      = {Pattern Recognition},
  pages        = {109236},
  shortjournal = {Pattern Recognition},
  title        = {Compact network embedding for fast node classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-hallucinating prototype for few-shot learning
promotion. <em>PR</em>, <em>136</em>, 109235. (<a
href="https://doi.org/10.1016/j.patcog.2022.109235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective way for few-shot learning (FSL) is to establish a metric space where the distance between a query and the prototype of each class is computed for classification, and the key lies on hallucinating the appropriate prototypes for each class of the given FSL task. Most existing prototypical approaches hallucinate the class-wise prototype based on the given support samples with an equal contribution assumption, i.e., each support sample contributes equally to the corresponding prototype. However, due to the limited-data regime as well as the strict assumption, the hallucinated prototypes often deviate from the ideal ones that are determined by the sample distribution of each unseen class, and thus causing poor generalization performance . To mitigate this problem, we present a prototype meta-hallucination approach which shows two aspects of advantages. On one hand, instead of directly inferring the complicated sample distribution, it meta-learns to establish a difference distribution based generative model that infers the distribution of inter-sample difference and synthesizes new labeled samples through fusing the sampled inter-sample difference and each given support sample. This empowers us to augment the support set with more content-diverse samples and is beneficial to reduce the bias in prototype hallucination. On the other hand, we argue that each support sample may contribute no-equally to the ideal prototype that it belongs to and their relations vary with class characteristics. Following this, our approach meta-learns to dynamically re-weight all support samples in prototype hallucination, which makes it flexible to locate the ideal prototype for each unseen class based on its characteristics. Experiments on four FSL benchmark datasets show that our approach can effectively improve the performance of the prototypical baseline and outperform several state-of-the-art competitors with a clear margin.},
  archive      = {J_PR},
  author       = {Lei Zhang and Fei Zhou and Wei Wei and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.109235},
  journal      = {Pattern Recognition},
  pages        = {109235},
  shortjournal = {Pattern Recognition},
  title        = {Meta-hallucinating prototype for few-shot learning promotion},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning spatiotemporal embedding with gated convolutional
recurrent networks for translation initiation site prediction.
<em>PR</em>, <em>136</em>, 109234. (<a
href="https://doi.org/10.1016/j.patcog.2022.109234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting translation initiation sites (TIS) from genomic sequences is crucial for understanding gene regulation and function. TIS prediction methods’ feature vectors are not discriminative enough to lead to unsatisfactory predictive results. In this work, we devise an efficient gated convolutional recurrent network (GCR-Net) with residual learning to dynamically extract dependency patterns of raw genomic sequences in an efficient fusion strategy and successfully improve the performance of the TIS prediction. GCR-Net mainly includes exponential gated convolutional residual networks (EGCRN) and bidirectional gated recurrent unit (Bi-GRU) networks. Particularly, we devise the novel EGCRN to extract multiple complex patterns of the spatial dimension from genomic sequences, where we design an exponential gated linear unit (EGLU) to reduce the vanishing gradient problem. Moreover, we combine EGLU with shortcut connections to develop the stacked gated mechanism based on convolutions that benefit information propagation across layers. Then, we use Bi-GRU with identity connections to learn long-term dependency patterns of the temporal dimension from genomic sequences. Besides, we evaluate our GCR-Net model on four TIS datasets, and experiments demonstrate that GCR-Net is an efficient deep learning-based TIS prediction tool and obtains superior performance compared to the baseline methods.},
  archive      = {J_PR},
  author       = {Weihua Li and Yanbu Guo and Bingyi Wang and Bei Yang},
  doi          = {10.1016/j.patcog.2022.109234},
  journal      = {Pattern Recognition},
  pages        = {109234},
  shortjournal = {Pattern Recognition},
  title        = {Learning spatiotemporal embedding with gated convolutional recurrent networks for translation initiation site prediction},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale local-temporal similarity fusion for continuous
sign language recognition. <em>PR</em>, <em>136</em>, 109233. (<a
href="https://doi.org/10.1016/j.patcog.2022.109233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous sign language recognition (cSLR) is a public significant task that transcribes a sign language video into an ordered gloss sequence. It is important to capture the fine-grained gloss-level details, since there is no explicit alignment between sign video frames and the corresponding glosses. Among the past works, one promising way is to adopt a one-dimensional convolutional network (1D-CNN) to temporally fuse the sequential frames. However, CNNs are agnostic to similarity or dissimilarity, and thus are unable to capture local consistent semantics within temporally neighboring frames. To address the issue, we propose to adaptively fuse local features via temporal similarity for this task. Specifically, we devise a M ulti-scale L ocal- T emporal S imilarity F usion Network (mLTSF-Net) as follows: 1) In terms of a specific video frame, we firstly select its similar neighbours with multi-scale receptive regions to accommodate different lengths of glosses. 2) To ensure temporal consistency, we then use position-aware convolution to temporally convolve each scale of selected frames. 3) To obtain a local-temporally enhanced frame-wise representation, we finally fuse the results of different scales using a content-dependent aggregator . We train our model in an end-to-end fashion, and the experimental results on RWTH-PHOENIX-Weather 2014 datasets (RWTH) demonstrate that our model achieves competitive performance compared with several state-of-the-art models.},
  archive      = {J_PR},
  author       = {Pan Xie and Zhi Cui and Yao Du and Mengyi Zhao and Jianwei Cui and Bin Wang and Xiaohui Hu},
  doi          = {10.1016/j.patcog.2022.109233},
  journal      = {Pattern Recognition},
  pages        = {109233},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale local-temporal similarity fusion for continuous sign language recognition},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting and grouping keypoints for multi-person pose
estimation using instance-aware attention. <em>PR</em>, <em>136</em>,
109232. (<a href="https://doi.org/10.1016/j.patcog.2022.109232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bottom-up human pose estimation models detect keypoints and learn associative information between keypoints, usually requiring human predefined offset fields or embeddings for keypoints grouping (clustering). In this paper, we present a brand new method that can entirely solve these problems based on Transformer, making the grouping process free of the human-defined associative signals. Specifically, the self-attention in vision Transformer measures feature similarity between any pair of locations, which provides a metric space to associate keypoints together into corresponding human instances. However, the naive attention patterns formed in Transformer are still not subjectively controlled, so there is no guarantee that the keypoints only attend to the instances to which they belong. To address it we propose a novel approach of supervising self-attention to be instance-aware, simultaneously accomplishing multi-person keypoint detection and clustering. By doing so, we can group the detected keypoints to their corresponding instances, according to the pairwise attention scores . An additional benefit of our method is that the instance segmentation results of any number of people can be directly obtained from the supervised attention matrix, thereby simplifying the pixel assignment pipeline. The qualitative and quantitative results on the COCO shows that, with a very simple architecture design, our method can achieve comparable performance against the CNN-based bottom-up counterparts with fewer parameters, which also demonstrate a promising way to control self-attention mechanism behavior for specific purposes.},
  archive      = {J_PR},
  author       = {Sen Yang and Ze Feng and Zhicheng Wang and Yanjie Li and Shoukui Zhang and Zhibin Quan and Shu-tao Xia and Wankou Yang},
  doi          = {10.1016/j.patcog.2022.109232},
  journal      = {Pattern Recognition},
  pages        = {109232},
  shortjournal = {Pattern Recognition},
  title        = {Detecting and grouping keypoints for multi-person pose estimation using instance-aware attention},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SpatioTemporal focus for skeleton-based action recognition.
<em>PR</em>, <em>136</em>, 109231. (<a
href="https://doi.org/10.1016/j.patcog.2022.109231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are widely adopted in skeleton-based action recognition due to their powerful ability to model data topology. We argue that the performance of recent proposed skeleton-based action recognition methods is limited by the following factors. First, the predefined graph structures are shared throughout the network, lacking the flexibility and capacity to model the multi-grain semantic information. Second, the relations among the global joints are not fully exploited by the graph local convolution, which may lose the implicit joint relevance. For instance, actions such as running and waving are performed by the co-movement of body parts and joints , e.g. , legs and arms, however, they are located far away in physical connection. Inspired by the recent attention mechanism , we propose a multi-grain contextual focus module, termed MCF, to capture the action associated relation information from the body joints and parts. As a result, more explainable representations for different skeleton action sequences can be obtained by MCF. In this study, we follow the common practice that the dense sample strategy of the input skeleton sequences is adopted and this brings much redundancy since number of instances has nothing to do with actions. To reduce the redundancy, a temporal discrimination focus module, termed TDF, is developed to capture the local sensitive points of the temporal dynamics. MCF and TDF are integrated into the standard GCN network to form a unified architecture, named STF-Net. It is noted that STF-Net provides the capability to capture robust movement patterns from these skeleton topology structures, based on multi-grain context aggregation and temporal dependency. Extensive experimental results show that our STF-Net significantly achieves state-of-the-art results on three challenging benchmarks NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-Skeleton.},
  archive      = {J_PR},
  author       = {Liyu Wu and Can Zhang and Yuexian Zou},
  doi          = {10.1016/j.patcog.2022.109231},
  journal      = {Pattern Recognition},
  pages        = {109231},
  shortjournal = {Pattern Recognition},
  title        = {SpatioTemporal focus for skeleton-based action recognition},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable clustering by aggregating representatives in
hierarchical groups. <em>PR</em>, <em>136</em>, 109230. (<a
href="https://doi.org/10.1016/j.patcog.2022.109230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appropriately handling the scalability of clustering is a long-standing challenge for the study of clustering techniques and is of fundamental interest to researchers in the community of data mining and knowledge discovery. In comparison to other clustering methods , hierarchical clustering demonstrates better interpretability of clustering results but poor scalability while handling large-scale data. Thus, more comprehensive studies on this problem need to be conducted. This paper develops a new scalable hierarchical clustering model called Election Tree, which can detect the most representative point for each sub-cluster via the process of node election in split data and adjust the members in sub-clusters by the operations of node merging and swap. Extensive experiments on real-world datasets reveal that the proposed computational framework has better clustering accuracy as opposed to the competing baseline methods . Meanwhile, with respect to the scalability tests on incremental synthetic datasets , the results show that the new model has a significantly lower time consumption than the state-of-the-art hierarchical clustering models such as PERCH, GRINCH, SCC and other classic baselines.},
  archive      = {J_PR},
  author       = {Wen-Bo Xie and Zhen Liu and Debarati Das and Bin Chen and Jaideep Srivastava},
  doi          = {10.1016/j.patcog.2022.109230},
  journal      = {Pattern Recognition},
  pages        = {109230},
  shortjournal = {Pattern Recognition},
  title        = {Scalable clustering by aggregating representatives in hierarchical groups},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding and combating robust overfitting via input
loss landscape analysis and regularization. <em>PR</em>, <em>136</em>,
109229. (<a href="https://doi.org/10.1016/j.patcog.2022.109229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods. Code is available at https://github.com/TreeLLi/Combating-RO-AdvLC .},
  archive      = {J_PR},
  author       = {Lin Li and Michael Spratling},
  doi          = {10.1016/j.patcog.2022.109229},
  journal      = {Pattern Recognition},
  pages        = {109229},
  shortjournal = {Pattern Recognition},
  title        = {Understanding and combating robust overfitting via input loss landscape analysis and regularization},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective CNN and transformer complementary network for
medical image segmentation. <em>PR</em>, <em>136</em>, 109228. (<a
href="https://doi.org/10.1016/j.patcog.2022.109228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer network was originally proposed for natural language processing. Due to its powerful representation ability for long-range dependency, it has been extended for vision tasks in recent years. To fully utilize the advantages of Transformers and Convolutional Neural Networks (CNNs), we propose a CNN and Transformer Complementary Network (CTC Net) for medical image segmentation. We first design two encoders by Swin Transformers and Residual CNNs to produce complementary features in Transformer and CNN domains, respectively. Then we cross-wisely concatenate these complementary features to propose a Cross-domain Fusion Block (CFB) for effectively blending them. In addition, we compute the correlation between features from the CNN and Transformer domains, and apply channel attention to the self-attention features by Transformers for capturing dual attention information. We incorporate cross-domain fusion, feature correlation and dual attention together to propose a Feature Complementary Module (FCM) for improving the representation ability of features. Finally, we design a Swin Transformer decoder to further improve the representation ability of long-range dependencies, and propose to use skip connections between the Transformer decoded features and the complementary features for extracting spatial details, contextual semantics and long-range information. Skip connections are performed in different levels for enhancing multi-scale invariance. Experimental results show that our CTC Net significantly surpasses the state-of-the-art image segmentation models based on CNNs, Transformers, and even Transformer and CNN combined models designed for medical image segmentation. It achieves superior performance on different medical applications, including multi-organ segmentation and cardiac segmentation.},
  archive      = {J_PR},
  author       = {Feiniu Yuan and Zhengxiao Zhang and Zhijun Fang},
  doi          = {10.1016/j.patcog.2022.109228},
  journal      = {Pattern Recognition},
  pages        = {109228},
  shortjournal = {Pattern Recognition},
  title        = {An effective CNN and transformer complementary network for medical image segmentation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep hybrid model for single image dehazing and detail
refinement. <em>PR</em>, <em>136</em>, 109227. (<a
href="https://doi.org/10.1016/j.patcog.2022.109227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning technologies have been applied in Single Image Dehazing (SID) tasks successfully. However, most SID algorithms seldom consider to refine image details during dehazing. Therefore, there exist some detail-loss regions in dehazed results. To solve this issue, we design a deep hybrid network to improve dehazing performance and remedy the loss of details. Different from existing algorithms that usually ignore detail refinement and adopt a unified framework to remove haze, we propose to treat dehazing and detail refinement as two separate tasks, so that each task could be solved via different ways. Particularly, we design two sub-networks with a multi-term loss function. First, for removing haze effectively, we introduce the Squeeze-and-Excitation (SE) to design a haze residual attention sub-network, which is used to reconstruct the dehazed image. Second, as for remedying details, we take the previous dehazed image as the input to a detail refinement sub-network, where the image details can be enhanced via multi-scale contextual information aggregation. Through the joint training of two sub-network, the haze can be removed clearly and the image details can be preserved well. Moreover, the detail refinement sub-network can be detached into other existing dehazing methods to improve their model performances. Extensive experiments also verify the superiority of our proposed network against recently proposed state-of-the-arts.},
  archive      = {J_PR},
  author       = {Nanfeng Jiang and Kejian Hu and Ting Zhang and Weiling Chen and Yiwen Xu and Tiesong Zhao},
  doi          = {10.1016/j.patcog.2022.109227},
  journal      = {Pattern Recognition},
  pages        = {109227},
  shortjournal = {Pattern Recognition},
  title        = {Deep hybrid model for single image dehazing and detail refinement},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pitfalls of assessing extracted hierarchies for multi-class
classification. <em>PR</em>, <em>136</em>, 109225. (<a
href="https://doi.org/10.1016/j.patcog.2022.109225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using hierarchies of classes is one of the standard methods to solve multi-class classification problems. In the literature, selecting the right hierarchy is considered to play a key role in improving classification performance. Although different methods have been proposed, there is still a lack of understanding of what makes a hierarchy good and what makes a method to extract hierarchies perform better or worse. To this effect, we analyze and compare some of the most popular approaches to extracting hierarchies. We identify some common pitfalls that may lead practitioners to make misleading conclusions about their methods. To address some of these problems, we demonstrate that using random hierarchies is an appropriate benchmark to assess how the hierarchy’s quality affects the classification performance. In particular, we show how the hierarchy’s quality can become irrelevant depending on the experimental setup: when using powerful enough classifiers, the final performance is not affected by the quality of the hierarchy. We also show how comparing the effect of the hierarchies against non-hierarchical approaches might incorrectly indicate their superiority. Our results confirm that datasets with a high number of classes generally present complex structures in how these classes relate to each other. In these datasets, the right hierarchy can dramatically improve classification performance.},
  archive      = {J_PR},
  author       = {Pablo del Moral and Sławomir Nowaczyk and Anita Sant’Anna and Sepideh Pashami},
  doi          = {10.1016/j.patcog.2022.109225},
  journal      = {Pattern Recognition},
  pages        = {109225},
  shortjournal = {Pattern Recognition},
  title        = {Pitfalls of assessing extracted hierarchies for multi-class classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D hand pose estimation from a single RGB image by weighting
the occlusion and classification. <em>PR</em>, <em>136</em>, 109217. (<a
href="https://doi.org/10.1016/j.patcog.2022.109217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new framework for 3D hand pose estimation using a single RGB image is proposed. The framework is composed of two blocks. The first block formulates the hand pose estimation as a classification problem. Since the human hand can perform numerous poses, the classification network needs a huge number of parameters. So, we propose to classify hand poses based on three different aspects, including hand gesture, hand direction, and palm direction. In this way, the number of parameters will be significantly reduced. The motivation behind the classification block is that the model deals with the image as a whole and extracts global features. Furthermore, the output of the classification model is a valid pose that does not include any unexpected angle at joints. The second block estimates the 3D coordinates of the hand joints and focuses more on the details of the image pattern. RGB-based 3D hand pose estimation is an inherently ill-posed problem due to the lack of depth information in the 2D image. We propose to use the occlusion status of the hand joints to solve this problem. The occlusion status of the joints has been labeled manually. Some joints are partially occluded, and we propose to compute the extent of the occlusion by semantic segmentation . The existing methods in this field mostly used synthetic datasets. But all the models proposed in this paper are trained on more than 50 K real images. Extensive experiments on our new dataset and two other benchmark datasets show that the proposed method can achieve good performance. We also analyze the validity of the predicted poses, and the results show that the classification block increases the validity of the poses.},
  archive      = {J_PR},
  author       = {Khadijeh Mahdikhanlou and Hossein Ebrahimnezhad},
  doi          = {10.1016/j.patcog.2022.109217},
  journal      = {Pattern Recognition},
  pages        = {109217},
  shortjournal = {Pattern Recognition},
  title        = {3D hand pose estimation from a single RGB image by weighting the occlusion and classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyper-sausage coverage function neuron model and learning
algorithm for image classification. <em>PR</em>, <em>136</em>, 109216.
(<a href="https://doi.org/10.1016/j.patcog.2022.109216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep neural networks (DNNs) promote mainly by network architectures and loss functions; however, the development of neuron models has been quite limited. In this study, inspired by the mechanism of human cognition, a hyper-sausage coverage function (HSCF) neuron model possessing a high flexible plasticity. Then, a novel cross-entropy and volume-coverage (CE_VC) loss is defined, which compresses the volume of the hyper-sausage to the hilt, and helps alleviate confusion among different classes, thus ensuring the intra-class compactness of the samples. Finally, a divisive iteration method is introduced, which considers each neuron model as a weak classifier , and iteratively increases the number of weak classifiers. Thus, the optimal number of the HSCF neuron is adaptively determined and an end-to-end learning framework is constructed. In particular, to improve the classification performance, the HSCF neuron can be applied to classical DNNs. Comprehensive experiments on eight datasets in several domains demonstrate the effectiveness of the proposed method. The proposed method exhibits the feasibility of boosting DNNs with neuron plasticity and provides a novel perspective for further developments in DNNs. The source code is available at https://github.com/Tough2011/HSCFNet.git .},
  archive      = {J_PR},
  author       = {Xin Ning and Weijuan Tian and Feng He and Xiao Bai and Le Sun and Weijun Li},
  doi          = {10.1016/j.patcog.2022.109216},
  journal      = {Pattern Recognition},
  pages        = {109216},
  shortjournal = {Pattern Recognition},
  title        = {Hyper-sausage coverage function neuron model and learning algorithm for image classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural operator search. <em>PR</em>, <em>136</em>, 109215.
(<a href="https://doi.org/10.1016/j.patcog.2022.109215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing neural architecture search (NAS) methods usually explore a limited feature-transformation-only search space, ignoring other advanced feature operations such as feature self-calibration by attention and dynamic convolutions. This disables the NAS algorithms to discover more advanced network architectures . We address this limitation by additionally exploiting feature self-calibration operations, resulting in a heterogeneous search space. To solve the challenges of operation heterogeneity and significantly larger search space, we formulate a neural operator search (NOS) method. NOS presents a novel heterogeneous residual block for integrating the heterogeneous operations in a unified structure, and an attention guided search strategy for facilitating the search process over a vast space. Extensive experiments show that NOS can search novel cell architectures with highly competitive performance on the CIFAR and ImageNet benchmarks.},
  archive      = {J_PR},
  author       = {Wei Li and Shaogang Gong and Xiatian Zhu},
  doi          = {10.1016/j.patcog.2022.109215},
  journal      = {Pattern Recognition},
  pages        = {109215},
  shortjournal = {Pattern Recognition},
  title        = {Neural operator search},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coherence-aware context aggregator for fast video object
segmentation. <em>PR</em>, <em>136</em>, 109214. (<a
href="https://doi.org/10.1016/j.patcog.2022.109214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised video object segmentation (VOS) is a highly challenging problem that has attracted much research attention in recent years. Temporal context plays an important role in VOS by providing object clues from the past frames. However, most of the prevailing methods directly use the predicted temporal results to guide the segmentation of the current frame, while ignoring the coherence of temporal context, which may be misleading and degrade the performance. In this paper, we propose a novel model named Coherence-aware Context Aggregator (CCA) for VOS, which consists of three modules. First, a coherence-aware module (CAM) is proposed to evaluate the coherence of the predicted result of the current frame and then fuses the coherent features to update the temporal context. CAM can determine whether the prediction is accurate, thus guiding the update of the temporal context and avoiding the introduction of erroneous information. Second, we devise a spatio-temporal context aggregation (STCA) module to aggregate the temporal context with the spatial feature of the current frame to learn a robust and discriminative target representation in the decoder part. Third, we design a refinement module to refine the coarse feature generated from the STCA module for more precise segmentation. Additionally, CCA uses a cropping strategy and takes small-size images as input, thus making it computationally efficient and achieving a real-time running speed. Extensive experiments on four challenging benchmarks show that CCA achieves a better trade-off between efficiency and accuracy compared to state-of-the-art methods. The code will be public.},
  archive      = {J_PR},
  author       = {Meng Lan and Jing Zhang and Zengmao Wang},
  doi          = {10.1016/j.patcog.2022.109214},
  journal      = {Pattern Recognition},
  pages        = {109214},
  shortjournal = {Pattern Recognition},
  title        = {Coherence-aware context aggregator for fast video object segmentation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint classification and prediction of random curves using
heavy‐tailed process functional regression. <em>PR</em>, <em>136</em>,
109213. (<a href="https://doi.org/10.1016/j.patcog.2022.109213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a heavy-tailed process functional regression to jointly perform classification and prediction of time-varying functional data. We use two independent scale mixtures of Gaussian Processes to respectively model random effects and random errors, yielding robust inferences against both magnitude and shape outliers. We classify random curves by posterior predictive probabilities of class labels and offer a weighted prediction of future curve trends. A Bayesian estimation procedure is implemented through an MCMC sampling algorithm . The performance of classification and prediction of the proposed model is evaluated using simulated studies and some real data sets .},
  archive      = {J_PR},
  author       = {Chunzheng Cao and Xin Liu and Shuren Cao and Jian Qing Shi},
  doi          = {10.1016/j.patcog.2022.109213},
  journal      = {Pattern Recognition},
  pages        = {109213},
  shortjournal = {Pattern Recognition},
  title        = {Joint classification and prediction of random curves using heavy‐tailed process functional regression},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpreting denoising autoencoders with complex
perturbation approach. <em>PR</em>, <em>136</em>, 109212. (<a
href="https://doi.org/10.1016/j.patcog.2022.109212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this study is to interpret denoising autoencoders by quantifying the importance of input pixel features for image reconstruction. The importance of pixel features is evaluated using the attributions of the pixel features to the latent variables of a denoising autoencoder used for image reconstruction. Pixel attributions are computed using a highly accurate and automatable perturbation approach and are plotted as saliency maps . Saliency maps highlight the contribution of the pixels for image reconstruction. The proposed approach produces more meaningful and understandable explanations than guided backpropagation and layer wise propagation methods. Three sanity checks are introduced to verify the fidelity of the generated saliency maps and also to elucidate the influence of inputs on the latent variables. The classification accuracy of images is significantly lowered when the most important pixel regions highlighted by the saliency maps are corrupted validating the proposed approach.},
  archive      = {J_PR},
  author       = {Dharanidharan Arumugam and Ravi Kiran},
  doi          = {10.1016/j.patcog.2022.109212},
  journal      = {Pattern Recognition},
  pages        = {109212},
  shortjournal = {Pattern Recognition},
  title        = {Interpreting denoising autoencoders with complex perturbation approach},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised cross-modal hashing via modality-specific
and cross-modal graph convolutional networks. <em>PR</em>, <em>136</em>,
109211. (<a href="https://doi.org/10.1016/j.patcog.2022.109211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing maps heterogeneous multimedia data into Hamming space for retrieving relevant samples across modalities, which has received great research interests due to its rapid retrieval and low storage cost. In real-world applications, due to high manual annotation cost of multi-media data, we can only make use of limited number of labeled data with rich unlabeled data . In recent years, several semi-supervised cross-modal hashing (SCH) methods have been presented. However, how to fully explore and jointly utilize the modality-specific (complementarity) and modality-shared (correlation) information for retrieval has not been well studied for existing SCH works. In this paper, we propose a novel SCH approach named Modality-specific and Cross-modal Graph Convolutional Networks (MCGCN). The network architecture contains two modality-specific channels and a cross-modal channel to learn modality-specific and shared representations for each modality, respectively. Graph convolutional network (GCN) is leveraged in these three channels to explore intra-modal and inter-modal similarity, and perform semantic information propagation from labeled data to unlabeled data . Modality-specific and shared representations for each modality are fused with attention scheme. To further reduce the modality gap, a discriminative model is designed, learning to classify the modality of representations, and network training is guided by adversarial scheme. Experiments on two widely used multi-modal datasets demonstrate MCGCN outperforms state-of-the-art semi-supervised/supervised cross-modal hashing methods .},
  archive      = {J_PR},
  author       = {Fei Wu and Shuaishuai Li and Guangwei Gao and Yimu Ji and Xiao-Yuan Jing and Zhiguo Wan},
  doi          = {10.1016/j.patcog.2022.109211},
  journal      = {Pattern Recognition},
  pages        = {109211},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised cross-modal hashing via modality-specific and cross-modal graph convolutional networks},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On better detecting and leveraging noisy samples for
learning with severe label noise. <em>PR</em>, <em>136</em>, 109210. (<a
href="https://doi.org/10.1016/j.patcog.2022.109210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the success of learning with noisy labels, existing approaches show limited performance when the noise level is extremely high, since deep neural networks (DNNs) are easily overfit to the training set with corrupted labels. In this paper, we introduce Lipschitz regularization to prevent the DNNs from over-fitting to noisy labels quickly. Meanwhile, to better detect and leverage the noisy samples, we propose a Lipschitz regularization based framework with a combination of adaptive modeling and detection module and improved semi-supervised learning. We propose to adaptively model the real distribution of the training set, and the implicit individual clean/noisy distribution, instead of parametric models . With Bayes’ rule, we then compute the posterior probability of a sample being clean, which provides a dynamic threshold for the detection of noisy labels. To reduce training instability caused by less labeled data with severe label noise, we improve the semi-supervised learning by combining the advantages of Mixup and FixMatch. It can not only increase the diversity of unlabeled samples , but also improve the generalization capability of the DNNs to avoid over-fitting. Experiments on several benchmarks demonstrate that our approach achieves comparable results with the state-of-the-art methods in the less-noisy environment, and obtains a substantial improvement ( ∼ ∼ 8\% and ∼ ∼ 6\% in accuracy on CIFAR-10 and CIFAR-100 respectively) with severe noise.},
  archive      = {J_PR},
  author       = {Qing Miao and Xiaohe Wu and Chao Xu and Wangmeng Zuo and Zhaopeng Meng},
  doi          = {10.1016/j.patcog.2022.109210},
  journal      = {Pattern Recognition},
  pages        = {109210},
  shortjournal = {Pattern Recognition},
  title        = {On better detecting and leveraging noisy samples for learning with severe label noise},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual question answering from another perspective: CLEVR
mental rotation tests. <em>PR</em>, <em>136</em>, 109209. (<a
href="https://doi.org/10.1016/j.patcog.2022.109209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different types of mental rotation tests have been used extensively in psychology to understand human visual reasoning and perception. Understanding what an object or visual scene would look like from another viewpoint is a challenging problem that is made even harder if it must be performed from a single image. We explore a controlled setting whereby questions are posed about the properties of a scene if that scene was observed from another viewpoint. To do this we have created a new version of the CLEVR dataset that we call CLEVR Mental Rotation Tests (CLEVR-MRT). Using CLEVR-MRT we examine standard methods, show how they fall short, then explore novel neural architectures that involve inferring volumetric representations of a scene. These volumes can be manipulated via camera-conditioned transformations to answer the question. We examine the efficacy of different model variants through rigorous ablations and demonstrate the efficacy of volumetric representations.},
  archive      = {J_PR},
  author       = {Christopher Beckham and Martin Weiss and Florian Golemo and Sina Honari and Derek Nowrouzezahrai and Christopher Pal},
  doi          = {10.1016/j.patcog.2022.109209},
  journal      = {Pattern Recognition},
  pages        = {109209},
  shortjournal = {Pattern Recognition},
  title        = {Visual question answering from another perspective: CLEVR mental rotation tests},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Region-wise loss for biomedical image segmentation.
<em>PR</em>, <em>136</em>, 109208. (<a
href="https://doi.org/10.1016/j.patcog.2022.109208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Region-wise (RW) loss for biomedical image segmentation . Region-wise loss is versatile, can simultaneously account for class imbalance and pixel importance, and it can be easily implemented as the pixel-wise multiplication between the softmax output and a RW map. We show that, under the proposed RW loss framework, certain loss functions, such as Active Contour and Boundary loss, can be reformulated similarly with appropriate RW maps, thus revealing their underlying similarities and a new perspective to understand these loss functions. We investigate the observed optimization instability caused by certain RW maps, such as Boundary loss distance maps, and we introduce a mathematically-grounded principle to avoid such instability. This principle provides excellent adaptability to any dataset and practically ensures convergence without extra regularization terms or optimization tricks. Following this principle, we propose a simple version of boundary distance maps called rectified Region-wise (RRW) maps that, as we demonstrate in our experiments, achieve state-of-the-art performance with similar or better Dice coefficients and Hausdorff distances than Dice, Focal, weighted Cross entropy, and Boundary losses in three distinct segmentation tasks . We quantify the optimization instability provided by Boundary loss distance maps, and we empirically show that our RRW maps are stable to optimize. The code to run all our experiments is publicly available at: https://github.com/jmlipman/RegionWiseLoss .},
  archive      = {J_PR},
  author       = {Juan Miguel Valverde and Jussi Tohka},
  doi          = {10.1016/j.patcog.2022.109208},
  journal      = {Pattern Recognition},
  pages        = {109208},
  shortjournal = {Pattern Recognition},
  title        = {Region-wise loss for biomedical image segmentation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The dahu graph-cut for interactive segmentation on 2D/3D
images. <em>PR</em>, <em>136</em>, 109207. (<a
href="https://doi.org/10.1016/j.patcog.2022.109207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive image segmentation is an important application in computer vision for selecting objects of interest in images. Several interactive segmentation methods are based on distance transform algorithms. However, the most known distance transform, geodesic distance , is sensitive to noise in the image and to seed placement. Recently, the Dahu pseudo-distance, a continuous version of the minimum barrier distance (MBD), is proved to be more powerful than the geodesic distance in noisy and blurred images. This paper presents a method for combining the Dahu pseudo-distance with edge information in a graph-cut optimization framework and leveraging each’s complementary strengths. Our method works efficiently on both 2D/3D images and videos. Results show that our method achieves better performance than other distance-based and graph-cut methods, thereby reducing the user’s efforts.},
  archive      = {J_PR},
  author       = {Minh Ôn Vũ Ngọc and Edwin Carlinet and Jonathan Fabrizio and Thierry Géraud},
  doi          = {10.1016/j.patcog.2022.109207},
  journal      = {Pattern Recognition},
  pages        = {109207},
  shortjournal = {Pattern Recognition},
  title        = {The dahu graph-cut for interactive segmentation on 2D/3D images},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Training compact DNNs with ℓ1/2 regularization. <em>PR</em>,
<em>136</em>, 109206. (<a
href="https://doi.org/10.1016/j.patcog.2022.109206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network(DNN) has achieved unprecedented success in many fields. However, its large model parameters which bring a great burden on storage and calculation hinder the development and application of DNNs. It is worthy of compressing the model to reduce the complexity of the DNN. Sparsity-inducing regularizer is one of the most common tools for compression. In this paper, we propose utilizing the ℓ 1 / 2 ℓ1/2 quasi-norm to zero out weights of neural networks and compressing the networks automatically during the learning process. To our knowledge, it is the first work applying the non-Lipschitz continuous regularizer for the compression of DNNs. The resulting sparse optimization problem is solved by stochastic proximal gradient algorithm. For further convenience of calculation, an approximation of the threshold-form solution to the proximal operator with ℓ 1 / 2 ℓ1/2 is given at the same time. Extensive experiments with various datasets and baselines demonstrate the advantages of our new method.},
  archive      = {J_PR},
  author       = {Anda Tang and Lingfeng Niu and Jianyu Miao and Peng Zhang},
  doi          = {10.1016/j.patcog.2022.109206},
  journal      = {Pattern Recognition},
  pages        = {109206},
  shortjournal = {Pattern Recognition},
  title        = {Training compact DNNs with ℓ1/2 regularization},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local linear embedding with adaptive neighbors. <em>PR</em>,
<em>136</em>, 109205. (<a
href="https://doi.org/10.1016/j.patcog.2022.109205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction is one of the most important techniques in the field of data mining. It embeds high-dimensional data into a low-dimensional vector space while keeping the main information as much as possible. Locally Linear Embedding (LLE) as a typical manifold learning algorithm computes neighborhood preserving embeddings of high-dimensional inputs. Based on the thought of LLE, we propose a novel unsupervised dimensionality reduction model called Local Linear Embedding with Adaptive Neighbors (LLEAN). To achieve a desirable dimensionality reduction result, we impose adaptive neighbor strategy and adopt a projection matrix to project data into an optimal subspace. The relationship between every pair-wise data is investigated to help reveal the data structure . Augmented Lagrangian Multiplier (ALM) is devised in optimization procedure to effectively solve the proposed objective function. Comprehensive experiments on toy data and benchmark datasets have been done and the results show that LLEAN outperforms other state-of-the-art dimensionality reduction methods.},
  archive      = {J_PR},
  author       = {Jiaqi Xue and Bin Zhang and Qianyao Qiang},
  doi          = {10.1016/j.patcog.2022.109205},
  journal      = {Pattern Recognition},
  pages        = {109205},
  shortjournal = {Pattern Recognition},
  title        = {Local linear embedding with adaptive neighbors},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relation-aware attention for video captioning via graph
learning. <em>PR</em>, <em>136</em>, 109204. (<a
href="https://doi.org/10.1016/j.patcog.2022.109204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning often uses an attentive encoder-decoder as the baseline model . However, the conventional attention mechanism still remains two problems. First, the attended visual feature is often irrelevant to the target word state, because the attention process only uses the unidirectional flow from vision to linguistics, while lacking the reverse flow. Second, each attention result is independent, because it is computed only based on the previous word states while not considering the attention information from the past and future. This does not suit the attention habits of human beings. In this paper, we improve the conventional attention mechanism to a relation-aware attention mechanism. To this end, we propose two kinds of graph learning strategies, namely the linguistics-to-vision heterogeneous graph (HTG) and the vision-to-vision homogeneous graph (HMG). The HTG aims to enhance the inter-relation of attention by reversely modeling the relation of each word with respect to every attended visual feature, supporting proper semantic alignment in between. The HMG aims to enhance the intra-relation of attention by capturing the relations among all of the attended visual features, which can leverage the attention information from the past and future to guide the current attention process. Extensive experiments on two public datasets show that our proposed method not only significantly improves the baseline model , but also outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yunbin Tu and Chang Zhou and Junjun Guo and Huafeng Li and Shengxiang Gao and Zhengtao Yu},
  doi          = {10.1016/j.patcog.2022.109204},
  journal      = {Pattern Recognition},
  pages        = {109204},
  shortjournal = {Pattern Recognition},
  title        = {Relation-aware attention for video captioning via graph learning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature learning network with transformer for multi-label
image classification. <em>PR</em>, <em>136</em>, 109203. (<a
href="https://doi.org/10.1016/j.patcog.2022.109203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of multi-label image classification task is to accurately assign a set of labels to the objects in images. Although promising results have been achieved, most of the existing methods cannot effectively learn multi-scale features, so it is difficult to identify small-scale objects from images. Besides, current attention-based methods tend to learn the most salient feature regions in images, but fail to excavate various potential useful features concealed by the most salient feature, thus limiting the further improvement of model performance. To address above issues, we propose a novel Feature Learning network based on Transformer to learn salient features and excavate potential useful features ( FL-Tran ). Specifically, in order to solve the problem that current methods are difficult to identify small-scale objects, we first present a novel multi-scale fusion module (MSFM) to align high-level features and low-level features to learn multi-scale features. Additionally, a spatial attention module (SAM) utilizing transformer encoder is introduced to capture salient object features in images to enhance the model performance. Furthermore, we devise a feature enhancement and suppression module (FESM) with the aim of excavating potential useful features concealed by the most salient features. By suppressing the most salient features obtained in current SAM layer, and then forcing subsequent SAM layer to excavate potential salient features in feature maps, FL-Tran model can learn various useful features more comprehensively. Extensive experiments on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE datasets demonstrate that our proposed FL-Tran model outperforms current state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Wei Zhou and Peng Dou and Tao Su and Haifeng Hu and Zhijie Zheng},
  doi          = {10.1016/j.patcog.2022.109203},
  journal      = {Pattern Recognition},
  pages        = {109203},
  shortjournal = {Pattern Recognition},
  title        = {Feature learning network with transformer for multi-label image classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-layer memory sharing network for video captioning.
<em>PR</em>, <em>136</em>, 109202. (<a
href="https://doi.org/10.1016/j.patcog.2022.109202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past several years, video captioning has received much attention in computer vision and machine learning communities. Many models utilize an RNN-based decoder to generate sentences describing the content of a video. They have achieved much progress; however, few methods adopt a decoder with more than three layers because an RNN-based model with more layers may become hard to train, time-consuming or even deteriorate at a certain depth. To address the limitation, we propose a M ulti-layer m e mory s haring Net work, MesNet for short, which allows more layers to be stacked without compromising performance. In MesNet, we construct a novel memory sharing structure to strengthen the connections between layers and make the model easier to train. More specifically, we design an Enhanced Gated Recurrent Unit (En-GRU) and stack it to construct a deeper network. Unlike traditional RNN-based multi-layer networks, the memory states of all layers in MesNet are cross-used at each iteration to mimic the brain’s complex connections. Extensive experiments on MSVD and MSR-VTT demonstrate that our method performs well and outperforms some state-of-the-art methods significantly. Our code is available at https://github.com/nbbb/MesNet .},
  archive      = {J_PR},
  author       = {Tian-Zi Niu and Shan-Shan Dong and Zhen-Duo Chen and Xin Luo and Zi Huang and Shanqing Guo and Xin-Shun Xu},
  doi          = {10.1016/j.patcog.2022.109202},
  journal      = {Pattern Recognition},
  pages        = {109202},
  shortjournal = {Pattern Recognition},
  title        = {A multi-layer memory sharing network for video captioning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep attentive time warping. <em>PR</em>, <em>136</em>,
109201. (<a href="https://doi.org/10.1016/j.patcog.2022.109201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity measures for time series are important problems for time series classification. To handle the nonlinear time distortions, Dynamic Time Warping (DTW) has been widely used. However, DTW is not learnable and suffers from a trade-off between robustness against time distortion and discriminative power . In this paper, we propose a neural network model for task-adaptive time warping. Specifically, we use the attention model, called the bipartite attention model, to develop an explicit time warping mechanism with greater distortion invariance. Unlike other learnable models using DTW for warping, our model predicts all local correspondences between two time series and is trained based on metric learning, which enables it to learn the optimal data-dependent warping for the target task. We also propose to induce pre-training of our model by DTW to improve the discriminative power. Extensive experiments demonstrate the superior effectiveness of our model over DTW and its state-of-the-art performance in online signature verification.},
  archive      = {J_PR},
  author       = {Shinnosuke Matsuo and Xiaomeng Wu and Gantugs Atarsaikhan and Akisato Kimura and Kunio Kashino and Brian Kenji Iwana and Seiichi Uchida},
  doi          = {10.1016/j.patcog.2022.109201},
  journal      = {Pattern Recognition},
  pages        = {109201},
  shortjournal = {Pattern Recognition},
  title        = {Deep attentive time warping},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive generation of 3D point clouds with hierarchical
consistency. <em>PR</em>, <em>136</em>, 109200. (<a
href="https://doi.org/10.1016/j.patcog.2022.109200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating 3D point cloud directly from latent prior ( e.g. , Gaussian distribution) plays a vital role in the representation learning and data augmentation in 3D vision tasks. Since point cloud is formed by irregular points, the generation process of point cloud requires rich semantic information, yet few studies are devoted to it. In this paper, we recast this generation task as a progressive learning problem to model the two-level hierarchy of distributions and address it by proposing a novel model called hierarchical consistency variational autoencoder (HC-VAE). This framework introduces a hierarchical consistent mechanism (HCM) to model the shape consistency and the pointwise representation consistency in a complementary manner. Specifically, we propose a stackable encoder-decoder framework and constrain the generation quality progressively to ensure that the underlying shape and fine-grained parts can be reconstructed with high fidelity. Additionally, given the progressively generated intermediate point cloud instances, a hierarchical-positive contrastive loss is introduced to learn the point-distribution-free instance representations to avoid explicitly parametrizing the distribution of points in a shape. In this way, our model suffices to generate diverse, high-resolution, and uniform point cloud instances. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art performance in point cloud generation.},
  archive      = {J_PR},
  author       = {Peipei Li and Xiyan Liu and Jizhou Huang and Deguo Xia and Jianzhong Yang and Zhen Lu},
  doi          = {10.1016/j.patcog.2022.109200},
  journal      = {Pattern Recognition},
  pages        = {109200},
  shortjournal = {Pattern Recognition},
  title        = {Progressive generation of 3D point clouds with hierarchical consistency},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The neglected background cues can facilitate finger vein
recognition. <em>PR</em>, <em>136</em>, 109199. (<a
href="https://doi.org/10.1016/j.patcog.2022.109199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, finger vein based biometric authentication has attracted considerable attention due to its high efficiency and high security. However, most existing finger vein representation methods focus on vein traits while ignoring background cues, although background cues also convey identity information specific to each individual. In this paper, we leverage background intensity variations in finger vein images as new features to enrich discriminative representation, and accordingly propose a new descriptor named Intensity Orientation Vector (IOV). IOV, scaleable to reflect characteristics of finger tissues, offers additional informative cues for finger vein representation. Furthermore, we propose a new learning scheme named Semantic Similarity Preserved Discrete Binary Feature Learning (SSP-DBFL) for finger vein recognition . Unlike the most bimodal binary feature representation methods, SSP-DBFL preserves high-level semantic similarity in a common Hamming space to exploit the consensus between vein traits and background cues. Specifically, given a finger vein image, we first extract the direction difference vectors (DDV) as the main vein traits and the IOV as the auxiliary background cues. Subsequently, we jointly learn projection functions from these two types of features in a supervised manner, converting the two features into discriminative binary codes with their semantic similarity preserved. Finally, the binary codes are pooled into histogram-based vectors for finger vein representation. Extensive experiments are conducted on five widely used finger vein databases and demonstrate the effectiveness of our proposed IOV and SSP-DBFL.},
  archive      = {J_PR},
  author       = {Pengyang Zhao and Shuping Zhao and Jing-Hao Xue and Wenming Yang and Qingmin Liao},
  doi          = {10.1016/j.patcog.2022.109199},
  journal      = {Pattern Recognition},
  pages        = {109199},
  shortjournal = {Pattern Recognition},
  title        = {The neglected background cues can facilitate finger vein recognition},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prior depth-based multi-view stereo network for online 3D
model reconstruction. <em>PR</em>, <em>136</em>, 109198. (<a
href="https://doi.org/10.1016/j.patcog.2022.109198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the online multi-view stereo (MVS) problem when reconstructing precise 3D models in real time. To solve this problem, most previous studies adopted a motion stereo approach that sequentially estimates depth maps from multiple localized images captured in a local time window. To compute the depth maps quickly, the motion stereo methods process down-sampled images or use a simplified algorithm for cost volume regularization ; therefore, they generally produce reconstructed 3D models that are inaccurate. In this paper, we propose a novel online MVS method that accurately reconstructs high-resolution 3D models. This method infers prior depth information based on sequentially estimated depths and leverages it to estimate depth maps more precisely. The method constructs a cost volume by using the prior-depth-based visibility information and then fuses the prior depths into the cost volume. This approach significantly improves the stereo matching performance and completeness of the estimated depths . Extensive experiments showed that the proposed method outperforms other state-of-the-art MVS and motion stereo methods. In particular, it significantly improves the completeness of 3D models.},
  archive      = {J_PR},
  author       = {Soohwan Song and Khang Giang Truong and Daekyum Kim and Sungho Jo},
  doi          = {10.1016/j.patcog.2022.109198},
  journal      = {Pattern Recognition},
  pages        = {109198},
  shortjournal = {Pattern Recognition},
  title        = {Prior depth-based multi-view stereo network for online 3D model reconstruction},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Annotator-dependent uncertainty-aware estimation of gait
relative attributes. <em>PR</em>, <em>136</em>, 109197. (<a
href="https://doi.org/10.1016/j.patcog.2022.109197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe an uncertainty-aware estimation framework for gait relative attributes. We specifically design a two-stream network model that takes a pair of gait videos as input. It then outputs a corresponding pair of Gaussian distributions of gait absolute attribute scores and annotator-dependent gait relative attribute label distributions. Moreover, we propose a differentiable annotator-independent uncertainty layer to estimate the gait relative attribute score distribution from the absolute distributions then map it to a relative attribute label distribution using the computation of cumulative distribution functions. Furthermore, we propose another annotator-dependent uncertainty layer to estimate the uncertainty on the gait relative attribute labels in terms of a set of trainable transition matrices . Finally, we design a joint loss function on the relative attribute label distribution to learn the model parameters. Experiments on two gait relative attribute datasets demonstrated the effectiveness of the proposed method against baselines in quantitative and qualitative evaluations.},
  archive      = {J_PR},
  author       = {Allam Shehata and Yasushi Makihara and Daigo Muramatsu and Md Atiqur Rahman Ahad and Yasushi Yagi},
  doi          = {10.1016/j.patcog.2022.109197},
  journal      = {Pattern Recognition},
  pages        = {109197},
  shortjournal = {Pattern Recognition},
  title        = {Annotator-dependent uncertainty-aware estimation of gait relative attributes},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear discriminant analysis with generalized kernel
constraint for robust image classification. <em>PR</em>, <em>136</em>,
109196. (<a href="https://doi.org/10.1016/j.patcog.2022.109196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) as a classical supervised dimensionality reduction method has shown powerful capability in various image classification tasks. The purpose of LDA seeks an optimal linear transformation that maps the original data to a low-dimensional space. Inspired by the fact that the kernel trick can capture the nonlinear similarity of features, we propose a novel generalized distance constraint dubbed intra-class and inter-class kernel constraint (IIKC). The proposed IIKC explicitly models the category kernel distance and focuses on helping the original LDA capture more discriminant features in order to further improve the separability and magnitude difference between nearby data points. Our proposed method with IIKC aims to achieve maximum category separability by minimizing the intra-class kernel distances as well as maximizing the inter-class kernel distance, simultaneously. Extensive experimental results on six publicly available benchmark databases illustrate that the LDA-based methods embedded with the proposed IIKC significantly improve the discrimination ability and achieve a better classification performance than the original and state-of-the-art LDA algorithms.},
  archive      = {J_PR},
  author       = {Shuyi Li and Hengmin Zhang and Ruijun Ma and Jianhang Zhou and Jie Wen and Bob Zhang},
  doi          = {10.1016/j.patcog.2022.109196},
  journal      = {Pattern Recognition},
  pages        = {109196},
  shortjournal = {Pattern Recognition},
  title        = {Linear discriminant analysis with generalized kernel constraint for robust image classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brain-like retinex: A biologically plausible retinex
algorithm for low light image enhancement. <em>PR</em>, <em>136</em>,
109195. (<a href="https://doi.org/10.1016/j.patcog.2022.109195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinex theory was first proposed by Land and McCann [1], where retinex is a portmanteau derived from the words of retina and cortex, implying that both the retina and cerebral cortex may participate in the perception of lightness and color. However, there are no recent reports on how the retina and visual cortex perform retinex decomposition. In this paper, we propose a biologically plausible solution to retinex decomposition. We develop an algorithm motivated by the primate’s retinal circuit to detect textural gradients, design an algorithm originating from the visual cortex to extract image contours, and thus split image edges into image contours and textural gradients. Then, we establish a variational model for retinex decomposition by using image contours and textural gradients to encode discontinuities in illumination and variations in reflectance, respectively. We also apply the proposed retinex model to low light image enhancement, high dynamic resolution image toning, and color constancy. Experiments show consistent superiority of the proposed algorithm. The code is available at Github .},
  archive      = {J_PR},
  author       = {Rongtai Cai and Zekun Chen},
  doi          = {10.1016/j.patcog.2022.109195},
  journal      = {Pattern Recognition},
  pages        = {109195},
  shortjournal = {Pattern Recognition},
  title        = {Brain-like retinex: A biologically plausible retinex algorithm for low light image enhancement},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modal hierarchical interaction network for RGB-d
salient object detection. <em>PR</em>, <em>136</em>, 109194. (<a
href="https://doi.org/10.1016/j.patcog.2022.109194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to effectively exchange and aggregate the information of multiple modalities ( e.g. RGB image and depth map) is a big challenge in the RGB-D salient object detection community. To address this problem, in this paper, we propose a cross-modal Hierarchical Interaction Network ( HINet ), which boosts the salient object detection by excavating the cross-modal feature interaction and progressively multi-level feature fusion . To achieve it, we design two modules: cross-modal information exchange (CIE) module and multi-level information progressively guided fusion (PGF) module. Specifically, the CIE module is proposed to exchange the cross-modal features for learning the shared representations, as well as the beneficial feedback to facilitate the discriminative feature learning of different modalities. Besides, the PGF module is designed to aggregate the hierarchical features progressively with the reverse guidance mechanism, which employs the high-level feature fusion to guide the low-level feature fusion and thus improve the saliency detection performance. Extensive experiments show that our proposed model significantly outperforms the existing nine state-of-the-art models on five challenging benchmark datasets. Codes and results are available at: https://github.com/RanwanWu/HINet.},
  archive      = {J_PR},
  author       = {Hongbo Bi and Ranwan Wu and Ziqi Liu and Huihui Zhu and Cong Zhang and Tian-Zhu Xiang},
  doi          = {10.1016/j.patcog.2022.109194},
  journal      = {Pattern Recognition},
  pages        = {109194},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal hierarchical interaction network for RGB-D salient object detection},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). FP-DARTS: Fast parallel differentiable neural architecture
search for image classification. <em>PR</em>, <em>136</em>, 109193. (<a
href="https://doi.org/10.1016/j.patcog.2022.109193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) has made remarkable progress in automatic machine learning . However, it still suffers massive computing overheads limiting its wide applications. In this paper, we present an efficient search method, Fast Parallel Differential Neural Architecture Search (FP-DARTS). The proposed method is carefully designed from three levels to construct and train the super-network. Firstly, at the operation-level, to reduce the computational burden, different from the standard DARTS search space (8 operations), we decompose the operation set into two non-overlapping operator sub-sets (4 operations for each). Adopting these two reduced search spaces, two over-parameterized sub-networks are constructed. Secondly, at the channel-level, the partially-connected strategy is adopted, where each sub-network only adopts partial channels. Then these two sub-networks construct a two-parallel-path super-network by addition. Thirdly, at the training-level, the binary gate is introduced to control whether a path participates in the super-network training. It may suffer an unfair issue when using softmax to select the best input for intermediate nodes across two operator sub-sets. To tackle this problem, the sigmoid function is introduced, which measures the performance of operations without compression. Extensive experiments demonstrate the effectiveness of the proposed algorithm. Specifically, FP-DARTS achieves 2.50\% test error with only 0.08 GPU-days on CIFAR10, and a state-of-the-art top-1 error rate of 23.7\% on ImageNet using only 2.44 GPU-days for search.},
  archive      = {J_PR},
  author       = {Wenna Wang and Xiuwei Zhang and Hengfei Cui and Hanlin Yin and Yannnig Zhang},
  doi          = {10.1016/j.patcog.2022.109193},
  journal      = {Pattern Recognition},
  pages        = {109193},
  shortjournal = {Pattern Recognition},
  title        = {FP-DARTS: Fast parallel differentiable neural architecture search for image classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Laplacian lp norm least squares twin support vector machine.
<em>PR</em>, <em>136</em>, 109192. (<a
href="https://doi.org/10.1016/j.patcog.2022.109192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning has become a hot learning framework, where large amounts of unlabeled data and small amounts of labeled data are available during the training process. The recently proposed Laplacian least squares twin support vector machine (Lap-LSTSVM) is an excellent tool to solve the semi-supervised classification problem. Motivated by the success of Lap-LSTSVM, in this paper, we propose a novel Laplacian Lp norm least squares twin support vector machine (Lap-LpLSTSVM). There are several advantages of our proposed method: (1) The performance of our proposed Lap-LpLSTSVM can be improved by the adjustability of the value of p p . (2) The introduced Lp norm graph regularization term can efficiently exploit the geometric information embedded in the data. (3) An efficient iterative strategy is employed to solve the optimization problem . Besides, to demonstrate that our proposed method can make use of unlabeled data effectively, least squares twin support vector machine (LSTSVM) which only uses the same labeled data is used to compare with our proposed method. The experimental results on both synthetic and real-world datasets show that our proposed method outperforms other state-of-the-art methods and can also deal with noisy datasets.},
  archive      = {J_PR},
  author       = {Xijiong Xie and Feixiang Sun and Jiangbo Qian and Lijun Guo and Rong Zhang and Xulun Ye and Zhijin Wang},
  doi          = {10.1016/j.patcog.2022.109192},
  journal      = {Pattern Recognition},
  pages        = {109192},
  shortjournal = {Pattern Recognition},
  title        = {Laplacian lp norm least squares twin support vector machine},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Automated lesion segmentation in fundus images with
many-to-many reassembly of features. <em>PR</em>, <em>136</em>, 109191.
(<a href="https://doi.org/10.1016/j.patcog.2022.109191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing CNN-based segmentation approaches have achieved remarkable progresses on segmenting objects in regular sizes. However, when migrating them to segment tiny retinal lesions, they encounter challenges. The feature reassembly operators that they adopt are prone to discard the subtle activations about tiny lesions and fail to capture long-term dependencies. This paper aims to solve these issues and proposes a novel Many-to-Many Reassembly of Features (M2MRF) for tiny lesion segmentation . Our proposed M2MRF reassembles features in a dimension-reduced feature space and simultaneously aggregates multiple features inside a large predefined region into multiple output features. In this way, subtle activations about small lesions can be maintained as much as possible and long-term spatial dependencies can be captured to further enhance the lesion features. Experimental results on two lesion segmentation benchmarks, i.e. , DDR and IDRiD, show that 1) our M2MRF outperforms existing feature reassembly operators, and 2) equipped with our M2MRF, the HRNetV2 is able to achieve substantially better performances and generalisation ability than existing methods. Our code is made publicly available at https://github.com/CVIU-CSU/M2MRF-Lesion-Segmentation .},
  archive      = {J_PR},
  author       = {Qing Liu and Haotian Liu and Wei Ke and Yixiong Liang},
  doi          = {10.1016/j.patcog.2022.109191},
  journal      = {Pattern Recognition},
  pages        = {109191},
  shortjournal = {Pattern Recognition},
  title        = {Automated lesion segmentation in fundus images with many-to-many reassembly of features},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pure graph-guided multi-view subspace clustering.
<em>PR</em>, <em>136</em>, 109187. (<a
href="https://doi.org/10.1016/j.patcog.2022.109187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering approaches have shown outstanding performance in revealing similarity relationships and complex structures hidden in data. Despite the progress, previous multi-view clustering methods still face two challenges: (1) it is difficult to simultaneously achieve sparsity and connectivity of the affinity graph; (2) existing methods usually separate the graph learning step from the clustering process , which leads to unsatisfactory clustering performance as the final results critically rely on the learned graph. In this paper, we propose to achieve a structured consensus graph for multi-view subspace clustering by leveraging the sparsity and connectivity of each affinity graph. In the proposed method, the pure graph for each view is searched by finding the good neighbors. The multiple pure graphs are further fused into a consensus graph with a block-diagonal structure. That is, the consensus graph is enforced to contain exactly c c connected components where c c is the number of the clusters. Hence the label to each sample can be directly assigned since each connected component precisely corresponds to an individual cluster. As a result, the proposed model seamlessly accomplishes the subtasks including graph construction , pure graph learning (i.e., good neighbors searching), and cluster label allocation in a mutual reinforcement manner. Extensive experimental results demonstrate the superiority and reliability of our proposed method.},
  archive      = {J_PR},
  author       = {Hongjie Wu and Shudong Huang and Chenwei Tang and Yancheng Zhang and Jiancheng Lv},
  doi          = {10.1016/j.patcog.2022.109187},
  journal      = {Pattern Recognition},
  pages        = {109187},
  shortjournal = {Pattern Recognition},
  title        = {Pure graph-guided multi-view subspace clustering},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast geometrical extraction of nearest neighbors from
multi-dimensional data. <em>PR</em>, <em>136</em>, 109183. (<a
href="https://doi.org/10.1016/j.patcog.2022.109183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-Nearest Neighbor (KNN) algorithm plays a significant role in various fields of data science and machine learning . Most variants of the KNN algorithm involve distance computations and a parameter (K) that represents the required number of neighbors. The recent research regarding distance computations and finding the optimal value of K have made neighborhood extraction a slow process. This research presents a fast geometrical approach for neighborhood extraction from multi-dimensional data. Instead of distance computations, the proposed algorithm creates a geometrical shape based on the number of features of data. This geometrical shape encompasses the reference data point and the neighboring points. The proposed algorithm&#39;s efficiency of time, classification, and hashing are evaluated and compared with existing state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Yasir Aziz and Kashif Hussain Memon},
  doi          = {10.1016/j.patcog.2022.109183},
  journal      = {Pattern Recognition},
  pages        = {109183},
  shortjournal = {Pattern Recognition},
  title        = {Fast geometrical extraction of nearest neighbors from multi-dimensional data},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial training with distribution normalization and
margin balance. <em>PR</em>, <em>136</em>, 109182. (<a
href="https://doi.org/10.1016/j.patcog.2022.109182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training is the most effective method to improve adversarial robustness. However, it does not explicitly regularize the feature space during training. Adversarial attacks usually move a sample iteratively along the direction which causes the steepest ascent of classification loss by crossing decision boundary. To alleviate this problem, we propose to regularize the distributions of different classes to increase the difficulty of finding an attacking direction. Specifically, we propose two strategies named Distribution Normalization (DN) and Margin Balance (MB) for adversarial training. The purpose of DN is to normalize the features of each class to have identical variance in every direction, in order to eliminate easy-to-attack intra-class directions. The purpose of MB is to balance the margins between different classes, making it harder to find confusing class directions (i.e., those with smaller margins) to attack. When integrated with adversarial training, our method can significantly improve adversarial robustness. Extensive experiments under white-box, black-box, and adaptive attacks demonstrate the effectiveness of our method over other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhen Cheng and Fei Zhu and Xu-Yao Zhang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2022.109182},
  journal      = {Pattern Recognition},
  pages        = {109182},
  shortjournal = {Pattern Recognition},
  title        = {Adversarial training with distribution normalization and margin balance},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). This looks more like that: Enhancing self-explaining models
by prototypical relevance propagation. <em>PR</em>, <em>136</em>,
109172. (<a href="https://doi.org/10.1016/j.patcog.2022.109172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current machine learning models have shown high efficiency in solving a wide variety of real-world problems. However, their black box character poses a major challenge for the comprehensibility and traceability of the underlying decision-making strategies. As a remedy, numerous post-hoc and self-explanation methods have been developed to interpret the models’ behavior. Those methods, in addition, enable the identification of artifacts that, inherent in the training data, can be erroneously learned by the model as class-relevant features. In this work, we provide a detailed case study of a representative for the state-of-the-art self-explaining network, ProtoPNet, in the presence of a spectrum of artifacts. Accordingly, we identify the main drawbacks of ProtoPNet, especially its coarse and spatially imprecise explanations. We address these limitations by introducing Prototypical Relevance Propagation (PRP), a novel method for generating more precise model-aware explanations. Furthermore, in order to obtain a clean, artifact-free dataset, we propose to use multi-view clustering strategies for segregating the artifact images using the PRP explanations, thereby suppressing the potential artifact learning in the models.},
  archive      = {J_PR},
  author       = {Srishti Gautam and Marina M.-C. Höhne and Stine Hansen and Robert Jenssen and Michael Kampffmeyer},
  doi          = {10.1016/j.patcog.2022.109172},
  journal      = {Pattern Recognition},
  pages        = {109172},
  shortjournal = {Pattern Recognition},
  title        = {This looks more like that: Enhancing self-explaining models by prototypical relevance propagation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lower bound estimation of recommendation error through user
uncertainty modeling. <em>PR</em>, <em>136</em>, 109171. (<a
href="https://doi.org/10.1016/j.patcog.2022.109171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning , the Bayesian error is the lower bound of the prediction error induced by data distribution. In recommender systems , this is also known as the magic barrier (MGBR). MGBR estimation is an important issue because the recommended data frequently contain considerable uncertainties that are difficult to quantify. It is possible to determine the extent to which the recommendation algorithm can be optimized by obtaining the MGBR for a given dataset. MGBR estimation generally requires real user ratings that are not affected by external factors such as human emotions and living environment, which can be extremely difficult or even impossible to gather. Existing theoretical approaches based on simple models, such as Gaussian distributions, have limited estimation capabilities. In this paper, we propose a more sophisticated mixture of exponential power (MoEP) model, which enables adaptive parameter selection for intricate uncertainty. To fit the distribution of the real data, we constructed a flexible learning model that automatically adjusts super- or sub-Gaussian uncertainties using the MoEP components. To select parameters adaptively, we employed an expectation-maximization algorithm to infer the parameters of the components. To estimate the MGBR, we explored an approach for calculating the lower bound of the prediction error under the guidance of a probability model. Experiments on the four datasets validated the rationality of the proposed method. The results show that the MGBR estimated using the new model is marginally lower than the prediction error of state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Heng-Ru Zhang and Ying Qiu and Ke-Lin Zhu and Fan Min},
  doi          = {10.1016/j.patcog.2022.109171},
  journal      = {Pattern Recognition},
  pages        = {109171},
  shortjournal = {Pattern Recognition},
  title        = {Lower bound estimation of recommendation error through user uncertainty modeling},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint optimization of scoring and thresholding models for
online multi-label classification. <em>PR</em>, <em>136</em>, 109167.
(<a href="https://doi.org/10.1016/j.patcog.2022.109167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing online multi-label classification works cannot well handle the online label thresholding problem and lack regret analysis for their online algorithms . This paper proposes a novel framework of joint optimization of scoring and thresholding models for online multi-label classification, with the aim to overcome the above drawbacks. The key feature of our framework is that both scoring and thresholding models are included as important components of the online multi-label classifier and are incorporated into one online optimization problem . Based on this framework, we present two adaptive label thresholding algorithms and two fixed thresholding algorithms. For each type of algorithms, a first-order method and a second-order one are provided for updating the online multi-label classifier. Both methods enjoy a closed-form update. Our proposed algorithms are proved to achieve a sub-linear regret. Using Mercer kernels, two first-order algorithms can be extended to handle nonlinear multi-label prediction tasks. Experiments show the advantage of the adaptive and the fixed thresholding algorithms, in terms of various multi-label performance metrics.},
  archive      = {J_PR},
  author       = {Tingting Zhai and Hao Wang and Hongcheng Tang},
  doi          = {10.1016/j.patcog.2022.109167},
  journal      = {Pattern Recognition},
  pages        = {109167},
  shortjournal = {Pattern Recognition},
  title        = {Joint optimization of scoring and thresholding models for online multi-label classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised instance segmentation via category-aware
centerness learning with localization supervision. <em>PR</em>,
<em>136</em>, 109165. (<a
href="https://doi.org/10.1016/j.patcog.2022.109165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNN) trained with pixel-level segmentation masks achieve high performance in the task of instance segmentation. The difficulty of acquiring such annotation limits the application and popularization of the DCNN-based approaches. To address the issue, a weakly supervised approach is proposed in the paper which performs instance segmentation only with the supervision of bounding box or coarse localization annotation. A novel DCNN model is constructed which consists of two branches: the centerness branch and the segmentation branch. The former branch is to learn the semantically spatial importance over the areas of object instances under the localization supervision. Object proposals with exact boundaries are automatically generated and are then ranked under the guidance of the output of the centerness branch. The most matched instance proposal is assigned to each object, which is then used to supervise the segmentation branch. The losses are calculated by both the outputs of the two branches and the entire DCNN model is trained end-to-end. Experiments are extensively conducted to verify the effectiveness. With the supervision of precise bounding box annotation, our approach achieves state-of-the-art (SOTA) accuracy in the comparison with recent related works. And in the case of coarse localization annotation, our approach only deduces a slight reduction in accuracy, which significantly outperforms other approaches. The excellent performance demonstrates that our approach would be helpful to further alleviate the workload of image annotation while maintaining competitive accuracy.},
  archive      = {J_PR},
  author       = {Jiabin Zhang and Hu Su and Yonghao He and Wei Zou},
  doi          = {10.1016/j.patcog.2022.109165},
  journal      = {Pattern Recognition},
  pages        = {109165},
  shortjournal = {Pattern Recognition},
  title        = {Weakly supervised instance segmentation via category-aware centerness learning with localization supervision},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weight matrix sharing for multi-label learning. <em>PR</em>,
<em>136</em>, 109156. (<a
href="https://doi.org/10.1016/j.patcog.2022.109156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning on real-world data is a challenging task due to sparse labels, missing labels, and sparse structures. Some existing approaches are effective in addressing the former two issues. In this paper, we propose a shared weight matrix with low-rank and sparse regularization for multi-label learning (2SML) algorithm to address the issues simultaneously. First, two explicit correlation matrices are constructed from the feature matrix and label matrix. Second, we select informative labels by instance representativeness to learn implicit correlations. Third, a feature manifold and label manifold are employed to guide the shared weight learning process. Extensive experiments are undertaken on multiple benchmark datasets with and without missing labels. The results show that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Kun Qian and Xue-Yang Min and Yusheng Cheng and Fan Min},
  doi          = {10.1016/j.patcog.2022.109156},
  journal      = {Pattern Recognition},
  pages        = {109156},
  shortjournal = {Pattern Recognition},
  title        = {Weight matrix sharing for multi-label learning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ICC++: Explainable feature learning for art history using
image compositions. <em>PR</em>, <em>136</em>, 109153. (<a
href="https://doi.org/10.1016/j.patcog.2022.109153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image compositions are helpful in the study of image structures and assist in discovering the semantics of the underlying scene portrayed across art forms and styles. With the digitization of artworks in recent years, thousands of images of a particular scene or narrative could potentially be linked together. However, manually linking this data with consistent objectiveness can be a highly challenging and time-consuming task. In this work, we present a novel approach called Image Composition Canvas (ICC + + ++ ) to compare and retrieve images having similar compositional elements. ICC + + ++ is an improvement over ICC, specializing in generating low and high-level features (compositional elements) motivated by Max Imdahl’s work. To this end, we present a rigorous quantitative and qualitative comparison of our approach with traditional and state-of-the-art (SOTA) methods showing that our proposed method outperforms all of them. In combination with deep features, our method outperforms the best deep learning-based method, opening the research direction for explainable machine learning for digital humanities. We will release the code and the data post-publication.},
  archive      = {J_PR},
  author       = {Prathmesh Madhu and Tilman Marquart and Ronak Kosti and Dirk Suckow and Peter Bell and Andreas Maier and Vincent Christlein},
  doi          = {10.1016/j.patcog.2022.109153},
  journal      = {Pattern Recognition},
  pages        = {109153},
  shortjournal = {Pattern Recognition},
  title        = {ICC++: Explainable feature learning for art history using image compositions},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilabel prototype generation for data reduction in
k-nearest neighbour classification. <em>PR</em>, <em>135</em>, 109190.
(<a href="https://doi.org/10.1016/j.patcog.2022.109190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prototype Generation (PG) methods are typically considered for improving the efficiency of the k k -Nearest Neighbour ( k k NN) classifier when tackling high-size corpora. Such approaches aim at generating a reduced version of the corpus without decreasing the classification performance when compared to the initial set. Despite their large application in multiclass scenarios, very few works have addressed the proposal of PG methods for the multilabel space. In this regard, this work presents the novel adaptation of four multiclass PG strategies to the multilabel case. These proposals are evaluated with three multilabel k k NN-based classifiers, 12 corpora comprising a varied range of domains and corpus sizes, and different noise scenarios artificially induced in the data. The results obtained show that the proposed adaptations are capable of significantly improving—both in terms of efficiency and classification performance—the only reference multilabel PG work in the literature as well as the case in which no PG method is applied, also presenting statistically superior robustness in noisy scenarios. Moreover, these novel PG strategies allow prioritising either the efficiency or efficacy criteria through its configuration depending on the target scenario, hence covering a wide area in the solution space not previously filled by other works.},
  archive      = {J_PR},
  author       = {Jose J. Valero-Mas and Antonio Javier Gallego and Pablo Alonso-Jiménez and Xavier Serra},
  doi          = {10.1016/j.patcog.2022.109190},
  journal      = {Pattern Recognition},
  pages        = {109190},
  shortjournal = {Pattern Recognition},
  title        = {Multilabel prototype generation for data reduction in K-nearest neighbour classification},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A novel label enhancement algorithm based on manifold
learning. <em>PR</em>, <em>135</em>, 109189. (<a
href="https://doi.org/10.1016/j.patcog.2022.109189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a label enhancement model to solve the multi-label learning (MLL) problem by using the incremental subspace learning to enrich the label space and to improve the ability of label recognition. In particular, we use the incremental estimation of the feature function representing the manifold structure to guide the construction of the label space and to transform the local topology from the feature space to the label space. First, we build a recursive form for incremental estimation of the feature function representing the feature space information. Second, the label propagation is used to obtain the hidden supervisory information of labels in the data. Finally, an enhanced maximum entropy model based on conditional random field is established as the objective, to obtain the predicted label distribution. The enriched label information in the manifold space obtained in first step and the estimated label distributions provided in second step are employed to train this enhanced maximum entropy model by a gradient-descent iterative optimization to obtain the label distribution predictor’s parameters with enhanced accuracy. We evaluate our method on 24 real-world datasets. Experimental results demonstrate that our label enhancement manifold learning model has advantages in predictive performance over the latest MLL methods.},
  archive      = {J_PR},
  author       = {Chao Tan and Sheng Chen and Xin Geng and Genlin Ji},
  doi          = {10.1016/j.patcog.2022.109189},
  journal      = {Pattern Recognition},
  pages        = {109189},
  shortjournal = {Pattern Recognition},
  title        = {A novel label enhancement algorithm based on manifold learning},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Generalized minimum error entropy for robust learning.
<em>PR</em>, <em>135</em>, 109188. (<a
href="https://doi.org/10.1016/j.patcog.2022.109188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The applications of error entropy (EE) are sometimes limited because its shape cannot be flexibly adjusted by the default Gaussian kernel function to adapt to noise variation and thus lowers the performance of algorithms based on minimum error entropy (MEE) criterion. In this paper, a generalized EE (GEE) is proposed by introducing the generalized Gaussian density (GGD) as its kernel function to improve the robustness of EE. In addition, GEE can be further improved to reduce its computational load by the quantized GEE (QGEE). Furthermore, two learning criteria, called generalized minimum error entropy (GMEE) and quantized generalized minimum error entropy (QGMEE), are developed based on GEE and QGEE, and new adaptive filtering (AF), kernel recursive least squares (KRLS), and multilayer perceptron (MLP) based on the proposed criteria are presented. Several numerical simulations indicate that the performance of proposed algorithms performs better than that of algorithms based on MEE.},
  archive      = {J_PR},
  author       = {Jiacheng He and Gang Wang and Kui Cao and He Diao and Guotai Wang and Bei Peng},
  doi          = {10.1016/j.patcog.2022.109188},
  journal      = {Pattern Recognition},
  pages        = {109188},
  shortjournal = {Pattern Recognition},
  title        = {Generalized minimum error entropy for robust learning},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COVID-19 and rumors: A dynamic nested optimal control model.
<em>PR</em>, <em>135</em>, 109186. (<a
href="https://doi.org/10.1016/j.patcog.2022.109186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unfortunately, the COVID-19 outbreak has been accompanied by the spread of rumors and depressing news. Herein, we develop a dynamic nested optimal control model of COVID-19 and its rumor outbreaks. The model aims to curb the epidemics by reducing the number of individuals infected with COVID-19 and reducing the number of rumor-spreaders while minimizing the cost associated with the control interventions. We use the modified approximation Karush–Kuhn–Tucker conditions with the Hamiltonian function to simplify the model before solving it using a genetic algorithm. The present model highlights three prevention measures that affect COVID-19 and its rumor outbreaks. One represents the interventions to curb the COVID-19 pandemic. The other two represent interventions to increase awareness, disseminate the correct information, and impose penalties on the spreaders of false rumors. The results emphasize the importance of interventions in curbing the spread of the COVID-19 pandemic and its associated rumor problems alike.},
  archive      = {J_PR},
  author       = {Ibrahim M. Hezam and Abdulkarem Almshnanah and Ahmed A. Mubarak and Amrit Das and Abdelaziz Foul and Adel Fahad Alrasheedi},
  doi          = {10.1016/j.patcog.2022.109186},
  journal      = {Pattern Recognition},
  pages        = {109186},
  shortjournal = {Pattern Recognition},
  title        = {COVID-19 and rumors: A dynamic nested optimal control model},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LPCL: Localized prominence contrastive learning for
self-supervised dense visual pre-training. <em>PR</em>, <em>135</em>,
109185. (<a href="https://doi.org/10.1016/j.patcog.2022.109185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised pre-training has attracted increasing attention given its promising performance in training backbone networks without using labels. By far, most methods focus on image classification with datasets containing iconic objects and simple background, e.g. ImageNet. However, these methods show sub-optimal performance for dense prediction tasks (e.g. object detection and scene parsing) when directly pre-training on datasets (e.g. PASCAL VOC and COCO) with multiple objects and cluttered backgrounds. Researchers explored self-supervised dense pre-training methods by adapting recent image pre-training methods. Nevertheless, they require a large number of negative samples and a long training time to reach reasonable performance. In this paper, we propose LPCL, a novel self-supervised representation learning method for dense predictions to settle these issues. To guide the instance information in multi-instance datasets, we define an online object patch selection module to select the local patches with the high possibility of containing instance area in the augmented views efficiently during learning. After obtaining the patches, we present a novel multi-level contrastive learning method considering the instance representation of global-level, local-level and position-level without using negative samples. We conduct extensive experiments with LPCL directly pre-trained on PASCAL VOC and COCO. For PASCAL VOC image classification task, our model achieves state-of-the-art 86.2\% 86.2\% accuracy pre-trained on COCO( + 9.7\% +9.7\% top-1 accuracy compared with baseline BYOL). On object detection, instance segmentation and semantic segmentation task, our proposed model also achieved competitive results compared with other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zihan Chen and Hongyuan Zhu and Hao Cheng and Siya Mi and Yu Zhang and Xin Geng},
  doi          = {10.1016/j.patcog.2022.109185},
  journal      = {Pattern Recognition},
  pages        = {109185},
  shortjournal = {Pattern Recognition},
  title        = {LPCL: Localized prominence contrastive learning for self-supervised dense visual pre-training},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining deep denoiser and low-rank priors for infrared
small target detection. <em>PR</em>, <em>135</em>, 109184. (<a
href="https://doi.org/10.1016/j.patcog.2022.109184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing low-rank methods have achieved good detection performance in uniform scenes, but they suffer from a high false alarm rate in complex noisy scenes. Therefore, it is important to improve the detection performance of low-rank models in noisy scenes. In this paper, we first formulate an implicit regularizer by plugging a denoising neural network (termed as deep denoiser), which can learn deep image priors from a large number of natural images. Then, we use the weighted sum of weighted tensor nuclear norm for more accurate background estimation. Finally, alternating direction multiplier method is used to solve the model under the plug-and-play framework. By integrating low-rank prior with deep denoiser prior, our model achieves higher accuracy. Experiments on different scenes demonstrate that our method achieves an improved performance in terms of visual effects and quantitative metrics. Specially, the overall accuracy of AUC value ( AU C OA AUCOA ) achieved by the proposed method on Sequences 1-6 are 1.24\% 1.24\% , 1.16\% 1.16\% , 0.63\% 0.63\% , 1.9\% 1.9\% , 0.82\% 0.82\% , 2.06\% 2.06\% higher than those achieved by the second top performing methods, respectively.},
  archive      = {J_PR},
  author       = {Ting Liu and Qian Yin and Jungang Yang and Yingqian Wang and Wei An},
  doi          = {10.1016/j.patcog.2022.109184},
  journal      = {Pattern Recognition},
  pages        = {109184},
  shortjournal = {Pattern Recognition},
  title        = {Combining deep denoiser and low-rank priors for infrared small target detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Large motion anime head animation using a cascade pose
transform network. <em>PR</em>, <em>135</em>, 109181. (<a
href="https://doi.org/10.1016/j.patcog.2022.109181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of talking head animation from a single image where a target anime talking head is generated to mimic the change of facial expression and head movement of source anime characters. Most existing methods focus on generating talking heads from real humans. However, few efforts have been made to create anime talking head. Compared with human head generation, the key challenges of anime head generation are: how to align the pose and facial expression of the target head with that of the source head without explicit facial landmarks. To address this, we propose CPTNetV2, a cascaded pose transform network that unifies face pose transformation and head pose transformation. At the core of CPTNetV2 is the implicit encoding of facial changes and head movement by a pose vector . Given the pose vector, we introduce a mask generator to animate facial expression ( e.g. , close eyes and open mouth) and a grid generator to simulate head movement, followed by a fusion module to generate talking heads. To tackle large displacement and improve the quality of generation, we further design a details inpainting module with pose vector decomposition to reduce the receptive field of network required for pose transformation. In particular, we collect an anime talking head dataset AniHead-2K that includes around 2000 anime characters with different face/head poses. Extensive experiments on AniHead-2K demonstrate that CPTNetV2 can achieve arbitrary pose transformation conditioned on the target pose vector and outperforms other state-of-the-art methods. We also verify the effectiveness of each module through ablative studies. Additional results show that CPTNetV2 has good generalization and is applicable to generate anime talking head even based on human videos. The dataset will be made available at: https://github.com/zhangjiale487/AniHead-2K .},
  archive      = {J_PR},
  author       = {Jiale Zhang and Chengxin Liu and Ke Xian and Zhiguo Cao},
  doi          = {10.1016/j.patcog.2022.109181},
  journal      = {Pattern Recognition},
  pages        = {109181},
  shortjournal = {Pattern Recognition},
  title        = {Large motion anime head animation using a cascade pose transform network},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). JRA-net: Joint representation attention network for
correspondence learning. <em>PR</em>, <em>135</em>, 109180. (<a
href="https://doi.org/10.1016/j.patcog.2022.109180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Joint Representation Attention Network (JRA-Net), an end-to-end network, to establish reliable correspondences for image pairs. The initial correspondences generated by the local feature descriptor usually suffer from heavy outliers, which makes the network unable to learn a powerful enough representation for distinguishing inliers and outliers. To this end, we design a novel attention mechanism. The proposed attention mechanism not only takes into account the correlations between global context and geometric information , but also introduces the joint representation of different scales to suppress trivial correspondences and highlight crucial correspondences. In addition, to improve the generalization ability of attention mechanism, we present an innovative weight function, to effectively adjust the importance of the attention mechanism in a learning manner. Finally, by combining the above components, the proposed JRA-Net is able to effectively infer the probabilities of correspondences being inliers. Empirical experiments on challenging datasets demonstrate the effectiveness and generalization of JRA-Net. We achieve remarkable improvements compared with the current state-of-the-art approaches on outlier rejection and relative pose estimation.},
  archive      = {J_PR},
  author       = {Ziwei Shi and Guobao Xiao and Linxin Zheng and Jiayi Ma and Riqing Chen},
  doi          = {10.1016/j.patcog.2022.109180},
  journal      = {Pattern Recognition},
  pages        = {109180},
  shortjournal = {Pattern Recognition},
  title        = {JRA-net: Joint representation attention network for correspondence learning},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Watching the BiG artifacts: Exposing DeepFake videos via
bi-granularity artifacts. <em>PR</em>, <em>135</em>, 109179. (<a
href="https://doi.org/10.1016/j.patcog.2022.109179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed significant advances in AI-based face manipulation techniques, known as DeepFakes , which has brought severe threats to society. Hence, an emerging and increasingly important research topic is how to detect DeepFake videos. In this paper, we propose a new DeepFake detection method based on Bi-granularity artifacts (BiG-Arts). We observe that the most of DeepFake video generation can commonly introduce bi-granularity artifacts: the intrinsic-granularity artifacts and extrinsic-granularity artifacts. Specifically, the intrinsic-granularity artifacts are caused by a common series of operations in model generation such as up-convolution or up-sampling, while the extrinsic-granularity artifacts are introduced by a common step in post-processing that blends the synthesized face to original video. To this end, we formulate DeepFake detection as multi-task learning problem, to simultaneously predict the intrinsic and extrinsic artifacts. Benefiting from the guidance of detecting Bi-granularity artifacts, our method is notably boosted in both within-datasets and cross-datasets scenarios. Extensive experiments are conducted on several DeepFake datasets, which corroborates the superiority of our method. Our method has been contributed as a part of the solution to achieve the Top-1 rank in DFGC competition ( https://competitions.codalab.org/competitions/29583 ).},
  archive      = {J_PR},
  author       = {Han Chen and Yuezun Li and Dongdong Lin and Bin Li and Junqiang Wu},
  doi          = {10.1016/j.patcog.2022.109179},
  journal      = {Pattern Recognition},
  pages        = {109179},
  shortjournal = {Pattern Recognition},
  title        = {Watching the BiG artifacts: Exposing DeepFake videos via bi-granularity artifacts},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised feature selection via neural networks and
self-expression with adaptive graph constraint. <em>PR</em>,
<em>135</em>, 109173. (<a
href="https://doi.org/10.1016/j.patcog.2022.109173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection (UFS), which selects the most important feature subset and eliminates the unnecessary information for the upcoming data analysis, is a significant problem in machine learning and has been explored for years. Most UFS methods map features into a pseudo label space by multiplying a projection matrix constrained with sparsity to learn the mapping from the features to the labels. However, the mapping relationship is usually not linear, and linear regression may result in a suboptimal selection. To address this issue, we propose a novel UFS method, called neural networks embedded self-expression (NNSE). NNSE replaces the linear regression of traditional spectral analysis methods with neural networks to learn the pseudo label space. Besides, we embed neural networks into the self-expression model to improve the representative ability by preserving the local structure with an adaptive graph regularization module. Then we propose an efficient alternative iterative algorithm to solve the proposed model. Experimental results on 8 public datasets show NNSE outperforms the other state-of-the-art methods. Moreover, experimental results are also presented to show the convergence of the proposed method. The source code is available at: https://github.com/misteru/NNSE .},
  archive      = {J_PR},
  author       = {Mengbo You and Aihong Yuan and Dongjian He and Xuelong Li},
  doi          = {10.1016/j.patcog.2022.109173},
  journal      = {Pattern Recognition},
  pages        = {109173},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised feature selection via neural networks and self-expression with adaptive graph constraint},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAPENet: Self-attention based prototype enhancement network
for few-shot learning. <em>PR</em>, <em>135</em>, 109170. (<a
href="https://doi.org/10.1016/j.patcog.2022.109170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning considers the problem of learning unseen categories given only a few labeled samples. As one of the most popular few-shot learning approaches, Prototypical Networks have received considerable attention owing to their simplicity and efficiency. However, a class prototype is typically obtained by averaging a few labeled samples belonging to the same class, which treats the samples as equally important and is thus prone to learning redundant features. Herein, we propose a self-attention based prototype enhancement network (SAPENet) to obtain a more representative prototype for each class. SAPENet utilizes multi-head self-attention mechanisms to selectively augment discriminative features in each sample feature map, and generates channel attention maps between intra-class sample features to attentively retain informative channel features for that class. The augmented feature maps and attention maps are finally fused to obtain representative class prototypes. Thereafter, a local descriptor-based metric module is employed to fully exploit the channel information of the prototypes by searching k k similar local descriptors of the prototype for each local descriptor in the unlabeled samples for classification. We performed experiments on multiple benchmark datasets: miniImageNet, tieredImageNet, and CUB-200-2011. The experimental results on these datasets show that SAPENet achieves a considerable improvement compared to Prototypical Networks and also outperforms related state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xilang Huang and Seon Han Choi},
  doi          = {10.1016/j.patcog.2022.109170},
  journal      = {Pattern Recognition},
  pages        = {109170},
  shortjournal = {Pattern Recognition},
  title        = {SAPENet: Self-attention based prototype enhancement network for few-shot learning},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust low tubal rank tensor completion via factor tensor
norm minimization. <em>PR</em>, <em>135</em>, 109169. (<a
href="https://doi.org/10.1016/j.patcog.2022.109169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has demonstrated that low tubal rank recovery based on tensor has received extensive attention. In this correspondence, we define tensor double nuclear norm and tensor Frobenius/nuclear hybrid norm to induce a surrogate for tensor tubal rank, and prove that they are equivalent to tensor Schatten- p p norm for p = 1 / 2 p=1/2 and p = 2 / 3 p=2/3 . Based on the definition, we propose two novel tractable tensor completion models called Double Nuclear norm regularized Tensor Completion (DNTC) and Frobenius/Nuclear hybrid norm regularized Tensor Completion (FNTC) by integrating these two norm minimization and factorization methods into a joint learning framework. Furthermore, we adopt invertible linear transforms to obtain low tubal rank tensors , which makes the model more flexible and effective. Two efficient algorithms are designed to solve the proposed tensor completion models by incorporating the convexity of the factor norms. Comprehensive experiments are conducted on synthetic and real datasets to achieve better results in comparison with some state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Wei Jiang and Jun Zhang and Changsheng Zhang and Lijun Wang and Heng Qi},
  doi          = {10.1016/j.patcog.2022.109169},
  journal      = {Pattern Recognition},
  pages        = {109169},
  shortjournal = {Pattern Recognition},
  title        = {Robust low tubal rank tensor completion via factor tensor norm minimization},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking local and global feature representation for dense
prediction. <em>PR</em>, <em>135</em>, 109168. (<a
href="https://doi.org/10.1016/j.patcog.2022.109168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although fully convolution networks (FCNs) have dominated dense prediction tasks ( e.g. , semantic segmentation , depth estimation and object detection) for decades, they are inherently limited in capturing long-range structured relationship with the layers of local kernels. While recent Transformer-based models have proven extremely successful in computer vision tasks by capturing global representation, they would deteriorate dense prediction results by over-smoothing the regions containing fine details ( e.g. , boundaries and small objects). To this end, we aim to provide an alternative perspective by rethinking local and global feature representation for the dense prediction task. Specifically, we deploy a Dual-Stream Convolution-Transformer architecture, called DSCT, by taking advantage of both the convolution and Transformer to learn a rich feature representation, combining with a task decoder to provide a powerful dense prediction model. DSCT extracts high resolution local feature representation from convolution layers and global feature representation from Transformer layers. With the local and global context modeled explicitly in every layer, the two streams can be combined with a decoder to perform task of semantic segmentation , monocular depth estimation or object detection. Extensive experiments show that DSCT can achieve superior performance on the three tasks above. For semantic segmentation, DSCT builds a new state of the art on Cityscapes validation set (83.31\% mIoU) with only 80,000 training iterations and appealing performance (49.27\% mIoU) on ADE20K validation set, outperforming most of the alternatives. For monocular depth estimation, our model achieves 2.423 RMSE on KITTI Eigen split, superior to most of the convolution or Transformer counterparts. For object detection, without using FPN, we can achieve 44.5\% AP b APb on COCO dataset when using Faster R-CNN, which is higher than Conformer .},
  archive      = {J_PR},
  author       = {Mohan Chen and Li Zhang and Rui Feng and Xiangyang Xue and Jianfeng Feng},
  doi          = {10.1016/j.patcog.2022.109168},
  journal      = {Pattern Recognition},
  pages        = {109168},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking local and global feature representation for dense prediction},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). M-CBN: Manifold constrained joint image dehazing and
super-resolution based on chord boosting network. <em>PR</em>,
<em>135</em>, 109166. (<a
href="https://doi.org/10.1016/j.patcog.2022.109166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a Manifold Constrained Chord Boosting Network (M-CBN), which incorporates the super-resolution principle to achieve image dehazing. M-CBN is a task-specific image restoration network that explicitly learns the mapping from Low-resolution (LR) hazy images to High-resolution (HR) haze-free images. Hence, we design a preliminary image degradation to imitate super-resolution training on hazy images. In M-CBN, a plug-and-play Cross-linked Dual Projection Module (CDPM) for skip connections is developed. In CDPM, back-projections for HR encoder features and LR decoder features are cross-linked for better recovery of spatial information, and a cross-resolution spatial attention is designed to enhance fusion features . Then, to boost the generation of image details and textures, we propose a Chord Residual Module (CRM), which can separately process High-frequency (HF) and Low-frequency (LF) features by progressive inner-frequency updating and dense inter-frequency cross-collaboration to enhance decoding features. Finally, a manifold constraint dual discriminator is established. The static discriminator explicitly constrains dehazed images in the expected manifold to unify the joint learning of image dehazing and super-resolution. And the dynamic discriminator implicitly optimizes the network by adversarial training . Extensive experiments on general, dense and non-homogeneous haze datasets and cross-domain dehazing tasks show the proposed M-CBN presents high-quality dehazed results with natural colors and clear details.},
  archive      = {J_PR},
  author       = {Pengyu Wang and Hongqing Zhu and Han Zhang and Nan Wang},
  doi          = {10.1016/j.patcog.2022.109166},
  journal      = {Pattern Recognition},
  pages        = {109166},
  shortjournal = {Pattern Recognition},
  title        = {M-CBN: Manifold constrained joint image dehazing and super-resolution based on chord boosting network},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimum bayesian thresholds for rebalanced classification
problems using class-switching ensembles. <em>PR</em>, <em>135</em>,
109158. (<a href="https://doi.org/10.1016/j.patcog.2022.109158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymmetric label switching is an effective and principled method for creating a diverse ensemble of learners for imbalanced classification problems. This technique can be combined with other rebalancing mechanisms, such as those based on cost policies or class proportion modifications. In this study, and under the Bayesian theory framework, we specify the optimal decision thresholds for the combination of these mechanisms. In addition, we propose using a gating network to aggregate the learners contributions as an additional mechanism to improve the overall performance of the system.},
  archive      = {J_PR},
  author       = {Aitor Gutiérrez-López and Francisco-Javier González-Serrano and Aníbal R. Figueiras-Vidal},
  doi          = {10.1016/j.patcog.2022.109158},
  journal      = {Pattern Recognition},
  pages        = {109158},
  shortjournal = {Pattern Recognition},
  title        = {Optimum bayesian thresholds for rebalanced classification problems using class-switching ensembles},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A discriminatively deep fusion approach with improved
conditional GAN (im-cGAN) for facial expression recognition.
<em>PR</em>, <em>135</em>, 109157. (<a
href="https://doi.org/10.1016/j.patcog.2022.109157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering most deep learning-based methods heavily depend on huge labels, it is still a challenging issue for facial expression recognition to extract discriminative features of training samples with limited labels. Given above, we propose a discriminatively deep fusion (DDF) approach based on an improved conditional generative adversarial network (im-cGAN) to learn abstract representation of facial expressions. First, we employ facial images with action units (AUs) to train the im-cGAN to generate more labeled expression samples. Subsequently, we utilize global features learned by the global-based module and the local features learned by the region-based module to obtain the fused feature representation. Finally, we design the discriminative loss function (D-loss) that expands the inter-class variations while minimizing the intra-class distances to enhance the discrimination of fused features. Experimental results on JAFFE, CK+, Oulu-CASIA, and KDEF datasets demonstrate the proposed approach is superior to some state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhe Sun and Hehao Zhang and Jiatong Bai and Mingyang Liu and Zhengping Hu},
  doi          = {10.1016/j.patcog.2022.109157},
  journal      = {Pattern Recognition},
  pages        = {109157},
  shortjournal = {Pattern Recognition},
  title        = {A discriminatively deep fusion approach with improved conditional GAN (im-cGAN) for facial expression recognition},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel multi-branch wavelet neural network for sparse
representation based object classification. <em>PR</em>, <em>135</em>,
109155. (<a href="https://doi.org/10.1016/j.patcog.2022.109155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in acquisition and display technologies have led to an enormous amount of visual data, which requires appropriate storage and management tools. One of the fundamental needs is the design of efficient image classification and recognition solutions. In this paper, we propose a wavelet neural network approach for sparse representation-based object classification. The proposed approach aims to exploit the advantages of sparse coding , multi-scale wavelet representation as well as neural networks . More precisely, a wavelet transform is firstly applied to the image datasets. The generated approximation and detail wavelet subbands are then fed into a multi-branch neural network architecture . This architecture produces multiple sparse codes that are efficiently combined during the classification stage. Extensive experiments, carried out on various types of standard object datasets , have shown the efficiency of the proposed method compared to the existing sparse coding and deep learning-based methods.},
  archive      = {J_PR},
  author       = {Tan-Sy Nguyen and Marie Luong and Mounir Kaaniche and Long H. Ngo and Azeddine Beghdadi},
  doi          = {10.1016/j.patcog.2022.109155},
  journal      = {Pattern Recognition},
  pages        = {109155},
  shortjournal = {Pattern Recognition},
  title        = {A novel multi-branch wavelet neural network for sparse representation based object classification},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prototype-guided feature learning for unsupervised domain
adaptation. <em>PR</em>, <em>135</em>, 109154. (<a
href="https://doi.org/10.1016/j.patcog.2022.109154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Domain Adaptation transfers knowledge from the source domain to the target domain. It makes remarkable progress in alleviating the label-shortage problem in machine learning . Existing methods focus on aligning the two domain distributions directly. However, due to domain discrepancy, there may be some samples in the source domain being unnecessary or even harmful to the target tasks. Avoiding transferring knowledge from these samples is crucial. Existing researches are limited in this area. To this end, we propose a new unsupervised domain adaptation approach named the prototype-guided feature learning . The proposed method contains three main innovations. Firstly, we propose to utilize the more representative source-domain samples, class prototypes, to learn a domain-invariant subspace with the target samples. Secondly, the modified nearest class prototype method is proposed to predict the target samples by exploiting the structural information of the target domain efficiently. Thirdly, a multi-stage label filtering method is proposed to alleviate the mislabeling problem during training. Extensive experiments manifest that our method is competitive compared to the current mainstream unsupervised domain adaptive methods.},
  archive      = {J_PR},
  author       = {Yongjie Du and Deyun Zhou and Yu Xie and Yu Lei and Jiao Shi},
  doi          = {10.1016/j.patcog.2022.109154},
  journal      = {Pattern Recognition},
  pages        = {109154},
  shortjournal = {Pattern Recognition},
  title        = {Prototype-guided feature learning for unsupervised domain adaptation},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast subspace clustering by learning projective block
diagonal representation. <em>PR</em>, <em>135</em>, 109152. (<a
href="https://doi.org/10.1016/j.patcog.2022.109152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Block Diagonal Representation (BDR) has attracted massive attention in subspace clustering, yet the high computational cost limits its widespread application. To address this issue, we propose a novel approach called Projective Block Diagonal Representation (PBDR), which rapidly pursues a representation matrix with the block diagonal structure . Firstly, an effective sampling strategy is utilized to select a small subset of the original large-scale data. Then, we learn a projection mapping to match the block diagonal representation matrix on the selected subset. After training, we employ the learned projection mapping to quickly generate the representation matrix with an ideal block diagonal structure for the original large-scale data. Additionally, we further extend the proposed PBDR model ( i . e . i.e. , PBDR ℓ 1 ℓ1 and PBDR * * ) by capturing the global or local structure of the data to enhance block diagonal coding capability. This paper also proves the effectiveness of the proposed model theoretically. Especially, this is the first work to directly learn a representation matrix with a block diagonal structure to handle the large-scale subspace clustering problem . Finally, experimental results on publicly available datasets show that our approaches achieve faster and more accurate clustering results compared to the state-of-the-art block diagonal-based subspace clustering approaches , which demonstrates its practical usefulness.},
  archive      = {J_PR},
  author       = {Yesong Xu and Shuo Chen and Jun Li and Chunyan Xu and Jian Yang},
  doi          = {10.1016/j.patcog.2022.109152},
  journal      = {Pattern Recognition},
  pages        = {109152},
  shortjournal = {Pattern Recognition},
  title        = {Fast subspace clustering by learning projective block diagonal representation},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Similarity learning with deep CRF for person
re-identification. <em>PR</em>, <em>135</em>, 109151. (<a
href="https://doi.org/10.1016/j.patcog.2022.109151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core of person re-identification (Re-ID) lies in robustly estimating similarities for each probe-gallery image pair. A common practice in existing works is to calculate the similarity of each image pair independently, ignoring relations between different probe-gallery pairs. In this paper, we present a deep learning conditional random field (Deep-CRF) graph to model group-wise similarities within a batch of images, and regard the Re-ID task as a CRF node labeling problem. Unlike the existing deep CRF based approach where the CRF inference is only involved in the training stage, our method intends to fully exploit the potential of CRF model, exhibiting inference consistency in both training and testing. Specifically, we design unary potentials for computing each probe-gallery similarity separately. To efficiently encode relationships between different probe-gallery pairs, pairwise potentials are built on an arbitrary node pair whose learning is achieved by a joint matching strategy using bidirectional LSTM . We pose the CRF inference as a RNN learning process, where unary and pairwise potentials are jointly optimized in an end-to-end manner. Extensive experiments on three large-scale person Re-ID datasets demonstrate the effectiveness of the proposed method. Our Deep-CRF achieves the best results compared with the previous graph-based deep learning approaches and substantially exceeds the existing deep CRF framework by 8\% in Rank1 accuracy on CUHK03 dataset. It also behaves competitive among the current state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jun Xiang and Ziyuan Huang and Xiaoping Jiang and Jianhua Hou},
  doi          = {10.1016/j.patcog.2022.109151},
  journal      = {Pattern Recognition},
  pages        = {109151},
  shortjournal = {Pattern Recognition},
  title        = {Similarity learning with deep CRF for person re-identification},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). G2DA: Geometry-guided dual-alignment learning for
RGB-infrared person re-identification. <em>PR</em>, <em>135</em>,
109150. (<a href="https://doi.org/10.1016/j.patcog.2022.109150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Infrared (IR) person re-identification aims to retrieve person-of-interest from heterogeneous cameras, easily suffering from large image modality discrepancy caused by different sensing wavelength ranges . Existing works usually minimize such discrepancy by aligning modality distribution of global features, while neglecting deep semantics and high-order structural relations within each class. This might render the misalignment between heterogeneous samples. In this paper, we propose Geometry-Guided Dual-Alignment (G 2 2 DA) learning, which yields better sample-level modality alignment for RGB-IR ReID by solving a graph-enabled distribution matching task that maximizes agreement between multi-modality node representations considering edge topology. Specifically, we covert RGB/IR images into semantic-aligned graphs, in which whole-part features and their similarities are represented by nodes and associated edges, respectively. To simultaneously implement node- and edge-wise alignment (Dual Alignment), we introduce Optimal Transport (OT) as a metric to calculate cross-modality human body matching scores. By minimizing the displacement cost across RGB-IR graphs, G 2 2 DA could learn not just modality-invariant but structurally consistent cross-modality representations. Furthermore, we advance a Message Fusion Attention (MFA) mechanism to adaptively smooth the node representations within each RGB/IR graph, effectively alleviating occlusions caused by other individuals and/or objects. Extensive experiments on two standard benchmark datasets validate the superiority of G 2 2 DA, yielding competitive performance against previous state-of-the-arts.},
  archive      = {J_PR},
  author       = {Lin Wan and Zongyuan Sun and Qianyan Jing and Yehansen Chen and Lijing Lu and Zhihang Li},
  doi          = {10.1016/j.patcog.2022.109150},
  journal      = {Pattern Recognition},
  pages        = {109150},
  shortjournal = {Pattern Recognition},
  title        = {G2DA: Geometry-guided dual-alignment learning for RGB-infrared person re-identification},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-driven daily activity recognition with enhanced
emergent modeling. <em>PR</em>, <em>135</em>, 109149. (<a
href="https://doi.org/10.1016/j.patcog.2022.109149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the population aging, elderly health monitoring is triggering more studies on daily activity recognition as the fundamental of ambient assisted living. It is remarkable that activity recognition remains difficulties including how to adequately extract feature structure and settle the issue of activity confusion. To address these challenges, we propose a novel activity modeling method under the emergent paradigm with marker-based stigmergy and the directed-weighted network with additional context-aware information. In the modeling process, stigmergy is first introduced to aggregate the context information at the low level for generating activity pheromone trails, and then the constructed stigmergic trails are represented in form of directed-weighted network with distinguishability of individual pheromone source corresponding to location. The potential advantage is that the robust trails with distinguishable individual initial positions are feasible to supplement user’s daily habits and thus both inter-class and intra-class distances can be kept at acceptable levels. Experiments on Aruba demonstrates that the proposed emergent modeling method can effectively deal with the problems of feature extraction and activity ambiguity and achieve good classification performance.},
  archive      = {J_PR},
  author       = {Zimin Xu and Guoli Wang and Xuemei Guo},
  doi          = {10.1016/j.patcog.2022.109149},
  journal      = {Pattern Recognition},
  pages        = {109149},
  shortjournal = {Pattern Recognition},
  title        = {Event-driven daily activity recognition with enhanced emergent modeling},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Person-specific face spoofing detection based on a siamese
network. <em>PR</em>, <em>135</em>, 109148. (<a
href="https://doi.org/10.1016/j.patcog.2022.109148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face spoofing detection is an essential prerequisite for face recognition applications. Previous face spoofing detection methods usually trained a binary classifier to classify the input face as a spoof face or a real face before face recognition, and client identity information was not utilized. In this paper, we propose a person-specific face spoofing detection method to employ client identity information for face spoofing detection. In our method, face spoofing is detected after face recognition rather than before face recognition; that is, the input face is recognized first, and the client identity is used to assist face spoofing detection. We train a deep Siamese network with image pairs. Each image pair consists of two real face images or one real and one spoof face image. The face images in each pair come from the same client. The deep Siamese network is trained by joint Bayesian loss together with contrastive loss and softmax loss. In testing, an input face image is recognized first, then the real face image of the identified client is retrieved, and an image pair is formed by the test face image and the retrieved real face image. The image pair is classified by the trained Siamese network to determine whether the input test image is a real face or not. The experimental results demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Mingtao Pei and Bin Yan and Huiling Hao and Meng Zhao},
  doi          = {10.1016/j.patcog.2022.109148},
  journal      = {Pattern Recognition},
  pages        = {109148},
  shortjournal = {Pattern Recognition},
  title        = {Person-specific face spoofing detection based on a siamese network},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face anti-spoofing using feature distilling and global
attention learning. <em>PR</em>, <em>135</em>, 109147. (<a
href="https://doi.org/10.1016/j.patcog.2022.109147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face anti-spoofing (FAS) is essential to assure the security of face recognition systems. Recently, some deep learning based FAS methods have achieved promising results under intra-dataset testing. However, they often fail in generalizing to unseen attacks due to the failure of extracting intrinsic features from face images. In this paper, we propose an end-to-end FAS method which consists of an anti-interference feature distillation module, a global spatial attention learning module and a pyramid binary mask supervision module. The deep features from the pretrained ResNet34 network are first distilled at multiple levels to capture intrinsic information via removing interference of features. Then, the multi-level distilled features are further refined by using a global spatial learning mechanism. Finally, the pyramid pixel-wise supervision is assembled to boost performance. Extensive experimental results on five benchmark datasets show the superior performance of our proposed method on intra-dataset testing and on cross-dataset testing.},
  archive      = {J_PR},
  author       = {Rui Huang and Xin Wang},
  doi          = {10.1016/j.patcog.2022.109147},
  journal      = {Pattern Recognition},
  pages        = {109147},
  shortjournal = {Pattern Recognition},
  title        = {Face anti-spoofing using feature distilling and global attention learning},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A learnable gradient operator for face presentation attack
detection. <em>PR</em>, <em>135</em>, 109146. (<a
href="https://doi.org/10.1016/j.patcog.2022.109146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face presentation attack detection (PAD) aims to protect the security of face recognition systems. The existing depth-supervised method using stacked vanilla convolutions cannot explicitly extract efficient fine-grained information (e.g., spatial gradient magnitude) for the distinction between bona fide and attack presentations. To address this issue, the Sobel operator has been demonstrated effective to acquire gradient magnitude due to the fast calculation capacity for high-frequency information. However, the Sobel operator is hand-crafted so cannot deal with complex textures. Differently, we develop a learnable gradient operator (LGO) to adaptively learn gradient information in a data-driven way, which is a generalization of existing gradient operators and effectively captures detailed discriminative clues from raw pixels. In parallel, we propose an adaptive gradient loss for better optimization. Extensive experimental comparisons with the state-of-the-art methods on the widely used Replay-Attack, CASIA-FASD, OULU-NPU, and SiW datasets demonstrate the superior performance of the proposed approach.},
  archive      = {J_PR},
  author       = {Caixun Wang and Bingyao Yu and Jie Zhou},
  doi          = {10.1016/j.patcog.2022.109146},
  journal      = {Pattern Recognition},
  pages        = {109146},
  shortjournal = {Pattern Recognition},
  title        = {A learnable gradient operator for face presentation attack detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring modality-shared appearance features and
modality-invariant relation features for cross-modality person
re-IDentification. <em>PR</em>, <em>135</em>, 109145. (<a
href="https://doi.org/10.1016/j.patcog.2022.109145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing cross-modality person Re-IDentification works rely on discriminative modality-shared features for reducing cross-modality variations and intra-modality variations. Despite their preliminary success, such modality-shared appearance features cannot capture enough modality-invariant discriminative information due to a massive discrepancy between RGB and IR images. To address this issue, on top of appearance features, we further capture the modality-invariant relations among different person parts (referred to as modality-invariant relation features), which help to identify persons with similar appearances but different body shapes. To this end, a Multi-level Two-streamed Modality-shared Feature Extraction (MTMFE) sub-network is designed, where the modality-shared appearance features and modality-invariant relation features are first extracted in a shared 2D feature space and a shared 3D feature space, respectively. The two features are then fused into the final modality-shared features such that both cross-modality variations and intra-modality variations can be reduced. Besides, a novel cross-modality center alignment loss is proposed to further reduce the cross-modality variations. Experimental results on several benchmark datasets demonstrate that our proposed method exceeds state-of-the-art algorithms by a wide margin.},
  archive      = {J_PR},
  author       = {Nianchang Huang and Jianan Liu and Yongjiang Luo and Qiang Zhang and Jungong Han},
  doi          = {10.1016/j.patcog.2022.109145},
  journal      = {Pattern Recognition},
  pages        = {109145},
  shortjournal = {Pattern Recognition},
  title        = {Exploring modality-shared appearance features and modality-invariant relation features for cross-modality person re-IDentification},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding compact and well-separated clusters: Clustering
using silhouette coefficients. <em>PR</em>, <em>135</em>, 109144. (<a
href="https://doi.org/10.1016/j.patcog.2022.109144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding compact and well-separated clusters in data sets is a challenging task. Most clustering algorithms try to minimize certain clustering objective functions. These functions usually reflect the intra-cluster similarity and inter-cluster dissimilarity. However, the use of such functions alone may not lead to the finding of well-separated and, in some cases, compact clusters . Therefore additional measures, called cluster validity indices, are used to estimate the true number of well-separated and compact clusters . Some of these indices are well-suited to be included into the optimization model of the clustering problem . Silhouette coefficients are among such indices. In this paper, a new optimization model of the clustering problem is developed where the clustering function is used as an objective and silhouette coefficients are used to formulate constraints. Then an algorithm, called CLUSCO (CLustering Using Silhouette COefficients), is designed to construct clusters incrementally. Three schemes are discussed to reduce the computational complexity of the algorithm. Its performance is evaluated using fourteen real-world data sets and compared with that of three state-of-the-art clustering algorithms . Results show that the CLUSCO is able to compute compact clusters which are significantly better separable in comparison with those obtained by other algorithms.},
  archive      = {J_PR},
  author       = {Adil M. Bagirov and Ramiz M. Aliguliyev and Nargiz Sultanova},
  doi          = {10.1016/j.patcog.2022.109144},
  journal      = {Pattern Recognition},
  pages        = {109144},
  shortjournal = {Pattern Recognition},
  title        = {Finding compact and well-separated clusters: Clustering using silhouette coefficients},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape robustness in style enhanced cross domain semantic
segmentation. <em>PR</em>, <em>135</em>, 109143. (<a
href="https://doi.org/10.1016/j.patcog.2022.109143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on domain adaptation method based on style transfer. Previous methods based on style transfer pay attention to the transformation of texture features between domains and maintain semantic consistency to the greatest extent. However, these methods have different effects on domain gaps in different types of categories. The categories with large texture difference and small structure difference can be improved better. For the categories with small texture difference and large structure difference, it causes negative transfer. In this paper, a shape robustness enhanced domain adaptive segmentation algorithm is proposed. Firstly, we adopt adjustable style transfer methods to enhance the style diversity of source domain images. Next, we differentiated different types of image features to weaken the negative transfer in the process of adversarial training . The results of this paper on general data sets GTA5 and SYNTHIA are better than other style transfer methods. Further experiments show that we improve the shape robustness of style enhancement method in domain adaptive segmentation task .},
  archive      = {J_PR},
  author       = {Siyu Zhu and Yingjie Tian},
  doi          = {10.1016/j.patcog.2022.109143},
  journal      = {Pattern Recognition},
  pages        = {109143},
  shortjournal = {Pattern Recognition},
  title        = {Shape robustness in style enhanced cross domain semantic segmentation},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PLFace: Progressive learning for face recognition with mask
bias. <em>PR</em>, <em>135</em>, 109142. (<a
href="https://doi.org/10.1016/j.patcog.2022.109142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outbreak of the COVID-19 coronavirus epidemic has promoted the development of masked face recognition (MFR). Nevertheless, the performance of regular face recognition is severely compromised when the MFR accuracy is blindly pursued. More facts indicate that MFR should be regarded as a mask bias of face recognition rather than an independent task. To mitigate mask bias, we propose a novel Progressive Learning Loss (PLFace) that achieves a progressive training strategy for deep face recognition to learn balanced performance for masked/mask-free faces recognition based on margin losses. Particularly, our PLFace adaptively adjusts the relative importance of masked and mask-free samples during different training stages. In the early stage of training, PLFace mainly learns the feature representations of mask-free samples. At this time, the regular sample embeddings shrink to the corresponding prototype, which represents the center of each class while being stored in the last linear layer. In the later stage of training, PLFace converges on mask-free samples and further focuses on masked samples until the masked sample embeddings are also gathered in the center of the class. The entire training process emphasizes the paradigm that normal samples shrink first and masked samples gather afterward. Extensive experimental results on popular regular and masked face benchmarks demonstrate that our proposed PLFace can effectively eliminate mask bias in face recognition. Compared to state-of-the-art competitors, PLFace significantly improves the accuracy of MFR while maintaining the performance of normal face recognition.},
  archive      = {J_PR},
  author       = {Baojin Huang and Zhongyuan Wang and Guangcheng Wang and Kui Jiang and Zhen Han and Tao Lu and Chao Liang},
  doi          = {10.1016/j.patcog.2022.109142},
  journal      = {Pattern Recognition},
  pages        = {109142},
  shortjournal = {Pattern Recognition},
  title        = {PLFace: Progressive learning for face recognition with mask bias},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time siamese multiple object tracker with enhanced
proposals. <em>PR</em>, <em>135</em>, 109141. (<a
href="https://doi.org/10.1016/j.patcog.2022.109141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining the identity of multiple objects in real-time video is a challenging task, as it is not always feasible to run a detector on every frame. Thus, motion estimation systems are often employed, which either do not scale well with the number of targets or produce features with limited semantic information. To solve the aforementioned problems and allow the tracking of dozens of arbitrary objects in real-time, we propose SiamMOTION. SiamMOTION includes a novel proposal engine that produces quality features through an attention mechanism and a region-of-interest extractor fed by an inertia module and powered by a feature pyramid network. Finally, the extracted tensors enter a comparison head that efficiently matches pairs of exemplars and search areas, generating quality predictions via a pairwise depthwise region proposal network and a multi-object penalization module. SiamMOTION has been validated on five public benchmarks, achieving leading performance against current state-of-the-art trackers. Code available at: https://www.github.com/lorenzovaquero/SiamMOTION},
  archive      = {J_PR},
  author       = {Lorenzo Vaquero and Víctor M. Brea and Manuel Mucientes},
  doi          = {10.1016/j.patcog.2022.109141},
  journal      = {Pattern Recognition},
  pages        = {109141},
  shortjournal = {Pattern Recognition},
  title        = {Real-time siamese multiple object tracker with enhanced proposals},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Iterative embedding distillation for open world vehicle
recognition. <em>PR</em>, <em>135</em>, 109140. (<a
href="https://doi.org/10.1016/j.patcog.2022.109140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle recognition poses a practical but challenging problem in many real-world surveillance applications. Since vehicle recognition is an open-set problem, it is a critical issue to learn a discriminative visual embedding space rather than a well-performing classifier. In this paper, we propose an iterative embedding distillation (IED) framework for open-set vehicle recognition. The soft target in knowledge distillation is utilized to establish the interclass relations from an instance level rather than a category level. Towards the open-set problem, we extend knowledge distillation to embedding distillation in an iterative learning way, in which three types of loss functions are studied to iteratively transfer the distributions of embeddings from the teacher network to the student network. To demonstrate the universal nature of IED, we implement the IED framework on two basic convolutional neural networks and verify it using the cross-dataset testing protocols without retraining or fine-tuning. Extensive experimental results show that IED obtains quite encouraging results and outperforms state-of-the-art methods on various large-scale vehicle recognition datasets including VeRi-776, Vehicle-ID, Vehicle-1M, VD1 and VD2.},
  archive      = {J_PR},
  author       = {Junxian Duan and Xiang Wu and Yibo Hu and Chaoyou Fu and Zi Wang and Ran He},
  doi          = {10.1016/j.patcog.2022.109140},
  journal      = {Pattern Recognition},
  pages        = {109140},
  shortjournal = {Pattern Recognition},
  title        = {Iterative embedding distillation for open world vehicle recognition},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M2RNet: Multi-modal and multi-scale refined network for
RGB-d salient object detection. <em>PR</em>, <em>135</em>, 109139. (<a
href="https://doi.org/10.1016/j.patcog.2022.109139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection is a fundamental topic in computer vision , which has promising application prospects. The previous methods based on RGB-D may potentially suffer from the incompatibility of multi-modal feature fusion and the insufficiency of multi-scale feature aggregation. To tackle these two dilemmas, we propose a novel multi-modal and multi-scale refined network (M 2 2 RNet). Specifically, three essential components are presented in this network. The nested dual attention module (NDAM) explicitly exploits the combined features of RGB and depth flows. The adjacent interactive aggregation module (AIAM) gradually integrates the neighbor features of high, middle and low levels. The joint hybrid optimization loss (JHOL) makes the predictions have a prominent outline. Extensive experiments quantitatively and qualitatively demonstrate that our method outperforms other state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Xian Fang and Mingfeng Jiang and Jinchao Zhu and Xiuli Shao and Hongpeng Wang},
  doi          = {10.1016/j.patcog.2022.109139},
  journal      = {Pattern Recognition},
  pages        = {109139},
  shortjournal = {Pattern Recognition},
  title        = {M2RNet: Multi-modal and multi-scale refined network for RGB-D salient object detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modal co-feedback cellular automata for RGB-t saliency
detection. <em>PR</em>, <em>135</em>, 109138. (<a
href="https://doi.org/10.1016/j.patcog.2022.109138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency cellular automata (CA), a temporally evolving model to efficiently locate salient object, has achieved great progress in saliency detection task. However, most the previous CA models originate from RGB data and are thus limited in some extreme scenes. Inspired by the observation that thermal infrared data (T) can overcome the limitation of RGB data themselves in some cases and RGB-T saliency detection has gained more and more attention. In this paper, we contribute a novel RGB-T saliency detection approach via Cross-modal Co-feedback Cellular Automata (C 3 3 A). Before this, we firstly present a novel weighted background-based map (WBM) to give each superpixel(image patch) an initial saliency value. Then C 3 3 A is proposed to improve the quality of the WBM. Specifically, it firstly establishes two complementary cellular automata (CA) mechanisms dependent on RGB and thermal infrared data, which respectively refine the WBM based on two different perspectives. To bridge RGB-T modalities, an iterative cross-modal co-feedback framework is contributed to optimize constantly their results. In other words, we regard the result of one modality(RGB or T)-induced CA as important feedback to update and optimize another modality(T or RGB)-induced CA during the iteration. Two modalities constantly pull out the useful and confident data to the opposite side, and so two CAs’ results are constantly refined until a stable state is generated, we then integrate the results of two modalities-induced CAs into the saliency map. Finally, a novel boundary-guided pixel-wise refinement (BPR) technology is proposed to further overcome the influence of inaccurate superpixel segmentation to the C 3 3 A and refine the saliency map generated by our C 3 3 A. For fairness, the proposed method is compared with other state-of-the-art methods on three RGB-T datasets, experimental results show the superiority of our model.},
  archive      = {J_PR},
  author       = {Yu Pang and Hao Wu and Chengdong Wu},
  doi          = {10.1016/j.patcog.2022.109138},
  journal      = {Pattern Recognition},
  pages        = {109138},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal co-feedback cellular automata for RGB-T saliency detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Classification of single-view object point clouds.
<em>PR</em>, <em>135</em>, 109137. (<a
href="https://doi.org/10.1016/j.patcog.2022.109137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object point cloud classification has drawn great research attention since the release of benchmarking datasets , such as the ModelNet and the ShapeNet. These benchmarks assume point clouds covering complete surfaces of object instances, for which plenty of high-performing methods have been developed. However, their settings deviate from those often met in practice, where, due to (self-)occlusion, a point cloud covering partial surface of an object is captured from an arbitrary view. We show in this paper that performance of existing point cloud classifiers drops drastically under the considered single-view, partial setting; the phenomenon is consistent with the observation that semantic category of a partial object surface is less ambiguous only when its distribution on the whole surface is clearly specified. To this end, we argue for a single-view, partial setting where supervised learning of object pose estimation should be accompanied with classification. Technically, we propose a baseline method of Pose-Accompanied Point cloud classification Network (PAPNet) ; built upon S E ( 3 ) SE(3) -equivariant convolutions, the PAPNet learns intermediate pose transformations for equivariant features defined on vector fields, which makes the subsequent classification easier (ideally) in the category-level, canonical pose. By adapting existing ModelNet40 and ScanNet datasets to the single-view, partial setting, experiment results can verify the necessity of object pose estimation and superiority of our PAPNet to existing classifiers.},
  archive      = {J_PR},
  author       = {Zelin Xu and Kangjun Liu and Ke Chen and Changxing Ding and Yaowei Wang and Kui Jia},
  doi          = {10.1016/j.patcog.2022.109137},
  journal      = {Pattern Recognition},
  pages        = {109137},
  shortjournal = {Pattern Recognition},
  title        = {Classification of single-view object point clouds},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequence patterns and HMM profiles to predict proteome wide
zinc finger motifs. <em>PR</em>, <em>135</em>, 109134. (<a
href="https://doi.org/10.1016/j.patcog.2022.109134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zinc finger (ZnF) is an important class of nucleic acid and protein recognition domain, wherein, zinc ion is the inorganic co-factor that forms a tetrahedral geometry with the cysteine and/or histidine residues. ZnF domains take up diverse architectures with different ZnF motifs and have a wide range of biological functions. Nonetheless, predicting the ZnF motif(s) from the sequence is quite challenging. To this end, 74 unique ZnF sequence patterns are collected from the literature and classified into 32 different classes. Since the shorter length of ZnF sequence patterns leads to inaccurate predictions, ZnF domain Pfam HMM profiles defined under 6 ZnF Pfam clans (215 HMM profiles) and a few undefined Pfam clans (74 HMM profiles) are used for the prediction. A web server, namely, ZnF-Prot ( https://project.iith.ac.in/znprot/ ) is developed which can predict the presence of 31 ZnF domains in a protein/proteome sequence of any organism. The use of ZnF sequence patterns and Pfam HMM profiles resulted in an accurate prediction of 610 test cases (taken randomly from 249 organisms) considered here. Additionally, the application of ZnF-Prot is demonstrated by considering Arabidopsis thaliana, Homo sapiens, Saccharomyces cerevisiae , Caenorhabditis elegans and Ciona intestinalis proteomes as test cases, wherein, 87–96\% of the predicted ZnF motifs are cross-validated.},
  archive      = {J_PR},
  author       = {Chakkarai Sathyaseelan and L Ponoop Prasad Patro and Thenmalarchelvi Rathinavelan},
  doi          = {10.1016/j.patcog.2022.109134},
  journal      = {Pattern Recognition},
  pages        = {109134},
  shortjournal = {Pattern Recognition},
  title        = {Sequence patterns and HMM profiles to predict proteome wide zinc finger motifs},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mix-ViT: Mixing attentive vision transformer for
ultra-fine-grained visual categorization. <em>PR</em>, <em>135</em>,
109131. (<a href="https://doi.org/10.1016/j.patcog.2022.109131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-fine-grained visual categorization (ultra-FGVC) moves down the taxonomy level to classify sub-granularity categories of fine-grained objects. This inevitably poses a challenge, i.e. , classifying highly similar objects with limited samples, which impedes the performance of recent advanced vision transformer methods. To that end, this paper introduces Mix-ViT, a novel mixing attentive vision transformer to address the above challenge towards improved ultra-FGVC. The core design is a self-supervised module that mixes the high-level sample tokens and learns to predict whether a token has been substituted after attentively substituting tokens. This drives the model to understand the contextual discriminative details among inter-class samples. Via incorporating such a self-supervised module, the network gains more knowledge from the intrinsic structure of input data and thus improves generalization capability with limited training sample. The proposed Mix-ViT achieves competitive performance on seven publicly available datasets, demonstrating the potential of vision transformer compared to CNN for the first time in addressing the challenging ultra-FGVC tasks. The code is available at https://github.com/Markin-Wang/MixViT},
  archive      = {J_PR},
  author       = {Xiaohan Yu and Jun Wang and Yang Zhao and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2022.109131},
  journal      = {Pattern Recognition},
  pages        = {109131},
  shortjournal = {Pattern Recognition},
  title        = {Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised adversarial learning via latent space for
hyperspectral target detection. <em>PR</em>, <em>135</em>, 109125. (<a
href="https://doi.org/10.1016/j.patcog.2022.109125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an advanced technique in remote sensing, hyperspectral target detection (HTD) is widely concerned in civilian and military applications. However, the limitation of prior and mixed pixels phenomenon makes HTD models sensitive to data corruption under various interference from environment. In this work, a novel two-stage detection framework based on adversarial learning is proposed, which extracts spectral features in latent space through background reconstruction under weak supervision. To address the issues of insufficient utilization of both background information and limited prior knowledge, the generative adversarial network (GAN) is applied to estimate background in a weakly supervised manner with target-based constraints and channel-wise attention, which produces the detection proposal in the first stage. Then, a refined result is produced in the second stage, in which the input data consists of the refined data and refined feature map based on previous detection proposal. To provide samples for weakly supervised learning (WSL), the pseudo datasets are produced by a coarse sample selection procedure, which makes full use of limited prior information. Finally, an exponential constrained nonlinear function is adopted to acquire pixel-level prediction via suppressing the background and combining features from different stages. Experiments on real hyperspectral images (HSIs) captured by different sensors at various scenes verify the effectiveness of the proposed framework.},
  archive      = {J_PR},
  author       = {Haonan Qin and Weiying Xie and Yunsong Li and Kai Jiang and Jie Lei and Qian Du},
  doi          = {10.1016/j.patcog.2022.109125},
  journal      = {Pattern Recognition},
  pages        = {109125},
  shortjournal = {Pattern Recognition},
  title        = {Weakly supervised adversarial learning via latent space for hyperspectral target detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Batch normalization embeddings for deep domain
generalization. <em>PR</em>, <em>135</em>, 109115. (<a
href="https://doi.org/10.1016/j.patcog.2022.109115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization aims at training machine learning models to perform robustly across different and unseen domains. Several methods train models from multiple datasets to extract domain-invariant features, hoping to generalize to unseen domains. Instead, first we explicitly train domain-dependent representations leveraging ad-hoc batch normalization layers to collect independent domain’s statistics. Then, we propose to use these statistics to map domains in a shared latent space, where membership to a domain is measured by means of a distance function. At test time, we project samples from an unknown domain into the same space and infer properties of their domain as a linear combination of the known ones. We apply the same mapping strategy at training and test time, learning both a latent representation and a powerful but lightweight ensemble model. We show a significant increase in classification accuracy over current state-of-the-art techniques on popular domain generalization benchmarks: PACS, Office-31 and Office-Caltech.},
  archive      = {J_PR},
  author       = {Mattia Segu and Alessio Tonioni and Federico Tombari},
  doi          = {10.1016/j.patcog.2022.109115},
  journal      = {Pattern Recognition},
  pages        = {109115},
  shortjournal = {Pattern Recognition},
  title        = {Batch normalization embeddings for deep domain generalization},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A label distribution manifold learning algorithm.
<em>PR</em>, <em>135</em>, 109112. (<a
href="https://doi.org/10.1016/j.patcog.2022.109112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel label distribution manifold learning (LDML) method for solving the multilabel distribution learning problem. First, using manifold learning, we extract the accurate and reduced-dimension features of the training data. Second, we estimate the unknown label distributions associated with the extracted reduced-dimension features based on multi-output kernel regression. Third, we use the extracted reduced-dimension features and their associated estimated label distributions to form an enhanced maximum entropy model, which enables us to accurately and efficiently estimate the unknown true label distributions for the training data. We refer to this algorithm as the LDML. We also propose to apply the tangent space alignment regression in the second stage, and the resulting algorithm is called the LDML-R. The LDML-R has better label distribution learning performance than the LDML but imposes higher complexity than the latter. We evaluate the proposed LDML and LDML-R algorithms on 15 real-world data sets with ground-truth label distributions, and the experimental results obtained show that our method has advantages in terms of learning accuracy compared to the latest multi-label distribution learning approaches. We also use another 10 real-world multi-class data sets, which do not have the ground-truth label distributions, to demonstrate the superior multilabel classification performance of our LDML-R algorithm over the existing state-of-the-art multi-label classification algorithms .},
  archive      = {J_PR},
  author       = {Chao Tan and Sheng Chen and Xin Geng and Genlin Ji},
  doi          = {10.1016/j.patcog.2022.109112},
  journal      = {Pattern Recognition},
  pages        = {109112},
  shortjournal = {Pattern Recognition},
  title        = {A label distribution manifold learning algorithm},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection confidence driven multi-object tracking to recover
reliable tracks from unreliable detections. <em>PR</em>, <em>135</em>,
109107. (<a href="https://doi.org/10.1016/j.patcog.2022.109107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) systems often rely on accurate object detectors; however, accurate detectors are not available in every application domain. We present Robust Confidence Tracking (RCT), an offline MOT algorithm designed for settings where detection quality is poor. Whereas prior methods simply threshold and discard detection confidence information, RCT relies on the exact detection confidence values to increase track quality throughout the entire tracking pipeline. This innovation (along with some simple and well-studied heuristics) allows RCT to achieve robust performance with minimal identity switches, even when provided with completely unfiltered detections. To compare trackers in the presence of unreliable detections, we present a challenging real-world underwater fish tracking dataset, FISHTRAC. In an large-scale evaluation across FISHTRAC, UA-DETRAC, and MOTChallenge data, RCT outperforms a wide variety of trackers, including deep trackers and more classic approaches. We have publically released our FISHTRAC codebase and training dataset at https://github.com/tmandel/fish-detrac , which will facilitate comparing trackers on understudied problems.},
  archive      = {J_PR},
  author       = {Travis Mandel and Mark Jimenez and Emily Risley and Taishi Nammoto and Rebekka Williams and Max Panoff and Meynard Ballesteros and Bobbie Suarez},
  doi          = {10.1016/j.patcog.2022.109107},
  journal      = {Pattern Recognition},
  pages        = {109107},
  shortjournal = {Pattern Recognition},
  title        = {Detection confidence driven multi-object tracking to recover reliable tracks from unreliable detections},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Center and scale prediction: Anchor-free approach for
pedestrian and face detection. <em>PR</em>, <em>135</em>, 109071. (<a
href="https://doi.org/10.1016/j.patcog.2022.109071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection traditionally requires sliding-window classifier in modern deep learning based approaches. However, both of these approaches requires tedious configurations in bounding boxes . Generally speaking, single-class object detection is to tell where the object is, and how big it is. Traditional methods combine the ”where” and ”how” subproblems into a single one through the overall judgement of various scales of bounding boxes . In view of this, we are interesting in whether the ”where” and ”how” subproblems can be separated into two independent subtasks to ease the problem definition and the difficulty of training. Accordingly, we provide a new perspective where detecting objects is approached as a high-level semantic feature detection task. Like edges, corners, blobs and other feature detectors, the proposed detector scans for feature points all over the image, for which the convolution is naturally suited. However, unlike these traditional low-level features, the proposed detector goes for a higher-level abstraction, that is, we are looking for central points where there are objects, and modern deep models are already capable of such a high-level semantic abstraction. Like blob detection, we also predict the scales of the central points, which is also a straightforward convolution. Therefore, in this paper, pedestrian and face detection is simplified as a straightforward center and scale prediction task through convolutions. This way, the proposed method enjoys an anchor-free setting, considerably reducing the difficulty in training configuration and hyper-parameter optimization. Though structurally simple, it presents competitive accuracy on several challenging benchmarks, including pedestrian detection and face detection. Furthermore, a cross-dataset evaluation is performed, demonstrating a superior generalization ability of the proposed method.},
  archive      = {J_PR},
  author       = {Wei Liu and Irtiza Hasan and Shengcai Liao},
  doi          = {10.1016/j.patcog.2022.109071},
  journal      = {Pattern Recognition},
  pages        = {109071},
  shortjournal = {Pattern Recognition},
  title        = {Center and scale prediction: Anchor-free approach for pedestrian and face detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised leaf segmentation under complex lighting
conditions. <em>PR</em>, <em>135</em>, 109021. (<a
href="https://doi.org/10.1016/j.patcog.2022.109021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential prerequisite task in image-based plant phenotyping, leaf segmentation has garnered increasing attention in recent years. While self-supervised learning is emerging as an effective alternative to various computer vision tasks, its adaptation for image-based plant phenotyping remains rather unexplored. In this work, we present a self-supervised leaf segmentation framework consisting of a self-supervised semantic segmentation model, a color-based leaf segmentation algorithm , and a self-supervised color correction model. The self-supervised semantic segmentation model groups the semantically similar pixels by iteratively referring to the self-contained information, allowing the pixels of the same semantic object to be jointly considered by the color-based leaf segmentation algorithm for identifying the leaf regions. Additionally, we propose to use a self-supervised color correction model for images taken under complex illumination conditions . Experimental results on datasets of different plant species demonstrate the potential of the proposed self-supervised framework in achieving effective and generalizable leaf segmentation.},
  archive      = {J_PR},
  author       = {Xufeng Lin and Chang-Tsun Li and Scott Adams and Abbas Z. Kouzani and Richard Jiang and Ligang He and Yongjian Hu and Michael Vernon and Egan Doeven and Lawrence Webb and Todd Mcclellan and Adam Guskich},
  doi          = {10.1016/j.patcog.2022.109021},
  journal      = {Pattern Recognition},
  pages        = {109021},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised leaf segmentation under complex lighting conditions},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video representation learning for temporal action detection
using global-local attention. <em>PR</em>, <em>134</em>, 109135. (<a
href="https://doi.org/10.1016/j.patcog.2022.109135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video representation is of significant importance for temporal action detection. The two sub-tasks of temporal action detection, i.e., action classification and action localization, have different requirements for video representation. Specifically, action classification requires video representations to be highly discriminative, so that action features and background features are as dissimilar as possible. For action localization, it is crucial to obtain information about the action itself and the surrounding context for accurate prediction of action boundaries. However, the previous methods failed to extract the optimal representations for the two sub-tasks, whose representations for both sub-tasks are obtained in a similar way. In this paper, a Global-Local Attention (GLA) mechanism is proposed to produce a more powerful video representation for temporal action detection without introducing additional parameters. The global attention mechanism predicts each action category by integrating features in the entire video that are similar to the action while suppressing other features, thus enhancing the discriminability of video representation during the training process. The local attention mechanism uses a Gaussian weighting function to integrate each action and its surrounding contextual information, thereby enabling precise localization of the action. The effectiveness of GLA is demonstrated on THUMOS’14 and ActivityNet-1.3 with a simple one-stage action detection network, achieving state-of-the-art performance among the methods using only RGB images as input. The inference speed of the proposed model reaches 1373 FPS on a single Nvidia Titan Xp GPU . The generalizability of GLA to other detection architectures is verified using R-C3D and Decouple-SSAD, both of which achieve consistent improvements. The experimental results demonstrate that designing representations with different properties for the two sub-tasks leads to better performance for temporal action detection compared to the representations obtained in a similar way.},
  archive      = {J_PR},
  author       = {Yiping Tang and Yang Zheng and Chen Wei and Kaitai Guo and Haihong Hu and Jimin Liang},
  doi          = {10.1016/j.patcog.2022.109135},
  journal      = {Pattern Recognition},
  pages        = {109135},
  shortjournal = {Pattern Recognition},
  title        = {Video representation learning for temporal action detection using global-local attention},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraphDPI: Partial label disambiguation by graph
representation learning via mutual information maximization.
<em>PR</em>, <em>134</em>, 109133. (<a
href="https://doi.org/10.1016/j.patcog.2022.109133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning (PLL) is a weakly supervised learning framework where each training instance is associated with more than one candidate label, and only one of them is the true label. Most of the existing PLL algorithms directly disambiguate the candidate labels according to the instance feature similarity, but fail to discover the latent semantic relationship over the entire dataset. In this paper, method GraphDPI, an innovative deep partial label disambiguation by graph representation via mutual information maximization , is proposed. This method can capture the semantic clusters with the most unique information in the latent space and automatically adapt to different feature distributions. Specifically, a new sampling method based on the graph is proposed to estimate mutual information, extending GCN to the field of weakly supervised learning. Therefore, the graph representation of the data can contain more distinguishing information to disambiguate candidate labels by maximizing the mutual information of the local graph representation and the global one. Furthermore, the triplet loss is introduced to fully exploit the relationship between instances and extract the latent embedding representation over the entire dataset. It thereby can make the model output as large as possible on the inter-class variation and as small as possible on the intra-class variation. Finally, the candidate labels can be disambiguated by the difference between semantic clusters. Experiments reveal the overwhelming performances of GraphDPI.},
  archive      = {J_PR},
  author       = {Jinfu Fan and Yang Yu and Linqing Huang and Zhongjie Wang},
  doi          = {10.1016/j.patcog.2022.109133},
  journal      = {Pattern Recognition},
  pages        = {109133},
  shortjournal = {Pattern Recognition},
  title        = {GraphDPI: Partial label disambiguation by graph representation learning via mutual information maximization},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data augmentation for univariate time series forecasting
with neural networks. <em>PR</em>, <em>134</em>, 109132. (<a
href="https://doi.org/10.1016/j.patcog.2022.109132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have been proven particularly accurate in univariate time series forecasting settings, requiring however a significant number of training samples to be effectively trained. In machine learning applications where available data are limited, data augmentation techniques have been successfully used to generate synthetic data that resemble and complement the original train set. Since the potential of data augmentation has been largely neglected in univariate time series forecasting, in this study we investigate nine data augmentation techniques, ranging from simple transformations and adjustments to sophisticated generative models and a novel upsampling approach. We empirically evaluate the impact of data augmentation on forecasting accuracy considering both shallow and deep feed-forward neural networks and time series data sets of different sizes from the M4 and the Tourism competitions. Our results suggest that certain data augmentation techniques that build on upsampling and time series combinations can improve forecasting performance, especially when deep networks are used. However, these improvements become less significant as the initial size of the train set increases.},
  archive      = {J_PR},
  author       = {Artemios-Anargyros Semenoglou and Evangelos Spiliotis and Vassilios Assimakopoulos},
  doi          = {10.1016/j.patcog.2022.109132},
  journal      = {Pattern Recognition},
  pages        = {109132},
  shortjournal = {Pattern Recognition},
  title        = {Data augmentation for univariate time series forecasting with neural networks},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning of global factors in deep generative
models. <em>PR</em>, <em>134</em>, 109130. (<a
href="https://doi.org/10.1016/j.patcog.2022.109130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel deep generative model based on non i.i.d. variational autoencoders that captures global dependencies among observations in a fully unsupervised fashion. In contrast to the recent semi-supervised alternatives for global modeling in deep generative models, our approach combines a mixture model in the local or data-dependent space and a global Gaussian latent variable, which lead us to obtain three particular insights. First, the induced latent global space captures interpretable disentangled representations with no user-defined regularization in the evidence lower bound (as in β β -VAE and its generalizations). Second, we show that the model performs domain alignment to find correlations and interpolate between different databases. Finally, we study the ability of the global space to discriminate between groups of observations with non-trivial underlying structures, such as face images with shared attributes or defined sequences of digits images.},
  archive      = {J_PR},
  author       = {Ignacio Peis and Pablo M. Olmos and Antonio Artés-Rodríguez},
  doi          = {10.1016/j.patcog.2022.109130},
  journal      = {Pattern Recognition},
  pages        = {109130},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised learning of global factors in deep generative models},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dirichlet process mixture of gaussian process functional
regressions and its variational EM algorithm. <em>PR</em>, <em>134</em>,
109129. (<a href="https://doi.org/10.1016/j.patcog.2022.109129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian Process Functional Regression (GPFR) is a powerful tool in functional data analysis . In practical applications, functional data may be generated from different signal sources, and a single GPFR is not flexible enough to accurately model the data. To tackle the heterogeneity problem, a finite mixture of Gaussian Process Functional Regressions (mix-GPFR) was suggested. However, the number of components in mix-GPFR needs to be specified a priori, which is difficult to determine in practice. In this paper, we propose a Dirichlet Process Mixture of Gaussian Process Functional Regressions (DPM-GPFR), in which there are potentially infinite many GPFR components dominated by a Dirichlet process. Thus, DPM-GPFR is far more flexible than a single GPFR, and sidestep the model selection problem in mix-GPFR. We further develop a fully Bayesian treatment for learning DPM-GPFR based on the Variational Expectation-Maximization (VEM) algorithm. Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Tao Li and Jinwen Ma},
  doi          = {10.1016/j.patcog.2022.109129},
  journal      = {Pattern Recognition},
  pages        = {109129},
  shortjournal = {Pattern Recognition},
  title        = {Dirichlet process mixture of gaussian process functional regressions and its variational EM algorithm},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Egocentric video co-summarization using transfer learning
and refined random walk on a constrained graph. <em>PR</em>,
<em>134</em>, 109128. (<a
href="https://doi.org/10.1016/j.patcog.2022.109128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of egocentric video co-summarization. We show how a shot level accurate summary can be obtained in a time-efficient manner using random walk on a constrained graph in transfer learned feature space with label refinement. While applying transfer learning , we propose a new loss function capturing egocentric characteristics in a pre-trained ResNet on the set of auxiliary egocentric videos. Transfer learning is used to generate i) an improved feature space and ii) a set of labels to be used as seeds for the test egocentric video. A complete weighted graph is created for a test video in the new transfer learned feature space with shots as the vertices. We derive two types of cluster label constraints in form of Must-Link (ML) and Cannot-link (CL) based on the similarity of the shots. ML constraints are used to prune the complete graph which is shown to result in substantial computational advantage, especially, for the long duration videos. We derive expressions for the number of vertices and edges for the ML-constrained graph and show that this graph remains connected. Random walk is applied to obtain labels of the unmarked shots in this new graph. CL constraints are applied to refine the cluster labels. Finally, shots closest to individual cluster centres are used to build the summary. Experiments on the short duration videos as in CoSum and TVSum datasets and long duration videos as in ADL and EPIC-Kitchens datasets clearly demonstrate the advantage of our solution over several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Abhimanyu Sahu and Ananda S. Chowdhury},
  doi          = {10.1016/j.patcog.2022.109128},
  journal      = {Pattern Recognition},
  pages        = {109128},
  shortjournal = {Pattern Recognition},
  title        = {Egocentric video co-summarization using transfer learning and refined random walk on a constrained graph},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain structure learning for visual data recognition.
<em>PR</em>, <em>134</em>, 109127. (<a
href="https://doi.org/10.1016/j.patcog.2022.109127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation methods are used to train an effect model by utilizing available knowledge from a labeled source domain for solving tasks in an unlabeled target domain. The most difficult challenge is determining methods to reduce distribution discrepancies and extract the largest number of domain-invariant features between the source and target domains to improve model performance. With the aim of minimizing the domain shift and maximizing domain-invariant feature extraction, we propose a cross-domain structure learning (CDSL) method for visual data recognition, which incorporates global distribution alignment and local discriminative structure preservation to capture the common, underlying features between domains. Specifically, we design a simple but effective classwise structure learning strategy with a specific compactness hierarchy to promote intraclass knowledge transfer and reduce the risk of negative transfer between domains. We also extend CDSL to different kinds of kernelization to address complex situations in the real world. Extensive experiments on several visual data benchmarks demonstrate the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Yuwu Lu and Xingping Luo and Jiajun Wen and Zhihui Lai and Xuelong Li},
  doi          = {10.1016/j.patcog.2022.109127},
  journal      = {Pattern Recognition},
  pages        = {109127},
  shortjournal = {Pattern Recognition},
  title        = {Cross-domain structure learning for visual data recognition},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep MinCut: Learning node embeddings by detecting
communities. <em>PR</em>, <em>134</em>, 109126. (<a
href="https://doi.org/10.1016/j.patcog.2022.109126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide insights into the graph structure, so that a separate clustering step is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss , which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.},
  archive      = {J_PR},
  author       = {Chi Thang Duong and Thanh Tam Nguyen and Trung-Dung Hoang and Hongzhi Yin and Matthias Weidlich and Quoc Viet Hung Nguyen},
  doi          = {10.1016/j.patcog.2022.109126},
  journal      = {Pattern Recognition},
  pages        = {109126},
  shortjournal = {Pattern Recognition},
  title        = {Deep MinCut: Learning node embeddings by detecting communities},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient point-set registration algorithm with dual
terms based on total least squares. <em>PR</em>, <em>134</em>, 109124.
(<a href="https://doi.org/10.1016/j.patcog.2022.109124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point set registration (PSR) is competitive with related techniques because it purposefully captures the overall structure between two point-set patterns. Typically, the point set registration problem can be divided into two sub-problems: (1) search point set correspondence (PSC); (2) estimate spatial transformation matrix (STM). Searching for the best PSC is a classical combinatorial explosion problem, and estimating the STM is a continuous space optimization problem. Also, two feature point sets detected by two low-quality images include point-position errors and often involve bilateral outliers composed of such feature points that cannot form a correspondence relationship. To address the above problems, we propose an efficient PSR algorithm with d ual (symmetrical) t erms based on the t otal l east s quares (DT-TLS), which can correct errors-in-variables and suppress multiple outliers. Meanwhile, the framework of soft decision-making is presented, and a TLS-based criterion is constructed to efficiently exploit the probability and global structures of two-point sets. Such TLS-based criterion with single row-orthonormal STM includes two interesting dual (symmetrical) terms that can be conveniently exploited to suppress bilateral outliers. The experimental results show that DT-TLS achieves better performance than the state-of-the-art algorithms in some multi-view computer vision tasks, indicating that our proposed algorithm is suitable for solving PSR problems.},
  archive      = {J_PR},
  author       = {Qing-Yan Chen and Da-Zheng Feng and Wei-Xing Zheng and Xiang-Wei Feng},
  doi          = {10.1016/j.patcog.2022.109124},
  journal      = {Pattern Recognition},
  pages        = {109124},
  shortjournal = {Pattern Recognition},
  title        = {An efficient point-set registration algorithm with dual terms based on total least squares},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Binary feature learning with local spectral context-aware
attention for classification of hyperspectral images. <em>PR</em>,
<em>134</em>, 109123. (<a
href="https://doi.org/10.1016/j.patcog.2022.109123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of hyperspectral images (HSIs) has achieved success in applications. For many approaches, features are directly extracted from whole spectral pixels, which can not well describe local characteristics. These methods are also susceptible to noise since each feature code is learned individually. Accordingly, a binary feature learning method with local spectral context-aware attention (BFLSC) is proposed for the classification. Specifically, for training samples, we first build the local spectrum models (LSMs) to describe local spectral properties , where each training sample is segmented into some parts and the difference between the central value and its neighborhoods is calculated in each part. Then, we construct the BFLSC model to learn a projection and binary features of training samples. In such model, the spectral context-awareness attention is established to collaboratively learn binary feature codes by enforcing one shift between 0/1 of each LSM, which enhances the robustness and stability of binary leaning. We also introduce the loss constraint, even distribution constraint, and variance constraint to reduce information loss and improve the quality of learned feature distribution. Additionally, an optimization scheme is designed to obtain the solution of the BFLSC model. Further, the learned binary features are added to train the support vector machine (SVM). For each testing sample, the LSMs are first extracted, and then mapped into binary features by the learned projection. The trained SVM is finally used for the mapped binary features to predict the label of the testing sample. Experimental results validate that our BFLSC realizes the better performance compared with some advanced approaches.},
  archive      = {J_PR},
  author       = {Changda Xing and Chaowei Duan and Zhisheng Wang and Meiling Wang},
  doi          = {10.1016/j.patcog.2022.109123},
  journal      = {Pattern Recognition},
  pages        = {109123},
  shortjournal = {Pattern Recognition},
  title        = {Binary feature learning with local spectral context-aware attention for classification of hyperspectral images},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel soft-coded error-correcting output codes algorithm.
<em>PR</em>, <em>134</em>, 109122. (<a
href="https://doi.org/10.1016/j.patcog.2022.109122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Error-Correcting Output Codes (ECOC) algorithms enable multiclass classification by reassigning multiple classes to the positive/negative group with the class reassignment schemes being recorded as binary/ternary hard-coded (HC) codematrices. Different classes tend to get diverse subordination degrees to the positive/negative group, providing clues to correct potential errors. However, the HC codematrices are unable to provide the information in the subordination degrees. In this paper, a Soft-Coded ECOC (SC-ECOC) scheme, namely, the Sequential Forward Floating Selection algorithm, is proposed by filling codematrices with real values instead of hard codes to improve classification performance. This algorithm divides multiple classes into two groups by maximizing the ratio of inter-group distance to intra-group distance. Then a new measure coverage is designed to evaluate the subordination degrees of different classes to both groups, which are set as the elements to form a codematrix. Furthermore, a self-adaptive strategy adjusts the value of each element to fit learners better. Experiments are carried out to verify the performance of our algorithm on various data sets, and results confirm that our algorithm can achieve more balanced results compared with the traditional HC ECOC algorithms. Besides, the values of soft codes correlate with the difficulty level of various classes to improve the multiclass classification ability.},
  archive      = {J_PR},
  author       = {Kun-Hong Liu and Jie Gao and Yong Xu and Kai-Jie Feng and Xiao-Na Ye and Sze-Teng Liong and Li-Yan Chen},
  doi          = {10.1016/j.patcog.2022.109122},
  journal      = {Pattern Recognition},
  pages        = {109122},
  shortjournal = {Pattern Recognition},
  title        = {A novel soft-coded error-correcting output codes algorithm},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScanMix: Learning from severe label noise via semantic
clustering and semi-supervised learning. <em>PR</em>, <em>134</em>,
109121. (<a href="https://doi.org/10.1016/j.patcog.2022.109121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new training algorithm , ScanMix, that explores semantic clustering and semi-supervised learning (SSL) to allow superior robustness to severe label noise and competitive robustness to non-severe label noise problems, in comparison to the state of the art (SOTA) methods. ScanMix is based on the expectation maximisation framework, where the E-step estimates the latent variable to cluster the training images based on their appearance and classification results , and the M-step optimises the SSL classification and learns effective feature representations via semantic clustering. We present a theoretical result that shows the correctness and convergence of ScanMix, and an empirical result that shows that ScanMix has SOTA results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In all benchmarks with severe label noise, our results are competitive to the current SOTA.},
  archive      = {J_PR},
  author       = {Ragav Sachdeva and Filipe Rolim Cordeiro and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
  doi          = {10.1016/j.patcog.2022.109121},
  journal      = {Pattern Recognition},
  pages        = {109121},
  shortjournal = {Pattern Recognition},
  title        = {ScanMix: Learning from severe label noise via semantic clustering and semi-supervised learning},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Robust sparse and low-redundancy multi-label feature
selection with dynamic local and global structure preservation.
<em>PR</em>, <em>134</em>, 109120. (<a
href="https://doi.org/10.1016/j.patcog.2022.109120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, joint feature selection and multi-label learning have received extensive attention as an open problem. However, there exist three general issues in previous multi-label feature selection methods. First of all, existing methods either consider local label correlations or global label correlations when they design multi-label feature selection methods, in fact, both two types of label correlations are significant for feature selection; second, previous methods use the low-quality graph to excavate local label correlations so that the results of these methods are under-performing; third, feature redundancy is ignored by most of the sparse learning methods. To overcome these challenges, we preserve global label correlations and dynamic local label correlations by preserving graph structure. Additionally, the l 2 , 1 l2,1 -norm and an inner product regularization term are imposed onto the objective function to preserve robust high row-sparsity and to select low redundant features. All the above terms are integrated into one learning framework, and then we utilize a simple yet effective scheme to optimize the framework. Experimental results demonstrate the classification superiority of the proposed method.},
  archive      = {J_PR},
  author       = {Yonghao Li and Liang Hu and Wanfu Gao},
  doi          = {10.1016/j.patcog.2022.109120},
  journal      = {Pattern Recognition},
  pages        = {109120},
  shortjournal = {Pattern Recognition},
  title        = {Robust sparse and low-redundancy multi-label feature selection with dynamic local and global structure preservation},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards deeper match for multi-view oriented multiple kernel
learning. <em>PR</em>, <em>134</em>, 109119. (<a
href="https://doi.org/10.1016/j.patcog.2022.109119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view representation learning aims to exploit the complementary information underlying multiple view data to enhance the expressive power of data representation. Given that kernels in multiple kernel learning naturally correspond to different views, previous shallow similarity learning models cannot fully capture the complex hierarchical information. This work presents an effective deeper match model for multi-view oriented kernel (DMMV) learning which brings a deeper insight into the kernel match for similarity based multi-view representation fusion. Specifically, we propose local deep view-specific self-kernel (LDSvK) by mimicking the deep neural networks to faithfully characterize the local similarity between view-specific samples. Thus, the representation capacity of each view can be saliently analyzed. We build the global deep multi-view fusion kernel (GDMvK) by learning deep fusion of LDSvKs to learn a comprehensive measurement of the cross-view similarity. Notably, the proposed learning framework of the deeper local information extraction and global deep multiple kernel fusion provides a robust way in fitting multi-view data, and yields better learning performance. Experimental results on several multi-view benchmark datasets well demonstrate the effectiveness of our DMMV over other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Wenzhu Yan and Yanmeng Li and Ming Yang},
  doi          = {10.1016/j.patcog.2022.109119},
  journal      = {Pattern Recognition},
  pages        = {109119},
  shortjournal = {Pattern Recognition},
  title        = {Towards deeper match for multi-view oriented multiple kernel learning},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clean affinity matrix learning with rank equality constraint
for multi-view subspace clustering. <em>PR</em>, <em>134</em>, 109118.
(<a href="https://doi.org/10.1016/j.patcog.2022.109118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing multi-view subspace clustering (MVSC) algorithm still has certain limitations. First, the affinity matrix obtained by them is not clean and robust enough since the original multi-view data usually contain noise. Second, they also have defects in exploring the consistency between views. To compensate for these two shortcomings, we propose a novel MVSC, i.e., clean affinity matrix learning with rank equality constraint (CAMR) for MVSC. By borrowing the idea from robust principal component analysis (RPCA), the representation matrix of each view obtained by low-rank representation (LRR) is first cleaned up to obtain a cleaner and more robust affinity matrix. In addition, the rank constraint is utilized to explore the same clustering properties between different views. An objective function solution method based on an augmented Lagrange multiplier (ALM) is designed and tested on four widely employed datasets to verify that CAMR has better clustering performance than certain state-of-the-art methods. We provide the code of CAMR at https://github.com/zhaojinbiao/CAMR.},
  archive      = {J_PR},
  author       = {Jinbiao Zhao and Gui-Fu Lu},
  doi          = {10.1016/j.patcog.2022.109118},
  journal      = {Pattern Recognition},
  pages        = {109118},
  shortjournal = {Pattern Recognition},
  title        = {Clean affinity matrix learning with rank equality constraint for multi-view subspace clustering},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Support subsets estimation for support vector machines
retraining. <em>PR</em>, <em>134</em>, 109117. (<a
href="https://doi.org/10.1016/j.patcog.2022.109117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of new data in previously trained Machine Learning ( ML ) models usually requires retraining and adjustment of the model. Support Vector Machine s ( SVM s) are widely used in ML because of their strong mathematical foundations and flexibility. However, SVM training is computationally expensive, both in time and memory. Hence, the training phase might be a limitation in problems where the model is updated regularly. As a solution, new methods for training and updating SVM s have been proposed in the past. In this paper, we introduce the concept of Support Subset and a new retraining methodology for SVM s. A Support Subset is a subset of the training set, such that retraining a ML model with this subset and the new data is equivalent to training with all the data. The performance of the proposal is evaluated in a variety of experiments on simulated and real datasets in terms of time, quality of the solution, resultant support vectors, and amount of employed data. The promising results provide a new research line for improving the effectiveness and adaptability of the proposed technique, including its generalization to other ML models.},
  archive      = {J_PR},
  author       = {Víctor Aceña and Isaac Martín de Diego and Rubén R． Fernández and Javier M． Moguerza},
  doi          = {10.1016/j.patcog.2022.109117},
  journal      = {Pattern Recognition},
  pages        = {109117},
  shortjournal = {Pattern Recognition},
  title        = {Support subsets estimation for support vector machines retraining},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-source change-point detection over local observation
models. <em>PR</em>, <em>134</em>, 109116. (<a
href="https://doi.org/10.1016/j.patcog.2022.109116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the problem of change-point detection (CPD) on high-dimensional, multi-source, and heterogeneous sequential data with missing values. We present a new CPD methodology based on local latent variable models and adaptive factorizations that enhances the fusion of multi-source observations with different statistical data-type and face the problem of high dimensionality . Our motivation comes from behavioral change detection in healthcare measured by smartphone monitored data and Electronic Health Records . Due to the high dimension of the observations and the differences in the relevance of each source information, other works fail in obtaining reliable estimates of the change-points location. This leads to methods that are not sensitive enough when dealing with interspersed changes of different intensity within the same sequence or partial missing components. Through the definition of local observation models (LOMs), we transfer the local CP information to homogeneous latent spaces and propose several factorizations that weight the contribution of each source to the global CPD. With the presented methods we demonstrate a reduction in both the detection delay and the number of not-detected CPs, together with robustness against the presence of missing values on a synthetic dataset . We illustrate its application on real-world data from a smartphone-based monitored study and add explainability on the degree of each source contributing to the detection.},
  archive      = {J_PR},
  author       = {Lorena Romero-Medrano and Antonio Artés-Rodríguez},
  doi          = {10.1016/j.patcog.2022.109116},
  journal      = {Pattern Recognition},
  pages        = {109116},
  shortjournal = {Pattern Recognition},
  title        = {Multi-source change-point detection over local observation models},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised adaptive kernel concept factorization.
<em>PR</em>, <em>134</em>, 109114. (<a
href="https://doi.org/10.1016/j.patcog.2022.109114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernelized concept factorization (KCF) has shown its advantage on handling data with nonlinear structures; however, the kernels involved in the existing KCF-based methods are empirically predefined, which may compromise the performance. In this paper, we propose semi-supervised adaptive kernel concept factorization (SAKCF), which integrates the data representation and kernel learning into a unified model to make the two learning processes adapt to each other. SAKCF extends traditional KCF in a semi-supervised manner, which encourages the high-dimensional representation to be consistent with both the limited supervisory and local geometric information . Besides, an alternating iterative algorithm is proposed to solve the resulting constrained optimization problem . Experimental results on six real-world data sets verify the effectiveness and advantages of our SAKCF over state-of-the-art methods when applied on the clustering task .},
  archive      = {J_PR},
  author       = {Wenhui Wu and Junhui Hou and Shiqi Wang and Sam Kwong and Yu Zhou},
  doi          = {10.1016/j.patcog.2022.109114},
  journal      = {Pattern Recognition},
  pages        = {109114},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised adaptive kernel concept factorization},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting group concept drift from multiple data streams.
<em>PR</em>, <em>134</em>, 109113. (<a
href="https://doi.org/10.1016/j.patcog.2022.109113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift may lead to a sharp downturn in the performance of streaming in data-based algorithms, caused by unforeseeable changes in the underlying distribution of data. In this paper, we are mainly concerned with concept drift across multiple data streams , and in situations where the drift of each data stream cannot be detected in time, due to slight underlying distribution drifts. We call this group concept drift. When compared to the detection of concept drift for a single data stream, the challenges of detecting group concept drift arise from three aspects: first, the training data become more complex; second, the underlying distribution becomes more complex; and third, the correlations between data streams become more complex. To address these challenges, the key idea of our method is to construct a distribution free test statistic, free from any underlying distribution in multiple data streams. Then, for streaming data, we design an online learning algorithm to obtain this test statistic, thereby determining the concept drift caused by the hypothesis test. The experiment evaluations with both synthetic and real-world datasets prove that our method can accurately detect concept drift from multiple data streams.},
  archive      = {J_PR},
  author       = {Hang Yu and Weixu Liu and Jie Lu and Yimin Wen and Xiangfeng Luo and Guangquan Zhang},
  doi          = {10.1016/j.patcog.2022.109113},
  journal      = {Pattern Recognition},
  pages        = {109113},
  shortjournal = {Pattern Recognition},
  title        = {Detecting group concept drift from multiple data streams},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified low-order information-theoretic feature selection
framework for multi-label learning. <em>PR</em>, <em>134</em>, 109111.
(<a href="https://doi.org/10.1016/j.patcog.2022.109111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approximation of low-order information-theoretic terms for feature selection approaches has achieved success in addressing high-dimensional multi-label data. However, three critical issues exist in such type of approaches: (1) existing approaches are devised based on single heuristic variable correlation assumption, which biases towards some specific scene; (2) high-order variable correlations are ignored by cumulative summation low-order information-theoretic terms; (3) abundant approaches confuse researchers to devise and utilize appropriate approaches. To address these issues, two types of probability distribution assumption in terms of candidate features and labels are derived based on low-order variable correlations. Afterwords, clearing up all information-theoretic terms, we propose a unified feature selection framework including three low-order information-theoretic terms for multi-label learning named Selected Terms of Feature Selection (STFS). STFS contains high-order variable correlations in the form of low-order information-theoretic terms. Furthermore, many previous multi-label feature selection approaches can be reduced to special forms of STFS. Finally, extensive experiments conducted on twelve benchmark data sets in comparison to seven state-of-the-art approaches demonstrate the classification superiority of STFS.},
  archive      = {J_PR},
  author       = {Wanfu Gao and Pingting Hao and Yang Wu and Ping Zhang},
  doi          = {10.1016/j.patcog.2022.109111},
  journal      = {Pattern Recognition},
  pages        = {109111},
  shortjournal = {Pattern Recognition},
  title        = {A unified low-order information-theoretic feature selection framework for multi-label learning},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatically detecting human-object interaction by an
instance part-level attention deep framework. <em>PR</em>, <em>134</em>,
109110. (<a href="https://doi.org/10.1016/j.patcog.2022.109110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatically detecting human-object interactions (HOIs) from an image is a very important but challenging task in computer vision . One of the significant problems in HOI detection is that similar human-object interactions are difficult to distinguish. Recently, many instance-centric HOI detection schemes, based on appearance features and coarse spatial information, have been proposed. These methods, however, lack the capacity of capturing and analyzing the fine-grained context between human poses and object parts, which plays a crucial role in HOI detection. To address these problems, we propose a novel instance part-level attention deep framework for HOI detection. Specifically, our approach consists of a human/object-part detection phase and an HOI detection phase. In the former phase, a part-level visual pattern estimation model is designed for capturing the fine-grained human body parts and object parts. In the latter phase, a self-attention-based deep network is proposed to learn the visual composite around the human-object pair that implicitly expresses the consistent spatial, scale, co-occurrence, and viewpoint relationships among human body parts and object parts across images, which are effective for predicting HOI. To the best of our knowledge, we are the first to propose a framework where the fine-grained part-level mutual context of a human-object pair is extracted to improve HOI detection. By comparing our approach with state-of-the-art HOI detection methods on benchmark datasets, we demonstrated that our proposed framework outperformed the existing HOI detection methods, such as significantly improving the performance of part-level visual pattern estimation, HOI detection, and the quality of the self-attention-based deep network structure.},
  archive      = {J_PR},
  author       = {Lin Bai and Fenglian Chen and Yang Tian},
  doi          = {10.1016/j.patcog.2022.109110},
  journal      = {Pattern Recognition},
  pages        = {109110},
  shortjournal = {Pattern Recognition},
  title        = {Automatically detecting human-object interaction by an instance part-level attention deep framework},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards open-set text recognition via label-to-prototype
learning. <em>PR</em>, <em>134</em>, 109109. (<a
href="https://doi.org/10.1016/j.patcog.2022.109109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition is a popular research topic which is also extensively utilized in the industry. Although many methods have achieved satisfactory performance for the close-set text recognition challenges, these methods lose feasibility in open-set scenarios, where collecting data or retraining models for novel characters could yield a high cost. For example, annotating samples for foreign languages can be expensive, whereas retraining the model each time when a “novel” character is discovered from historical documents costs both time and resources. In this paper, we introduce and formulate a new open-set text recognition task which demands the capability to spot and recognize novel characters without retraining. A label-to-prototype learning framework is also proposed as a baseline for the new task. Specifically, the framework introduces a generalizable label-to-prototype mapping function to build prototypes (class centers) for both seen and unseen classes. An open-set predictor is then utilized to recognize or reject samples according to the prototypes. The implementation of rejection capability over out-of-set characters allows automatic spotting of unknown characters in the incoming data stream. Extensive experiments show that our method achieves promising performance on a variety of zero-shot, close-set, and open-set text recognition datasets.},
  archive      = {J_PR},
  author       = {Chang Liu and Chun Yang and Hai-Bo Qin and Xiaobin Zhu and Cheng-Lin Liu and Xu-Cheng Yin},
  doi          = {10.1016/j.patcog.2022.109109},
  journal      = {Pattern Recognition},
  pages        = {109109},
  shortjournal = {Pattern Recognition},
  title        = {Towards open-set text recognition via label-to-prototype learning},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PFEMed: Few-shot medical image classification using prior
guided feature enhancement. <em>PR</em>, <em>134</em>, 109108. (<a
href="https://doi.org/10.1016/j.patcog.2022.109108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have recently demonstrated outstanding performance on general image classification tasks. As optimization of these methods is dependent on a large amount of labeled data, their application in medical image classification is limited. To address this issue, we propose PFEMed, a novel few-shot classification method for medical images. To extract general and specific features from medical images, this method employs a dual-encoder structure, that is, one encoder with fixed weights pre-trained on public image classification datasets and another encoder trained on the target medical dataset. In addition, we introduce a novel prior-guided Variational Autoencoder (VAE) module to enhance the robustness of the target feature, which is the concatenation of the general and specific features. Then, we match the target features extracted from both the support and query medical image samples and predict the category attribution of the query examples. Extensive experiments on several publicly available medical image datasets show that our method outperforms current state-of-the-art few-shot methods by a wide margin, particularly outperforming MetaMed on the Pap smear dataset by over 2.63\%.},
  archive      = {J_PR},
  author       = {Zhiyong Dai and Jianjun Yi and Lei Yan and Qingwen Xu and Liang Hu and Qi Zhang and Jiahui Li and Guoqiang Wang},
  doi          = {10.1016/j.patcog.2022.109108},
  journal      = {Pattern Recognition},
  pages        = {109108},
  shortjournal = {Pattern Recognition},
  title        = {PFEMed: Few-shot medical image classification using prior guided feature enhancement},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale enhanced graph convolutional network for mild
cognitive impairment detection. <em>PR</em>, <em>134</em>, 109106. (<a
href="https://doi.org/10.1016/j.patcog.2022.109106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an early stage of Alzheimer&#39;s disease (AD), mild cognitive impairment (MCI) is able to be detected by analyzing the brain connectivity networks. For this reason, we devise a new framework via multi-scale enhanced graph convolutional network (MSE-GCN) for MCI detection, which integrates the structural and functional information from the diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (R-fMRI), respectively. Specifically, both information in the brain connective networks is first integrated based on the local weighted clustering coefficients (LWCC), which is concatenated as the feature vector for representing a population graph&#39;s vertice. Simultaneously, the gender and age information in each subject are integrated with the structural and functional features to construct a sparse graph. Then, various parallel graph convolutional network (GCN) layers with multiple inputs are designed from the embedding from random walk embeddings in the GCN to identify the essential MCI graph information. Finally, all GCN layers’ outputs are concatenated via the fully connection layer to perform disease detection. The experimental results on the public Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database show that our method is promising to detect MCI and superior to other competing algorithms, with a mean classification accuracy of 90.39\% in the detection tasks.},
  archive      = {J_PR},
  author       = {Baiying Lei and Yun Zhu and Shuangzhi Yu and Huoyou Hu and Yanwu Xu and Guanghui Yue and Tianfu Wang and Cheng Zhao and Shaobin Chen and Peng Yang and Xuegang Song and Xiaohua Xiao and Shuqiang Wang},
  doi          = {10.1016/j.patcog.2022.109106},
  journal      = {Pattern Recognition},
  pages        = {109106},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale enhanced graph convolutional network for mild cognitive impairment detection},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A consistent and flexible framework for deep matrix
factorizations. <em>PR</em>, <em>134</em>, 109102. (<a
href="https://doi.org/10.1016/j.patcog.2022.109102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep matrix factorizations (deep MFs) are recent unsupervised data mining techniques inspired by constrained low-rank approximations. They aim to extract complex hierarchies of features within high-dimensional datasets. Most of the loss functions proposed in the literature to evaluate the quality of deep MF models and the underlying optimization frameworks are not consistent because different losses are used at different layers. In this paper, we introduce two meaningful loss functions for deep MF and present a generic framework to solve the corresponding optimization problems . We illustrate the effectiveness of this approach through the integration of various constraints and regularizations , such as sparsity, nonnegativity and minimum-volume. The models are successfully applied on both synthetic and real data, namely for hyperspectral unmixing and extraction of facial features .},
  archive      = {J_PR},
  author       = {Pierre De Handschutter and Nicolas Gillis},
  doi          = {10.1016/j.patcog.2022.109102},
  journal      = {Pattern Recognition},
  pages        = {109102},
  shortjournal = {Pattern Recognition},
  title        = {A consistent and flexible framework for deep matrix factorizations},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dense light field reconstruction algorithm for
four-dimensional optical flow constraint equation. <em>PR</em>,
<em>134</em>, 109101. (<a
href="https://doi.org/10.1016/j.patcog.2022.109101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense light field sampling is an important basis for refocusing, depth estimation and 3-D imaging. It is difficult to obtain high resolution dense light field with a large-scale camera array and expensive equipment. At the same time, the current storage devices and transmission bandwidth also limit this technology&#39;s post-processing and application. In order to effectively reconstruct the angle domain of the light field based on the sparse light field data, this paper analyzes the correlation and constraint relationship between the optical flow field and the motion field of multi-view images in the same scene, extends the traditional optical flow constraint equation of two-dimensional imaging to the optical flow constraint equation of four-dimensional light field, and establishes an effective mathematical model. The coordinate position of the original pixel in the new angle image is determined by coordinate search, and its intensity is reconstructed. The experimental results of multi-scene dense reconstruction show that the proposed method can reconstruct the texture, shadow and color information in the light field of a long-baseline scene with high quality. The quantitative evaluation results show that the algorithm can be applied to dense light field reconstruction of complex scenes. The algorithm in this paper is only suitable for the case of optical flow constraint in a linear light field, and the follow-up research will focus on the case of nonlinear optical flow constraint.},
  archive      = {J_PR},
  author       = {Jian Liu and Na Song and Zhengde Xia and Bin Liu and Jinxiao Pan and Abdul Ghaffar and Jianbin Ren and Ming Yang},
  doi          = {10.1016/j.patcog.2022.109101},
  journal      = {Pattern Recognition},
  pages        = {109101},
  shortjournal = {Pattern Recognition},
  title        = {A dense light field reconstruction algorithm for four-dimensional optical flow constraint equation},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous robust matching pursuit for multi-view
learning. <em>PR</em>, <em>134</em>, 109100. (<a
href="https://doi.org/10.1016/j.patcog.2022.109100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint sparse representation (JSR) has attracted massive attention with many successful applications in pattern recognition recently. In this paper, we propose a novel robust multi-view JSR method referred to as Simultaneous Robust Matching Pursuit (SRMP) based on the outlier-resistant M-estimator originating from robust statistics. Because of the complexity of the objective function, we design an efficient optimization algorithm to implement SRMP based on the half-quadratic theory. In addition, we have also extended the proposed method for the problems of multi-view subspace clustering and multi-view pattern classification, respectively. The experimental results corroborate the efficacy and robustness of SRMP for multi-view data recovery, subspace clustering and classification.},
  archive      = {J_PR},
  author       = {Yulong Wang and Kit Ian Kou and Hong Chen and Yuan Yan Tang and Luoqing Li},
  doi          = {10.1016/j.patcog.2022.109100},
  journal      = {Pattern Recognition},
  pages        = {109100},
  shortjournal = {Pattern Recognition},
  title        = {Simultaneous robust matching pursuit for multi-view learning},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UDNet: Uncertainty-aware deep network for salient object
detection. <em>PR</em>, <em>134</em>, 109099. (<a
href="https://doi.org/10.1016/j.patcog.2022.109099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing deep learning based salient object detection (SOD) models adopt multi-level feature fusion strategies, and have achieved remarkable progress. However, current SOD models still suffer from the uncertainty dilemma in predicting salient probabilities of the pixels surrounding the contour of salient objects. To solve this issue, we propose a novel uncertainty-aware SOD model, where multiple supervision signals, i.e ., internal contour uncertainty map, saliency map and external contour uncertainty map, are used to guide the network to not only focus on the pixels in the salient object but also shift its partial attention to the pixels surrounding the contour of salient objects. Furthermore, we introduce a new feature interaction module to aggregate internal contour uncertainty features, saliency features and external contour uncertainty features in the decoding stage, aiming to enhance the model’s ability in dealing with the “uncertain” pixels. Extensive experiments on four public benchmark datasets demonstrate the superiority of the proposed method over the existing state-of-the-art SOD methods. Furthermore, the proposed method shows better attribute-based performance on the SOC dataset, suggesting that the proposed model can also handle challenging scenarios in SOD.},
  archive      = {J_PR},
  author       = {Yuming Fang and Haiyan Zhang and Jiebin Yan and Wenhui Jiang and Yang Liu},
  doi          = {10.1016/j.patcog.2022.109099},
  journal      = {Pattern Recognition},
  pages        = {109099},
  shortjournal = {Pattern Recognition},
  title        = {UDNet: Uncertainty-aware deep network for salient object detection},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). AugFCOS: Augmented fully convolutional one-stage object
detection network. <em>PR</em>, <em>134</em>, 109098. (<a
href="https://doi.org/10.1016/j.patcog.2022.109098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a pioneering work of introducing the idea of full convolutional network into the field of object detection, the fully convolutional one-stage object detection network (FCOS) has the advantage of excellent performance with low memory overhead. However, there are certain problems with FCOS that merit more research: the centerness quality assessment loss does not decrease during the late training stage, and its adaptive training sample selection (ATSS) relies heavily on the hyperparameter. To solve the aforementioned problems, we propose a novel object detection network, named augmented fully convolutional one-stage object detection network (AugFCOS). First of all, we propose an improved dynamic optimization loss (DOL) to mitigate the impact of the original centerness loss not decreasing. Then, a Robust Training Sample Selection (RTSS) is proposed to get rid of the dependence of hyper-parameter in ATSS of FCOS. Finally, a novel mixed attention feature pyramid network (MAFPN) is presented to enhance the multi-scale representation ability of feature pyramid network (FPN) and further improve the ability of multi-scale detection. The experimental results on MS COCO demonstrate the effectiveness of our proposed AugFCOS, where AugFCOS achieves approximate 2.0\% to 2.9\% increase when compared with ATSS and FCOS.},
  archive      = {J_PR},
  author       = {Xiuwei Zhang and Wei Guo and Yinghui Xing and Wenna Wang and Hanlin Yin and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.109098},
  journal      = {Pattern Recognition},
  pages        = {109098},
  shortjournal = {Pattern Recognition},
  title        = {AugFCOS: Augmented fully convolutional one-stage object detection network},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing 3D-2D representations for convolution occupancy
networks. <em>PR</em>, <em>134</em>, 109097. (<a
href="https://doi.org/10.1016/j.patcog.2022.109097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Occupancy Networks (ConvONet) have gained popularity in object-level and scene-level reconstruction. However, how to better represent the 3D features for ConvONet remains an open question. In this paper, we propose to improve the representation for ConvONet by enhancing both 3D positional information and 3D-2D correlations. Considering that position information acts as the fundamental component of a 3D shape, we propose a Position-Aware Transformer (PAT) architecture that incorporates the Adaptive Multi-Scale Position Encoding (AMSPE) into the self-attention computation. By leveraging both global and local position aggregations in a multi-level manner, AMSPE enables better representations of both coarse and fine structures of the 3D shape. Meanwhile, since projecting 3D features to 2D planes for convolution inevitably introduces ambiguous or noisy representations, we propose a 3D Correlation-Guided Enhancement (CGE) network to bridge the gap between 3D and 2D shape representations. Specifically, we leverage the projected 3D correlations from PAT as the structural guidance, then compute the 3D Correlation-Guided Attentions (CGAs) to enhance the most representative features in the 2D space. In this way, the proposed architecture preserves the most informative structural representations while alleviating the impact of the mis-projected and noisy features. Experiments on ShapeNet and indoor scene dataset demonstrate the superiority of our method. Both quantitative and qualitative experiments show that our method achieves state-of-the-art performance for implicit-based 3D reconstruction .},
  archive      = {J_PR},
  author       = {Qing Mao and Rui Li and Yu Zhu and Jinqiu Sun and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.109097},
  journal      = {Pattern Recognition},
  pages        = {109097},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing 3D-2D representations for convolution occupancy networks},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IDA: Improving distribution analysis for reducing data
complexity and dimensionality in hyperspectral images. <em>PR</em>,
<em>134</em>, 109096. (<a
href="https://doi.org/10.1016/j.patcog.2022.109096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) are known for their high dimensionality and wide spectral bands that increase redundant information and complicate classification. Outliers and mixed data are common problems in HSIs. Thus, preprocessing methods are essential in enhancing and reducing data complexity, redundant information, and the number of bands. This study introduces a novel feature reduction method (FRM) called improving distribution analysis (IDA). IDA works to increase the correlation between related data, decrease the distance between big and small data, and correct each value&#39;s location to be inside its group range. In IDA, the input data passes through three stages. Getting rid of outliers and improving data correlation is the first step. The second stage involves increasing the variance. The third is to simplify the data and normalize the distribution. IDA is compared with four popular FRMs in four available HSIs. It is also tested and evaluated in various classification models, including spatial, spectral, and spectral-spatial models. The experimental results demonstrate that IDA performs admirably in enhancing data distribution, reducing complexity, and accelerating performance.},
  archive      = {J_PR},
  author       = {Dalal AL-Alimi and Mohammed A.A. Al-qaness and Zhihua Cai and Eman Ahmed Alawamy},
  doi          = {10.1016/j.patcog.2022.109096},
  journal      = {Pattern Recognition},
  pages        = {109096},
  shortjournal = {Pattern Recognition},
  title        = {IDA: Improving distribution analysis for reducing data complexity and dimensionality in hyperspectral images},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BehavePassDB: Public database for mobile behavioral
biometrics and benchmark evaluation. <em>PR</em>, <em>134</em>, 109089.
(<a href="https://doi.org/10.1016/j.patcog.2022.109089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile behavioral biometrics have become a popular topic of research, reaching promising results in terms of authentication, exploiting a multimodal combination of touchscreen and background sensor data. However, there is no way of knowing whether state-of-the-art classifiers in the literature can distinguish between the notion of user and device. In this article, we present a new database, BehavePassDB, structured into separate acquisition sessions and tasks to mimic the most common aspects of mobile Human-Computer Interaction (HCI). BehavePassDB is acquired through a dedicated mobile app installed on the subjects devices, also including the case of different users on the same device for evaluation. We propose a standard experimental protocol and benchmark for the research community to perform a fair comparison of novel approaches with the state of the art 1 . We propose and evaluate a system based on Long-Short Term Memory (LSTM) architecture with triplet loss and modality fusion at score level.},
  archive      = {J_PR},
  author       = {Giuseppe Stragapede and Ruben Vera-Rodriguez and Ruben Tolosana and Aythami Morales},
  doi          = {10.1016/j.patcog.2022.109089},
  journal      = {Pattern Recognition},
  pages        = {109089},
  shortjournal = {Pattern Recognition},
  title        = {BehavePassDB: Public database for mobile behavioral biometrics and benchmark evaluation},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised domain adaptation via deep conditional
adaptation network. <em>PR</em>, <em>134</em>, 109088. (<a
href="https://doi.org/10.1016/j.patcog.2022.109088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to generalize the supervised model trained on a source domain to an unlabeled target domain. Previous works mainly rely on the marginal distribution alignment of feature spaces, which ignore the conditional dependence between features and labels, and may suffer from negative transfer. To address this problem, some UDA methods focus on aligning the conditional distributions of feature spaces. However, most of these methods rely on class-specific Maximum Mean Discrepancy or adversarial training , which may suffer from mode collapse and training instability. In this paper, we propose a Deep Conditional Adaptation Network (DCAN) that aligns the conditional distributions by minimizing Conditional Maximum Mean Discrepancy, and extracts discriminant information from the target domain by maximizing the mutual information between samples and the prediction labels. Conditional Maximum Mean Discrepancy measures the difference between conditional distributions directly through their conditional embedding in Reproducing Kernel Hilbert Space , thus DCAN can be trained stably and converge fast. Mutual information can be expressed as the difference between the entropy and conditional entropy of the predicted category variable, thus DCAN can extract the discriminant information of individual and overall distributions in the target domain, simultaneously. In addition, DCAN can be used to address a special scenario, Partial UDA, where the target domain category is a subset of the source domain category. Experiments on both UDA and Partial UDA show that DCAN achieves superior classification performance over state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Pengfei Ge and Chuan-Xian Ren and Xiao-Lin Xu and Hong Yan},
  doi          = {10.1016/j.patcog.2022.109088},
  journal      = {Pattern Recognition},
  pages        = {109088},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptation via deep conditional adaptation network},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning isometry-invariant representations for point cloud
analysis. <em>PR</em>, <em>134</em>, 109087. (<a
href="https://doi.org/10.1016/j.patcog.2022.109087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D shape analysis has drawn broad attention due to its increasing demands in various fields. Despite that impressive performance has been achieved on several databases, most researchers focus their efforts on improving the performance of shape classification, retrieval, segmentation, etc. They neglect the fact that the disturbances, such as orientation and deformation, may impact much on the perception, restricting the capacity of generalizing to real applications where the prior of orientation and pose is often unknown. In this paper, we conduct shape analysis on point clouds and propose the point projection feature, which is rotation-invariant. Specifically, a novel architecture is designed to mine features of different levels. We adopt a PointNet-based backbone to extract global feature for the point cloud, and the graph aggregation operation to perceive local pose variance in the Euclidean space or geodesic space. An efficient key point descriptor is designed to assign each point with different response and help recognize the overall geometry. Furthermore, a novel dataset, PKUnon-rigid, is built that is composed of non-rigid 3D objects, based on which we evaluate the capacity of several mainstream methods in terms of processing non-rigid shapes. Mathematical analyses and experimental results demonstrate that the proposed method can extract isometry-invariant representations for 3D shape analysis tasks without rotation augmentation, and outperforms other state-of-the-art methods. The proposed dataset is publicly available at https://github.com/tasx0823/PKUnon-rigid.},
  archive      = {J_PR},
  author       = {Xiao Sun and Yang Huang and Zhouhui Lian},
  doi          = {10.1016/j.patcog.2022.109087},
  journal      = {Pattern Recognition},
  pages        = {109087},
  shortjournal = {Pattern Recognition},
  title        = {Learning isometry-invariant representations for point cloud analysis},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain generalization by joint-product distribution
alignment. <em>PR</em>, <em>134</em>, 109086. (<a
href="https://doi.org/10.1016/j.patcog.2022.109086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the problem of domain generalization for classification, where the goal is to learn a classification model on a set of source domains and generalize it to a target domain. The source and target domains are different, which weakens the generalization ability of the learned model. To tackle the domain difference, we propose to align a joint distribution and a product distribution using a neural transformation, and minimize the Relative Chi-Square (RCS) divergence between the two distributions to learn that transformation. In this manner, we conveniently achieve the alignment of multiple domains in the neural transformation space. Specifically, we show that the RCS divergence can be explicitly estimated as the maximal value of a quadratic function , which allows us to perform joint-product distribution alignment by minimizing the divergence estimate. We demonstrate the effectiveness of our solution through comparison with the state-of-the-art methods on several image classification datasets.},
  archive      = {J_PR},
  author       = {Sentao Chen and Lei Wang and Zijie Hong and Xiaowei Yang},
  doi          = {10.1016/j.patcog.2022.109086},
  journal      = {Pattern Recognition},
  pages        = {109086},
  shortjournal = {Pattern Recognition},
  title        = {Domain generalization by joint-product distribution alignment},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distance-based positive and unlabeled learning for ranking.
<em>PR</em>, <em>134</em>, 109085. (<a
href="https://doi.org/10.1016/j.patcog.2022.109085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to rank – producing a ranked list of items specific to a query and with respect to a set of supervisory items – is a problem of general interest. The setting we consider is one in which no analytic description of what constitutes a good ranking is available. Instead, we have a collection of representations and supervisory information consisting of a (target item, interesting items set) pair. We demonstrate analytically, in simulation, and in real data examples that learning to rank via combining representations using an integer linear program is effective when the supervision is as light as “these few items are similar to your item of interest.” While this nomination task is quite general, for specificity we present our methodology from the perspective of vertex nomination in graphs. The methodology described herein is model agnostic.},
  archive      = {J_PR},
  author       = {Hayden S. Helm and Amitabh Basu and Avanti Athreya and Youngser Park and Joshua T. Vogelstein and Carey E. Priebe and Michael Winding and Marta Zlatic and Albert Cardona and Patrick Bourke and Jonathan Larson and Marah Abdin and Piali Choudhury and Weiwei Yang and Christopher W. White},
  doi          = {10.1016/j.patcog.2022.109085},
  journal      = {Pattern Recognition},
  pages        = {109085},
  shortjournal = {Pattern Recognition},
  title        = {Distance-based positive and unlabeled learning for ranking},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularizing autoencoders with wavelet transform for
sequence anomaly detection. <em>PR</em>, <em>134</em>, 109084. (<a
href="https://doi.org/10.1016/j.patcog.2022.109084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, systems or entities are usually monitored by devices, generating large amounts of time series. Detecting anomalies in them help prevent potential losses, thus arousing much research interest. Existing studies have adopted autoencoders to detect anomalies , where reconstruction errors are used to indicate outliers. However, sometimes autoencoders may also reconstruct anomalies well due to the learned general features in latent spaces. To solve the above problem, we propose to regularize autoencoders to grasp specific features of normal sequences. Specifically, spectral unique patterns are captured by statistical analysis on discrete wavelet transform (DWT) coefficients of input sequences, restricting latent spaces to reflect unique patterns of normal sequences in both time and frequency domains. Furthermore, a Weight Controller calculating sample-adaptive regularization weights is designed to fully utilize the regularization effect. Extensive experiments on three public benchmarks demonstrate the effectiveness and superiority of the proposed model compared with state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Yueyue Yao and Jianghong Ma and Yunming Ye},
  doi          = {10.1016/j.patcog.2022.109084},
  journal      = {Pattern Recognition},
  pages        = {109084},
  shortjournal = {Pattern Recognition},
  title        = {Regularizing autoencoders with wavelet transform for sequence anomaly detection},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto-weighted tensor schatten p-norm for robust multi-view
graph clustering. <em>PR</em>, <em>134</em>, 109083. (<a
href="https://doi.org/10.1016/j.patcog.2022.109083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, tensor-singular value decomposition based tensor-nuclear norm ( t t -TNN) has achieved impressive performance for multi-view graph clustering . This primarily ascribes the superiority of t t -TNN in exploring high-order structure information among views. However, 1) t t -TNN cannot ideally approximate to the original rank minimization, which produces the suboptimal graph tensor; in addition, t t -TNN treats different singular values equally, such that the larger singular values corresponding to certain significant feature information ( i.e ., prior information) has not been utilized fully; 2) the data of original high-dimensional space are often corrupted by noise and outliers, which always makes adaptive neighbors graph learning (ANGL) generate low-quality affinity graphs. To address these issues, we propose a novel multi-view graph clustering method termed auto-weighted tensor Schatten p p -norm ( t t -ATSN) for robust multi-view graph clustering ( t t -ATSN-RMGC). Concretely, we first propose t t -SVD based t t -ATSN with 0 0&amp;lt;p&amp;lt;1 to make the learned graph tensor better approximate the target rank than t t -TNN. Meanwhile, it can also automatically and appropriately shrink singular values for constructing a more refined graph tensor, so as to fully capture spatial structure in the graph tensor. Moreover, we introduce the Geman McClure loss function to enhance the robustness of ANGL for noise and outliers. Experimental results on benchmarks across different scenarios and sizes show that the proposed method consistently outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xingfeng Li and Zhenwen Ren and Quansen Sun and Zhi Xu},
  doi          = {10.1016/j.patcog.2022.109083},
  journal      = {Pattern Recognition},
  pages        = {109083},
  shortjournal = {Pattern Recognition},
  title        = {Auto-weighted tensor schatten p-norm for robust multi-view graph clustering},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A goal-driven unsupervised image segmentation method
combining graph-based processing and markov random fields. <em>PR</em>,
<em>134</em>, 109082. (<a
href="https://doi.org/10.1016/j.patcog.2022.109082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is the process of partitioning a digital image into a set of homogeneous regions (according to some homogeneity criterion) to facilitate a subsequent higher-level analysis. In this context, the present paper proposes an unsupervised and graph-based method of image segmentation, which is driven by an application goal, namely, the generation of image segments associated with a user-defined and application-specific goal. A graph, together with a random grid of source elements, is defined on top of the input image. From each source satisfying a goal-driven predicate, called seed, a propagation algorithm assigns a cost to each pixel on the basis of similarity and topological connectivity, measuring the degree of association with the reference seed. Then, the set of most significant regions is automatically extracted and used to estimate a statistical model for each region. Finally, the segmentation problem is expressed in a Bayesian framework in terms of probabilistic Markov random field (MRF) graphical modeling. An ad hoc energy function is defined based on parametric models, a seed-specific spatial feature, a background-specific potential, and local-contextual information. This energy function is minimized through graph cuts and, more specifically, the alpha-beta swap algorithm, yielding the final goal-driven segmentation based on the maximum a posteriori (MAP) decision rule. The proposed method does not require deep a priori knowledge (e.g., labelled datasets), as it only requires the choice of a goal-driven predicate and a suited parametric model for the data. In the experimental validation with both magnetic resonance (MR) and synthetic aperture radar (SAR) images, the method demonstrates robustness, versatility, and applicability to different domains, thus allowing for further analyses guided by the generated products.},
  archive      = {J_PR},
  author       = {Marco Trombini and David Solarna and Gabriele Moser and Silvana Dellepiane},
  doi          = {10.1016/j.patcog.2022.109082},
  journal      = {Pattern Recognition},
  pages        = {109082},
  shortjournal = {Pattern Recognition},
  title        = {A goal-driven unsupervised image segmentation method combining graph-based processing and markov random fields},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parzen window approximation on riemannian manifold.
<em>PR</em>, <em>134</em>, 109081. (<a
href="https://doi.org/10.1016/j.patcog.2022.109081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In graph motivated learning, label propagation largely depends on data affinity represented as edges between connected data points. The affinity assignment implicitly assumes even distribution of data on the manifold. This assumption may not hold and may lead to inaccurate metric assignment due to drift towards high-density regions. The drift affected heat kernel based affinity with a globally fixed Parzen window either discards genuine neighbors or forces distant data points to become a member of the neighborhood. This yields a biased affinity matrix . In this paper, the bias due to uneven data sampling on the Riemannian manifold is catered to by a variable Parzen window determined as a function of neighborhood size, ambient dimension, flatness range, etc. Additionally, affinity adjustment is used which offsets the effect of uneven sampling responsible for the bias. An affinity metric which takes into consideration the irregular sampling effect to yield accurate label propagation is proposed. Extensive experiments on synthetic and real-world data sets confirm that the proposed method increases the classification accuracy significantly and outperforms existing Parzen window estimators in graph Laplacian manifold regularization methods .},
  archive      = {J_PR},
  author       = {Abhishek and Rakesh Kumar Yadav and Shekhar Verma},
  doi          = {10.1016/j.patcog.2022.109081},
  journal      = {Pattern Recognition},
  pages        = {109081},
  shortjournal = {Pattern Recognition},
  title        = {Parzen window approximation on riemannian manifold},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-paced resistance learning against overfitting on noisy
labels. <em>PR</em>, <em>134</em>, 109080. (<a
href="https://doi.org/10.1016/j.patcog.2022.109080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy labels composed of correct and corrupted ones are pervasive in practice. They might significantly deteriorate the performance of convolutional neural networks (CNNs), because CNNs are easily overfitted on corrupted labels. To address this issue, inspired by an observation, deep neural networks might first memorize the probably correct-label data and then corrupt-label samples, we propose a novel yet simple self-paced resistance framework to resist corrupted labels, without using any clean validation data. The proposed framework first utilizes the memorization effect of CNNs to learn a curriculum, which contains confident samples and provides meaningful supervision for other training samples. Then it adopts selected confident samples and a proposed resistance loss to update model parameters; the resistance loss tends to smooth model parameters’ update or attain equivalent prediction over each class, thereby resisting model overfitting on corrupted labels. Finally, we unify these two modules into a single loss function and optimize it in an alternative learning. Extensive experiments demonstrate the significantly superior performance of the proposed framework over recent state-of-the-art methods on noisy-label data. Source codes of the proposed method are available on https://github.com/xsshi2015/Self-paced-Resistance-Learning .},
  archive      = {J_PR},
  author       = {Xiaoshuang Shi and Zhenhua Guo and Kang Li and Yun Liang and Xiaofeng Zhu},
  doi          = {10.1016/j.patcog.2022.109080},
  journal      = {Pattern Recognition},
  pages        = {109080},
  shortjournal = {Pattern Recognition},
  title        = {Self-paced resistance learning against overfitting on noisy labels},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curvilinear structure tracking based on dynamic
curvature-penalized geodesics. <em>PR</em>, <em>134</em>, 109079. (<a
href="https://doi.org/10.1016/j.patcog.2022.109079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geodesic models are considered as a fundamental and powerful tool in the applications of curvilinear structure extraction, where the target structures are usually modeled as geodesic paths connecting prescribed points. Despite great advances in geodesic models, it still remains an unsolved problem of detecting weak curvilinear structures from complicated scenarios. In this paper, a dynamic high-order geodesic model for curvilinear structure extraction is introduced to alleviate the shortcuts or short branches combination problems suffered in the classical geodesic approaches. For that purpose, we take into account the nonlocal pattern of curvilinear structures and the local curvature of geodesic paths for the construction of geodesic metrics. Accordingly, the proposed model is able to blend the benefits from the on-the-fly nonlocal smoothness property, curvature regularization and appearance coherence penalization. The nonlocal smoothness property carried out via a local bending operator is constructed to provide a quantitative measure of geodesic advancing directions, meanwhile the coherence penalization is established to guarantee the consistency of the local appearance features extracted via a vessel detector. The experiment results on synthetic and real images illustrate that the proposed method obtains outperformance when compared to the classical geodesic-based tracing algorithms.},
  archive      = {J_PR},
  author       = {Li Liu and Mingzhu Wang and Shuwang Zhou and Minglei Shu and Laurent D. Cohen and Da Chen},
  doi          = {10.1016/j.patcog.2022.109079},
  journal      = {Pattern Recognition},
  pages        = {109079},
  shortjournal = {Pattern Recognition},
  title        = {Curvilinear structure tracking based on dynamic curvature-penalized geodesics},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lie algebra representation for efficient 2D shape
classification. <em>PR</em>, <em>134</em>, 109078. (<a
href="https://doi.org/10.1016/j.patcog.2022.109078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Riemannian manifold plays a vital role as a powerful mathematical tool in computer vision , with important applications in curved shape analysis and classification. Significant progress has recently been made by Riemannian framework based methods that achieved state-of-the-art classification accuracy and robustness. However, these Riemannian manifold and Lie group methods require a very high computational complexity and do not include a description of the shape regions. This paper presents a novel mathematical tool, called Block Diagonal Symmetric Positive Definite Matrix Lie Algebra (BDSPDMLA) to represent curves, which extends the existing Lie group representations to a compact yet informative Lie algebra representation. The proposed Lie algebra based method addresses the computational bottleneck problem of the Riemannian framework based methods. In addition, it allows the natural fusion of various regions information with curved shape features for a more discriminative shape description. Here the region information is represented by values of distance maps, local binary patterns (LBP) and image intensity. Extensive experiments on five publicly available databases demonstrate that the proposed Lie algebra based method can achieve a speed of over ten thousand times faster than the Riemannian manifold and Lie group based baseline methods , while obtaining comparable accuracies for 2D shape classification.},
  archive      = {J_PR},
  author       = {Xiaohan Yu and Yongsheng Gao and Mohammed Bennamoun and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2022.109078},
  journal      = {Pattern Recognition},
  pages        = {109078},
  shortjournal = {Pattern Recognition},
  title        = {A lie algebra representation for efficient 2D shape classification},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Deep graph clustering with multi-level subspace fusion.
<em>PR</em>, <em>134</em>, 109077. (<a
href="https://doi.org/10.1016/j.patcog.2022.109077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering combines both node attributes and graph structure information of data samples and has demonstrated satisfactory performance in various applications. However, how to choose the proper neighborhood for attributed graph clustering remains to be a challenge. A larger neighborhood may cause over-smoothed representations with less discrimination for clustering while the short-range ignore distant nodes and fails to capture the global information. In this paper, we propose a novel deep attributed graph clustering network with a multi-level subspace fusion module to address this issue. The first contribution of our work is to insert multiple self-expressive modules between low-level and high-level layers to promote more favorable features for clustering. The constraint of shared self-expressive matrix facilitates to preserve intrinsic structure without pre-defined neighborhoods as the previous methods do. Moreover, we introduce a novel loss function that leverages traditional reconstruction and the proposed structure fusion loss to effectively preserve multi-level clustering structures with both global and local discriminative features . Extensive experiments on public benchmark datasets validate the effectiveness of our proposed model compared with the state-of-the-art attribute graph clustering competitors by considerable margins.},
  archive      = {J_PR},
  author       = {Wang Li and Siwei Wang and Xifeng Guo and En Zhu},
  doi          = {10.1016/j.patcog.2022.109077},
  journal      = {Pattern Recognition},
  pages        = {109077},
  shortjournal = {Pattern Recognition},
  title        = {Deep graph clustering with multi-level subspace fusion},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ML-DSVM+: A meta-learning based deep SVM+ for computer-aided
diagnosis. <em>PR</em>, <em>134</em>, 109076. (<a
href="https://doi.org/10.1016/j.patcog.2022.109076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning (TL) can improve the performance of a single-modal medical imaging-based computer-aided diagnosis (CAD) by transferring knowledge from related imaging modalities. Support vector machine plus (SVM+) is a supervised TL classifier specially designed for TL between the paired data in the source and target domains with shared labels. In this work, a novel deep neural network (DNN) based SVM+ (DSVM+) algorithm is proposed for single-modal imaging-based CAD . DSVM+ integrates the bi-channel DNNs and SVM+ classifier into a unified framework to improve the performance of both feature representation and classification. In particular, a new coupled hinge loss function is developed to conduct bidirectional TL between the source and target domains, which further promotes knowledge transfer together with the feature representation under the guidance of shared labels. To alleviate the overfitting caused by the increased parameters in DNNs for limited training samples, the meta-learning based DSVM+ (ML-DSVM+) is further developed, which designs randomly selecting samples from the training data instead of other CAD tasks for meta-tasks. This sampling strategy also can avoid the issue of class imbalance. ML-DSVM+ is evaluated on three medical imaging datasets. It achieves the best results of 88.26±1.40\%, 90.45±5.00\%, and 87.63±5.56\% on accuracy, sensitivity and specificity, respectively, on the Bimodal Breast Ultrasound Image dataset, 90.00±1.05\%, 72.55±3.87\%, and 96.40±2.26\% of the corresponding indices on the Alzheimer&#39;s Disease Neuroimaging Initiative dataset, and 85.76±3.12\% of classification accuracy, 88.73±7.22\% of sensitivity, and 82.60±1.56\% of specificity for the Autism Brain Imaging Data Exchange dataset.},
  archive      = {J_PR},
  author       = {Xiangmin Han and Jun Wang and Shihui Ying and Jun Shi and Dinggang Shen},
  doi          = {10.1016/j.patcog.2022.109076},
  journal      = {Pattern Recognition},
  pages        = {109076},
  shortjournal = {Pattern Recognition},
  title        = {ML-DSVM+: A meta-learning based deep SVM+ for computer-aided diagnosis},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Frequency domain regularization for iterative adversarial
attacks. <em>PR</em>, <em>134</em>, 109075. (<a
href="https://doi.org/10.1016/j.patcog.2022.109075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have attracted more and more attentions with the prosperity of convolutional neural networks . The transferability of adversarial examples is an important property that makes black-box attacks possible in real-world applications. On the other side, many adversarial defense methods have been proposed to improve the robustness, leading to the requirement for more transferable adversarial examples. Inspired by the regularization term for network parameters at training process, we treat adversarial attacks as training process of inputs and propose regularization constraint for inputs to prevent adversarial examples from overfitting the white-box networks and enhance the transferability. Specifically, we find a universal attribute that the outputs of convolutional neural networks have consistency to the low frequencies of inputs, and based on this, we construct a frequency domain regularization to inputs for iterative attacks. In this way, our method is compatible with existing iterative attack methods and can learn more transferable adversarial examples. Extensive experiments on ImageNet validate the superiority of our method, and compared with several attacks, we achieve attack success rate improvements of 8.0\% and 11.5\% on average to normal models and defense methods respectively.},
  archive      = {J_PR},
  author       = {Tengjiao Li and Maosen Li and Yanhua Yang and Cheng Deng},
  doi          = {10.1016/j.patcog.2022.109075},
  journal      = {Pattern Recognition},
  pages        = {109075},
  shortjournal = {Pattern Recognition},
  title        = {Frequency domain regularization for iterative adversarial attacks},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-label feature selection via robust flexible sparse
regularization. <em>PR</em>, <em>134</em>, 109074. (<a
href="https://doi.org/10.1016/j.patcog.2022.109074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection is an efficient technique to deal with the high dimensional multi-label data by selecting the optimal feature subset. Existing researches have demonstrated that l 1 l1 -norm and l 2 , 1 l2,1 -norm are promising roles for multi-label feature selection. However, two important issues are ignored when existing l 1 l1 -norm and l 2 , 1 l2,1 -norm based methods select discriminative features for multi-label data. First, l 1 l1 -norm can enforce sparsity on each feature across all instances while numerous selected features lack discrimination due to the generated zero weight values. Second, l 2 , 1 l2,1 -norm not only neglects label-specific features but also ignores the redundancy among features. To this end, we design a Robust Flexible Sparse Regularization norm (RFSR), furthermore, proposing a global optimization framework named Robust Flexible Sparse regularized multi-label Feature Selection (RFSFS) based on RFSR. Finally, an efficient alternating multipliers based optimization scheme is developed to iteratively optimize RFSFS. Empirical studies on fifteen benchmark multi-label data sets demonstrate the effectiveness and efficiency of RFSFS.},
  archive      = {J_PR},
  author       = {Yonghao Li and Liang Hu and Wanfu Gao},
  doi          = {10.1016/j.patcog.2022.109074},
  journal      = {Pattern Recognition},
  pages        = {109074},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label feature selection via robust flexible sparse regularization},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video object segmentation using point-based memory network.
<em>PR</em>, <em>134</em>, 109073. (<a
href="https://doi.org/10.1016/j.patcog.2022.109073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the prevalence of memory-based methods for Semi-supervised Video Object Segmentation (SVOS) which utilise past frames efficiently for label propagation. When conducting feature matching, fine-grained multi-scale feature matching has typically been performed using all query points , which inevitably results in redundant computations and thus makes the fusion of multi-scale results ineffective. In this paper, we develop a new Point-based Memory Network, termed as PMNet, to perform fine-grained feature matching on hard samples only, assuming that easy samples can already obtain satisfactory matching results without the need for complicated multi-scale feature matching. Our approach first generates an uncertainty map from the initial decoding outputs. Next, the fine-grained features at uncertain locations are sampled to match the memory features on the same scale. Finally, the matching results are further decoded to provide a refined output. The point-based scheme works with the coarsest feature matching in a complementary and efficient manner. Furthermore, we propose an approach to adaptively perform global or regional matching based on the motion history of memory points , making our method more robust against ambiguous backgrounds. Experimental results on several benchmark datasets demonstrate the superiority of our proposed method over state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Mingqi Gao and Jungong Han and Feng Zheng and James J.Q. Yu and Giovanni Montana},
  doi          = {10.1016/j.patcog.2022.109073},
  journal      = {Pattern Recognition},
  pages        = {109073},
  shortjournal = {Pattern Recognition},
  title        = {Video object segmentation using point-based memory network},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SC-GAN: Subspace clustering based GAN for automatic
expression manipulation. <em>PR</em>, <em>134</em>, 109072. (<a
href="https://doi.org/10.1016/j.patcog.2022.109072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the topics of facial attribute manipulation and decomposition have gained great popularity in computer vision and human computer interaction . Even though such methods have been preliminarily employed in some photo beautification applications, it still remains challenging due to the highly-versatile facial attributes and their drastic appearance changes subject to variation of deformation, illumination, pose, etc. The prevailing problems are especially severe when we are faced with group photos involving many faces. To overcome such critical limitations and discover more meaningful visual attributes and their possible decompositions, we develop a subspace clustering based generative adversarial network (SC-GAN) in this paper. Our SC-GAN can simultaneously decompose multiple subspaces and generate diverse samples correspondingly, thus the training of the generative models could be more effectively guided by facial attribute and its decomposition and manipulation in a natural and meaningful fashion. Our SC-GAN incorporates the SIFT K-means cluster, which could split the holistic semantic facial space into different subspaces without supervision, and help the new GAN generate more convincing results within specific subspaces. Extensive experiments and comprehensive evaluations confirm that, our method can greatly reduce the unexpected influences caused by portrait diversities and outperform the state-of-the-art facial attribute manipulation approaches. 1},
  archive      = {J_PR},
  author       = {Shuai Li and Liang Liu and Ji Liu and Wenfeng Song and Aimin Hao and Hong Qin},
  doi          = {10.1016/j.patcog.2022.109072},
  journal      = {Pattern Recognition},
  pages        = {109072},
  shortjournal = {Pattern Recognition},
  title        = {SC-GAN: Subspace clustering based GAN for automatic expression manipulation},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Specialized re-ranking: A novel retrieval-verification
framework for cloth changing person re-identification. <em>PR</em>,
<em>134</em>, 109070. (<a
href="https://doi.org/10.1016/j.patcog.2022.109070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloth changing person re-identification(Re-ID) can work under more complicated scenarios with higher security than normal Re-ID and biometric techniques and is therefore extremely valuable in applications. Meanwhile, the wide range of appearance flexibility results in more similar-looking, confusing images, which is the weakness of the widely used retrieval methods . In this work, we shed light on how to handle these similar images. Specifically, we propose a novel retrieval-verification framework. Given an image, the retrieval module will search for a shot list of similar images quickly. Our proposed verification network will then compare the probe image with these candidate images by contrasting local details for their similarity scores. An innovative ranking strategy is also introduced to achieve a good balance between retrieval and verification results. Comprehensive experiments are conducted to show the effectiveness of our framework and its capability in improving the state-of-the-art methods remarkably on both synthetic and realistic datasets.},
  archive      = {J_PR},
  author       = {Renjie Zhang and Yu Fang and Huaxin Song and Fangbin Wan and Yanwei Fu and Hirokazu Kato and Yang Wu},
  doi          = {10.1016/j.patcog.2022.109070},
  journal      = {Pattern Recognition},
  pages        = {109070},
  shortjournal = {Pattern Recognition},
  title        = {Specialized re-ranking: A novel retrieval-verification framework for cloth changing person re-identification},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Thermal to visual person re-identification using
collaborative metric learning based on maximum margin matrix
factorization. <em>PR</em>, <em>134</em>, 109069. (<a
href="https://doi.org/10.1016/j.patcog.2022.109069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thermal to visual person re-identification (T2V-ReID) is a cross-domain image retrieval problem. In this problem, the matching of a person’s image takes place, where the image is taken by different cameras (thermal and visual) at different times. This problem has numerous applications in night-time security surveillance. It is challenging due to the large intra-class variations and cross-domain discrepancies. Recently, deep metric learning methods are proposed for this problem. Still, there is a scope to improve the metric learning by generalizing the metric. In this paper, we have proposed the collaborative metric learning using Maximum Margin Matrix Factorization . It uses the group-wise similarities and collaboratively predicts the similarities. We can learn a more generalized metric by utilizing the maximized margin in this method. The proposed method is tested on the RegDB and RGB-D-T data sets, and the method outperforms the existing works in the few-shot learning settings.},
  archive      = {J_PR},
  author       = {Yaswanth Gavini and Arun Agarwal and B.M. Mehtre},
  doi          = {10.1016/j.patcog.2022.109069},
  journal      = {Pattern Recognition},
  pages        = {109069},
  shortjournal = {Pattern Recognition},
  title        = {Thermal to visual person re-identification using collaborative metric learning based on maximum margin matrix factorization},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning comprehensive global features in person
re-identification: Ensuring discriminativeness of more local regions.
<em>PR</em>, <em>134</em>, 109068. (<a
href="https://doi.org/10.1016/j.patcog.2022.109068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) aims to retrieve person images from a large gallery given a query image of a person of interest. Global information and fine-grained local features are both essential for the representation. However, global embedding learned by naive classification model tends to be trapped in the most discriminative local region, leading to poor evaluation performance. To address the issue, we propose a novel baseline network that learns strong global feature termed as Comprehensive Global Embedding (CGE), ensuring more local regions of global feature maps to be discriminative. In this work, two key modules are proposed including Non-parameterized Local Classifier (NLC) and Global Logits Revise (GLR). The NLC is designed to obtain a score vector of each local region on feature maps in a non-parametric manner. The GLR module directly revises the logits such that the subsequent cross entropy loss up-weights the loss assigned to samples with hard-to-learn local regions. The convergence of the deep model indicates more local regions (the number of local regions is manually defined) on the feature maps of each sample are discriminative. We implement these two modules on two strong baseline methods including the BagTricks (BOT) [1] and AGW [2]. The network achieves 65.9\% mAP, 85.1\% rank1 on MSMT17, 86.4\% mAP, 87.4\% rank1 on CUHK03 labeled, 84.2\% mAP, 85.9\% rank1 on CUHK03 detected, and 92.2\% mAP, 96.3\% rank1 on Market-1501. The results show that the proposed baseline achieves a new state-of-the-art when using only global embedding during inference without any re-ranking technique.},
  archive      = {J_PR},
  author       = {Jiali Xi and Jianqiang Huang and Shibao Zheng and Qin Zhou and Bernt Schiele and Xian-Sheng Hua and Qianru Sun},
  doi          = {10.1016/j.patcog.2022.109068},
  journal      = {Pattern Recognition},
  pages        = {109068},
  shortjournal = {Pattern Recognition},
  title        = {Learning comprehensive global features in person re-identification: Ensuring discriminativeness of more local regions},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-order manifold regularized multi-view subspace
clustering with robust affinity matrices and weighted TNN. <em>PR</em>,
<em>134</em>, 109067. (<a
href="https://doi.org/10.1016/j.patcog.2022.109067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering achieves impressive performance for high-dimensional data. However, many of these models do not sufficiently mine the intrinsic information among samples and consider the robustness problem of the affinity matrices, resulting in the degradation of clustering performance. To address these problems, we propose a novel high-order manifold regularized multi-view subspace clustering with robust affinity matrices and a weighted tensor nuclear norm (TNN) model (termed HMRMSC) to characterize real-world data. Specifically, all the similarity matrices of different views are first stacked into a third-order tensor. However, the constructed tensor may contain an additional inter-class representation since the data are usually noisy. Then, we use a technique similar to tensor principal component analysis (TPCA) to obtain a more robust similarity tensor, which is constrained by the so-called weighted TNN since the original TNN treats each singular value equally and usually considers no prior information of singular values. In addition, a high-order manifold regularized term is also added to utilize the manifold information of data. Finally, all the steps are unified into a framework, which is resolved by the augmented Lagrange multiplier (ALM) method. Experimental results on six representative datasets show that our model outperforms several state-of-the-art counterparts.},
  archive      = {J_PR},
  author       = {Bing Cai and Gui-Fu Lu and Liang Yao and Hua Li},
  doi          = {10.1016/j.patcog.2022.109067},
  journal      = {Pattern Recognition},
  pages        = {109067},
  shortjournal = {Pattern Recognition},
  title        = {High-order manifold regularized multi-view subspace clustering with robust affinity matrices and weighted TNN},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). TAA-GCN: A temporally aware adaptive graph convolutional
network for age estimation. <em>PR</em>, <em>134</em>, 109066. (<a
href="https://doi.org/10.1016/j.patcog.2022.109066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel age estimation algorithm , the Temporally-Aware Adaptive Graph Convolutional Network (TAA-GCN). Using a new representation based on graphs, the TAA-GCN utilizes skeletal, posture, clothing, and facial information to enrich the feature set associated with various ages. Such a novel graph representation has several advantages: First, reduced sensitivity to facial expression and other appearance variances; Second, robustness to partial occlusion and non-frontal-planar viewpoint, which is commonplace in real-world applications such as video surveillance. The TAA-GCN employs two novel components, (1) the Temporal Memory Module (TMM) to compute temporal dependencies in age; (2) Adaptive Graph Convolutional Layer (AGCL) to refine the graphs and accommodate the variance in appearance. The TAA-GCN outperforms the state-of-the-art methods on four public benchmarks, UTKFace, MORPHII, CACD, and FG-NET. Moreover, the TAA-GCN showed reliability in different camera viewpoints and reduced quality images.},
  archive      = {J_PR},
  author       = {Matthew Korban and Peter Youngs and Scott T. Acton},
  doi          = {10.1016/j.patcog.2022.109066},
  journal      = {Pattern Recognition},
  pages        = {109066},
  shortjournal = {Pattern Recognition},
  title        = {TAA-GCN: A temporally aware adaptive graph convolutional network for age estimation},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-learning-based adversarial training for deep 3D face
recognition on point clouds. <em>PR</em>, <em>134</em>, 109065. (<a
href="https://doi.org/10.1016/j.patcog.2022.109065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep face recognition using 2D face images has made great advances mainly due to the readily available large-scale face data. However, deep face recognition using 3D face scans, especially on point clouds, has been far from fully explored. In this paper, we propose a novel meta-learning-based adversarial training (MLAT) algorithm for deep 3D face recognition (3DFR) on point clouds. It consists of two alternate modules: adversarial sample generating for 3D face data augmentation and meta-learning-based deep network training. In the first module, adversarial samples of given 3D face scans are dynamically generated based on current deep 3DFR model. In the second module, a meta-learning framework is designed to avoid the performance decrease caused by the generated adversarial samples. Overall, MLAT algorithm combines the adversarial sample generating and meta-learning-based network training in a uniform framework, in which adversarial samples and network parameters are optimized alternately. Thus, it can continuously generate diverse and suitable adversarial samples, and then the meta-learning framework can further improve the accuracy of 3DFR model. Comprehensive experimental results show that the proposed approach consistently achieves competitive rank-one recognition accuracies on the BU-3DFE (100\%), Bosphorus (99.78\%), BU-4DFE (98.02\%) and FRGC v2 (98.01\%) database, and thereby substantiate its superiority.},
  archive      = {J_PR},
  author       = {Cuican Yu and Zihui Zhang and Huibin Li and Jian Sun and Zongben Xu},
  doi          = {10.1016/j.patcog.2022.109065},
  journal      = {Pattern Recognition},
  pages        = {109065},
  shortjournal = {Pattern Recognition},
  title        = {Meta-learning-based adversarial training for deep 3D face recognition on point clouds},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ImageNet-patch: A dataset for benchmarking machine learning
robustness against adversarial patches. <em>PR</em>, <em>134</em>,
109064. (<a href="https://doi.org/10.1016/j.patcog.2022.109064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. The dataset is built by first optimizing a set of adversarial patches against an ensemble of models, using a state-of-the-art attack that creates transferable patches. The corresponding patches are then randomly rotated and translated, and finally applied to the ImageNet data. We use ImageNet-Patch to benchmark the robustness of 127 models against patch attacks, and also validate the effectiveness of the given patches in the physical domain (i.e., by printing and applying them to real-world objects). We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at https://github.com/pralab/ImageNet-Patch .},
  archive      = {J_PR},
  author       = {Maura Pintor and Daniele Angioni and Angelo Sotgiu and Luca Demetrio and Ambra Demontis and Battista Biggio and Fabio Roli},
  doi          = {10.1016/j.patcog.2022.109064},
  journal      = {Pattern Recognition},
  pages        = {109064},
  shortjournal = {Pattern Recognition},
  title        = {ImageNet-patch: A dataset for benchmarking machine learning robustness against adversarial patches},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust learning from noisy web data for fine-grained
recognition. <em>PR</em>, <em>134</em>, 109063. (<a
href="https://doi.org/10.1016/j.patcog.2022.109063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to DNNs’ memorization effect, label noise lessens the performance of the web-supervised fine-grained visual categorization task. Previous literature primarily relies on small-loss instances for subsequent training. The current state-of-the-art approach JoCoR additionally employs explicit consistency constraints to make clean samples more confident. However, a joint loss designed for both sample selection criteria and parameter updating is not competent for training a robust model in the presence of web noise. Especially, false positives are assigned with larger weights, causing the model to pay more attention to misclassified noisy images . Besides, leveraging weight decay to forget discarded noisy instances is too slow and implicit to take effect. Therefore, we propose a simple yet effective approach named MS-DeJOR ( M ulti- S cale training with De coupled J oint O ptimization and R efurbishment). In contrast to JoCoR, we decouple sample selection from training procedure to handle the above problems. Specifically, a negative entropy term is applied to prevent false positives from being overemphasized. The model can explicitly forget those samples identified as noise by imposing such a regularization term on all training data. Furthermore, we use accumulated predictions to refurbish the noisy labels and re-weight training images to boost the model performance. A multi-scale feature enhancement module is adopted to extract discriminative and subtle feature representations. Extensive experiments show that MS-DeJOR yields state-of-the-art performances on three web-supervised fine-grained datasets, demonstrating the effectiveness of our approach. The data and source code have been available at https://github.com/msdejor/MS-DeJOR .},
  archive      = {J_PR},
  author       = {Zhenhuang Cai and Guo-Sen Xie and Xingguo Huang and Dan Huang and Yazhou Yao and Zhenmin Tang},
  doi          = {10.1016/j.patcog.2022.109063},
  journal      = {Pattern Recognition},
  pages        = {109063},
  shortjournal = {Pattern Recognition},
  title        = {Robust learning from noisy web data for fine-grained recognition},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Towards parameter-free clustering for real-world data.
<em>PR</em>, <em>134</em>, 109062. (<a
href="https://doi.org/10.1016/j.patcog.2022.109062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many clustering algorithms have been published, existing algorithms are often afflicted by some problems in processing real-world data. We present an algorithm to deal with two of these problems in this paper. First, the majority of clustering algorithms depend on one or more parameters. Second, some algorithms are not suitable for clusters of Gaussian distribution, whereas clusters of many real datasets are of Gaussian distribution approximately. Our algorithm generates clusters sequentially, and each cluster is obtained by expanding an initial cluster. The initial cluster is extracted with the dominant set algorithm, and we study the correlation between the pairwise data similarity matrix and clustering result to determine the involved scaling parameter adaptively. In expanding the initial cluster, we improve the density peak algorithm so that the expansion will not cross the boundary between two clusters, and the involved density parameter has little influence on clustering results. In our algorithm, the cluster expansion enables our algorithm to work well with clusters of Gaussian distribution, and two involved parameters can be fixed or determined adaptively. Our algorithm goes a step forward in parameter-free clustering for real-world data, and it is shown to perform better than or comparably to some commonly used algorithms with parameters in experiments with synthetic datasets composed of Gaussian clusters and real datasets.},
  archive      = {J_PR},
  author       = {Jian Hou and Huaqiang Yuan and Marcello Pelillo},
  doi          = {10.1016/j.patcog.2022.109062},
  journal      = {Pattern Recognition},
  pages        = {109062},
  shortjournal = {Pattern Recognition},
  title        = {Towards parameter-free clustering for real-world data},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework based on local cores and synthetic examples
generation for self-labeled semi-supervised classification. <em>PR</em>,
<em>134</em>, 109060. (<a
href="https://doi.org/10.1016/j.patcog.2022.109060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-labeled techniques are semi-supervised classification models that overcome the shortage of labeled samples via an iterative process. Most relevant proposals are inspired by boosting schemes to iteratively enlarge labeled data, but these methods are constrained by the number and distribution of the initial labeled data. Up to the present, the only exceptions which can solve the above problem are SEG-SSC, k -means-SSC and LC-SSC. However, SEG-SSC relies on too many parameters. Besides, it is hard to improve the distribution of the initial labeled data when the initial labeled set can not roughly represent the distribution of the original data. k -means-SSC and LC-SSC fail to significantly improve the number of the initial labeled data by a limited number of representative points. To address the above issues, this paper proposes a framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification (LCSEG-SSC). First, a new method for finding local cores on labeled and unlabeled data is proposed to improve the distribution of the initial labeled data. Second, STOPF or active labeling is used to predict found local cores. Third, a new example generation technique is proposed to create synthetic labeled samples, intending to improve the number of the initial labeled data. After that, any self-labeled with boosting schemes can be executed on the improved labeled data effectively. Intensive experiments prove that LCSEG-SSC outperforms state-of-the-art methods, especially in a relatively low ratio of labeled data.},
  archive      = {J_PR},
  author       = {Junnan Li and MingQiang Zhou and Qingsheng Zhu and Quanwang Wu},
  doi          = {10.1016/j.patcog.2022.109060},
  journal      = {Pattern Recognition},
  pages        = {109060},
  shortjournal = {Pattern Recognition},
  title        = {A framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Joint graph learning and matching for semantic feature
correspondence. <em>PR</em>, <em>134</em>, 109059. (<a
href="https://doi.org/10.1016/j.patcog.2022.109059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, powered by the learned discriminative representation via graph neural network (GNN) models, deep graph matching methods have made great progresses in the task of matching semantic features . However, these methods usually rely on heuristically generated graph patterns, which may introduce unreliable relationships to hurt the matching performance. In this paper, we propose a joint graph learning and matching network, named GLAM, to explore reliable graph structures for boosting graph matching. GLAM adopts a pure attention-based framework for both graph learning and graph matching. Specifically, it employs two types of attention mechanisms, self-attention and cross-attention for the task. The self-attention discovers the relationships between features to further update feature representations over the learnt structures; and the cross-attention computes cross-graph correlations between the two feature sets to be matched for feature reconstruction. Moreover, the final matching solution is directly derived from the output of the cross-attention layer, without employing a specific matching decision module. The proposed method is evaluated on three popular visual matching benchmarks (Pascal VOC, Willow Object and SPair-71k), and it outperforms previous state-of-the-art graph matching methods on all benchmarks. Furthermore, the graph patterns learnt by our model are validated to be able to remarkably enhance previous deep graph matching methods by replacing their handcrafted graph structures with the learnt ones.},
  archive      = {J_PR},
  author       = {He Liu and Tao Wang and Yidong Li and Congyan Lang and Yi Jin and Haibin Ling},
  doi          = {10.1016/j.patcog.2022.109059},
  journal      = {Pattern Recognition},
  pages        = {109059},
  shortjournal = {Pattern Recognition},
  title        = {Joint graph learning and matching for semantic feature correspondence},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mitigating the effect of dataset shift in clustering.
<em>PR</em>, <em>134</em>, 109058. (<a
href="https://doi.org/10.1016/j.patcog.2022.109058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataset shift is a relevant topic in unsupervised learning since many applications face evolving environments, causing an important loss of generalization and performance. Most techniques that deal with this issue are designed for data stream clustering, whose goal is to process sequences of data efficiently under Big Data. In this study, we claim dataset shift is an issue for static clustering tasks in which data is collected over a long period. To mitigate it, we propose Time-weighted kernel k k -means, a k k -means variant that includes a time-dependent weighting process. We do this via the induced ordered weighted average (IOWA) operator. The weighting process acts as a gradual forgetting mechanism, prioritizing recent examples over outdated ones in the clustering algorithm . The computational experiments show the potential Time-weighted kernel k k -means has in evolving environments.},
  archive      = {J_PR},
  author       = {Sebastián Maldonado and Ramiro Saltos and Carla Vairetti and José Delpiano},
  doi          = {10.1016/j.patcog.2022.109058},
  journal      = {Pattern Recognition},
  pages        = {109058},
  shortjournal = {Pattern Recognition},
  title        = {Mitigating the effect of dataset shift in clustering},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end kernel learning via generative random fourier
features. <em>PR</em>, <em>134</em>, 109057. (<a
href="https://doi.org/10.1016/j.patcog.2022.109057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Fourier features (RFFs) provide a promising way for kernel learning in a spectral case. Current RFFs-based kernel learning methods usually work in a two-stage way. In the first-stage process, learning an optimal feature map is often formulated as a target alignment problem, which aims to align the learned kernel with a pre-defined target kernel (usually the ideal kernel). In the second-stage process, a linear learner is conducted with respect to the mapped random features. Nevertheless, the pre-defined kernel in target alignment is not necessarily optimal for the generalization of the linear learner. Instead, in this paper, we consider a one-stage process that incorporates the kernel learning and linear learner into a unifying framework. To be specific, a generative network via RFFs is devised to implicitly learn the kernel, followed by a linear classifier parameterized as a full-connected layer. Then the generative network and the classifier are jointly trained by solving an empirical risk minimization (ERM) problem to reach a one-stage solution. This end-to-end scheme naturally allows deeper features, in correspondence to a multi-layer structure, and shows superior generalization performance over the classical two-stage, RFFs-based methods in real-world classification tasks . Moreover, inspired by the randomized resampling mechanism of the proposed method, its enhanced adversarial robustness is investigated and experimentally verified.},
  archive      = {J_PR},
  author       = {Kun Fang and Fanghui Liu and Xiaolin Huang and Jie Yang},
  doi          = {10.1016/j.patcog.2022.109057},
  journal      = {Pattern Recognition},
  pages        = {109057},
  shortjournal = {Pattern Recognition},
  title        = {End-to-end kernel learning via generative random fourier features},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving adversarial robustness by learning shared
information. <em>PR</em>, <em>134</em>, 109054. (<a
href="https://doi.org/10.1016/j.patcog.2022.109054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of improving the adversarial robustness of neural networks while retaining natural accuracy. Motivated by the multi-view information bottleneck formalism, we seek to learn a representation that captures the shared information between clean samples and their corresponding adversarial samples while discarding these samples’ view-specific information. We show that this approach leads to a novel multi-objective loss function, and we provide mathematical motivation for its components towards improving the robust vs. natural accuracy tradeoff. We demonstrate enhanced tradeoff compared to current state-of-the-art methods with extensive evaluation on various benchmark image datasets and architectures. Ablation studies indicate that learning shared representations is key to improving performance.},
  archive      = {J_PR},
  author       = {Xi Yu and Niklas Smedemark-Margulies and Shuchin Aeron and Toshiaki Koike-Akino and Pierre Moulin and Matthew Brand and Kieran Parsons and Ye Wang},
  doi          = {10.1016/j.patcog.2022.109054},
  journal      = {Pattern Recognition},
  pages        = {109054},
  shortjournal = {Pattern Recognition},
  title        = {Improving adversarial robustness by learning shared information},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density peaks clustering based on balance density and
connectivity. <em>PR</em>, <em>134</em>, 109052. (<a
href="https://doi.org/10.1016/j.patcog.2022.109052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) algorithm regards the density peaks as the potential cluster centers, and assigns the non-center point into the cluster of its nearest higher-density neighbor. Although DPC can discover clusters with arbitrary shapes, it has some limitations. On the one hand, the density measure of DPC fails to eliminate the density difference among different clusters, which affects the accuracy of recognizing cluster center. On the other hand, the nearest higher-density point is determined without considering connectivity, which leads to continuously clustering errors. Therefore, DPC fails to obtain satisfactory clustering results on datasets with great density difference among clusters. In order to eliminate these limitations, a novel DPC algorithm based on balance density and connectivity (BC-DPC) is proposed. First, the balance density is proposed to eliminate the density difference among different clusters to accurately recognize cluster centers. Second, the connectivity between a data point and its nearest higher-density point is guaranteed by mutual nearest neighbor relationship to avoid continuously clustering errors. Finally, a fast search strategy is proposed to find the nearest higher-density point. The experimental results on synthetic, UCI, and image datasets demonstrate the efficiency and effectiveness of the proposed algorithm in this paper.},
  archive      = {J_PR},
  author       = {Qinghua Zhang and Yongyang Dai and Guoyin Wang},
  doi          = {10.1016/j.patcog.2022.109052},
  journal      = {Pattern Recognition},
  pages        = {109052},
  shortjournal = {Pattern Recognition},
  title        = {Density peaks clustering based on balance density and connectivity},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stage image denoising with the wavelet transform.
<em>PR</em>, <em>134</em>, 109050. (<a
href="https://doi.org/10.1016/j.patcog.2022.109050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) are used for image denoising via automatically mining accurate structure information. However, most of existing CNNs depend on enlarging depth of designed networks to obtain better denoising performance, which may cause training difficulty. In this paper, we propose a multi-stage image denoising CNN with the wavelet transform (MWDCNN) via three stages, i.e., a dynamic convolutional block (DCB), two cascaded wavelet transform and enhancement blocks (WEBs) and a residual block (RB). DCB uses a dynamic convolution to dynamically adjust parameters of several convolutions for making a tradeoff between denoising performance and computational costs. WEB uses a combination of signal processing technique (i.e., wavelet transformation) and discriminative learning to suppress noise for recovering more detailed information in image denoising. To further remove redundant features, RB is used to refine obtained features for improving denoising effects and reconstruct clean images via improved residual dense architectures. Experimental results show that the proposed MWDCNN outperforms some popular denoising methods in terms of quantitative and qualitative analysis. Codes are available at https://github.com/hellloxiaotian/MWDCNN.},
  archive      = {J_PR},
  author       = {Chunwei Tian and Menghua Zheng and Wangmeng Zuo and Bob Zhang and Yanning Zhang and David Zhang},
  doi          = {10.1016/j.patcog.2022.109050},
  journal      = {Pattern Recognition},
  pages        = {109050},
  shortjournal = {Pattern Recognition},
  title        = {Multi-stage image denoising with the wavelet transform},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Unauthorized AI cannot recognize me: Reversible adversarial
example. <em>PR</em>, <em>134</em>, 109048. (<a
href="https://doi.org/10.1016/j.patcog.2022.109048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a new methodology to control how user’s data is recognized and used by AI via exploiting the properties of adversarial examples . For this purpose, we propose reversible adversarial example (RAE), a new type of adversarial example. A remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation. On the other hand, other unauthorized AI models cannot recognize it correctly because it functions as an adversarial example. Moreover, RAE can be considered as one type of encryption to computer vision since reversibility guarantees the decryption. To realize RAE, we combine three technologies, adversarial example, reversible data hiding for exact recovery of adversarial perturbation, and encryption for selective control of AIs who can remove adversarial perturbation. Experimental results show that the proposed method can achieve comparable attack ability with the corresponding adversarial attack method and similar visual quality with the original image, including white-box attacks and black-box attacks.},
  archive      = {J_PR},
  author       = {Jiayang Liu and Weiming Zhang and Kazuto Fukuchi and Youhei Akimoto and Jun Sakuma},
  doi          = {10.1016/j.patcog.2022.109048},
  journal      = {Pattern Recognition},
  pages        = {109048},
  shortjournal = {Pattern Recognition},
  title        = {Unauthorized AI cannot recognize me: Reversible adversarial example},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning for image inpainting: A survey. <em>PR</em>,
<em>134</em>, 109046. (<a
href="https://doi.org/10.1016/j.patcog.2022.109046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting has been widely exploited in the field of computer vision and image processing . The main purpose of image inpainting is to produce visually plausible structure and texture for the missing regions of damaged images. In the past decade, the success of deep learning has brought new opportunities to many vision tasks, which promoted the development of a large number of deep learning-based image inpainting methods. Although these methods have many similarities, they also have their own characteristics due to the differences in data types , application scenarios, computing platforms, etc. It is necessary to classify and summarize these methods to provide a reference for the research community. In this survey, we present a comprehensive overview of recent advances in deep learning-based image inpainting. First, we categorize the deep learning-based techniques from multiple perspectives: inpainting strategies, network structures, and loss functions. Second, we summarize the open source codes and representative public datasets, and introduce the evaluation metrics for quantitative comparisons . Third, we summarize the real-world applications of image inpainting in different scenarios, and give a detailed analysis on the performance of different inpainting algorithms. At last, we conclude the survey and discuss about the future directions.},
  archive      = {J_PR},
  author       = {Hanyu Xiang and Qin Zou and Muhammad Ali Nawaz and Xianfeng Huang and Fan Zhang and Hongkai Yu},
  doi          = {10.1016/j.patcog.2022.109046},
  journal      = {Pattern Recognition},
  pages        = {109046},
  shortjournal = {Pattern Recognition},
  title        = {Deep learning for image inpainting: A survey},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy superpixel-based image segmentation. <em>PR</em>,
<em>134</em>, 109045. (<a
href="https://doi.org/10.1016/j.patcog.2022.109045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a multi-phase image segmentation methodology based on fuzzy superpixel decomposition, aggregation and merging. First, a collection of layers of dense fuzzy superpixels is generated by the variational fuzzy decomposition algorithm. Then a layer of refined superpixels is extracted by aggregating various layers of dense fuzzy superpixels using the hierarchical normalized cuts. Finally, the refined superpixels are projected into the low dimensional feature spaces by the multidimensional scaling and the segmentation result is obtained via the mean-shift-based merging approach with the spatial bandwidth adjustment strategy. Our algorithm utilizes the superimposition of fuzzy superpixels to impose more accurate spatial constraints on the final segmentation through the fuzzy superpixel aggregation. The fuzziness of superpixels also provides spatial features to measure affinities between fuzzy superpixels and refined superpixels, and guide the merging process. Comparative experiments with the existing approaches reveal a superior performance of the proposed method.},
  archive      = {J_PR},
  author       = {Tsz Ching Ng and Siu Kai Choy and Shu Yan Lam and Kwok Wai Yu},
  doi          = {10.1016/j.patcog.2022.109045},
  journal      = {Pattern Recognition},
  pages        = {109045},
  shortjournal = {Pattern Recognition},
  title        = {Fuzzy superpixel-based image segmentation},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tripartite sub-image histogram equalization for slightly low
contrast gray-tone image enhancement. <em>PR</em>, <em>134</em>, 109043.
(<a href="https://doi.org/10.1016/j.patcog.2022.109043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a neoteric tripartite sub-image histogram equalization method is proposed to enhance slightly low contrast gray-tone images, which is a less explored area in the literature. An image is decomposed into three sub-images to preserve its mean brightness, and the histograms of the sub-images are calculated. Then, the snipping procedure is applied to each histogram to constrain the pace of contrast enhancement. Subsequently, the equalization of the three histograms is performed independently, and finally, the three equalized sub-images are composed into a single image. The proposed method offers better outcomes as compared to several common and state-of-the-art histogram equalization-based methods regarding contrast improvement, blind/reference-less image spatial quality evaluator , mean brightness preservation, peak signal-to-noise ratio, mean structural similarity, gradient magnitude similarity deviation, feature similarity, bit-plane to bit-plane similarity, and visual image quality.},
  archive      = {J_PR},
  author       = {Hafijur Rahman and Gour Chandra Paul},
  doi          = {10.1016/j.patcog.2022.109043},
  journal      = {Pattern Recognition},
  pages        = {109043},
  shortjournal = {Pattern Recognition},
  title        = {Tripartite sub-image histogram equalization for slightly low contrast gray-tone image enhancement},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional GAN with 3D discriminator for MRI generation of
alzheimer’s disease progression. <em>PR</em>, <em>133</em>, 109061. (<a
href="https://doi.org/10.1016/j.patcog.2022.109061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies aim to predict the degree of deformation on affected brain regions as Alzheimer’s disease (AD) progresses. However, those studies have been often limited since it is difficult to obtain sequential longitudinal MR data of affected patients. Recently, conditional generative adversarial networks (cGANs) have been used to estimate the changes between unpaired images by modeling their differences. However, generating high-quality 3D magnetic resonance (MR) brain images with cGANs requires a large amount of computation. Previous models have been mostly designed to operate in 2D space taking individual slices or down-sampled 3D space, but these approaches often cause spatial artifacts such as discontinuities between slices or unnatural changes in 3D space. To address these limitations, we propose a novel cGAN that can synthesize high-quality 3D MR images at different stages of AD by integrating an additional module that ensures smooth and realistic transitions in 3D space. Specifically, the proposed cGAN model consists of an attention-based 2D generator, a 2D discriminator , and a 3D discriminator that is able to synthesize continuous 2D slices along the axial view resulting in good quality 3D MR volumes. Moreover, we propose an adaptive identity loss so that relevant transformations take place without compromising the features to identify patients. In our experiments, the proposed method showed better image generation performance than previously proposed GAN methods in terms of image quality and image generation suitable for the condition.},
  archive      = {J_PR},
  author       = {Euijin Jung and Miguel Luna and Sang Hyun Park},
  doi          = {10.1016/j.patcog.2022.109061},
  journal      = {Pattern Recognition},
  pages        = {109061},
  shortjournal = {Pattern Recognition},
  title        = {Conditional GAN with 3D discriminator for MRI generation of alzheimer’s disease progression},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous label distribution learning. <em>PR</em>,
<em>133</em>, 109056. (<a
href="https://doi.org/10.1016/j.patcog.2022.109056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label distribution learning (LDL) is a suitable paradigm to deal with label ambiguity through learning the correlations among different labels. Most existing label distribution learning methods consider the labels to be discrete and directly establish the mapping from features to labels. However, in many real-world applications, labels naturally form a continuous distribution, which is ignored by the existing methods. As a result, the distribution information of labels can not be accurately described and finally affects the whole learning system. The goal of this paper is to propose a novel approach which can capture the continuous distribution of different labels explicitly and effectively. Specifically, we propose Continuous Label Distribution Learning (CLDL) which describes labels as a continuous density function and learns the distribution information of the labels in the latent space. In this way, the high-order correlations among different labels can be effectively extracted and only a few parameters for describing the continuous distribution need to be learned. Extensive description degree prediction experiments on real-world datasets validate the superiority of CLDL over the existing approaches.},
  archive      = {J_PR},
  author       = {Xingyu Zhao and Yuexuan An and Ning Xu and Xin Geng},
  doi          = {10.1016/j.patcog.2022.109056},
  journal      = {Pattern Recognition},
  pages        = {109056},
  shortjournal = {Pattern Recognition},
  title        = {Continuous label distribution learning},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disentangling the correlated continuous and discrete
generative factors of data. <em>PR</em>, <em>133</em>, 109055. (<a
href="https://doi.org/10.1016/j.patcog.2022.109055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data typically include discrete generative factors, such as category labels and the existence of objects, as well as continuous generative factors. Continuous generative factors may be dependent on or independent of discrete generative factors. For instance, an intra-class variation of a category is dependent on the discrete generative factor, whereas a common variation of all categories is not. Most previous attempts to integrate discrete generative factors into disentanglement assumed statistical independence between the continuous and discrete variables. In this paper, we propose a Variational Autoencoder(VAE) model capable of disentangling both continuous generative factors. To represent these generative factors, we introduce two sets of continuous latent variables: a private variable and a public variable . The private and public variables represent the intra-class variations and common variations in categories, respectively. Our proposed framework models the private variable as a Gaussian mixture and the public variable as a Gaussian. Each mode of the private variable is responsible for a class of discrete variables. Our proposed model, called Discond-VAE , DISentangles the class-dependent CONtinuous factors from the Discrete factors by introducing private variables. The experiments showed that Discond-VAE could discover private and public factors from the data. Moreover, even under the dataset with only public factors, Discond-VAE does not fail and adapts private variables to represent public factors.},
  archive      = {J_PR},
  author       = {Jaewoong Choi and Geonho Hwang and Myungjoo Kang},
  doi          = {10.1016/j.patcog.2022.109055},
  journal      = {Pattern Recognition},
  pages        = {109055},
  shortjournal = {Pattern Recognition},
  title        = {Disentangling the correlated continuous and discrete generative factors of data},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lane detection with versatile AtrousFormer and local
semantic guidance. <em>PR</em>, <em>133</em>, 109053. (<a
href="https://doi.org/10.1016/j.patcog.2022.109053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lane detection is one of the core functions in autonomous driving and has aroused widespread attention recently. The networks to segment lane instances, especially with bad appearance, must be able to explore lane distribution properties. Most existing methods tend to resort to CNN-based techniques. A few have a try on incorporating the recent adorable, the seq2seq Transformer [1]. However, their innate drawbacks of weak global information collection ability and exorbitant computation overhead prohibit a wide range of the further applications. In this work, we propose Global Atrous Transformer (AtrousFormer) to solve the problem. Its variant local AtrousFormer is interleaved into feature extractor to enhance extraction. Their collecting information first by rows and then by columns in a dedicated manner finally equips our network with stronger information gleaning ability and better computation efficiency. To further improve the performance, we also propose a local semantic guided decoder to delineate the identities and shapes of lanes more accurately. Extensive results on three challenging benchmarks (CULane, TuSimple, and BDD100K) show that our network performs favorably against the state of the arts.},
  archive      = {J_PR},
  author       = {Jiaxing Yang and Lihe Zhang and Huchuan Lu},
  doi          = {10.1016/j.patcog.2022.109053},
  journal      = {Pattern Recognition},
  pages        = {109053},
  shortjournal = {Pattern Recognition},
  title        = {Lane detection with versatile AtrousFormer and local semantic guidance},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DcTr: Noise-robust point cloud completion by dual-channel
transformer with cross-attention. <em>PR</em>, <em>133</em>, 109051. (<a
href="https://doi.org/10.1016/j.patcog.2022.109051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current point cloud completion research mainly utilizes the global shape representation and local features to recover the missing regions of 3D shape for the partial point cloud. However, these methods suffer from inefficient utilization of local features and unstructured points prediction in local patches, hardly resulting in a well-arranged structure for points. To tackle these problems, we propose to employ Dual-channel Transformer and Cross-attention (CA) for point cloud completion (DcTr). The DcTr is apt at using local features and preserving a well-structured generation process. Specifically, the dual-channel transformer leverages point-wise attention and channel-wise attention to summarize the deconvolution patterns used in the previous Dual-channel Transformer Point Deconvolution (DCTPD) stage to produce the deconvolution in the current DCTPD stage. Meanwhile, we employ cross-attention to convey the geometric information from the local regions of incomplete point clouds for the generation of complete ones at different resolutions. In this way, we can generate the locally compact and structured point cloud by capturing the structure characteristic of 3D shape in local patches. Our experimental results indicate that DcTr outperforms the state-of-the-art point cloud completion methods under several benchmarks and is robust to various kinds of noise.},
  archive      = {J_PR},
  author       = {Ben Fei and Weidong Yang and Lipeng Ma and Wen-Ming Chen},
  doi          = {10.1016/j.patcog.2022.109051},
  journal      = {Pattern Recognition},
  pages        = {109051},
  shortjournal = {Pattern Recognition},
  title        = {DcTr: Noise-robust point cloud completion by dual-channel transformer with cross-attention},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Query-guided networks for few-shot fine-grained
classification and person search. <em>PR</em>, <em>133</em>, 109049. (<a
href="https://doi.org/10.1016/j.patcog.2022.109049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot fine-grained classification and person search appear as distinct tasks and literature has treated them separately. But a closer look unveils important similarities: both tasks target categories that can only be discriminated by specific object details; and the relevant models should generalize to new categories, not seen during training. We propose a novel unified Query-Guided Network (QGN) applicable to both tasks. QGN consists of a Query-guided Siamese-Squeeze-and-Excitation subnetwork which re-weights both the query and gallery features across all network layers, a Query-guided Region Proposal subnetwork for query-specific localisation , and a Query-guided Similarity subnetwork for metric learning. QGN improves on a few recent few-shot fine-grained datasets, outperforming other techniques on CUB by a large margin. QGN also performs competitively on the person search CUHK-SYSU and PRW datasets, where we perform in-depth analysis.},
  archive      = {J_PR},
  author       = {Bharti Munjal and Alessandro Flaborea and Sikandar Amin and Federico Tombari and Fabio Galasso},
  doi          = {10.1016/j.patcog.2022.109049},
  journal      = {Pattern Recognition},
  pages        = {109049},
  shortjournal = {Pattern Recognition},
  title        = {Query-guided networks for few-shot fine-grained classification and person search},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From distortion manifold to perceptual quality: A data
efficient blind image quality assessment approach. <em>PR</em>,
<em>133</em>, 109047. (<a
href="https://doi.org/10.1016/j.patcog.2022.109047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though current no-reference image quality assessment (NR-IQA) approaches have achieved impressive performance gain thanks to deep learning techniques, it is claimed that the risk of over-fitting exists. To improve model generalization ability , most of the current researches incorporate mass data to train or tune the data-driven models. However, the process of image data collection and quality label annotation is quite time-consuming and labour-intensive. Therefore, in this paper, we explore an alternative solution to promote model generalizability but with relatively small fractions of training data. Compared with previous approaches which make effort to approximate the whole complex image distribution, we propose to explicitly learn an image distortion manifold first, which lies in a much lower dimension space and also representative in capturing general degradation patterns. We then project the images to their perceived quality from the learned manifold to obtain quality predictions. Since the manifold embeds general distortion features despite of varying image contents, it can be learned with relatively small amount of samples. In order to learn the manifold and quality projection, we introduce a two-branched network to learn both low level distortions and high level semantics . We also propose a simple but efficient training framework, composing of a masked labelling strategy and a gradual weighting curriculum to fulfill the task. Thanks to the learned distortion manifold, the proposed model achieves superior generalizability compared with previous models. Extensive experiments demonstrate its effectiveness in terms of training with limited data, testing on large scale images, and with unseen types of distorted images.},
  archive      = {J_PR},
  author       = {Shaolin Su and Qingsen Yan and Yu Zhu and Jinqiu Sun and Yanning Zhang},
  doi          = {10.1016/j.patcog.2022.109047},
  journal      = {Pattern Recognition},
  pages        = {109047},
  shortjournal = {Pattern Recognition},
  title        = {From distortion manifold to perceptual quality: A data efficient blind image quality assessment approach},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rethinking referring relationships from a perspective of
mask-level relational reasoning. <em>PR</em>, <em>133</em>, 109044. (<a
href="https://doi.org/10.1016/j.patcog.2022.109044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring relationship aims at localizing subject and object entities in an image, according to a triple text &amp;lt; subject, predicate, object &gt; &amp;gt; . Previous methods use iterative attention to shift between image regions for modeling predicate. However, predicate sometimes is implicit and difficult to be represented in the image domain. Convolution modeling method to express predicate is simple and inappropriate. Besides, relational reasoning information in the text itself is not fully utilized. To this end, we rethink referring relationship from a mask-level relational reasoning perspective to improve model interpretability . For text-to-image reasoning, we design Mask Generate and Mask Transfer modules, so as to fully integrate the text priors into the reasoning and prediction of masks. For image-to-text reasoning, we propose an unsupervised triple reconstruction method to guide text-to-image reasoning and improve multimodal generalization. By bi-directional reasoning between image and text, the proposed method MRR fully conforms to the multimodal relational reasoning process. Experiments show that MRR achieves state-of-the-art performance on two datasets of referring relationships, VRD and Visual Genome.},
  archive      = {J_PR},
  author       = {Chengyang Li and Liping Zhu and Gangyi Tian and Yi Hou and Heng Zhou},
  doi          = {10.1016/j.patcog.2022.109044},
  journal      = {Pattern Recognition},
  pages        = {109044},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking referring relationships from a perspective of mask-level relational reasoning},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploratory adversarial attacks on graph neural networks for
semi-supervised node classification. <em>PR</em>, <em>133</em>, 109042.
(<a href="https://doi.org/10.1016/j.patcog.2022.109042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been successfully used to analyze non-Euclidean network data. Recently, there emerge a number of works to investigate the robustness of GNNs by adding adversarial noises into the graph topology , where the gradient-based attacks are widely studied due to their inherent efficiency and high effectiveness. However, the gradient-based attacks often lead to sub-optimal results due to the discrete structure of graph data. To address this issue, we propose a novel exploratory adversarial attack (termed as EpoAtk ) to boost the gradient-based perturbations on graphs. The exploratory strategy in EpoAtk includes three phases, generation, evaluation and recombination , with the goal of sidestepping the possible misinformation that the maximal gradient provides. In particular, our evaluation phase introduces a self-training objective containing three effective evaluation functions to fully exploit the useful information of unlabeled nodes. EpoAtk is evaluated on multiple benchmark datasets for the task of semi-supervised node classification in different attack settings. Extensive experimental results demonstrate that the proposed method achieves consistent and significant improvements over the state-of-the-art adversarial attacks with the same attack budgets.},
  archive      = {J_PR},
  author       = {Xixun Lin and Chuan Zhou and Jia Wu and Hong Yang and Haibo Wang and Yanan Cao and Bin Wang},
  doi          = {10.1016/j.patcog.2022.109042},
  journal      = {Pattern Recognition},
  pages        = {109042},
  shortjournal = {Pattern Recognition},
  title        = {Exploratory adversarial attacks on graph neural networks for semi-supervised node classification},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigating intrinsic degradation factors by multi-branch
aggregation for real-world underwater image enhancement. <em>PR</em>,
<em>133</em>, 109041. (<a
href="https://doi.org/10.1016/j.patcog.2022.109041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, improving the visual quality of underwater images has received extensive attentions in both computer vision and ocean engineering fields. However, existing works mostly focus on directly learning clear images from degraded observations but without careful investigations on the intrinsic degradation factors, thus require mass training data and lack generalization ability . In this work, we propose a new method, named Multi-Branch Aggregation Network (termed as MBANet) to partially address the above issue. Specifically, by analyzing underwater degradation factors from the perspective of both color distortions and veil effects, MBANet first constructs a multi-branch multi-variable architecture to obtain one intermediate coarse result and two degraded factors. We then establish a physical model inspired process to fully utilize our estimated degraded factors and thus obtain the desired clear output images. A series of evaluations on multiple datasets show the superiority of our method against existing state-of-the-art approaches, both in execution speed and accuracy. Furthermore, we demonstrate that our MBANet can significantly improve the performance of salience object detection in the underwater environment.},
  archive      = {J_PR},
  author       = {Xinwei Xue and Zexuan Li and Long Ma and Qi Jia and Risheng Liu and Xin Fan},
  doi          = {10.1016/j.patcog.2022.109041},
  journal      = {Pattern Recognition},
  pages        = {109041},
  shortjournal = {Pattern Recognition},
  title        = {Investigating intrinsic degradation factors by multi-branch aggregation for real-world underwater image enhancement},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image inpainting via spatial projections. <em>PR</em>,
<em>133</em>, 109040. (<a
href="https://doi.org/10.1016/j.patcog.2022.109040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting is now-a-days sought after due to its wide variety of applications in the reconstruction of the corrupted image, occlusion removal, reflection removal, etc. Existing image inpainting approaches utilize different types of attention mechanisms to inpaint the image and produce visibly admirable results. These methods are more concerned at weighing the feature maps of the hole region with some weight from the non-hole region. But, due to the lack of spatial contextual correlation in the attention maps, the inpainted image may suffer from the inconsistencies among hole and non-hole regions. Transformer-based inpainting methods give significant results by capturing the relationship between the patches with a compromise of high computational complexity . In this context, we propose a novel spatial projection layer (SPL) without any attention mechanism to project the spatial contextual information in the hole region from non-hole regions for producing a spatially plausible inpainted image. The SPL is proposed mainly to focus on the non-hole spatial information in the high-level feature maps for filling the hole regions efficiently. Also, while training the network, we propose the use of edge loss with a Canny edge operator for image inpainting to focus on the relevant edges instead of noise contents. Analysis with the extensive experiments, ablation, and user study on the proposed architecture demonstrates the superiority over existing state-of-the-art methods for image inpainting. The code is available at: https://github.com/shrutiphutke/spatial_projection_inpainting .},
  archive      = {J_PR},
  author       = {Shruti S Phutke and Subrahmanyam Murala},
  doi          = {10.1016/j.patcog.2022.109040},
  journal      = {Pattern Recognition},
  pages        = {109040},
  shortjournal = {Pattern Recognition},
  title        = {Image inpainting via spatial projections},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LAE-net: A locally-adaptive embedding network for low-light
image enhancement. <em>PR</em>, <em>133</em>, 109039. (<a
href="https://doi.org/10.1016/j.patcog.2022.109039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the low-light enhancement task, one of the major challenges lies in how to balance the image enhancement properties of light intensity, detail presentation and color fidelity. In natural scenes, the multi-distribution of frequency and illumination characteristics in the spatial domain makes the balance more difficult. To solve this problem, we propose a Locally-Adaptive Embedding Network, namely LAE-Net, to realize high-quality low-light image enhancement with locally-adaptive kernel selection and feature adaptation for multi-distribution issues. Specifically, for the frequency multi-distribution, we rethink the spatial-frequency characteristic of human eyes, experimentally explore the relationship among the receptive field size, the image spatial frequency and the light enhancement properties, and propose an Entropy-Inspired Kernel-Selection Convolution, where each neuron can adaptively adjust the receptive field size according to its spatial frequency characterized by information entropy. For the illumination multi-distribution, we propose an Illumination Attentive Transfer subnet, where the neurons can simultaneously sense global consistency and local details, and accordingly hint where to focus the efforts on, thereby adjusting the refined features. Extensive experiments with ablation analysis show the effectiveness of our method and the proposed method outperforms many related state-of-the-art techniques on four benchmark datasets: MEF, LIME, NPE and DICM.},
  archive      = {J_PR},
  author       = {Xiaokai Liu and Weihao Ma and Xiaorui Ma and Jie Wang},
  doi          = {10.1016/j.patcog.2022.109039},
  journal      = {Pattern Recognition},
  pages        = {109039},
  shortjournal = {Pattern Recognition},
  title        = {LAE-net: A locally-adaptive embedding network for low-light image enhancement},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single image super‐resolution based on progressive fusion of
orientation‐aware features. <em>PR</em>, <em>133</em>, 109038. (<a
href="https://doi.org/10.1016/j.patcog.2022.109038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution (SISR) is an active research topic in the fields of image processing , computer vision and pattern recognition, restoring high-frequency details and textures based on the low-resolution input image. In this paper, we aim to build more accurate and faster SISR models via developing better-performing feature extraction and fusion techniques. Firstly, we proposed a novel Orientation-Aware feature extraction/selection Module (OAM), which contains a mixture of 1D and 2D convolutional kernels (i.e., 3 × 1 3×1 , 1 × 3 1×3 , and 3 × 3 3×3 ) for extracting orientation-aware features. The channel attention mechanism is deployed within each OAM, performing scene-specific selection of informative outputs of the orientation-dependent kernels (e.g., horizontal, vertical, and diagonal). Secondly, we present an effective fusion architecture to progressively integrate multi-scale features extracted in different convolutional stages. Instead of directly combining low-level and high-level features, similar outputs of adjacent feature extraction modules are grouped and further compressed to generate a more concise representation of a specific convolutional stage for high-accuracy SISR task. Based on the above two important improvements, we present a compact but effective CNN-based model for high-quality SISR via Progressive Fusion of Orientation-Aware features (SISR-PF-OA). Extensive experimental results verify the superiority of the proposed SISR-PF-OA model, performing favorably against the state-of-the-art models in terms of both restoration accuracy and computational efficiency (e.g., SISR-PF-OA outperforms RCAN model, achieving higher PSNR 31.25 dB vs. 31.21 dB and using fewer FLOPs 764.41 G vs. 1020.28 G on the Manga109 dataset for scale factor × 4 ×4 SISR task.). The source codes will be made publicly available.},
  archive      = {J_PR},
  author       = {Zewei He and Du Chen and Yanpeng Cao and Jiangxin Yang and Yanlong Cao and Xin Li and Siliang Tang and Yueting Zhuang and Zhe-ming Lu},
  doi          = {10.1016/j.patcog.2022.109038},
  journal      = {Pattern Recognition},
  pages        = {109038},
  shortjournal = {Pattern Recognition},
  title        = {Single image super‐resolution based on progressive fusion of orientation‐aware features},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Query efficient black-box adversarial attack on deep neural
networks. <em>PR</em>, <em>133</em>, 109037. (<a
href="https://doi.org/10.1016/j.patcog.2022.109037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have demonstrated excellent performance on various tasks, yet they are under the risk of adversarial examples that can be easily generated when the target model is accessible to an attacker (white-box setting). As plenty of machine learning models have been deployed via online services that only provide query outputs from inaccessible models ( e.g. , Google Cloud Vision API2), black-box adversarial attacks raise critical security concerns in practice rather than white-box ones. However, existing query-based black-box adversarial attacks often require excessive model queries to maintain a high attack success rate. Therefore, in order to improve query efficiency, we explore the distribution of adversarial examples around benign inputs with the help of image structure information characterized by a Neural Process, and propose a Neural Process based black-box adversarial attack (NP-Attack) in this paper. Our proposed NP-Attack could be further boosted when applied with surrogate models or tiling tricks. Extensive experiments show that NP-Attack could greatly decrease the query counts under the black-box setting.},
  archive      = {J_PR},
  author       = {Yang Bai and Yisen Wang and Yuyuan Zeng and Yong Jiang and Shu-Tao Xia},
  doi          = {10.1016/j.patcog.2022.109037},
  journal      = {Pattern Recognition},
  pages        = {109037},
  shortjournal = {Pattern Recognition},
  title        = {Query efficient black-box adversarial attack on deep neural networks},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DisRFC: A dissimilarity-based random forest clustering
approach. <em>PR</em>, <em>133</em>, 109036. (<a
href="https://doi.org/10.1016/j.patcog.2022.109036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a novel Random Forest Clustering approach , called Dissimilarity Random Forest Clustering (DisRFC) , which requires in input only pairwise dissimilarities. Thanks to this characteristic, the proposed approach is appliable to all those problems which involve non-vectorial representations, such as strings, sequences, graphs or 3D structures. In the proposed approach, we first train an Unsupervised Dissimilarity Random Forest (UD-RF), a novel variant of Random Forest which is completely unsupervised and based on dissimilarities. Then, we exploit the trained UD-RF to project the patterns to be clustered in a binary vectorial space, where the clustering is finally derived using fast and effective K-means procedures. In the paper we introduce different variants of DisRFC, which are thoroughly and positively evaluated on 12 different problems, also in comparison with alternative state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Manuele Bicego},
  doi          = {10.1016/j.patcog.2022.109036},
  journal      = {Pattern Recognition},
  pages        = {109036},
  shortjournal = {Pattern Recognition},
  title        = {DisRFC: A dissimilarity-based random forest clustering approach},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Game-theoretic hypergraph matching with density
enhancement. <em>PR</em>, <em>133</em>, 109035. (<a
href="https://doi.org/10.1016/j.patcog.2022.109035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching plays a fundamental role in computer vision and pattern recognition. As straightforward comparison of feature descriptors is not enough to provide reliable matching results in many situations, graph matching makes use of the pairwise relationship between features to improve matching accuracy. Hypergraph matching further employs the relationship among multiple features to provide more invariance between feature correspondences. Existing hypergraph matching algorithms usually solve an assignment problem, where outliers may result in a large number of false matches. In this paper we cast the hypergraph matching problem as a non-cooperative multi-player game, and obtain the matches by extracting the evolutionary stable strategies. Our algorithm exerts a strong constraint on the consistency of obtained matches, and false matches are excluded effectively. In order to increase the number of matches without increasing the computation load evidently, we present a density enhancement method to improve the matching results. We further propose two methods to enforce the one-to-one constraint, thereby removing false matches and maintaining a high matching accuracy. Experiments with both synthetic and real datasets validate the effectiveness of our algorithm.},
  archive      = {J_PR},
  author       = {Jian Hou and Huaqiang Yuan and Marcello Pelillo},
  doi          = {10.1016/j.patcog.2022.109035},
  journal      = {Pattern Recognition},
  pages        = {109035},
  shortjournal = {Pattern Recognition},
  title        = {Game-theoretic hypergraph matching with density enhancement},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank 2D local discriminant graph embedding for robust
image feature extraction. <em>PR</em>, <em>133</em>, 109034. (<a
href="https://doi.org/10.1016/j.patcog.2022.109034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular feature extraction algorithm, the 2D local preserving projections (2DLPP) algorithm has been successfully applied in many fields. Using 2D image representation, the 2DLPP algorithm preserves the manifold attributes and retains the local information of high-dimensional space data. However, the 2DLPP algorithm may encounter some problems in real-world applications, such as a lack of discriminatory ability, singularity problems, and sensitivity to occlusion and noise in data. Therefore, this paper introduces low-rank into the 2DLPP algorithm and proposes a new feature extraction algorithm, which is the low-rank two-dimensional local discriminant graph embedding (LR-2DLDGE), to solve these problems. To improve the LR-2DLDGE algorithm robustness, we fuse the discriminant information in graph embedding and the low-rank properties of the data. The algorithm has three advantages: First, the algorithm uses a graph embedding (GE) framework to maintain the local neighbourhood discrimination information between data. Second, the LR-2DLDGE algorithm ensures that the data points are as independent as possible from different classes in the feature space. Finally, the algorithm uses the L 1 L1 -norm as a constraint and reduces the influence of noise and corruption through low-rank learning. The theoretical computational complexity and convergence of the algorithm are explicated and proved. Extensive experimental results on three occluded and noisy image datasets confirm the effectively and robustness of LR-2DLDGE, respectively.},
  archive      = {J_PR},
  author       = {Minghua Wan and Xueyu Chen and Tianming Zhan and Guowei Yang and Hai Tan and Hao Zheng},
  doi          = {10.1016/j.patcog.2022.109034},
  journal      = {Pattern Recognition},
  pages        = {109034},
  shortjournal = {Pattern Recognition},
  title        = {Low-rank 2D local discriminant graph embedding for robust image feature extraction},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AEDNet: Adaptive edge-deleting network for subgraph
matching. <em>PR</em>, <em>133</em>, 109033. (<a
href="https://doi.org/10.1016/j.patcog.2022.109033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgraph matching is to find all subgraphs in a data graph that are isomorphic to an existing query graph. Subgraph matching is an NP-hard problem, yet has found its applications in many areas. Many learning-based methods have been proposed for graph matching , whereas few have been designed for subgraph matching. The subgraph matching problem is generally more challenging, mainly due to the different sizes between the two graphs, resulting in considerable large space of solutions. Also the extra edges existing in the data graph connecting to the matched nodes may lead to two matched nodes of two graphs having different adjacency structures and often being identified as distinct objects. Due to the extra edges, the existing learning based methods often fail to generate sufficiently similar node-level embeddings for matched nodes. This study proposes a novel Adaptive Edge-Deleting Network (AEDNet) for subgraph matching. The proposed method is trained in an end-to-end fashion. In AEDNet, a novel sample-wise adaptive edge-deleting mechanism removes extra edges to ensure consistency of adjacency structure of matched nodes, while a unidirectional cross-propagation mechanism ensures consistency of features of matched nodes. We applied the proposed method on six datasets with graph sizes varying from 20 to 2300. Our evaluations on six open datasets demonstrate that the proposed AEDNet outperforms six state-of-the-arts and is much faster than the exact methods on large graphs.},
  archive      = {J_PR},
  author       = {Zixun Lan and Ye Ma and Limin Yu and Linglong Yuan and Fei Ma},
  doi          = {10.1016/j.patcog.2022.109033},
  journal      = {Pattern Recognition},
  pages        = {109033},
  shortjournal = {Pattern Recognition},
  title        = {AEDNet: Adaptive edge-deleting network for subgraph matching},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative learning with unreliability adaptation for
semi-supervised image classification. <em>PR</em>, <em>133</em>, 109032.
(<a href="https://doi.org/10.1016/j.patcog.2022.109032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing training goals for unlabeled data is crucial for image classification in the semi-supervised setting. Consistency regularization typically encourages a model to produce consistent predictions with the given training goals, while unreliability adaptation aims to learn the transition probabilities from model predictions to training goals, instead of enforcing their consistency. In this paper, we present a model of Collaborative learning with Unreliability Adaptation (CoUA), in which multiple constituent networks collaboratively learn with each other by adapting their predictions. Toward this end, an additional adaptation module is incorporated into each network to learn a transition probability from its own prediction to that of the paired network. Therefore, the networks can exchange training experience, without being overly sensitive to the unreliability of predictions. To further enhance the collaborative learning, each network is encouraged to produce consistent predictions with the consensus results, while being resistant to the adversarial perturbations against others. Therefore, the networks are able to mutually reinforce each other. We perform extensive experiments on multiple image classification benchmarks to verify the superiority of the co-adaptation based collaborative learning mechanism.},
  archive      = {J_PR},
  author       = {Xiaoyang Huo and Xiangping Zeng and Si Wu and Wenjun Shen and Hau-San Wong},
  doi          = {10.1016/j.patcog.2022.109032},
  journal      = {Pattern Recognition},
  pages        = {109032},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative learning with unreliability adaptation for semi-supervised image classification},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformed domain convolutional neural network for
alzheimer’s disease diagnosis using structural MRI. <em>PR</em>,
<em>133</em>, 109031. (<a
href="https://doi.org/10.1016/j.patcog.2022.109031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural magnetic resonance imaging (sMRI) has become a prevalent and potent imaging modality for the computer-aided diagnosis (CAD) of neurological diseases like dementia. Recently, a handful of deep learning techniques such as convolutional neural networks (CNNs) have been proposed to diagnose Alzheimer&#39;s disease (AD) by learning the atrophy patterns available in sMRIs. Although CNN-based techniques have demonstrated superior performance and characteristics compared to conventional learning-based classifiers, their diagnostic performance still needs to be improved for reliable classification results. The drawback of current CNN-based approaches is the requirement to locate discriminative landmark (LM) locations by identifying regions of interest (ROIs) in sMRIs, thus the performance of the whole framework is highly influenced by the LM detection step. To overcome this issue, we propose a novel three-dimensional Jacobian domain convolutional neural network (JD-CNN) to diagnose AD subjects and achieve excellent classification performance without the involvement of the LM detection framework. We train the proposed JD-CNN model on the basis of features generated by transforming the sMRI from the spatial domain to the Jacobian domain. The proposed JD-CNN is evaluated on baseline T1-weighted sMRI data collected from 154 healthy control (HC) and 84 Alzheimer&#39;s disease (AD) subjects in the Alzheimer&#39;s disease neuroimaging initiative (ADNI) database. The proposed JD-CNN exhibits superior classification performance to previously reported state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {S. Qasim Abbas and Lianhua Chi and Yi-Ping Phoebe Chen},
  doi          = {10.1016/j.patcog.2022.109031},
  journal      = {Pattern Recognition},
  pages        = {109031},
  shortjournal = {Pattern Recognition},
  title        = {Transformed domain convolutional neural network for alzheimer&#39;s disease diagnosis using structural MRI},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSR: Cascade conditional variational auto encoder with
socially-aware regression for pedestrian trajectory prediction.
<em>PR</em>, <em>133</em>, 109030. (<a
href="https://doi.org/10.1016/j.patcog.2022.109030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction is a key technology in many real applications such as video surveillance, social robot navigation , and autonomous driving , and significant progress has been made in this research topic. However, there remain two limitations of previous studies. First, the losses of the last time steps are heavier weighted than that of the beginning time steps in the objective function at the learning stage, causing the prediction errors generated at the beginning to accumulate to large errors at the last time steps at the inference stage. Second, the prediction results of multiple pedestrians in the prediction horizon might be socially incompatible with the interactions modeled by past trajectories. To overcome these limitations, this work proposes a novel trajectory prediction method called CSR, which consists of a cascaded conditional variational autoencoder (CVAE) module and a socially-aware regression module. The CVAE module estimates the future trajectories in a cascaded sequential manner. Specifically, each CVAE concatenates the past trajectories and the predicted location points so far as the input and predicts the adjacent location at the following time step. The socially-aware regression module generates offsets from the estimated future trajectories to produce the corrected predictions, which are more reasonable and accurate than the estimated trajectories. Experiments results demonstrate that the proposed method exhibits significant improvements over state-of-the-art methods on the Stanford Drone Dataset (SDD) and the ETH/UCY dataset of approximately 38.0\% and 22.2\%, respectively. The code is available at https://github.com/zhouhao94/CSR .},
  archive      = {J_PR},
  author       = {Hao Zhou and Dongchun Ren and Xu Yang and Mingyu Fan and Hai Huang},
  doi          = {10.1016/j.patcog.2022.109030},
  journal      = {Pattern Recognition},
  pages        = {109030},
  shortjournal = {Pattern Recognition},
  title        = {CSR: Cascade conditional variational auto encoder with socially-aware regression for pedestrian trajectory prediction},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale multi-hierarchy attention convolutional neural
network for fetal brain extraction. <em>PR</em>, <em>133</em>, 109029.
(<a href="https://doi.org/10.1016/j.patcog.2022.109029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fetal brain extraction from in utero magnetic resonance imaging (MRI) scans is a key step for fetal brain development analysis. As the unpredicted fetal motion and maternal breathing generally result in blurring and ghosting in the slices of phase encoding direction, using the conventional 3D convolutional neural networks for fetal brain extraction with pseudo 3D fetal brain MR scans will lead to sub-optimal extraction performance. To address this issue, in this paper, we propose a novel multi-scale multi-hierarchy attention convolutional neural network (MSMHA-CNN) for fetal brain extraction in MR images. Specifically, to effectively utilize the 3D contextual information of the in utero MR image for fetal brain extraction, we employ multiple convolutional operations with different local receptive fields ( i.e. , with different kernel sizes) in each layer to learn the multi-scale feature representation for fetal brain extraction. To effectively use the learned multi-scale feature maps, we introduce a channel-wise spatial attention architecture to adaptively fuse those multi-scale feature maps derived from convolutional operations with different kernel sizes. In this way, the learned multi-scale features can be explicitly used to fetal brain extraction process. Besides, to take advantage of high-level feature maps at all spatial resolutions, we adopt the feature pyramid architecture to learn multi-hierarchy features for boosting the performance. We compare our proposed method with several state-of-the-art methods on two in utero MRI scan datasets (a total of 180 scans) for fetal brain extraction. The experimental results suggest the superior performance of the proposed MSMHA-CNN in comparison with its competitors.},
  archive      = {J_PR},
  author       = {Liang Sun and Wei Shao and Qi Zhu and Meiling Wang and Gang Li and Daoqiang Zhang},
  doi          = {10.1016/j.patcog.2022.109029},
  journal      = {Pattern Recognition},
  pages        = {109029},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale multi-hierarchy attention convolutional neural network for fetal brain extraction},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Temporal sparse adversarial attack on sequence-based gait
recognition. <em>PR</em>, <em>133</em>, 109028. (<a
href="https://doi.org/10.1016/j.patcog.2022.109028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition is widely used in social security applications due to its advantages in long-distance human identification. Recently, sequence-based methods have achieved high accuracy by learning abundant temporal and spatial information. However, their robustness under adversarial attacks in an open world has not been clearly explored. In this paper, we demonstrate that the state-of-the-art gait recognition model is vulnerable to such attacks. To this end, we propose a novel temporal sparse adversarial attack method. Different from previous additive noise models which add perturbations on original samples, we employ a generative adversarial network based architecture to semantically generate adversarial high-quality gait silhouettes or video frames. Moreover, by sparsely substituting or inserting a few adversarial gait silhouettes, the proposed method ensures its imperceptibility and achieves a strong attack ability. The experimental results show that if only one-fortieth of the frames are attacked, the accuracy of the target model drops dramatically.},
  archive      = {J_PR},
  author       = {Ziwen He and Wei Wang and Jing Dong and Tieniu Tan},
  doi          = {10.1016/j.patcog.2022.109028},
  journal      = {Pattern Recognition},
  pages        = {109028},
  shortjournal = {Pattern Recognition},
  title        = {Temporal sparse adversarial attack on sequence-based gait recognition},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OW-TAL: Learning unknown human activities for open-world
temporal action localization. <em>PR</em>, <em>133</em>, 109027. (<a
href="https://doi.org/10.1016/j.patcog.2022.109027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current temporal action localization methods work well on a closed-world assumption, in which all action categories to be localized are known as a priori. However, this assumption doesn’t apply to open-world scenarios, as novel categories that never appeared in the training stage will be encountered without explicit supervision. Distinct from the closed-world setting, localizing actions under the open-world setup poses two significant challenges: 1) identifying unknown actions from diverse knowns and localizing their temporal boundaries. 2) defying forgetting of previous actions when incrementally updating knowledge of identified unknown actions. To address the aforementioned challenges, we develop a two-branch framework with Unknown and Known action modeling Networks , a.k.a. UK-Net, for the problem of Open-World Temporal Action Localization (OW-TAL). The potential patterns underlying unknown and known actions, as well as their dynamic transformation, are modeled in a unified pipeline . Specifically, a self-attention based position-sensitive module is designed to produce actionness scores for unknown actions in a class-agnostic way. Besides, an iterative optimization strategy is developed to enable knowledge derived from known categories to be shared with the unknowns. In addition, a self-paced learning strategy is proposed to instructionally guide class-incremental learning while defying catastrophic forgetting. Benefiting from the above components, our UK-Net yields superior performance on three challenging datasets, i.e., THUMOS14, ActivityNet1.2, and MUSES. Experimental results also demonstrate the competitive performance of our method when compared with traditional closed-world counterparts.},
  archive      = {J_PR},
  author       = {Yaru Zhang and Xiao-Yu Zhang and Haichao Shi},
  doi          = {10.1016/j.patcog.2022.109027},
  journal      = {Pattern Recognition},
  pages        = {109027},
  shortjournal = {Pattern Recognition},
  title        = {OW-TAL: Learning unknown human activities for open-world temporal action localization},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image manipulation detection by multiple tampering traces
and edge artifact enhancement. <em>PR</em>, <em>133</em>, 109026. (<a
href="https://doi.org/10.1016/j.patcog.2022.109026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image manipulation detection has attracted considerable attention owing to the increasing security risks posed by fake images. Previous studies have proven that tampering traces hidden in images are essential for detecting manipulated regions. However, existing methods have limitations in generalization and the ability to tackle post-processing methods. This paper presents a novel Network to learn and Enhance Multiple tampering Traces (EMT-Net), including noise distribution and visual artifacts. For better generalization, EMT-Net extracts global and local noise features from noise maps using transformers and captures local visual artifacts from original RGB images using convolutional neural networks . Moreover, we enhance fused tampering traces using the proposed edge artifacts enhancement modules and edge supervision strategy to discover subtle edge artifacts hidden in images. Thus, EMT-Net can prevent the risks of losing slight visual clues against well-designed post-processing methods. Experimental results indicate that the proposed method can detect manipulated regions and outperform state-of-the-art approaches under comprehensive quantitative metrics and visual qualities. In addition, EMT-Net shows robustness when various post-processing methods further manipulate images.},
  archive      = {J_PR},
  author       = {Xun Lin and Shuai Wang and Jiahao Deng and Ying Fu and Xiao Bai and Xinlei Chen and Xiaolei Qu and Wenzhong Tang},
  doi          = {10.1016/j.patcog.2022.109026},
  journal      = {Pattern Recognition},
  pages        = {109026},
  shortjournal = {Pattern Recognition},
  title        = {Image manipulation detection by multiple tampering traces and edge artifact enhancement},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Environmental sound classiﬁcation on the edge: A pipeline
for deep acoustic networks on extremely resource-constrained devices.
<em>PR</em>, <em>133</em>, 109025. (<a
href="https://doi.org/10.1016/j.patcog.2022.109025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant efforts are being invested to bring state-of-the-art classification and recognition to edge devices with extreme resource constraints (memory, speed, and lack of GPU support). Here, we demonstrate the first deep network for acoustic recognition that is small, flexible and compression-friendly yet achieves state-of-the-art performance for raw audio classification. Rather than handcrafting a once-off solution, we present a generic pipeline that automatically converts a large deep convolutional network via compression and quantization into a network for resource-impoverished edge devices. After introducing ACDNet, which produces above state-of-the-art accuracy on ESC-10 (96.65\%), ESC-50 (87.10\%), UrbanSound8K (84.45\%) and AudioEvent (92.57\%), we describe the compression pipeline and show that it allows us to achieve 97.22\% size reduction and 97.28\% FLOP reduction while maintaining close to state-of-the-art accuracy 96.25\%, 83.65\%, 78.27\% and 89.69\% on these datasets. We describe a successful implementation on a standard off-the-shelf microcontroller and, beyond laboratory benchmarks, report successful tests on real-world datasets.},
  archive      = {J_PR},
  author       = {Md Mohaimenuzzaman and Christoph Bergmeir and Ian West and Bernd Meyer},
  doi          = {10.1016/j.patcog.2022.109025},
  journal      = {Pattern Recognition},
  pages        = {109025},
  shortjournal = {Pattern Recognition},
  title        = {Environmental sound classiﬁcation on the edge: A pipeline for deep acoustic networks on extremely resource-constrained devices},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SaberNet: Self-attention based effective relation network
for few-shot learning. <em>PR</em>, <em>133</em>, 109024. (<a
href="https://doi.org/10.1016/j.patcog.2022.109024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning is an essential and challenging field in machine learning since the agent needs to learn novel concepts with a few data. Recent methods aim to learn comparison or relation between query and support samples to tackle few-shot tasks but have not exceeded human performance and made full use of relations in few-shot tasks. Humans can recognize multiple variants of objects located anywhere in images and compare the relation among learned instances. Inspired by the human learning mechanism, we explore the definition of relations in relation networks and propose self-attention relation modules for feature and learning ability. First, we introduce vision self-attention to generate and purify features in few-shot learning. The comparison of different patches leads the backbone to infer relations between local features , which enforces feature extraction focus on more details. Second, we propose task-specific feature augmentation modules to infer relations and weight different contributions of components in few-shot tasks. The proposed SaberNet is conceptually simple and empirically powerful. Its performance surpasses the baseline a great margin, including pushing 5-way 1-shot CUB accuracy to 89.75\% (12.73\% absolute improvement), Cars to 76.71\% (12.99\% absolute improvement) and Flowers to 84.33\% (7.67\% absolute improvement).},
  archive      = {J_PR},
  author       = {Zijun Li and Zhengping Hu and Weiwei Luo and Xiao Hu},
  doi          = {10.1016/j.patcog.2022.109024},
  journal      = {Pattern Recognition},
  pages        = {109024},
  shortjournal = {Pattern Recognition},
  title        = {SaberNet: Self-attention based effective relation network for few-shot learning},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic dense CRF inference for video segmentation and
semantic SLAM. <em>PR</em>, <em>133</em>, 109023. (<a
href="https://doi.org/10.1016/j.patcog.2022.109023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dense conditional random field (dense CRF) is an effective post-processing tool for image/video segmentation and semantic SLAM. In this paper, we extend the traditional dense CRF inference algorithm to incremental sensor data modelling. The algorithm efficiently infers the maximum a posteriori probability (MAP) solution for a dynamically changing dense CRF model that is applied to incremental multi-class video segmentation and semantic SLAM. The computational cost is roughly proportional to the total change in the Gaussian pairwise edges of the dense CRF. In our system, with an increase in the number of frames of the sensor data, MAP calculations take approximately the same time to compute the overall three-dimensional dense CRF modelled for the entire video. Compared with the traditional dense CRF for video segmentation, this method is more suitable for incremental (in-line) video segmentation and robot semantic SLAM. The results of experiments show that if part of a pairwise edge is altered, our dynamic algorithm is significantly faster than the widely known standard dense CRF algorithm. In addition, the accuracy of its inference does not change. Several multi-class video segmentation tests confirmed the efficiency of inference of the algorithm. In another application, we used the dynamic dense CRF to incrementally integrate robot SLAM and video segmentation. The results show that an accurate SLAM can improve the accuracy of video segmentation, and the computational cost of the dense CRF MAP can be constrained over a constant range. The application of our algorithm is not limited to video segmentation: It is generic, and can be used to yield similar improvements in many optimization solutions for MAP in dynamically changing models.},
  archive      = {J_PR},
  author       = {Mingyu You and Chaoxian Luo and Hongjun Zhou and Shaoqing Zhu},
  doi          = {10.1016/j.patcog.2022.109023},
  journal      = {Pattern Recognition},
  pages        = {109023},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic dense CRF inference for video segmentation and semantic SLAM},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online change-point detection with kernels. <em>PR</em>,
<em>133</em>, 109022. (<a
href="https://doi.org/10.1016/j.patcog.2022.109022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-points in time series data are usually defined as the time instants at which changes in their properties occur. Detecting change-points is critical in a number of applications as diverse as detecting credit card and insurance frauds, or intrusions into networks. Recently the authors introduced an online kernel-based change-point detection method built upon direct estimation of the density ratio on consecutive time intervals. This paper further investigates this algorithm, making improvements and analyzing its behavior in the mean and mean square sense, in the absence and presence of a change point. These theoretical analyses are validated with Monte Carlo simulations . The detection performance of the algorithm is illustrated through experiments on real-world data and compared to state of the art methodologies.},
  archive      = {J_PR},
  author       = {André Ferrari and Cédric Richard and Anthony Bourrier and Ikram Bouchikhi},
  doi          = {10.1016/j.patcog.2022.109022},
  journal      = {Pattern Recognition},
  pages        = {109022},
  shortjournal = {Pattern Recognition},
  title        = {Online change-point detection with kernels},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Pyramid geometric consistency learning for semantic
segmentation. <em>PR</em>, <em>133</em>, 109020. (<a
href="https://doi.org/10.1016/j.patcog.2022.109020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a critical in vision fields. Randomly transforms each image into different augmented samples and supervise the views with transformed semantics labels . However, even if the views are expanded from the same sample, the prediction results obtained by the same network will be very different. Therefore, we argue that between the augmented samples, the transformation-equivariance and the representational consistency also need to be supervised. Motivated by this, we propose a simple cross-data augmentation for semantic segmentation, in which we also leverage the pixel-level consistency constraint learning between pairs of augmented samples. As a result, our scheme significantly can improve the performances of existing semantic segmentation models without additional computation overhead. We verified the effectiveness of this method on Deeplab V3 Plus. Experiments show that our method can achieve stable performance improvement on mainstream data sets such as Pascal VOC 2012, Camvid, Cityscapes, etc.},
  archive      = {J_PR},
  author       = {Xian Zhang and Qiang Li and Zhibin Quan and Wankou Yang},
  doi          = {10.1016/j.patcog.2022.109020},
  journal      = {Pattern Recognition},
  pages        = {109020},
  shortjournal = {Pattern Recognition},
  title        = {Pyramid geometric consistency learning for semantic segmentation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UAVformer: A composite transformer network for urban scene
segmentation of UAV images. <em>PR</em>, <em>133</em>, 109019. (<a
href="https://doi.org/10.1016/j.patcog.2022.109019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban scenes segmentation based on UAV (Unmanned aerial vehicle) view is a fundamental task for the applications of smart city such as city planning, land use monitoring, traffic monitoring, and crowd estimation. While urban scenes in UAV image characteristic by large scale variation of objects size and complexity background, which posed challenges to urban scenes segmentation of UAV image. The feature extracting backbone of existing networks cannot extract complex features of UAV image effectively, which limits the performance of urban scenes segmentation. To design segmentation network capable of extracting features of large scale variation urban ground scenes, this study proposed a novel composite transformer network for urban scenes segmentation of UAV image. A composite backbone with aggregation windows multi-head self-attention transformer blocks is proposed to make the extracted features more representatives by adaptive multi-level features fusion , and the full utilisation of contextual information and local information. Position attention modules are inserted in each stage between encoder and decoder to further enhance the spatial attention of extracted feature maps. Finally, a V-shaped decoder which is capable of utilising multi-level features is designed to get accurately dense prediction. The accuracy of urban scenes segmentation could significantly be enhanced in this way and successfully segmented the large scale variation objects from UAV views. Extensive ablation experiments and comparative experiments for the proposed network have been conducted on the public available urban scenes segmentation datasets for UAV imagery. Experimental results have demonstrated the effectiveness of designed network structure and the superiority of proposed network over state-of-the-art methods. Specifically, reached 53.2\% mIoU on the UAVid dataset and 77.6\% mIoU on the UDD6 dataset, respectively.},
  archive      = {J_PR},
  author       = {Shi Yi and Xi Liu and Junjie Li and Ling Chen},
  doi          = {10.1016/j.patcog.2022.109019},
  journal      = {Pattern Recognition},
  pages        = {109019},
  shortjournal = {Pattern Recognition},
  title        = {UAVformer: A composite transformer network for urban scene segmentation of UAV images},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-regularized prototypical network for few-shot semantic
segmentation. <em>PR</em>, <em>133</em>, 109018. (<a
href="https://doi.org/10.1016/j.patcog.2022.109018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep CNNs in image semantic segmentation typically require a large number of densely-annotated images for training and have difficulties in generalizing to unseen object categories. Therefore, few-shot segmentation has been developed to perform segmentation with just a few annotated examples. In this work, we tackle the few-shot segmentation using a self-regularized prototypical network (SRPNet) based on prototype extraction for better utilization of the support information. The proposed SRPNet extracts class-specific prototype representations from support images and generates segmentation masks for query images by a distance metric - the fidelity. A direct yet effective prototype regularization on support set is proposed in SRPNet, in which the generated prototypes are evaluated and regularized on the support set itself. The extent to which the generated prototypes restore the support mask imposes an upper limit on performance. The performance on the query set should never exceed the upper limit no matter how complete the knowledge is generalized from support set to query set. With the specific prototype regularization , SRPNet fully exploits knowledge from the support and offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. The query performance is further improved by an iterative query inference (IQI) module that combines a set of regularized prototypes. Our proposed SRPNet achieves new state-of-art performance on 1-shot and 5-shot segmentation benchmarks.},
  archive      = {J_PR},
  author       = {Henghui Ding and Hui Zhang and Xudong Jiang},
  doi          = {10.1016/j.patcog.2022.109018},
  journal      = {Pattern Recognition},
  pages        = {109018},
  shortjournal = {Pattern Recognition},
  title        = {Self-regularized prototypical network for few-shot semantic segmentation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Answering knowledge-based visual questions via the
exploration of question purpose. <em>PR</em>, <em>133</em>, 109015. (<a
href="https://doi.org/10.1016/j.patcog.2022.109015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering has been greatly advanced by deep learning technologies, but still remains an open problem subjected to two aspects of factors. First, previous works estimate the correctness of each candidate answer mainly by its semantic correlations with visual questions, overlooking the fact that some questions and their answers are semantically inconsistent. Second, previous works that require external knowledge mainly uses the knowledge facts retrieved by key words or visual objects. However, the retrieved knowledge facts may only be related to the semantics of the question, but are useless or even misleading for answer prediction. To address these issues, we investigate how to capture the purpose of visual questions and propose a Purpose Guided Visual Question Answering model, called PGVQA. It mainly has two appealing properties: (1) It can estimate the correctness of candidate answers based on the Question Purpose (QP) that reveals which aspects of the concept are examined by visual questions. This is helpful for avoiding the negative effect of the semantic inconsistency between answers and questions. (2) It can incorporate the knowledge facts accordant with the QP into answer prediction, which helps to improve the probability of answering visual questions correctly. Empirical studies on benchmark datasets show that PGVQA achieves state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Lingyun Song and Jianao Li and Jun Liu and Yang Yang and Xuequn Shang and Mingxuan Sun},
  doi          = {10.1016/j.patcog.2022.109015},
  journal      = {Pattern Recognition},
  pages        = {109015},
  shortjournal = {Pattern Recognition},
  title        = {Answering knowledge-based visual questions via the exploration of question purpose},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep autoregressive models with spectral attention.
<em>PR</em>, <em>133</em>, 109014. (<a
href="https://doi.org/10.1016/j.patcog.2022.109014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is an important problem across many domains, playing a crucial role in multiple real-world applications. In this paper, we propose a forecasting architecture that combines deep autoregressive models with a Spectral Attention (SA) module, which merges global and local frequency domain information in the model’s embedded space. By characterizing in the spectral domain the embedding of the time series as occurrences of a random process, our method can identify global trends and seasonality patterns. Two spectral attention models, global and local to the time series, integrate this information within the forecast and perform spectral filtering to remove time series’s noise. The proposed architecture has a number of useful properties: it can be effectively incorporated into well-known forecast architectures, requiring a low number of parameters and producing explainable results that improve forecasting accuracy . We test the Spectral Attention Autoregressive Model (SAAM) on several well-known forecast datasets, consistently demonstrating that our model compares favorably to state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Fernando Moreno-Pino and Pablo M. Olmos and Antonio Artés-Rodríguez},
  doi          = {10.1016/j.patcog.2022.109014},
  journal      = {Pattern Recognition},
  pages        = {109014},
  shortjournal = {Pattern Recognition},
  title        = {Deep autoregressive models with spectral attention},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LongReMix: Robust learning with high confidence samples in a
noisy label environment. <em>PR</em>, <em>133</em>, 109013. (<a
href="https://doi.org/10.1016/j.patcog.2022.109013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art noisy-label learning algorithms rely on an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) that minimises the empirical vicinal risk using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. The classification accuracy of such noisy-label learning methods depends on the precision of the unsupervised classification of clean and noisy samples, and the robustness of SSL to small clean sets. We address these points with a new noisy-label training algorithm , called LongReMix, which improves the precision of the unsupervised classification of clean and noisy samples and the robustness of SSL to small clean sets with a two-stage learning process. The stage one of LongReMix finds a small but precise high-confidence clean set, and stage two augments this high-confidence clean set with new clean samples and oversamples the clean data to increase the robustness of SSL to small clean sets. We test LongReMix on CIFAR-10 and CIFAR-100 with introduced synthetic noisy labels, and the real-world noisy-label benchmarks CNWL (Red Mini-ImageNet), WebVision, Clothing1M, and Food101-N. The results show that our LongReMix produces significantly better classification accuracy than competing approaches, particularly in high noise rate problems. Furthermore, our approach achieves state-of-the-art performance in most datasets. The code is available at https://github.com/filipe-research/LongReMix .},
  archive      = {J_PR},
  author       = {Filipe R. Cordeiro and Ragav Sachdeva and Vasileios Belagiannis and Ian Reid and Gustavo Carneiro},
  doi          = {10.1016/j.patcog.2022.109013},
  journal      = {Pattern Recognition},
  pages        = {109013},
  shortjournal = {Pattern Recognition},
  title        = {LongReMix: Robust learning with high confidence samples in a noisy label environment},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards prior gap and representation gap for long-tailed
recognition. <em>PR</em>, <em>133</em>, 109012. (<a
href="https://doi.org/10.1016/j.patcog.2022.109012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep learning models are elaborately designed for balanced datasets, and thus they inevitably suffer performance degradation in practical long-tailed recognition tasks, especially to the minority classes. There are two crucial issues in learning from imbalanced datasets: skew decision boundary and unrepresentative feature space. In this work, we establish a theoretical framework to analyze the sources of these two issues from Bayesian perspective , and find that they are closely related to the prior gap and the representation gap, respectively. Under this framework, we show that existing long-tailed recognition methods manage to remove either the prior gap or the presentation gap. Different from these methods, we propose to simultaneously remove the two gaps to achieve more accurate long-tailed recognition. Specifically, we propose the prior calibration strategy to remove the prior gap and introduce three strategies (representative feature extraction, optimization strategy adjustment and effective sample modeling) to mitigate the representation gap. Extensive experiments on five benchmark datasets validate the superiority of our method against the state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Ming-Liang Zhang and Xu-Yao Zhang and Chuang Wang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2022.109012},
  journal      = {Pattern Recognition},
  pages        = {109012},
  shortjournal = {Pattern Recognition},
  title        = {Towards prior gap and representation gap for long-tailed recognition},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScribbleNet: Efficient interactive annotation of urban city
scenes for semantic segmentation. <em>PR</em>, <em>133</em>, 109011. (<a
href="https://doi.org/10.1016/j.patcog.2022.109011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Annotation is a crucial first step in the semantic segmentation of urban images that facilitates the development of autonomous navigation systems. However, annotating complex urban images is time-consuming and challenging. It requires significant human effort making it expensive and error-prone. To reduce human effort during annotation, multiple images need to be annotated in a short time-span. In this paper, we introduce ScribbleNet, an interactive image segmentation algorithm to address this issue. Our approach provides users with a pre-segmented image that iteratively improves the segmentation using scribble as an annotation input. This method is based on conditional inference and exploits the learnt correlations in a deep neural network (DNN). ScribbleNet can: (1) work with urban city scenes captured in unseen environments, (2) annotate new classes not present in the training set, and (3) correct several labels at once. We compare this method with other interactive segmentation approaches on multiple datasets such as CityScapes, BDD, Mapillary Vistas, KITTI, and IDD. ScribbleNet reduces the annotation time of an image by up to 14.7 × × over manual annotation and up to 5.4 × × over the current approaches. The algorithm is integrated into the publicly available LabelMe image annotation tool and will be released as an open-source software.},
  archive      = {J_PR},
  author       = {Bhavani Sambaturu and Ashutosh Gupta and C.V. Jawahar and Chetan Arora},
  doi          = {10.1016/j.patcog.2022.109011},
  journal      = {Pattern Recognition},
  pages        = {109011},
  shortjournal = {Pattern Recognition},
  title        = {ScribbleNet: Efficient interactive annotation of urban city scenes for semantic segmentation},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BH2I-GAN: Bidirectional hash_code-to-image translation using
multi-generative multi-adversarial nets. <em>PR</em>, <em>133</em>,
109010. (<a href="https://doi.org/10.1016/j.patcog.2022.109010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the benefits of high retrieval efficiency and low storage cost, hashing method has received an increasing attention. In particular, deep learning-based hashing has been widely used in data mining and information retrieval. However, almost all the existing methods only achieve the goal of high retrieval precision, and limit the evaluation of hashing methods to objective aspect. In this paper, we propose a novel bidirectional hash_code-to-image translation model by using multi-generative multi-adversarial nets to reduce storage cost truly and obtain satisfactory user acceptance on the basis of high retrieval precision. Firstly, we propose supervised manifold metric to reduce Hamming distance between similar instances while increasing the Hamming distance between dissimilar instances, which have been proved to be helpful for high retrieval precision and good user acceptance. Then, we utilize multi-generative and multi-adversarial networks to construct hash mapping and inverse hash generation. During inverse generation, theoretical analysis is conducted to show that inverse hash network can avoid unstable training and mode collapse. Besides, we prove that Poisson distribution induced by hash codes can be initialized as generative distribution to fit real distribution. Experimental results show that our method outperforms several state-of-the-art approaches on three popular datasets. Specifically, ours yields average about 9.3\% increment in Mean Average Precision(MAP) on three datasets, and achieves over 90\% user satisfaction. Besides, it successfully reduces storage cost by 1,634 times in COCO 2017 large-scale dataset.},
  archive      = {J_PR},
  author       = {Liming Xu and Xianhua Zeng and Weisheng Li and Yicai Xie},
  doi          = {10.1016/j.patcog.2022.109010},
  journal      = {Pattern Recognition},
  pages        = {109010},
  shortjournal = {Pattern Recognition},
  title        = {BH2I-GAN: Bidirectional hash_code-to-image translation using multi-generative multi-adversarial nets},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust physical-world attacks on face recognition.
<em>PR</em>, <em>133</em>, 109009. (<a
href="https://doi.org/10.1016/j.patcog.2022.109009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has been greatly facilitated by the development of deep neural networks (DNNs) and has been widely applied to many safety-critical applications. However, recent studies have shown that DNNs are very vulnerable to adversarial examples , raising severe concerns on the security of real-world face recognition. In this work, we study sticker-based physical attacks on face recognition for better understanding its adversarial robustness. To this end, we first analyze in-depth the complicated physical-world conditions confronted by attacking face recognition, including the different variations of stickers, faces, and environmental conditions. Then, we propose a novel robust physical attack framework, dubbed PadvFace, to model these challenging variations specifically. Furthermore, we reveal that the attack complexities vary under different physical-world conditions and propose an efficient Curriculum Adversarial Attack (CAA) algorithm that gradually adapts adversarial stickers to environmental variations from easy to complex. Finally, we construct a standardized testing protocol to facilitate the fair evaluation of physical attacks on face recognition, and extensive experiments on both physical dodging and impersonation attacks demonstrate the superior performance of the proposed method.},
  archive      = {J_PR},
  author       = {Xin Zheng and Yanbo Fan and Baoyuan Wu and Yong Zhang and Jue Wang and Shirui Pan},
  doi          = {10.1016/j.patcog.2022.109009},
  journal      = {Pattern Recognition},
  pages        = {109009},
  shortjournal = {Pattern Recognition},
  title        = {Robust physical-world attacks on face recognition},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noise-robust oversampling for imbalanced data
classification. <em>PR</em>, <em>133</em>, 109008. (<a
href="https://doi.org/10.1016/j.patcog.2022.109008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class imbalance problem is characterized by an unequal data distribution in which majority classes have a greater number of data samples than minority classes. Oversampling methods generate samples for minority classes to balance the data distribution. However, the generated minority samples may overlap with majority samples, resulting in noise. In this paper, we propose a noise-robust oversampling algorithm for mixed-type and multi-class imbalanced data . Our proposed noise-robust designs include an algorithm to eliminate noise within clusters of data samples, adaptive embedding to generate samples safely, and a safe boundary for enlarging class boundaries. The heterogeneous distance metric and adapted decomposition strategy render our noise-robust algorithm suitable for mixed-type and multi-class imbalanced data. Experimental results on 20 benchmark datasets demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Yongxu Liu and Yan Liu and Bruce X.B. Yu and Shenghua Zhong and Zhejing Hu},
  doi          = {10.1016/j.patcog.2022.109008},
  journal      = {Pattern Recognition},
  pages        = {109008},
  shortjournal = {Pattern Recognition},
  title        = {Noise-robust oversampling for imbalanced data classification},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online and offline streaming feature selection methods with
bat algorithm for redundancy analysis. <em>PR</em>, <em>133</em>,
109007. (<a href="https://doi.org/10.1016/j.patcog.2022.109007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming feature selection (SFS), is the task of selecting the most informative features in dealing with high-dimensional or incrementally growing problems. Several SFS algorithms have been proposed in the literature. However, they do not consider all feature subsets at the redundancy analysis step due to computational concerns. Moreover, they do not reconsider previously removed features which leads to losing most of the useful information. In this paper, the redundancy analysis step is defined as a binary optimization problem . Then, a binary bat algorithm (BBA) is adopted to find the minimal informative subsets. In this way, a large number of feature subsets can be considered effectively at the redundancy analysis step. In addition, an effective priority list is used to maintain previously removed redundant features. Such a list allows the re-examination of informative features. As a result, it is possible to consider the mutual information between features that are not streamed in an small time interval. Experimental studies on fifteen different types of datasets show that our approach is superior to state-of-the-art online and offline streaming feature selection methods in terms of classification accuracy .},
  archive      = {J_PR},
  author       = {S. Eskandari and M. Seifaddini},
  doi          = {10.1016/j.patcog.2022.109007},
  journal      = {Pattern Recognition},
  pages        = {109007},
  shortjournal = {Pattern Recognition},
  title        = {Online and offline streaming feature selection methods with bat algorithm for redundancy analysis},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust table detection and structure recognition from
heterogeneous document images. <em>PR</em>, <em>133</em>, 109006. (<a
href="https://doi.org/10.1016/j.patcog.2022.109006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new table detection and structure recognition approach named RobusTabNet to detect the boundaries of tables and reconstruct the cellular structure of each table from heterogeneous document images . For table detection, we propose to use CornerNet as a new region proposal network to generate higher quality table proposals for Faster R-CNN, which has significantly improved the localization accuracy of Faster R-CNN for table detection. Consequently, our table detection approach achieves state-of-the-art performance on three public table detection benchmarks, namely cTDaR TrackA, PubLayNet and IIIT-AR-13K, by only using a lightweight ResNet-18 backbone network . Furthermore, we propose a new split-and-merge based table structure recognition approach, in which a novel spatial CNN based separation line prediction module is proposed to split each detected table into a grid of cells, and a Grid CNN based cell merging module is applied to recover the spanning cells. As the spatial CNN module can effectively propagate contextual information across the whole table image, our table structure recognizer can robustly recognize tables with large blank spaces and geometrically distorted (even curved) tables. Thanks to these two techniques, our table structure recognition approach achieves state-of-the-art performance on three public benchmarks, including SciTSR, PubTabNet and cTDaR TrackB2-Modern. Moreover, we have further demonstrated the advantages of our approach in recognizing tables with complex structures, large blank spaces, as well as geometrically distorted or even curved shapes on a more challenging in-house dataset.},
  archive      = {J_PR},
  author       = {Chixiang Ma and Weihong Lin and Lei Sun and Qiang Huo},
  doi          = {10.1016/j.patcog.2022.109006},
  journal      = {Pattern Recognition},
  pages        = {109006},
  shortjournal = {Pattern Recognition},
  title        = {Robust table detection and structure recognition from heterogeneous document images},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-embedded subspace support vector data description.
<em>PR</em>, <em>133</em>, 108999. (<a
href="https://doi.org/10.1016/j.patcog.2022.108999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel subspace learning framework for one-class classification. The proposed framework presents the problem in the form of graph embedding. It includes the previously proposed subspace one-class techniques as its special cases and provides further insight on what these techniques actually optimize. The framework allows to incorporate other meaningful optimization goals via the graph preserving criterion and reveals a spectral solution and a spectral regression-based solution as alternatives to the previously used gradient-based technique. We combine the subspace learning framework iteratively with Support Vector Data Description applied in the subspace to formulate Graph-Embedded Subspace Support Vector Data Description. We experimentally analyzed the performance of newly proposed different variants. We demonstrate improved performance against the baselines and the recently proposed subspace learning methods for one-class classification.},
  archive      = {J_PR},
  author       = {Fahad Sohrab and Alexandros Iosifidis and Moncef Gabbouj and Jenni Raitoharju},
  doi          = {10.1016/j.patcog.2022.108999},
  journal      = {Pattern Recognition},
  pages        = {108999},
  shortjournal = {Pattern Recognition},
  title        = {Graph-embedded subspace support vector data description},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A full data augmentation pipeline for small object detection
based on generative adversarial networks. <em>PR</em>, <em>133</em>,
108998. (<a href="https://doi.org/10.1016/j.patcog.2022.108998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection accuracy on small objects, i.e., objects under 32 × × 32 pixels, lags behind that of large ones. To address this issue, innovative architectures have been designed and new datasets have been released. Still, the number of small objects in many datasets does not suffice for training. The advent of the generative adversarial networks (GANs) opens up a new data augmentation possibility for training architectures without the costly task of annotating huge datasets for small objects. In this paper, we propose a full pipeline for data augmentation for small object detection which combines a GAN-based object generator with techniques of object segmentation, image inpainting, and image blending to achieve high-quality synthetic data. The main component of our pipeline is DS-GAN, a novel GAN-based architecture that generates realistic small objects from larger ones. Experimental results show that our overall data augmentation method improves the performance of state-of-the-art models up to 11.9\% AP s @ . 5 s@.5 on UAVDT and by 4.7\% AP s @ . 5 s@.5 on iSAID, both for the small objects subset and for a scenario where the number of training instances is limited.},
  archive      = {J_PR},
  author       = {Brais Bosquet and Daniel Cores and Lorenzo Seidenari and Víctor M. Brea and Manuel Mucientes and Alberto Del Bimbo},
  doi          = {10.1016/j.patcog.2022.108998},
  journal      = {Pattern Recognition},
  pages        = {108998},
  shortjournal = {Pattern Recognition},
  title        = {A full data augmentation pipeline for small object detection based on generative adversarial networks},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single image super-resolution based on directional variance
attention network. <em>PR</em>, <em>133</em>, 108997. (<a
href="https://doi.org/10.1016/j.patcog.2022.108997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in single image super-resolution (SISR) explore the power of deep convolutional neural networks (CNNs) to achieve better performance. However, most of the progress has been made by scaling CNN architectures, which usually raise computational demands and memory consumption. This makes modern architectures less applicable in practice. In addition, most CNN-based SR methods do not fully utilize the informative hierarchical features that are helpful for final image recovery. In order to address these issues, we propose a directional variance attention network (DiVANet), a computationally efficient yet accurate network for SISR. Specifically, we introduce a novel directional variance attention (DiVA) mechanism to capture long-range spatial dependencies and exploit inter-channel dependencies simultaneously for more discriminative representations. Furthermore, we propose a residual attention feature group (RAFG) for parallelizing attention and residual block computation. The output of each residual block is linearly fused at the RAFG output to provide access to the whole feature hierarchy. In parallel, DiVA extracts most relevant features from the network for improving the final output and preventing information loss along the successive operations inside the network. Experimental results demonstrate the superiority of DiVANet over the state of the art in several datasets, while maintaining relatively low computation and memory footprint . The code is available at https://github.com/pbehjatii/DiVANet .},
  archive      = {J_PR},
  author       = {Parichehr Behjati and Pau Rodriguez and Carles Fernández and Isabelle Hupont and Armin Mehri and Jordi Gonzàlez},
  doi          = {10.1016/j.patcog.2022.108997},
  journal      = {Pattern Recognition},
  pages        = {108997},
  shortjournal = {Pattern Recognition},
  title        = {Single image super-resolution based on directional variance attention network},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An accurate stereo matching method based on color segments
and edges. <em>PR</em>, <em>133</em>, 108996. (<a
href="https://doi.org/10.1016/j.patcog.2022.108996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching algorithms of binocular vision suffer from low accuracy when dealing with natural scenes (such as industrial robot scenes). Biological vision is sensitive to object edges; it divides objects by their edges, and then perceives their distances. Similar to the biological eye mechanism, this study proposes a matching algorithm that combines segment- and edge-matching to obtain the disparity. In segment matching, pixel strings from the same row of the left and right images are divided into pixel segments, whose colors and lengths are used as clues to determine several types of matching pixel segment pairs according to non-crossing mapping. The analysis of the spatial state yields several types of stimulus bars. Disparities can be obtained from the relation between pixel segment pairs and stimulus bars. In edge matching, the DTW (Dynamic Time Warping) algorithm and the gradient are used to determine the initial edge pixel matching results. The remaining edge point disparity is obtained by fitting a fill to the existing edge point disparity. Finally, segment and edge matching results are combined to check and fill and post-processing. This new matching method transforms pixel matching to pixel segment matching and edge matching, which can reduces the time complexity. The algorithm can be implemented in an industrial robot environment for high-precision needle threading guidance, which neither traditional binocular matching nor deep learning matching algorithms can do.},
  archive      = {J_PR},
  author       = {Hui Wei and Lingjiang Meng},
  doi          = {10.1016/j.patcog.2022.108996},
  journal      = {Pattern Recognition},
  pages        = {108996},
  shortjournal = {Pattern Recognition},
  title        = {An accurate stereo matching method based on color segments and edges},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decoupling multi-task causality for improved skin lesion
segmentation and classification. <em>PR</em>, <em>133</em>, 108995. (<a
href="https://doi.org/10.1016/j.patcog.2022.108995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning has been used widely in many computer aided diagnosis applications recently, while the trade-off between different tasks remains challenging. Also, the inherent causality is less studied. In this paper, we focus on skin lesion analysis, including lesion classification, detection and segmentation. By defining the chain relationship (i.e., lesion detection boosts contour segmentation, and segmentation boosts lesion classification in turn), and further decoupling each pair-wise causality (e.g., detection to segmentation) from the Pareto efficiency view, we can solve the common trade-off issue between multi-task. On this basis, we propose a novel paradigm to improve the skin lesion segmentation and classification separately, and favourable feature fusion ways for each task are explored. Moreover, to address the huge model size problem, we design an effective model compression scheme (MCS). Extensive experiments on the ISIC2017 and PH2 datasets are conducted to evaluate the proposed paradigm. The results demonstrate that the popular models such as ResNet , DenseNet and UNet for lesion analysis can be boosted by applying the proposed paradigm, and the designed MCS reduces the amount of model parameters efficiently. We achieve performance improvements on skin lesion segmentation and classification without strenuous network design and soaring model complexity. This proposed approach is promising for the multi-task diagnosis setting in other medical applications.},
  archive      = {J_PR},
  author       = {Lei Song and Haoqian Wang and Z. Jane Wang},
  doi          = {10.1016/j.patcog.2022.108995},
  journal      = {Pattern Recognition},
  pages        = {108995},
  shortjournal = {Pattern Recognition},
  title        = {Decoupling multi-task causality for improved skin lesion segmentation and classification},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional mixture modeling and model-based clustering.
<em>PR</em>, <em>133</em>, 108994. (<a
href="https://doi.org/10.1016/j.patcog.2022.108994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to a potentially high number of parameters, finite mixture models are often at the risk of overparameterization even for a moderate number of components. This can lead to overfitting individual components and result in mixture order underestimation. One of the most popular approaches to address this issue is to reduce the number of parameters by considering parsimonious models. The vast majority of techniques in this direction focuses on the reparameterization of covariance matrices associated with mixture components. We propose an alternative approach based on the parsimonious parameterization of location parameters that enjoys remarkable modeling flexibility especially in the presence of non-compact clusters. Due to an attractive closed form formulation, speedy parameter estimation is available by means of the EM algorithm . The utility of the proposed method is illustrated on synthetic and well-known classification data sets.},
  archive      = {J_PR},
  author       = {Volodymyr Melnykov and Yang Wang},
  doi          = {10.1016/j.patcog.2022.108994},
  journal      = {Pattern Recognition},
  pages        = {108994},
  shortjournal = {Pattern Recognition},
  title        = {Conditional mixture modeling and model-based clustering},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BP-triplet net for unsupervised domain adaptation: A
bayesian perspective. <em>PR</em>, <em>133</em>, 108993. (<a
href="https://doi.org/10.1016/j.patcog.2022.108993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triplet loss, one of the deep metric learning (DML) methods, is to learn the embeddings where examples from the same class are closer than examples from different classes. Motivated by DML, we propose an effective BP-triplet Loss for unsupervised domain adaption (UDA) from the perspective of Bayesian learning and we name the model as BP-Triplet Net . In previous metric learning based methods for UDA, sample pairs across domains are treated equally, which is not appropriate due to the domain bias. In our work, considering the different importance of pair-wise samples for both feature learning and domain alignment, we deduce our BP-triplet loss for effective UDA from the perspective of Bayesian learning. Our BP-triplet loss adjusts the weights of pair-wise samples in intra-domain and inter-domain. Especially, it can self attend to the hard pairs (including hard positive pair and hard negative pair). Together with the commonly used adversarial loss for domain alignment, the quality of target pseudo labels is progressively improved. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David’s theorem. Comprehensive evaluations on four benchmark datasets demonstrate the effectiveness of the proposed approach for UDA. Code is available at https://github.com/wangshanshanAHU/BP-Triplet-Net .},
  archive      = {J_PR},
  author       = {Shanshan Wang and Lei Zhang and Pichao Wang and MengZhu Wang and Xingyi Zhang},
  doi          = {10.1016/j.patcog.2022.108993},
  journal      = {Pattern Recognition},
  pages        = {108993},
  shortjournal = {Pattern Recognition},
  title        = {BP-triplet net for unsupervised domain adaptation: A bayesian perspective},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grouping-based oversampling in kernel space for imbalanced
data classification. <em>PR</em>, <em>133</em>, 108992. (<a
href="https://doi.org/10.1016/j.patcog.2022.108992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class-imbalanced classification is a difficult problem because not only traditional classifiers are more biased towards the majority classes and inclined to generate incorrect predictions, but also the existing algorithms often have difficulty tackling this kind of problem with the class overlapping. Oversampling is a widely used and effective method to obtain balanced samples for imbalanced data, but the existing oversampling methods usually result in more serious class overlapping due to improper choice of the reference samples. To circumvent this shortcoming, according to the different possibilities of minority class samples appearing in the overlapping regions in the feature space, a grouping scheme for the minority class samples is first designed to identify the overlapping region samples. Then, a new oversampling method based on this grouping scheme is proposed to make the new samples far away from the overlapping region and rectify the decision boundary properly. Subsequently, a new effective classification algorithm is developed for imbalanced data. Extensive experiments show that the proposed algorithm is superior to the seventeen benchmark algorithms in terms of three performance metrics, especially on high imbalance ratio data sets.},
  archive      = {J_PR},
  author       = {Jinjun Ren and Yuping Wang and Yiu-ming Cheung and Xiao-Zhi Gao and Xiaofang Guo},
  doi          = {10.1016/j.patcog.2022.108992},
  journal      = {Pattern Recognition},
  pages        = {108992},
  shortjournal = {Pattern Recognition},
  title        = {Grouping-based oversampling in kernel space for imbalanced data classification},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Margin embedding net for robust margin collaborative
representation-based classification. <em>PR</em>, <em>133</em>, 108991.
(<a href="https://doi.org/10.1016/j.patcog.2022.108991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative Representation-based Classification method (CRC) shows great potential in classification task . However, redundancies in both features and samples limit the application of CRC seriously. The existing works only solve one of them and ignore the other, which leads to performance degradation . To address this problem, we explore collaborative representation mechanism and propose a classification method termed Robust Margin Collaborative Representation-based Classification (RMCRC) which uses a few but more representative robust marginal samples to eliminate redundancy between samples. As the performance of RMCRC is related to robust marginal samples and class separability assumption closely, we further propose a feature extraction method termed Margin Embedding Net (MEN) for RMCRC. In MEN, virtual samples are generated by a generative model to enhance effectiveness of robust marginal samples and generalizability of RMCRC. Then, an embedding network with triplet loss is used to eliminate the redundancy in features and ensure the assumption is satisfied. Specifically, we construct triplet according to the collaborative representation. Hence, MEN fits RMCRC very well. Extensive experimental results validate effectiveness of proposed method.},
  archive      = {J_PR},
  author       = {Zhichao Zheng and Huaijiang Sun and Ying Zhou},
  doi          = {10.1016/j.patcog.2022.108991},
  journal      = {Pattern Recognition},
  pages        = {108991},
  shortjournal = {Pattern Recognition},
  title        = {Margin embedding net for robust margin collaborative representation-based classification},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-attention fusion network for multimodal skin cancer
diagnosis. <em>PR</em>, <em>133</em>, 108990. (<a
href="https://doi.org/10.1016/j.patcog.2022.108990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multimodal image-based methods have shown great performance in skin cancer diagnosis. These methods usually use convolutional neural networks (CNNs) to extract the features of two modalities (i.e., dermoscopy and clinical images), and fuse these features for classification. However, they commonly have the following two shortcomings: 1) the feature extraction processes of the two modalities are independent and lack cooperation, which may lead to limited representation ability of the extracted features, and 2) the multimodal fusion operation is a simple concatenation followed by convolutions, thus causing rough fusion features . To address these two issues, we propose a co-attention fusion network (CAFNet), which uses two branches to extract the features of dermoscopy and clinical images and a hyper-branch to refine and fuse these features at all stages of the network. Specifically, the hyper-branch is composed of multiple co-attention fusion (CAF) modules. In each CAF module, we first design a co-attention (CA) block with a cross-modal attention mechanism to achieve the cooperation of two modalities, which enhances the representation ability of the extracted features through mutual guidance between the two modalities. Following the CA block, we further propose an attention fusion (AF) block that dynamically selects appropriate fusion ratios to conduct the pixel-wise multimodal fusion, which can generate fine-grained fusion features. In addition, we propose a deep-supervised loss and a combined prediction method to obtain a more robust prediction result. The results show that CAFNet achieves the average accuracy of 76.8\% on the seven-point checklist dataset and outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xiaoyu He and Yong Wang and Shuang Zhao and Xiang Chen},
  doi          = {10.1016/j.patcog.2022.108990},
  journal      = {Pattern Recognition},
  pages        = {108990},
  shortjournal = {Pattern Recognition},
  title        = {Co-attention fusion network for multimodal skin cancer diagnosis},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new algorithm for support vector regression with automatic
selection of hyperparameters. <em>PR</em>, <em>133</em>, 108989. (<a
href="https://doi.org/10.1016/j.patcog.2022.108989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hyperparameters in support vector regression (SVR) determine the effectiveness of the support vectors with fitting and predictions. However, the choice of these hyperparameters has always been challenging in both theory and practice. The ν ν -support vector regression eliminates the need to specify an ϵ ϵ value elegantly, but at the cost of specifying or postulating a ν ν value. We propose an extended primal objective function arising from probability regularization leading to an automatic selection of ϵ ϵ , and we can express ν ν as an explicit function of ϵ ϵ . The resultant hyperparameter values can be interpreted as ‘working’ values required only in training but not testing or prediction. This regularized algorithm, namely ϵ * ϵ* -SVR, automatically provides a data-dependent ϵ ϵ and is found to have a close connection to the ν ν -support vector regression in the sense that ν ν as a fraction is a sensible function of ϵ ϵ . The ϵ * ϵ* -SVR automatically selects both ν ν and ϵ ϵ values. We illustrate these findings with some public benchmark datasets.},
  archive      = {J_PR},
  author       = {You-Gan Wang and Jinran Wu and Zhi-Hua Hu and Geoffrey J. McLachlan},
  doi          = {10.1016/j.patcog.2022.108989},
  journal      = {Pattern Recognition},
  pages        = {108989},
  shortjournal = {Pattern Recognition},
  title        = {A new algorithm for support vector regression with automatic selection of hyperparameters},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Better pseudo-label: Joint domain-aware label and
dual-classifier for semi-supervised domain generalization. <em>PR</em>,
<em>133</em>, 108987. (<a
href="https://doi.org/10.1016/j.patcog.2022.108987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the goal of directly generalizing trained model to unseen target domains, domain generalization (DG), a newly proposed learning paradigm, has attracted considerable attention. Previous DG models usually require a sufficient quantity of annotated samples from observed source domains during training. In this paper, we relax this requirement about full annotation and investigate semi-supervised domain generalization (SSDG) where only one source domain is fully annotated along with the other domains totally unlabeled in the training process. With the challenges of tackling the domain gap between observed source domains and predicting unseen target domains, we propose a novel deep framework via joint domain-aware labels and dual-classifier to produce high-quality pseudo-labels. Concretely, to predict accurate pseudo-labels under domain shift, a domain-aware pseudo-labeling module is developed. Also, considering inconsistent goals between generalization and pseudo-labeling: former prevents overfitting on all source domains while latter might overfit the unlabeled source domains for high accuracy, we employ a dual-classifier to independently perform pseudo-labeling and domain generalization in the training process. When accurate pseudo-labels are generated for unlabeled source domains, the domain mixup operation is applied to augment new domains between labeled and unlabeled domains, which is beneficial for boosting the generalization capability of the model. Extensive results on publicly available DG benchmark datasets show the efficacy of our proposed SSDG method.},
  archive      = {J_PR},
  author       = {Ruiqi Wang and Lei Qi and Yinghuan Shi and Yang Gao},
  doi          = {10.1016/j.patcog.2022.108987},
  journal      = {Pattern Recognition},
  pages        = {108987},
  shortjournal = {Pattern Recognition},
  title        = {Better pseudo-label: Joint domain-aware label and dual-classifier for semi-supervised domain generalization},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot learning with unsupervised part discovery and
part-aligned similarity. <em>PR</em>, <em>133</em>, 108986. (<a
href="https://doi.org/10.1016/j.patcog.2022.108986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize novel concepts with only a few examples. To this end, previous studies resort to acquiring a strong inductive bias via meta-learning on a group of similar tasks, which however needs a large labeled base dataset to sample training tasks. In this paper, we show that such inductive bias can be learned from a flat collection of unlabeled images, and instantiated as transferable representations among seen and unseen classes. Specifically, we propose a novel unsupervised Part Discovery Network (PDN) to learn transferable representations from unlabeled images, which automatically selects the most discriminative part from an input image and then maximizes its similarities to the global view of the input and other neighbors with similar semantics. To better leverage the learned representations for few-shot learning, we further propose Part-Aligned Similarity (PAS), the key of which is to measure image similarities based on a set of discriminative and aligned parts. We conduct extensive studies on five popular few-shot learning datasets to evaluate our approach. The experimental results show that our approach outperforms previous unsupervised methods by a large margin and is even comparable with state-of-the-art supervised methods.},
  archive      = {J_PR},
  author       = {Wentao Chen and Zhang Zhang and Wei Wang and Liang Wang and Zilei Wang and Tieniu Tan},
  doi          = {10.1016/j.patcog.2022.108986},
  journal      = {Pattern Recognition},
  pages        = {108986},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot learning with unsupervised part discovery and part-aligned similarity},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial scratches: Deployable attacks to CNN
classifiers. <em>PR</em>, <em>133</em>, 108985. (<a
href="https://doi.org/10.1016/j.patcog.2022.108985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing body of work has shown that deep neural networks are susceptible to adversarial examples . These take the form of small perturbations applied to the model’s input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L 0 L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage Bézier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer queries and modifying very few pixels.},
  archive      = {J_PR},
  author       = {Loris Giulivi and Malhar Jere and Loris Rossi and Farinaz Koushanfar and Gabriela Ciocarlie and Briland Hitaj and Giacomo Boracchi},
  doi          = {10.1016/j.patcog.2022.108985},
  journal      = {Pattern Recognition},
  pages        = {108985},
  shortjournal = {Pattern Recognition},
  title        = {Adversarial scratches: Deployable attacks to CNN classifiers},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental learning for transductive support vector
machine. <em>PR</em>, <em>133</em>, 108982. (<a
href="https://doi.org/10.1016/j.patcog.2022.108982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning is ubiquitous in real-world machine learning applications due to its good performance for handling the data where only a few number of samples are labeled while most of then are unlabeled. Transductive support vector machine (TSVM) is an important semi-supervised learning method which formulates the problem as a nonconvex combinatorial optimization problem . The infinitesimal annealing algorithm is a novel training method of TSVM which can alleviate the impact of the combinatorial and non-convex natures in TSVM and achieve a fast training of TSVM. However, it is still a challenging problem to handle large-scale data for TSVM even using the infinitesimal annealing algorithm. To mitigate this problem, in this paper, we propose an incremental learning algorithm for TSVM (ILTSVM) based on the path following technique under the framework of infinitesimal annealing. Specifically, for new samples, we call CP-Step to change the solution and partition by increasing the size of the penalty coefficient. The difference between training labeled samples and training unlabeled samples is that the variation range of the penalty coefficient of labeled samples is larger than that of unlabeled samples . If in the process of CP-Step, pseudo-labels of unlabeled samples are classified incorrectly, call DJ-Step to flip the pseudo-labels, and use incremental and decremental algorithms to make the KKT condition satisfied. We also analyze the time complexity and convergence of ILTSVM. The experimental results show that compared with other incremental or batch learning algorithms, our algorithm is the most effective and fastest method for training TSVM.},
  archive      = {J_PR},
  author       = {Haiyan Chen and Ying Yu and Yizhen Jia and Bin Gu},
  doi          = {10.1016/j.patcog.2022.108982},
  journal      = {Pattern Recognition},
  pages        = {108982},
  shortjournal = {Pattern Recognition},
  title        = {Incremental learning for transductive support vector machine},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Back-compatible color QR codes for colorimetric
applications. <em>PR</em>, <em>133</em>, 108981. (<a
href="https://doi.org/10.1016/j.patcog.2022.108981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color correction techniques in digital photography often rely on the use of color correction charts, which require including this relatively large object in the field of view. We propose here to use QR Codes to pack these color charts in a compact form factor, in a fully compatible manner with conventional black and white QR Codes; this is, without losing any of their easy location, sampling and digital data storage features. First, we present an algorithm to build these new colored QR Codes that preserves the original QR Code functionality - much more than other coloring proposals based on the random substitution of black and white pixels by colors - that relies on the ability of the native CRC code to correct and counteract these alterations. Second, we demonstrate that, as a result, these QR Codes can allocate far many more colors than the conventional color correction charts, enabling much more accurate color correction schemes in a more convenient and usable format.},
  archive      = {J_PR},
  author       = {Ismael Benito-Altamirano and David Martínez-Carpena and Olga Casals and Cristian Fábrega and Andreas Waag and Joan Daniel Prades},
  doi          = {10.1016/j.patcog.2022.108981},
  journal      = {Pattern Recognition},
  pages        = {108981},
  shortjournal = {Pattern Recognition},
  title        = {Back-compatible color QR codes for colorimetric applications},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive momentum variance for attention-guided sparse
adversarial attacks. <em>PR</em>, <em>133</em>, 108979. (<a
href="https://doi.org/10.1016/j.patcog.2022.108979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The phenomenon that deep neural networks are vulnerable to adversarial examples has been found for several years. Under the black-box setting, transfer-based methods usually produce the adversarial examples on a white-box model, which serves as the surrogate model in the black-box attack, and hope that the same adversarial examples can also fool the black-box model. However, these methods have high success rates for the surrogate model and exhibit weak transferability for the black-box model. In addition, some studies have shown that deep neural networks are also vulnerable to sparse alterations of the input, but existing sparse attacks mainly focus on the number of attacked pixels without restricting the size of the perturbations, which is perceptible to human eyes. To address the above problems, we propose a transfer-based sparse attack method, called adaptive momentum variance based iterative gradient method with a class activation map, where the method considers a simple adaptive momentum variance and a refining perturbation mechanism to improve the transferability of adversarial examples. Also, a class activation map, which is also known as attention mechanism , is employed to explore the relationship between the number of the perturbed pixels and the attack performance in the case of limiting the intensity of perturbation. The proposed method is compared with a number of the state-of-the-art transfer-based adversarial attack methods on the ImageNet dataset, and the empirical results demonstrate that our method achieves a significant increase in transferability with only attacking about 50\% of the pixels.},
  archive      = {J_PR},
  author       = {Chao Li and Wen Yao and Handing Wang and Tingsong Jiang},
  doi          = {10.1016/j.patcog.2022.108979},
  journal      = {Pattern Recognition},
  pages        = {108979},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive momentum variance for attention-guided sparse adversarial attacks},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural graph embeddings as explicit low-rank matrix
factorization for link prediction. <em>PR</em>, <em>133</em>, 108977.
(<a href="https://doi.org/10.1016/j.patcog.2022.108977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning good quality neural graph embeddings has long been achieved by minimzing the pointwise mutual information (PMI) for co-occuring nodes in simulated random walks. This design choice has been mostly popularized by the direct application of the highly-successful word embedding algorithm word2vec to predicting the formation of new links in social, co-citation, and biological networks . However, such a skeuomorphic design of graph embedding methods entails a truncation of information coming from pairs of nodes with low PMI. To circumvent this issue, we propose an improved approach to learning low-rank factorization embeddings that incorporate information from such unlikely pairs of nodes and show that it can improve the link prediction performance of baseline methods from 1.2\% to 24.2\%. Based on our results and observations, we outline further steps that could improve the design of next graph embedding algorithms that are based on matrix factorizaion.},
  archive      = {J_PR},
  author       = {Asan Agibetov},
  doi          = {10.1016/j.patcog.2022.108977},
  journal      = {Pattern Recognition},
  pages        = {108977},
  shortjournal = {Pattern Recognition},
  title        = {Neural graph embeddings as explicit low-rank matrix factorization for link prediction},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified model for the sparse optimal scoring problem.
<em>PR</em>, <em>133</em>, 108976. (<a
href="https://doi.org/10.1016/j.patcog.2022.108976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal scoring (OS), an equivalent form of linear discriminant analysis (LDA), is an important supervised learning method and dimensionality reduction tool. However, it is still a challenge for the classical OS on small sample size (SSS) datasets. In this paper, to find sparse discriminant vectors, we propose a unified model for sparse optimal scoring (SOS) by virtue of the generalized ℓ q ℓq -norm ( 0 ≤ q ≤ 1 0≤q≤1 ). To overcome the difficulty in treating the generalized ℓ q ℓq -norm, we propose an efficient alternative direction method of multipliers (ADMM), where proximity operator of ℓ q ℓq -norm is employed for different q q values. Meanwhile, the convergence results of our method are also established. Numerical experiments on artificial and benchmark datasets demonstrate the effectiveness and feasibility of our proposed method.},
  archive      = {J_PR},
  author       = {Guoquan Li and Linxi Yang and Kequan Zhao},
  doi          = {10.1016/j.patcog.2022.108976},
  journal      = {Pattern Recognition},
  pages        = {108976},
  shortjournal = {Pattern Recognition},
  title        = {A unified model for the sparse optimal scoring problem},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WITS: Weakly-supervised individual tooth segmentation model
trained on box-level labels. <em>PR</em>, <em>133</em>, 108974. (<a
href="https://doi.org/10.1016/j.patcog.2022.108974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately and automatically segmenting teeth from cone-beam computed tomography (CBCT) images plays an essential role in dental disease diagnosis and treatment. This paper presents an automatic tooth segmentation model that combines deep learning methods and level-set approaches. The proposed model uses a deep learning method to detect each tooth’s location and size and generates prior ellipses from those detected boundary boxes. Calculating each point’s signed distance to the prior edge and using them as prior weights, the restriction term can constrain the evolution of level set functions according to the distance to the prior ellipses. Then, we use the curvature direction to find out joint points of teeth and employ a variational model to separate them to get individual results. By quantitative evaluation , we show that the proposed model can accurately segment teeth. The performance is more accurate and stable than those of classical level-set models and deep-learning models. For example, the Dice coefficient is increased by 7\% 7\% than that of the U-Net model. Besides, we will release the code on https://github.com/ruicx/Individual-Tooth-Segmentation-with-Rectangle-Labels .},
  archive      = {J_PR},
  author       = {Ruicheng Xie and Yunyun Yang and Zhaoyang Chen},
  doi          = {10.1016/j.patcog.2022.108974},
  journal      = {Pattern Recognition},
  pages        = {108974},
  shortjournal = {Pattern Recognition},
  title        = {WITS: Weakly-supervised individual tooth segmentation model trained on box-level labels},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GripNet: Graph information propagation on supergraph for
heterogeneous graphs. <em>PR</em>, <em>133</em>, 108973. (<a
href="https://doi.org/10.1016/j.patcog.2022.108973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous graph representation learning aims to learn low-dimensional vector representations of different types of entities and relations to empower downstream tasks. Existing popular methods either capture semantic relationships but indirectly leverage node/edge attributes in a complex way, or leverage node/edge attributes directly without taking semantic relationships into account. When involving multiple convolution operations, they also have poor scalability. To overcome these limitations, this paper proposes a flexible and efficient Gr aph i nformation p ropagation Net work (GripNet) framework. Specifically, we introduce a new supergraph data structure consisting of supervertices and superedges. A supervertex is a semantically-coherent subgraph. A superedge defines an information propagation path between two supervertices. GripNet learns new representations for the supervertex of interest by propagating information along the defined path using multiple layers. We construct multiple large-scale graphs and evaluate GripNet against competing methods to show its superiority in link prediction, node classification, and data integration. The code and data are available at https://github.com/nyxflower/GripNet .},
  archive      = {J_PR},
  author       = {Hao Xu and Shengqi Sang and Peizhen Bai and Ruike Li and Laurence Yang and Haiping Lu},
  doi          = {10.1016/j.patcog.2022.108973},
  journal      = {Pattern Recognition},
  pages        = {108973},
  shortjournal = {Pattern Recognition},
  title        = {GripNet: Graph information propagation on supergraph for heterogeneous graphs},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A landmark-free approach for automatic, dense and robust
correspondence of 3D faces. <em>PR</em>, <em>133</em>, 108971. (<a
href="https://doi.org/10.1016/j.patcog.2022.108971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global dense registration of 3D faces commonly prioritizes correspondences of facial landmarks which are fiducial points for the anatomical structures. However, it is not always easy to pre-annotate the landmarks accurately in raw scans of 3D faces. Contrary to the current state-of-the-art in dense 3D face correspondence, we propose a general framework without pre-annotated landmarks, which promotes its robustness and allows the meshes to deform in a uniform manner. The proposed framework includes two stages: first the correspondences are established using a template face; and then we select some well-reconstructed samples to build a prior model and leverage it into the correspondence process of other samples. In both stages, the dense registration is revisited in two perspectives: semantic and topological correspondence. In the latter stage, we further incorporate shape and normal statistics of 3D faces to regularize the correspondence process for more robust results. This provides a feasible way to handle data with noises and occlusions, as well as large deformation caused by facial expressions. Our basic idea is to gradually refine the correspondence of individual points in a way global-to-local. At the same time, we solve the local-to-global deformation based on the refined correspondences. The two processes are alternated, and aided by some confidence checks for each individual points. In the experiments, the proposed method is evaluated both qualitatively and quantitatively on three datasets including two publicly available ones: FRGC v2.0 and BU-3DFE datasets, demonstrating its effectiveness.},
  archive      = {J_PR},
  author       = {Zhenfeng Fan and Xiyuan Hu and Chen Chen and Xiaolian Wang and Silong Peng},
  doi          = {10.1016/j.patcog.2022.108971},
  journal      = {Pattern Recognition},
  pages        = {108971},
  shortjournal = {Pattern Recognition},
  title        = {A landmark-free approach for automatic, dense and robust correspondence of 3D faces},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GCM: Efficient video recognition with glance and combine
module. <em>PR</em>, <em>133</em>, 108970. (<a
href="https://doi.org/10.1016/j.patcog.2022.108970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present an efficient and powerful building block for video action recognition, dubbed Glance and Combine Module (GCM). In order to obtain a broader perspective of the video features, GCM introduces an extra glancing operation with a larger receptive field over both the spatial and temporal dimensions, and combines features with different receptive fields for further processing. We show in our ablation studies that the proposed GCM is much more efficient than other forms of 3D spatio-temporal convolutional blocks. We build a series of GCM networks by stacking GCM repeatedly, and train them from scratch on the target datasets directly. On the Kinetics-400 dataset which focuses more on appearance rather than action, our GCM networks can achieve similar accuracy as others without pre-training on ImageNet. For the more action-centric recognition datasets such as Something-Something (V1 &amp; V2) and Multi-Moments in Time, the GCM networks achieve state-of-the-art performance with less than two thirds the computational complexity of other models. With only 19.2 GFLOPs of computation, our GCMNet 15 15 can obtain 63.9\% top-1 classification accuracy on Something-Something-V2 validation set under single-crop testing. On the fine-grained action recognition dataset FineGym, we beat the previous state-of-the-art accuracy achieved with 2-stream methods by more than 6\% using only RGB input.},
  archive      = {J_PR},
  author       = {Yichen Zhou and Ziyuan Huang and Xulei Yang and Marcelo Ang and Teck Khim Ng},
  doi          = {10.1016/j.patcog.2022.108970},
  journal      = {Pattern Recognition},
  pages        = {108970},
  shortjournal = {Pattern Recognition},
  title        = {GCM: Efficient video recognition with glance and combine module},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retinal image enhancement with artifact reduction and
structure retention. <em>PR</em>, <em>133</em>, 108968. (<a
href="https://doi.org/10.1016/j.patcog.2022.108968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancement of low-quality retinal fundus images is beneficial to clinical diagnosis of ophthalmic diseases and computer-aided analysis. Enhancement accuracy is a challenge for image generation models, especially when there is no supervision by paired images. To reduce artifacts and retain structural consistency for accuracy improvement, we develop an unpaired image generation method for fundus image enhancement with the proposed high-frequency extractor and feature descriptor . Specifically, we summarize three causes of tiny vessel-like artifacts which always appear in other image generation methods. A high frequency prior is incorporated into our model to reduce artifacts by the proposed high-frequency extractor. In addition, the feature descriptor is trained alternately with the generator using segmentation datasets and generated image pairs to ensure the fidelity of the image structure. Pseudo-label loss is proposed to improve the performance of the feature descriptor. Experimental results show that the proposed method performs better than other methods both qualitatively and quantitatively. The enhancement can improve the performance of segmentation and classification in retinal images.},
  archive      = {J_PR},
  author       = {Bingyu Yang and He Zhao and Lvchen Cao and Hanruo Liu and Ningli Wang and Huiqi Li},
  doi          = {10.1016/j.patcog.2022.108968},
  journal      = {Pattern Recognition},
  pages        = {108968},
  shortjournal = {Pattern Recognition},
  title        = {Retinal image enhancement with artifact reduction and structure retention},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
