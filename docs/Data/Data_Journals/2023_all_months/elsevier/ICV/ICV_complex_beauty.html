<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv---175">ICV - 175</h2>
<ul>
<li><details>
<summary>
(2023). Exploiting recollection effects for memory-based video
object segmentation. <em>ICV</em>, <em>140</em>, 104866. (<a
href="https://doi.org/10.1016/j.imavis.2023.104866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep learning have led to numerous studies on video object segmentation (VOS). Memory-based models, in particular, have demonstrated superior performance by leveraging the ability to store and recall information from previous frames. While extensive research efforts have been devoted to developing memory networks for effective VOS, only a few studies have investigated the quality of memory in terms of determining which information should be stored. In fact, in most recent memory-based VOS studies, the frame information is regularly stored in the memory without specific consideration. In other words, there is a lack of explicit criteria or guidelines for determining the essential information that should be retained in memory. In this study, we introduce a new method for evaluating the effect of storing the features, which can be used for various memory-based networks to improve performance in a plug-and-play manner. For this purpose, we introduce the concept of recollection effects, which refers to the stability of predictions based on the presence or absence of specific features in memory. By explicitly measuring the recollection effects, we establish a criterion for evaluating the relevance of information and determining whether features from a particular frame should be stored. This approach effectively encourages memory-based networks to construct memory that contains valuable cues. To validate the effectiveness of our method, we conduct comparative experiments . Experimental results demonstrate the effectiveness of our method to enhance the selection and retention of useful cues within the memory, leading to improving segmentation results.},
  archive      = {J_ICV},
  author       = {Enki Cho and Minkuk Kim and Hyung-Il Kim and Jinyoung Moon and Seong Tae Kim},
  doi          = {10.1016/j.imavis.2023.104866},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104866},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploiting recollection effects for memory-based video object segmentation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A few-shot learning-based ischemic stroke segmentation
system using weighted MRI fusion. <em>ICV</em>, <em>140</em>, 104865.
(<a href="https://doi.org/10.1016/j.imavis.2023.104865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke, particularly ischemic stroke , is a major cause of disability and one of the leading causes of adult mortality worldwide. Early and prompt management of stroke patients can reduce the severity of the disease. Doctors usually determine the severity of a stroke by focusing on the region of interest (ROI) in the MRI or CT scan images. An accurate and effective automatic image segmentation system can assist medical professionals as well as automatic detection and classification systems. Deep learning is the current advanced approach for dealing with machine learning and artificial intelligence . However, conventional deep learning requires a large amount of data for training, and the amount of labeled data in the medical field is limited. In this paper, we propose a few-shot learning strategy and integrate it with a base convolutional neural network model , which utilizes a self-attention mechanism to segment MRI for ischemic stroke. By combining the base model with self-attention, we can focus more on the ROI and disregard less important features. Additionally, the proposed system only selects slices with lesions and ignores unlesioned slices. This helps to improve efficiency and reduce the computational load by eliminating the need to tune unnecessary parameters. To achieve even better results, the system also combines two weighted images, FLAIR and DWI , as an early fusion process. Experiments have shown that this approach leads to higher performance compared to using the same system without fusion. The proposed system is evaluated using a publicly available dataset, ISLES 2015 SSIS, and compared with other state-of-the-art (SOTA) systems. It achieves a dice coefficient score of 0.68, which is significantly better than that of other SOTA systems.},
  archive      = {J_ICV},
  author       = {Fatima Alshehri and Ghulam Muhammad},
  doi          = {10.1016/j.imavis.2023.104865},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104865},
  shortjournal = {Image Vis. Comput.},
  title        = {A few-shot learning-based ischemic stroke segmentation system using weighted MRI fusion},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). External knowledge-assisted transformer for image
captioning. <em>ICV</em>, <em>140</em>, 104864. (<a
href="https://doi.org/10.1016/j.imavis.2023.104864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internal relationship exploring based on the descriptive region features of image objects and grid features have contributed significantly to the development of image captioning , especially when combined with Transformer architecture. However, when conducting self-attention calculation, most of these methods only consider the relationship intra-objects and ignore the connection between entities and background. Besides, the way of exploring the relation information inside the image can also be extended. In this paper, we introduce a novel Mixed Knowledge Relation Transformer (MKRT) to explore the relationship between objects from both internal attribute relationship and external the object-verb-subject relationship. Furthermore we embed the important image background information into the relation module. In MKRT, the semantic relation obtained from the external knowledge is incorporated into the relation modeling in the novel Mixed Knowledge Relation Attention (MKRA). To validate the effectiveness of our model, we conduct extensive experiments on the most popular MSCOCO dataset, and achieve 134.5 CIDEr score on the offline test split and 133.5 CIDEr (c40) score on the official online testing server.},
  archive      = {J_ICV},
  author       = {Zhixin Li and Qiang Su and Tianyu Chen},
  doi          = {10.1016/j.imavis.2023.104864},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104864},
  shortjournal = {Image Vis. Comput.},
  title        = {External knowledge-assisted transformer for image captioning},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STRFormer: Spatial–temporal–ReTemporal transformer for 3D
human pose estimation. <em>ICV</em>, <em>140</em>, 104863. (<a
href="https://doi.org/10.1016/j.imavis.2023.104863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based methods have emerged as the golden standard in 2D-3D human pose estimation from video sequences, largely thanks to their powerful spatial–temporal feature encoders. In the past, researchers have made concerted efforts to engineer spatial and temporal encoders using transformer blocks. This approach involved a dramatic reshaping of the input, transforming it from mere joint information to dynamic joint trajectories. Despite this, the inherent limitations of the spatial–temporal structure have resulted in an inadequate acquisition and subsequent utilization of temporal information. In an attempt to rectify this prevalent issue, our paper proposes a new model, dubbed Spatial–Temporal-ReTemporal Transformer (i.e., STRFormer). This model ingeniously employs two separate temporal transformer blocks to extract the essential temporal motion information from video sequences. Intriguingly, one temporal transformer block is dedicated to the original video sequence, while the other concerns itself with the reversed order video. This novel approach allows for a more thorough investigation and utilization of temporal information from the video sequences. In order to alternate the processing of these two blocks effectively with the spatial block, we focus on maximizing the extraction of temporal domain information. This method leads to a more comprehensive understanding of the pose estimation and its evolution over time. Furthermore, we introduce a novel error metric, Mean Per-Joint Position Acceleration Error (i.e., MPJAE). This advanced metric takes into account the body part velocity in adjacent predicted frames, allowing for a more detailed evaluation of the predicted poses. We conduct extensive experiments on various open benchmarks to evaluate the effectiveness of our proposed model. The results demonstrate that our STRFormer, coupled with the MPJAE loss, achieves highly competitive results when compared with other state-of-the-art models. This illustrates its promising potential and practical applicability in 2D-3D human pose estimation tasks. We plan to release our code publicly for further research.},
  archive      = {J_ICV},
  author       = {Xing Liu and Hao Tang},
  doi          = {10.1016/j.imavis.2023.104863},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104863},
  shortjournal = {Image Vis. Comput.},
  title        = {STRFormer: Spatial–Temporal–ReTemporal transformer for 3D human pose estimation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial color projection: A projector-based
physical-world attack to DNNs. <em>ICV</em>, <em>140</em>, 104861. (<a
href="https://doi.org/10.1016/j.imavis.2023.104861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep neural networks (DNNs) have made remarkable advancements in various fields recently, the latest research indicates that DNNs are susceptible to disruptions from minor perturbations. However, conventional physical attacks employing stickers as physical perturbations to deceive classifiers encounter challenges in achieving stealthiness and are susceptible to issues such as printing quality loss. Recent advancements in physical attacks have harnessed light beams to execute attacks, producing artificial optical patterns rather than natural ones. In this study, we introduce a black-box projector-based physical attack called Adversarial Color Projection ( AdvCP ), which manipulates the physical parameters of color projection to execute adversarial attacks . AdvCP revolves around three pivotal criteria: effectiveness, stealthiness, and robustness. In a digital environment, our approach attains an impressive attack success rate of 97.60% on a subset of ImageNet. In the physical realm, we achieve a remarkable 100% attack success rate in indoor testing and 82.14% in outdoor testing. To underscore the stealthiness of our approach, we juxtapose the adversarial samples generated by AdvCP with baseline samples. When applied to challenge advanced and robust DNNs, our experimental results reveal that our method achieves an attack success rate exceeding 85% across most all of the models, establishing the robustness of AdvCP. Finally, we contemplate the potential threats posed by AdvCP to future vision-based systems and applications , and proffer some innovative concepts pertaining to light-based physical attacks. Our code can be accessed from the following link: https://github.com/ChengYinHu/AdvCP.git},
  archive      = {J_ICV},
  author       = {Chengyin Hu and Weiwen Shi and Ling Tian},
  doi          = {10.1016/j.imavis.2023.104861},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104861},
  shortjournal = {Image Vis. Comput.},
  title        = {Adversarial color projection: A projector-based physical-world attack to DNNs},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving metric-based few-shot learning with dynamically
scaled softmax loss. <em>ICV</em>, <em>140</em>, 104860. (<a
href="https://doi.org/10.1016/j.imavis.2023.104860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metric-based learning framework has been widely used in data-scarce few-shot visual classification. However, the current loss function limits the effectiveness of metric learning. One issue is that the nearest neighbor classification technique used greatly narrows the value range of similarity between the query and class prototypes, which limits the guiding ability of the loss function. The other issue is that the episode-based training setting randomizes the class combination in each iteration, which reduces the perception of the traditional softmax losses for effective learning from episodes with various data distributions .To solve these problems, we first review some variants of the softmax loss from a unified perspective, and then propose a novel Dynamically-Scaled Softmax Loss (DSSL). By adding a probability regulator (for scaling probabilities) and a loss regulator (for scaling losses), the loss function can adaptively adjust the prediction distribution and the training weights of the samples, which forces the model to focus on more informative samples. Finally, we found the proposed DSSL strategy for few-shot classifiers can achieve competitive results on four generic benchmarks and a fine-grained benchmark, demonstrating the effectiveness of improving the distinguishability (for base classes) and generalizability (for novel classes) of the learned feature space .},
  archive      = {J_ICV},
  author       = {Yu Zhang and Xin Zuo and Xuxu Zheng and Xiaoyong Gao and Bo Wang and Weiming Hu},
  doi          = {10.1016/j.imavis.2023.104860},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104860},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving metric-based few-shot learning with dynamically scaled softmax loss},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AGLC-GAN: Attention-based global-local cycle-consistent
generative adversarial networks for unpaired single image dehazing.
<em>ICV</em>, <em>140</em>, 104859. (<a
href="https://doi.org/10.1016/j.imavis.2023.104859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is a critical image pre-processing task to estimate the haze-free images corresponding to the input hazy images. Despite the recent advances, the task of image dehazing remains challenging, especially in the unsupervised scenario. Several efforts can be found in the literature to dehaze images in a supervised set-up, where a huge number of paired (clear and hazy images) images are required for training. The supervised approaches often become biased towards the nature of haze present in the training hazy images, and produce less realistic images for query hazy images. We propose an Attention-based Global–Local Cycle-consistent Generative Adversarial Network (AGLC-GAN) for Unpaired Single Image Dehazing. The proposed CycleGAN-based AGLC-GAN model contains a dehazing generator encapsulating an autoencoder-like network with an attention mechanism comprising channel attention and pixel attention to deal with uneven haze intensity across the image. We use a global–local consistent discriminator to identify spatially varying haze and improve the stability of the discriminator . We adopt cyclic perceptual consistency loss to maintain consistency in the feature space . A dynamic feature enhancement module and an adaptive mix-up module are included in the proposed generator to dynamically obtain more spatially structured features and hence, adaptively preserve the flow of shallow features. Furthermore, we extensively experiment with the proposed model on multiple benchmark datasets for evaluating the efficacy of removing haze. The results of the experiments conducted in the study, demonstrate a significant quantitative and qualitative improvement over the existing methods for unpaired image dehazing. 1},
  archive      = {J_ICV},
  author       = {R.S. Jaisurya and Snehasis Mukherjee},
  doi          = {10.1016/j.imavis.2023.104859},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104859},
  shortjournal = {Image Vis. Comput.},
  title        = {AGLC-GAN: Attention-based global-local cycle-consistent generative adversarial networks for unpaired single image dehazing},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pose aligned modality-invariant feature learning for NIR–VIS
heterogeneous face recognition. <em>ICV</em>, <em>140</em>, 104858. (<a
href="https://doi.org/10.1016/j.imavis.2023.104858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both pose disparities and modality differences with various modalities are significant difficulties that have an impact on recognition accuracy in heterogeneous face recognition . In this paper, we propose the pose aligned modality-invariant feature learning (PAMFL) method for NIR–VIS face recognition. This method disentangles the processing of the face pose and modality into independent stages. In the first phase, we construct the face pose alignment module (PAM). The built StyleGAN2-based generator incorporates pose estimation and feature mapping structures to alter the face shape in accordance with pose yaw angle instructions, eliminating the face pose misalignment. In the second phase, we build the modality-invariant feature learning module (MFLM). Modality-specific feature representations are learned using a pseudo-Siamese network in the shallow layer of the network, while modality-invariant feature representations are learned using a parameter sharing layer embedded in the deeper layer of the network. This module preserves all modality-invariant features while minimizing cross-modality variation. Finally, comparative experiments on BUAA VisNir, CASIA NIR–VIS 2.0 and Oulu CASIA NIR–VIS datasets validate that the proposed PAMFL shows advanced performance in overcoming face pose misalignment and improving heterogeneous face recognition accuracy.},
  archive      = {J_ICV},
  author       = {Rui Sun and Xiaoquan Shan and Fei Wang and Zhiguo Fan},
  doi          = {10.1016/j.imavis.2023.104858},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104858},
  shortjournal = {Image Vis. Comput.},
  title        = {Pose aligned modality-invariant feature learning for NIR–VIS heterogeneous face recognition},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconstruction with robustness: A semantic prior guided face
super-resolution framework for multiple degradations. <em>ICV</em>,
<em>140</em>, 104857. (<a
href="https://doi.org/10.1016/j.imavis.2023.104857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the rapid advancements made with the support of deep learning , Face Super-Resolution (FSR) methods still suffer from challenges under multiple degradations. These challenges significantly impede the practical applications of FSR methods in real-world scenarios. Incorporating facial priors could potentially relieve this issue. However, ground truth priors are not feasible in real-world applications, meanwhile the accuracy of predicted priors is difficult to guarantee, especially for low-resolution faces under multiple degradations. Hence, it is worth exploring how to effectively leverage facial priors for improving the robustness of FSR under multiple degradations. To tackle these problems, we propose RSemFace, a robust semantic prior guided FSR framework to reconstruct multiple degraded faces. In RSemFace, we design the Degradation Stage to synthesize multiple degraded low-resolution faces with a variety of interpolations, noise levels , blurring kernels, and even the real-world interference. The Generation Stage generates Coarse-SR faces, and extracts semantic features from the Coarse-SR as priors, which are used to the reconstruction of Fine-SR faces with the support of Semantic Feature Attention Blocks (SFABs) and Semantic Loss. Both quantitative and qualitative results show the better robustness of our RSemFace for content recovery and perceptual quality in simultaneously handling multiple degraded faces compared with other state-of-the-art methods. Lastly, faces reconstructed by RSemFace are proven to improve the high-level vision task due to better recovered identities.},
  archive      = {J_ICV},
  author       = {Hongjun Wu and Haoran Qi and Huanrong Zhang and Zhi Jin and Driton Salihu and Jian-Fang Hu},
  doi          = {10.1016/j.imavis.2023.104857},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104857},
  shortjournal = {Image Vis. Comput.},
  title        = {Reconstruction with robustness: A semantic prior guided face super-resolution framework for multiple degradations},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RTDOD: A large-scale RGB-thermal domain-incremental object
detection dataset for UAVs. <em>ICV</em>, <em>140</em>, 104856. (<a
href="https://doi.org/10.1016/j.imavis.2023.104856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, visual understanding using unmanned aerial vehicles (UAVs) has gained significant attention due to its wide range of applications, including delivery, security investigation and surveillance. However, most existing UAV-based datasets only capture color images under ideal illumination and weather conditions, typically sunny days. This limitation fails to account for the complexity of real-world scenarios, such as cloudy or foggy weather, and nighttime conditions. Deep learning methods trained on color images with good lighting and weather conditions struggle to adapt to the complex visual scenes in these scenarios. Moreover, color images may not provide sufficient visual information under the complex visual scenes. To bridge this gap and meet the demands of real-world applications, we propose a large-scale RGB-Thermal Domain-incremental Object Detection (RTDOD) dataset in this paper. Our dataset includes RGB and thermal videos synchronously captured using calibrated color thermal cameras mounted on UAVs. It covers various weather conditions, from sunny to foggy to rainy, and spans from day to night. We sample and obtain approximately 16,200 pairs of images, and manually label dense annotations, including object bounding boxes and object categories. With the proposed dataset, we introduce a challenging domain-incremental object detection task. We also present a baseline approach that uses task-related gates to filter features for knowledge distillation to reduce forgetting. Experimental results on the RTDOD dataset demonstrate the effectiveness of our proposed method in domain-incremental object detection. To facilitate future research and development in domain-incremental object detection tasks on aerial images , the RTDOD dataset and our baseline model are made available at https://github.com/fenght96/RTDOD . ARTICLE INFO.},
  archive      = {J_ICV},
  author       = {Hangtao Feng and Lu Zhang and Siqi Zhang and Dong Wang and Xu Yang and Zhiyong Liu},
  doi          = {10.1016/j.imavis.2023.104856},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104856},
  shortjournal = {Image Vis. Comput.},
  title        = {RTDOD: A large-scale RGB-thermal domain-incremental object detection dataset for UAVs},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight detection network based on receptive-field
feature enhancement convolution and three dimensions attention for
images captured by UAVs. <em>ICV</em>, <em>140</em>, 104855. (<a
href="https://doi.org/10.1016/j.imavis.2023.104855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV sampling can not only adapt to various complex terrain environments but also provide a broader vision. However, images captured by UAVs usually contain complex backgrounds and a large number of small objects. This poses a significant challenge to some existing advanced object detectors. Moreover, some existing state-of-the-art lightweight detectors have too many parameters and computational overheads, which are not friendly to lightweight devices. Responding to the above issues, we propose a single-stage detector named features enhancement and shift lightweight network in this work. Firstly, a lightweight adjust convolution is proposed, which unfolds the features and encodes the 3 × 3 background information into information-rich 1 × 1 1×1 features by averaging the pooling and convolution layers , which efficiently enhances the representation of 1 × 1 convolutional extracted features. Next, to efficiently suppress complex background information, we propose a three-dimensions attention module, which interacts information on the C-W, C-H and H-W dimensions in a unique way to obtain three efficient attention maps that highlight important information to weaken irrelevant information. Moreover, we create a novel receptive-field feature enhancement convolution, which unfolds the features and then interacts the 3 × 3 features to obtain weighted weights. The 3 × 3 convolution combining weighted features becomes parametric unshared convolution in principle, which enhances the ability to capture detailed information. Finally, in order to retain richer object and semantic information, we carefully analyze the down-sampling convolution and propose a feature shift down-sampling convolution. Then we combine it and improve Neck to get a new lightweight Neck. Furthermore, experiments on the VisDrone-DET2021 dataset show that our method obtained 36.21% on mAP50, which is 9.78% higher than the baseline model YOLOv5n. Meanwhile, compared with the advanced lightweight networks YOLOX-tiny, YOLOv6n, YOLOv7-tiny, and YOLOv8n, our network achieves superior detection results using fewer number of parameters. We also compare our network with the latest networks trained on images captured by UAVs, and experimentally demonstrate that our network achieves excellent performance using only 1.7 M parameters and 8.3 GFLOPS .},
  archive      = {J_ICV},
  author       = {Tingting Song and Xin Zhang and Degang Yang and Yichen Ye and Chen Liu and Jie Zhou and Yingze Song},
  doi          = {10.1016/j.imavis.2023.104855},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104855},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight detection network based on receptive-field feature enhancement convolution and three dimensions attention for images captured by UAVs},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Perception-guided defocus blur detection based on SVD
feature. <em>ICV</em>, <em>140</em>, 104845. (<a
href="https://doi.org/10.1016/j.imavis.2023.104845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defocus blur detection aims to identify out-of-focus regions in a single image. Although defocus blur detection has attracted more and more attention in recent years, it still faces some challenges. In particular, the in-focus regions with low contrast are easily misidentified as out-of-focus regions. To address this problem, a perception-guided defocus blur detection method is proposed to estimate defocus blur amounts at edge locations from a single image based on the singular value decomposition (SVD) features. Inspired by the fact that blurring the clear region produces significant changes in SVD domain as compared with re-blurring the blurred one, new SVD features are extracted based on re-blurred singular value difference (RESVD) of the corresponding local gradient patches. Then, perceptual weight based on Just Noticeable Blur (JNB) is introduced to guide the sparse blur map estimation obtained with the SVD features. Finally, the full defocus blur map is constructed from the sparse defocus blur map by the Matting Laplace algorithm. Visual evaluations are conducted on the CUHK, DUT and CTCUG datasets, employing mean absolute error (MAE) and F β Fβ -measure as quantitative evaluation metrics. The experimental results demonstrate that the proposed defocus blur detection method is superior to 13 state-of-the-art methods. On the DUT dataset, the proposed method yields a high F β Fβ -measure (0.802) with a low MAE (0.081) compared to other methods ( F β ≤ 0.799 Fβ≤0.799 , MAE ≥ 0.099 ≥0.099 ). On the CUHK and CTCUG datasets, our method achieves the best balance between F β Fβ -measure and MAE. Furthermore, our method results in better visual effects than other methods regarding realism and quality.},
  archive      = {J_ICV},
  author       = {Xiaopan Li and Shiqian Wu and Jiaxin Wu and Shoulie Xie and Sos Agaian},
  doi          = {10.1016/j.imavis.2023.104845},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104845},
  shortjournal = {Image Vis. Comput.},
  title        = {Perception-guided defocus blur detection based on SVD feature},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DGSN: Learning how to segment pedestrians from other
datasets for occluded person re-identification. <em>ICV</em>,
<em>140</em>, 104844. (<a
href="https://doi.org/10.1016/j.imavis.2023.104844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present three major challenges in occluded person Re-Identification (ReID): different occlusions, background interference, and dataset bias. To address the first and second challenges, our approach incorporates pedestrian segmentation to distinguish between pedestrian and non-pedestrian regions. Additionally, to tackle the third challenge, we introduce an effective image enhancement method called Enhanced Random Occluding (ERO). ERO leverages other datasets with segmentation annotations to compensate for the lack of detailed annotations in ReID datasets. We compare the effectiveness of ERO with existing methods. To utilize the prior knowledge obtained by ERO, we introduce the Priori Segmentation Module (PSM) and the Domain Generalization Module (DGM). The PSM module enables learning out-of-domain prior knowledge without relying on external networks, while the DGM module transfers this knowledge to the current domain. Finally, we utilize the obtained segmentation results as attention maps for feature aggregation. ERO, PSM, and DGM together constitute the Domain Generalization Segmentation Network (DGSN). Our experimental results on occluded and holistic person ReID benchmarks demonstrate the superiority of the proposed DGSN. On the Occluded-Duke dataset, we achieved a mAP of 69.9% (+ 2.0%) and a rank-1 accuracy of 60.7% (+ 0.3%), surpassing state-of-the-art methods significantly.},
  archive      = {J_ICV},
  author       = {Yujie Liu and Zhaoyong Wang and Wenxin Zhang and Zongmin Li},
  doi          = {10.1016/j.imavis.2023.104844},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104844},
  shortjournal = {Image Vis. Comput.},
  title        = {DGSN: Learning how to segment pedestrians from other datasets for occluded person re-identification},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face and body-shape integration model for cloth-changing
person re-identification. <em>ICV</em>, <em>140</em>, 104843. (<a
href="https://doi.org/10.1016/j.imavis.2023.104843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the existing deep learning-based person re-identification (ReID) methods, human parsing based on semantic segmentation is the most promising solution for ReID because such models can learn to identify fine-grained details of different body parts or apparel of a target semantically. However, intra-class variations such as illumination changes, multi-pose angles, and cloth-changing (CC) across different non-overlapping camera viewpoints present a crucial challenge for this approach. Among these challenges, a person CC is the most distinctive problem for ReID models, which often fail to associate the target in new cloth against the learned feature semantics of the previous cloth worn in a different timeline. In this paper, we propose a face and body-shape integration (FBI) network as a tactical solution to address the long-term person CC-ReID problem. The FBI comprises hierarchically stacked parsing and edge prediction (PEP) CNN blocks that generate fine-grained human-parsing output at the initial stage. We then aligned the PEP to our proposed model agnostic plug-in feature overlay module (FOM) to mask cloth-relevant body attributes except the facial features pooled from the input sample. Thus, our human parsing PEP and FOM modules are attuned to discriminatively learn cloth-irrelevant features of the target pedestrian(s) to optimize the effectiveness of person ReID in solitary or minimally crowded areas. In our extensive person CC-ReID experiments, our FBI model achieves 83.4/61.8 in R1 and 91.7/65.8 in mAP evaluation results on the PRCC and LTCC datasets, respectively; thereby significantly out-competing several previous state-of-the-art ReID methods, and validating the effectiveness of the FBI.},
  archive      = {J_ICV},
  author       = {Obinna Agbodike and Weijin Zhang and Jenhui Chen and Lei Wang},
  doi          = {10.1016/j.imavis.2023.104843},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104843},
  shortjournal = {Image Vis. Comput.},
  title        = {Face and body-shape integration model for cloth-changing person re-identification},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving defocus blur detection via adaptive supervision
prior-tokens. <em>ICV</em>, <em>140</em>, 104842. (<a
href="https://doi.org/10.1016/j.imavis.2023.104842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Defocus Blur Detection (DBD) technique is devised to accurately identify regions of blurriness within images. The prediction difficulty of defocused pixels is closely associated with their spatial location . Owing to the cluttered background, pixels near the edges are more prone to erroneous predictions. To address the issue of uneven pixel distribution at the edges of defocused regions, we deliberately decouple the original labels into Prior-Tokaens: Edge Transition Detail Region (EDR) and Structure Body Region (SBR). Subsequently, we propose a novel adaptive multi-supervised network comprising a feature extraction module, a feature fusion network (FFN), and a Multi-scale Channel Attention Module (MCAM). This method harnesses complementary features between SBR and EDR, furnishing a tailored feature learning strategy that outperforms traditional single-supervised techniques. Furthermore, considering that features generated with varying receptive fields contain information at different levels, we introduce MCAM to identify feature pixels at different scales, enhancing semantic relevance . Moreover, for images with complex scenes, an adaptive learning scheme is developed to selectively fuse low-level detail features and high-level semantic information, thereby enhancing the model&#39;s generalization capability. The proposed approach outperforms state-of-the-art techniques on various evaluation metrics , as demonstrated through qualitative and quantitative analyses of popular public datasets.},
  archive      = {J_ICV},
  author       = {Huaguang Li and Wenhua Qian and Jinde Cao and Peng Liu},
  doi          = {10.1016/j.imavis.2023.104842},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104842},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving defocus blur detection via adaptive supervision prior-tokens},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relation-balanced graph convolutional network for 3D human
pose estimation. <em>ICV</em>, <em>140</em>, 104841. (<a
href="https://doi.org/10.1016/j.imavis.2023.104841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have been applied to 2D-to-3D human pose estimation (HPE) and have shown encouraging performance. However, existing GCNs model the relations between joints via individual kernels, which can be overly flexible and fail to capture common relational patterns due to the symmetric nature of the human body. Although some GCNs share kernels to capture common relations, the unified way for all neighbors limits relational diversity to some extent. In order to balance the diversity and commonality of relations, we conduct a comprehensive study of existing kernel-sharing strategies and propose a Relation-balanced Graph Convolutional Network (RbGC-Net). RbGC-Net introduces the Part-Specific Kernel-Sharing strategy (PSKS) that assigns kernels based on the semantic meanings of neighbors to establish specific relational patterns for different types of neighborhoods. Furthermore, RbGC-Net incorporates a Local–Global Feature Fusion module (LGFF) that extracts the local relations among joints and balances them with the final global relations to improve the interactions between joints. Compared with state-of-the-art methods for 3D HPE, our RbGC-Net achieves the optimal balance between model size and estimation errors . Results on two benchmark Human3.6 M and MPI-INF-3DHP datasets demonstrate the excellent performance and strong generalization ability of our pure GCN-based method.},
  archive      = {J_ICV},
  author       = {Lu Chen and Qiong Liu},
  doi          = {10.1016/j.imavis.2023.104841},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104841},
  shortjournal = {Image Vis. Comput.},
  title        = {Relation-balanced graph convolutional network for 3D human pose estimation},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal spatial relational attention networks for visual
question answering. <em>ICV</em>, <em>140</em>, 104840. (<a
href="https://doi.org/10.1016/j.imavis.2023.104840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a task that requires VQA model to fully understand the visual information of the image and the language information of the question, and then combine both to provide an answer. Recently, a large amount of VQA approaches focus on modeling intra- and inter-modal interactions with respect to vision and language using a deep modular co-attention network, which can achieve a good performance . Despite their benefits, they also have their limitations. First, the question representation is obtained through Glove word embeddings and Recurrent Neural Network , which may not be sufficient to capture the intricate semantics of the question features. Second, they mostly use visual appearance features extracted by Faster R-CNN to interact with language features , and they ignore important spatial relations between objects in images, resulting in incomplete use of image information . To overcome the limitations of previous methods, we propose a novel Multi-modal Spatial Relation Attention Network (MSRAN) for VQA, which can introduce spatial relationships between objects to fully utilize the image information, thus improving the performance of VQA. In order to achieve the above, we design two types of spatial relational attention modules to comprehensively explore the attention schemes: (i) Self-Attention based on Explicit Spatial Relation (SA-ESR) module that explores geometric relationships between objects explicitly; and (ii) Self-Attention based on Implicit Spatial Relation (SA-ISR) module that can capture the hidden dynamic relationships between objects by using spatial relationship. Moreover, the pre-training model BERT , which replaces Glove word embeddings and Recurrent Neural Network , is applied to MSRAN in order to obtain the better question representation. Extensive experiments on two large benchmark datasets, VQA 2.0 and GQA, demonstrate that our proposed model achieves the state-of-the-art performance.},
  archive      = {J_ICV},
  author       = {Haibo Yao and Lipeng Wang and Chengtao Cai and Yuxin Sun and Zhi Zhang and Yongkang Luo},
  doi          = {10.1016/j.imavis.2023.104840},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104840},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-modal spatial relational attention networks for visual question answering},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KAN-AV dataset for audio-visual face and speech analysis in
the wild. <em>ICV</em>, <em>140</em>, 104839. (<a
href="https://doi.org/10.1016/j.imavis.2023.104839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-computer interaction is becoming increasingly prevalent in daily life with the adoption of intelligent devices. These devices must be capable of interacting in diverse settings, such as environments with noise, music and differing illumination and occlusion conditions. They must also interact with a variety of end users across ages and backgrounds. Therefore, the machine learning community needs in-the-wild multi-modal datasets to develop models for face and speech analysis so that they can be applicable in most real world scenarios. However, most existing audio and audio-visual databases are captured in controlled conditions with few or no age and kinship labels. In this paper, we introduce the KAN-AV dataset which contains 98 h of audio-visual data from 970 identities across ages. Two thirds of the identities have kin relations in the dataset. The dataset is manually annotated with labels for kinship, age, and gender and is intended to drive future research in face and speech analysis.},
  archive      = {J_ICV},
  author       = {Triantafyllos Kefalas and Eftychia Fotiadou and Markos Georgopoulos and Yannis Panagakis and Pingchuan Ma and Stavros Petridis and Themos Stafylakis and Maja Pantic},
  doi          = {10.1016/j.imavis.2023.104839},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104839},
  shortjournal = {Image Vis. Comput.},
  title        = {KAN-AV dataset for audio-visual face and speech analysis in the wild},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DiPS: Discriminative pseudo-label sampling with
self-supervised transformers for weakly supervised object localization.
<em>ICV</em>, <em>140</em>, 104838. (<a
href="https://doi.org/10.1016/j.imavis.2023.104838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised vision transformers (SSTs) have shown great potential to yield rich localization maps that highlight different objects in an image. However, these maps remain class-agnostic since the model is unsupervised. They often tend to decompose the image into multiple maps containing different objects while being unable to distinguish the object of interest from background noise objects. In this paper, Di scriminative P seudo-label S ampling (DiPS) is introduced to leverage these class-agnostic maps for weakly-supervised object localization (WSOL), where only image-class labels are available. Given multiple attention maps, DiPS relies on a pre-trained classifier to identify the most discriminative regions of each attention map. This ensures that the selected ROIs cover the correct image object while discarding the background ones, and, as such, provides a rich pool of diverse and discriminative proposals to cover different parts of the object. Subsequently, these proposals are used as pseudo-labels to train our new transformer-based WSOL model designed to perform classification and localization tasks. Unlike standard WSOL methods , DiPS optimizes performance in both tasks by using a transformer encoder and a dedicated output head for each task, each trained using dedicated loss functions. To avoid overfitting a single proposal and promote better object coverage, a single proposal is randomly selected among the top ones for a training image at each training step. Experimental results 1 on the challenging CUB, ILSVRC, OpenImages, and TelDrone datasets indicate that our architecture, in combination with our transformer-based proposals, can yield better localization performance than state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Shakeeb Murtaza and Soufiane Belharbi and Marco Pedersoli and Aydin Sarraf and Eric Granger},
  doi          = {10.1016/j.imavis.2023.104838},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104838},
  shortjournal = {Image Vis. Comput.},
  title        = {DiPS: Discriminative pseudo-label sampling with self-supervised transformers for weakly supervised object localization},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate and robust visual SLAM with a novel ray-to-ray line
measurement model. <em>ICV</em>, <em>140</em>, 104837. (<a
href="https://doi.org/10.1016/j.imavis.2023.104837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line feature is regarded as a more intuitive and accurate landmark than point feature in visual SLAM for its multiple-pixel comprehensiveness. However, uncertain factors, such as partial occlusion and noise, frequently hinder the mapping accuracy and destabilize the line-assisted SLAM system. Structural regulations and prior hypotheses are often used to tackle the issues, whereas only few people explore the impact of line feature optimization. In this paper, we attempt to improve the accuracy and robustness of visual SLAM system through line feature optimization process. First, a concise ray-to-ray residual model is proposed to replace the prevalent point-to-line model to integrally use line features. Second, the information matrix related to observation uncertainties is calculated to normalize the residual model, which aims to better balance the weights of different lines. Third, we add the line model to ORB-SLAM3 system and design the method of point-and-line based tracking and optimization. Finally, quantitative criteria are proposed to objectively evaluate the line feature map. Both synthetical and real datasets experiments are carried out to demonstrate the advantages of our algorithm in terms of camera ego-motion estimation and mapping. For camera ego-motion estimation experiments, the proposed ray-to-ray residual model produces more accurate results compared to state-of-the-art line-assisted SLAM/VIO algorithms. Furthermore, the model runs faster and obtains more robust results than the prevalent point-to-line reprojection residual model. For mapping experiments, quantitative criteria are proposed, which also open a new perspective to evaluate line-assisted SLAM systems, and give clues to evidence that the proposed method builds a more accurate line feature map.},
  archive      = {J_ICV},
  author       = {Chengran Zhang and Zheng Fang and Xingjian Luo and Wei Liu},
  doi          = {10.1016/j.imavis.2023.104837},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104837},
  shortjournal = {Image Vis. Comput.},
  title        = {Accurate and robust visual SLAM with a novel ray-to-ray line measurement model},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain car detection model with integrated
convolutional block attention mechanism. <em>ICV</em>, <em>140</em>,
104834. (<a href="https://doi.org/10.1016/j.imavis.2023.104834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Car detection, especially through camera vision, has become a major focus in the field of computer vision and has gained widespread adoption. While current car detection systems are capable of achieving good detection performance, reliable detection can still be challenging due to factors such as car proximity, varying light conditions, and environmental visibility. To address these issues, we propose C ross- D omain C ar D etection M odel with integrated convolutional block A ttention mechanism(CDCDMA) that is specifically designed for car recognition in autonomous driving and related domains. CDCDMA includes several novelties: 1)Building a complete cross-domain target detection framework. 2)Developing an unpaired target domain picture generation module with an integrated convolutional attention mechanism which specifically emphasizes the car headlights feature. 3)Adopting Generalized Intersection over Union (GIOU) as the loss function of the target detection framework. 4)Designing an object detection model integrated with two-headed Convolutional Block Attention Module(CBAM). To evaluate the model&#39;s effectiveness, we performed experiments on the SODA 10 M and BDD100K datasets by applying a reduced resolution process to the data, which served as our benchmark dataset for the task. The experimental results demonstrate that the performance of the cross-domain car target detection model improves by 40% compared to the model without our CDCDMA framework. Moreover, our improvements have a significant impact on cross-domain car recognition, surpassing the performance of most advanced cross-domain models.},
  archive      = {J_ICV},
  author       = {Haoxuan Xu and Songning Lai and Xianyang Li and Yang Yang},
  doi          = {10.1016/j.imavis.2023.104834},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {104834},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-domain car detection model with integrated convolutional block attention mechanism},
  volume       = {140},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDE-RAE: CLIP-based realistic image reconstruction and
editing network using stochastic differential diffusion. <em>ICV</em>,
<em>139</em>, 104836. (<a
href="https://doi.org/10.1016/j.imavis.2023.104836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) has long dominated the field of image reconstruction and editing. It is capable to train a generator in an adversarial way, which can fool the discriminator and enable the generated image to be of high quality. However, this approach is often difficult to train, and the final result is hard to converge. Each different style of image requires construction of different datasets and complex optimization functions, and the training process is uncertain. To solve this problem, we propose a realistic image reconstruction and editing method based on Stochastic Differential Equation (SDE-RAE), where the diffusion model converts Gaussian noise to real photos by iterative denoising. What we only need to do is to construct simple loss functions in the reconstruction process to achieve high-quality image reconstruction, and we propose a novel semantic enhancement CLIP (Contrastive Language-Image Pre-Training) to interfere with the SDE parameter optimization direction in the editing process. Simple text is needed to achieve unique image editing. Our method generates high-quality images that retain the texture and contour features of the original image. Specifically, we manipulate the initial image, perturb the image by adding random noise, and then iteratively denoise the image by reverse SDE, manipulating the image&#39;s RGB pixels to achieve image reconstruction and editing. Code and dataset https://github.com/haizhu12/SDE-RAE .},
  archive      = {J_ICV},
  author       = {Honggang Zhao and Guozhu Jin and Xiaolong Jiang and Mingyong Li},
  doi          = {10.1016/j.imavis.2023.104836},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104836},
  shortjournal = {Image Vis. Comput.},
  title        = {SDE-RAE: CLIP-based realistic image reconstruction and editing network using stochastic differential diffusion},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BF3D: Bi-directional fusion 3D detector with semantic
sampling and geometric mapping. <em>ICV</em>, <em>139</em>, 104835. (<a
href="https://doi.org/10.1016/j.imavis.2023.104835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection is a key task in environmental awareness, which plays a vital role in autonomous driving safety. Lidars and cameras are widely used sensors that provide complementary information for accurate 3D detection. However, due to the domain difference between the two modalities, it is challenging to leverage their respective strengths and fuse them perfectly. In this paper, an end-to-end detector termed BF3D is proposed, which integrates with the semantic sampling module, geometric point-image mapping module, and bi-directional attention fusion module. Specifically, the semantic sampling module incorporates a novel downsampling strategy to preserve more foreground points and pixels. Additionally, the geometric point-image mapping module is developed to find geometric correlation pixels of the point and take advantage of the high density of image features . We also introduce a bi-directional attention fusion module to combine useful information from the two modalities by attention mechanism . Extensive experiments demonstrate that BF3D outperforms both single- and multi-modal 3D detectors. Codes are available at: https://github.com/hustzyj/BF3D .},
  archive      = {J_ICV},
  author       = {Yijie Zhu and Jingming Xie and Moyun Liu and Lei Yao and Youping Chen},
  doi          = {10.1016/j.imavis.2023.104835},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104835},
  shortjournal = {Image Vis. Comput.},
  title        = {BF3D: Bi-directional fusion 3D detector with semantic sampling and geometric mapping},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-layer capsule network with joint dynamic routing for
fire recognition. <em>ICV</em>, <em>139</em>, 104825. (<a
href="https://doi.org/10.1016/j.imavis.2023.104825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fire recognition has emerged as a critical concern in the field of fire safety. Deep learning techniques, specifically convolutional neural networks (CNNs), have found widespread application in fire recognition tasks. The capsule network, a higher-level variant of CNN, offers distinct advantages in terms of enhanced recognition accuracy , making it suitable for fire recognition applications. However, the capsule network faces challenges in effectively determining the presence or absence of fire objects due to the idiosyncrasies of its dynamic routing algorithm . To address this issue and enable effective fire recognition, we propose a novel approach that involves a multi-layer capsule network. Within this framework, we introduce a joint dynamic routing algorithm that computes output values during forward propagation within the multi-layer capsule network. Additionally, we present a new loss function and a fully connected auxiliary training layer designed to train the multi-layer capsule networks effectively. Comparative evaluations against conventional CNN architectures and recent state-of-the-art fire recognition methods demonstrate that the enhanced multi-layer capsule network achieves higher test accuracy, even with limited training samples and fewer training iterations. Furthermore, owing to its reduced model parameter scale , the multi-layer capsule network exhibits faster recognition speeds compared to the aforementioned methods.},
  archive      = {J_ICV},
  author       = {Yuming Wu and Lihui Cen and Shichao Kan and Yongfang Xie},
  doi          = {10.1016/j.imavis.2023.104825},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104825},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-layer capsule network with joint dynamic routing for fire recognition},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GFFT: Global-local feature fusion transformers for facial
expression recognition in the wild. <em>ICV</em>, <em>139</em>, 104824.
(<a href="https://doi.org/10.1016/j.imavis.2023.104824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition in the wild has become more challenging owing to various unconstrained conditions, such as facial occlusion and pose variation. Previous methods usually recognize expressions by holistic or relatively coarse local methods, but only capture limited features and are susceptible to be influenced. In this paper, we propose the Global–local Feature Fusion Transformers (GFFT) that is centered on cross-patch communication between features by self-attentive fusion. This method solves the problems of facial occlusion and pose variation effectively. Firstly, the Global Contextual Information Perception (GCIP) is designed to fuse global and local features, learning the relationship between them. Subsequently, the Facial Salient Feature Perception (FSFP) module is proposed to guide the fusion features to understand the key regions of facial features using facial landmark features to further capture face-related salient features. In addition, the Multi-scale Feature Fusion (MFF) is constructed to combine different stages of fusion features to reduce the sensitivity of the deep network to facial occlusion. Extensive experiments show that our GFFT outperforms existing state-of-the-art methods with 92.05% on RAF-DB, 67.46% on AffectNet-7, 63.62% on AffectNet-8, and 91.04% on FERPlus, demonstrating its effectiveness and robustness.},
  archive      = {J_ICV},
  author       = {Rui Xu and Aibin Huang and Yuanjing Hu and Xibo Feng},
  doi          = {10.1016/j.imavis.2023.104824},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104824},
  shortjournal = {Image Vis. Comput.},
  title        = {GFFT: Global-local feature fusion transformers for facial expression recognition in the wild},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight multi-scale attention-guided network for
real-time semantic segmentation. <em>ICV</em>, <em>139</em>, 104823. (<a
href="https://doi.org/10.1016/j.imavis.2023.104823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide application of small mobile devices makes the demand for lightweight real-time semantic segmentation algorithm become more and more intense, which makes it become one of the most popular research topics in the field of computer vision . However, some current methods blindly pursue low parameter numbers and high inference speeds, resulting in excessively low model accuracy and a loss of practical value. Therefore, a lightweight multi-scale attention-guided network for real-time semantic segmentation(LMANet) based on asymmetric encoder-decoder is proposed in this paper to solve the above dilemmas. In the encoder, we propose multi-scale asymmetric residual(MAR) modules to extract local spatial information and context information to enhance feature expression. In the decoder, we design an attention feature fusion(AFF) module and an attention pyramid refining (APR) module. AFF module guides the fusion of low-level and middle-level feature information through high-level semantic information, and finally refines the fusion result through APR module. In addition, we improve the segmentation performance of the model with the help of the attention modules in the network. Our network is tested on two complex urban road datasets. The experimental results show that LMANet achieves 70.6% mIoU and 66.5% mIoU on Cityscapes and Camvid datasets at 112FPS and 333FPS respectively, only 0.95 M parameters without any pre-training or pre-processing. Compared with most of existing state-of-the-art models, our network not only guarantees reasonable inference speed and parameter quantity, but also improves the accuracy as much as possible, which makes it more practical.},
  archive      = {J_ICV},
  author       = {Xuegang Hu and Yuanjing Liu},
  doi          = {10.1016/j.imavis.2023.104823},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104823},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight multi-scale attention-guided network for real-time semantic segmentation},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring cross-video matching for few-shot video
classification via dual-hierarchy graph neural network learning.
<em>ICV</em>, <em>139</em>, 104822. (<a
href="https://doi.org/10.1016/j.imavis.2023.104822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot video classification methods aim to recognize a new class with only a few training examples. Distinct from previous few-shot methods, we explicitly consider the relations in cross-video domains and take full advantage of the cross-video frame matching in a hierarchy learning fashion. In this paper, we propose a Dual-Hierarchy Graph Neural Network to realize comprehensive cross-video frame matching and video relation modeling. In the first hierarchy of the graph neural network, we build a Cross-video Frame Matching Graph to extract robust frame-level features via accumulating information across frames sampled from both query and support videos. Then, frame representations are accumulated to obtain the video-level features. In the second hierarchy of the graph neural network, we construct a Video Relation Graph by taking the video-level features as nodes, which can adaptively learn positive relations between query and support videos. We get the predicted label of the query video through the matching learning of edges connecting video nodes. We evaluate the model on three benchmarks: HMDB51, Kinetics, and UCF101. Extensive experiments on benchmark datasets demonstrate that our model significantly improves few-shot video classification across a wide range of competitive baselines and showcases the strong generalization of our framework. The source code and models will be publicly available at https://github.com/JiaMingZhong2621/DHGNN .},
  archive      = {J_ICV},
  author       = {Fuqin Deng and Jiaming Zhong and Nannan Li and Lanhui Fu and Dong Wang and Tin Lun Lam},
  doi          = {10.1016/j.imavis.2023.104822},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104822},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring cross-video matching for few-shot video classification via dual-hierarchy graph neural network learning},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diabetic retinopathy grading review: Current techniques and
future directions. <em>ICV</em>, <em>139</em>, 104821. (<a
href="https://doi.org/10.1016/j.imavis.2023.104821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is widely recognized as a leading cause of blindness among individuals with diabetes worldwide. Therefore, early diagnosis of DR plays a crucial role in preserving patients&#39; vision and halting the progression of the disease to advanced stages. However, manual diagnosis of DR in clinical practice is time-consuming and susceptible to human error, especially during the early stages when the lesions associated with DR are often minute and challenging to identify. Furthermore, with the projected surge in the number of diabetic patients and a concurrent shortage of ophthalmologists, there will be insufficient healthcare professionals available to examine all individuals at risk. The application of machine- and deep learning-based techniques has proven effective in diagnosing medical images, including those related to DR. In this review, we surveyed and analyzed 55 DR grading studies published between 2018 and 2022 extracted from four academic digital libraries: Scopus , Web of Science, Google Scholar, and Science Direct. The review provides a comprehensive discussion and analysis of these selected studies, considering various aspects such as benchmark DR datasets, classification tasks , preprocessing techniques , learning approaches , and performance evaluation measures. Within the literature on DR grading, supervised-based learning techniques have been found to be more prevalent than semi-supervised learning techniques. Furthermore, researchers predominantly addressed this problem as an image-level classification task, overlooking the distinctive characteristics of lesions within each grade. Numerous proposed techniques primarily concentrate on detecting the early stages of DR, while a limited number of studies address the disease&#39;s advanced stages. The primary findings of our analysis reveal a potential direction for future research that emphasizes data- and model-centric approaches.},
  archive      = {J_ICV},
  author       = {Wadha Almattar and Hamzah Luqman and Fakhri Alam Khan},
  doi          = {10.1016/j.imavis.2023.104821},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104821},
  shortjournal = {Image Vis. Comput.},
  title        = {Diabetic retinopathy grading review: Current techniques and future directions},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frequency and content dual stream network for image
dehazing. <em>ICV</em>, <em>139</em>, 104820. (<a
href="https://doi.org/10.1016/j.imavis.2023.104820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing can improve image clarity and visual effect, which plays a pivotal role in many computer vision tasks . Existing dehazing methods are mostly based on a single feature stream and tend to ignore the low-frequency characteristics of haze. In this paper, we propose a dual stream network for image dehazing. To enhance the edge information and texture detail of the image, we construct a frequency stream based on attention octave convolution. We decompose the features into high and low-frequency branches in the frequency stream to obtain different structural information. By adding a residual channel attention block, the attention octave convolution can extract frequency features more efficiently and effectively. Due to the lower resolution of low-frequency features in the frequency stream, the frequency stream features alone are insufficient for recovering the overall content of the image. Therefore, a content stream was added to compensate for the information lost in the frequency stream. By fusing the outputs of two feature streams, the network achieves an enhanced dehazing performance. The results show that our method is superior to other state-of-the-art algorithms in quantitative evaluation and visual impact.},
  archive      = {J_ICV},
  author       = {Meihua Wang and Lei Liao and De Huang and Zhun Fan and Jiafan Zhuang and Wensheng Zhang},
  doi          = {10.1016/j.imavis.2023.104820},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104820},
  shortjournal = {Image Vis. Comput.},
  title        = {Frequency and content dual stream network for image dehazing},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). μPEWFace: Parallel ensemble of weighted deep convolutional
neural networks with novel loss functions for face-based authentication.
<em>ICV</em>, <em>139</em>, 104819. (<a
href="https://doi.org/10.1016/j.imavis.2023.104819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training Deep Convolutional Neural Networks (DCNN) with large-scale face images takes a significant amount of processing resources and the tuning/optimization time cost for face-based authentication. It must continue to improve its accuracy and speed during the matching phase. In this study, we propose a method, μ μ PEWFace, that inherits not only the benefits of recent DCNNs with adaptive loss functions, such as MagFace, ElasticFace, and AdaFace, for boosting accuracy but also the means to reduce matching time. Consequently, our method expands on the weighted voting mechanism that leverages suboptimal trained models to improve the discriminative capabilities of face-based authentication, as opposed to searching for the best optimal model. In order to boost the efficiency of face-based authentication, we also propose performing the matching phase for each model in parallel. To demonstrate the speed and accuracy of our method, we conduct exhaustive experiments using a variety of well-known benchmarks, including LFW, CFP-FP, AgeDB-30, CALFW, CPLFW, and IJB-B. The experimental findings demonstrate that the proposed method for face-based authentication achieves state-of-the-art results and exhibits remarkable performance. © 2023 Published by Elsevier Ltd.},
  archive      = {J_ICV},
  author       = {Hanh P. Du and Anh D. Nguyen and Dat T. Nguyen and Hoa N. Nguyen},
  doi          = {10.1016/j.imavis.2023.104819},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104819},
  shortjournal = {Image Vis. Comput.},
  title        = {μPEWFace: Parallel ensemble of weighted deep convolutional neural networks with novel loss functions for face-based authentication},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-scale point cloud completion network based on
high-frequency feature fusion. <em>ICV</em>, <em>139</em>, 104818. (<a
href="https://doi.org/10.1016/j.imavis.2023.104818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many vision tasks and intelligent robotics applications, it is common that the scanned 3D point cloud is not complete, so inferring from the residual defect shape to the intact shape becomes an essential task. Previous 3D completion neural network models generally use voxel-based or point-based methods to learn and process 3D data. For the voxel-based models, the computational cost and memory increase exponentially with the improvement of input resolution, and fine-grained features cannot be guaranteed in the completed point cloud due to limited computational resources . Point-based models suffer from the lack of precision in feature acquisition and crude reconstruction of complicated structures, making it extremely hard to accomplish elaborated semantic shapes. Combining advantages of voxel-based and point-based feature extraction through the high-frequency feature fusion module, this paper proposes a dual-scale point cloud completion network called DSNet, which performs global feature analysis at the voxel scale, and local feature analysis at the point cloud scale. The fused features are then integrated into the decoding and generation process, so as to complete the point cloud completion task from coarse to fine. Experimental results, at both quantitative and qualitative perspectives, in several prevailing datasets demonstrate that our approach surpasses state-of-the-art point cloud completion networks and has a good generalization performance . Code is available at https://github.com/engqing/DSNet.},
  archive      = {J_ICV},
  author       = {Fang Gao and Yong Liu and Pengbo Shi and Yan Jin and Jun Yu and Shaodong Li},
  doi          = {10.1016/j.imavis.2023.104818},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104818},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual-scale point cloud completion network based on high-frequency feature fusion},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage coarse-to-fine image anomaly segmentation and
detection model. <em>ICV</em>, <em>139</em>, 104817. (<a
href="https://doi.org/10.1016/j.imavis.2023.104817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Convolutional Neural Network (CNN) based anomaly detection and segmentation approaches are overly sensitive or not sensitive enough to noise, resulting in anomaly patterns, partially detected in the testing stage. The previous methods may also differentiate normal and abnormal images, but they cannot identify the location of anomaly presented in test images with high accuracy. To address this issue, we propose a two-stage CNN model for coarse-to-fine anomaly segmentation and detection called (TASAD). In both stages of TASAD, we train our model on a mixture of normal and abnormal training samples. The abnormal images are obtained by inserting pseudo-anomaly patterns that are automatically generated from anomaly source images. We use a novel and sophisticated anomaly insertion technique to generate various anomalous samples. In the first stage, we design a coarse anomaly segmentation (CAS) model that takes a whole image as an input, while in the second stage, we train a fine anomaly segmentation (FAS) model on image patches. FAS model improves detection and segmentation performance by refining anomaly patterns partially detected by CAS model. We train our framework on MVTec dataset and compare it with state-of-the-art (SOTA) methods. The proposed architecture leads to a compact model size – four times smaller than the SOTA method, while exhibiting better pixel-level accuracy. TASAD can also be applied to SOTAs to further improve their anomaly detection performance. Our experiments demonstrate that when applied to the latest SOTAs, TASAD improves the average precision (AP) performance of previous methods by 6.2%. For reproducibility of the results, code is provided at https://github.com/RizwanAliQau/tasad.git .},
  archive      = {J_ICV},
  author       = {Rizwan Ali Shah and Odilbek Urmonov and HyungWon Kim},
  doi          = {10.1016/j.imavis.2023.104817},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104817},
  shortjournal = {Image Vis. Comput.},
  title        = {Two-stage coarse-to-fine image anomaly segmentation and detection model},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised vision transformers for 3D pose estimation
of novel objects. <em>ICV</em>, <em>139</em>, 104816. (<a
href="https://doi.org/10.1016/j.imavis.2023.104816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object pose estimation is important for object manipulation and scene understanding. In order to improve the general applicability of pose estimators, recent research focuses on providing estimates for novel objects, that is, objects unseen during training. Such works use deep template matching strategies to retrieve the closest template connected to a query image, which implicitly provides object class and pose. Despite the recent success and improvements of Vision Transformers over CNNs for many vision tasks, the state of the art uses CNN-based approaches for novel object pose estimation. This work evaluates and demonstrates the differences between self-supervised CNNs and Vision Transformers for deep template matching . In detail, both types of approaches are trained using contrastive learning to match training images against rendered templates of isolated objects. At test time such templates are matched against query images of known and novel objects under challenging settings, such as clutter, occlusion and object symmetries, using masked cosine similarity. The presented results not only demonstrate that Vision Transformers improve matching accuracy over CNNs but also that for some cases pre-trained Vision Transformers do not need fine-tuning to achieve the improvement. Furthermore, we highlight the differences in optimization and network architecture when comparing these two types of networks for deep template matching.},
  archive      = {J_ICV},
  author       = {Stefan Thalhammer and Jean-Baptiste Weibel and Markus Vincze and Jose Garcia-Rodriguez},
  doi          = {10.1016/j.imavis.2023.104816},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104816},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-supervised vision transformers for 3D pose estimation of novel objects},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-shot ultra-high-resolution generative adversarial
network that synthesizes 16K images on a single GPU. <em>ICV</em>,
<em>139</em>, 104815. (<a
href="https://doi.org/10.1016/j.imavis.2023.104815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a one-shot ultra-high-resolution generative adversarial network (OUR-GAN) framework that generates non-repetitive 16K ( 16 , 384 × 8 , 640 16,384×8,640 ) images from a single training image and is trainable on a single consumer GPU. OUR-GAN generates an initial image that is visually plausible and varied in shape at low resolution, and then gradually increases the resolution by adding detail through super-resolution. Since OUR-GAN learns from a real ultra-high-resolution (UHR) image, it can synthesize large shapes with fine details and long-range coherence, which is difficult to achieve with conventional generative models that rely on the patch distribution learned from relatively small images. OUR-GAN can synthesize high-quality 16K images with 12.5 GB of GPU memory and 4K images with only 4.29 GB as it synthesizes a UHR image part by part through seamless subregion-wise super-resolution. Additionally, OUR-GAN improves visual coherence while maintaining diversity by applying vertical positional convolution. In experiments on the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual coherency, and diversity compared with the baseline one-shot synthesis models. To the best of our knowledge, OUR-GAN is the first one-shot image synthesizer that generates non-repetitive UHR images on a single consumer GPU. The synthesized image samples are presented at https://our-gan.github.io .},
  archive      = {J_ICV},
  author       = {Junseok Oh and Donghwee Yoon and Injung Kim},
  doi          = {10.1016/j.imavis.2023.104815},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {104815},
  shortjournal = {Image Vis. Comput.},
  title        = {One-shot ultra-high-resolution generative adversarial network that synthesizes 16K images on a single GPU},
  volume       = {139},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-focus image fusion using structure-guided flow.
<em>ICV</em>, <em>138</em>, 104814. (<a
href="https://doi.org/10.1016/j.imavis.2023.104814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep learning based methods have shown their advantages in multi-focus image fusion task. However, most methods still suffer from inaccurate focus region detection. In this paper, we employ the property of part-whole relationships embedded by the Capsule Network (CapsNet) to solve the problem. Specifically, we introduce CapsNet in multi-focus image fusion task, and design a structure-guided flow module, which fully utilizes structure information to help locate focus regions. CapsNet is introduced to extract structure features by supervising gradient information of the image. Compared with traditional convolutional neural networks (CNNs), CapsNet takes into account the correlation of features from different positions, such that it encodes more compact features. Once structure features are obtained, a flow alignment module is introduced to learn flow field between structure and image features, and propagate effectively structure features to image features to make confident focus region detection. Experimental results show the proposed method achieves robust fusion performance on three publicly available multi-focus datasets, and outperforms or is comparable to the state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Zhao Duan and Xiaoliu Luo and Taiping Zhang},
  doi          = {10.1016/j.imavis.2023.104814},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104814},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-focus image fusion using structure-guided flow},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancement method for non-uniform scattering images of
turbid underwater based on neural network. <em>ICV</em>, <em>138</em>,
104813. (<a href="https://doi.org/10.1016/j.imavis.2023.104813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the influence of light absorption and scattering in the water medium, underwater images often appear with different degrees of blur, color distortion, and so on. The physical model-based method needs to establish image degradation processes and improve image quality by assuming and estimating complex parameters, which limits the generalization ability and practical application effect of these methods. To solve the above problems, this paper proposes an image enhancement method of non-uniform scattering in turbid underwater based on neural network . First, the high-frequency and low-frequency components of underwater images were obtained through the difference of gaussian (DOG) and bilateral filtering (BF), respectively. Then, for the high-frequency component, pulse coupled neural networks (PCNN) to improve the edge details of the image. For low-frequency components, it is carried out in two steps: an adaptive automatic white balance and polynomial regression (AAWBPR) algorithm is proposed, which can effectively eliminate the color bias under different color temperatures and improve the brightness of images; the adaptive color and contrast enhancement (ACCE) method is used to further enhance the color and contrast of images. Finally, Decom-Net and the improved U-Net method are used to reduce the errors caused by the non-uniform scattering effect, weaken the noise interference, and enhance the overall effect of images. Extensive experiments show that the proposed method can effectively improve the quality of turbid underwater non-uniform scattering images, enhance the contrast of images, reduce the complexity of parameters, and has good generalization performance and practical application effects.},
  archive      = {J_ICV},
  author       = {Ke Liu and Yongquan Liang},
  doi          = {10.1016/j.imavis.2023.104813},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104813},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancement method for non-uniform scattering images of turbid underwater based on neural network},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A supervised approach for the detection of AM-FM signals’
interference regions in spectrogram images. <em>ICV</em>, <em>138</em>,
104812. (<a href="https://doi.org/10.1016/j.imavis.2023.104812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge curves retrieval in time–frequency (TF) domains is fundamental in many application fields as they convey most of information concerning the instantaneous frequencies of non-stationary signals. However, it represents a very hard task in the case of multicomponent signals having non-separable modes as they generate interference in TF domains. A preliminary detection of these interference regions may be then useful for the definition of effective strategies for ridge curve recovery. This paper introduces SIID-CNN ( Spectrogram Image Interference Detection via CNN) , that is a novel approach based on machine learning for the automatic detection of interference regions in spectrogram images. Each spectrogram sample is suitably classified as interference, single mode or background by accounting for its relative information. Some critical problems, such as the training set size and the type of examples to use for populating the training set, are dealt with. Experimental results show that a local linear model for spectrogram image and a small training set can guarantee good classification rates for a wide class of non-stationary signals, even in the presence of moderate noise.},
  archive      = {J_ICV},
  author       = {Vittoria Bruni and Domenico Vitulano and Silvia Marconi},
  doi          = {10.1016/j.imavis.2023.104812},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104812},
  shortjournal = {Image Vis. Comput.},
  title        = {A supervised approach for the detection of AM-FM signals’ interference regions in spectrogram images},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-scale perceptual polyp segmentation network based on
boundary guidance. <em>ICV</em>, <em>138</em>, 104811. (<a
href="https://doi.org/10.1016/j.imavis.2023.104811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation of polyps can provide an important basis for the diagnosis of colorectal cancer and has a high clinical application value. Segmentation of polyp regions is very challenging due to the high similarity between polyps and background mucosal tissue, many existing methods have failed to produce satisfactory polyp segmentation results. Therefore, scholars have recently used transformer backbone networks to extract features, which captures global information better than CNNs, resulting in more accurate detection results, but their boundary results are still not accurate enough due to the lack of processing for the boundary. In this paper, we propose a multi-scale perceptual polyp segmentation network based on boundary guidance, to obtain higher segmentation accuracy in both regions and boundary. To increase the feature response region, we first propose a multi-scale global perception module to expand the receptive field and aggregate multi-scale contextual information to capture the primary location of polyps at local and global levels. Then, we design a boundary-guided feature enhancement module that utilizes contextual features to mine hidden polyp boundary and employs the boundary to guide region learning to improve segmentation boundary accuracy. Finally, we propose a complementary fusion module that uses higher-level features to filter out the background noise of lower-level features and fuses the features layer by layer. In particular, to refine the extracted features, a detail refinement module is designed to complement the spatial details to improve the segmentation performance. Extensive experiments using seven evaluation metrics on five publicly available polyp datasets have shown that the proposed a multi-scale perceptual polyp segmentation network based on boundary guidance outperforms most state-of-the-art models.},
  archive      = {J_ICV},
  author       = {Lu Lu and Shuhan Chen and Haonan Tang and Xinfeng Zhang and Xuelong Hu},
  doi          = {10.1016/j.imavis.2023.104811},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104811},
  shortjournal = {Image Vis. Comput.},
  title        = {A multi-scale perceptual polyp segmentation network based on boundary guidance},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time-shift image enhancement method. <em>ICV</em>,
<em>138</em>, 104810. (<a
href="https://doi.org/10.1016/j.imavis.2023.104810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a time-shift image enhancement method for representing constant images in spacetime, deriving the associated events in a specified time, and using them to enhance the degraded images. We assumed constant images as moving objects on a z-plane with different velocities and adopted the spacetime using the Lorentz factor to represent the movement of objects at the constant moment. The changes between the events and a reference frame are used to create adjacent z-plane events. The events provide statistical and pixel-based relationships in order to reconstruct a single frame for the common perspective of the reference image in time. Then, the gamma correction is applied in the reconstructed image based on the obtained statistics to provide enhancement of images by preserving the brightness and improving the contrast and details. In order to demonstrate the proposed methods’ general applicability, several challenging referenced and non-referenced datasets from different problem domains are considered. A comparative study has been performed, and the proposed method has been compared to state-of-the-art and recent image enhancement techniques. Both qualitative and quantitative results are analyzed for color correction, detail enhancement, and additional noise for each dataset. The results show that the Time-Shift Method could be effectively used for image enhancement for different problem domains without producing over- or under-enhancement.},
  archive      = {J_ICV},
  author       = {Boran Sekeroglu},
  doi          = {10.1016/j.imavis.2023.104810},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104810},
  shortjournal = {Image Vis. Comput.},
  title        = {Time-shift image enhancement method},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AESPNet: Attention enhanced stacked parallel network to
improve automatic diabetic foot ulcer identification. <em>ICV</em>,
<em>138</em>, 104809. (<a
href="https://doi.org/10.1016/j.imavis.2023.104809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Foot Ulcer (DFU) is a severe complication of diabetes, and it may cause lower limb amputation. However, the manual diagnosis of DFU is a complicated and expensive process. The primary objective of this work is to design an efficient Convolutional Neural Network (CNN) approach to identify DFU. Therefore, a novel CNN-based approach (AESPNet) is proposed in this paper, where convolution layers are stacked together in a parallel fashion and with an intermediate attention module to perform DFU vs -normal skin classification. The AESPNet consists of 2 blocks, where varying-sized kernel convolution layers are connected in a parallel fashion for better local and global feature abstraction. A bottleneck attention module is associated after every concatenation operation in the network. The Stochastic Gradient Descent (SGD) (with momentum) optimizer with 1 e - 2 1e-2 learning rate is used to train the proposed network on a privately accessed DFU dataset. The results of the proposed approach are compared with other standard CNN-based schemes, namely AlexNet, VGG16, DenseNet121, and InceptionV3. It has been found that the proposed AESPNet outperforms state-of-the-art schemes with a sensitivity score of 98.44% and 0.98 F1-Scores.},
  archive      = {J_ICV},
  author       = {Sujit Kumar Das and Suyel Namasudra and Awnish Kumar and Nageswara Rao Moparthi},
  doi          = {10.1016/j.imavis.2023.104809},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104809},
  shortjournal = {Image Vis. Comput.},
  title        = {AESPNet: Attention enhanced stacked parallel network to improve automatic diabetic foot ulcer identification},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StyleDiff: Attribute comparison between unlabeled datasets
in latent disentangled space. <em>ICV</em>, <em>138</em>, 104808. (<a
href="https://doi.org/10.1016/j.imavis.2023.104808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One major challenge in machine learning applications is coping with mismatches between the datasets used in the development and those obtained in real-world applications. These mismatches may lead to inaccurate predictions and errors, resulting in poor product quality and unreliable systems. To address the mismatches, it is important to understand in what sense the two datasets differ. In this study, we propose StyleDiff to inform developers about the types of mismatches between the two datasets in an unsupervised manner. Given two unlabeled image datasets, StyleDiff automatically extracts latent attributes that are distributed differently between the given datasets and visualizes the differences in a human-understandable manner. For example, for an object detection dataset, latent attributes might include the time of day, weather, and traffic congestion of an image that are not explicitly labeled. StyleDiff helps developers understand the differences between the datasets with respect to such latent attribute distributions. Developers can then, for example, collect additional development data with these attributes and conduct additional tests for these attributes to enhance reliability. We demonstrate that StyleDiff accurately detects differences between datasets and presents them in an understandable format using, for example, driving scene datasets.},
  archive      = {J_ICV},
  author       = {Keisuke Kawano and Takuro Kutsuna and Ryoko Tokuhisa and Akihiro Nakamura and Yasushi Esaki},
  doi          = {10.1016/j.imavis.2023.104808},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104808},
  shortjournal = {Image Vis. Comput.},
  title        = {StyleDiff: Attribute comparison between unlabeled datasets in latent disentangled space},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visible-infrared person re-identification using high
utilization mismatch amending triplet loss. <em>ICV</em>, <em>138</em>,
104797. (<a href="https://doi.org/10.1016/j.imavis.2023.104797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VIPR) is a task of retrieving a specific pedestrian monitored by cameras in different spectra. A dilemma of VIPR is how to reasonably use intra-modal pairs. Fully discarding intra-modal pairs causes a low utilization of training data , while using intra-modal pairs brings a danger of distracting a VIPR model&#39;s concentration on handling cross-modal pairs, harming the cross-modal similarity metric learning. For that, a high utilization mismatch amending (HUMA) triplet loss function is proposed for VIPR. The key of HUMA is the multi-modal matching regularization (MMMR), which restricts variations of distance matrices calculated from cross- and intra-modal pairs to cohere cross- and intra-modal similarity metrics, allowing for a high utilization of training data and amending the adverse distractions of intra-modal pairs. In addition, to avoid the risk of harming feature discrimination caused by MMMR preferring coherence in similarity metrics, a novel separated loss function assignment (SLFA) strategy is designed to arrange MMMR well. Experimental results show that the proposed method is superior to state-of-the-art approaches.},
  archive      = {J_ICV},
  author       = {Jianqing Zhu and Hanxiao Wu and Qianqian Zhao and Huanqiang Zeng and Xiaobin Zhu and Jingchang Huang and Canhui Cai},
  doi          = {10.1016/j.imavis.2023.104797},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104797},
  shortjournal = {Image Vis. Comput.},
  title        = {Visible-infrared person re-identification using high utilization mismatch amending triplet loss},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain adaptive person search via GAN-based scene synthesis
for cross-scene videos. <em>ICV</em>, <em>138</em>, 104796. (<a
href="https://doi.org/10.1016/j.imavis.2023.104796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search has recently been a challenging task in the computer vision domain, which aims to search specific pedestrians from real cameras.Nevertheless, most surveillance videos comprise only a handful of images of each pedestrian, which often feature identical backgrounds and clothing. Hence, it is difficult to learn more discriminative features for person search in real scenes. To tackle this challenge, we draw on Generative Adversarial Networks (GAN) to synthesize data from surveillance videos. GAN has thrived in computer vision problems because it produces high-quality images efficiently. We merely alter the popular Fast R-CNN model, which is capable of processing videos and yielding accurate detection outcomes. In order to appropriately relieve the pressure brought by the two-stage model, we design an Assisted-Identity Query Module (AIDQ) to provide positive images for the behind part. Besides, the proposed novel GAN-based Scene Synthesis model that can synthesize high-quality cross-id person images for person search tasks. In order to facilitate the feature learning of the GAN-based Scene Synthesis model, we adopt an online learning strategy that collaboratively learns the synthesized images and original images. Extensive experiments on two widely used person search benchmarks, CUHK-SYSU and PRW, have shown that our method has achieved great performance, and the extensive ablation study further justifies our GAN-synthetic data can effectively increase the variability of the datasets and be more realistic. The code is available at https://github.com/crsm424/DA-GSS .},
  archive      = {J_ICV},
  author       = {Huibing Wang and Tianxiang Cui and Mingze Yao and Huijuan Pang and Yushan Du},
  doi          = {10.1016/j.imavis.2023.104796},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104796},
  shortjournal = {Image Vis. Comput.},
  title        = {Domain adaptive person search via GAN-based scene synthesis for cross-scene videos},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bidirectional attentional interaction networks for RGB-d
salient object detection. <em>ICV</em>, <em>138</em>, 104792. (<a
href="https://doi.org/10.1016/j.imavis.2023.104792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the issues of insufficient cross-modality feature interaction and ineffective utilization of cross-modality data in RGB-D saliency object detection (SOD) tasks, we propose a Bidirectional Attentional Interaction Network (BAINet) for RGB-D SOD, which employs an encoder-decoder structure for bidirectional interaction of cross-modality features through a dual-branch progressive fusion approach. To begin with, based on the fact that RGB and depth information streams can complement each other, the bidirectional attention interaction module accomplishes bidirectional interaction between cross-modality features by capturing complementary cues from different modality data. In order to enhance the expressiveness of the fused RGB-D features, the global feature perception module endows the features with rich multi-scale contextual semantic information by enlarging the field of perception. In addition, exploring the correlation of cross-level features is vital to achieve accurate salient inference. Specifically, We introduce a cross-level guidance aggregation module to capture inter-layer dependencies and complete the integration of cross-level features, which effectively suppresses shallow cross-modality features and refines the saliency map during decoding. To improve the model training speed, a hybrid loss function is employed to train multi-branch saliency inference maps simultaneously. Extensive experiments on five publicly available datasets clearly show that the proposed model outperforms 18 state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Weiyi Wei and Mengyu Xu and Jian Wang and Xuzhe Luo},
  doi          = {10.1016/j.imavis.2023.104792},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104792},
  shortjournal = {Image Vis. Comput.},
  title        = {Bidirectional attentional interaction networks for RGB-D salient object detection},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware and part alignment for visible-infrared person
re-identification. <em>ICV</em>, <em>138</em>, 104791. (<a
href="https://doi.org/10.1016/j.imavis.2023.104791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is a challenging problem of matching a person from visible and infrared modalities. Existing researches adopt the last convolutional layer features of the off-the-shelf backbone network as the representation to refine, which is unable to represent the heterogeneous cross-modality features with discriminant information. In this paper, we propose a novel graph-based aggregation learning network (GALNet) with visual Transformer embedding to mine both multi-layer features and part-level contextual cues for VI-ReID. We propose a novel feature memory module (FMM) to reserve global discriminative features of the low layers with the correlation modeling ability of the graph convolution network (GCN) which is supplemented to the final person representation. To learn more discriminant part-level features, an attentive part aggregation module (PAM) is designed to mine part relationships, leveraging the self-attention mechanism of the Transformer. By fusing these components, the global-level, and part-level discriminant information can be utilized. Extensive experiments on SYSU-MM01 and RegDB benchmarks demonstrate the effectiveness of our proposed methods compared with several state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Jiaqi Zhao and Hanzheng Wang and Yong Zhou and Rui Yao and Lixu Zhang and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.imavis.2023.104791},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104791},
  shortjournal = {Image Vis. Comput.},
  title        = {Context-aware and part alignment for visible-infrared person re-identification},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Speaker independent VSR: A systematic review and futuristic
applications. <em>ICV</em>, <em>138</em>, 104787. (<a
href="https://doi.org/10.1016/j.imavis.2023.104787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speaker-independent visual speech recognition (VSR) is a complex task that involves identifying spoken words or phrases from video recordings of a speaker’s facial movements. Decoding the intricate visual dynamics of a speaker’s mouth in a high-dimensional space is a significant challenge in this field. To address this challenge, researchers have employed advanced techniques that enable machines to recognize human speech through visual cues automatically. Over the years, there has been a considerable amount of research in the field of VSR involving different algorithms and datasets to evaluate system performance. These efforts have resulted in significant progress in developing effective VSR models, creating new opportunities for further research in this area. This survey provides a detailed examination of the progression of VSR over the past three decades, with a particular emphasis on the transition from speaker-dependent to speaker-independent systems. We also provide a comprehensive overview of the various datasets used in VSR research and the preprocessing techniques employed to achieve speaker independence. The survey covers the works published from 1990 to 2023, thoroughly analyzing each work and comparing them on various parameters. This survey provides an in-depth analysis of speaker-independent VSR systems evolution from 1990 to 2023. It outlines the development of VSR systems over time and highlights the need to develop end-to-end pipelines for speaker-independent VSR. The pictorial representation offers a clear and concise overview of the techniques used in speaker-independent VSR, thereby aiding in the comprehension and analysis of the various methodologies. The survey also highlights the strengths and limitations of each technique and provides insights into developing novel approaches for analyzing visual speech cues. Overall, This comprehensive review provides insights into the current state-of-the-art speaker-independent VSR and highlights potential areas for future research.},
  archive      = {J_ICV},
  author       = {Praneeth Nemani and Ghanta Sai Krishna and Kundrapu Supriya and Santosh Kumar},
  doi          = {10.1016/j.imavis.2023.104787},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104787},
  shortjournal = {Image Vis. Comput.},
  title        = {Speaker independent VSR: A systematic review and futuristic applications},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time gait biometrics for surveillance applications: A
review. <em>ICV</em>, <em>138</em>, 104784. (<a
href="https://doi.org/10.1016/j.imavis.2023.104784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) pipelines have evolved for over a decade now and are efficient at solving many challenging problems of image and signal processing applications. Designing deep learning pipelines for a particular application requires a good understanding of deep learning and various intermediate layers available. To develop a DL pipeline, one uses available dataset(s) suitable for an application, and the pipeline is refined by iterating over intermediate layers. A large amount of time and extensive thinking goes into these selections and validating the performance of each configuration. Thus, it is hard to choose the correct and robust DL pipeline that performs well on all relevant datasets. This review aims to aid researchers in understanding different gait sensing technologies and provide foundational knowledge of the deep learning concepts for faster solutions for a given problem. Gait recognition is more recent since it hasn’t yet been used in a real-world situation. This article provides a comprehensive overview of gait biometrics suited to real-time surveillance applications. All the important parameters of deep learning pipelines are explained, along with their selection and implication for a given problem. Authors have reviewed important research articles recently on deep learning models and how these perform across different application datasets. The benefits and drawbacks of the approaches are elucidated to help arrive at the optimized pipeline derived from a fusion of available pipelines to achieve faster but accurate results for a given problem.},
  archive      = {J_ICV},
  author       = {Anubha Parashar and Apoorva Parashar and Andrea F. Abate and Rajveer Singh Shekhawat and Imad Rida},
  doi          = {10.1016/j.imavis.2023.104784},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104784},
  shortjournal = {Image Vis. Comput.},
  title        = {Real-time gait biometrics for surveillance applications: A review},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fuzzy set-based bernoulli random noise weighted loss for
unsupervised person re-identification. <em>ICV</em>, <em>138</em>,
104783. (<a href="https://doi.org/10.1016/j.imavis.2023.104783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering-based methods have achieved impressive performance on unsupervised person ReID task. However, most of these models suffer from noisy labels. In this paper, we propose a novel method to evaluate and suppress noisy labels. Since the feature network is barely trained and the clustering results are unreliable consequently, noisy samples can be hardly screened out with a hard threshold in the early stage of training. To address this issue, we construct a N oisy L abel F uzzy S et (NLFS) whose membership function can be used to evaluate the noise level of each sample. Furthermore, we propose a new loss function, referred to as B ernoulli R andom N oise W eighted L oss (BRNWL), with which some samples with higher noise membership will be recognized as noisy samples in higher probability and assigned with corresponding weights to limit their contributions to the training. Through above improvements, randomness is introduced into the alternating update between the feature network and the clustering results, which can effectively prevent the model from trapping into local minima or vicious circles. Extensive experiments demonstrate that our method achieves state-of-the-art performance on popular ReID datasets.},
  archive      = {J_ICV},
  author       = {Chunren Tang and Dingyu Xue and Dongyue Chen},
  doi          = {10.1016/j.imavis.2023.104783},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {104783},
  shortjournal = {Image Vis. Comput.},
  title        = {Fuzzy set-based bernoulli random noise weighted loss for unsupervised person re-identification},
  volume       = {138},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D human body modeling with orthogonal human mask image
based on multi-channel swin transformer architecture. <em>ICV</em>,
<em>137</em>, 104795. (<a
href="https://doi.org/10.1016/j.imavis.2023.104795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction based on RGB images of dressed human body lacks the shape information of the human body under clothing, while the naked 3D human body scanning will violate the user&#39;s privacy. To overcome these limitations, a new method, based on Swin transformer (Swin-T), for reconstructing 3D human body shape from human orthogonal mask image is proposed. Its core is to express the reconstruction problem as solving regression mapping function. A fast body shape type classification method based on the human front mask is proposed. The regression function is innovatively represented as a piecewise function, with the body shape of the human body as the segmentation criterion. A multi-channel Swin-T architecture is designed, which can not only extract features from front and side mask images, but also their mixed features to construct the regression mapping function. Different body types for different genders are predicted with separate regression function to help estimate an accurate human model. Extensive experimental results show that the proposed method effectively achieves visually realistic and accurate body reconstruction, and significantly outperforms the current state-of-the-art methods. In addition, the classification of body types can compensate for the errors caused by partial clothing laxity in practical applications, which is beneficial for users to obtain a more accurate 3D human model.},
  archive      = {J_ICV},
  author       = {Xihang Li and Guiqin Li and Ming Li and Kuiliang Liu and Peter Mitrouchev},
  doi          = {10.1016/j.imavis.2023.104795},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104795},
  shortjournal = {Image Vis. Comput.},
  title        = {3D human body modeling with orthogonal human mask image based on multi-channel swin transformer architecture},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach for bias mitigation of gender
classification algorithms using consistency regularization.
<em>ICV</em>, <em>137</em>, 104793. (<a
href="https://doi.org/10.1016/j.imavis.2023.104793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Published research has confirmed the bias of automated face-based gender classification algorithms across gender-racial groups. Specifically, unequal accuracy rates were obtained for women and dark-skinned people for face-based automated gender classification algorithms. To mitigate the bias of gender classification and other facial-analysis-based algorithms in general, the vision community has proposed several techniques. However, most of the existing bias mitigation techniques suffer from a lack of generalizability, need a demographically-annotated training set, are application-specific, and often offer a trade-off between fairness and classification accuracy. This means that fairness is often obtained at the cost of a reduction in the classification accuracy of the best-performing demographic sub-group. In this paper, we propose a novel bias mitigation technique that leverages the power of semantic preserving augmentations at the image- and feature-level in a self-consistency setting for the downstream gender classification task. Thorough experimental validation on gender-annotated facial image datasets confirms the efficacy of our bias mitigation technique in improving overall gender classification accuracy as well as reducing bias across all gender-racial groups over state-of-the-art bias mitigation techniques. Specifically, our proposed technique obtained a reduction in the bias by an average of 30 % 30% over existing bias mitigation techniques as well as an improvement in the overall classification accuracy of about 5 % 5% over the baseline gender classifier. Therefore, resulting in state-of-the-art generalization performance in the intra- and cross-dataset evaluations. Additionally, our proposed technique operates in the absence of demographic labels and is application agnostic, compared to most of the existing bias mitigation techniques.},
  archive      = {J_ICV},
  author       = {Anoop Krishnan and Ajita Rattani},
  doi          = {10.1016/j.imavis.2023.104793},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104793},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel approach for bias mitigation of gender classification algorithms using consistency regularization},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GW-net: An efficient grad-CAM consistency neural network
with weakening of random erasing features for semi-supervised person
re-identification. <em>ICV</em>, <em>137</em>, 104790. (<a
href="https://doi.org/10.1016/j.imavis.2023.104790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) subject to loss of detailed information (sacrificing certain intricate body details) in samples caused by conventional regularization algorithms and degraded cross-domain performance due to limited generalization capacity is a challenging task. The majority of the existing research efforts address this problem either for attention regularization or cross-domain task, but neglect to explore a powerful framework to consider solving both cases simultaneously. To overcome this limitation, this paper develops an efficient semi-supervised person Re-ID network with Grad-CAM consistency regularization and weakening of random erasing features (GW-Net) to explore rich features and expect non-degraded performance in cross-domain condition. Specifically, a Grad-CAM consistency regularization (GCCR) module is designed to capture detailed information by using Grad-CAM for consistency regularization, thereby enhancing the capacity of intra-class feature mining. Secondly, a weakening of random erasing features (WREF) module is presented to reduce the impact of erased regions on texture features , thereby maintaining the performance on the general source domain while preventing performance degradation in the target domain. Thirdly, a Grad-CAM consistency regularization loss is introduced to enable our model to maintain consistency between Grad-CAM results of the unlabeled and its augmented images. Meanwhile, a feature consistency loss is reported to keep consistency between features of unlabeled samples . Finally, sufficient experiments are carried out on three representative datasets, which validate the efficacy and meliority of our presented approach over state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Shangdong Zhu and Yunzhou Zhang and Yu Feng},
  doi          = {10.1016/j.imavis.2023.104790},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104790},
  shortjournal = {Image Vis. Comput.},
  title        = {GW-net: An efficient grad-CAM consistency neural network with weakening of random erasing features for semi-supervised person re-identification},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perceiving local relative motion and global correlations for
weakly supervised group activity recognition. <em>ICV</em>,
<em>137</em>, 104789. (<a
href="https://doi.org/10.1016/j.imavis.2023.104789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a weakly supervised approach for group activity recognition by exploiting the local relative motion and global correlations among entities. Most existing approaches of group recognition characterize motions of group activities based on the coordinates of individuals or feature maps without excluding the camera motion, which is a combination of local relative motion and camera motion. To address this problem, we utilize a simple but effective Local Relative Motion Module (LRMM): a 3D-CNN-based network to exploit the local movement. We further employ a Global Correlation Module (GCM) to establish relationships among different feature patches for capturing the entire scene. We have evaluated the proposed method on sports and group activity video. The method has achieved state-of-the-art performance on three challenging datasets for weakly supervised group activity recognition. The method has also outperformed some approaches trained with much stronger supervision in the comparative evaluation .},
  archive      = {J_ICV},
  author       = {Zexing Du and Xue Wang and Qing Wang},
  doi          = {10.1016/j.imavis.2023.104789},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104789},
  shortjournal = {Image Vis. Comput.},
  title        = {Perceiving local relative motion and global correlations for weakly supervised group activity recognition},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An end-to-end anti-shaking multi-focus image fusion
approach. <em>ICV</em>, <em>137</em>, 104788. (<a
href="https://doi.org/10.1016/j.imavis.2023.104788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an end-to-end multi-focus fusion network is proposed to solve the mis-registration problem of the captured images. This paper takes the shaking and camera breathing effects that usually exist when acquiring multiple partially focused images into account. The new network consists of a homography pre-correction module, a deformable convolutional fine-correction module, an attention based fusion module and a reconstruction module. The homography pre-correction module is proposed to achieve a coarse correction for multi-focus images. The deformable convolution fine-correction module enables the fusion algorithm to achieve an effective fine alignment for small shaking in multi-focus images. The attention based fusion module and the image reconstruction module are proposed to achieve high definition fusion results. This paper produces a dedicated shaking dataset for training and testing. The algorithm’s superior performance in depth-of-field extension and generalisation to different scenes can be seen through sufficient experiments on test datasets, real scenes and image sequences. The ablation experiments also demonstrate the necessity of each module in the algorithm.},
  archive      = {J_ICV},
  author       = {Jiayu Ji and Feng Pan and Xuanyin Wang and Jixiang Tang and Ze’an Liu and Bin Pu},
  doi          = {10.1016/j.imavis.2023.104788},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104788},
  shortjournal = {Image Vis. Comput.},
  title        = {An end-to-end anti-shaking multi-focus image fusion approach},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised person re-identification by dynamic hybrid
contrastive learning. <em>ICV</em>, <em>137</em>, 104786. (<a
href="https://doi.org/10.1016/j.imavis.2023.104786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (re-ID) aims at cross-camera pedestrian retrieval without manual annotation. Recently, the contrastive learning has been introduced into the field of unsupervised person re-ID. However, existing methods usually focus only on mining the intra-category similarity and neglect the negative effects of clustering noise, which limits the performance in unsupervised person re-ID. In this paper, we propose a Dynamic Hybrid Contrastive Learning (DHCL) method for unsupervised person re-ID. Specifically, we perform the clustering algorithm and the dynamic refinement policy to divide the unlabeled training dataset into two subsets, i.e., clustered samples with pseudo labels and un-clustered independent instances, at each training epoch. Then, the proposed DHCL guides the feature extraction network to mine the intra-category similarity from the clustered samples by applying attraction within samples of the same cluster. Meanwhile, the inter-instance discrimination is also mined by pushing away different instances. Besides, we integrate the two levels of contrastive learning into an end-to-end framework and exploit the complementarity between them to improve the separability of the feature space. To reduce the negative effect of over-focusing on positive samples, a penalty item is added to the hybrid contrastive loss . Extensive experiments demonstrate the effectiveness of the proposed method in unsupervised person re-ID.},
  archive      = {J_ICV},
  author       = {Yu Zhao and Qiaoyuan Shu and Xi Shi and Jian Zhan},
  doi          = {10.1016/j.imavis.2023.104786},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104786},
  shortjournal = {Image Vis. Comput.},
  title        = {Unsupervised person re-identification by dynamic hybrid contrastive learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal information oriented motion accumulation and
selection network for RGB-based action recognition. <em>ICV</em>,
<em>137</em>, 104785. (<a
href="https://doi.org/10.1016/j.imavis.2023.104785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous studies have highlighted the crucial role of motion information in accurate action recognition in videos. However, current methods heavily rely on temporal differences of features extracted by convolutional neural networks (CNNs) to represent motion, which may have two potential limitations: (1) incomplete representation of the moving target contour due to the difference operation, and (2) equal treatment of all extracted motion features, regardless of their relevance to the classification task, which may negatively impact performance. To address these limitations, we propose a novel approach called the Motion Accumulation and Selection Network (MAS-Net). Although our new approach also considers spatial attributes, it draws inspiration from the cumulative and selective nature of human visual attention, with a primary focus on capturing the temporal attributes of actions for recognition. Further, an motion selection module is exploited to prioritize relevant temporal features while filtering out irrelevant ones. Currently, there is a growing demand for action recognition with strong temporal information, as opposed to conventional scene-related datasets such as UCF-101 and HMDB-51. Therefore, we evaluated MAS-Net on benchmark video datasets that primarily emphasize temporal information, including Something-Something V1&amp;V2, Diving48, and Kinetics-400. Our experimental results demonstrate that MAS-Net achieves state-of-the-art performance on Something-Something V1&amp;V2 and Diving48 datasets. Furthermore, when compared to other 2D CNN-based models, MAS-Net exhibits competitive results on the Kinetics-400 dataset while maintaining computational efficiency. These findings highlight the effectiveness and efficiency of MAS-Net for temporal modeling in video analysis tasks.},
  archive      = {J_ICV},
  author       = {Huafeng Wang and Hanlin Li and Wanquan Liu and Xianfeng Gu},
  doi          = {10.1016/j.imavis.2023.104785},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104785},
  shortjournal = {Image Vis. Comput.},
  title        = {Temporal information oriented motion accumulation and selection network for RGB-based action recognition},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporally consistent reconstruction of 3D clothed human
surface with warp field. <em>ICV</em>, <em>137</em>, 104782. (<a
href="https://doi.org/10.1016/j.imavis.2023.104782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit functions are widely used in 3D human surface reconstruction due to their advantage to represent details. However, human reconstruction based on implicit functions struggles to maintain the integrity (unbroken body structure) and accuracy (no non-human parts) of human models. To address these issues, we propose a method, called TCR, for temporally consistent reconstruction of 3D clothed human surface with warp field. The fact that the general shape of a person does not change largely over time inspires us to exploit the temporally consistent shape information from previous frames to refine the human model of current frame. Therefore, we construct a canonical space and then store the shape information by updating the canonical model. To align the observed space with the canonical space, a warp field is firstly estimated for the forward and inverse warping of the human model. A probabilistic fusion strategy is then used to update the canonical model. In addition, the reconstructed result is further refined through the orthogonality constraints between the surface and its normal, which fully exploits the detailed information of estimated normal maps. Experiments on the Adobe and MonoPerfCap datasets show that TCR achieves the state-of-the-art performance. Furthermore, TCR is more robust and can maintain the integrity and accuracy of the reconstructed human body even with extreme poses and partial occlusions.},
  archive      = {J_ICV},
  author       = {Yong Deng and Baoxing Li and Yehui Yang and Xu Zhao},
  doi          = {10.1016/j.imavis.2023.104782},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104782},
  shortjournal = {Image Vis. Comput.},
  title        = {Temporally consistent reconstruction of 3D clothed human surface with warp field},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anchor-based discriminative dual distribution calibration
for transductive zero-shot learning. <em>ICV</em>, <em>137</em>, 104772.
(<a href="https://doi.org/10.1016/j.imavis.2023.104772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) is machine learning task to recognize samples from classes that are not observed during training. Transductive ZSL (TZSL) is a more realistic and effective paradigm that leverages unlabeled unseen data during training to reduce the bias towards seen classes. However, most existing TZSL methods neglect the information gap between visual and semantic spaces, and thus fail to generate distribution-consistent unseen features. To address this issue, we propose a novel TZSL approach named Anchor-based Discriminative Dual Distribution Calibrated Feature Generative Network (AD 3 C-FGN), which performs anchor-based distribution calibration in both visual and semantic spaces for improving generalization ability of the model. In AD 3 C-FGN, we adopt conditional generative adversarial network with an unseen discriminator to construct a Y-shape generation model that mitigates the domain shift problem. Moreover, an AD 3 C module is elaborated for calibrating the distribution of generated and real samples in both visual and semantic spaces with real sample anchors, and also enhance the discriminability of the generated samples. AD 3 C enforces the generated sample to be closer to its homogenous anchor but farther away from inhomogeneous anchors in both spaces. Extensive experimental results on six popular ZSL benchmarks demonstrate that our method achieves promising performances in different settings. The source codes of our model have been released on https://github.com/ZYi-CQU/AD3C-FGN .},
  archive      = {J_ICV},
  author       = {Yi Zhang and Sheng Huang and Wanli Yang and Wenhao Tang and Xiaohong Zhang and Dan Yang},
  doi          = {10.1016/j.imavis.2023.104772},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104772},
  shortjournal = {Image Vis. Comput.},
  title        = {Anchor-based discriminative dual distribution calibration for transductive zero-shot learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Qualitative failures of image generation models and their
application in detecting deepfakes. <em>ICV</em>, <em>137</em>, 104771.
(<a href="https://doi.org/10.1016/j.imavis.2023.104771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The remarkable advancement of image and video generation models has led to the creation of exceptionally realistic content, posing challenges in differentiating between genuine and fabricated instances in numerous scenarios. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting generated images and deepfakes. The prevalence of deepfakes in today’s society is a serious concern, and our findings can help mitigate their negative impact. In order to support research in this field, a collection of instances where models have failed is made available at here .},
  archive      = {J_ICV},
  author       = {Ali Borji},
  doi          = {10.1016/j.imavis.2023.104771},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104771},
  shortjournal = {Image Vis. Comput.},
  title        = {Qualitative failures of image generation models and their application in detecting deepfakes},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FgbCNN: A unified bilinear architecture for learning a
fine-grained feature representation in facial expression recognition.
<em>ICV</em>, <em>137</em>, 104770. (<a
href="https://doi.org/10.1016/j.imavis.2023.104770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A facial expression recognition system has been proposed in this paper. The challenges of the facial expression recognition system lie due to low intra-class variance within a class of negative emotions, such as anger, disgust, and fear. A conventional Convolution Neural Network (CNN) model may extract discriminative features and has shown outstanding performance in various computer vision tasks. However, it is unable to extract the second-order feature information that demonstrates the interaction between features in the image of wild-environment datasets. This paper presents a novel method to solve the facial expression recognition problem by addressing several limitations concerning emotion recognition problems. A deep learning-based bilinear convolutional neural network framework has been proposed, termed an Fine-Grained Bilinear CNN (FgbCNN) model that consists of two branches with optimized CNN along with a normalization layer composed of batch-normalization, square-root normalization, L2-normalization, and drop-out layers. Here, local and holistic features have been aggregated using a dot-product layer to extract more discriminant features. Finally, experimenting with two wild-environments (SFEW 1.0 and SFEW 2.0) and two lab-controlled datasets (KDEF and CK+), it has been observed that the proposed model can minimize the intra-class variances and has attained outstanding performance compared to other state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Nazir Shabbir and Ranjeet Kumar Rout},
  doi          = {10.1016/j.imavis.2023.104770},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104770},
  shortjournal = {Image Vis. Comput.},
  title        = {FgbCNN: A unified bilinear architecture for learning a fine-grained feature representation in facial expression recognition},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep robust multi-channel learning subspace clustering
networks. <em>ICV</em>, <em>137</em>, 104769. (<a
href="https://doi.org/10.1016/j.imavis.2023.104769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering methods are now widely used for unsupervised high-dimensional data processing in computer vision and other domains. Deep subspace clustering methods based on auto-encoder networks have made a significant improvement in nonlinear subspace clustering problems in comparison to previous works. However, these methods ignore the valid information lost during feature extraction, resulting in incomplete information and imprecise feature representations for subspace clustering. In addition, the clustering performance of the existing clustering methods is excessively dependent on hyper-parameters, making training difficult and unstable. In this paper, we propose Deep Robust Multi-Channel Learning Subspace Clustering Networks (DRMCLSC), a novel deep subspace clustering network for learning more comprehensive feature representations with good robustness for subspace clustering. The multi-channel learning strategy allows the model to extract, retain and fuse features simultaneously, enabling all valid information from the sample data to be obtained. Moreover, the multi-channel learning structure of the proposed method produces a more stable integration network that is less dependent on hyper-parameters and more resistant to training errors than previous works. Extensive experimental results on four benchmark datasets demonstrate the proposed method is superior and more effective than the state-of-the-art subspace clustering methods.},
  archive      = {J_ICV},
  author       = {Mengzhu Fang and Wei Gao and Zirui Feng},
  doi          = {10.1016/j.imavis.2023.104769},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104769},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep robust multi-channel learning subspace clustering networks},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PatchMixer: Rethinking network design to boost
generalization for 3D point cloud understanding. <em>ICV</em>,
<em>137</em>, 104768. (<a
href="https://doi.org/10.1016/j.imavis.2023.104768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent trend in deep learning methods for 3D point cloud understanding is to propose increasingly sophisticated architectures either to better capture 3D geometries or by introducing possibly undesired inductive biases. Moreover, prior works introducing novel architectures compared their performance on the same domain, devoting less attention to their generalization to other domains. We argue that the ability of a model to transfer the learnt knowledge to different domains is an important feature that should be evaluated to exhaustively assess the quality of a deep network architecture . In this work we propose PatchMixer, a simple yet effective architecture that extends the ideas behind the recent MLP-Mixer paper to 3D point clouds. The novelties of our approach are the processing of local patches instead of the whole shape to promote robustness to partial point clouds, and the aggregation of patch-wise features using an MLP as a simpler alternative to the graph convolutions or the attention mechanisms that are used in prior works. We evaluated our method on the shape classification and part segmentation tasks , achieving superior generalization performance compared to a selection of the most relevant deep architectures.},
  archive      = {J_ICV},
  author       = {Davide Boscaini and Fabio Poiesi},
  doi          = {10.1016/j.imavis.2023.104768},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104768},
  shortjournal = {Image Vis. Comput.},
  title        = {PatchMixer: Rethinking network design to boost generalization for 3D point cloud understanding},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi parallel u-net encoder network for effective polyp
image segmentation. <em>ICV</em>, <em>137</em>, 104767. (<a
href="https://doi.org/10.1016/j.imavis.2023.104767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating polyps&#39; detection and segmentation is very helpful in achieving early detection and potentially effective treatment of colon cancer. Because of the constant rise in processing power at a reasonable cost and the availability of training data , deep learning more than ever is becoming very powerful and increasingly widespread in many domains, including computer vision and medical image processing and segmentation. In this paper, we propose a novel method for polyp image segmentation using convolutional neural networks (CNNs) and utilizing multi-parallel U-Net encoder architecture. The proposed method uses multiple encoders that utilize pre-trained CNNs to enrich the extracted features. Skip connections from these parallel encoders are concatenated and propagated to the decoder in partially and fully meshed fashions allowing for data from shallow to deeper layers in the network to be exchanged and vice versa. We trained and tested the method on five publicly available datasets: Kvasir, CVC-Clinic DB, CVC-Colon DB, CVC-T, and ETIS-Larib, which are well-known benchmark datasets for polyp image segmentation. We tested the performance of the method on these five datasets using multiple testing scenarios including the effect of using attention, vision transformers , and multiple decoders. Experimental results showed that the proposed method outperforms other state-of-the-art methods on two of the used benchmark datasets and ranked second on a third.},
  archive      = {J_ICV},
  author       = {Hamdan Al Jowair and Mansour Alsulaiman and Ghulam Muhammad},
  doi          = {10.1016/j.imavis.2023.104767},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104767},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi parallel U-net encoder network for effective polyp image segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight image denoising network with four-channel
interaction transform. <em>ICV</em>, <em>137</em>, 104766. (<a
href="https://doi.org/10.1016/j.imavis.2023.104766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising has always been a fundamental task in computer vision. In recent years, deep learning methods have emerged as the dominant approach for image denoising and have significantly improved denoising performance. However, these deep denoising methods typically require large model sizes, making network training prohibitively expensive and limiting their applicability in realistic scenarios. To address this issue, we propose a Lightweight Image Denoising Network (LWNet) with a four-channel interaction transform that effectively reduces the model size. The proposed four-channel interaction transform first constructs the LWNet using four channels within the input and output dimensions. Specifically, an additional empty channel with all zeros is attached to the input image, and the output dimension has four channels. This additional channel significantly enhances the robustness of network training, as the expansion of features in the channel dimension provides richer information. Compared to three-channel networks, LWNet exhibits greater fault tolerance. Furthermore, the proposed LWNet uses a dual-branch structure to achieve the four-channel interaction transform in the feature space. One branch focuses on the feature learning of the additional channel within the input dimension, while the other branch handles the original three channels. This mechanism enables the network to retrieve abundant denoising features and adaptively inject them into the denoised images, significantly enhancing the denoising performance. Thanks to the powerful feature retrieval ability of the four-channel transform, the proposed LWNet can significantly decrease the required number of parameters. Extensive experimental results show that LWNet achieves the best denoising results on synthetic datasets using much fewer parameters. Even when extrapolating to real datasets for validation, it maintains better denoising performance with effective model size. Overall, the proposed LWNet offers an effective solution to reduce model size without compromising denoising performance and has potential practical applications in various image denoising scenarios.},
  archive      = {J_ICV},
  author       = {Jiahuan Wang and Yao Lu and Guangming Lu},
  doi          = {10.1016/j.imavis.2023.104766},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104766},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight image denoising network with four-channel interaction transform},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attentive spatial-temporal contrastive learning for
self-supervised video representation. <em>ICV</em>, <em>137</em>,
104765. (<a href="https://doi.org/10.1016/j.imavis.2023.104765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing self-supervised works learn video representation by using a single pretext task. A single pretext task, providing single supervision from unlabeled data , may neglect to describe the difference between spatial features and temporal features. The similar spatial features and temporal features may hinder distinguishing between two similar videos with different class labels. In this paper, we propose an attentive spatial–temporal contrastive learning network (ASTCNet), which learns self-attention spatial–temporal features by contrastive learning between multiple spatial and temporal pretext tasks. The spatial features are learned by multiple spatial pretext tasks, including spatial rotation, and spatial jigsaw. Each spatial feature is enhanced with spatial self-attention by learning the relation between patches. The temporal features are learned by multiple temporal pretext tasks, including temporal order, and temporal pace. Each temporal feature is enhanced with temporal self-attention by learning the relation between frames, and is enhanced by feeding the optical flow features into a motion module. To separate the spatial feature and the temporal feature learned in one video, we represent the video as different features for each pretext task, and design pretext task-based contrastive loss . The pretext task-based contrastive loss encourages the different pretext tasks to learn dissimilar features, and encourages the same pretext task to learn similar features. The pretext task-based contrastive loss can learn the discriminative features for each pretext task in one video. The experiments show that our method achieves state-of-the-art performance for self-supervised action recognition on the UCF101 dataset and the HMDB51 dataset.},
  archive      = {J_ICV},
  author       = {Xingming Yang and Sixuan Xiong and Kewei Wu and Dongfeng Shan and Zhao Xie},
  doi          = {10.1016/j.imavis.2023.104765},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104765},
  shortjournal = {Image Vis. Comput.},
  title        = {Attentive spatial-temporal contrastive learning for self-supervised video representation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient unsupervised learning of biological images with
compressed deep features. <em>ICV</em>, <em>137</em>, 104764. (<a
href="https://doi.org/10.1016/j.imavis.2023.104764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has significantly impacted the analysis of biological images and is now an important part of many biological data analysis pipelines. A variety of biological and biomedical domain-related tasks is gaining benefit from image analysis and pattern recognition tools developed currently. Applications include diagnostic histopathology, environmental monitoring, synthetic biology, genomics, and proteomics. Particularly in the last decade, several deep learning and advanced computer vision methods such as convolutional neural networks (CNNs), typically trained in a supervised fashion, have started to be largely employed in biological image classification. Moreover, the advancement of automatic acquisition systems has been generating a massive amount of biological data, which requires to be analyzed by domain experts. However, the cost of manual annotation of such data has become a bottleneck, impairing the application of supervised machine learning algorithms. Biological images generally have an intrinsic high variability, whose identity is sometimes hard to assign and strongly dependent on the annotator’s expertise. In this context, a limited number of annotation-free (i.e., unsupervised) learning solutions have been proposed, typically based on hand-crafted features, specifically tailored for a certain biological domain. Nonetheless, a successful unsupervised learning approach must be accurate, and sufficiently robust to deal with different biological domains. This paper aims at providing a viable solution to these issues, proposing an unsupervised learning algorithm based on compressed deep features for image classification. We exploit features extracted from ImageNet pre-trained transformers and CNNs, further compressed with a customized β β -Variational AutoEncoder ( β β -VAE), that we call reconstruction VAE (R-VAE). We test our algorithm on biological images coming from diverse domains characterized by high variability in shape and texture information and acquired with widely differing imaging platforms. Considered image datasets range from multi-cellular organisms (plankton, coral) to sub-cellular organelles (budding yeast vacuoles, human cells’ nuclei, etc.). Our results show that the compressed deep features extracted from different pre-trained vision models establish new unsupervised learning state-of-the-art performances for the investigated datasets.},
  archive      = {J_ICV},
  author       = {Vito Paolo Pastore and Massimiliano Ciranni and Simone Bianco and Jennifer Carol Fung and Vittorio Murino and Francesca Odone},
  doi          = {10.1016/j.imavis.2023.104764},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104764},
  shortjournal = {Image Vis. Comput.},
  title        = {Efficient unsupervised learning of biological images with compressed deep features},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distilling knowledge for occlusion robust monocular 3D face
reconstruction. <em>ICV</em>, <em>137</em>, 104763. (<a
href="https://doi.org/10.1016/j.imavis.2023.104763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been significant advancements in the 3D face reconstruction field, largely driven by monocular image-based deep learning methods. However, these methods still face challenges in reliable deployments due to their sensitivity to facial occlusions and inability to maintain identity consistency across different occlusions within the same facial image. To address these issues, we propose two frameworks: D istillation A ssisted M ono I mage O cclusion R obustification ( DAMIOR ) and D uplicate I mages A ssisted M ulti O cclusions R obustification ( DIAMOR ) . The DAMIOR framework leverages the knowledge from the O cclusion F rail T rainer ( OFT ) network to enhance robustness against facial occlusions. Our proposed method overcomes the sensitivity to occlusions and improves reconstruction accuracy. To tackle the issue of identity inconsistency, the DIAMOR framework utilizes the estimates from DAMIOR to mitigate inconsistencies in geometry and texture, collectively known as identity, of the reconstructed 3D faces. We evaluate the performance of DAMIOR on two variations of the CelebA test dataset: empirical occlusions and irrational occlusions. Furthermore, we analyze the performance of the proposed DIAMOR framework using the irrational occlusion-based variant of the CelebA test dataset. Our methods outperform state-of-the-art approaches by a significant margin. For example, DAMIOR reduces the 3D vertex-based shape error by 41.1 % 41.1% and the texture error by 21.8 % 21.8% for empirical occlusions. Besides, for facial data with irrational occlusions, DIAMOR achieves a substantial decrease in shape error by 42.5 % 42.5% and texture error by 30.5 % 30.5% . These results demonstrate the effectiveness of our proposed methods.},
  archive      = {J_ICV},
  author       = {Hitika Tiwari and Vinod K. Kurmi and Venkatesh K. Subramanian and Yong Sheng Chen},
  doi          = {10.1016/j.imavis.2023.104763},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104763},
  shortjournal = {Image Vis. Comput.},
  title        = {Distilling knowledge for occlusion robust monocular 3D face reconstruction},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EatSense: Human centric, action recognition and localization
dataset for understanding eating behaviors and quality of motion
assessment. <em>ICV</em>, <em>137</em>, 104762. (<a
href="https://doi.org/10.1016/j.imavis.2023.104762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current datasets for computer vision-based action recognition and localization cover a wide range of classes and challenging scenarios. However, these datasets don&#39;t cater to healthcare applications that involve long-term monitoring, tracking minor changes in movements over time for healthcare purposes, or completely modeling a specific human behavior that includes multiple sub-actions. Specifically, there are no existing datasets for research on either health monitoring on atomic-action-based eating behavior or for a full range of eating sub-actions that fully segment the main action. Addressing these gaps is valuable for extending research on the health monitoring of elderly people and is needed for creating richer and more complete descriptions of actions. This paper introduces a new benchmark dataset named EatSense that targets both the computer vision and healthcare communities and fills in the aforementioned gaps. EatSense is recorded while a person eats in an uncontrolled dining setting. The key features of EatSense are the introduction of challenging atomic actions for action recognition, the significantly diverse durations of actions that make it difficult for current temporal action localization frameworks to localize, the capability to model comprehensive eating behavior in terms of a sequence of action-based behaviors, and the simulation of minor variations in motion or performance. We conduct extensive experiments on EatSense with baseline deep learning-based approaches for benchmarking and hand-crafted feature-based approaches for explainable applications. We believe this dataset will benefit future researchers in building robust temporal action localization networks, behavior recognition, and performance assessment models for eating.},
  archive      = {J_ICV},
  author       = {Muhammad Ahmed Raza and Longfei Chen and Li Nanbo and Robert B. Fisher},
  doi          = {10.1016/j.imavis.2023.104762},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104762},
  shortjournal = {Image Vis. Comput.},
  title        = {EatSense: Human centric, action recognition and localization dataset for understanding eating behaviors and quality of motion assessment},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). USMLP: U-shaped sparse-MLP network for mass segmentation in
mammograms. <em>ICV</em>, <em>137</em>, 104761. (<a
href="https://doi.org/10.1016/j.imavis.2023.104761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNN) are widely used in computer-aided diagnosis (CAD). However, there are several limitations of this structure for mass segmentation in mammograms, which may lead to incorrect segmentation results. Recently, multi-layer perceptron (MLP) based methods introduce a new way to solve computer vision (CV) problems. In this paper we propose U-shaped Sparse-MLP (USMLP), which is a MLP-based segmentation model with U-shaped architecture. Our proposed method consists of CNN layers and sparse MLP (sMLP) blocks. Detailed experiments on two public datasets show that our method achieves state-of-the-art performance while remaining efficiency, compared with other benchmarks. We hope the investigation of new network architecture can be beneficial to mass segmentation tasks as well as CAD systems.},
  archive      = {J_ICV},
  author       = {Jiaming Luo and Yongzhe Tang and Jie Wang and Hongtao Lu},
  doi          = {10.1016/j.imavis.2023.104761},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104761},
  shortjournal = {Image Vis. Comput.},
  title        = {USMLP: U-shaped sparse-MLP network for mass segmentation in mammograms},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual tracking using transformer with a combination of
convolution and attention. <em>ICV</em>, <em>137</em>, 104760. (<a
href="https://doi.org/10.1016/j.imavis.2023.104760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Siamese-based trackers in the field of single object tracking, cross-correlation operation plays an important role. However, the cross-correlation essentially uses target feature to locally linearly match the search region, which leads to insufficient utilization or even loss of feature information. To effectively employ global context and sufficiently explore the relevance of template and search region, a novel matching operator is designed inspired by Transformer, which uses multi-head attention and embed a designed modulation module across the inputs of operator. Meanwhile, we equip our tracker with a multi-scale encoder/decoder strategy to gradually make more precise tracking. Finally, a complete tracking framework is presented named VTTR. The tracker consists of a feature extractor, a multi-scale encoder based on depth-wise convolution, a modified decoder as the matching operator and a prediction head. The proposed tracker is tested on many benchmarks and achieve excellent performance while running with fast speed.},
  archive      = {J_ICV},
  author       = {Yuxuan Wang and Liping Yan and Zihang Feng and Yuanqing Xia and Bo Xiao},
  doi          = {10.1016/j.imavis.2023.104760},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104760},
  shortjournal = {Image Vis. Comput.},
  title        = {Visual tracking using transformer with a combination of convolution and attention},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint representation and classifier learning for long-tailed
image classification. <em>ICV</em>, <em>137</em>, 104759. (<a
href="https://doi.org/10.1016/j.imavis.2023.104759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed classification with fine-grained appearance, e.g. , in chest X-ray images, is very challenging due to the very similar appearance and imbalanced distribution between normal and abnormal samples, which extremely limits the ability of deep networks to learn powerful representations and discriminative classifiers. In this paper, we propose a novel Joint Representation and Classifier Learning (JRCL) framework to achieve the above purposes, simultaneously. In terms of representation learning, we propose a One-to-All supervised contrastive learning strategy to avoid the medium or tail classes mixing in the head classes. For the classifier cleaning, we propose a novel Binary Distribution Consistency (BDC) loss to learn a discriminative classifier that could separate the normal and abnormal samples.The BDC loss measures the binary distribution consistency between the designed multi-class classifier and an auxiliary binary classifier. Consequently, the JRCL framework is optimized with a supervised contrastive learning loss, a binary distribution consistency loss, and a multi-classification loss. We conduct experiments on large-scale, long-tail image datasets, NIH-CXR-LT, MIMIC-CXR-LT, iNaturalist 2018, and Places-LT. Experimental results demonstrate JRCL could improve the discriminate ability of the imbalanced data and thus obtain better classification performance. Compared with the state-of-the-art methods, our proposed JRCL achieves comparable or even better performance. The source codes are available at https://github.com/guanqj932/JRCL .},
  archive      = {J_ICV},
  author       = {Qingji Guan and Zhuangzhuang Li and Jiayu Zhang and Yaping Huang and Yao Zhao},
  doi          = {10.1016/j.imavis.2023.104759},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104759},
  shortjournal = {Image Vis. Comput.},
  title        = {Joint representation and classifier learning for long-tailed image classification},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generation-based contrastive model with semantic alignment
for generalized zero-shot learning. <em>ICV</em>, <em>137</em>, 104758.
(<a href="https://doi.org/10.1016/j.imavis.2023.104758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) is an important research area in image computing, video processing, multimedia understanding, and other visual computing tasks. GZSL normally uses transferable semantic features to represent the visual features to predict unseen classes without training the unseen samples. The state-of-the-art zero-shot learning methods combine Generative Adversarial Network (GAN) and Contrastive Learning (CL) together to deeply transfer semantic features to visual features. However, the combined GAN module and CL module inevitably encounter the “semantic-visual inconsistent problem” in both the feature-generating process and the contrastive learning process. To handle the above problems, we propose the generation-based contrastive model with semantic alignment for generalized zero-shot learning. The proposed network is based on existing ZSL models combining GAN and CL, but with two additional alignment modules that are Feedback Alignment Module (FAM) and Negative sample Alignment Module (NAM). FAM applies an MLP (Multilayer Perceptron) to align the synthesized visual feature back to its semantic feature for keeping the semantic-visual consistency in the generator. NAM provides a new contrastive learning mechanism to align the negative pairs for keeping semantic-visual consistency during contrastive learning. Experimental results on massive real-world datasets show the proposed method achieves the new state-of-the-art in the field of generalized zero-shot learning. The source code of the proposed method is available at https://github.com/yangjingqi99/GCSA .},
  archive      = {J_ICV},
  author       = {Jingqi Yang and Qi Shen and Cheng Xie},
  doi          = {10.1016/j.imavis.2023.104758},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104758},
  shortjournal = {Image Vis. Comput.},
  title        = {Generation-based contrastive model with semantic alignment for generalized zero-shot learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WPE: Weighted prototype estimation for few-shot learning.
<em>ICV</em>, <em>137</em>, 104757. (<a
href="https://doi.org/10.1016/j.imavis.2023.104757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) is a challenging task that aims to transfer a model trained on base classes with adequate labeled data to accommodate novel classes with only a few training examples. In this paper, we propose a simple but effective weighted prototype estimation (WPE) method to improve FSL. We assume that similar classes have similar distributions in the feature space so that the prototypes of novel classes can be estimated by transferring information from their similar base classes. Specifically, the proposed method learns the Gaussian-like feature distributions of similar base classes with sufficient samples and then transfers the learned distributions to calibrate the prototype of the novel class, which is weighted by its similarities. With the estimated prototype, more robust samples can be generated to improve the FSL task. Comparative experiments are conducted to evaluate the effectiveness of our proposed algorithm on three benchmark FSL datasets. The results show that our proposed method can generate more robust samples and significantly improve FSL, outperforming the state-of-the-art methods on these datasets.},
  archive      = {J_ICV},
  author       = {Jiangzhong Cao and Zijie Yao and Lianggeng Yu and Bingo Wing-Kuen Ling},
  doi          = {10.1016/j.imavis.2023.104757},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104757},
  shortjournal = {Image Vis. Comput.},
  title        = {WPE: Weighted prototype estimation for few-shot learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminable feature enhancement for unsupervised domain
adaptation. <em>ICV</em>, <em>137</em>, 104755. (<a
href="https://doi.org/10.1016/j.imavis.2023.104755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation addresses the problem of knowledge transformation from source domain to target domain, aiming to effectively alleviate data distribution mismatch and data labeling consumption. The issue of data distribution mismatches is widespread in actual agricultural visual tasks. Moreover, it is expensive and time-consuming to construct and label visual image data. For in-field cotton boll, its maturing status can greatly affect the yield and quality. Uneven distribution restrains the performance for maturing status recognition. Therefore, domain adaptation is essential for identifying cross-domain cotton boll maturity. Existing unsupervised domain adaptation methods obtain domain invariant feature for achieving domain alignment. However, the discriminability of features is less considered, which may result in unsatisfactory classification results. In this paper, an unsupervised domain adaptation method called discriminable feature enhancement(DFE-DA) is proposed to identify cross-domain cotton boll maturity. It enables to minimize intra-class distance by maximizing intra-domain density(MID) loss and realizes discriminable feature enhancement. The effectiveness of DFE-DA is verified on in-field cotton boll V2(IFCB-V2) dataset containing 2400 images. The experimental results demonstrate that DFE-DA has an average improvement of 12.8%, 10.3% and 7.6% compared with other methods in three different transfer tasks. Furthermore, the MID loss can cooperate well with other adversarial methods. To verity the robustness of DFE-DA, additional experiments conducted on the public benchmark Office-31 and Office-Home indicates it is comparable to the state-of-the-arts.},
  archive      = {J_ICV},
  author       = {Yanan Li and Yifei Liu and Dingrun Zheng and Yuhan Huang and Yuling Tang},
  doi          = {10.1016/j.imavis.2023.104755},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104755},
  shortjournal = {Image Vis. Comput.},
  title        = {Discriminable feature enhancement for unsupervised domain adaptation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer-based feature interactor for person
re-identification with margin self-punishment loss. <em>ICV</em>,
<em>137</em>, 104752. (<a
href="https://doi.org/10.1016/j.imavis.2023.104752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification aims to retrieve specific pedestrians from different cameras and scenes, in which extracting robust and discriminative features is crucial for this task. To explore the potential interactions among images and learn more robust representations, this paper proposes Transformer-based Feature Interactor(TFI) and improved Margin Self-punishment Softmax loss(MS-Softamx). The Transformer-based Feature Interactor consists of Group Channel Pyramid Attention(GCPA) and Neighbor Interaction Modeling(NIM). Firstly, the Group Channel Pyramid Attention module provides prior information for high-level semantics via low-level semantics. The attention information is gradually stacked from coarse to fine to obtain enhanced hierarchical multi-scale features. Then, Neighbor Interaction Modeling effectively model the input and similar neighbors to produce a more robust and discriminative image representation. To make TFI more focused on intra-class embedding learning, we also propose Margin Self-punishment Softmax guide deep network learning , which obtains a tighter custom classification boundary by pushing the inter-class threshold and minimizing the intra-class variance. The proposed method is verified on four datasets, and this achieves 92.8%/95.6% mAP/Rank-1 on Market1501, 86.1%/90.8% mAP/Rank-1 on DukeMTMC, 64.4%/ 81.2% mAP/Rank-1 on MSMT17, 79.7%/80.8% mAP/Rank-1 on CUHK03-detected and 81.8%/81.9% mAP/Rank-1 on CUHK03-labeled. Extensive experiments demonstrate that the proposed method achieves competitive performance with other state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Zhiyong Huang and Pinzhong Qin and Zhi Yu and Lamia Tahsin and Mengyao Wang and Man Liu},
  doi          = {10.1016/j.imavis.2023.104752},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104752},
  shortjournal = {Image Vis. Comput.},
  title        = {Transformer-based feature interactor for person re-identification with margin self-punishment loss},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ATOM: Self-supervised human action recognition using atomic
motion representation learning. <em>ICV</em>, <em>137</em>, 104750. (<a
href="https://doi.org/10.1016/j.imavis.2023.104750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) is a promising method for gaining perception and common sense from unlabelled data. Existing approaches to analyzing human body skeletons address the problem similar to SSL models for image and video understanding, but pixel data is far more challenging than coordinates. This paper presents ATOM, an SSL model designed for skeleton-based data analysis. Unlike video-based SSL approaches, ATOM leverages atomic movements within skeleton actions to achieve a more fine-grained representation. The proposed architecture predicts the action order at the frame level, leading to improved perceptions and representations of each action. ATOM outperforms state-of-the-art approaches in two well-known datasets (NTU RGB + D and NTU-120 RGB + D), and its weight transferability enables performance improvements on supervised and semi-supervised tasks, up to 4.4% (3.3% p.p.) and 14.1% (6.3% p.p.), respectively, in Top-1 Accuracy.},
  archive      = {J_ICV},
  author       = {Bruno Degardin and Vasco Lopes and Hugo Proença},
  doi          = {10.1016/j.imavis.2023.104750},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104750},
  shortjournal = {Image Vis. Comput.},
  title        = {ATOM: Self-supervised human action recognition using atomic motion representation learning},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic texture analysis using temporal gray scale pattern
image for water surface velocity measurement. <em>ICV</em>,
<em>137</em>, 104749. (<a
href="https://doi.org/10.1016/j.imavis.2023.104749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water surface velocity (WSV) is one of the critical parameters in hydrology. The development of non-intrusive measurement of this parameter using cameras is increasing. Traditionally, measuring WSV using a camera utilizes tracking of moving objects on the water surfaces. Recently, another method has emerged that utilizes the movement of water ripples to estimate the WSV. This paper proposes a novel method for estimating WSV based on camera measurements. The authors call this method Temporal Gray-scale Pattern Image (TGPI) since it extracts Gray-scale patterns of pixels of water flow video in the temporal domain using the XOR operator and create new image. The new image pattern formed is then used to predict WSV using predictor. There are four predictors being compared, namely Multiple Input Linear Regression (MILR), Multiple Input Logistic Regression (MILgR), Convolutional Neural Network Regression (CNN-R), and Convolutional Neural Network Classification (CNN-K). CNN-R and CNN-K predict WSV directly from the results of TGPI. Meanwhile, MILR and MILgR predict WSV from five TGPI features. The five features are the Mean and Median of the Histogram, Mean and Median of the Histogram of Oriented Gradient (HOG), and the Maximum Mid-Value of the Fast Fourier Transform (FFT). MILR and CNN-R are predictors for regression problems, so the testing metrics for them are Trend-Line equation and R 2 from the predicted WSV and actual values graph. Meanwhile, for MILgR and CNN-K, which are predictors for classification problems, the testing metrics are Confusion Matrix (CM), Accuracy, Precision, Recall, and F1-Score. To test the four methods without distinguishing their predictor types, a 2-dimensional histogram graph is used. The data-set used for training and testing is video footage of water flow with known WSV. The WSV measurement points used in this study are 1.7 m/s, 3.1 m/s, and 4.2 m/s. The video dataset and these three points are generated by the Mini Open Channel Water Flow Simulator (MOCWFS) developed by the author in this study. From the comparison results, it can be seen that the classification type predictor is superior to the regression type. For the regression type predictor, MILR is better than CNN-R. Meanwhile, for the classification type, CNN-K is superior to MILgR. The best accuracy produced by CNN-K is 98.4%. Although there are shortcomings, the TGPI method is quite feasible for predict Water Surface Velocity.},
  archive      = {J_ICV},
  author       = {Bernadus Herdi Sirenden and Petrus Mursanto and Sensus Wijonarko},
  doi          = {10.1016/j.imavis.2023.104749},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104749},
  shortjournal = {Image Vis. Comput.},
  title        = {Dynamic texture analysis using temporal gray scale pattern image for water surface velocity measurement},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LDWS-net: A learnable deep wavelet scattering network for
RGB salient object detection. <em>ICV</em>, <em>137</em>, 104748. (<a
href="https://doi.org/10.1016/j.imavis.2023.104748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the salient object detection task, convolutional neural network (CNN) based models have been extensively used. However, preserving a variety of boundary features of objects is equally important while detecting the salient objects. Detecting salient objects with poor boundaries have a significant detrimental effect on the salient object detection (SOD) models&#39; robustness and accuracy. The proposed method leverages a unique and novel edge-directed salient object detection network, which combines wavelet scattering network features with CNN-based features. This integration enables the network to capture both textural and high-level semantic information, leading to improved SOD performance. Additionally, a learnable wavelet scattering network allows for the efficient collection and preservation of textural aspects of objects. This network is seamlessly embedded into the encoder section of the proposed architecture, enhancing the discriminative power of the model. Furthermore, a weighted feature integration module (WFIM) is proposed in the decoder section to adaptively integrate linked nearby features by evaluating their relevance, resulting in improved representation and discrimination capabilities. Extensive testing on well-known benchmark SOD datasets demonstrates that the proposed LDWS-Net outperforms state-of-the-art techniques, exhibiting accurate identification of salient objects and efficient detection of their edges.},
  archive      = {J_ICV},
  author       = {Bhagyashree V. Lad and Mohammad Farukh Hashmi and Avinash G. Keskar},
  doi          = {10.1016/j.imavis.2023.104748},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104748},
  shortjournal = {Image Vis. Comput.},
  title        = {LDWS-net: A learnable deep wavelet scattering network for RGB salient object detection},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VAESim: A probabilistic approach for self-supervised
prototype discovery. <em>ICV</em>, <em>137</em>, 104746. (<a
href="https://doi.org/10.1016/j.imavis.2023.104746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical image datasets, discrete labels are often used to describe a continuous spectrum of conditions, making unsupervised image stratification a challenging task. In this work, we propose VAESim, an architecture for image stratification based on a conditional variational autoencoder . VAESim learns a set of prototypical vectors during training, each associated with a cluster in a continuous latent space. We perform a soft assignment of each data sample to the clusters and reconstruct the sample based on a similarity measure between the sample embedding and the prototypical vectors. To update the prototypical embeddings, we use an exponential moving average of the most similar representations between actual prototypes and samples in the batch size. We test our approach on the MNIST handwritten digit dataset and the PneumoniaMNIST medical benchmark dataset, where we show that our method outperforms baselines in terms of kNN accuracy (up to + 15 % +15% improvement in performance) and performs at par with classification models trained in a fully supervised way. Our model also outperforms current end-to-end models for unsupervised stratification.},
  archive      = {J_ICV},
  author       = {Matteo Ferrante and Tommaso Boccato and Simeon Spasov and Andrea Duggento and Nicola Toschi},
  doi          = {10.1016/j.imavis.2023.104746},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104746},
  shortjournal = {Image Vis. Comput.},
  title        = {VAESim: A probabilistic approach for self-supervised prototype discovery},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective hybrid attention network based on pseudo-color
enhancement in ultrasound image segmentation. <em>ICV</em>,
<em>137</em>, 104742. (<a
href="https://doi.org/10.1016/j.imavis.2023.104742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound image segmentation plays a vital role in the early diagnosis of human diseases. It helps diagnose many diseases, such as breast cancer, hemangioma, and other gynecological disorders. However, the intrinsic imaging characteristics of ultrasound images result in substantially lower resolution and clarity than CT, MRI, and other imaging modalities, and they are sensitive to interference from external influences. With its inherent artifacts, blurred lesion boundaries, and uneven intensity distribution, ultrasound images present a challenging task when it comes to segmenting lesion areas accurately. In recent years, convolutional neural networks (CNNs) have achieved remarkable results in medical image segmentation tasks. However, CNNs are limited in capturing the remote dependencies of the input image, leading to degraded accuracy in segmenting ultrasound lesions. In this paper, we developed a deep convolutional neural network that incorporates the pseudo-color enhancement algorithm and hybrid attention modules that enhance the network’s ability to extract fine features and remote modeling capabilities. We propose a novel multi-scale channel attention-based decoder that efficiently uses the feature maps from the encoder as a complement and fuses them with the upsampled feature maps. The hybrid attention combination captures cross-channel interactions efficiently and enhances the context modeling capability, further improving the extraction of coarse and delicate features, and resulting in significant performance improvements. We found that the dice performance improved by 2.54%, 2.47%, 1.39%, 0.99%, and 1.23% on the BUL, BUSI, Hemangioma, BP, and VUI. Results from four public datasets and one self-collected dataset indicate that the proposed method outperforms other medical image segmentation methods for ultrasound image lesion segmentation.},
  archive      = {J_ICV},
  author       = {Xuping Huang and Qian Wang and Junxi Chen and Lingna Chen and Zhiyi Chen},
  doi          = {10.1016/j.imavis.2023.104742},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104742},
  shortjournal = {Image Vis. Comput.},
  title        = {Effective hybrid attention network based on pseudo-color enhancement in ultrasound image segmentation},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FSformer: Fast-slow transformer for video action
recognition. <em>ICV</em>, <em>137</em>, 104740. (<a
href="https://doi.org/10.1016/j.imavis.2023.104740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-stream networks have achieved good results on action recognition datasets by modeling the interdependence of various motions. However, previous two-stream networks focus on action modeling but ignore concentrating on the difference in importance between different short-term actions, causing the limitation of the model’s action modeling capabilities between different short-term actions. Therefore, we propose a Short-term Action Differentiated Attention (SADA) module based on the two-stream structure with different temporal resolution inputs. We embed the SADA module in a novel two-stream transformer architecture called Fast-Slow Transformer (FSformer). The SADA module dramatically pays attention to the difference between the importance of different short-term actions. It can: ( i ) (i) deploy attention from the video frames to learn the differentiated knowledge of the importance of different short-term action feature information for action recognition, ( ii ) (ii) fuse rich importance difference knowledge and context information through a novel Fast-Slow Attention mechanism. Overall, the SADA module significantly focuses on the difference in importance of short-term actions and improves action recognition performance. We evaluate our method’s effectiveness on three challenging densely-labeled action datasets and achieve results over the state-of-the-art.},
  archive      = {J_ICV},
  author       = {Shibao Li and Zhaoyu Wang and Yixuan Liu and Yunwu Zhang and Jinze Zhu and Xuerong Cui and Jianhang Liu},
  doi          = {10.1016/j.imavis.2023.104740},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {104740},
  shortjournal = {Image Vis. Comput.},
  title        = {FSformer: Fast-slow transformer for video action recognition},
  volume       = {137},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PAGML: Precise alignment guided metric learning for
sketch-based 3D shape retrieval. <em>ICV</em>, <em>136</em>, 104756. (<a
href="https://doi.org/10.1016/j.imavis.2023.104756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based 3D shape retrieval has always been a hot research topic in the computer vision community. The main challenge is to alleviate the cross-modality discrepancies such that the retrieval accuracies can be improved. In this paper, we propose a novel Precise Alignment Guided Metric Learning (PAGML) method based on master-auxiliary cross-modality retrieval framework. An auxiliary learning network is developed to indirectly guide the master learning model to extract features of rich semantic information, so as to achieve a semantic alignment between the cross-modality data. Furthermore, affected by the intra-class variability and inter-class imbalance issue, the learned class distributions may exhibit unevenness in the common embedding space and cause poor retrieval performance. A loss function dedicated for cross-modality retrieval is designed to achieve a rigid alignment between sketches and 3D shapes of the same category by pulling their rich semantic representations to the rigid center of the category. As a result, a more precise alignment between the cross-modality embedding features of the same category is approached gradually, which further alleviates the cross-modality discrepancies, inter-class variability, and inter-class imbalance, thus improving the cross-modality retrieval accuracies. Extensive experiments on two public benchmark datasets demonstrate that the proposed PAGML surpasses the state-of-the-art methods in retrieval accuracy and has excellent generalization abilities to unseen classes.},
  archive      = {J_ICV},
  author       = {Shaojin Bai and Jing Bai and Hao Xu and Jiwen Tuo and Min Liu},
  doi          = {10.1016/j.imavis.2023.104756},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104756},
  shortjournal = {Image Vis. Comput.},
  title        = {PAGML: Precise alignment guided metric learning for sketch-based 3D shape retrieval},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive learning with semantic consistency constraint.
<em>ICV</em>, <em>136</em>, 104754. (<a
href="https://doi.org/10.1016/j.imavis.2023.104754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive representation learning (CL) can be viewed as an anchor-based learning paradigm that learns representations by maximizing the similarity between an anchor and positive samples while reducing the similarity with negative samples. A randomly adopted data augmentation strategy generates positive and negative samples, resulting in semantic inconsistency in the learning process. The randomness may introduce additional disturbances to the original sample, thereby reversing the sample identity. Also, the negative sample demarcation strategy makes the negative samples containing semantically similar samples to the anchors, called false negative samples. Therefore, CL&#39;s maximization and reduction process cause distractors to be incorporated into the learned feature representation. In this paper, we propose a novel Semantic Consistency Regularization (SCR) method to alleviate this problem. Specifically, we introduce a new regularization item, pairwise subspace distance, to constrain the consistency of distributions across different views. Furthermore, we propose a divide-and-conquer strategy to ensure that the proposed SCR is well-suited for large mini-batch cases. Empirically, results across multiple benchmark mini and large datasets demonstrate that SCR outperforms state-of-the-art methods. Codes are available at https://github.com/PaulGHJ/SCR.git .},
  archive      = {J_ICV},
  author       = {Huijie Guo and Lei Shi},
  doi          = {10.1016/j.imavis.2023.104754},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104754},
  shortjournal = {Image Vis. Comput.},
  title        = {Contrastive learning with semantic consistency constraint},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning more discriminative clues with gradual attention
for fine-grained visual categorization. <em>ICV</em>, <em>136</em>,
104753. (<a href="https://doi.org/10.1016/j.imavis.2023.104753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization, which aims to identify the different subcategories of images within the same category, is a very challenging task due to the large intra-class differences and subtle inter-class variances. The existing methods mostly focus on the salient local regions and ignore other features which probably help to recognize the images more precisely. To address this issue, in this paper, we propose a novel end-to-end network composed of the self-calibrated convolution, gradual attention module and feature inverse module for fine-grained visual categorization. To extract the salient features, the self-calibrated convolution is exploited which can avoid the influence of irrelevant information and locate salient regions more accurately. In aiming to extract the discriminative features, we propose the gradual attention module which consists of alternate channel-spatial attention and hierarchical feature grouping. The gradual attention module can extract the subtle discriminative features gradually even when the semantic information of shallow stages is not rich. Moreover, we design the feature inverse module which forces the next stage of network to search for other different useful features by feature inverse. The gradual attention module combined with the feature inverse module is capable of finding more detailed regions and of benefit to improving classification performance. Finally, the stage features and fused features are jointly used for classification. The proposed method is evaluated on three classical fine-grained image datasets and compared with a number of state-of-the-art methods. Our method achieves 89.5 % , 95.2 % 89.5%,95.2% and 93.9 % 93.9% accuracies on CUB-200–2011, Stanford Cars and FGVC-Aircraft datasets respectively. The experimental results demonstrate the effectiveness and superiority of the proposed method.},
  archive      = {J_ICV},
  author       = {Qin Xu and Mengquan Zhang and Yun Li and Zhifu Tao},
  doi          = {10.1016/j.imavis.2023.104753},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104753},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning more discriminative clues with gradual attention for fine-grained visual categorization},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COME: Clip-OCR and master ObjEct for text image captioning.
<em>ICV</em>, <em>136</em>, 104751. (<a
href="https://doi.org/10.1016/j.imavis.2023.104751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text image captioning aims to understand the scene text in images for generating image captions. The key challenge of this task is to accurately and comprehensively understand the OCR tokens of scene text. Due to the dual modal of visual and textual features of scene text, expressing the multimodal semantic features of OCR tokens accurately is a challenging task. Additionally, since scene text cannot exist independently of specific objects and is always associated with its surroundings, establishing a scene graph centered around OCR tokens is also an important approach to understand its relationship with other objects in the image. In this paper, we propose a novel model named C lip- O CR and M aster Obj E ct (dubbed as COME ) for text image captioning. First, we introduce a CLIP-OCR module to enhance the multimodal representation of OCR tokens. We separate the OCR representation into visual and textual items and narrow the similarity by contrastive learning . With the assistance of the CLIP-OCR module, we realize correlation alignment between different modes. Next, we propose the concept of master object for each OCR text and purify the OCR-oriented scene graph with it. The master object is defined as the object to which the OCR is attached, which bridges the semantic relationship between the OCR tokens and the image. We consider the master object as a proxy that connects OCR tokens and other regions in the image. By exploring the master object for each OCR token, we build a purified scene graph based on the master object and then enrich the visual embedding by the Graph Convolution Network (GCN). Furthermore, we cluster the OCR tokens and append the hierarchical information on the input embedding to provide a complete representation. Experiments on the TextCaps validation set and test set demonstrate the effectiveness of the proposed framework.},
  archive      = {J_ICV},
  author       = {Gang Lv and Yining Sun and Fudong Nian and Maofei Zhu and Wenliang Tang and Zhenzhen Hu},
  doi          = {10.1016/j.imavis.2023.104751},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104751},
  shortjournal = {Image Vis. Comput.},
  title        = {COME: Clip-OCR and master ObjEct for text image captioning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single image dehazing using extended local dark channel
prior. <em>ICV</em>, <em>136</em>, 104747. (<a
href="https://doi.org/10.1016/j.imavis.2023.104747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is an important technique aimed at eliminating the haze in the atmosphere to enhance the image&#39;s visual quality. There are many applications where it has been used as a prepossessing step, such as in event detection . In recent years, the dark channel prior methodology has been recognised as an effective approach for eliminating haze from hazy images. However, the main drawback of the existing dark channel prior methodology is that it only considers a single colour channel of the RGB image with pixels having minimum intensity values. This non-uniform selection of the dark channel from a single channel eradicates the effect of the transmission across the different channels of the hazy image. Hence, the haze cannot be removed to a great extent using the existing method. Thus, to address the problem of non-uniform estimation of the dark channel by the existing dark channel prior method, we propose an approach where the dark channel will be computed from all three channels of an image by selecting the minimum intensity. The main advantage of using the proposed prior-based methodology for image dehazing over deep neural network-based models such as CNN or GANs is that training deep models requires a large amount of training data , thus resulting in a longer training time. Experimental outcomes exhibit that the proposed technique outperforms state-of-the-art methods on synthetic datasets as well as real-world hazy images. The findings demonstrate that the proposed technique obtains better accuracy as compared to the state-of-the-art methods and recent deep learning-based models over the D-HAZY, I-HAZE, O-HAZE and Middlebury databases.},
  archive      = {J_ICV},
  author       = {Pulkit Dwivedi and Soumendu Chakraborty},
  doi          = {10.1016/j.imavis.2023.104747},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104747},
  shortjournal = {Image Vis. Comput.},
  title        = {Single image dehazing using extended local dark channel prior},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loss-aware automatic selection of structured pruning
criteria for deep neural network acceleration. <em>ICV</em>,
<em>136</em>, 104745. (<a
href="https://doi.org/10.1016/j.imavis.2023.104745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured pruning is a well-established technique for compressing neural networks , making them suitable for deployment in resource-limited edge devices. This study presents an efficient loss-aware automatic selection of structured pruning (LAASP) criteria for slimming and accelerating deep neural networks . The majority of pruning methods employ a sequential process consisting of three stages, 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed pruning technique adopts a pruning-while-training approach that eliminates the first stage and integrates the second and third stages into a single cycle. The automatic selection of magnitude or similarity-based filter pruning criteria from a specified pool of criteria and the specific pruning layer at each pruning iteration is guided by the network&#39;s overall loss on a small subset of training data . To mitigate the abrupt accuracy drop due to pruning, the network is retrained briefly after each reduction of a predefined number of floating-point operations (FLOPs). The optimal pruning rates for each layer in the network are automatically determined, eliminating the need for manual allocation of fixed or variable pruning rates for each layer. Experiments on the VGGNet, ResNet , and MobileNet models on the CIFAR-10 and ImageNet benchmark datasets demonstrate the effectiveness of the proposed method. In particular, the ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the top-1 accuracy compared to state-of-the-art methods while reducing the network FLOPs by 52%. Furthermore, pruning the ResNet50 model on the ImageNet dataset reduces FLOPs by more than 42% with a negligible 0.33% drop in the top-5 accuracy. The source code of this study is publicly available on GitHub: https://github.com/ghimiredhikura/laasp .},
  archive      = {J_ICV},
  author       = {Deepak Ghimire and Kilho Lee and Seong-heum Kim},
  doi          = {10.1016/j.imavis.2023.104745},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104745},
  shortjournal = {Image Vis. Comput.},
  title        = {Loss-aware automatic selection of structured pruning criteria for deep neural network acceleration},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate video saliency prediction via hierarchical fusion
and temporal recurrence. <em>ICV</em>, <em>136</em>, 104744. (<a
href="https://doi.org/10.1016/j.imavis.2023.104744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ability to extract spatiotemporal features, 3D convolutional networks have become the mainstream method for Video Saliency Prediction (VSP). However, these methods cannot make full use of hierarchical spatiotemporal features and also lack focus on past salient features , which hinders further improvements in accuracy. To address these issues, we propose a 3D convolutional Network based on Hierarchical Fusion and Temporal Recurrence (HFTR-Net) for VSP. Specifically, we propose a Bi-directional Temporal–Spatial Feature Pyramid (BiTSFP), which adds the flow of shallow location information based on the previous flow of deep semantic information. Then, different from simple addition and concatenation, we design a Hierarchical Adaptive Fusion (HAF) mechanism that can adaptively learn the fusion weights of adjacent features to integrate them appropriately. Moreover, to utilize previous salient information, a Recall 3D convGRU (R3D GRU) module is integrated into the 3D convolution-based method for the first time. It subtly combines the local feature extraction of the 3D backbone with the long-term relationship modeling of the temporal recurrence mechanism. Experimental results on the three common datasets demonstrate that the HFTR-Net outperforms existing state-of-the-art methods in accuracy.},
  archive      = {J_ICV},
  author       = {Yunzuo Zhang and Tian Zhang and Cunyu Wu and Yuxin Zheng},
  doi          = {10.1016/j.imavis.2023.104744},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104744},
  shortjournal = {Image Vis. Comput.},
  title        = {Accurate video saliency prediction via hierarchical fusion and temporal recurrence},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pruning-and-distillation: One-stage joint compression
framework for CNNs via clustering. <em>ICV</em>, <em>136</em>, 104743.
(<a href="https://doi.org/10.1016/j.imavis.2023.104743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network pruning and knowledge distillation , as two effective network compression techniques , have drawn extensive attention due to their success in reducing model complexity. However, previous works regard them as two independent methods and combine them in an isolated manner rather than joint , leading to a sub-optimal optimization. In this paper, we propose a collaborative compression scheme named P runing-and- D istillation via C lustering (PDC), which integrates pruning and distillation into an end-to-end single-stage framework that takes both advantages of them. Specifically, instead of directly deleting or zeroing out unimportant filters within each layer, we reconstruct them based on clustering, which preserves the learned features as much as possible. The guidance from the teacher is integrated into the pruning process to further improve the generalization of pruned model, which alleviates the randomness caused by reconstruction to some extent. After convergence, we can equivalently remove reconstructed filters within each cluster through the proposed channel addition operation. Benefiting from such equivalence, we no longer require the time-consuming fine-tuning step to regain accuracy. Extensive experiments on CIFAR-10/100 and ImageNet datasets show that our method achieves the best trade-off between performance and complexity compared with other state-of-the-art algorithms. For example, for ResNet-110, we achieve a 61.5 % % FLOPs reduction with even 0.14 % % top-1 accuracy increase on CIFAR-10 and remove 55.2 % % FLOPs with only 0.32 % % accuracy drop on CIFAR-100.},
  archive      = {J_ICV},
  author       = {Tao Niu and Yinglei Teng and Lei Jin and Panpan Zou and Yiding Liu},
  doi          = {10.1016/j.imavis.2023.104743},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104743},
  shortjournal = {Image Vis. Comput.},
  title        = {Pruning-and-distillation: One-stage joint compression framework for CNNs via clustering},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ABC: Aligning binary centers for single-stage monocular 3D
object detection. <em>ICV</em>, <em>136</em>, 104741. (<a
href="https://doi.org/10.1016/j.imavis.2023.104741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precisely perceiving the environment through a 3D perspective is essential and challenging for autonomous navigation and driving. Existing techniques rely on depth from LiDAR data or disparity in stereo vision to solve the poorly presented problem of detecting far-off and occluded objects. This increases structure complexity and computation burden, especially for single-stage systems. We argue that existing well-established detectors have the intrinsic potential to detect full-scene objects, but the extrinsic capabilities are limited by the structure form and optimization. Hence, we propose a double-branch single-stage monocular 3D object detection framework that aligns binary centers of object. Structurally, we construct two symmetrical and independent detectors, respectively using different prediction manners for 3D box parameters. Functionally, two detection heads have different sensitivities for the same object due to disentangling alignment. During the training, the detection heads were trained separately to obtain specific ability and aligned to promote the convergence. At inference, predictions of two branches are filtered via depth-aware non-maximal suppression (NMS) to acquire comprehensive detection results. Extensive experiments demonstrate that the proposed method achieves the state-of-the-art performance in monocular 3D detection on the KITTI-3D benchmark.},
  archive      = {J_ICV},
  author       = {Yong Feng and Jinglong Chen and Shuilong He and Enyong Xu},
  doi          = {10.1016/j.imavis.2023.104741},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104741},
  shortjournal = {Image Vis. Comput.},
  title        = {ABC: Aligning binary centers for single-stage monocular 3D object detection},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spontaneous visual database for detecting learning-centered
emotions during online learning. <em>ICV</em>, <em>136</em>, 104739. (<a
href="https://doi.org/10.1016/j.imavis.2023.104739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions significantly affect learning. The affective states of students can be automatically recognized through behavioral cues using machine learning and deep learning techniques . The accuracy and robustness of the recognition results mainly depend on the quality of the database used. However, no databases developed from actual online learning scenarios currently exist to detect and analyze learning-centered emotions (academic emotions). To address this shortcoming, an emotional database was compiled in this study using the facial expressions and hand gestures of students. The nonverbal emotions and behaviors in this dataset are spontaneous because all images are from videos recorded during actual online learning activities , leading to a robust learner emotion recognition model trained on these images that consider variates such as occlusion, illumination, background clutter, and pose. The database includes 1301 video clips and 21,632 images from 78 college students. The samples were labeled by both novice judges and researchers using six types of academic emotions (engaged, confused, frustrated, happy, sleepy, and neutral). To perform an elementary assessment of the database, an extensive analysis was conducted on it using state-of-the-art machine learning and deep learning algorithms . The experimental results demonstrate that the improved convolutional neural network algorithm provides the best recognition performance, with an average accuracy of 79%. In particular, when recognizing happy expressions, the model achieves the best score, with a recognition rate of 94%. This study provides a foundation for comparative studies of affective analysis approaches in specific online learning scenarios and enables these methods to be generalized in the educational field.},
  archive      = {J_ICV},
  author       = {Yaping Xu and Yanyan Li and Yunshan Chen and Haogang Bao and Yaqian Zheng},
  doi          = {10.1016/j.imavis.2023.104739},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104739},
  shortjournal = {Image Vis. Comput.},
  title        = {Spontaneous visual database for detecting learning-centered emotions during online learning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). IAC-ReCAM: Two-dimensional attention modulation and
category label guidance for weakly supervised semantic segmentation.
<em>ICV</em>, <em>136</em>, 104738. (<a
href="https://doi.org/10.1016/j.imavis.2023.104738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation (WSSS) approaches aim at pixel-level semantic category prediction using only image-level labels. The existing classifier-based method ReCAM has achieved good results, however, the classifiers tend to only focus on the most discriminative regions, resulting in an uneven distribution of features in the resulting class activation maps (CAMs). Besides, the classifiers are susceptible to image background interference and generate false activation mapping. To solve the above problems, we propose an improved method IAC-ReCAM, which introduces an activation network that integrated attention modulation and category label guidance based on the ReCAM method. We utilize the attention modulation module to reassign the feature distribution of the CAMs from the perspective of channels and spaces in turn. Meanwhile, we use the class label guidance module to suppress the generation of false activation mapping. Furthermore, we verified the effectiveness of the IAC-ReCAM method improvement work on both PASCAL VOC 2012 and MS COCO 2014 datasets, our method outperforms a large number of existing mainstream methods. Among them, compared with the ReCAM method, the mIoU of the pseudo-labels on the two datasets is improved by 2.9% and 1%, respectively.},
  archive      = {J_ICV},
  author       = {Yuqiang Li and Ying Wu and Chun Liu and Xinyi Wu},
  doi          = {10.1016/j.imavis.2023.104738},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104738},
  shortjournal = {Image Vis. Comput.},
  title        = {IAC-ReCAM: Two-dimensional attention modulation and category label guidance for weakly supervised semantic segmentation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Who is partner: A new perspective on data association of
multi-object tracking. <em>ICV</em>, <em>136</em>, 104737. (<a
href="https://doi.org/10.1016/j.imavis.2023.104737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion problem refers to the challenge of accurately tracking blocked or occluded objects. It occurs when multiple objects are moving nearby or overlapping with each other in the scene. Despite its frequent occurrence in multi-object tracking (MOT) tasks, occlusion is often overlooked by researchers. This paper proposes a simple and effective method to partially solve the occlusion problems of multi-object tracking by developing the Partner Mining Module (PMM) and the Partner Updating Module (PUM). The PMM module mines the space relationship between objects, and the PUM module uses the relationship obtained by the PMM module to update lost tracklets&#39; positions. Importantly, these two modules can be integrated into existing data association-based multi-object tracking methods without any additional training expenses. Furthermore, this study proposes novel methods for computing measurement uncertainty to enhance trajectory accuracy. Experiments conducted on MOT16 and MOT17 datasets show the effectiveness of the proposed modules. Integration of PMM and PUM into original methods substantially enhances the IDF1 score by 1 point.},
  archive      = {J_ICV},
  author       = {Yuqing Ding and Yanpeng Sun and Zechao Li},
  doi          = {10.1016/j.imavis.2023.104737},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104737},
  shortjournal = {Image Vis. Comput.},
  title        = {Who is partner: A new perspective on data association of multi-object tracking},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). P3DC-shot: Prior-driven discrete data calibration for
nearest-neighbor few-shot classification. <em>ICV</em>, <em>136</em>,
104736. (<a href="https://doi.org/10.1016/j.imavis.2023.104736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest-Neighbor (NN) classification has been proven as a simple and effective approach for few-shot learning. The query data can be classified efficiently by finding the nearest support class based on features extracted by pre-trained deep models. However, NN-based methods are sensitive to the data distribution and may produce false prediction if the samples in the support set happen to lie around the distribution boundary of different classes. To solve this issue, we present P3DC-shot, an improved nearest-neighbor based few-shot classification method empowered by prior-driven data calibration. Inspired by the distribution calibration technique which utilizes the distribution or statistics of the base classes to calibrate the data for few-shot tasks, we propose a novel discrete data calibration operation which is more suitable for NN-based few-shot classification. Specifically, we treat the prototypes representing each base class as priors and calibrate each support data based on its similarity to different base prototypes. Then, we perform NN classification using these discretely calibrated support data. Results from extensive experiments on various datasets show our efficient non-learning based method can outperform or at least comparable to SOTA methods which need additional learning steps.},
  archive      = {J_ICV},
  author       = {Shuangmei Wang and Rui Ma and Tieru Wu and Yang Cao},
  doi          = {10.1016/j.imavis.2023.104736},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104736},
  shortjournal = {Image Vis. Comput.},
  title        = {P3DC-shot: Prior-driven discrete data calibration for nearest-neighbor few-shot classification},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving distinctiveness in video captioning with
text-video similarity. <em>ICV</em>, <em>136</em>, 104728. (<a
href="https://doi.org/10.1016/j.imavis.2023.104728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning is a task of automatically describing visual content of a video with a sentence. Recent works in video captioning focus on improving the performance of sentence accuracy. However, the distinctiveness of sentence, i.e., highlighting unique and accurate details of video, in video captioning is still underexplored. This paper aims to improve sentence distinctiveness by incorporating video retrieval into the training process of the video captioning model. Specifically, the video retrieval will calculate similarity scores between the input text generated by the video captioning and videos. These similarity scores are then incorporated into the training loss of video captioning, which serves as distinctiveness constraint where the generated sentence and its corresponding video should have the highest similarity scores. To further improve the sentence distinctiveness, we additionally use reference scores, i.e., similarity scores between ground truth sentences and videos, as weights to scale the training loss of video captioning. This reference score serves as a target score for the model, indicating the desired level of distinctiveness for the generated sentence on how similar the generated sentence should be to the ground truth sentence for the corresponding video. Our qualitative and quantitative results show that our method improves sentence distinctiveness while simultaneously increasing its accuracy on MSVD and MSR-VTT datasets.},
  archive      = {J_ICV},
  author       = {Vania Velda and Steve Andreas Immanuel and Willy Fitra Hendria and Cheol Jeong},
  doi          = {10.1016/j.imavis.2023.104728},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104728},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving distinctiveness in video captioning with text-video similarity},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Easy pair selection method for kinship verification using
fixed age group images. <em>ICV</em>, <em>136</em>, 104727. (<a
href="https://doi.org/10.1016/j.imavis.2023.104727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinship Verification (KV) by using facial images is a relevant and challenging research problem in the field of computer vision. Facial features are extracted from the input pair of images and subsequently analyzed to find out existence of kin relation between them. Weights of the training model are suitably learned and adapted for the same. In most of the existing schemes, non kin pairs are randomly generated from the existing kin pair database for the learning process of the model. Since non-kin pairs are formed randomly from the true kin pair database itself, hence there is a significant probability that some non kin pairs have more similarity in feature space which is ideally undesirable and leads to improper learning of the model and results into errors. To reduce such errors, we have proposed a new scheme called easy-pair selection method for KV. Kin and non kin pairs are categorized into easy and hard pairs based on sum of square distance with respect to the feature space of the pairs. Suitable kin-margin (KM) value is chosen to classify the pair as easy or hard. Further, selected easy pairs are used to compute the optimized training weights (W) from the training dataset. The obtained weights will ensure to minimize the distance of kin pairs and maximize the distance of non-kin pairs in feature space. A fixed age group image database (FAG) have also been proposed through this article which contains the corresponding facial images of the parents and their children from the same age group to minimize the issue of age in-variance as well. Extensive experiments have been performed to validate the accuracy of the proposed scheme and is found to outperform the existing relevant schemes for KV.},
  archive      = {J_ICV},
  author       = {Madhu Oruganti and T. Meenpal and Saikat Majumder},
  doi          = {10.1016/j.imavis.2023.104727},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104727},
  shortjournal = {Image Vis. Comput.},
  title        = {Easy pair selection method for kinship verification using fixed age group images},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Student behavior recognition for interaction detection in
the classroom environment. <em>ICV</em>, <em>136</em>, 104726. (<a
href="https://doi.org/10.1016/j.imavis.2023.104726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of multimedia technologies , surveillance videos and other multimedia data have received widespread attention in several fields. Surveillance videos can monitor students&#39; learning statuses in real time. However, the current action recognition methods for teaching have limitations. First, the ethical privacy of AI and education makes public datasets on student behavior scarce. Therefore, based on the summarization of seven typical student behaviors in the classroom, course videos were obtained from the smart classroom to generate a dataset of student behavior. Compared with existing student behavior recognition datasets, the proposed dataset is distinguished by cluttered backgrounds, crowded scenes, and occlusions. Second, relational reasoning using existing methods is not ideal for distinguishing between students&#39; body parts and small objects in a cluttered background; the interactive utilization rate of different relational features is low, and it cannot take advantage of the complementarity of different relational features, resulting in poor performance of interaction action recognition. Therefore, the attention-based relational reasoning module strengthens the interactive representation between small objects and human body parts. At the same time, considering that there is a certain complementary relationship between relational features, this study constructs a relational feature fusion module which models a human-to-human interaction relationship built upon supporting human&#39;s body part and surrounding context. Finally, the reconstructed features and human-appearance features were fused to achieve accurate interactive action recognition. Through an experimental comparison between the proposed and current mainstream algorithms on the generated student behavior dataset, it was verified that the proposed model achieves state-of-the-art performance in action recognition.},
  archive      = {J_ICV},
  author       = {Yating Li and Xin Qi and Abdul Khader Jilani Saudagar and Abdul Malik Badshah and Khan Muhammad and Shuai Liu},
  doi          = {10.1016/j.imavis.2023.104726},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104726},
  shortjournal = {Image Vis. Comput.},
  title        = {Student behavior recognition for interaction detection in the classroom environment},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A regressive encoder-decoder-based deep attention model for
segmentation of fetal head in 2D-ultrasound images. <em>ICV</em>,
<em>136</em>, 104725. (<a
href="https://doi.org/10.1016/j.imavis.2023.104725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound imaging is the most commonly used imaging during pregnancy for tracking the fetus&#39;s growth and monitoring other biological parameters. The assessment of the development of the baby&#39;s growth requires imaging-based analysis in every trimester. The automatic computerized software and systems provide the platform for radiologists to more accurately access the fetus&#39;s head circumference as compared to manual estimation. The improvement of such computerized algorithms is always the key demand to improve accuracy and precision. This paper proposes an improved encoder-decoder model for the segmentation of the fetal head segmentation in 2D-ultrasound images. The proposed model uses regression in combination with attention to the encoder-decoder model to determine the fetus&#39;s head circumference. The model is further extended with the post-processing ellipse fitting to superimpose the segmentation region on ultrasound images for clear visualization of the fetus&#39;s head. Further, the proposed model performance is evaluated by using various statistical measures using segmented regions and available ground truth. The experimental results demonstrate a similarity score of 94.56%. The comparative result suggests that the proposed model is providing a more accurate fetus head segmentation region on 2D-ultrasound images as compared to other existing approaches.},
  archive      = {J_ICV},
  author       = {Somya Srivastava and Ankit Vidyarthi and Shikha Jain},
  doi          = {10.1016/j.imavis.2023.104725},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104725},
  shortjournal = {Image Vis. Comput.},
  title        = {A regressive encoder-decoder-based deep attention model for segmentation of fetal head in 2D-ultrasound images},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion recognition at a distance: The robustness of machine
learning based on hand-crafted facial features vs deep learning models.
<em>ICV</em>, <em>136</em>, 104724. (<a
href="https://doi.org/10.1016/j.imavis.2023.104724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion estimation from face expression analysis is nowadays a widely-explored computer vision task. In turn, the classification of expressions relies on relevant facial features and their dynamics. Despite the promising accuracy results achieved in controlled and favorable conditions, the processing of faces acquired at a distance, entailing low-quality images, still suffers from a significant performance decrease. In particular, most approaches and related computational models become extremely unstable in the case of the very small amount of useful pixels that is typical in these conditions. Therefore, their behavior should be investigated more carefully. On the other hand, real-time emotion recognition at a distance may play a critical role in smart video surveillance, especially when controlling particular kinds of events, e.g., political meetings, to try to prevent adverse actions. This work compares facial expression recognition at a distance by: 1) a deep learning architecture based on state-of-the-art (SOTA) proposals, which exploits the whole images to autonomously learn the relevant embeddings; 2) a machine learning approach that relies on hand-crafted features, namely the facial landmarks preliminarily extracted using the popular Mediapipe framework. Instead of using either the complete sequence of frames or only the final still image of the expression, like current SOTA approaches, the two proposed methods are designed to use rich temporal information to identify three different stages of emotion. Expressions are time-split accordingly into four phases to better exploit their temporal-dependent dynamics. Experiments were conducted on the popular Extended Cohn-Kanade dataset (CK+). It was chosen for its wide use in related literature, and because it includes videos of facial expressions and not only still images. The results show that the approach relying on machine learning via hand-crafted features is more suitable for classifying the initial phases of the expression and does not decay in terms of accuracy when images are at a distance (only 0.08% of decay). On the contrary, deep learning not only has difficulties classifying the initial phases of the expressions but also suffers from relevant performance decay when considering images at a distance (52.68% accuracy decay).},
  archive      = {J_ICV},
  author       = {Carmen Bisogni and Lucia Cimmino and Maria De Marsico and Fei Hao and Fabio Narducci},
  doi          = {10.1016/j.imavis.2023.104724},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104724},
  shortjournal = {Image Vis. Comput.},
  title        = {Emotion recognition at a distance: The robustness of machine learning based on hand-crafted facial features vs deep learning models},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MODE: Monocular omnidirectional depth estimation via
consistent depth fusion. <em>ICV</em>, <em>136</em>, 104723. (<a
href="https://doi.org/10.1016/j.imavis.2023.104723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation has seen significant progress in recent years, especially in outdoor scenes. However, depth estimation results are not satisfying in omnidirectional images . As compared to perspective images, estimating the depth map from an omnidirectional image captured in the outdoor scene, using neural networks , has two additional challenges: (i) the depth range of outdoor images varies a lot across different scenes, making it difficult for the depth network to predict accurate depth results for training with an indoor dataset, besides the maximum distance in outdoor scenes mostly stays the same as the camera sees the sky, but depth labels in this region are entirely missing in existing datasets; (ii) a standard representation of omnidirectional images introduces spherical distortion, which causes difficulties for the vanilla network to predict accurate relative structural depth details. In this paper, we propose a novel network-MODE by giving special considerations to those challenges and designing a set of flexible modules for improving the performance of omnidirectional depth estimation. First, a consistent depth structure module is proposed to estimate a consistent depth structure map, and the predicted structural map can improve depth details. Second, to suit the characteristics of spherical sampling, we propose a strip convolution fusion module to enhance long-range dependencies. Third, rather than using a single depth decoder branch as in previous methods, we propose a semantics decoder branch to estimate sky regions in the omnidirectional image. The proposed method is validated on three widely used datasets, demonstrating the state-of-the-art performance. Moreover, the effectiveness of each module is shown through an ablation study on real-world datasets. Our code is available at https://github.com/lkku1/MODE . © 2017 Elsevier Inc. All rights reserved.},
  archive      = {J_ICV},
  author       = {Yunbiao Liu and Chunyi Chen},
  doi          = {10.1016/j.imavis.2023.104723},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104723},
  shortjournal = {Image Vis. Comput.},
  title        = {MODE: Monocular omnidirectional depth estimation via consistent depth fusion},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial anchor-guided feature refinement for adversarial
defense. <em>ICV</em>, <em>136</em>, 104722. (<a
href="https://doi.org/10.1016/j.imavis.2023.104722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training (AT), which is known as a robust training method for defending against adversarial examples, usually loses the performance of models for clean examples due to the feature distribution discrepancy between clean and adversarial. In this paper, we propose a novel Adversarial Anchor-guided Feature Refinement (AAFR) defense method aimed at reducing the discrepancy and delivering reliable performances for both clean and adversarial examples. We devise adversarial anchor that detects whether the feature comes from clean or adversarial example. Then, we use adversarial anchor to refine the feature to reduce the discrepancy. As a result, the proposed method substantially achieves adversarial robustness while preserving the performance for clean examples. The effectiveness of the proposed method is verified with comprehensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets.},
  archive      = {J_ICV},
  author       = {Hakmin Lee and Yong Man Ro},
  doi          = {10.1016/j.imavis.2023.104722},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104722},
  shortjournal = {Image Vis. Comput.},
  title        = {Adversarial anchor-guided feature refinement for adversarial defense},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-view human gait recognition using hybrid whale and
gray wolf optimization algorithm with a random forest classifier.
<em>ICV</em>, <em>136</em>, 104721. (<a
href="https://doi.org/10.1016/j.imavis.2023.104721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition has become one of the furthest promising behavioral biometric techniques for identifying individuals. Gait recognition models are capable of identifying humans at a distance based on their walking manner without their permission or interference. However, it is usually noted that the performance of a gait recognition approach will drop drastically in the presence of covariates such as carrying conditions, clothing conditions, and variations in the view angle. Therefore, it is necessary to develop a robust gait recognition system in order to identify the most significant gait features. In this paper, we introduced a recently developed hybrid whale and gray wolf optimization algorithm (WGWOA) for determining the optimal subset of gait features by combining the advantages of whale optimization and gray wolf optimization techniques. Moreover, we employed principal component analysis (PCA) to extract the essential gait features from the gradient gait energy image (GGEI) and random forest (RF) approach to classify the optimal gait features. The proposed method has been assessed on the publicly available largest multi-view CASIA-B and OU-MVLP benchmark datasets. Experimental results indicate that the proposed model achieved an accuracy of 99.25%, 98.39%, and 97.97% under normal, carrying a bag and wearing coat walking conditions, respectively on the CASIA-B dataset. Furthermore, the proposed model achieved an accuracy of 97.63% under normal conditions on the OU-MVLP dataset. The comparative results also confirm that the proposed algorithm is superior to the contemporary approaches.},
  archive      = {J_ICV},
  author       = {P. Sankara Rao and Priyadarsan Parida and Gupteswar Sahu and Sonali Dash},
  doi          = {10.1016/j.imavis.2023.104721},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104721},
  shortjournal = {Image Vis. Comput.},
  title        = {A multi-view human gait recognition using hybrid whale and gray wolf optimization algorithm with a random forest classifier},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Engagement detection and enhancement for STEM education
through computer vision, augmented reality, and haptics. <em>ICV</em>,
<em>136</em>, 104720. (<a
href="https://doi.org/10.1016/j.imavis.2023.104720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {STEM (Science, technology, engineering, and mathematics) based education is being revolutionized with the rapid development of the applications of intelligent Haptics. Theoretical knowledge may now be put into practice, and complex ideas can now be presented using three-dimensional models that are enhanced in the actual environment via Augmented Reality (AR) and computer Haptics to enhance the student&#39;s interest in concepts and studies. This article presents a novel design and development of an AR and Haptics-based STEM product comprising of World Map with Haptic feedback using Vuforia, Unity 3D, and Open-Haptics to enhance student engagement. To detect students&#39; engagement in online learning, a computer vision-based system is also designed and deployed in the real-time website that uses the features, such as facial emotions, pose estimation, and head rotation. Marker-based augmentation is employed, and a sense of touch is included for an immersive experience for the students with the use of the haptic Touch Omni device. Hence, the proposed approach can improve classroom learning activities by using Augmented reality and Haptics-based STEM product and by its evaluation using a computer vision system. To validate the improvement in engagement, a comprehensive user study is performed and analyzed on the proposed setup.},
  archive      = {J_ICV},
  author       = {Hasnain Ali Poonja and Muhammad Ayaz Shirazi and Muhammad Jawad Khan and Kashif Javed},
  doi          = {10.1016/j.imavis.2023.104720},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104720},
  shortjournal = {Image Vis. Comput.},
  title        = {Engagement detection and enhancement for STEM education through computer vision, augmented reality, and haptics},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoSegEdge: Searching for the edge device real-time
semantic segmentation based on multi-task learning. <em>ICV</em>,
<em>136</em>, 104719. (<a
href="https://doi.org/10.1016/j.imavis.2023.104719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time semantic segmentation is a challenging task for resource-constrained edge devices. We propose AutoSegEdge, based on Neural Architecture Search (NAS), a semantic segmentation approach that runs on edge devices in real-time. Besides accuracy, we employ FLOPs and latency on the target edge devices as search constraints . Our work is probably one of the first attempts to translate multi-objectives NAS into Multi-Task Learning. Be inspired by Multi-Task Learning, we regard the sub-objective in multi-objective NAS as a learning task in Multi-Task Learning. The total loss function of the multi-objective NAS is deconstructed into the weighted sum of the sub-objective loss function. However, the conflict among the sub-objective will cause the searched networks to “architecture collapse.” To avoid the multi-objectives NAS falls into “architecture collapse.” Based on uncertainty, this paper proposes a method to learn the weights of sub-objective loss functions automatically. AutoSegEdge was discovered from an efficient cell-level search space that integrates multi-resolution branches. Additionally, AutoSegEdge employs knowledge distillation to further boost accuracy. Finally, we accelerated AutoSegEdge using NVIDIA&#39;s TensorRT and deployed it on the Nvidia Jetson NX. Experiments demonstrate that multi-objectives NAS only requires 1.5 GPU days to obtain the best result on a single Nvidia Tesla V100 GPU. On the Cityscapes dataset, AutoSegEdge achieved an mIoU of 70.3% with 16.6 FPS on the Nvidia Jetson NX (and 194.54 FPS on an Nvidia Tesla V100 GPU) at the original resolution (1024 × 2048) using TensorRT. Our method is 2–3 × faster than existing state-of-the-art real-time methods while maintaining competitive accuracy. We also conducted robustness experiments to analyze our method and modules. The code is available: https://github.com/douziwenhit/AutoSeg_edge.git .},
  archive      = {J_ICV},
  author       = {Ziwen Dou and Dong Ye and Boya Wang},
  doi          = {10.1016/j.imavis.2023.104719},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104719},
  shortjournal = {Image Vis. Comput.},
  title        = {AutoSegEdge: Searching for the edge device real-time semantic segmentation based on multi-task learning},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dedicated benchmark for contour-based corner detection
evaluation. <em>ICV</em>, <em>136</em>, 104716. (<a
href="https://doi.org/10.1016/j.imavis.2023.104716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous contour-based corner detection (CBCD) algorithms have been proposed recently, necessitating effective and practical evaluation. Most existing methods evaluate corner detection accuracy through metrics between the testing image and its attacked versions or rely on image-specific ground truth for corner evaluation. These methods use images as input, failing to solely evaluate the corner detection performance but combining it with contour extraction evaluation. Since contour extraction is another important research topic and existing CBCD algorithms almost have no contribution to contour extraction, this intertwining may negatively impact the evaluation results, hindering corner detection development. Furthermore, most evaluation methods directly provide simple statistical scores of evaluation metrics, such as the mean value, which are inadequate to reflect the overall performance distribution. This study presents a novel benchmark that is specifically designed for assessing CBCD methods, which includes two major contributions. Firstly, we design two dedicated datasets, one with the ground-truth corner and the other without them. The dedicated contours instead of images are employed as input to evaluate numerous CBCD methods, eliminating the impact of the extracted contour quality. When the ground-truth corners are unavailable, we employ additional contour attacks, including Gaussian noise , projective, and combined geometry on contours, to simulate real-world complex image processes compared with the attacks in existing evaluation methods. Secondly, we evaluate the performance of twelve CBCD methods using six distinct metrics based on the constructed contour datasets. To gain a deep insight into the overall performance distribution, the sign test method for hypothesis testing is utilized alongside some simple statistical measures for evaluation metric analysis. Experimental results demonstrated that no individual method performs the best across all six evaluation metrics, while different CBCD algorithms have their positive scenarios. The evaluation code will be publicly available at https://github.com/roylin1229/CBCD_eva .},
  archive      = {J_ICV},
  author       = {Xinyu Lin and Yingjie Zhou and Yipeng Liu and Ce Zhu},
  doi          = {10.1016/j.imavis.2023.104716},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104716},
  shortjournal = {Image Vis. Comput.},
  title        = {A dedicated benchmark for contour-based corner detection evaluation},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MCANet: Hierarchical cross-fusion lightweight transformer
based on multi-ConvHead attention for object detection. <em>ICV</em>,
<em>136</em>, 104715. (<a
href="https://doi.org/10.1016/j.imavis.2023.104715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visual Transformer model based on self-attention has achieved better performance than convolutional neural networks in object detection tasks. However, existing visual Transformer models are typically heavy-weight to extract global features. In contrast, CNNs can extract features with fewer parameters and computational costs. To combine the advantages of convolutional processing at the local level with the advantages of the Transformer&#39;s global interaction, this paper proposes MCANet, a Hierarchical Cross-Fusion Lightweight Transformer Based on Multi-ConvHead Attention for Object Detection. To bi-directionally fuse local and global features, MCANet adds two improved transformers (MCA-Former) for global interaction and two novel feature fusion modules MCA-CSP. MCA-Former uses a novel self-attention computation method named Multi-ConvHead Attention(MCA) based on multi-scale depth-separable convolution, which reduces the computational cost by 2/3. Meanwhile, the number of model parameters is reduced to 9.49 M by using channel segmentation and multi-layer cross- fusion strategies. On the Pascal VOC and COCO datasets, the proposed model outperforms YOLOv4-Tiny in terms of AP by 2.43% and 1.8%, respectively. Additionally, MCANet is also superior to many latest lightweight object detection models. Results of various ablation experiments also verify the effectiveness of the proposed method.},
  archive      = {J_ICV},
  author       = {Zuopeng Zhao and Kai Hao and Xiaofeng Liu and Tianci Zheng and Junjie Xu and Shuya Cui and Chen He and Jie Zhou and Guangming Zhao},
  doi          = {10.1016/j.imavis.2023.104715},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104715},
  shortjournal = {Image Vis. Comput.},
  title        = {MCANet: Hierarchical cross-fusion lightweight transformer based on multi-ConvHead attention for object detection},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic deep sparse multi-trial vector-based differential
evolution clustering with manifold learning and incremental technique.
<em>ICV</em>, <em>136</em>, 104712. (<a
href="https://doi.org/10.1016/j.imavis.2023.104712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most deep clustering methods despite utilizing complex networks to learn better from data, use a shallow clustering method. These methods have difficulty in finding good clusters due to the lack of ability to handle between local search and global search to prevent premature convergence. In other words, they do not consider different aspects of the search and it causes them to get stuck in the local optimum. In addition, the majority of existing deep clustering approaches perform clustering with the knowledge of the number of clusters, which is not practical in most real scenarios where such information is not available. To address these problems, this paper presents a novel automatic deep sparse clustering approach based on an evolutionary algorithm called Multi-Trial Vector-based Differential Evolution (MTDE). Sparse auto-encoder is first applied to extract embedded features. Manifold learning is then adopted to obtain representation and extract the spatial structure of features. Afterward, MTDE clustering is performed without prior information on the number of clusters to find the optimal clustering solution. The proposed approach was evaluated on various datasets, including images and time-series. The results demonstrate that the proposed method improved MTDE by 18.94% on average and compared to the most recent deep clustering algorithms, is consistently among the top three in the majority of datasets.},
  archive      = {J_ICV},
  author       = {Parham Hadikhani and Daphne Teck Ching Lai and Wee-Hong Ong and Mohammad H. Nadimi-Shahraki},
  doi          = {10.1016/j.imavis.2023.104712},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {104712},
  shortjournal = {Image Vis. Comput.},
  title        = {Automatic deep sparse multi-trial vector-based differential evolution clustering with manifold learning and incremental technique},
  volume       = {136},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dictionary-enabled efficient training of ConvNets for image
classification. <em>ICV</em>, <em>135</em>, 104718. (<a
href="https://doi.org/10.1016/j.imavis.2023.104718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional networks (ConvNets) are computationally expensive but well known for their performance on image data . One way to reduce their complexity is to explore inherited data sparsity . However, since the gradients involved in ConvNets require dynamic updates, applying data sparsity in the training step is not straightforward. Dictionary-based learning methods can be useful since they encode the original data in a sparse form. This paper proposes a new dictionary-based training paradigm for ConvNets by exploiting redundancy in the training data while keeping the distinctive features intact. The ConvNet is then trained on the reduced, sparse dataset. The new approach significantly reduces the training time without compromising accuracy. To the best of our knowledge, this is the first implementation of ConvNet on dictionary-based sparse training data. The proposed method is validated on three publicly available datasets â€“MNIST, USPS, and MNIST FASHION. The experimental results show a significant reduction of 4.5 times in the overall computational burden of vanilla ConvNet for all the datasets. Whereas the accuracy is intact at 97.21% for MNIST, 96.81% for USPS, and 88.4% for FASHION datasets. These results are comparable to state-of-the-art algorithms, such as ResNet-{18,34,50}, trained on the full training dataset.},
  archive      = {J_ICV},
  author       = {Usman Haider and Muhammad Hanif and Ahmar Rashid and Syed Fawad Hussain},
  doi          = {10.1016/j.imavis.2023.104718},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104718},
  shortjournal = {Image Vis. Comput.},
  title        = {Dictionary-enabled efficient training of ConvNets for image classification},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep expectation-maximization network for unsupervised image
segmentation and clustering. <em>ICV</em>, <em>135</em>, 104717. (<a
href="https://doi.org/10.1016/j.imavis.2023.104717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised learning, such as unsupervised image segmentation and clustering, are fundamental tasks in image representation learning. In this paper, we design a deep expectation-maximization (DEM) network for unsupervised image segmentation and clustering. It is based on the statistical modeling of image in its latent feature space by Gaussian mixture model (GMM), implemented in a novel deep learning framework. Specifically, in the unsupervised setting, we design an auto-encoder network and an EM module over the image latent features, for jointly learning the image latent features and GMM model of the latent features in a single framework. To construct the EM-module, we unfold the iterative operations of EM algorithm and the online EM algorithm in fixed steps to be differentiable network blocks, plugged into the network to estimate the GMM parameters of the image latent features. The proposed network parameters can be end-to-end optimized using losses based on log-likelihood of GMM, entropy of Gaussian component assignment probabilities and image reconstruction error. Extensive experiments confirm that our proposed networks achieve favorable results compared with several state-of-the-art methods in unsupervised image segmentation and clustering.},
  archive      = {J_ICV},
  author       = {Yannan Pu and Jian Sun and Niansheng Tang and Zongben Xu},
  doi          = {10.1016/j.imavis.2023.104717},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104717},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep expectation-maximization network for unsupervised image segmentation and clustering},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale features integration based multiple-in-single-out
network for object detection. <em>ICV</em>, <em>135</em>, 104714. (<a
href="https://doi.org/10.1016/j.imavis.2023.104714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single-level feature map-based object detection has been a challenging task due to the feature scale limitation. Therefore, enriching multiscale information of single-level features is considered a promising approach to deal with this challenge. Although most existing methods have attempted to augment the feature scale of single-level features, the detection performance is still unsatisfactory because these methods mine multiscale features only based on a one-level feature map. To address this problem, we propose a multiple-in-single-out network (MiSoNet) to integrate multiscale information from multilevel feature maps into a single-level feature map. To achieve this, MiSoNet’s key component is equipped with two cascaded modules: a multilevel feature integration module (MFIM) and a depthwise convolutional residual encoder (DWEncoder). Specifically, MFIM adaptively fuses features of inconsistent semantics and scales from multilevel feature maps. DWEncoder stacks several residual blocks with depthwise convolutions to extract multiscale contexts in the single feature map, which can further extend the scale range of the receptive fields. Extensive experiments are conducted on the Common Objects in Context (COCO) dataset, where the MiSoNet achieves a 41.0AP, which surpasses the YOLOF by 1.4AP with negligible computational overhead. Moreover, the MiSoNet, with fewer parameters and FLOPs, outperforms some advanced detectors based on the feature pyramid network.},
  archive      = {J_ICV},
  author       = {Kequan Yang and Jide Li and Songmin Dai and Xiaoqiang Li},
  doi          = {10.1016/j.imavis.2023.104714},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104714},
  shortjournal = {Image Vis. Comput.},
  title        = {Multiscale features integration based multiple-in-single-out network for object detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SFincBuster: Spoofed fingerprint buster via incremental
learning using leverage bagging classifier. <em>ICV</em>, <em>135</em>,
104713. (<a href="https://doi.org/10.1016/j.imavis.2023.104713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fingerprint presentation attack detection (PAD) studies are extensively explored by investigators to augment the security aspects of human authentication in biometric systems. Although existing approaches yield promising results while evaluated on data having same distribution as the training data , but their performance is declined in scenarios when data is gathered from unknown environment. In these situations model is learned with the new type of fake samples by re-training the entire model. Nevertheless, it has been a non-trivial concern to re-train the entire system to tackle the newly created fake samples as unseen artifacts are generated continuously in real-time scenarios. In this work, we expound a novel incremental learning-based approach namely; SFincBuster that can work effectively in the challenging real-time scenarios and can handle both new data as well as new observations from old classes. We train a leveraging bagging ensemble (LBE) in incremental fashion regardless of the extracted deep-level features (using pre-trained VGG19 network) being too large to accommodate into system memory, it is still possible to train our model effectively. Furthermore, the LBE integrates the simplicity of classical bagging with augmented randomization to the input and outcome of the base classifiers . To tackle the issue of change in distribution that arises with gradual changes from learned fakes to entirely new fingerprint artifacts, the SFincBuster employs LBE with ADWIN (Adaptive WINDowing) technique that continuously evaluate the performance of underlying base model and whenever a change is detected the weakest classifier is substituted with a new one. Our approach achieves high classification accuracy , even though it is not prerequisite to access all features at once. The SFincBuster is trained and evaluated on LivDet 2009, LivDet 2011, LivDet 2013, LivDet 2015 and LivDet 2021 benchmark datasets and yields maximum average classification accuracy (ACA) of 98.65% on LivDet 2013 and LivDet 2015 datasets. The model exhibits stupendous generalization capabilities with an average classification error rate (ACER) of 1.39% for known fakes (KF) and 2.84% for unknown fakes (UF). Finally, the comparable investigation perceives that the SFincBuster model demonstrates a noteworthy performance gain over the similar state-of-the-art (SOTA) approaches and achieve an improved benchmark for real-time cross-sensor, cross-material and cross-dataset scenarios.},
  archive      = {J_ICV},
  author       = {Deepika Sharma and Arvind Selwal},
  doi          = {10.1016/j.imavis.2023.104713},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104713},
  shortjournal = {Image Vis. Comput.},
  title        = {SFincBuster: Spoofed fingerprint buster via incremental learning using leverage bagging classifier},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic sign recognition using proposed lightweight twig-net
with linear discriminant classifier for biometric application.
<em>ICV</em>, <em>135</em>, 104711. (<a
href="https://doi.org/10.1016/j.imavis.2023.104711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and rise in Artificial Intelligence (AI) applications including Intelligent Transport System (ITS) has significantly reshaped and transformed. A key research area that is, an intelligent visual aid for Traffic Sign Recognition (TSR) has been certainly persuaded and transformed to control automobiles and ensure safe driving. Similarly, TSR is an important feature of Advanced Driver Assistance Systems (ADAS) that contributes to the safety of drivers, pedestrians, and automobiles. This work is aimed to recognize traffic signs robustly and efficiently in challenging environmental conditions. Hence, keeping the diversity of the problem in mind, a novel 30-layered deep Convolutional Neural Network (CNN) model is proposed. In the proposed model each convolutional layer produced around a minimum number of salient and highly discriminative features incorporating different datasets for classification purposes. This novel approach used the extracted features to train the model without the assistance of a Graphics Processing Unit (GPU). Once the model becomes trained, different competitive classifiers including Tree, SVM, KNN , and discriminant analysis are used to classify the data. The result reflects that Linear Discriminant Analysis (LDA) achieves comparable results among all other competitive classifiers with 97.4% accuracy with a minimum training time of 5.12 s on the CURE-TSR dataset. To evaluate and monitor the efficacy of the novel approach, it is further tested on two more datasets including GTSRB and BTSRB. The proposed approach acquired 99.12% and 98.16% accuracy on both datasets respectively. Finally, the comparable results on all the benchmark datasets reflect the proposed approach&#39;s applicability and advantage.},
  archive      = {J_ICV},
  author       = {Aisha Batool and Muhammad Wasif Nisar and Muhammad Attique Khan and Jamal Hussain Shah and Usman Tariq and Robertas Damaševičius},
  doi          = {10.1016/j.imavis.2023.104711},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104711},
  shortjournal = {Image Vis. Comput.},
  title        = {Traffic sign recognition using proposed lightweight twig-net with linear discriminant classifier for biometric application},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of anomaly in surveillance videos using quantum
convolutional neural networks. <em>ICV</em>, <em>135</em>, 104710. (<a
href="https://doi.org/10.1016/j.imavis.2023.104710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalous behavior identification is the process of detecting behavior that differs from its normal. These incidents will vary from violence to war, road crashes to kidnapping, and so on in a surveillance model. Video anomaly detection from video surveillance is a difficult research activity due to the frequency of anomalous cases. Since certain devices need manual evaluation for the detection of violent or criminal situations at the same time video monitoring of security cameras is also a challenging task and is unreliable. When the data or model dimension is sufficiently large, convolutional neural networks have the limitation of learning inefficiently. Quantum Convolutional Neural Network (QCNN) is the name given to a technology that combines CNN and quantum computing . Quantum computation and CNN are combined to create a more efficient and outperforming solution for solving complicated machine-learning problems. To analyze the anomalies in a sequence of video frames, two models are proposed in this research. In this research 07 layers of Javeria deep convolutional neural network (DCNN) are proposed on the selected hyperparameters named J. DCNN which is also different from the existing models to analyze the abnormal behavior in a video segment. Furthermore, for a comprehensive analysis of the abnormal video frames a model is proposed which is the combination of Javeria quantum and convolutional neural networks (J. QCNN). In this model 04-qubit quantum neural network is used with five layers and an optimal loss rate named J. QCNN. The proposed J. QCNN model is different from the existing deep learning architectures. The proposed models are trained from the scratch for the detection of anomalous from top challenging publicly available video surveillance datasets such as UNI-Crime and UCF Crime. The proposed J. QCNN model classifies the number of violent robberies such as armed thefts containing handguns or knives, and robberies displaying varying levels of viciousness with 0.99 accuracy while J. DCNN model gives 0.97 accuracy. The obtained results are superior in comparison with recent existing cutting-edge published work for real-time anomaly detection in video CCTV.},
  archive      = {J_ICV},
  author       = {Javaria Amin and Muhammad Almas Anjum and Kainat Ibrar and Muhammad Sharif and Seifedine Kadry and Ruben González Crespo},
  doi          = {10.1016/j.imavis.2023.104710},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104710},
  shortjournal = {Image Vis. Comput.},
  title        = {Detection of anomaly in surveillance videos using quantum convolutional neural networks},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extended neighborhood-based road and median filter for
impulse noise removal from depth map. <em>ICV</em>, <em>135</em>,
104709. (<a href="https://doi.org/10.1016/j.imavis.2023.104709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years there has been growing interest in the study of depth map&#39;s impulse noise detection and removal. The Rank-ordered Absolute Differences (ROAD) based on neighborhood statistics can effectively detect impulse noise for RGB images . However, since the statistical object of the ROAD is each pixel value in the neighborhood, and the neighborhood may contain other impulse noise, invalid depth zero-pixel points and other singular points , especially for large areas with depth missing, it is difficult for the algorithm to ensure the correctness and accuracy of the detection results. Therefore, the ROAD algorithm is not fully applicable to depth maps. Similarly, the median filter , as an effective impulse noise filtering algorithm , also has the same deficiency, so it is not suitable for areas in depth maps with no depth value. To address these issues, we present here an Extended Neighborhood-based ROAD and Median Filter algorithm to remove impulse noise from depth map. This approach improves the functionality of the ROAD algorithm and the Median Filter by extending the neighborhood of the current pixel adaptively, shielding zero-valued pixels and filtering out detected impulse noises in real time during detection, therefore effectively improving the accuracy of impulse noise filtering. It consequently reduces the miss rate and the error detection rate, and ultimately achieves a better edge-preserving and denoising effect. Our empirical evidence shows that the proposed algorithm outperforms existing denoising algorithms on both quantitative measures and visual perception qualities.},
  archive      = {J_ICV},
  author       = {Shuaihao Li and Xiang Bi and Yajun Zhao and Hongliang Bi},
  doi          = {10.1016/j.imavis.2023.104709},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104709},
  shortjournal = {Image Vis. Comput.},
  title        = {Extended neighborhood-based road and median filter for impulse noise removal from depth map},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semantic fusion based approach for express bill detection
in complex scenes. <em>ICV</em>, <em>135</em>, 104708. (<a
href="https://doi.org/10.1016/j.imavis.2023.104708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of the express logistics industry, hundreds of millions of images of express bills need to be recognized in the process of express transportation. However, it is challenging to detect express bills in an automated manner as the images of express bills acquired on the assembly line are of inferior quality and in complex scenes. Existing methods have difficulty in extracting semantic texture features at the pixel level for express bills in complex scenes. To solve the problem mentioned above, we propose an oriented frame target detection method Semantic Fusion Rotated Object Detector (SFRDet). In order to enhance the feature extraction capability in complex scenarios by fusing pixel-level texture features, SFRDet employs a semantic fusion mechanism to extract low-level semantic information from images to guide training. On this basis, the Semantic Reinforcement Feature Pyramid Network (SRFPN) is used to enhance the model’s attention to semantic information during the feature extraction process. This enables the model to obtain better feature extraction capability and faster inference at the same time. Extensive experiments are conducted on multiple datasets in practical application scenes. The result indicates the proposed method outperforms other state-of-the-art methods in precision and efficiency. It has a wild application prospect in the industry.},
  archive      = {J_ICV},
  author       = {Luming Zhang and Junjie Peng and Wenfu Liu and Haochen Yuan and Shuhua Tan and Lu Wang and Fen Yi},
  doi          = {10.1016/j.imavis.2023.104708},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104708},
  shortjournal = {Image Vis. Comput.},
  title        = {A semantic fusion based approach for express bill detection in complex scenes},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-knowledge distillation based on knowledge transfer from
soft to hard examples. <em>ICV</em>, <em>135</em>, 104700. (<a
href="https://doi.org/10.1016/j.imavis.2023.104700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To fully exploit knowledge from self-knowledge distillation network in which a student model is progressively trained to distill its own knowledge without a pre-trained teacher model, a self-knowledge distillation method based on knowledge transfer from soft to hard examples is proposed. A knowledge transfer module is designed to exploit the dark knowledge of hard examples, which can force the class probability consistency between hard and soft examples. It reduces the confidence of wrong prediction by transferring the class information from soft probability distributions of auxiliary self-teacher network to classifier network (self-student network). Furthermore, a dynamic memory bank for softened probability distribution is introduced, whose updating strategy is also presented. Experiments show the method improves the accuracy by 0.64% on classification datasets in average and by 3.87% on fine-grained visual recognition tasks in average, which makes its performance superior to the state-of-the-arts.},
  archive      = {J_ICV},
  author       = {Yuan Tang and Ying Chen and Linbo Xie},
  doi          = {10.1016/j.imavis.2023.104700},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104700},
  shortjournal = {Image Vis. Comput.},
  title        = {Self-knowledge distillation based on knowledge transfer from soft to hard examples},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FCR-TrackNet: Towards high-performance 6D pose tracking with
multi-level features fusion and joint classification-regression.
<em>ICV</em>, <em>135</em>, 104698. (<a
href="https://doi.org/10.1016/j.imavis.2023.104698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking an object’s 6D pose is significant in various real-world applications, such as robotic grasping, virtual reality, and self-driving. Still the existing methods suffer from following challenges: i) geometric shapes, such as rotational symmetry shapes, and ii) complex scenes, for instance, cluttered backgrounds and occluded scenes. To tackle these problems, we propose FCR-TrackNet, a novel tracking network that utilizes a residual iterative framework with low-and high-level feature fusion and joint classification-regression. FCR-TrackNet inputs the rendered RGB-D image from the previous frame pose and the current RGB-D image to extract low-level features. Then, high-level features are also obtained by the convolutional network and fused with low-level one to capture subtle variations in target object features across adjacent frames. To reduce computational complexity and ensure high tracking speed, we adopt decoupled branches to estimate the translation and rotation of the pose independently. At last, a joint classification-regression is designed to address the boundary problem of the rotation angle. We introduce a smooth classification label that effectively enhances the accuracy of rotation vector classification. We evaluate the performance of FCR-TrackNet on two well-known datasets, YCB-Video and YCBInEOAT. FCR-TrackNet achieves state-of-the-art ADD values of 94.5% and 93.2%, and ADD-S values of 96.7% and 96.0%, respectively, with a tracking speed of 89.6 Hz. It also outperforms competing algorithms in target pose tracking in occlusion and rotational symmetry shapes. The quantitative and qualitative results validate the high performance of FCR-TrackNet on 6D pose tracking.},
  archive      = {J_ICV},
  author       = {Wenjun Zhu and Haida Feng and Yang Yi and Mengyi Zhang},
  doi          = {10.1016/j.imavis.2023.104698},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104698},
  shortjournal = {Image Vis. Comput.},
  title        = {FCR-TrackNet: Towards high-performance 6D pose tracking with multi-level features fusion and joint classification-regression},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved YOLOX-x based UAV aerial photography object
detection algorithm. <em>ICV</em>, <em>135</em>, 104697. (<a
href="https://doi.org/10.1016/j.imavis.2023.104697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV) aerial photography object detection has high research significance in the fields of disaster rescue, ecological environmental protection, and military reconnaissance. The larger width of UAV photography introduces background interference into the detection task, whereas the relatively high imaging height of the UAV results in mostly small objects in the aerial images. YOLOX-X operated fast and achieved advanced results on MS COCO of natural scene images, so YOLOX-X was used as the baseline network in this paper. A UAV aerial photography object detection algorithm YOLOX_w with improved YOLOX-X is proposed to handle the characteristics of complex backgrounds and the large number of small objects in UAV aerial photography images. The model’s performance in detecting small objects is first improved by preprocessing the training set with the slicing aided hyper inference (SAHI) algorithm and by data augmentation. Then, a shallow feature map with rich spatial information is introduced into the path aggregation network (PAN), and a detection head is added to detect small objects. Next, the ultra-lightweight subspace attention module (ULSAM) is added to the PAN stage to highlight the target features and weaken the background features, which improves the detection accuracy of the network. Finally, the loss function of the bounding box regression is optimized to further improve network prediction accuracy. Experimental results on the VisDrone dataset demonstrate that the detection accuracy of the proposed YOLOX_w algorithm improved by 8% when compared with the baseline YOLOX-X. Moreover, migration experiments on the DIOR dataset verify the effectiveness and robustness of the improved method.},
  archive      = {J_ICV},
  author       = {Xin Wang and Ning He and Chen Hong and Qi Wang and Ming Chen},
  doi          = {10.1016/j.imavis.2023.104697},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104697},
  shortjournal = {Image Vis. Comput.},
  title        = {Improved YOLOX-X based UAV aerial photography object detection algorithm},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Problem-dependent attention and effort in neural networks
with applications to image resolution and model selection. <em>ICV</em>,
<em>135</em>, 104696. (<a
href="https://doi.org/10.1016/j.imavis.2023.104696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces two new ensemble-based methods to reduce the data and computation costs of image classification. They can be used with any set of classifiers and do not require additional training. In the first approach, data usage is reduced by only analyzing a full-sized image if the model has low confidence in classifying a low-resolution pixelated version. When applied on the best performing classifiers considered here, data usage is reduced by 61.2% on MNIST, 69.6% on KMNIST, 56.3% on FashionMNIST, 84.6% on SVHN, 40.6% on ImageNet, and 27.6% on ImageNet-V2, all with a less than 5% reduction in accuracy. However, for CIFAR-10, the pixelated data are not particularly informative, and the ensemble approach increases data usage while reducing accuracy. In the second approach, compute costs are reduced by only using a complex model if a simpler model has low confidence in its classification. Computation cost is reduced by 82.1% on MNIST, 47.6% on KMNIST, 72.3% on FashionMNIST, 86.9% on SVHN, 89.2% on ImageNet, and 81.5% on ImageNet-V2, all with a less than 5% reduction in accuracy; for CIFAR-10 the corresponding improvements are smaller at 13.5%. When cost is not an object, choosing the projection from the most confident model for each observation increases validation accuracy to 81.0% from 79.3% for ImageNet and to 69.4% from 67.5% for ImageNet-V2. Code available at: https://github.com/carohlfs/imavis .},
  archive      = {J_ICV},
  author       = {Chris Rohlfs},
  doi          = {10.1016/j.imavis.2023.104696},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104696},
  shortjournal = {Image Vis. Comput.},
  title        = {Problem-dependent attention and effort in neural networks with applications to image resolution and model selection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Alleviating the generalization issue in adversarial domain
adaptation networks. <em>ICV</em>, <em>135</em>, 104695. (<a
href="https://doi.org/10.1016/j.imavis.2023.104695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, adversarial learning has dominated domain adaptation , which is a popular branch of transfer learning . The basic idea of adversarial domain adaptation networks (ADAN) is to learn domain-invariant features which is able to confuse the domain discriminator . By sharing the spirit of generative adversarial networks (GANs), ADAN achieved state-of-the-art performance. However, ADAN also inherits the drawbacks of GANs. One of the most critical issues of GANs is that the learned distribution may be far from the expected one even if the training is successful, which is known as the generalization issue in GANs. As a result, it is no guarantee that the learned representations are domain-invariant even if the domain discriminator is successfully confused. To address this, we propose a new domain adaptation approach under the framework of ADAN. Specifically, we reformulate the conventional ADAN and propose ADAN plus metric protocol under the new ADAN formulation, ADANM for short, which leverages both adversarial learning and metric learning. The proposed method, on one hand, challenges the generalization issue in previous ADAN approaches. On the other hand, it guarantees that domain divergence is minimized during the adversarial training . Extensive experiments on three public benchmarks verify that the proposed protocol is favorable for unsupervised domain adaptation tasks.},
  archive      = {J_ICV},
  author       = {Xiao Zhe and Zhekai Du and Chunwei Lou and Jingjing Li},
  doi          = {10.1016/j.imavis.2023.104695},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104695},
  shortjournal = {Image Vis. Comput.},
  title        = {Alleviating the generalization issue in adversarial domain adaptation networks},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A complementary and contrastive network for stimulus
segmentation and generalization. <em>ICV</em>, <em>135</em>, 104694. (<a
href="https://doi.org/10.1016/j.imavis.2023.104694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing convolutional neural networks (CNNs) have achieved remarkable performance in medical image segmentation tasks. However, they still fail to generalize well to unseen datasets due to the limited size and diversity of training data as well as distribution shifts. Meanwhile, CNN-based methods have inherent limitations in capturing global contexts and suffer semantic dilution issues in the decoder stage, which leads to suboptimal predictions especially under low inter-class discrepancy and complex backgrounds. In this paper, we propose a novel framework named CCNet that learns complementary and contrastive features for accurate segmentation. Firstly, a novel complementary feature extraction module is formulated to learn global–local features by coordinating Transformer and CNN-style parallel branches. Secondly, a global context refinement module is constructed to adaptively generate a set of layer-specific global maps, so as to remedy semantic dilution. Thirdly, a mutual attentive module is designed to alleviate background confusion, in which contrastive cues are mutually captured from the foreground and background view by cascaded dual attention blocks. Moreover, we implement synthetic data augmentation to deal with training data scarcity and distribution shifts, thereby improving the out-of-distribution generalization of our model. Extensive experiments demonstrate that our CCNet achieves outstanding performance in polyp, skin lesion, and nuclei segmentation tasks, outperforming the state-of-the-arts.},
  archive      = {J_ICV},
  author       = {Na Ta and Haipeng Chen and Yingda Lyu and Xue Wang and Zenan Shi and Zhehao Liu},
  doi          = {10.1016/j.imavis.2023.104694},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104694},
  shortjournal = {Image Vis. Comput.},
  title        = {A complementary and contrastive network for stimulus segmentation and generalization},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrast enhancement of region of interest of backlit image
for surveillance systems based on multi-illumination fusion.
<em>ICV</em>, <em>135</em>, 104693. (<a
href="https://doi.org/10.1016/j.imavis.2023.104693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surveillance images under backlit conditions is a potential and challenging research problem in image processing and computer vision. Low contrast regions of interest of backlit images form an indiscernible part for the real-time surveillance and biometrics applications. However, current state-of-the-art techniques typically offer limited dark-region contrast enhancement, are susceptible to colour distortion. This research work uses region of interest segmentation and a transfer learning-based strategy to improve the contrast of the dark intensity regions in backlit image using multi-illumination mappings, motivated by its usefulness. To generate a diversified collection of illumination-maps and enhance the brightness and contrast of the backlit image, the proposed framework applies the varied set of log and gamma transforms. The experimental findings confirm the assertion that the proposed method provides better performance than state-of-the-art methods both objectively and subjectively.},
  archive      = {J_ICV},
  author       = {Gaurav Yadav and Dilip Kumar Yadav},
  doi          = {10.1016/j.imavis.2023.104693},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104693},
  shortjournal = {Image Vis. Comput.},
  title        = {Contrast enhancement of region of interest of backlit image for surveillance systems based on multi-illumination fusion},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SyPer: Synthetic periocular data for quantized light-weight
recognition in the NIR and visible domains. <em>ICV</em>, <em>135</em>,
104692. (<a href="https://doi.org/10.1016/j.imavis.2023.104692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-learning based periocular recognition systems typically use overparameterized deep neural networks associated with high computational costs and memory requirements. This is especially problematic for mobile and embedded devices in shared resource environments. To perform model quantization for lightweight periocular recognition in a privacy-aware manner, we propose and release SyPer, a synthetic dataset and generation model of periocular images. To enable this, we propose to perform the knowledge transfer in the quantization process on the embedding level and thus not identity-labeled data. This does not only allow the use of synthetic data for quantization, but it also successfully allows to perform the quantization on different domains to additionally boost the performance in new domains. In a variety of experiments on a diverse set of model backbones, we demonstrate the ability to build compact and accurate models through an embedding-level knowledge transfer using synthetic data. We also demonstrate very successfully the use of embedding-level knowledge transfer for near-infrared quantized models towards accurate and efficient periocular recognition on near-infrared images. The SyPer dataset, together with the evaluation protocol, the training code, and model checkpoints are made publicly available at https://github.com/jankolf/SyPer .},
  archive      = {J_ICV},
  author       = {Jan Niklas Kolf and Jurek Elliesen and Fadi Boutros and Hugo Proença and Naser Damer},
  doi          = {10.1016/j.imavis.2023.104692},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104692},
  shortjournal = {Image Vis. Comput.},
  title        = {SyPer: Synthetic periocular data for quantized light-weight recognition in the NIR and visible domains},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust visual tracking based on modified mayfly optimization
algorithm. <em>ICV</em>, <em>135</em>, 104691. (<a
href="https://doi.org/10.1016/j.imavis.2023.104691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, MOA is applied to visual target tracking for the first time, and a novel meta-heuristic tracking algorithm with efficiency and precision is obtained. Similar to the common problems of other classical swarm intelligence algorithms, standard MOA faces a high probability of falling into local extremals, early maturity and a low efficiency of a late convergence speed. Therefore, super-MOA, a modified MOA method, is proposed in this study. By designing the updating mechanism, the position and velocity parameters of ephemera are monitored and dynamically adjusted with the iteration degree, and the balance of the global and local optimization process in different iteration stages is improved. A mayfly progeny mutation strategy was proposed to alleviate the prematurity problem of the standard MOA algorithm. Meanwhile, we introduce a chaos algorithm to reconstruct the velocity parameter iteration mechanism, which alleviates the efficiency loss caused by the frequent repeated searching of historical locations by mayflies in standard MOA. In addition, we further understand and improve the parameters such as the dynamic gravitational coefficient and dance coefficient during the courtship flight of mayflies. We also design a frame size adaptive adjustment strategy, which effectively decreases the interference of invalid features. In terms of experiments, we compare this algorithm with other classical trackers from the qualitative, quantitative and statistical perspectives through OTB2015, VOT2018 and typical large-scale benchmarks. Sufficient tracking experiments in various tracking scenes show that our tracker performs great in terms of efficiency, robustness and accuracy.},
  archive      = {J_ICV},
  author       = {Yuqi Xiao and Yongjun Wu},
  doi          = {10.1016/j.imavis.2023.104691},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104691},
  shortjournal = {Image Vis. Comput.},
  title        = {Robust visual tracking based on modified mayfly optimization algorithm},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D hand reconstruction with both shape and appearance from
an RGB image. <em>ICV</em>, <em>135</em>, 104690. (<a
href="https://doi.org/10.1016/j.imavis.2023.104690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling 3D hands with geometry details and appearance can increase perceptual immersion and realism in many applications. However, traditional 3D representations such as meshes and voxel grids can not represent high-quality hand shapes with their property of the discrete surface and fixed topology. To address these problems, we introduce implicit function for more detailed shape reconstruction , and design a Structure-aware Signed Distance Function (S-SDF) to reconstruct hand shape in arbitrary resolution. In this way, our method not only focuses on independent fingers but also keeps the relationship between fingers. Such implicit function of 3D representation associated with part semantics and structure of the hand is central to generating realistic hand shapes of diverse variations. Meanwhile, in order to avoid time-consuming and laborious 3D annotating of texture, we present a self-supervised appearance synthesis approach. Extensive experiments on widely used datasets demonstrate that the proposed method reconstructs more realistic hands compared with previous methods.},
  archive      = {J_ICV},
  author       = {Xiaoyun Chang and Wentao Yi and Xiangbo Lin and Yi Sun},
  doi          = {10.1016/j.imavis.2023.104690},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104690},
  shortjournal = {Image Vis. Comput.},
  title        = {3D hand reconstruction with both shape and appearance from an RGB image},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-stream contrastive learning for self-supervised
skeleton-based action recognition. <em>ICV</em>, <em>135</em>, 104689.
(<a href="https://doi.org/10.1016/j.imavis.2023.104689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised skeleton-based action recognition enjoys a rapid growth along with the development of contrastive learning. The existing methods rely on imposing invariance to augmentations of 3D skeleton within a single data stream, which merely leverages the easy positive pairs and limits the ability to explore the complicated movement patterns. In this paper, we advocate that the defect of single-stream contrast and the lack of necessary feature transformation are responsible for easy positives, and therefore propose a Cross-Stream Contrastive Learning framework for skeleton-based action Representation learning (CSCLR). Specifically, the proposed CSCLR not only utilizes intra-stream contrast pairs, but introduces inter-stream contrast pairs as hard samples to formulate a better representation learning. Besides, to further exploit the potential of positive pairs and increase the robustness of self-supervised representation learning, we propose a Positive Feature Transformation (PFT) strategy which adopts feature-level manipulation to increase the variance of positive pairs. To validate the effectiveness of our method, we conduct extensive experiments on three benchmark datasets NTU-RGB + D 60, NTU-RGB + D 120 and PKU-MMD. Experimental results show that our proposed CSCLR exceeds the state-of-the-art methods on a diverse range of evaluation protocols.},
  archive      = {J_ICV},
  author       = {Ding Li and Yongqiang Tang and Zhizhong Zhang and Wensheng Zhang},
  doi          = {10.1016/j.imavis.2023.104689},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104689},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-stream contrastive learning for self-supervised skeleton-based action recognition},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthetic data for face recognition: Current state and
future prospects. <em>ICV</em>, <em>135</em>, 104688. (<a
href="https://doi.org/10.1016/j.imavis.2023.104688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past years, deep learning capabilities and the availability of large-scale training datasets advanced rapidly, leading to breakthroughs in face recognition accuracy. However, these technologies are foreseen to face a major challenge in the next years due to the legal and ethical concerns about using authentic biometric data in AI model training and evaluation along with increasingly utilizing data-hungry state-of-the-art deep learning models. With the recent advances in deep generative models and their success in generating realistic and high-resolution synthetic image data, privacy-friendly synthetic data has been recently proposed as an alternative to privacy-sensitive authentic data to overcome the challenges of using authentic data in face recognition development. This work aims at providing a clear and structured picture of the use-cases taxonomy of synthetic face data in face recognition along with the recent emerging advances of face recognition models developed on the bases of synthetic data. We also discuss the challenges facing the use of synthetic data in face recognition development and several future prospects of synthetic data in the domain of face recognition.},
  archive      = {J_ICV},
  author       = {Fadi Boutros and Vitomir Struc and Julian Fierrez and Naser Damer},
  doi          = {10.1016/j.imavis.2023.104688},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104688},
  shortjournal = {Image Vis. Comput.},
  title        = {Synthetic data for face recognition: Current state and future prospects},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-level feature disentanglement network for
cross-dataset face forgery detection. <em>ICV</em>, <em>135</em>,
104686. (<a href="https://doi.org/10.1016/j.imavis.2023.104686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing videos with forged faces is a fundamental yet important safety-critical task that has caused severe security issues in recent years. Although many existing face forgery detection methods have achieved superior performance on such synthetic videos, they are severely limited by the domain-specific training data and generally perform unsatisfied when transferred to the cross-dataset scenario due to the domain gaps. Based on this observation, in this paper, we propose a multi-level feature disentanglement network to be robust to this domain bias induced by the different types of fake artifacts in different datasets. Specifically, we first detect the face image and transform it into both color-aware and frequency-aware inputs for multi-modal contextual representation learning . Then, we introduce a novel feature disentangling module that mainly utilizes a pair of complementary attention maps, to disentangle the synthetic features into separate realistic features and the features of fake artifacts. Since the features of fake artifacts are indirectly obtained from the latent features instead of the dataset-specific distribution, our forgery detection model is robust to the dataset-specific domain gaps. By applying the disentangling module to multi-levels of the feature extraction network with multi-modal inputs, we can obtain more robust feature representations. In addition, a realistic-aware adversary loss and a domain-aware adversary loss are adopted to facilitate the network for better feature disentanglement and extraction. Extensive experiments on four datasets verify the generalization of our method and present the state-of-the-art performance.},
  archive      = {J_ICV},
  author       = {Zhixiao Fu and Xinyuan Chen and Daizong Liu and Xiaoye Qu and Jianfeng Dong and Xuhong Zhang and Shouling Ji},
  doi          = {10.1016/j.imavis.2023.104686},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {104686},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-level feature disentanglement network for cross-dataset face forgery detection},
  volume       = {135},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised video segmentation for multi-view daily action
recognition. <em>ICV</em>, <em>134</em>, 104687. (<a
href="https://doi.org/10.1016/j.imavis.2023.104687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-layer representations have achieved outstanding performances in the complex daily action recognition. However, the fixed length of the sliding window (SW) leads to the segmented motion atoms incomplete or non-unique. To deal with this problem, we propose unsupervised video segmentation (UVS) for multi-view daily action recognition. Firstly, the average cosine similarity is designed to ensure the integrity of motion atoms. Secondly, we utilize the ordered combination of motion atoms to construct the table of multi-scale motion phrases in a top-down manner, instead of the fixed-scale traditional motion phrases. Finally, the experimental results based on the WVU dataset, the NTU RGB-D 120 dataset, and the N-UCLA dataset show that the proposed UVS method has state-of-the-art performance, compared with the classic methods such as IDT, MoFAP, JLMF, FGCN, and MVMLR.},
  archive      = {J_ICV},
  author       = {Zhigang Liu and Yin Wu and Ziyang Yin and Chunlei Gao},
  doi          = {10.1016/j.imavis.2023.104687},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104687},
  shortjournal = {Image Vis. Comput.},
  title        = {Unsupervised video segmentation for multi-view daily action recognition},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face deidentification with controllable privacy protection.
<em>ICV</em>, <em>134</em>, 104678. (<a
href="https://doi.org/10.1016/j.imavis.2023.104678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy protection has become a crucial concern in today’s digital age. Particularly sensitive here are facial images, which typically not only reveal a person’s identity, but also other sensitive personal information. To address this problem, various face deidentification techniques have been presented in the literature. These techniques try to remove or obscure personal information from facial images while still preserving their usefulness for further analysis. While a considerable amount of work has been proposed on face deidentification, most state-of-the-art solutions still suffer from various drawbacks, and (a) deidentify only a narrow facial area, leaving potentially important contextual information unprotected, (b) modify facial images to such degrees, that image naturalness and facial diversity is suffering in the deidentify images, (c) offer no flexibility in the level of privacy protection ensured, leading to suboptimal deployment in various applications, and (d) often offer an unsatisfactory trade-off between the ability to obscure identity information, quality and naturalness of the deidentified images, and sufficient utility preservation. In this paper, we address these shortcomings with a novel controllable face deidentification technique that balances image quality, identity protection, and data utility for further analysis. The proposed approach utilizes a powerful generative model (StyleGAN2), multiple auxiliary classification models, and carefully designed constraints to guide the deidentification process. The approach is validated across four diverse datasets (CelebA-HQ, RaFD, XM2VTS, AffectNet) and in comparison to 7 state-of-the-art competitors. The results of the experiments demonstrate that the proposed solution leads to: (a) a considerable level of identity protection, (b) valuable preservation of data utility, (c) sufficient diversity among the deidentified faces, and (d) encouraging overall performance.},
  archive      = {J_ICV},
  author       = {Blaž Meden and Manfred Gonzalez-Hernandez and Peter Peer and Vitomir Štruc},
  doi          = {10.1016/j.imavis.2023.104678},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104678},
  shortjournal = {Image Vis. Comput.},
  title        = {Face deidentification with controllable privacy protection},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel facial expression recognition algorithm using
geometry β –skeleton in fusion based on deep CNN. <em>ICV</em>,
<em>134</em>, 104677. (<a
href="https://doi.org/10.1016/j.imavis.2023.104677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) methods based on single-source facial data often suffer from reduced accuracy or unpredictability due to facial occlusion or illumination changes. To address this, a new technique called Fusion-CNN is proposed. It improves accuracy by extracting hybrid features using a β-skeleton undirected graph and an ellipse with parameters trained using a 1D-CNN. In addition, a 2D-CNN is trained on the same image. The outputs from these two subnetworks are fused, and their features are concatenated to create a feature vector for classification in a deep neural network . The proposed method is evaluated on four public face datasets: the extended Cohn-Kanade (CK +) dataset, the Japanese Female Facial Expression (JAFFE) dataset, Karolinska Directed Emotional Faces (KDEF), and Oulu-CASIA. The experimental results show that Fusion-CNN outperforms other algorithms, achieving recognition accuracy of 98.22%, 93.07%, 90.30%, and 90.13% for the CK +, JAFFE, KDEF, and Oulu-CASIA datasets, respectively.},
  archive      = {J_ICV},
  author       = {Abbas Issa Jabbooree and Leyli Mohammad Khanli and Pedram Salehpour and Shahin Pourbahrami},
  doi          = {10.1016/j.imavis.2023.104677},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104677},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel facial expression recognition algorithm using geometry β –skeleton in fusion based on deep CNN},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MVPCC-net: Multi-view based point cloud completion network
for MLS data. <em>ICV</em>, <em>134</em>, 104675. (<a
href="https://doi.org/10.1016/j.imavis.2023.104675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel multi view-based method for completing high-resolution 3D point clouds of partial object shapes obtained by mobile laser scanning (MLS) platforms. Our approach estimates both the geometry and color cues of the missing or incomplete object segments, by projecting the 3D input point cloud by multiple virtual cameras, and performing 2D inpainting in the image domains of the different views. In contrast to existing state-of-the-art methods, our method can generate point clouds consisting of a variable number of points, depending on the detailedness of the input measurement, which property highly facilitates the efficient processing of MLS data with inhomogeneous point density. For training and quantitative evaluation of the proposed method, we provide a new point cloud dataset that consists of both synthetic point clouds of four different street objects with accurate ground truth, and real MLS measurements of partially or fully scanned vehicles. The quantitative and qualitative experiments on the provided dataset demonstrate that our method surpasses state-of-the-art approaches in reconstructing the local fine geometric structures as well as in estimating the overall shape and color pattern of the objects.},
  archive      = {J_ICV},
  author       = {Yahya Ibrahim and Csaba Benedek},
  doi          = {10.1016/j.imavis.2023.104675},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104675},
  shortjournal = {Image Vis. Comput.},
  title        = {MVPCC-net: Multi-view based point cloud completion network for MLS data},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human activity recognition from UAV videos using a novel
DMLC-CNN model. <em>ICV</em>, <em>134</em>, 104674. (<a
href="https://doi.org/10.1016/j.imavis.2023.104674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) remains a challenging issue that requires to be resolved. Utilizing images, smart phones, or sensors, HAR could be done. Recent studies have explored HAR utilizing Unmanned Aerial Vehicle (UAV) videos. But, owing to numerous restrictions correlated to the platform, HAR from videos captured by drones remains a challenge. To resolve this issue, a novel mechanism for HAR from UAV videos utilizing the Diminutive Multi-Dimensional Locality Coding based Convolutional Neural Network (DMLC-CNN) model is proposed. Primarily, the input video is converted into frames; also, Bounding Boxes (BBs) are produced for humans. For attaining the number of clusters grounded on human presence, the BB is given to the Robust Lucrative Sensitive Amalgam K-Nearest Neighbor (RoLSA-KNN) clustering. After that, utilizing the LR-fit-centric Superior Photogrammetric Silhouette (LR-SPS) segmentation, the humans&#39; silhouettes are segmented. The attained output is divided into patches; then, it is converted into stacked layer frames. Afterward, these frames are converted into a 3D kernel. The Diminutive Multi-Dimensional Hyper Graph (DMHG) extracted the features; then, the Locality-constrained Orthonormal Feature Space Coding (LOFSC) Algorithm encoded the extracted features. Lastly, for recognizing Human Activity (HA), DMLC-CNN is wielded. The proposed framework is contrasted with the conventional techniques. After experimental evaluation, the proposed mechanism was found to be more efficient in HAR.},
  archive      = {J_ICV},
  author       = {Kumari Priyanka Sinha and Prabhat Kumar},
  doi          = {10.1016/j.imavis.2023.104674},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104674},
  shortjournal = {Image Vis. Comput.},
  title        = {Human activity recognition from UAV videos using a novel DMLC-CNN model},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stacked graph bone region u-net with bone representation for
hand pose estimation and semi-supervised training. <em>ICV</em>,
<em>134</em>, 104673. (<a
href="https://doi.org/10.1016/j.imavis.2023.104673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D hand estimation from 2D joint information is an essential task in human-machine interaction, which has achieved great progress as an application of deep learning . However, regression-based methods do not perform well because the structural information is not effectively exploited, and the joint coordinates are variable. To address these issues, the hand pose is represented with bone vectors instead of joint coordinates in this study, which are stabler to learn and allow for easier encoding of the hand geometric structure and joint dependency. A novel graph bone region U-Net is specifically designed for bone representation to learn multiscale structural features, where the proposed novel elements (graph convolution, pooling and unpooling) incorporate hand structural knowledge. Under the introduced “finger-to-hand” framework, the network gradually decreases the scale from bone to finger to hand for learning more meaningful multiscale features. Moreover, the unit network is stacked repeatedly to extract multilevel features. Based on the above network, a simple but effective semi-supervised approach is introduced to address the lack of 3D hand pose labels. Many experiments are conducted to evaluate the proposed approach on two challenging datasets. The experimental results show that the proposed supervised approach outperforms the state-of-the-art methods, and the proposed semi-supervised approach can still achieve favorable performance when the labeled data are scarce.},
  archive      = {J_ICV},
  author       = {Zhiwei Zheng and Zhongxu Hu and Hui Qin and Jie Liu},
  doi          = {10.1016/j.imavis.2023.104673},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104673},
  shortjournal = {Image Vis. Comput.},
  title        = {Stacked graph bone region U-net with bone representation for hand pose estimation and semi-supervised training},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Future pedestrian location prediction in first-person videos
for autonomous vehicles and social robots. <em>ICV</em>, <em>134</em>,
104671. (<a href="https://doi.org/10.1016/j.imavis.2023.104671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Future pedestrian trajectory prediction in first-person videos offers great prospects to help autonomous vehicles and social robots to enable better human-vehicle interactions. Given an egocentric video stream, we aim to predict the location and depth (distance between the observed person and the camera) of his/her neighbors in future frames. To locate their future trajectories, we mainly consider three main factors: a) It is necessary to restore the spatial distribution of pedestrians in 2D image to 3D space, i.e., to extract the distance between the pedestrian and the camera which is often neglected. b) It is critical to utilize neighbors’ poses to recognize their intentions. c) It is important to learn human-vehicle interactions from the pedestrian’s historical trajectories. We propose to incorporate these three factors into a multi-channel tensor to represent the main features in real-life 3D space. We then put this tensor into an innovative end-to-end fully convolutional network based on transformer architecture. Experimental results reveal our method outperforms other state-of-the-art methods on public benchmarks MOT15, MOT16 and MOT17. The proposed method will be useful to understand human-vehicle interaction and helpful for pedestrian collision avoidance.},
  archive      = {J_ICV},
  author       = {Kai Chen and Haihua Zhu and Dunbing Tang and Kun Zheng},
  doi          = {10.1016/j.imavis.2023.104671},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104671},
  shortjournal = {Image Vis. Comput.},
  title        = {Future pedestrian location prediction in first-person videos for autonomous vehicles and social robots},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EGRA-NeRF: Edge-guided ray allocation for neural radiance
fields. <em>ICV</em>, <em>134</em>, 104670. (<a
href="https://doi.org/10.1016/j.imavis.2023.104670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Neural Radiance Fields (NeRF) has demonstrated great potential in synthesizing novel views for realistic video generation. However, renderings from NeRF appear excessively blurred and contain aliasing artifacts in some textures or edges. To alleviate this problem, Edge-Guided Ray Allocation (EGRA-NeRF) module is proposed in this paper. Such method proposes a novel ray allocation strategy to focus more rays on the textures and edges of the scene during the training stage. Specially, the Canny edge detector is introduced to generate the ray guidance map. This guidance map can lead the model to find areas with more textures. To focus more rays on edges and textures, a method is explored to re-adjust the ray guidance map and re-allocate rays dynamically. Compared with the ray allocation in most existing NeRF-based methods, EGRA-NeRF focuses rays more on textures and edges, which can achieve the refinement of edges and texture details. Experiments show that NeRF-based algorithms (NeRF and Instant-NGP) with our EGRA module can achieve more realistic video generation, while maintaining almost the same computation time.},
  archive      = {J_ICV},
  author       = {Zhenbiao Gai and Zhenyang Liu and Min Tan and Jiajun Ding and Jun Yu and Mingzhao Tong and Junqing Yuan},
  doi          = {10.1016/j.imavis.2023.104670},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104670},
  shortjournal = {Image Vis. Comput.},
  title        = {EGRA-NeRF: Edge-guided ray allocation for neural radiance fields},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthetic multi-view clustering with missing relationships
and instances. <em>ICV</em>, <em>134</em>, 104669. (<a
href="https://doi.org/10.1016/j.imavis.2023.104669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning not only excavates the supplement information thoroughly, but also avoids noisy features and redundancies to improve down-stream tasks, e.g. Cluster, by constructing the fusing model. However, in practices, it is difficult to obtain complete multi-view data, which is summarized as both of P artially S ample-Missing P roblem (PSP) originating from any view and P artially V iew-unaligned P roblem (PVP). SURE (robust multi-view clustering with incomplete information) builds up the novel loss function to remove the noisy samples in the embedding space and presents the aligning method to solve both of PVP and PSP. However, generated instances and relationships contain relatively little discriminate information and semantic consistency , moreover, it needs a large size of complete samples to pre-train. Aiming to above problems, we propose the novel framework (termed Synthetic Multi-view Data Clustering , SMDC). For PSP problem, we induce feature-distillation in each feature-map level to generate features containing more semantic information helping with the guidance of complete feature-maps, furthermore, exploits self-distillation to strengthen the discriminability information of deeper levels supervised from the contrastive constraint in the embedding space. Moreover, for PVP problem, we leverage attention mechanism to adaptively fuse multi-view features and auto-align instances located in different views. The proposed model is verified in images and video clustering tasks , respectively. Comparing with popular methods achieve satisfactory performances. The code of model is published in: https://github.com/ LNNU-computer-research-526/SMDC.},
  archive      = {J_ICV},
  author       = {Jing Zhang and Guiyan Wei and Fang Sun},
  doi          = {10.1016/j.imavis.2023.104669},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104669},
  shortjournal = {Image Vis. Comput.},
  title        = {Synthetic multi-view clustering with missing relationships and instances},
  volume       = {134},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal emotion recognition using cross modal audio-video
fusion with attention and deep metric learning. <em>ICV</em>,
<em>133</em>, 104676. (<a
href="https://doi.org/10.1016/j.imavis.2023.104676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, the multi-modal emotion recognition has become an important research issue in the affective computing community due to its wide range of applications that include mental disease diagnosis, human behavior understanding, human machine/robot interaction or autonomous driving systems. In this paper, we introduce a novel end-to-end multimodal emotion recognition methodology, based on audio and visual fusion designed to leverage the mutually complementary nature of features while maintaining the modality-specific information. The proposed method integrates spatial, channel and temporal attention mechanisms into a visual 3D convolutional neural network (3D-CNN) and temporal attention into an audio 2D convolutional neural network (2D-CNN) to capture the intra-modal features characteristics. Further, the inter-modal information is captured with the help of an audio-video (A-V) cross-attention fusion technique that effectively identifies salient relationships across the two modalities. Finally, by considering the semantic relations between the emotion categories, we design a novel classification loss based on an emotional metric constraint that guides the attention generation mechanisms. We demonstrate that by exploiting the relations between the emotion categories our method yields more discriminative embeddings, with more compact intra-class representations and increased inter-class separability. The experimental evaluation carried out on the RAVDESS ( The Ryerson Audio-Visual Database of Emotional Speech and Song ), and CREMA-D ( Crowd-sourced Emotional Multimodal Actors Dataset ) datasets validates the proposed methodology, which leads to average accuracy scores of 89.25% and 84.57%, respectively. In addition, when compared to state-of-the-art techniques, the proposed solution shows superior performances, with gains in accuracy ranging in the [1.72%, 11.25%] interval.},
  archive      = {J_ICV},
  author       = {Bogdan Mocanu and Ruxandra Tapu and Titus Zaharia},
  doi          = {10.1016/j.imavis.2023.104676},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104676},
  shortjournal = {Image Vis. Comput.},
  title        = {Multimodal emotion recognition using cross modal audio-video fusion with attention and deep metric learning},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting spatial and temporal context for online tracking
with improved transformer. <em>ICV</em>, <em>133</em>, 104672. (<a
href="https://doi.org/10.1016/j.imavis.2023.104672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the transformer is becoming more and more popular in computer vision tasks due to its ability to capture long-range dependencies via self-attention. In this paper, we propose a transformer-based classification regression network TrCAR utilizing the transformer to exploit deeper spatial and temporal context. Different from the classic architecture of the transformer, we introduce convolution operation into the transformer and change the calculation of features to make it suitable for the tracking task. After that, the improved transformer encoder is introduced into the regression branch of TrCAR and combined with the feature pyramid to complete multi-layer feature fusion, which is conducive to obtaining a high-quality target representation. To further enable the target model to adapt to the change of the target appearance, we bring the gradient descent to the regression branch so that it can be updated online to produce a more precise bounding box. Meanwhile, the new transformer is integrated into the classification branch of TrCAR, which as much as possible extracts the essential feature of the target across historical frames via the global computing capability, and uses it to emphasize the target position of the current frame via cross-attention. Which helps the classifier to more easily identify the correct target. Experimental results on OTB, LaSOT, VOT2018, NFS, GOT-10k, and TrackingNet benchmarks show that our TrCAR achieves comparable performance to the popular trackers.},
  archive      = {J_ICV},
  author       = {Jianwei Zhang and Jingchao Wang and Huanlong Zhang and Mengen Miao and Jie Zhang and Di Wu},
  doi          = {10.1016/j.imavis.2023.104672},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104672},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploiting spatial and temporal context for online tracking with improved transformer},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimized deep supervised hashing model for fast image
retrieval. <em>ICV</em>, <em>133</em>, 104668. (<a
href="https://doi.org/10.1016/j.imavis.2023.104668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multimedia data grows exponentially, searching for and retrieving a relevant image is becoming a challenge for researchers. Hashing is a widely adopted method because of its high performance in image retrieval with deep neural networks and multiple convolutional layers . Even so, most hashing methods ignore the computational cost and memory storage consumption. When the deep hashing model size is large, it leads to a slowdown in the response time of the model compared to the small model. Addressing these issues, a novel optimized deep supervised hashing based on a teacher-student approach for swift and precise image retrieval is proposed in this paper. In this work, the small student model is trained using the knowledge distillation from the large teacher model and the information from the one-hot labels. Therefore, a weight allocation loss function based on the teacher and student models is defined. Meanwhile, we apply model pruning to decrease the amount of the student model further to increase the response time. Therefore, knowledge distillation is performed on the pruned model. After that, the remaining weights are quantized to reach the smaller size of the model. Extensive experimental outcomes on two widely used datasets prove the outstanding efficiency of our proposed method.},
  archive      = {J_ICV},
  author       = {Abid Hussain and Heng-Chao Li and Danish Ali and Muqadar Ali and Fakhar Abbas and Mehboob Hussain},
  doi          = {10.1016/j.imavis.2023.104668},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104668},
  shortjournal = {Image Vis. Comput.},
  title        = {An optimized deep supervised hashing model for fast image retrieval},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An automated hyperparameter tuned deep learning model
enabled facial emotion recognition for autonomous vehicle drivers.
<em>ICV</em>, <em>133</em>, 104659. (<a
href="https://doi.org/10.1016/j.imavis.2023.104659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progress of autonomous driving cars is a difficult movement that causes problems regarding safety, ethics, social acceptance, and cybersecurity. Currently, the automotive industry is utilizing these technologies to assist drivers with advanced driver assistance systems . This system helps different functions to careful driving and predict drivers&#39; ability of stable driving behavior and road safety. A great number of researches have shown that the driver&#39;s emotion is the major factor that handles the emotions, resulting in serious vehicle collisions. As a result, continuous monitoring of drivers&#39; behavior could assist to evaluate their behavior to prevent accidents. The study proposes a new Squirrel Search Optimization with Deep Learning Enabled Facial Emotion Recognition (SSO-DLFER) technique for Autonomous Vehicle Drivers. The proposed SSO-DLFER technique focuses mainly on the identification of driver facial emotions in the AVs. The proposed SSO-DLFER technique follows two major processes namely face detection and emotion recognition. The RetinaNet model is employed at the initial phase of the face detection process. For emotion recognition, the SSO-DLFER technique applied the Neural Architectural Search (NASNet) Large feature extractor with a gated recurrent unit (GRU) model as a classifier. For improving the emotion recognition performance, the SSO-based hyperparameter tuning procedure is performed. The simulation analysis of the SSO-DLFER technique is tested under benchmark datasets and the experimental outcome was investigated under various aspects. The comparative analysis reported the enhanced performance of the SSO-DLFER algorithm on recent approaches.},
  archive      = {J_ICV},
  author       = {Deepak Kumar Jain and Ashit Kumar Dutta and Elena Verdú and Shtwai Alsubai and Abdul Rahaman Wahab Sait},
  doi          = {10.1016/j.imavis.2023.104659},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104659},
  shortjournal = {Image Vis. Comput.},
  title        = {An automated hyperparameter tuned deep learning model enabled facial emotion recognition for autonomous vehicle drivers},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face mask detection using deep convolutional neural network
and multi-stage image processing. <em>ICV</em>, <em>133</em>, 104657.
(<a href="https://doi.org/10.1016/j.imavis.2023.104657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face mask detection has several applications including real-time surveillance, biometrics, etc. Face mask detection is also useful for surveillance of the public to ensure face mask wearing in public places. Ensuring that people are wearing a face mask is not possible with monitoring staff; instead, automatic systems are a much better choice for face mask detection and monitoring to help manage public behaviour and contribute to restricting the outbreak of COVID-19. Despite the availability of several such systems, the lack of a real image dataset is a big hurdle to validating state-of-the-art face mask detection systems. In addition, using the simulated datasets lack the analysis needed for real-world scenarios. This study builds a new dataset namely RILFD by taking real pictures using a camera and annotating them with two labels (with mask, without mask) which are publicly available for future research. In addition, this study investigates various machine learning models and off-the-shelf deep learning models YOLOv3 and Faster R-CNN for the detection of face masks. The customized CNN models in combination with the 4 steps of image processing are proposed for face mask detection. The proposed approach outperforms other models and proved its robustness with a 97.5% of accuracy score in face mask detection on the RILFD dataset and two publicly available datasets (MAFA and MOXA).},
  archive      = {J_ICV},
  author       = {Muhammad Umer and Saima Sadiq and Reemah M. Alhebshi and Shtwai Alsubai and Abdullah Al Hejaili and Ala’ Abdulmajid Eshmawi and Michele Nappi and Imran Ashraf},
  doi          = {10.1016/j.imavis.2023.104657},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104657},
  shortjournal = {Image Vis. Comput.},
  title        = {Face mask detection using deep convolutional neural network and multi-stage image processing},
  volume       = {133},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Language and vision based person re-identification for
surveillance systems using deep learning with LIP layers. <em>ICV</em>,
<em>132</em>, 104658. (<a
href="https://doi.org/10.1016/j.imavis.2023.104658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time surveillance systems have become a necessity of today&#39;s life owing to their relevance in the contemporary era for security reasons to ensure a secure and safe environment. Presently, Person re-identification (Re-ID)-based surveillance systems are becoming increasingly more prevalent and sophisticated since they do not require human intervention and are more reliable to deploy in public spaces leveraging multi-camera networks. However, one of the major problems in Person ReID is the visual appearance i-e the appearance of a person in an image is greatly affected by different camera views. As a result, the discriminative set of features must be learned in a deep learning model in order to re-identify persons from opposing camera viewpoints. To address this challenge, we propose an image/text-retrieval-based Person ReId method in which both visual and text-based features are exploited to carry out person re-identification. More precisely, the textual descriptions of the images are taken into account as text features with Glove Word Embedding followed by 1D-MAPCNN and fused with image-level features extracted using the GoogLeNet model. In addition, the feature discriminability is enhanced using local importance-based pooling (LIP) layers in which adaptive significance weights are learned during downsampling. Moreover, from two different modalities, feature refinement is done during training with the help of attention mechanisms using the Convolutional Block Attention module (CBAM) and the proposed shared attention neural network . It is observed that LIP layers along with both vision and textual features are playing a key role in acquiring discriminative features even if the visual appearance of the same person is greatly affected due to camera pose conditions. The proposed method is validated on the CUHK-PADES dataset and has 15.34% and 24.39% rank-1 improvement in text and image-based retrievals.},
  archive      = {J_ICV},
  author       = {Maryam Bukhari and Sadaf Yasmin and Sheneela Naz and Muazzam Maqsood and Jehyeok Rew and Seungmin Rho},
  doi          = {10.1016/j.imavis.2023.104658},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104658},
  shortjournal = {Image Vis. Comput.},
  title        = {Language and vision based person re-identification for surveillance systems using deep learning with LIP layers},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time 3D human pose estimation without skeletal a priori
structures. <em>ICV</em>, <em>132</em>, 104649. (<a
href="https://doi.org/10.1016/j.imavis.2023.104649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is about real-time 2D-3D human pose estimation without using the a priori structure of the skeleton and with a low number of parameters for regression tasks. Current graph convolution-based 3D human pose tasks require structural knowledge of the skeleton, which limits the exploration of pose estimation for unknown structures. Inspired by tyre rotation and circular convolution , weights rotation is used to fully learn the potential connections between the human joints. We refer to this process as the weight cyclic sharing mechanism, a novel method for updating features. It does not require knowledge of the structure of the human skeleton and learns different constraints between joints with a low number of parameters. We propose an end-to-end weight circular sharing network (WCirSNet) based on the weight circular sharing mechanism. We propose a simple and efficient weighted residual block in this WCirSNet. The superiorities of the weight circular sharing mechanism and weighted residual block were verified by abundant ablation studies. Extensive evaluations on two challenging benchmark datasets (Human 3.6 M, MPI-INF-3DHP) show that the performance and generalization capabilities of our framework are superior to the results of many previously advanced methods.},
  archive      = {J_ICV},
  author       = {Guihu Bai and Yanmin Luo and Xueliang Pan and Jia Wang and Jing-Ming Guo},
  doi          = {10.1016/j.imavis.2023.104649},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104649},
  shortjournal = {Image Vis. Comput.},
  title        = {Real-time 3D human pose estimation without skeletal a priori structures},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Region selection for occluded person re-identification via
policy gradient. <em>ICV</em>, <em>132</em>, 104648. (<a
href="https://doi.org/10.1016/j.imavis.2023.104648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-id) has attracted lots of attention in the past few years. However, most of work in this area focus on the re-identification of the holistic pedestrian images. In the real application scenarios, the task of re-identification is usually influenced by the occlusion problems. Most of previous work tackle occlusion problem through pose estimation, human parsing or manually labelling occlusion objects. Such reliance on additional annotation information severely limits the generalization ability on practical usage. To address this problem, we propose a novel region selection learning strategy based on the policy gradient to remove irrelevant parts without using any extra information outside dataset. A transformer-based feature extractor is also constructed to learn discriminate features with self-attention mechanism. We evaluate the performance of the proposed method on three occluded re-identification datasets. The experiments show that we achieve 87%, 85.2% and 68.3% in Rank-1 accuracy on Occluded-REID, Partial-REID and Occluded-Duke datasets respectively.},
  archive      = {J_ICV},
  author       = {Bolei Xu},
  doi          = {10.1016/j.imavis.2023.104648},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104648},
  shortjournal = {Image Vis. Comput.},
  title        = {Region selection for occluded person re-identification via policy gradient},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical interaction and pooling network for co-salient
object detection. <em>ICV</em>, <em>132</em>, 104647. (<a
href="https://doi.org/10.1016/j.imavis.2023.104647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-salient object detection (CoSOD) aims to detect common and salient objects across the given image group. Due to the particularity of CoSOD, images in the given group are processed synergistically to excavate the relevance between them. Inspired by the tracking methods, previous works tend to utilize pixel-wise correspondences to measure the relevance. However, because of the complexity of the image groups, the obtained feature maps could be easily affected by the common interference. Moreover, current works tend to utilize classification labels to ensure intra-group coherence and inter-group separability, which may cause some overfitting problems. In this paper, we propose the hierarchical interaction and pooling network to alleviate the above problems. We first design a pyramid pooling interaction module and perform convolution with dimension permutation, making full use of convolution and multi-receptive field information. We further propose the coherence confirmation module along with the four-branch architecture. Without the classification labels, the module achieves comparable or even better performance. Extensive experiments demonstrate that the proposed method can detect common and salient objects more accurately and achieves the new state-of-the-art.},
  archive      = {J_ICV},
  author       = {Yu Wang and Shuxiao Li},
  doi          = {10.1016/j.imavis.2023.104647},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104647},
  shortjournal = {Image Vis. Comput.},
  title        = {Hierarchical interaction and pooling network for co-salient object detection},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Occluded thermal face recognition using BoCNN and radial
derivative gaussian feature descriptor. <em>ICV</em>, <em>132</em>,
104646. (<a href="https://doi.org/10.1016/j.imavis.2023.104646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a Radial derivative Gaussian feature ( RDGF ) descriptor, a novel handcrafted feature descriptor for disguised thermal face recognition. The feature encoding has been done so that the performance is least affected by noise and works well over challenging datasets. We propose a cascaded framework that combines two modules, namely Bo CNN and the RDGF descriptor. The cascading architecture estimates the performance of Bo CNN before classification. It also uses a dynamic classifier selector in run time to choose between handcrafted features and the CNN framework to enhance the overall performance. We also propose a thermal face dataset with partial occlusion . We have compared the performance of the RDGF descriptor with state-of-the-art descriptors on the IIIT-Delhi disguised thermal face dataset and our proposed dataset. RDGF exhibits better performance compared to other state-of-the-art descriptors. Our proposed descriptor shows relative increment of 56.84%, 64.92%, 67.25%, 64.03%, 48.06%, and 7.28% on IIIT-D Occluded Thermal Dataset when compared with LBP , LDP , LBDP , LVP , LGHP , and HOG , respectively. A similar enhancement of accuracy has been observed on our proposed dataset as well. An exhaustive comparison based on the performance of the cascaded framework with state-of-the-art CNN models has also been done in a similar fashion.},
  archive      = {J_ICV},
  author       = {Sumit Kumar and Satish Kumar Singh and Peter Peer},
  doi          = {10.1016/j.imavis.2023.104646},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104646},
  shortjournal = {Image Vis. Comput.},
  title        = {Occluded thermal face recognition using BoCNN and radial derivative gaussian feature descriptor},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual and textual explainability for a biometric
verification system based on piecewise facial attribute analysis.
<em>ICV</em>, <em>132</em>, 104645. (<a
href="https://doi.org/10.1016/j.imavis.2023.104645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decisions behind the mechanics of a biometric verification system based on Machine Learning (ML) are difficult to comprehend. Although there is now well-established research in various fields of application, such as health or justice, the use of ML-based methods is accompanied by a lack of confidence that results in their limited use. The explainability of a ML system and the comprehension of what lies behind its prediction is one of the numerous characteristics that define “trust” in these systems. Over the years, face-based biometric authentication has been the subject of extensive research in both academia and industry. However, existing biometric authentication systems still have problems regarding accuracy, robustness and, explainability. Still lacking in the literature is a comprehensive examination of the use of post-hoc explainability techniques for such systems. Cognitive neuroscience has always been interested in the method by which people perceive faces; local elements such as the nose, eyes, and mouth are critical to the perception and recognition of a face. In this work, starting from this assumption, we propose a framework of visual and textual explainability based on the parts of a face by analyzing them with respect to the facial attributes reported in the CelebA dataset. The primary objective is to be able to explain why two pictures of different subjects are distinct. This is done by sinthesizing pairs of images that illustrate how dissimilar the various parts of the face under investigation are and incisive and direct textual explanations of the distinguishing features are generated. A further study analyzes an interpretable mapping between the semantic space of the text and the space of the image.},
  archive      = {J_ICV},
  author       = {Lucia Cascone and Chiara Pero and Hugo Proença},
  doi          = {10.1016/j.imavis.2023.104645},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104645},
  shortjournal = {Image Vis. Comput.},
  title        = {Visual and textual explainability for a biometric verification system based on piecewise facial attribute analysis},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An investigational FW-MPM-LSTM approach for face recognition
using defective data. <em>ICV</em>, <em>132</em>, 104644. (<a
href="https://doi.org/10.1016/j.imavis.2023.104644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial recognition systems are based on the features and traits of the face, since the systems are classified as biometric systems. Additionally, they are founded on the image processing , machine vision and machine learning principles. From images, imperfect information is considered by face recognition systems. A variety of image reconstruction mechanisms is vital in this situation in order to match faces. The proposed method calls for image enhancement at the pre-processing stage. Following the image segmentation and reconstruction stage , the best facial features are extracted using features such the eyes, cheeks, face area and lips. By means of fractal model and wavelet transform the operation is performed. Using the Moore Penrose Matrix, the LSTM neural network is then improved also known as the MPM-LSTM, to train and test the system. From experimental results, the outcomes show that the proposed methodology performs better than the contemporary techniques.},
  archive      = {J_ICV},
  author       = {Baraa Adil Mahmood and Sefer Kurnaz},
  doi          = {10.1016/j.imavis.2023.104644},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104644},
  shortjournal = {Image Vis. Comput.},
  title        = {An investigational FW-MPM-LSTM approach for face recognition using defective data},
  volume       = {132},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-branch adaptive attention transformer for occluded
person re-identification. <em>ICV</em>, <em>131</em>, 104633. (<a
href="https://doi.org/10.1016/j.imavis.2023.104633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded person re-identification is still a common and challenging task because people are often occluded by some obstacles (e.g. cars and trees) in the real world. In order to locate the unoccluded parts and extract local fine-grained features of the occluded human body, State-of-the-Art (SOTA) methods usually use a pose estimation model, which usually causes additional bias and this two-stage architecture also complicates the model. To solve this problem, an end-to-end dual-branch Transformer network for occluded person re-identification is designed. Specifically, one of the branches is the transformer-based global branch, which is responsible for extracting global features, while in the other local branch, we design the Selective Token Attention (STA) module. STA can utilize the multi-headed self-attention mechanism to select discriminating tokens for effectively extracting the local features. Further, in order to alleviate the inconsistency between Softmax Loss and Triplet Loss convergence goals, Circle Loss is introduced to design the Goal Consistency Loss (GC Loss) to supervise the network. Experiments on four challenging datasets for Re-ID tasks (including occluded person Re-ID and holistic person Re-ID) illustrate that our method can achieve SOTA performance.},
  archive      = {J_ICV},
  author       = {Yunhua Lu and Mingzi Jiang and Zhi Liu and Xinyu Mu},
  doi          = {10.1016/j.imavis.2023.104633},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104633},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual-branch adaptive attention transformer for occluded person re-identification},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative segment-pose representation based augmentation
(GSRA) for unsupervised person re-identification. <em>ICV</em>,
<em>131</em>, 104632. (<a
href="https://doi.org/10.1016/j.imavis.2023.104632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification matches the images of a person captured in multiple cameras in a smart surveillance environment. The process of matching the images captured from multiple viewing angles is challenging due to the variations caused by illumination, occlusion, dynamic pose change, etc., To tackle such challenges, large number of samples are required to identify the unique features of a person. In real-world crowded surveillance environment, it is highly difficult to capture the sufficient number of images to build a deep model. This scarcity in samples can be resolved by generating images using generative networks. The existing literature lacks robust discriminators and validation techniques to validate the generative network in an unsupervised person re-identification setup. Thus, we propose an unsupervised adversarial segment-pose distance threshold representation to validate the generated images in addition to the conventional discriminator. The images are generated and cross-validated with the determined segment-pose distance threshold. Labelling process is performed by matching the unoccluded segment with its appropriate ground truth parent cluster based on the segment-pose distance threshold. We have performed experiments on the benchmark person re-ID datasets like DukeMTMC re-ID, Market1501, CUHK03 and MSMT17. The effectiveness of the proposed unsupervised generative model is proved by reporting a +2.6% highest ranking accuracy over the state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Sridhar Raj S and Munaga V.N.K. Prasad and Ramadoss Balakrishnan},
  doi          = {10.1016/j.imavis.2023.104632},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104632},
  shortjournal = {Image Vis. Comput.},
  title        = {Generative segment-pose representation based augmentation (GSRA) for unsupervised person re-identification},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified RGB-t crowd counting learning framework.
<em>ICV</em>, <em>131</em>, 104631. (<a
href="https://doi.org/10.1016/j.imavis.2023.104631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel Unified RGB-T Crowd Counting Learning Framework (UCCF) is proposed, which utilizes an image fusion network architecture and a crowd counting network architecture to estimate the density map and count results simultaneously. Since there are few deep learning methods for the RGB-T crowd counting task, current research lacks unified learning frameworks to solve the image fusion and crowd counting problems synchronously. To fill this gap, our framework aims to fuse two modalities, visible and thermal infrared images, that exploit the complementary information to accurately count the dense population and construct the end-to-end training process. To this end, we first propose the unified RGB-T crowd counting learning framework to complete the image fusion and crowd counting tasks simultaneously by redesigning the unified training loss function. Also, to further narrow the gap between the two models and simplify the end-to-end training process, we design the Assisted Learning Module (ALM) by merging the density map feature into the image fusion encoding process. Meanwhile, we propose an Extensive Context Extraction Module (ECEM) and apply the Multi-domain Attention Block (MAB) to further improve the counting accuracy. The experimental results show that our method outperforms all single-modal input methods (26.9 % % improvements over the best RGB-input approaches and 3 % % improvements over the best thermal infrared input methods for MAE) and is at the forefront of multi-modal input methods, which demonstrates the robustness and effectiveness of our framework.},
  archive      = {J_ICV},
  author       = {Siqi Gu and Zhichao Lian},
  doi          = {10.1016/j.imavis.2023.104631},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104631},
  shortjournal = {Image Vis. Comput.},
  title        = {A unified RGB-T crowd counting learning framework},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decoupled contributed attribute-object composition
detection. <em>ICV</em>, <em>131</em>, 104630. (<a
href="https://doi.org/10.1016/j.imavis.2023.104630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition is quite a well known task in computer vision . Objects are often associated with attributes. It becomes further challenging to correctly classify the object and associated attribute as a composition. Most of the methods for attribute-object pair detection involve discriminating approach that detects the attribute and object separately. Such approaches fail to consider some important facts regarding the composition viz. appearance of attributes is dependent on object and that of an object changes with the attribute. Making use of this interdependence, we propose a model, ContribNet to learn attribute-object composition representation. The model uses the semantic linguistic features to learn robust visual composition while highlighting the importance of component features in identifying its counterpart of the composition. The factors responsible for model performance are also discussed.},
  archive      = {J_ICV},
  author       = {Charulata Patil and Aditya Abhyankar},
  doi          = {10.1016/j.imavis.2023.104630},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104630},
  shortjournal = {Image Vis. Comput.},
  title        = {Decoupled contributed attribute-object composition detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial–temporal graph attention network for video anomaly
detection. <em>ICV</em>, <em>131</em>, 104629. (<a
href="https://doi.org/10.1016/j.imavis.2023.104629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection , which is weakly supervised by video-level annotations, is a frequent yet challenging task in computer vision owing to its unexpectedness, equivocality, rarity, irregularity, and diversity. Although previous seminal works successfully leveraged graph convolutions to assist in the detection of anomalies, they failed to subsequently explore the potential of this approach. In this study, we developed a spatial–temporal graph attention network (STGA), which, to the best of our knowledge, is the first effort to combine graph convolutions with a multi-head graph attention mechanism for video anomaly detection. Specifically, a spatial correlation graph and temporal dependence graph were devised to learn distinguishable representations with the complementation of graph attention techniques. Furthermore, STGA was incorporated within the multiple instance learning (MIL) pipeline, and optimized via top- k MIL ranking loss after a sparse continuous sampling strategy was put into effect. We conducted experiments using four multi-scale datasets to validate the efficacy of our model. Quantitatively, our method performs equivalently to the current best result on ShanghaiTech with a frame-level area under the curve (AUC) of 97.21%, obtains the second-best result on UCSD-Ped2 with a frame-level AUC of 97.4%, gains a frame-level AUC of 80.28% on UCF-Crime , and achieves a new state-of-the-art performance on TAD with a frame-level AUC of 91.42%. Additionally, our false alarm rate results outperform those obtained in previous studies on ShanghaiTech and UCF-Crime , which demonstrates the robustness of our approach. All relevant code has been made available at https://github.com/hychen96/STGA-VAD .},
  archive      = {J_ICV},
  author       = {Haoyang Chen and Xue Mei and Zhiyuan Ma and Xinhong Wu and Yachuan Wei},
  doi          = {10.1016/j.imavis.2023.104629},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104629},
  shortjournal = {Image Vis. Comput.},
  title        = {Spatial–temporal graph attention network for video anomaly detection},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent multimodal pedestrian detection using hybrid
metaheuristic optimization with deep learning model. <em>ICV</em>,
<em>131</em>, 104628. (<a
href="https://doi.org/10.1016/j.imavis.2023.104628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For video surveillance, pedestrian detection assists in providing baseline data for crowd monitoring, people counting, and event detection; for smart transport system, pedestrian detection acts as a vital part in the semantic understanding of the environment. Pedestrian detection is frequently confronted with substantial intra-class variability because human tends to have great variation in human appearance and pose. Currently, the emergence of deep learning (DL) model has received considerable attention in computer vision techniques like object detection and object classification, and this application is based on supervised learning which required labels. Convolution neural networks (CNN) have assisted substantial improvement in pedestrian recognition due to the stronger representative capability of the CNN feature. But it is usually hard to decrease false positives on hard negative samples namely poles, tree leaves, traffic lights, and so on. Therefore, this study develops an intelligent multimodal pedestrian detection and classification using hybrid metaheuristic optimization with deep learning (IPDC-HMODL) algorithm. The major aim of the presented IPDC-HMODL model is the recognition and classification of multiple pedestrians exist in the input frames. It follows a three stage process namely multimodal object detection, pedestrian classification, and parameter tuning. At the initial stage, the IPDC-HMODL model uses multimodal object detector using YOLO-v5 and RetinaNet model. In addition, the IPDC-HMODL model applies kernel extreme learning machine (KELM) algorithm for pedestrian classification. Finally, hybrid salp swarm optimization (HSSO) model is used for optimal parameter adjustment. To depict the improvised outcomes of the IPDC-HMODL technique, a wide spread simulation analysis was conducted. The comparison study highlighted the enhanced outcomes of the IPDC-HMODL model over other approaches on multimodal pedestrian detection.},
  archive      = {J_ICV},
  author       = {Johnson Kolluri and Ranjita Das},
  doi          = {10.1016/j.imavis.2023.104628},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104628},
  shortjournal = {Image Vis. Comput.},
  title        = {Intelligent multimodal pedestrian detection using hybrid metaheuristic optimization with deep learning model},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional generative data-free knowledge distillation.
<em>ICV</em>, <em>131</em>, 104627. (<a
href="https://doi.org/10.1016/j.imavis.2023.104627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation has made remarkable achievements in model compression . However, most existing methods require the original training data , which is usually unavailable due to privacy and security issues. This paper proposes a conditional generative data-free knowledge distillation (CGDD) framework for training lightweight networks without real data. This framework realizes efficient knowledge distillation based on conditional image generation . Specifically, we treat the preset labels as ground truth to train a semi-supervised conditional generator. The trained generator can produce specified classes of training images. During training, we force the student model to extract the hidden knowledge in teacher feature maps, which provide crucial cues to the learning process. Meanwhile, we construct an adversarial training framework to promote distillation performance. The framework will help the student model to explore larger data space. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on different datasets. Compared with other data-free works, our method obtains state-of-the-art results on CIFAR100, Caltech101, and different versions of ImageNet datasets. The codes will be released.},
  archive      = {J_ICV},
  author       = {Xinyi Yu and Ling Yan and Yang Yang and Libo Zhou and Linlin Ou},
  doi          = {10.1016/j.imavis.2023.104627},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104627},
  shortjournal = {Image Vis. Comput.},
  title        = {Conditional generative data-free knowledge distillation},
  volume       = {131},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Faster and finer pose estimation for multiple instance
objects in a single RGB image. <em>ICV</em>, <em>130</em>, 104618. (<a
href="https://doi.org/10.1016/j.imavis.2022.104618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the 6 degrees of freedom (6DoF) object pose parameters for multiple instance objects with high accuracy and less time complexity is a challenging issue in robotics and computer vision . Many bottom-up approaches have been proposed for rapid multiple pose estimation but these are much less accurate than top-down approaches. Moreover, most post-processing is affected from more detected objects because non-maximum suppression is used. This study uses a bottom-up approach that is faster and more precise to estimate poses for multiple instance objects in a single RGB image . This method can overcome the occlusion/overlapping of instances in the same/different object category. Informative features and techniques including a semantic segmentation map , multi-keypoint vector fields, a 3D coordinate map, and diagonal graph clustering post-processing are proposed to segment the entire mask into instance masks at once and then to estimate the corresponding poses. The experimental results and ablation studies show that the proposed system features competitive accuracy at a speed of 24.7 frames per second for more than 7 objects using the Occlusion LINEMOD dataset.},
  archive      = {J_ICV},
  author       = {Lee Aing and Wen-Nung Lie and Guo-Shiang Lin},
  doi          = {10.1016/j.imavis.2022.104618},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104618},
  shortjournal = {Image Vis. Comput.},
  title        = {Faster and finer pose estimation for multiple instance objects in a single RGB image},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human object interaction detection: Design and survey.
<em>ICV</em>, <em>130</em>, 104617. (<a
href="https://doi.org/10.1016/j.imavis.2022.104617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) detection is the process of estimating the interaction between a human and an object in an image. The first attempts at HOI involved two-stage detection methods, then improved to one-stage, and finally end-to-end methods in the last year with the development of transformers. The objective of this paper is to first provide a guideline to researchers seeking to design a human object interaction detection model in addition to describing the benchmark datasets and evaluation metrics used in this task. Second, it presents a survey about the different existing methods for HOI detection and critically analyzes their design decisions through every step. Finally, we report the characteristics of future research directions and present some open issues on human-object interaction detection by presenting the limitations facing HOI detection.},
  archive      = {J_ICV},
  author       = {Maya Antoun and Daniel Asmar},
  doi          = {10.1016/j.imavis.2022.104617},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104617},
  shortjournal = {Image Vis. Comput.},
  title        = {Human object interaction detection: Design and survey},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An end-to-end convolutional network for estimating the
essential matrix. <em>ICV</em>, <em>130</em>, 104616. (<a
href="https://doi.org/10.1016/j.imavis.2022.104616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Essential matrix (E-matrix) estimation is a crucial aspect of pose estimation. In this study, we developed an end-to-end method (E-net) to estimate the E-matrix without correspondences. A pair of the corresponding images was placed in the twin transformer architecture to simultaneously extract the features. We developed a feature matching module for matching the extracted features based on their commonalities. To avoid excessive network parameters, matched features with their weights obtained by multilayer perceptron were transmitted to the flatten layer, where the Max-Pooling was used to eliminate their useless portions. We further constructed three self-defined layers to ensure that E-matrix is rank-2 with 5 degrees of freedom using reserved helpful features. Besides, we presented two self-defined loss functions (Loss 1 and Loss 2 ) to train the E-net and improve the estimated E-matrix&#39;s accuracy. E-net&#39;s performance was evaluated on the KITTI and TUM SLAM datasets using two self-defined metrics, M 1 (mean value of matching error) and M 2 (mean squared value of matching error). The E-net achieved M 1 0.107 and M 2 0.091 on the KITTI dataset and M 1 0.253 and M 2 0.144 on the TUM SLAM dataset. The results demonstrated that the E-net trained with self-defined loss functions outperforms other algorithms when compared to the 5-point algorithm of M 1 10.411 and M 2 8.332.},
  archive      = {J_ICV},
  author       = {Ruiqi Yang and Junhua Zhang and Bo Li},
  doi          = {10.1016/j.imavis.2022.104616},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104616},
  shortjournal = {Image Vis. Comput.},
  title        = {An end-to-end convolutional network for estimating the essential matrix},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object detection via inner-inter relational reasoning
network. <em>ICV</em>, <em>130</em>, 104615. (<a
href="https://doi.org/10.1016/j.imavis.2022.104615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting relationships between objects and (or) labels under graph message passing mechanism to facilitate object detection has been widely investigated in recent years. However, these methods heavily rely on hand-crafted graph structures, which may introduce unreliable relationships and in turn hurt the object detection performance. Aiming to address this issue, we propose a novel object detection framework that fully explores the relational representations for objects and labels under a full attention architecture. Specifically, we directly regard the extracted proposals and candidate labels as two independent sets in visual feature space and label embedding space, respectively. And we design a self-attention module to discover the inner-relationships within the visual feature space or label embedding space. In addition, a cross-attention module is developed to explore the inter-relationships between the two spaces. Furthermore, both the inner-relationships and inter-relationships are utilized to enhance the object features and label embedding representations to facilitate the object detection. To validate the proposed framework in improving object detection performance, we embed it into several state-of-the-art baselines and perform extensive experiments on two public benchmarks (named Pascal VOC and COCO 2017). The experimental results demonstrate the effectiveness and flexibility of the proposed framework.},
  archive      = {J_ICV},
  author       = {He Liu and Xiuting You and Tao Wang and Yidong Li},
  doi          = {10.1016/j.imavis.2022.104615},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104615},
  shortjournal = {Image Vis. Comput.},
  title        = {Object detection via inner-inter relational reasoning network},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature enhancement network for stereo matching.
<em>ICV</em>, <em>130</em>, 104614. (<a
href="https://doi.org/10.1016/j.imavis.2022.104614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general stereo matching methods, there are mainly two kinds of weaknesses: the interior of large objects with unclear texture; object boundaries and small objects. One of the reasons for the first case is the underuse of inter-feature and intra-feature contexts, leading to the lack of discriminability of feature representations. And the second case could be caused result from the loss of fine-grained structural information in the features. In this work, we present a novel stereo matching network to enhance the feature representations, which contains two prominent components: pyramid context enhancement module and cross-modal enhancement module. Pyramid context enhancement module first constructs multi-scale pyramid features to promote global representation, and then exploits the inter-feature and intra-feature contextual information to improve the discriminability of features. Cross-modal enhancement module proactively degrades features of RGB image and disparity map to implicitly extract structures, and fuses both structures to recover the modal consistent structural information of features. Moreover, we design a disparity weight loss function which can calculates the adaptive weight according to prediction error to balance the loss response. Extensive experiments demonstrate that our method boosts the performance, and achieves better results on five challenging datasets than other typical methods. Code is available at: https://github.com/csl1994/FENet .},
  archive      = {J_ICV},
  author       = {Shenglun Chen and Hong Zhang and Baoli Sun and Haojie Li and Xinchen Ye and Zhihui Wang},
  doi          = {10.1016/j.imavis.2022.104614},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104614},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature enhancement network for stereo matching},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single stage architecture for improved accuracy real-time
object detection on mobile devices. <em>ICV</em>, <em>130</em>, 104613.
(<a href="https://doi.org/10.1016/j.imavis.2022.104613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {YOLOv4-tiny is one of the most representative lightweight one-stage object detection algorithms . In this paper, we propose Mini-YOLOv4-tiny, an improved lightweight one-stage object detector based on the YOLOv4-tiny. Typical compression techniques address both memory optimization and computational complexity , but compromise model accuracy. For model compression and speedup, we selectively cut the width of the last convolutional layers . To increase model performance, we propose several improvements with minimal impact on memory and inference time. First, we replace the Leaky-RELU activation function with Mish and fine-tune the data augmentation parameters. To enlarge the receptive field of the network, we propose a modified Spatial Pyramid Pooling module with a reduced number of convolutional blocks and filters, thus adding fewer Floating Point Operations (FLOPs) during the inference process. We performed experiments on the PASCAL VOC and MS COCO datasets and evaluated the inference speed on NVidia Jetson Nano and Raspberry PI 4 (model B). The experimental results show that our methods achieve 72.1% mAP@0.5 and 23.4% mAP@ [0.5:0.95] on the PASCAL VOC and MS COCO datasets, achieving state-of-the-art results among lightweight object detectors. Compared with YOLOv4-tiny on MS COCO, our method has 37% fewer parameters and requires 19% fewer FLOPs, while improving the Intersection over Union (IoU) by 4.02% and the average precision (interval 0.50:0.95) by 2.8%. On PASCAL VOC, Mini-YOLOv4-tiny with an input size of 288 improves mAP@0.5 by 0.3%, while requiring 61% fewer FLOPS and running almost twice as fast.},
  archive      = {J_ICV},
  author       = {Dan-Sebastian Bacea and Florin Oniga},
  doi          = {10.1016/j.imavis.2022.104613},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104613},
  shortjournal = {Image Vis. Comput.},
  title        = {Single stage architecture for improved accuracy real-time object detection on mobile devices},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video prediction by efficient transformers. <em>ICV</em>,
<em>130</em>, 104612. (<a
href="https://doi.org/10.1016/j.imavis.2022.104612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video prediction is a challenging computer vision task that has a wide range of applications. In this work, we present a new family of Transformer-based models for video prediction. Firstly, an efficient local spatial–temporal separation attention mechanism is proposed to reduce the complexity of standard Transformers. Then, a full autoregressive model , a partial autoregressive model and a non-autoregressive model are developed based on the new efficient Transformer. The partial autoregressive model has a similar performance with the full autoregressive model but a faster inference speed. The non-autoregressive model not only achieves a faster inference speed but also mitigates the quality degradation problem of the autoregressive counterparts, but it requires additional parameters and loss function for learning. Given the same attention mechanism, we conducted a comprehensive study to compare the proposed three video prediction variants. Experiments show that the proposed video prediction models are competitive with more complex state-of-the-art convolutional-LSTM based models. The source code is available at https://github.com/XiYe20/VPTR .},
  archive      = {J_ICV},
  author       = {Xi Ye and Guillaume-Alexandre Bilodeau},
  doi          = {10.1016/j.imavis.2022.104612},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104612},
  shortjournal = {Image Vis. Comput.},
  title        = {Video prediction by efficient transformers},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Face reenactment via generative landmark guidance.
<em>ICV</em>, <em>130</em>, 104611. (<a
href="https://doi.org/10.1016/j.imavis.2022.104611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identity preserving problem is one of the major obstacles in face reenactment. The problem occurs when the model fails to preserve the detailed information of the source identity, and especially obvious when reenacting different identities. The underlying factors may include the leaking of driver identity, due to the identity mismatch, and unseen large head poses. In this paper, we propose a novel face reenactment approach via generative landmark coordinates. Specifically, a conditional generative adversarial network is developed to estimate reenacted landmark coordinates for the driving image, which successfully excludes its identity information. These generated coordinates are further injected into the subsequent inference style transferal module to increase the realism of face images. We evaluated our method on the VoxCeleb1 dataset for self-reenactment and the CelebV dataset for reenacting different identities. Extensive experiments demonstrate that our method can produce more realistic reenacted face images.},
  archive      = {J_ICV},
  author       = {Chen Hu and Xianghua Xie and Lin Wu},
  doi          = {10.1016/j.imavis.2022.104611},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104611},
  shortjournal = {Image Vis. Comput.},
  title        = {Face reenactment via generative landmark guidance},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on computer vision based human analysis in the
COVID-19 era. <em>ICV</em>, <em>130</em>, 104610. (<a
href="https://doi.org/10.1016/j.imavis.2022.104610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of COVID-19 has had a global and profound impact, not only on society as a whole, but also on the lives of individuals. Various prevention measures were introduced around the world to limit the transmission of the disease, including face masks, mandates for social distancing and regular disinfection in public spaces, and the use of screening applications. These developments also triggered the need for novel and improved computer vision techniques capable of ( i ) (i) providing support to the prevention measures through an automated analysis of visual data, on the one hand, and ( ii ) (ii) facilitating normal operation of existing vision-based services, such as biometric authentication schemes, on the other. Especially important here, are computer vision techniques that focus on the analysis of people and faces in visual data and have been affected the most by the partial occlusions introduced by the mandates for facial masks. Such computer vision based human analysis techniques include face and face-mask detection approaches, face recognition techniques, crowd counting solutions, age and expression estimation procedures, models for detecting face-hand interactions and many others, and have seen considerable attention over recent years. The goal of this survey is to provide an introduction to the problems induced by COVID-19 into such research and to present a comprehensive review of the work done in the computer vision based human analysis field. Particular attention is paid to the impact of facial masks on the performance of various methods and recent solutions to mitigate this problem. Additionally, a detailed review of existing datasets useful for the development and evaluation of methods for COVID-19 related applications is also provided. Finally, to help advance the field further, a discussion on the main open challenges and future research direction is given at the end of the survey. This work is intended to have a broad appeal and be useful not only for computer vision researchers but also the general public.},
  archive      = {J_ICV},
  author       = {Fevziye Irem Eyiokur and Alperen Kantarcı and Mustafa Ekrem Erakın and Naser Damer and Ferda Ofli and Muhammad Imran and Janez Križaj and Albert Ali Salah and Alexander Waibel and Vitomir Štruc and Hazım Kemal Ekenel},
  doi          = {10.1016/j.imavis.2022.104610},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104610},
  shortjournal = {Image Vis. Comput.},
  title        = {A survey on computer vision based human analysis in the COVID-19 era},
  volume       = {130},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-level contrastive learning for unsupervised person
re-identification. <em>ICV</em>, <em>129</em>, 104607. (<a
href="https://doi.org/10.1016/j.imavis.2022.104607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (re-ID) has drawn increasing attention in the community because it is more data-friendly when applied in the real world. Most existing works leverage instance discrimination learning to guide the model to learn image features for person matching. However, the instance discrimination may cause unexpected repulsion among similar samples, which makes the unsupervised feature learning unstable. To address this problem, we propose a dual-level contrastive learning (DLCL) framework to mine both the intra-instance and inter-instance similarities. The DLCL framework consists of two tasks: instance-instance contrastive learning (IICL) and instance-community contrastive learning (ICCL). The IICL aims to mine the intra-instance similarity via pulling an original sample and its augmented versions closer and pushing different instances away. The ICCL is proposed to capture the inter-instance similarity by attracting similar instances to the same sample community, which can reduce the unexpected repulsion brought by instance discrimination. The combination of IICL and ICCL can enable the model to learn more robust and discriminative image features. Extensive experimental results on Market-1501 and DukeMTMC-reID indicate the effectiveness of our method for unsupervised person re-ID.},
  archive      = {J_ICV},
  author       = {Yu Zhao and Qiaoyuan Shu and Xi Shi},
  doi          = {10.1016/j.imavis.2022.104607},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104607},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual-level contrastive learning for unsupervised person re-identification},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guest editorial: Learning with manifolds in computer vision.
<em>ICV</em>, <em>129</em>, 104599. (<a
href="https://doi.org/10.1016/j.imavis.2022.104599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Mohamed Daoudi and Mehrtash Harandi and Vittorio Murino},
  doi          = {10.1016/j.imavis.2022.104599},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104599},
  shortjournal = {Image Vis. Comput.},
  title        = {Guest editorial: Learning with manifolds in computer vision},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aggregation of attention and erasing for weakly supervised
object localization. <em>ICV</em>, <em>129</em>, 104598. (<a
href="https://doi.org/10.1016/j.imavis.2022.104598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In weakly supervised object localization (WSOL), objects are localized using image-level labels only. Most previous WSOL methods erase the most discriminative region using a fixed threshold to cover the entire region of the object, or they use random selection between attention and erasing. However, these methods have several limitations. First, the previous erasing method does not guarantee the erasing of adjacent pixels in the most discriminative region. Because adjacent pixels have similar information, this property may decrease the effect of the erasing. Next, random selection does not ensure the consistent use of attention and erasing, and it can be less effective to utilize both of them. To overcome these limitations, we propose a new sequential block, namely, a spatial attention branch followed by a refinement branch. Initially, the spatial attention branch generates a feature map with enhanced spatial information. Subsequently, the refinement branch helps the network capture the entire area of the object using attention and erasing modules. The attention module enhances the positional information by considering the spatial relationships. The erasing module erases all values within the specified region using various erasing sizes to increase the erasing effect. Finally, to make full use of the advantages of attention and erasing, these modules are combined through element-wise addition. Using such approaches, the proposed method achieves a state-of-the-art localization performance on the CUB-200-2011 and ILSVRC 2012 datasets. Furthermore, the proposed block requires only lightweight trainable parameters, demonstrating the efficiency of the proposed approach.},
  archive      = {J_ICV},
  author       = {Bongyeong Koo and Han-Soo Choi and Myungjoo Kang},
  doi          = {10.1016/j.imavis.2022.104598},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104598},
  shortjournal = {Image Vis. Comput.},
  title        = {Aggregation of attention and erasing for weakly supervised object localization},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting crowd counting: State-of-the-art, trends, and
future perspectives. <em>ICV</em>, <em>129</em>, 104597. (<a
href="https://doi.org/10.1016/j.imavis.2022.104597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is an effective tool for situational awareness in public places. Automated crowd counting using images and videos is an interesting yet challenging problem that has gained significant attention in computer vision . Over the past few years, various deep learning methods have been developed to achieve state-of-the-art performance. The methods evolved over time vary in many aspects such as model architecture, input pipeline, learning paradigm, computational complexity , and accuracy gains etc. In this paper, we present a systematic and comprehensive review of the most significant contributions in the area of crowd counting. Although few surveys exist on the topic, our survey is most up-to date and different in several aspects. First, it provides a more meaningful categorization of the most significant contributions by model architectures, learning methods (i.e., loss functions), and evaluation methods (i.e., evaluation metrics). We chose prominent and distinct works and excluded similar works. We also sort the well-known crowd counting models by their performance over benchmark datasets. We believe that this survey can be a good resource for novice researchers to understand the progressive developments and contributions over time and the current state-of-the-art.},
  archive      = {J_ICV},
  author       = {Muhammad Asif Khan and Hamid Menouar and Ridha Hamila},
  doi          = {10.1016/j.imavis.2022.104597},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104597},
  shortjournal = {Image Vis. Comput.},
  title        = {Revisiting crowd counting: State-of-the-art, trends, and future perspectives},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and accurate superpixel segmentation algorithm with a
guidance image. <em>ICV</em>, <em>129</em>, 104596. (<a
href="https://doi.org/10.1016/j.imavis.2022.104596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixels are generated by automatically grouping image pixels into lots and lots of compact segments. In computer vision , it is widely used as an effective way to reduce the number of image primitives for subsequent tasks and for recognizing objects’ contours due to its excellent boundary adhesion. The primary concerns of a superpixel generation algorithm are its efficiency and accuracy. In this document, we aim to propose a fast and precise superpixel segmentation algorithm with a guidance image. Specifically, we introduce a rolling filter that can remove details while well preserving the boundaries to obtain the guidance image. In the meantime, we introduce a robust edge confidence operator to accurately detect image boundaries. From this, we define a distance measurement with adaptive parameters for each image. In this way, we can adapt and accurately group pixels into regions according to the new distance measurement. Furthermore, we adopt a noniterative framework to generate superpixels in real time by processing all pixels once. The experimental results show that the proposed methodology achieves state-of-the-art performance on two sets of reference data while operating in real time.},
  archive      = {J_ICV},
  author       = {Yongsheng Zhang and Yongxia Zhang and Linwei Fan and Nannan Wang},
  doi          = {10.1016/j.imavis.2022.104596},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104596},
  shortjournal = {Image Vis. Comput.},
  title        = {Fast and accurate superpixel segmentation algorithm with a guidance image},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TENet: Accurate light-field salient object detection with a
transformer embedding network. <em>ICV</em>, <em>129</em>, 104595. (<a
href="https://doi.org/10.1016/j.imavis.2022.104595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current light-field salient object detection methods have difficulty in accurately distinguishing objects from complex backgrounds. In this paper, we believe that this problem can be mitigated by optimizing feature fusion and enlarging receptive field, and thus propose a novel transformer embedding network named TENet. The main idea of the network is to (1) selectively aggregate multi-features for fuller feature fusion; (2) integrate the Transformer for larger receptive field, so as to accurately identify salient objects. For the former, firstly, a multi-modal feature fusion module (MMFF) is designed to mine the different contributions of multi-modal features ( i.e. , all-in-focus image features and focal stack features). Then, a multi-level feature fusion module (MLFF) is developed to iteratively select and fuse complementary cues from multi-level features. For the latter, we integrate the Transformer for the first time and propose a transformer-based feature enhancement module (TFE), to provide a wider receptive field for each pixel of high-level features. To validate our idea, we comprehensively evaluate the performance of our TENet on three challenging datasets. Experimental results show that our method outperforms the state-of-the-art method, e.g. , the detection accuracy is improved by 28.1%, 20.3%, and 14.9% in MAE metric, respectively.},
  archive      = {J_ICV},
  author       = {Xingzheng Wang and Songwei Chen and Guoyao Wei and Jiehao Liu},
  doi          = {10.1016/j.imavis.2022.104595},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104595},
  shortjournal = {Image Vis. Comput.},
  title        = {TENet: Accurate light-field salient object detection with a transformer embedding network},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DFAF3D: A dual-feature-aware anchor-free single-stage 3D
detector for point clouds. <em>ICV</em>, <em>129</em>, 104594. (<a
href="https://doi.org/10.1016/j.imavis.2022.104594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, anchor-free single-stage 3D object detection methods based on point clouds have attracted extensive attention due to their high efficiency. It is crucial to enhance the ability of the center features to represent the object for such methods. In this paper, we propose a dual spatial-context feature extraction (SCFE) module to extract both spatial and context features of point clouds in parallel for 3D object detection , in which, we design a deformable offset self-attention (DOSA) structure in the context feature extraction branch to learn the relative structure information of point clouds. Correspondingly, we design a coordinate attentional feature fusion (CAFF) module, which adaptively fuses spatial and context features of different resolutions while preserving coordinate information thus making the features of center point more representative. Experiments on KITTI demonstrate that the proposed method outperforms all previous anchor-free methods in general and has comparable performance to anchor-based methods in comprehensive performance and it achieves good trade-offs in terms of accuracy, speed and parameter size.},
  archive      = {J_ICV},
  author       = {Qingsong Tang and Xinyu Bai and Jinting Guo and Bolin Pan and Wuming Jiang},
  doi          = {10.1016/j.imavis.2022.104594},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104594},
  shortjournal = {Image Vis. Comput.},
  title        = {DFAF3D: A dual-feature-aware anchor-free single-stage 3D detector for point clouds},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cross-modal crowd counting method combining CNN and
cross-modal transformer. <em>ICV</em>, <em>129</em>, 104592. (<a
href="https://doi.org/10.1016/j.imavis.2022.104592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal crowd counting aims to use the information between different modalities to generate crowd density images, so as to estimate the number of pedestrians more accurately in unconstrained scenes. Due to the huge differences between different modal images, how to effectively fuse the information between different modalities is still a challenging problem. To address this problem, we propose a cross-modal crowd counting method based on CNN and novel cross-modal transformer, which effectively fuses the information between different modalities and boosts the accuracy of crowd counting in unconstrained scenes. Concretely, we first design double CNN branches to capture the modality-specific features of images. After that, we design a novel cross-modal transformer to extract cross-modal global features from the modality-specific features. Furthermore, we a propose cross layer connection structure to connect the front-end information and back-end information of the network by adding different layer features. At the end of the network, we develop a cross- modal attention module to strengthen the cross-modal feature representation by extracting the complementarities between different modal features. The experimental results show that the method combining CNN and novel cross-modal transformer proposed in this paper achieves state-of-the-art performance, which not only effectively improves the accuracy and robustness of cross-modal crowd counting, but also has good generalization under multimodal crowd counting.},
  archive      = {J_ICV},
  author       = {Shihui Zhang and Wei Wang and Weibo Zhao and Lei Wang and Qunpeng Li},
  doi          = {10.1016/j.imavis.2022.104592},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104592},
  shortjournal = {Image Vis. Comput.},
  title        = {A cross-modal crowd counting method combining CNN and cross-modal transformer},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling graph-structured contexts for image captioning.
<em>ICV</em>, <em>129</em>, 104591. (<a
href="https://doi.org/10.1016/j.imavis.2022.104591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of image captioning has been significantly improved recently through deep neural network architectures combining with attention mechanisms and reinforcement learning optimization. Exploring visual relationships and interactions between different objects appearing in the image, however, is far from being investigated. In this paper, we present a novel approach that combines scene graphs with Transformer, which we call SGT, to explicitly encode available visual relationships between detected objects. Specifically, we pretrain an scene graph generation model to predict graph representations for images. After that, for each graph node , a Graph Convolutional Network (GCN) is employed to acquire relationship knowledge by aggregating the information of its local neighbors. As we train the captioning model, we feed the potential relation-aware information into the Transformer to generate descriptive sentence. Experiments on the MSCOCO dataset and the Flickr30k dataset validate the superiority of our SGT model, which can realize state-of-the-art results in terms of all the standard evaluation metrics .},
  archive      = {J_ICV},
  author       = {Zhixin Li and Jiahui Wei and Feicheng Huang and Huifang Ma},
  doi          = {10.1016/j.imavis.2022.104591},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104591},
  shortjournal = {Image Vis. Comput.},
  title        = {Modeling graph-structured contexts for image captioning},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring viewport features for semi-supervised saliency
prediction in omnidirectional images. <em>ICV</em>, <em>129</em>,
104590. (<a href="https://doi.org/10.1016/j.imavis.2022.104590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with the annotated data for the 2D image saliency prediction task, the annotated data for training omnidirectional image (or 360 ° ° image) saliency prediction models are not sufficient. Most existing fully-supervised saliency prediction methods for omnidirectional images (ODIs) adopt a scheme, first training the methods on a labeled large 2D image saliency prediction dataset and then fine-tuning the methods on the labeled tiny ODI saliency prediction dataset. However, this strategy is time-consuming and may not inadequately mine the visual features built in ODIs. To explore the visual attributes targeted at ODIs and address the shortage of labels on these ODIs, in this paper, we propose an end-to-end semi-supervised network, namely VFNet, which relies on viewport features and only utilizes ODIs as training data, for ODI saliency prediction. Concretely, we adopt consistency regularization as our semi-supervised learning framework. The predictions between main and auxiliary saliency inference networks in the VFNet enforce consistency. Aiming at ODIs, we introduce a new form of perturbation, i.e. , DropView, to improve the effectiveness of consistency regularization. By randomly dropping out different 360 ° ° cubemap viewport features before the auxiliary saliency inference network, the proposed DropView enhances the robustness of the final ODI saliency prediction. To adaptively interact with the equirectangular and different cubemap viewport features according to their contributions, we introduce a Viewport Feature Adaptive Integration (VFAI) module and deploy the VFAI module at different levels in the VFNet to raise the capacity of feature encoding of our VFNet. Compared with state-of-the-art fully-supervised methods, our VFNet with fewer labeled training data achieves competitive performance demonstrated by extensive experiments on two publicly available datasets.},
  archive      = {J_ICV},
  author       = {Mengke Huang and Gongyang Li and Zhi Liu and Yong Wu and Chen Gong and Linchao Zhu and Yi Yang},
  doi          = {10.1016/j.imavis.2022.104590},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104590},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring viewport features for semi-supervised saliency prediction in omnidirectional images},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale interaction transformer for temporal action
proposal generation. <em>ICV</em>, <em>129</em>, 104589. (<a
href="https://doi.org/10.1016/j.imavis.2022.104589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal generation is to localize the time intervals with actions in untrimmed videos. Action instances in untrimmed videos have dramatically varied temporal scales which brings about great challenges for temporal action proposal generation. While temporal action proposal generation has achieved tremendous progress over the past years, multi-scale issue in action proposal generation is still an open problem. In this paper, we propose a Multi-scale Interaction Transformer (MSIT) architecture, which adopts a directly set prediction method to work out the temporal action proposal generation task. MSIT constructs multi-scale feature pyramids and incorporates a novel multi-scale mechanism into Transformer framework. A customized top-down interaction structure is designed to perform self-scale attention and cross-scale attention at different levels of the feature pyramids. Through the top-down interaction, the semantic and location information in each feature level is strengthened and therefore the proposal generation performance can be improved. Furthermore, to model the accurate action locations for each frame, we incorporate an actionness prediction structure to constrain the features output from the encoder. The proposed method was tested on two challenging datasets: THUMOS14 and ActivityNet-1.3. Experiments show that our method achieves comparable performance with state-of-the-art methods. Extensive studies and visualizations also demonstrate the strength of our method.},
  archive      = {J_ICV},
  author       = {Jiahui Shang and Ping Wei and Huan Li and Nanning Zheng},
  doi          = {10.1016/j.imavis.2022.104589},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104589},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-scale interaction transformer for temporal action proposal generation},
  volume       = {129},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
