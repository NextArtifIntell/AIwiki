<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csda---165">CSDA - 165</h2>
<ul>
<li><details>
<summary>
(2023). Bayesian modeling of spatial integer-valued time series.
<em>CSDA</em>, <em>188</em>, 107827. (<a
href="https://doi.org/10.1016/j.csda.2023.107827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many infectious diseases spread through person-to-person contact, either directly or indirectly. The proposal incorporates spatial-temporal patterns into multivariate integer-valued GARCH models with either a generalized Poisson distribution or zero-inflated generalized Poisson distribution in order to describe these features of the data. By considering the neighboring locations of the target series, the set-up incorporates a flexible and continuous conceptualization of distance to present the spatial components, thereby highlighting for the non-separability of space and time. Such an approach eliminates the need to pre-assign a spatial weight matrix . Newly designed models are utilized to investigate time-series counts of infectious diseases, enabling inference, prediction, and model selection within a Bayesian framework through Markov chain Monte Carlo (MCMC) algorithms. As an illustration, design simulation studies and multivariate weekly dengue cases are scrutinized for the performance of the Bayesian methods. The proposed models successfully capture the characteristics of spatial dependency, over-dispersion, and a large portion of zeros, providing a comprehensive model for the observed phenomena in the data.},
  archive      = {J_CSDA},
  author       = {Cathy W.S. Chen and Chun-Shu Chen and Mo-Hua Hsiung},
  doi          = {10.1016/j.csda.2023.107827},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107827},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian modeling of spatial integer-valued time series},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regional quantile regression for multiple responses.
<em>CSDA</em>, <em>188</em>, 107826. (<a
href="https://doi.org/10.1016/j.csda.2023.107826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study high-dimensional multiple response quantile regression model for an interval of quantile levels, in which a common set of covariates is used to analyze multiple responses simultaneously. We assume that the underlying quantile coefficient matrix is simultaneously element-wise and row-wise sparse. We address high dimensional issues to identify globally relevant variables for multiple responses when any τ th conditional quantile is considered, where τ ∈ Δ τ∈Δ , and Δ is an interval of quantile levels of interest. We develop a novel penalized globally concerned quantile regression with double group Lasso penalties and propose an information criterion for penalty parameter choice. We prove that the proposed method consistently selects both element-wise and row-wise sparsity patterns of the regression coefficient matrix function and that it achieves the oracle convergence rate. Numerical examples and applications to Cancer Cell Line Encyclopedia data illustrate the advantages of the proposed method over separate penalized quantile regression on each response.},
  archive      = {J_CSDA},
  author       = {Seyoung Park and Hyunjin Kim and Eun Ryung Lee},
  doi          = {10.1016/j.csda.2023.107826},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107826},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regional quantile regression for multiple responses},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Under-reported time-varying MINAR(1) process for modeling
multivariate count series. <em>CSDA</em>, <em>188</em>, 107825. (<a
href="https://doi.org/10.1016/j.csda.2023.107825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A time-varying multivariate integer-valued autoregressive of order one (tvMINAR(1)) model is introduced for the non-stationary time series of correlated counts when under-reporting is likely present. A non-diagonal autoregression probability network is structured to preserve the cross-correlation of multivariate series, provide a necessary condition to ease model-fittings computations, and derive the full likelihood using the Viterbi algorithm . The motivating construction applies to fully under-reported counts that rely on a mixture presentation of the random thinning operator. Simulation studies are conducted to examine the proposed model, and the analysis of COVID-19 daily cases is accomplished to highlight its usefulness in applications. Finally, the comparison of models is presented using the posterior predictive checking method.},
  archive      = {J_CSDA},
  author       = {Zeynab Aghabazaz and Iraj Kazemi},
  doi          = {10.1016/j.csda.2023.107825},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107825},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Under-reported time-varying MINAR(1) process for modeling multivariate count series},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-profiled distributed estimation for high-dimensional
partially linear model. <em>CSDA</em>, <em>188</em>, 107824. (<a
href="https://doi.org/10.1016/j.csda.2023.107824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a leading example in the semiparametric model, the partially linear model is prominent in modeling complex data. The challenges associated with designing an efficient estimation algorithm in the distributed environment are not yet well addressed. The existing works require a constraint on the number of local machines to guarantee the optimality of the global estimator. In addition, a multi-round profiled estimator will lead to huge communication complexity. To further reduce communication costs, a novel semi-profiled estimation procedure is proposed, which provides an iterative skeleton to reduce estimation error. A new multi-round distributed algorithm based on the centralized semi-profiled estimator is developed, which can estimate the parametric and nonparametric simultaneously. The theoretical results indicate that the corresponding convergence rates can achieve the optimal orders within a constant number of communication rounds. The advantages of the proposed estimation method are demonstrated via numerical experiments on synthetic and real data.},
  archive      = {J_CSDA},
  author       = {Yajie Bao and Haojie Ren},
  doi          = {10.1016/j.csda.2023.107824},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107824},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semi-profiled distributed estimation for high-dimensional partially linear model},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble LDA via the modified cholesky decomposition.
<em>CSDA</em>, <em>188</em>, 107823. (<a
href="https://doi.org/10.1016/j.csda.2023.107823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A binary classification problem in the high-dimensional settings is studied via the ensemble learning with each base classifier constructed from the linear discriminant analysis (LDA), and these base classifiers are integrated by the weighted voting. The precision matrix in the LDA rule is estimated by the modified Cholesky decomposition (MCD), which is able to provide us with a set of precision estimates by considering multiple variable orderings, and hence yield a group of different LDA classifiers. Such available LDA classifiers are then integrated to improve the classification performance. The simulation and the application studies are conducted to demonstrate the merits of the proposed method.},
  archive      = {J_CSDA},
  author       = {Zhenguo Gao and Xinye Wang and Xiaoning Kang},
  doi          = {10.1016/j.csda.2023.107823},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107823},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Ensemble LDA via the modified cholesky decomposition},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian multivariate nonlinear state space copula models.
<em>CSDA</em>, <em>188</em>, 107820. (<a
href="https://doi.org/10.1016/j.csda.2023.107820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel flexible class of multivariate nonlinear non-Gaussian state space models, based on copulas , is proposed. Specifically, it is assumed that the observation equation and the state equation are defined by copula families that are not necessarily equal. Inference is performed within the Bayesian framework, using the Hamiltonian Monte Carlo method. Simulation studies show that the proposed copula-based approach is extremely flexible, since it is able to describe a wide range of dependence structures and, at the same time, allows us to deal with missing data. The application to atmospheric pollutant measurement data shows that the approach is suitable for accurate modeling and prediction of data dynamics in the presence of missing values. Comparison to a Gaussian linear state space model and to Bayesian additive regression trees shows the superior performance of the proposed model with respect to predictive accuracy .},
  archive      = {J_CSDA},
  author       = {Alexander Kreuzer and Luciana Dalla Valle and Claudia Czado},
  doi          = {10.1016/j.csda.2023.107820},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107820},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian multivariate nonlinear state space copula models},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goodness-of-fit testing for normal mixture densities.
<em>CSDA</em>, <em>188</em>, 107815. (<a
href="https://doi.org/10.1016/j.csda.2023.107815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel goodness-of-fit test for assessing the validity of maximum likelihood estimates of normal mixture densities with known number of components is introduced. The theoretical contributions include analytic quantification of the test statistic&#39;s size and power functions under fixed and local alternatives. These are used to derive a closed-form bandwidth rule which optimizes the test&#39;s power while keeping its size constant at a given significance level , and a cut-off point suitable for finite sample implementations of the test. An extensive simulation study compares the performance of the new test to well-established tests in the literature and demonstrates the superiority of the former in all examples considered. Finally, its practical usefulness is demonstrated in the analysis of two real world datasets.},
  archive      = {J_CSDA},
  author       = {Dimitrios Bagkavos and Prakash N. Patil},
  doi          = {10.1016/j.csda.2023.107815},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107815},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Goodness-of-fit testing for normal mixture densities},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel spatio-temporal clustering algorithm with
applications on COVID-19 data from the united states. <em>CSDA</em>,
<em>188</em>, 107810. (<a
href="https://doi.org/10.1016/j.csda.2023.107810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new clustering algorithm for spatio-temporal data is developed. The proposed method leverages a weighted combination of a spatial haversine distance matrix and a spectral-density based temporal distance matrix between the locations. Concepts of partition around medoids algorithm and the gap statistic are utilized to develop the algorithm and to determine the optimal number of clusters. Such a non-parametric algorithm is novel as it incorporates both spatial and temporal distances of the units and it can work for time-series of possibly different lengths. Theoretical guarantee of consistency of the proposed method is provided. An elaborate simulation study is also given to demonstrate the efficacy of the algorithm. As an interesting real life application, the proposed algorithm is implemented to analyze the spatio-temporal dynamics of the time series of coronavirus (COVID-19) incidence rates observed at county-level in the United States of America . The results are demonstrated on datasets of different sizes: the entire country, the Midwest region and the state of California . Special emphasis is given on the last two cases to display how the clustering results offer interesting insights into the epidemic progression in these areas. Particularly, it sheds light on whether state-mandated restrictions impacted the entire state similarly or if there are interesting local behaviors in terms of the COVID-19 spread.},
  archive      = {J_CSDA},
  author       = {Soudeep Deb and Sayar Karmakar},
  doi          = {10.1016/j.csda.2023.107810},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107810},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A novel spatio-temporal clustering algorithm with applications on COVID-19 data from the united states},
  volume       = {188},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online missing value imputation for high-dimensional
mixed-type data via generalized factor models. <em>CSDA</em>,
<em>187</em>, 107822. (<a
href="https://doi.org/10.1016/j.csda.2023.107822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complete-observation requirement of most machine learning methods necessitates new statistical methods to handle datasets messy with missing values. This is especially urgent for streaming data that are generated at high speed and with a lack of quality control. Missing data imputation becomes an inevitable preprocessing step before subsequent analysis. A practical and meaningful online imputation algorithm should be not only scalable to large-scale datasets but also able to manage high-dimensional mixed-type data containing binary, count and continuous variables. To fill this gap, a novel online imputation algorithm, called OMIG, is proposed for streaming data under the framework of generalized factor models. To obtain deeper insight, OMIG is theoretically and empirically compared to its other two versions, the oracle version and the offline version. Theoretical and numerical findings show that (a) the imputed data obtained by OMIG are not equivalent to but instead at a slower rate than those obtained by its oracle version in terms of imputation accuracy; (b) OMIG outperforms its offline version in imputation accuracy; and (c) OMIG is equivalent to its oracle version in estimation accuracy for the factor loading, which largely facilitates interpretation and follow-up analysis. Extensive numerical experiments and two real datasets are used to demonstrate the performance of the proposed method.},
  archive      = {J_CSDA},
  author       = {Wei Liu and Lan Luo and Ling Zhou},
  doi          = {10.1016/j.csda.2023.107822},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107822},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Online missing value imputation for high-dimensional mixed-type data via generalized factor models},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conformal prediction bands for two-dimensional functional
time series. <em>CSDA</em>, <em>187</em>, 107821. (<a
href="https://doi.org/10.1016/j.csda.2023.107821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time evolving surfaces can be modeled as two-dimensional Functional time series, exploiting the tools of Functional data analysis. Leveraging this approach, a forecasting framework for such complex data is developed. The main focus revolves around Conformal Prediction , a versatile nonparametric paradigm used to quantify uncertainty in prediction problems. Building upon recent variations of Conformal Prediction for Functional time series, a probabilistic forecasting scheme for two-dimensional functional time series is presented, while providing an extension of Functional Autoregressive Processes of order one to this setting. Estimation techniques for the latter process are introduced, and their performance are compared in terms of the resulting prediction regions. Finally, the proposed forecasting procedure and the uncertainty quantification technique are applied to a real dataset, collecting daily observations of Sea Level Anomalies of the Black Sea.},
  archive      = {J_CSDA},
  author       = {Niccolò Ajroldi and Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
  doi          = {10.1016/j.csda.2023.107821},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107821},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conformal prediction bands for two-dimensional functional time series},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast estimation for generalised multivariate joint models
using an approximate EM algorithm. <em>CSDA</em>, <em>187</em>, 107819.
(<a href="https://doi.org/10.1016/j.csda.2023.107819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and survival data have become an established tool for optimally handling scenarios when both types of data co-exist. Multivariate extensions to the classic univariate joint model have started to emerge but are typically restricted to the Gaussian case , deployed in a Bayesian framework or focused on dimension reduction. An approximate EM algorithm is utilised which circumvents the oft-lamented curse of dimensionality and offers a likelihood-based implementation which ought to appeal to clinicians and practitioners alike. The proposed method is validated in a pair of simulation studies, which demonstrate both its accuracy in parameter estimation and efficiency in terms of computational cost. Its clinical use is demonstrated via an application to primary billiary cirrhosis data. The proposed methodology for estimation of these joint models is available in R package gmvjoint .},
  archive      = {J_CSDA},
  author       = {James Murray and Pete Philipson},
  doi          = {10.1016/j.csda.2023.107819},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107819},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast estimation for generalised multivariate joint models using an approximate EM algorithm},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Principal weighted least square support vector machine: An
online dimension-reduction tool for binary classification.
<em>CSDA</em>, <em>187</em>, 107818. (<a
href="https://doi.org/10.1016/j.csda.2023.107818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As relevant technologies advance, streamed data are frequently encountered in various applications, and the need for scalable algorithms becomes urgent. In this article, we propose the principal weighted least square support vector machine (PWLSSVM) as a novel tool for SDR in binary classification where most SDR methods suffer since they assume continuous Y . We further show that the PWLSSVM can be employed for the online SDR for the streamed data. Namely, the PWLSSVM estimator can be directly updated from the new data without having old data. We explore the asymptotic properties of the PWLSSVM estimator and demonstrate its promising performance in terms of both estimation accuracy and computational efficiency for both simulated and real data.},
  archive      = {J_CSDA},
  author       = {Hyun Jung Jang and Seung Jun Shin and Andreas Artemiou},
  doi          = {10.1016/j.csda.2023.107818},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107818},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Principal weighted least square support vector machine: An online dimension-reduction tool for binary classification},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Potts-cox survival regression. <em>CSDA</em>, <em>187</em>,
107816. (<a href="https://doi.org/10.1016/j.csda.2023.107816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian semi-parametric survival regression model with latent partitions is introduced. Its goal is to predict survival and to cluster survival patients within the context of building prognosis systems. In order to drive cluster formation on individuals, the Potts random partition model is chosen as a prior on the covariates space. For any given partition, the proposed model assumes an interval-wise exponential distribution for the baseline hazard rate. The number of intervals is unknown. It can be estimated with a fused-lasso type penalty given by a sequential double exponential prior. Estimation and inference are done with the aid of Markov chain Monte Carlo . To simplify the computations, the Laplace&#39;s integral approximation method is used to estimate some constants and to propose parameter updates within Markov chain Monte Carlo . The methodology is illustrated with an application to cancer survival.},
  archive      = {J_CSDA},
  author       = {Danae Martinez-Vargas and Alejandro Murua-Sazo},
  doi          = {10.1016/j.csda.2023.107816},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107816},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Potts-cox survival regression},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiclass laplacian support vector machine with functional
analysis of variance decomposition. <em>CSDA</em>, <em>187</em>, 107814.
(<a href="https://doi.org/10.1016/j.csda.2023.107814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In classification problems, acquiring a sufficient amount of labeled samples sometimes proves expensive and time-consuming, while unlabeled samples are relatively easier to obtain. The Laplacian Support Vector Machine (LapSVM) is one of the successful methods that learn better classification functions by incorporating unlabeled samples . However, since LapSVM was originally designed for binary classification , it can not be applied directly to multiclass classification problems commonly encountered in practice. Thus we derive an extension of LapSVM to multiclass classification problems using an appropriate multiclass formulation. Another problem with LapSVM is that irrelevant variables easily degrade classification performance. The irrelevant variables can increase the variance of predicted values and make the model difficult to interpret. Therefore, this paper also proposes the multiclass LapSVM with functional analysis of variance decomposition to identify relevant variables. Through comprehensive simulations and real-world datasets, we demonstrate the efficiency and improved classification performance of the proposed methods.},
  archive      = {J_CSDA},
  author       = {Beomjin Park and Changyi Park},
  doi          = {10.1016/j.csda.2023.107814},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107814},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multiclass laplacian support vector machine with functional analysis of variance decomposition},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical depth for point process via the isometric
log-ratio transformation. <em>CSDA</em>, <em>187</em>, 107813. (<a
href="https://doi.org/10.1016/j.csda.2023.107813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical depth, a useful tool to measure the center-outward rank of multivariate and functional data, is still under-explored in temporal point processes. Recent studies on point process depth proposed a weighted product of two terms - one indicates the depth of the cardinality of the process, and the other characterizes the conditional depth of the temporal events given the cardinality. The second term is of great challenge because of the apparent nonlinear structure of event times, and so far only parametric representations such as Gaussian and Dirichlet densities have been adopted in the definitions. However, these parametric forms ignore the underlying distribution of the process events and are difficult to apply to complicated patterns. To deal with these problems, a novel distribution-based approach is proposed to the conditional depth via the well-known Isometric Log-Ratio (ILR) transformation on the inter-event times. Motivated by a uniform distribution on simplex, the new method, called the ILR depth, is formally defined for general point process via a Time Rescaling. The mathematical properties of the ILR depth are thoroughly examined and the new method is well illustrated by using Poisson and non-Poisson processes to demonstrate its superiority over previous methods. Finally, the ILR depth is applied in a real dataset and the result clearly shows its effectiveness in ranking.},
  archive      = {J_CSDA},
  author       = {Xinyu Zhou and Yijia Ma and Wei Wu},
  doi          = {10.1016/j.csda.2023.107813},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107813},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Statistical depth for point process via the isometric log-ratio transformation},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-efficient estimation of quantile matrix
regression for massive datasets. <em>CSDA</em>, <em>187</em>, 107812.
(<a href="https://doi.org/10.1016/j.csda.2023.107812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern scientific applications, more and more data sets contain natural matrix predictors and traditional regression methods are not directly applicable. Matrix regression has been adapted to such data structure and received increasing attention in recent years. In this paper, we consider estimation of the conditional quantile in high-dimensional regularized matrix regression with a nuclear norm penalty and establish the convergence rate of the estimator. In order to construct a quantile matrix regression estimator in the distributed setting or for massive data sets, we propose a regularized communication-efficient surrogate loss (CSL) function. The proposed CSL method only needs the worker machines to compute the gradient based on local data and the central machine solves a regularized estimation problem. We prove that the estimation error based on the proposed CSL method matches the estimation error bound of the centralized method that analyzes the entire data set. An alternating direction method of multipliers algorithm is developed to efficiently obtain the distributed CSL estimator. The finite-sample performance of the proposed estimator is studied through simulations and an application to Beijing Air Quality data set.},
  archive      = {J_CSDA},
  author       = {Yaohong Yang and Lei Wang and Jiamin Liu and Rui Li and Heng Lian},
  doi          = {10.1016/j.csda.2023.107812},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107812},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Communication-efficient estimation of quantile matrix regression for massive datasets},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A partially separable model for dynamic valued networks.
<em>CSDA</em>, <em>187</em>, 107811. (<a
href="https://doi.org/10.1016/j.csda.2023.107811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Exponential-family Random Graph Model (ERGM) is a powerful model to fit networks with complex structures. However, for dynamic valued networks whose observations are matrices of counts that evolve over time, the development of the ERGM framework is still in its infancy. To facilitate the modeling of dyad value increment and decrement, a Partially Separable Temporal ERGM is proposed for dynamic valued networks. The parameter learning algorithms inherit state-of-the-art estimation techniques to approximate the maximum likelihood , by drawing Markov chain Monte Carlo (MCMC) samples conditioning on the valued network from the previous time step. The ability of the proposed model to interpret network dynamics and forecast temporal trends is demonstrated with real data.},
  archive      = {J_CSDA},
  author       = {Yik Lun Kei and Yanzhen Chen and Oscar Hernan Madrid Padilla},
  doi          = {10.1016/j.csda.2023.107811},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107811},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A partially separable model for dynamic valued networks},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online regularized matrix regression with streaming data.
<em>CSDA</em>, <em>187</em>, 107809. (<a
href="https://doi.org/10.1016/j.csda.2023.107809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As extensions of vector data with ultrahigh dimensionality and complex structures, matrix data are fast emerging in a large variety of scientific applications. In this paper, we consider the matrix regression with streaming data and propose two-stage online regularized estimators with nuclear norm (NN) and adaptive nuclear norm (ANN) penalties, respectively. In the first stage, an equivalent form of offline matrix regression loss function using current raw data and summary statistics from historical data is established. In the second stage, gradient descent algorithm and soft thresholding methods are implemented iteratively to obtain the proposed online NN and ANN estimators. We establish the asymptotic properties of the resulting online regularized estimators and show the rank selection consistency for the online ANN estimator. The finite-sample performance of the proposed estimators is studied through simulations and an application to Beijing Air Quality data set.},
  archive      = {J_CSDA},
  author       = {Yaohong Yang and Weihua Zhao and Lei Wang},
  doi          = {10.1016/j.csda.2023.107809},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107809},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Online regularized matrix regression with streaming data},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The bayesian regularized quantile varying coefficient model.
<em>CSDA</em>, <em>187</em>, 107808. (<a
href="https://doi.org/10.1016/j.csda.2023.107808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantile varying coefficient (VC) model can flexibly capture dynamical patterns of regression coefficients . In addition, due to the quantile check loss function, it is robust against outliers and heavy-tailed distributions of the response variable, and can provide a more comprehensive picture of modeling via exploring the conditional quantiles of the response variable. Although extensive studies have been conducted to examine variable selection for the high-dimensional quantile varying coefficient models, the Bayesian analysis has been rarely developed. The Bayesian regularized quantile varying coefficient model has been proposed to incorporate robustness against data heterogeneity while accommodating the non-linear interactions between the effect modifier and predictors. Selecting important varying coefficients can be achieved through Bayesian variable selection. Incorporating the multivariate spike-and-slab priors further improves performance by inducing exact sparsity . The Gibbs sampler has been derived to conduct efficient posterior inference of the sparse Bayesian quantile VC model through Markov chain Monte Carlo (MCMC). The merit of the proposed model in selection and estimation accuracy over the alternatives has been systematically investigated in simulation under specific quantile levels and multiple heavy-tailed model errors. In the case study, the proposed model leads to identification of biologically sensible markers in a non-linear gene-environment interaction study using the NHS data.},
  archive      = {J_CSDA},
  author       = {Fei Zhou and Jie Ren and Shuangge Ma and Cen Wu},
  doi          = {10.1016/j.csda.2023.107808},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107808},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The bayesian regularized quantile varying coefficient model},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reparameterization of extreme value framework for improved
bayesian workflow. <em>CSDA</em>, <em>187</em>, 107807. (<a
href="https://doi.org/10.1016/j.csda.2023.107807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using Bayesian methods for extreme value analysis offers an alternative to frequentist ones, with several advantages such as easily dealing with parametric uncertainty or studying irregular models. However, computations can be challenging and the efficiency of algorithms can be altered by poor parametrization choices. The focus is on the Poisson process characterization of univariate extremes and outline two key benefits of an orthogonal parameterization. First, Markov chain Monte Carlo convergence is improved when applied on orthogonal parameters. This analysis relies on convergence diagnostics computed on several simulations. Second, orthogonalization also helps deriving Jeffreys and penalized complexity priors, and establishing posterior propriety thereof. The proposed framework is applied to return level estimation of Garonne flow data (France).},
  archive      = {J_CSDA},
  author       = {Théo Moins and Julyan Arbel and Stéphane Girard and Anne Dutfoy},
  doi          = {10.1016/j.csda.2023.107807},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107807},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Reparameterization of extreme value framework for improved bayesian workflow},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On graphical models and convex geometry. <em>CSDA</em>,
<em>187</em>, 107800. (<a
href="https://doi.org/10.1016/j.csda.2023.107800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixture-model of beta distributions framework is introduced to identify significant correlations among P features when P is large. The method relies on theorems in convex geometry, which are used to show how to control the error rate of edge detection in graphical models . The proposed ‘betaMix’ method does not require any assumptions about the network structure, nor does it assume that the network is sparse. The results hold for a wide class of data-generating distributions that include light-tailed and heavy-tailed spherically symmetric distributions. The results are robust for sufficiently large sample sizes and hold for non-elliptically-symmetric distributions.},
  archive      = {J_CSDA},
  author       = {Haim Bar and Martin T. Wells},
  doi          = {10.1016/j.csda.2023.107800},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107800},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On graphical models and convex geometry},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Additive dynamic models for correcting numerical model
outputs. <em>CSDA</em>, <em>187</em>, 107799. (<a
href="https://doi.org/10.1016/j.csda.2023.107799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical air quality models are pivotal for the prediction and assessment of air pollution, but numerical model outputs may be systematically biased. An additive dynamic model is proposed to correct large-scale raw model outputs using data from other sources, including readings collected at ground monitoring networks and weather outputs from other numerical models. An additive partially linear model specification is employed for the nonlinear relationships between air pollutants and covariates . In addition, a multi-resolution basis function approximate is proposed to capture the different small-scale variations of biases, and a discretized stochastic integro-differential equation is constructed to characterize the dynamic evolution of the random coefficients at each spatial resolution. An expectation-maximization algorithm is developed for parameter estimation and a multi-resolution ensemble-based scheme is embedded to accelerate the computation. For statistical inference , a conditional simulation technique is applied to quantify the uncertainty of parameter estimates and bias correction results. The proposed approach is used to correct the biased raw outputs of PM 2.5 from the Community Multiscale Air Quality (CMAQ) system for China&#39;s Beijing-Tianjin-Hebei region. Our method improves the root mean squared error and continuous rank probability score by 43.70\% and 34.76\%, respectively. Compared to other statistical methods under different metrics, our model has advantages in both correction accuracy and computational efficiency.},
  archive      = {J_CSDA},
  author       = {Yewen Chen and Xiaohui Chang and Fangzhi Luo and Hui Huang},
  doi          = {10.1016/j.csda.2023.107799},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107799},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Additive dynamic models for correcting numerical model outputs},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust estimation for functional quadratic regression
models. <em>CSDA</em>, <em>187</em>, 107798. (<a
href="https://doi.org/10.1016/j.csda.2023.107798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional quadratic regression models postulate a polynomial relationship rather than a linear one between a scalar response and a functional covariate . As in functional linear regression, vertical and especially high–leverage outliers may affect the classical estimators. For that reason, providing reliable estimators in such situations is an important issue. Taking into account that the functional polynomial model is equivalent to a regression model that is a polynomial of the same order in the functional principal component scores of the predictor processes, our proposal combines robust estimators of the principal directions with robust regression estimators based on a bounded loss function and a preliminary residual scale estimator . Fisher–consistency of the proposed method is derived under mild assumptions. Consistency, asymptotic robustness as well as an expression for the influence function of the related functionals are derived when the covariates have a finite–dimensional expansion. The results of a numerical study show the benefits of the robust proposal over the one based on sample principal directions and least squares for the considered contaminating scenarios. The usefulness of the proposed approach is also illustrated through the analysis of a real data set which reveals that when the potential outliers are removed the classical method behaves very similarly to the robust one computed with all the data.},
  archive      = {J_CSDA},
  author       = {Graciela Boente and Daniela Parada},
  doi          = {10.1016/j.csda.2023.107798},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107798},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust estimation for functional quadratic regression models},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformations in semi-parametric bayesian synthetic
likelihood. <em>CSDA</em>, <em>187</em>, 107797. (<a
href="https://doi.org/10.1016/j.csda.2023.107797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian synthetic likelihood (BSL) is an established method for performing approximate Bayesian inference when the likelihood function is intractable. In synthetic likelihood methods, the likelihood function is approximated parametrically via model simulations, and then standard likelihood-based techniques are used to perform inference. The Gaussian synthetic likelihood estimator has become ubiquitous in BSL literature, primarily for its simplicity and ease of implementation. However, it is often too restrictive and may lead to poor posterior approximations . Recently, a more flexible semi-parametric Bayesian synthetic likelihood (semiBSL) estimator has been introduced, which is significantly more robust to irregularly distributed summary statistics. A number of extensions to semiBSL are proposed. First, even more flexible estimators of the marginal distributions are considered, using transformation kernel density estimation . Second, whitening semiBSL (wsemiBSL) is proposed – a method to significantly improve the computational efficiency of semiBSL. wsemiBSL uses an approximate whitening transformation to decorrelate summary statistics at each algorithm iteration . The methods developed herein significantly improve the versatility and efficiency of BSL algorithms.},
  archive      = {J_CSDA},
  author       = {Jacob W. Priddle and Christopher Drovandi},
  doi          = {10.1016/j.csda.2023.107797},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107797},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Transformations in semi-parametric bayesian synthetic likelihood},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate distributional stochastic frontier models.
<em>CSDA</em>, <em>187</em>, 107796. (<a
href="https://doi.org/10.1016/j.csda.2023.107796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary objective of Stochastic Frontier (SF) Analysis is the deconvolution of the estimated composed error terms into noise and inefficiency. Thus, getting unbiased estimates of the composed error terms should be of uttermost importance. Assuming a parametric production function (e.g. Cobb-Douglas, Translog, etc.), homoscedastic noise and (in)efficiency are restrictions, which can counteract this objective. In order to overcome these three restrictions, the SF model can be cast into the framework of distributional regression models. Within this model class, all parameters of the output (or cost) distribution can be modelled as functions of covariates via smoothers. Consequently, non-linear, random and spatial effects can be included with ease. The Distributional Stochastic Frontier Model ( DSFM ) is introduced. Furthermore, the model is extended to allow for the production of several outputs, which may be dependent. The multivariate distribution of the outputs (or costs) is modeled utilizing copulas . The serviceability of the approach is demonstrated via a Monte Carlo simulation . Additionally, the applicability of the proposed method is showcased by estimating the production of palm oil and rubber in Indonesia.},
  archive      = {J_CSDA},
  author       = {Rouven Schmidt and Thomas Kneib},
  doi          = {10.1016/j.csda.2023.107796},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107796},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multivariate distributional stochastic frontier models},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The tenets of quantile-based inference in bayesian models.
<em>CSDA</em>, <em>187</em>, 107795. (<a
href="https://doi.org/10.1016/j.csda.2023.107795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference can be extended to probability distributions defined in terms of their inverse distribution function, i.e. their quantile function. This applies to both prior and likelihood. Quantile-based likelihood is useful in models with sampling distributions which lack an explicit probability density function . Quantile-based prior allows for flexible distributions to express expert knowledge. The principle of quantile-based Bayesian inference is demonstrated in the univariate setting with a Govindarajulu likelihood, as well as in a parametric quantile regression , where the error term is described by a quantile function of a Flattened Skew-Logistic distribution.},
  archive      = {J_CSDA},
  author       = {Dmytro Perepolkin and Benjamin Goodrich and Ullrika Sahlin},
  doi          = {10.1016/j.csda.2023.107795},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107795},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The tenets of quantile-based inference in bayesian models},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed community detection algorithm for large scale
networks under stochastic block models. <em>CSDA</em>, <em>187</em>,
107794. (<a href="https://doi.org/10.1016/j.csda.2023.107794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection for large scale networks is of great importance in modern data analysis. In this work, we develop a distributed spectral clustering algorithm to handle this task. Specifically, we distribute a certain number of pilot network nodes on the master server and the others on worker servers. A spectral clustering algorithm is first conducted on the master to select pseudo centers. Next, the indexes of the pseudo centers are broadcasted to workers to complete the distributed community detection task using an SVD (singular value decomposition) type algorithm. The proposed distributed algorithm has three advantages. First, the communication cost is low, since only the indexes of pseudo centers are communicated. Second, no further iterative algorithm is needed on workers while a “one-shot” computation suffices. Third, both the computational complexity and the storage requirements are much lower compared to using the whole adjacency matrix . We develop a Python package DCD (The Python package is provided in https://github.com/Ikerlz/dcd .) to implement the distributed algorithm on a Spark system and establish theoretical properties with respect to the estimation accuracy and mis-clustering rates under the stochastic block model. Experiments on a variety of synthetic and empirical datasets are carried out to further illustrate the advantages of the methodology.},
  archive      = {J_CSDA},
  author       = {Shihao Wu and Zhe Li and Xuening Zhu},
  doi          = {10.1016/j.csda.2023.107794},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107794},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A distributed community detection algorithm for large scale networks under stochastic block models},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of projection pursuit regression via alternating
linearization. <em>CSDA</em>, <em>187</em>, 107793. (<a
href="https://doi.org/10.1016/j.csda.2023.107793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The projection pursuit regression (PPR) has played an important role in statistical modeling . It can be used both as a data model for statistical interpretation and as an algorithmic model for approximating general non-parametric regressions. Existing estimation methods of PPR usually involve complicated minimization in order to achieve desired efficiency under general settings. This paper presents an algorithm by alternatively linearizing the estimation loss function, referred to as aPPR hereafter, which is easy to implement. The asymptotic theory is also established for both the PPR data model and the algorithmic model. Numerical performance of aPPR in model estimation and model interpretation is demonstrated through simulations and real data analysis.},
  archive      = {J_CSDA},
  author       = {Xin Tan and Haoran Zhan and Xu Qin},
  doi          = {10.1016/j.csda.2023.107793},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107793},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of projection pursuit regression via alternating linearization},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian analysis of longitudinal data via empirical
likelihood. <em>CSDA</em>, <em>187</em>, 107785. (<a
href="https://doi.org/10.1016/j.csda.2023.107785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal data consists of repeated observations that are typically correlated, which makes the likelihood-based inference challenging. This limits the use of Bayesian methods for longitudinal data in many general situations. To address this issue, empirical likelihood is used to develop a fully Bayesian method for analyzing longitudinal data based on a set of moment equations parallel to the form of generalized estimating equations. It is demonstrated in the context of two popular priors for Bayesian inference and regularization , the Laplace prior and the horseshoe prior. The proposed Bayesian shrinkage method performs well in both estimation accuracy and variable selection, while also providing a full quantification of uncertainty. The method is illustrated using a yeast cell-cycle microarray time course gene expression data set .},
  archive      = {J_CSDA},
  author       = {Jiangrong Ouyang and Howard Bondell},
  doi          = {10.1016/j.csda.2023.107785},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107785},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian analysis of longitudinal data via empirical likelihood},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partial sufficient variable screening with categorical
controls. <em>CSDA</em>, <em>187</em>, 107784. (<a
href="https://doi.org/10.1016/j.csda.2023.107784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable screening as a fast and effective dimension reduction tool plays an important role in analyzing ultrahigh dimensional data. While a very large number of actual datasets contain both continuous and categorical variables , existing methods are mostly designed for continuous data. Partial sufficient variable screening, which aims to reduce the predictive set of primary interest without loss of regression information in the presence of some control variables, is proposed with theoretical guarantees. Specifically, for regression analyses involving mixed types of predictors, variable screening is approached under the notion of sufficiency by constraining the reduction of the continuous variables through the subpopulations identified by the categorical variables . The effectiveness of the proposed method is demonstrated through simulation studies encompassing a variety of regression and classification models , and an application in prognostic gene screening for diffuse large-B-cell lymphoma.},
  archive      = {J_CSDA},
  author       = {Chenlu Ke and Wei Yang and Qingcong Yuan and Lu Li},
  doi          = {10.1016/j.csda.2023.107784},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107784},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Partial sufficient variable screening with categorical controls},
  volume       = {187},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian predictive modeling of multi-source multi-way data.
<em>CSDA</em>, <em>186</em>, 107783. (<a
href="https://doi.org/10.1016/j.csda.2023.107783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian approach to predict a continuous or binary outcome from data that are collected from multiple sources with a multi-way (i.e., multidimensional tensor) structure is described. As a motivating example, molecular data from multiple &#39;omics sources, each measured over multiple developmental time points , as predictors of early-life iron deficiency (ID) in a rhesus monkey model are considered. The method uses a linear model with a low-rank structure on the coefficients to capture multi-way dependence and model the variance of the coefficients separately across each source to infer their relative contributions. Conjugate priors facilitate an efficient Gibbs sampling algorithm for posterior inference, assuming a continuous outcome with normal errors or a binary outcome with a probit link. Simulations demonstrate that the model performs as expected in terms of misclassification rates and correlation of estimated coefficients with true coefficients, with large gains in performance by incorporating multi-way structure and modest gains when accounting for differing signal sizes across the different sources. Moreover, it provides robust classification of ID monkeys for the motivating application.},
  archive      = {J_CSDA},
  author       = {Jonathan Kim and Brian J. Sandri and Raghavendra B. Rao and Eric F. Lock},
  doi          = {10.1016/j.csda.2023.107783},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107783},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian predictive modeling of multi-source multi-way data},
  volume       = {186},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locally sparse quantile estimation for a partially
functional interaction model. <em>CSDA</em>, <em>186</em>, 107782. (<a
href="https://doi.org/10.1016/j.csda.2023.107782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis has been extensively conducted. In this study, we consider a partially functional model, under which some covariates are scalars and have linear effects, while some other variables are functional and have unspecified nonlinear effects . Significantly advancing from the existing literature, we consider a model with interactions between the functional and scalar covariates . To accommodate long-tailed error distributions which are not uncommon in data analysis, we adopt the quantile technique for estimation. To achieve more interpretable estimation, and to accommodate many practical settings, we assume that the functional covariate effects are locally sparse (that is, there exist subregions on which the effects are exactly zero), which naturally leads to a variable/model selection problem. We propose respecting the “main effect, interaction” hierarchy, which postulates that if a subregion has a nonzero effect in an interaction term, then its effect has to be nonzero in the corresponding main functional effect. For estimation, identification of local sparsity , and respect of the hierarchy, we propose a penalization approach. An effective computational algorithm is developed, and the consistency properties are rigorously established under mild regularity conditions . Simulation shows the practical effectiveness of the proposed approach. The analysis of the Tecator data further demonstrates its practical applicability. Overall, this study can deliver a novel and practically useful model and a statistically and numerically satisfactory estimation approach.},
  archive      = {J_CSDA},
  author       = {Weijuan Liang and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1016/j.csda.2023.107782},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107782},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Locally sparse quantile estimation for a partially functional interaction model},
  volume       = {186},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighborhood-based cross fitting approach to treatment
effects with high-dimensional data. <em>CSDA</em>, <em>186</em>, 107780.
(<a href="https://doi.org/10.1016/j.csda.2023.107780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data are increasingly popular in various physical, biological and social disciplines. A common existing approach of repeatedly splitting data was suggested to address the overfitting problem in high-dimensional statistics, however it is computationally expensive in high dimensions . A computationally efficient data splitting method is proposed and referred to as Neighborhood-Based Cross Fitting (NBCF) double machine learning in causal inference for structural causal models with high-dimensional data. The proposed method deals well with the problem of post-selection bias in causal inference in the presence of high-dimensional confounding . It provides an equivalent basis in unbiased estimation as repeated data splitting, which is suggested to expand the complexity scope of function class by empirical process methods. Numerical simulation studies were conducted to demonstrate that the proposed neighborhood-based approach is not only more computationally efficient than the existing sample splitting methods, but also better in bias reduction compared with other existing methods. Under certain conditions, simulation results further showed that the proposed estimators are asymptotically unbiased and normally distributed, which allows construction of valid confidence intervals. The practical application of NBCF is illustrated with a real dataset.},
  archive      = {J_CSDA},
  author       = {Oluwagbenga David Agboola and Han Yu},
  doi          = {10.1016/j.csda.2023.107780},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107780},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Neighborhood-based cross fitting approach to treatment effects with high-dimensional data},
  volume       = {186},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online renewable smooth quantile regression. <em>CSDA</em>,
<em>185</em>, 107781. (<a
href="https://doi.org/10.1016/j.csda.2023.107781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns quantile regression for streaming data , where large amounts of data arrive batch by batch. Limited memory and non-smoothness of quantile regression loss all pose challenges in both computation and theoretical development. To address the challenges, we first introduce a convex smooth quantile loss, which is infinitely differentiable and converges to the quantile loss uniformly. Then an online renewable framework is proposed, in which the quantile regression estimator is renewed with current data and summary statistics of historical data. In theory, the estimation consistency and asymptotic normality of the renewable estimator are established without any restriction on the total number of data batches, which leads to the oracle property, and gives theoretical guarantee that the new method is adaptive to the situation where streaming data sets arrive perpetually. Numerical experiments on both synthetic and real data verify the theoretical results and illustrate the good performance of the new method.},
  archive      = {J_CSDA},
  author       = {Xiaofei Sun and Hongwei Wang and Chao Cai and Mei Yao and Kangning Wang},
  doi          = {10.1016/j.csda.2023.107781},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107781},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Online renewable smooth quantile regression},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypothesis testing on compound symmetric structure of
high-dimensional covariance matrix. <em>CSDA</em>, <em>185</em>, 107779.
(<a href="https://doi.org/10.1016/j.csda.2023.107779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the population covariance matrix is an important topic in multivariate statistical analysis . Owing to the difficulty of establishing the central limit theorem for the test statistic based on sample covariance matrix , in most of the existing literature, it is assumed that the population covariance matrix is bounded in spectral norm as the dimension tends to infinity. Four test statistics are proposed for testing the compound symmetric structure of the population covariance matrix, which has an unbounded spectral norm as the dimension tends to infinity. Three of these tests maintain high power against different kinds of dense alternatives. The asymptotic properties of these statistics are constructed under the null hypothesis and a specific alternative hypothesis. Moreover, extensive simulation studies and an analysis of real data are conducted to evaluate the performance of our proposed tests. The simulation results show that the proposed tests outperform existing methods.},
  archive      = {J_CSDA},
  author       = {Kaige Zhao and Tingting Zou and Shurong Zheng and Jing Chen},
  doi          = {10.1016/j.csda.2023.107779},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107779},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hypothesis testing on compound symmetric structure of high-dimensional covariance matrix},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast calculation of p-values for one-sided
kolmogorov-smirnov type statistics. <em>CSDA</em>, <em>185</em>, 107769.
(<a href="https://doi.org/10.1016/j.csda.2023.107769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel method for computing exact p-values of one-sided statistics from the Kolmogorov-Smirnov family is presented. It covers the Higher Criticism statistic, one-sided weighted Kolmogorov-Smirnov statistics, and the one-sided Berk-Jones statistics. In addition to p-values, the method can also be used for power analysis, finding alpha-level thresholds, and the construction of confidence bands for the empirical distribution function . With its quadratic runtime and numerical stability, the method easily scales to sample sizes in the hundreds of thousands and takes less than a second to run on a sample size of 25,000. This allows practitioners working on large data sets to use exact finite-sample computations instead of approximation schemes . The method is based on a reduction to the boundary-crossing probability of a pure jump stochastic process . FFT convolutions of two different sizes are then used to efficiently propagate the probabilities of the non-crossing paths. This approach has applications beyond statistics, for example in financial risk modeling.},
  archive      = {J_CSDA},
  author       = {Amit Moscovich},
  doi          = {10.1016/j.csda.2023.107769},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107769},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast calculation of p-values for one-sided kolmogorov-smirnov type statistics},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Max-sum test based on spearman’s footrule for
high-dimensional independence tests. <em>CSDA</em>, <em>185</em>,
107768. (<a href="https://doi.org/10.1016/j.csda.2023.107768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing high-dimensional data independence is an essential task of multivariate data analysis in many fields. Typically, the quadratic and extreme value type statistics based on the Pearson correlation coefficient are designed to test dense and sparse alternatives for evaluating high-dimensional independence. However, the two existing popular test methods are sensitive to outliers and are invalid for heavy-tailed error distributions. To overcome these problems, two test statistics, a Spearman&#39;s footrule rank-based quadratic scheme and an extreme value type test for dense and sparse alternatives, are proposed, respectively. Under mild conditions, the large sample properties of the resulting test methods are established. Furthermore, the proposed two test statistics are proved to be asymptotically independent. The max-sum test based on Spearman&#39;s footrule statistic is developed by combining the proposed quadratic with extreme value statistics, and the asymptotic distribution of the resulting statistical test is established. The simulation results demonstrate that the proposed max-sum test performs well in empirical power and robustness, regardless of whether the data is sparse dependence or not. Finally, to illustrate the use of the proposed test method, two empirical examples of Leaf and Parkinson&#39;s disease datasets are provided.},
  archive      = {J_CSDA},
  author       = {Xiangyu Shi and Min Xu and Jiang Du},
  doi          = {10.1016/j.csda.2023.107768},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107768},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Max-sum test based on spearman&#39;s footrule for high-dimensional independence tests},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predictive performance of psychological tests: Is it better
to use items than subscales? <em>CSDA</em>, <em>185</em>, 107767. (<a
href="https://doi.org/10.1016/j.csda.2023.107767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using psychological tests to predict outcomes involves generating a prediction rule from these tests. For multidimensional tests, the standard approach to generate a prediction rule is to use the subscale scores of the test as predictor variables in a regression model to estimate an outcome value for each individual. The coefficients in this model are estimated with ordinary least squares and the predictive performance of the rule is estimated out-of-sample. Recently, studies used the separate items as predictors and estimated the regression coefficients with statistical learning methods to improve the predictive performance of these tests. However, it is unclear whether this approach is always beneficial. The aim is to identify factors that influence the decision whether to use items or subscales in a prediction rule, or letting the data decide between these two types of rules. Several statistical methods are used to derive the prediction rules: ordinary least squares, factor score regression, elastic net , supervised principal components, and principal covariates regression. Data from two empirical studies is analyzed and a simulation study is performed. Overall, results showed that, contrary to earlier findings, item rules are not always better than subscale rules. Subscale rules from elastic net often performed best.},
  archive      = {J_CSDA},
  author       = {Bunga C. Pratiwi and Elise Dusseldorp and Julian D. Karch and Mark de Rooij},
  doi          = {10.1016/j.csda.2023.107767},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107767},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Predictive performance of psychological tests: Is it better to use items than subscales?},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional clustering methods for binary longitudinal data
with temporal heterogeneity. <em>CSDA</em>, <em>185</em>, 107766. (<a
href="https://doi.org/10.1016/j.csda.2023.107766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the analysis of binary longitudinal data , it is of interest to model a dynamic relationship between a response and covariates as a function of time, while also investigating similar patterns of time-dependent interactions. We present a novel generalized varying-coefficient model that accounts for within-subject variability and simultaneously clusters varying-coefficient functions, without restricting the number of clusters nor overfitting the data. In the analysis of a heterogeneous series of binary data, the model extracts population-level fixed effects, cluster-level varying effects, and subject-level random effects. Various simulation studies show the validity and utility of the proposed method to correctly specify cluster-specific varying-coefficients when the number of clusters is unknown. The proposed method is applied to a heterogeneous series of binary data in the German Socioeconomic Panel (GSOEP) study, where we identify three major clusters demonstrating the different varying effects of socioeconomic predictors as a function of age on the working status.},
  archive      = {J_CSDA},
  author       = {Jinwon Sohn and Seonghyun Jeong and Young Min Cho and Taeyoung Park},
  doi          = {10.1016/j.csda.2023.107766},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107766},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Functional clustering methods for binary longitudinal data with temporal heterogeneity},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mediation analysis for high-dimensional mediators and
outcomes with an application to multimodal imaging data. <em>CSDA</em>,
<em>185</em>, 107765. (<a
href="https://doi.org/10.1016/j.csda.2023.107765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal neuroimaging data have attracted increasing attention for brain research. An integrated analysis of multimodal neuroimaging data and behavioral or clinical measurements provides a promising approach for comprehensively and systematically investigating the underlying neural mechanisms of different phenotypes. However, such an integrated data analysis is intrinsically challenging due to the complex interactive relationships between the multimodal multivariate imaging variables. To address this challenge, a novel m ultivariate-mediator and m ultivariate- o utcome mediation model ( MMO ) is proposed to simultaneously extract the latent systematic mediation patterns and estimate the mediation effects based on a dense bi-cluster graph approach. A computationally efficient algorithm is developed for dense bi-cluster structure estimation and inference to identify the mediation patterns with multiple testing correction. The performance of the proposed method is evaluated by an extensive simulation analysis with comparison to the existing methods. The results show that MMO performs better in terms of both the false discovery rate and sensitivity compared to existing models. The MMO is applied to a multimodal imaging dataset from the Human Connectome Project to investigate the effect of systolic blood pressure on whole-brain imaging measures for the regional homogeneity of the blood oxygenation level-dependent signal through the cerebral blood flow.},
  archive      = {J_CSDA},
  author       = {Zhiwei Zhao and Chixiang Chen and Bhim Mani Adhikari and L. Elliot Hong and Peter Kochunov and Shuo Chen},
  doi          = {10.1016/j.csda.2023.107765},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107765},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Mediation analysis for high-dimensional mediators and outcomes with an application to multimodal imaging data},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised mixture estimation via approximate maximum
likelihood based on the cramér - von mises distance. <em>CSDA</em>,
<em>185</em>, 107764. (<a
href="https://doi.org/10.1016/j.csda.2023.107764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture distributions with dynamic weights are an efficient way of modeling loss data characterized by heavy tails. However, maximum likelihood estimation of this family of models is difficult, mostly because of the need to evaluate numerically an intractable normalizing constant. In such a setup, simulation-based estimation methods are an appealing alternative. The approximate maximum likelihood estimation (AMLE) approach is employed. It is a general method that can be applied to mixtures with any component densities, as long as simulation is feasible. The focus is on the dynamic lognormal-generalized Pareto distribution , and the Cramér - von Mises distance is used to measure the discrepancy between observed and simulated samples. After deriving the theoretical properties of the estimators, a hybrid procedure is developed, where standard maximum likelihood is first employed to determine the bounds of the uniform priors required as input for AMLE. Simulation experiments and two real-data applications suggest that this approach yields a major improvement with respect to standard maximum likelihood estimation.},
  archive      = {J_CSDA},
  author       = {Marco Bee},
  doi          = {10.1016/j.csda.2023.107764},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107764},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Unsupervised mixture estimation via approximate maximum likelihood based on the cramér - von mises distance},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic covariance estimation via predictive wishart process
with an application on brain connectivity estimation. <em>CSDA</em>,
<em>185</em>, 107763. (<a
href="https://doi.org/10.1016/j.csda.2023.107763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling the complex dependence in multivariate time series data is a fundamental problem in statistics and machine learning . Traditionally, the task has been approached with methods such as multivariate autoregressive models and multivariate generalized autoregressive conditional heteroskedasticity models, and Gaussian process based methods are recently becoming popular by leveraging the flexibility of non-parametric learning. However, few methods exist that directly model the dynamics of the covariance matrices except generalized Wishart process ( GWP GWP ), and even the generalized Wishart process is limited with applications on small dataset due to the extremely high computational capacity induced by multiple Gaussian processes. In this regard, a novel stochastic process named as Predictive Wishart Process ( PWP PWP ) is proposed, which provides a collection of positive semi-definite random matrices indexed by input variables. The PWP PWP projects process realizations of GWP GWP to a lower dimensional subspace to efficiently estimate every GWP GWP . The theoretical properties of it are examined, and both Bayesian inference and efficient variational expectation maximization are explored in relation to it. Moreover, the PWP PWP is empirically tested on synthetically generated time-series data to validate competitive reconstructive performance and efficient predictive performance , and applied on a large-scale real functional magnetic resonance imaging (fMRI) dataset from Human Connectome Project (HCP) to demonstrate its practicality. A thorough statistical analysis with visualizations is conducted on the brain connectivity, and also a PWP-based multi-task learning framework is proposed to extract meaningful features from individual fMRIs.},
  archive      = {J_CSDA},
  author       = {Rui Meng and Fan Yang and Won Hwa Kim},
  doi          = {10.1016/j.csda.2023.107763},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107763},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dynamic covariance estimation via predictive wishart process with an application on brain connectivity estimation},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural networks for parameter estimation in intractable
models. <em>CSDA</em>, <em>185</em>, 107762. (<a
href="https://doi.org/10.1016/j.csda.2023.107762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal is to use deep learning models to estimate parameters in statistical models when standard likelihood estimation methods are computationally infeasible. For instance, inference for max-stable processes is exceptionally challenging even with small datasets, but simulation is straightforward. Data from model simulations are used to train deep neural networks and learn statistical parameters from max-stable models. The proposed neural network-based method provides a competitive alternative to current approaches, as demonstrated by considerable accuracy and computational time improvements. It serves as a proof of concept for deep learning in statistical parameter estimation and can be extended to other estimation problems.},
  archive      = {J_CSDA},
  author       = {Amanda Lenzi and Julie Bessac and Johann Rudi and Michael L. Stein},
  doi          = {10.1016/j.csda.2023.107762},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107762},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Neural networks for parameter estimation in intractable models},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of multivariate tail quantities. <em>CSDA</em>,
<em>185</em>, 107761. (<a
href="https://doi.org/10.1016/j.csda.2023.107761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For d ≥ 2 d≥2 risk variables, three methods have been proposed to estimate the multivariate tail quantities, including multivariate tail probabilities, tail dependence functions and tail quantile sets. The methods are based on weak assumptions on the joint tails of the copulas of the d variables. The first method is developed based on the tail expansion of copula along different directions to the joint upper or lower orthant. The latter two methods are based on the asymptotic expansion of a family of tail-weighted functions defined from the copula. Extensive simulation experiments are conducted to evaluate and compare the three methods under different scenarios. The simulation results show that the methods yield accurate estimates of the tail quantities and effectively distinguish the tail properties, such as reflection asymmetry, permutation asymmetry, and heterogeneous tail dependence. One data example is presented to illustrate the applicability of the proposed methods as inference and diagnostic tools.},
  archive      = {J_CSDA},
  author       = {Xiaoting Li and Harry Joe},
  doi          = {10.1016/j.csda.2023.107761},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107761},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of multivariate tail quantities},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating inference for stochastic kinetic models.
<em>CSDA</em>, <em>185</em>, 107760. (<a
href="https://doi.org/10.1016/j.csda.2023.107760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic kinetic models (SKMs) are increasingly used to account for the inherent stochasticity exhibited by interacting populations of species in areas such as epidemiology, population ecology and systems biology . Species numbers are modelled using a continuous-time stochastic process , and, depending on the application area of interest, this will typically take the form of a Markov jump process or an Itô diffusion process . Widespread use of these models is typically precluded by their computational complexity . In particular, performing exact fully Bayesian inference in either modelling framework is challenging due to the intractability of the observed data likelihood , necessitating the use of computationally intensive techniques such as particle Markov chain Monte Carlo (particle MCMC). It is proposed to increase the computational and statistical efficiency of this approach by leveraging the tractability of an inexpensive surrogate derived directly from either the jump or diffusion process. The surrogate is used in three ways: in the design of a gradient-based parameter proposal, to construct an appropriate bridge and in the first stage of a delayed-acceptance step. The resulting approach, which exactly targets the posterior of interest, offers substantial gains in efficiency over a standard particle MCMC implementation.},
  archive      = {J_CSDA},
  author       = {Tom E. Lowe and Andrew Golightly and Chris Sherlock},
  doi          = {10.1016/j.csda.2023.107760},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107760},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Accelerating inference for stochastic kinetic models},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric model averaging method for survival
probability predictions of patients. <em>CSDA</em>, <em>185</em>,
107759. (<a href="https://doi.org/10.1016/j.csda.2023.107759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical and clinical research, predicting the survival probabilities for patients is a core task. Accurate survival probability predictions can help physicians make better treatments or prevention plans for patients. A novel semiparametric proportional hazards model averaging prediction technique is introduced to address this problem. Under the potential partly linear additive structures, the conditional survival probabilities of individuals can be predicted by the weighted averages of submodels which are estimated by maximizing the partial likelihood functions. The selection of weights is a crucial part of model averaging since the weights can affect the accuracy of survival probability prediction. A Brier score type criterion is employed to choose the optimal model averaging weights and the rate of convergence of the selected weights is studied. In addition, the finite sample performance of the proposed method is evaluated via abundant simulation studies. To further illustrate the effectiveness of the proposed approach, the model averaging is applied to heart failure data.},
  archive      = {J_CSDA},
  author       = {Mengyu Li and Xiaoguang Wang},
  doi          = {10.1016/j.csda.2023.107759},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107759},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric model averaging method for survival probability predictions of patients},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exceedance control of the false discovery proportion via
high precision inversion method of berk-jones statistics. <em>CSDA</em>,
<em>185</em>, 107758. (<a
href="https://doi.org/10.1016/j.csda.2023.107758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exceedance control of the false discovery proportion (FDP) can provide an interpretable method for addressing the variability in the false discovery proportion estimates. Exceedance control of FDP can be viewed as constructing a confidence interval for FDP and as such inverting a hypothesis test is a viable method for achieving exceedance control. A novel powerful approach for exceedance control is presented based on using a directional Berk-Jones goodness-of-fit statistic. The approach employs a high-precision implementation procedure to accurately compute confidence envelopes for FDP. The procedure is compared against other methods and generalized to include other goodness-of-fit statistics that follow an isotropy condition.},
  archive      = {J_CSDA},
  author       = {Jeffrey C. Miecznikowski and Jiefei Wang},
  doi          = {10.1016/j.csda.2023.107758},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107758},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Exceedance control of the false discovery proportion via high precision inversion method of berk-jones statistics},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Indicator-based bayesian variable selection for gaussian
process models in computer experiments. <em>CSDA</em>, <em>185</em>,
107757. (<a href="https://doi.org/10.1016/j.csda.2023.107757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian process (GP) models are commonly used in the analysis of computer experiments. Variable selection in GP models is of significant scientific interest but existing solutions remain unsatisfactory. For each variable in a GP model, there are two potential effects with different implications: one is on the mean function, and the other is on the covariance function . However, most of the existing research on variable selection for GP models has focused only on one of the effects. To tackle this problem, we propose an indicator-based Bayesian variable selection procedure to take into account the effects from both the mean and covariance functions. A variable is defined to be inactive if both effects are not significant, and an indicator is used to represent the variable being active or not. For active variables, the proposed method adopts different prior assumptions to capture the two effects. The performance of the proposed method is evaluated by both simulations and real applications in computer experiments.},
  archive      = {J_CSDA},
  author       = {Fan Zhang and Ray-Bing Chen and Ying Hung and Xinwei Deng},
  doi          = {10.1016/j.csda.2023.107757},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107757},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Indicator-based bayesian variable selection for gaussian process models in computer experiments},
  volume       = {185},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerate the warm-up stage in the lasso computation via a
homotopic approach. <em>CSDA</em>, <em>184</em>, 107747. (<a
href="https://doi.org/10.1016/j.csda.2023.107747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In optimization of the least absolute shrinkage and selection operator (Lasso) problem, the fastest algorithm has a convergence rate of O ( 1 / ϵ ) O(1/ϵ) . This polynomial order of 1 / ϵ 1/ϵ is caused by the undesirable behavior of the absolute function at the origin. To expedite the convergence, an algorithm called homotopy shrinkage yielding (HOSKY) is proposed. It helps expedite the warm-up stage of the existing algorithms. With the acceleration by HOSKY in the warm-up stage, one can get a provable convergence rate lower than O ( 1 / ϵ ) O(1/ϵ) . The main idea of the proposed HOSKY algorithm is to use a sequence of surrogate functions to approximate the ℓ 1 ℓ1 penalty that is used in Lasso. This sequence of surrogate functions, on the one hand, gets closer and closer to the ℓ 1 ℓ1 penalty; on the other hand, they are strictly convex and well-conditioned, which enables a provable exponential rate of convergence by gradient-based approaches. As the proof shows, the convergence rate of the HOSKY algorithm is O ( [ log ⁡ ( 1 / ϵ w ) ] 2 ) O([log⁡(1/ϵw)]2) , where ϵ w ϵw is the precision used in the warm-up stage ( ϵ w ↛ 0 ϵw↛0 ). Additionally, the numerical simulations also show that HOSKY empirically performs better in the warm-up stage and accelerates the overall convergence rate.},
  archive      = {J_CSDA},
  author       = {Yujie Zhao and Xiaoming Huo},
  doi          = {10.1016/j.csda.2023.107747},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107747},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Accelerate the warm-up stage in the lasso computation via a homotopic approach},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to implement signed-rank wilcox.test() type procedures
when a center of symmetry is unknown. <em>CSDA</em>, <em>184</em>,
107746. (<a href="https://doi.org/10.1016/j.csda.2023.107746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim is twofold: (1) to indicate that the one-sample Wilcoxon signed rank test cannot be used directly when a center of symmetry is unknown; and (2) to propose and examine correct schemes for applying the Wilcoxon signed rank test with an estimated center of symmetry. It turns out that the Wilcoxon signed rank test and the sign test for symmetry do not provide valued outputs, when unknown centers of symmetry are estimated using underlying data. In such scenarios, these tests are not null-distribution-free and break down completely even based on samples with large numbers of observations. Theoretical propositions are shown to propose a simple correction of the corresponding R built-in function, employing p-value-based procedures. To perform the proposed algorithms, we develop new customized procedures for estimating the integrated squares of densities, probability weighted moments and special values of density functions. It is shown that the proposed testing strategies have Type I error rates under good control as well as exhibit high and stable power characteristics. The proposed algorithms can be applied for modifying Wilcoxon tests type procedures in different statistical software.},
  archive      = {J_CSDA},
  author       = {Albert Vexler and Xinyu Gao and Jiaojiao Zhou},
  doi          = {10.1016/j.csda.2023.107746},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107746},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {How to implement signed-rank wilcox.test() type procedures when a center of symmetry is unknown},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional principal component analysis for partially
observed elliptical process. <em>CSDA</em>, <em>184</em>, 107745. (<a
href="https://doi.org/10.1016/j.csda.2023.107745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robust principal component estimators for partially observed functional data with heavy-tail behaviors are presented, where sample trajectories are collected over individual-specific subintervals . The method considers partially sampled trajectories as the elliptical process filtered by the missing indicator process and implements robust functional principal component analysis under this framework. The proposed method is computationally efficient and straightforward by estimating the robust correlation function through pairwise covariance computation combined with M-estimation. The asymptotic consistency of the estimators is established under general conditions. The simulation studies demonstrate the superior performance of the method in the approximation of the subspace of data and reconstruction of full trajectories. The proposed method is then applied to hourly monitored air pollutant data containing anomaly trajectories with random missing segments.},
  archive      = {J_CSDA},
  author       = {Yeonjoo Park and Hyunsung Kim and Yaeji Lim},
  doi          = {10.1016/j.csda.2023.107745},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107745},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Functional principal component analysis for partially observed elliptical process},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bootstrapping the transformed goodness-of-fit test on
heavy-tailed GARCH models. <em>CSDA</em>, <em>184</em>, 107744. (<a
href="https://doi.org/10.1016/j.csda.2023.107744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the bootstrap inference on the goodness-of-fit test for generalized autoregressive conditional heteroskedastic (GARCH) models. Note that the commonly-used portmanteau tests for model adequacy checking necessarily impose moment conditions on innovations, we hence construct the test on the sample autocorrelations of a bounded transformation of absolute residuals, which are obtained by the least absolute deviation estimation from a fitted GARCH model. Specifically, we employ the empirical distribution function of absolute residuals as the transformation. Thus the corresponding portmanteau tests are applicable for very heavy-tailed innovations with only finite fractional moments. We bootstrap both the estimation equation and sample autocorrelations of transformed residuals to approximate the test statistics. The asymptotic validity of the bootstrap procedure is established. Monte Carlo experiments compare the finite-sample performance of the proposed bootstrap-based test with other existing tests. An empirical analysis of modeling exchange rates illustrates its usefulness.},
  archive      = {J_CSDA},
  author       = {Xuqin Wang and Muyi Li},
  doi          = {10.1016/j.csda.2023.107744},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107744},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bootstrapping the transformed goodness-of-fit test on heavy-tailed GARCH models},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weighted least squares model averaging for accelerated
failure time models. <em>CSDA</em>, <em>184</em>, 107743. (<a
href="https://doi.org/10.1016/j.csda.2023.107743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new model averaging method for the accelerated failure time models with right censored data . A weighted least squares procedure is used to estimate the parameters of candidate models. In this procedure, the candidate models are not required to be nested, and the weights selected by Mallows criterion are not limited to be discrete, which make the proposed method very flexible and general. The asymptotic optimality of the proposed method is proved under some mild conditions. Particularly, it is shown that the optimality remains valid even when the variances of the error terms are estimated and the feasible weighted least squares estimators are averaged. Simulation studies show that the proposed method has better prediction performance than many popular model selection or model averaging methods when all candidate models are misspecified. Finally, an application about primary biliary cirrhosis is provided.},
  archive      = {J_CSDA},
  author       = {Qingkai Dong and Binxia Liu and Hui Zhao},
  doi          = {10.1016/j.csda.2023.107743},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107743},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Weighted least squares model averaging for accelerated failure time models},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating heterogeneous causal effects in observational
studies using small area predictors. <em>CSDA</em>, <em>184</em>,
107742. (<a href="https://doi.org/10.1016/j.csda.2023.107742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The official statistics produced by National Statistical Institutes are mainly used by policy makers to take decisions. In particular, when policy makers and decision takers would like to know the impact of a given policy, it is important to acknowledge the heterogeneity of the treatment effects for different domains. If the domain of interest is small with regard to its sample size, then the evaluator has entered the small area estimation (SAE) dilemma. Based on the modification of the Inverse Propensity Weighting estimator and the traditional small area predictors, new estimators of area specific average treatment effects are proposed for unplanned domains. A robustified version of the predictor against presence of the outliers is also developed. Analytical Mean Squared Error (MSE) estimators of the proposed predictors are derived. These methods provide a tool to map the policy impacts that can help to better target the treatment group(s). The properties of these small area estimators are illustrated by means of a design-based simulation using a real data set where the aim is to study the effects of permanent versus temporary contracts on the economic insecurity of households in different regions of Italy.},
  archive      = {J_CSDA},
  author       = {Setareh Ranjbar and Nicola Salvati and Barbara Pacini},
  doi          = {10.1016/j.csda.2023.107742},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107742},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimating heterogeneous causal effects in observational studies using small area predictors},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small area estimation of general finite-population
parameters based on grouped data. <em>CSDA</em>, <em>184</em>, 107741.
(<a href="https://doi.org/10.1016/j.csda.2023.107741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new model-based approach to small area estimation of general finite-population parameters based on grouped data or frequency data, often available from sample surveys. Grouped data contains information on frequencies of some pre-specified groups in each area, for example, the numbers of households in the income classes. Thus, grouped data provide more detailed insight into small areas than area-level aggregated data. A direct application of the widely used small area methods, such as the Fay–Herriot model for area-level data and nested error regression model for unit-level data, is not appropriate since they are not designed for grouped data. Our novel method adopts the multinomial likelihood function for the grouped data. In order to connect the group probabilities of the multinomial likelihood and the auxiliary variables within the framework of small area estimation, we introduce the unobserved unit-level quantities of interest. They follow a linear mixed model with random intercepts and dispersions after some transformation. Then the probabilities that a unit belongs to the groups can be derived and are used to construct the likelihood function for the grouped data given the random effects. The unknown model parameters (hyperparameters) are estimated by a newly developed Monte Carlo EM algorithm which uses an efficient importance sampling. The empirical best predicts (empirical Bayes estimates) of small area parameters are calculated by a simple Gibbs sampling algorithm . The numerical performance of the proposed method is illustrated based on the model-based and design-based simulations. In the application to the city-level grouped income data of Japan , we complete the patchy maps of the Gini coefficient as well as mean income across the country.},
  archive      = {J_CSDA},
  author       = {Yuki Kawakubo and Genya Kobayashi},
  doi          = {10.1016/j.csda.2023.107741},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107741},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Small area estimation of general finite-population parameters based on grouped data},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subsampling based variable selection for generalized linear
models. <em>CSDA</em>, <em>184</em>, 107740. (<a
href="https://doi.org/10.1016/j.csda.2023.107740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel variable selection method for low-dimensional generalized linear models is introduced. The new approach called AIC OPTimization via STABility Selection (OPT-STABS) repeatedly subsamples the data, minimizes Akaike&#39;s Information Criterion (AIC) over a sequence of nested models for each subsample, and includes in the final model those predictors selected in the minimum AIC model in a large fraction of the subsamples. New methods are also introduced to establish an optimal variable selection cutoff over repeated subsamples. An extensive simulation study examining a variety of proposed variable selection methods shows that, although no single method uniformly outperforms the others in all the scenarios considered, OPT-STABS is consistently among the best-performing methods in most settings while it performs competitively for the rest. This is in contrast to other candidate methods which either have poor performance across the board or exhibit good performance in some settings, but very poor in others. In addition, the asymptotic properties of the OPT-STABS estimator are derived, and its root-n consistency and asymptotic normality are proved. The methods are applied to two datasets involving logistic and Poisson regressions .},
  archive      = {J_CSDA},
  author       = {Marinela Capanu and Mihai Giurcanu and Colin B. Begg and Mithat Gönen},
  doi          = {10.1016/j.csda.2023.107740},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107740},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Subsampling based variable selection for generalized linear models},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial for the 2nd special issue on high-dimensional and
functional data analysis. <em>CSDA</em>, <em>184</em>, 107726. (<a
href="https://doi.org/10.1016/j.csda.2023.107726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CSDA},
  author       = {Jeng-Min Chiou and Frederic Ferraty and Jeff Goldsmith and Debashis Paul and Jian Qing Shi},
  doi          = {10.1016/j.csda.2023.107726},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107726},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Editorial for the 2nd special issue on high-dimensional and functional data analysis},
  volume       = {184},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unifying implementation of stratum (aka strong) orthogonal
arrays. <em>CSDA</em>, <em>183</em>, 107739. (<a
href="https://doi.org/10.1016/j.csda.2023.107739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructions of so-called “strong orthogonal arrays” (SOAs) have been previously proposed. The approaches and notations taken in the different proposals vary widely. The SOAs and their constructions are reviewed using a unifying notation with a simple set of equations. In addition to providing a unifying overview, some constructions are improved, e.g., by enforcing column orthogonality via a bipartite pair matching algorithm where the original constructions pay no attention to column orthogonality . All presented constructions are implemented in the R package SOAs. As an aside, it is argued that “stratum” is a better choice than “strong” for the “S” in the acronym SOAs.},
  archive      = {J_CSDA},
  author       = {Ulrike Grömping},
  doi          = {10.1016/j.csda.2023.107739},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107739},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A unifying implementation of stratum (aka strong) orthogonal arrays},
  volume       = {183},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust variable selection and estimation via adaptive
elastic net s-estimators for linear regression. <em>CSDA</em>,
<em>183</em>, 107730. (<a
href="https://doi.org/10.1016/j.csda.2023.107730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heavy-tailed error distributions and predictors with anomalous values are ubiquitous in high-dimensional regression problems and can seriously jeopardize the validity of statistical analyses if not properly addressed. For more reliable variable selection and prediction under these adverse conditions , adaptive PENSE, a new robust regularized regression estimator , is proposed. Adaptive PENSE yields reliable variable selection and coefficient estimates even under aberrant contamination in the predictors or residuals. It is shown that the adaptive penalty leads to more robust and reliable variable selection than other penalties, particularly in the presence of gross outliers in the predictor space. It is further demonstrated that adaptive PENSE has strong variable selection properties and that it possesses the oracle property even under heavy-tailed errors and without the need to estimate the error scale. Numerical studies on simulated and real data sets highlight the superior finite-sample performance in a vast range of settings compared to other robust regularized estimators in the case of contaminated samples. An R package implementing a fast algorithm for computing the proposed method and additional simulation results are provided in the supplementary materials.},
  archive      = {J_CSDA},
  author       = {David Kepplinger},
  doi          = {10.1016/j.csda.2023.107730},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107730},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust variable selection and estimation via adaptive elastic net S-estimators for linear regression},
  volume       = {183},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tuning parameter selection for penalized estimation via r2.
<em>CSDA</em>, <em>183</em>, 107729. (<a
href="https://doi.org/10.1016/j.csda.2023.107729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tuning parameter selection strategy for penalized estimation is crucial to identify a model that is both interpretable and predictive. However, popular strategies (e.g., minimizing average squared prediction error via cross-validation) tend to select models with more predictors than necessary. A simple yet powerful cross validation strategy is proposed which is based on maximizing the squared correlation between the observed and predicted values, rather than minimizing squared error loss for the purposes of support recovery. The strategy can be applied to all penalized least-squares estimators and, under certain conditions, the metric implicitly performs a bias adjustment named the α -modification. When applied to the Lasso estimator, the α -modification is closely related to the relaxed Lasso estimator. The approach is demonstrated on a functional variable selection problem to identify optimal placement of surface electromyogram sensors to control a robotic hand prosthesis.},
  archive      = {J_CSDA},
  author       = {Julia C. Holter and Jonathan W. Stallrich},
  doi          = {10.1016/j.csda.2023.107729},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107729},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Tuning parameter selection for penalized estimation via r2},
  volume       = {183},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric estimation of conditional cure models for
heavy-tailed distributions and under insufficient follow-up.
<em>CSDA</em>, <em>183</em>, 107728. (<a
href="https://doi.org/10.1016/j.csda.2023.107728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When analyzing time-to-event data, it often happens that some subjects do not experience the event of interest. Survival models that take this feature into account (called ‘cure models’) have been developed in the presence of covariates . However, the nonparametric cure models with covariates , in the current literature, cannot be applied when the follow-up is insufficient, i.e., when the right endpoint of the support of the censoring time is strictly smaller than that of the survival time of the susceptible subjects. New estimators of the conditional cure rate and the conditional survival function are proposed using extrapolation techniques coming from extreme value theory . The proposed methodology can also be used to estimate the conditional survival function when no cure rate is present. The asymptotic normality of the proposed estimators is established and their performances for small samples are shown by means of a simulation study. Their practical applicability is illustrated through the analysis of two short applications with real datasets on the repayment of student bullet loans and the employee&#39;s turnover in a company.},
  archive      = {J_CSDA},
  author       = {Mikael Escobar-Bach and Ingrid Van Keilegom},
  doi          = {10.1016/j.csda.2023.107728},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107728},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric estimation of conditional cure models for heavy-tailed distributions and under insufficient follow-up},
  volume       = {183},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatio-temporal generalized complex covariance models based
on convolution. <em>CSDA</em>, <em>183</em>, 107709. (<a
href="https://doi.org/10.1016/j.csda.2023.107709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling covariance functions , with values on a complex domain, is essential for geostatistical interpolation or stochastic simulation of complex-valued random fields in space or space-time. However, little has been done for complex spatio-temporal modeling. For this aim, the construction of new classes of spatio-temporal complex-valued covariance models , based on convolution , is provided. Indeed, starting from the Lajaunie and Béjaoui models extended to a space-time domain, generalized families of complex models are obtained through the integration with respect to a positive measure. A procedure for fitting the two parts of the spatio-temporal complex models and for defining the density function considered for the integration is also illustrated. The computational details of this procedure are discussed through a case study on a spatio-temporal dataset of sea currents and the performance of these classes of models is assessed.},
  archive      = {J_CSDA},
  author       = {S. De Iaco},
  doi          = {10.1016/j.csda.2023.107709},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107709},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Spatio-temporal generalized complex covariance models based on convolution},
  volume       = {183},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric function-on-function quantile regression
model with dynamic single-index interactions. <em>CSDA</em>,
<em>182</em>, 107727. (<a
href="https://doi.org/10.1016/j.csda.2023.107727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new semiparametric function-on-function quantile regression model with time-dynamic single-index interactions. Our model is very flexible in taking into account of the nonlinear time-dynamic interaction effects of the multivariate longitudinal/functional covariates on the longitudinal response, that most existing quantile regression models for longitudinal data are special cases of our proposed model. We propose to approximate the bivariate nonparametric coefficient functions by tensor product B-splines, and employ a check loss minimization approach to estimate the bivariate coefficient functions and the index parameter vector . Under some mild conditions, we establish the asymptotic normality of the estimated single-index coefficients using projection orthogonalization technique, and obtain the convergence rates of the estimated bivariate coefficient functions. Furthermore, we propose a score test to examine whether there exist interaction effects between the covariates. The finite sample performance of the proposed method is illustrated by Monte Carlo simulations and an empirical data analysis.},
  archive      = {J_CSDA},
  author       = {Hanbing Zhu and Yuanyuan Zhang and Yehua Li and Heng Lian},
  doi          = {10.1016/j.csda.2023.107727},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107727},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric function-on-function quantile regression model with dynamic single-index interactions},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to test the missing data mechanism in a hidden markov
model. <em>CSDA</em>, <em>182</em>, 107723. (<a
href="https://doi.org/10.1016/j.csda.2023.107723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Hidden Markov Model with missing data in the outcome variable is considered. The initial and transition probabilities of the Markov chain and the emission probability of the HMM are allowed to depend on fully observed covariables. Tests for the ignorable and for the MCAR mechanisms are proposed. These tests do not require grouping the individuals by their missing pattern, making them easier to apply in practice. They are based on the estimates of the conditional probabilities of emitting a missing data given the latent state of the Markov chain and some observed covariables. When the ignorable mechanism holds, the conditional probabilities of emitting a missing value are the same for a given value of the observed variables. On the contrary, when the MCAR mechanism holds, these probabilities are all the same. A practical implementation of these tests based on simulations is proposed, along with a presentation of their performances. A real example from piglet farming illustrates their use.},
  archive      = {J_CSDA},
  author       = {Malika Chassan and Didier Concordet},
  doi          = {10.1016/j.csda.2023.107723},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107723},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {How to test the missing data mechanism in a hidden markov model},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density estimation for spherical data using nonparametric
mixtures. <em>CSDA</em>, <em>182</em>, 107715. (<a
href="https://doi.org/10.1016/j.csda.2023.107715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric density estimation is studied for spherical data that may arise in many scientific and practical fields. In particular, nonparametric mixture models based on likelihood maximization are used. A nonparametric mixture has component distributions mixed together with a mixing distribution that is completely unspecified and needs to be determined from data. For mixture components, a two-parameter distribution family can be used, with one parameter as the mixing variable and the other to control the smoothness of the density estimator. For example, the popular von Mises-Fisher distributions can be readily used for this purpose. Numerical studies with various spherical data sets show that the resultant mixture-based density estimators are strong competitors with the best of the other density estimators.},
  archive      = {J_CSDA},
  author       = {Danli Xu and Yong Wang},
  doi          = {10.1016/j.csda.2023.107715},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107715},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Density estimation for spherical data using nonparametric mixtures},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMLSOM: The shrinking maximum likelihood self-organizing
map. <em>CSDA</em>, <em>182</em>, 107714. (<a
href="https://doi.org/10.1016/j.csda.2023.107714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the number of clusters in a dataset is a fundamental issue in data clustering . Many methods have been proposed to solve the problem of selecting the number of clusters, considering it to be a problem with regard to model selection. This paper proposes an efficient algorithm that automatically selects a suitable number of clusters based on a probability distribution model framework. The algorithm includes the following two components. First, a generalization of Kohonen&#39;s self-organizing map (SOM) is introduced. In Kohonen&#39;s SOM, clusters are modeled as mean vectors. In the generalized SOM, each cluster is modeled as a probabilistic distribution and constructed by samples classified based on the likelihood. Second, the dynamically updating method of the SOM structure is introduced. In Kohonen&#39;s SOM, each cluster is tied to a node of a fixed two-dimensional lattice space and learned using neighborhood relations between nodes based on Euclidean distance . The extended SOM defines a graph with clusters as vertices and neighborhood relations as links and updates the graph structure by cutting weakly connected and unnecessary vertex deletions. The weakness of a link is measured using the Kullback–Leibler divergence, and the redundancy of a vertex is measured using the minimum description length . Those extensions make it efficient to determine the appropriate number of clusters. Compared with existing methods, the proposed method is computationally efficient and can accurately select the number of clusters.},
  archive      = {J_CSDA},
  author       = {Ryosuke Motegi and Yoichi Seki},
  doi          = {10.1016/j.csda.2023.107714},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107714},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {SMLSOM: The shrinking maximum likelihood self-organizing map},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imputed quantile tensor regression for near-sited
spatial-temporal data. <em>CSDA</em>, <em>182</em>, 107713. (<a
href="https://doi.org/10.1016/j.csda.2023.107713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern spatial temporal data are collected from sensor networks. Missing data problems are common for this kind of data. Making robust and accurate imputation is important in many applications. There are complex correlations in both spatial and temporal dimensions. Thus, it is even a challenge to model missing spatial-temporal data. In this article, the imputation of missing values is with the help of related covariates . First, we transform the original sensor × time observational matrix to a high order tensor by adding an extra temporal dimension. Then we integrate quantile tensor regression with tensor completion. The objective function includes check loss and nuclear norm penalty. An alternating update algorithm combined with alternating direction method of multipliers (ADMM) is developed to solve the objective function. Theoretical properties of the proposed estimator are investigated. Simulation studies show our proposed method is more robust and can get more accurate imputation results. Real data analysis about Beijing&#39;s PM 2.5 concentration level is conducted to verify the efficiency of the estimation procedure.},
  archive      = {J_CSDA},
  author       = {Jinwen Liang and Wolfgang Karl Härdle and Maozai Tian},
  doi          = {10.1016/j.csda.2023.107713},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107713},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Imputed quantile tensor regression for near-sited spatial-temporal data},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GMM estimation of partially linear additive spatial
autoregressive model. <em>CSDA</em>, <em>182</em>, 107712. (<a
href="https://doi.org/10.1016/j.csda.2023.107712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on studying the estimation method of partially linear additive spatial autoregressive model (PLASARM) by combining both parametric and nonparametric terms. With the nonparametric functions approximated by local linear estimator , the generalized method of moment (GMM) estimators is proposed. The large sample properties of the estimators are derived for the case with a single nonparametric term and extended to an arbitrary number of nonparametric additive terms under some mild conditions. The small sample performance for our estimators is assessed by Monte Carlo simulation . In addition, the proposed method is used to analyze the forces of Chinese housing price.},
  archive      = {J_CSDA},
  author       = {Suli Cheng and Jianbao Chen},
  doi          = {10.1016/j.csda.2023.107712},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107712},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {GMM estimation of partially linear additive spatial autoregressive model},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Composite quantile regression analysis of survival data with
missing cause-of-failure information and its application to breast
cancer clinical trial. <em>CSDA</em>, <em>182</em>, 107711. (<a
href="https://doi.org/10.1016/j.csda.2023.107711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of survival data can be challenging due to the presence of missing data. This paper proposes weighted composite quantile regression (CQR) for estimating a lot of quantile regression (QR) of survival data based on single-index coefficient model (SICM), which is a very general and flexible tool for exploring the relationship between response variable and a set of predictors. The statistical inference for SICM is considered when cause-of-failure information (censored or non-censored) is always observed. However, the cause-of-failure information may be missing at random (MAR) for various reasons. Regression calibration, imputation and inverse probability weighted approaches are applied to deal with the MAR assumption. The asymptotic normalities of the proposed estimators are established. Meanwhile, the oracle property of the variable selection based on adaptive LASSO penalty procedure is conducted. To assess the finite sample performance of the proposed estimators, simulation study with normal error and heavy-tail error are considered. As expected, the CQR estimators perform as good as the least-square estimators for normal error, and are more robust to heavy-tailed error. Finally, a breast cancer real data analysis is carried out to illustrate the proposed methodologies.},
  archive      = {J_CSDA},
  author       = {Yuye Zou and Chengxin Wu},
  doi          = {10.1016/j.csda.2023.107711},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107711},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Composite quantile regression analysis of survival data with missing cause-of-failure information and its application to breast cancer clinical trial},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast estimation of multiple group generalized linear latent
variable models for categorical observed variables. <em>CSDA</em>,
<em>182</em>, 107710. (<a
href="https://doi.org/10.1016/j.csda.2023.107710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computationally efficient method for marginal maximum likelihood estimation of multiple group generalized linear latent variable models for categorical data is introduced. The approach utilizes second-order Laplace approximations of the integrals in the likelihood function. It is demonstrated how second-order Laplace approximations can be utilized highly efficiently for generalized linear latent variable models by considering symmetries that exist for many types of model structures. In a simulation with binary observed variables and four correlated latent variables in four groups, the method has similar bias and mean squared error compared to adaptive Gauss-Hermite quadrature with five quadrature points while substantially improving computational efficiency. An empirical example from a large-scale educational assessment illustrates the accuracy and computational efficiency of the method when compared against adaptive Gauss-Hermite quadrature with three, five, and 13 quadrature points.},
  archive      = {J_CSDA},
  author       = {Björn Andersson and Shaobo Jin and Maoxin Zhang},
  doi          = {10.1016/j.csda.2023.107710},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107710},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast estimation of multiple group generalized linear latent variable models for categorical observed variables},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-sample nonparametric test for proportional reversed
hazards. <em>CSDA</em>, <em>182</em>, 107708. (<a
href="https://doi.org/10.1016/j.csda.2023.107708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few decades, several works have been undertaken in the context of proportional reversed hazard rates (PRHR) but any specific statistical methodology for the PRHR hypothesis is absent in the literature. This paper proposes a two-sample nonparametric test based on two independent samples to verify the PRHR assumption. Based on a consistent U-statistic three statistical methodologies have been developed exploiting U-statistics theory, jackknife empirical likelihood and adjusted jackknife empirical likelihood method . A simulation study has been performed to assess the merit of the proposed test procedure. Finally, the proposed procedure is applied to a data set in the context of brain injury-related biomarkers and a data set related to Duchenne muscular dystrophy.},
  archive      = {J_CSDA},
  author       = {Ruhul Ali Khan},
  doi          = {10.1016/j.csda.2023.107708},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107708},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Two-sample nonparametric test for proportional reversed hazards},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On a nonlinear extension of the principal fitted component
model. <em>CSDA</em>, <em>182</em>, 107707. (<a
href="https://doi.org/10.1016/j.csda.2023.107707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a nonlinear sufficient dimension reduction method called the kernel principal fitted component model using the kernel method under a reproducing kernel Hilbert space . The kernel principal fitted component is a nonlinear extension of the principal fitted component model, and it is found in the theory of mapping low dimensional input space to the higher dimensional feature space so that we can apply well-developed linear methods to the nonlinear dataset. We derive our method coincides with the generalized sliced inverse regression under some mild assumptions and show the dimension reduction subspace extracted from the kernel principal fitted component model is contained in the central class. In the numerical experiments, we present the kernel principal fitted component model with the Gaussian kernel can extract the linear and nonlinear features well for the models from both forward and inverse regression settings. By applying our method to ovarian cancer microarray dataset, we demonstrate the kernel principal fitted component can provide a competitive prediction accuracy and computational efficiency in the high-dimensional classification problem.},
  archive      = {J_CSDA},
  author       = {Jun Song and Kyongwon Kim and Jae Keun Yoo},
  doi          = {10.1016/j.csda.2023.107707},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107707},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On a nonlinear extension of the principal fitted component model},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian semiparametric multivariate density deconvolution
via stochastic rotation of replicates. <em>CSDA</em>, <em>182</em>,
107706. (<a href="https://doi.org/10.1016/j.csda.2023.107706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multivariate density deconvolution , the distribution of a random vector needs to be estimated from replicates contaminated with measurement errors. A novel approach to multivariate deconvolution is proposed by stochastically rotating and stretching or contracting the replicates toward the corresponding true latent values. The method further accommodates conditionally heteroscedastic measurement errors commonly observed in many real data applications. The estimation and inference schemes are developed within a Bayesian framework implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The method&#39;s efficacy is demonstrated empirically through simulation experiments and practically in estimating the long-term joint average intakes of different dietary components from their measurement error-contaminated 24-hour dietary recalls.},
  archive      = {J_CSDA},
  author       = {Arkaprava Roy and Abhra Sarkar},
  doi          = {10.1016/j.csda.2023.107706},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107706},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian semiparametric multivariate density deconvolution via stochastic rotation of replicates},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Change-point testing for parallel data sets with FDR
control. <em>CSDA</em>, <em>182</em>, 107705. (<a
href="https://doi.org/10.1016/j.csda.2023.107705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large parallel data sets—consisting of paired measurements of responses and covariates—collected over time from numerous sources are ubiquitous. It is of great interest to identify the data sources where the underlying regression relationship of each data set has shifted. To be specific, the regression coefficient is changed to another one at some time point for each data set. Borrowing the strength of recent developments of multiple testing procedures, a residual-aggregated testing (RAT) method is proposed for recovering such data sources by controlling the false discovery rate (FDR). The proposed method can effectively incorporate the dependence structure among different data sets, and is more robust than the conventional Benjamini-Hochberg method based on asymptotic p-values or numerical approximations . Under mild conditions, the asymptotic validity for both the false discovery proportion and FDR control is established. Extensive numerical results further confirm the effectiveness and robustness of the proposed method.},
  archive      = {J_CSDA},
  author       = {Junfeng Cui and Guanghui Wang and Changliang Zou and Zhaojun Wang},
  doi          = {10.1016/j.csda.2023.107705},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107705},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Change-point testing for parallel data sets with FDR control},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Order determination for spiked-type models with a divergent
number of spikes. <em>CSDA</em>, <em>182</em>, 107704. (<a
href="https://doi.org/10.1016/j.csda.2023.107704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For large dimensional spiked models, the order (number of spikes) determination is an important issue for dimension reduction. The authors propose a generic criterion to estimate the order when the dimension is proportional to the sample size and the order is divergent as the dimension goes to infinity. To handle the divergence of the order, the criterion is defined by location-shift truncated eigenvalues, unlike the existing criteria. They suggest two versions of the criterion: the first defines an objective function that is a sequence of ridge ratios of the defined eigenvalues in order to have a clear separation between the ratio at the true order and other ratios; and the second uses an objective function of double ridge ratios to enhance such a separation. To alleviate the effect of the bias in the scale estimation when the order is large, an iterative procedure is utilized for the estimation. Numerical studies are conducted on spiked population models and spiked Fisher matrices to examine the finite sample performances of the proposed methods.},
  archive      = {J_CSDA},
  author       = {Yicheng Zeng and Lixing Zhu},
  doi          = {10.1016/j.csda.2023.107704},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107704},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Order determination for spiked-type models with a divergent number of spikes},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regularized regression for two phase failure time studies.
<em>CSDA</em>, <em>182</em>, 107703. (<a
href="https://doi.org/10.1016/j.csda.2023.107703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase study designs are ideal for focused sub-studies based on large prospective cohorts when the outcome of interest is an event that is rare in the full cohort, and additional covariates are expensive or difficult to measure. Researchers often wish to examine large numbers of covariates for association with outcomes of interest. In the context of cancer, hundreds to millions of genetic markers may be considered, along with environmental exposures. A computationally efficient variable selection method is proposed for two-phase failure time studies with stratified sampling under the Cox proportional hazards model . The penalized estimator is obtained from a penalized (weighted) Cox log partial likelihood using a pathwise cyclical coordinate descent algorithm which is scalable for high dimensional datasets where the number of features is much larger than the sample size ( p ≫ n ) (p≫n) . A detailed simulation study to examine the performance of the proposed methodology is described. The variable selection and estimation procedure is then used to obtain a model for predicting acute myeloid leukaemia using somatic stem cell mutation profiles derived from blood samples, based on a two-phase sample from the European Prospective Investigation into Cancer and Nutrition (EPIC) study.},
  archive      = {J_CSDA},
  author       = {David Soave and Jerald F. Lawless},
  doi          = {10.1016/j.csda.2023.107703},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107703},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regularized regression for two phase failure time studies},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantile three-factor model with heteroskedasticity,
skewness, and leptokurtosis. <em>CSDA</em>, <em>182</em>, 107702. (<a
href="https://doi.org/10.1016/j.csda.2023.107702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fama-French three-factor model advances the capital asset pricing model by expanding size risk and value risk factors to market risk factors. A quantile Fama-French three-factor model with GARCH-type dynamics, leptokurtosis, and skewness via asymmetric Student t errors is proposed to overcome the limitations of the existing ones. One can investigate how the daily volatility and market risk factors act under different market conditions represented by quantile levels via the proposed model. Bayesian adaptive Markov chain Monte Carlo methods are used to estimate model parameters in the proposed model over various quantile levels. This study assesses the Bayesian inference performance via simulation studies in which the designated models are misspecified and considers some daily stock returns from NASDAQ to help further select the best model via the posterior odds ratio. It is clear that the various market conditions and the GARCH effect should be incorporated into the model. Findings show that the estimation of the size factor turns insignificant for lower quantiles - i.e., when the market is in a panic, investors ignore the size effect of a company&#39;s assets.},
  archive      = {J_CSDA},
  author       = {Kai Y.K. Wang and Cathy W.S. Chen and Mike K.P. So},
  doi          = {10.1016/j.csda.2023.107702},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107702},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Quantile three-factor model with heteroskedasticity, skewness, and leptokurtosis},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining classifiers with measures of statistical
association. <em>CSDA</em>, <em>182</em>, 107701. (<a
href="https://doi.org/10.1016/j.csda.2023.107701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new class of probabilistic sensitivity measures that quantifies the degree of association between covariates and generic targets used in classification is proposed, and it is shown that such class possesses the zero-independence property. Corresponding estimators are introduced, asymptotic consistency is proven and bootstrap is used to quantify uncertainty in the estimates. The use of the new dependence measures as explanations in a statistical machine learning context is illustrated. The resulting approach, called Xi-method, is demonstrated through applications involving different data formats : tabular, visual and textual.},
  archive      = {J_CSDA},
  author       = {Emanuele Borgonovo and Valentina Ghidini and Roman Hahn and Elmar Plischke},
  doi          = {10.1016/j.csda.2023.107701},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107701},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Explaining classifiers with measures of statistical association},
  volume       = {182},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new avenue for bayesian inference with INLA.
<em>CSDA</em>, <em>181</em>, 107692. (<a
href="https://doi.org/10.1016/j.csda.2023.107692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated Nested Laplace Approximations (INLA) has been a successful approximate Bayesian inference framework since its proposal by Rue et al. (2009) . The increased computational efficiency and accuracy when compared with sampling-based methods for Bayesian inference like MCMC methods, are some contributors to its success. Ongoing research in the INLA methodology and implementation thereof in the R package R-INLA , ensures continued relevance for practitioners and improved performance and applicability of INLA. The era of big data and some recent research developments, presents an opportunity to reformulate some aspects of the classic INLA formulation, to achieve even faster inference, improved numerical stability and scalability. The improvement is especially noticeable for data-rich models. Various examples of data-rich models, like Cox&#39;s proportional hazards model , an item-response theory model, a spatial model including prediction, and a three-dimensional model for fMRI data are used to illustrate the efficiency gains in a tangible manner.},
  archive      = {J_CSDA},
  author       = {Janet Van Niekerk and Elias Krainski and Denis Rustand and Håvard Rue},
  doi          = {10.1016/j.csda.2023.107692},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107692},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A new avenue for bayesian inference with INLA},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Densely connected sub-gaussian linear structural equation
model learning via ℓ1- and ℓ2-regularized regressions. <em>CSDA</em>,
<em>181</em>, 107691. (<a
href="https://doi.org/10.1016/j.csda.2023.107691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new algorithm for learning densely connected sub-Gaussian linear structural equation models (SEMs) in high-dimensional settings, where the number of nodes increases with increasing number of samples. The proposed algorithm consists of two main steps: (i) the component-wise ordering estimation using ℓ 2 ℓ2 -regularized regression and (ii) the presence of edge estimation using ℓ 1 ℓ1 -regularized regression. Hence, the proposed algorithm can recover a large degree graph with a small indegree constraint. Also proven is that the sample size n = Ω ( p ) n=Ω(p) is sufficient for the proposed algorithm to recover a sub-Gaussian linear SEM provided that d = O ( p log ⁡ p ) d=O(plog⁡p) , where p is the number of nodes and d is the maximum indegree. In addition, the computational complexity is polynomial, O ( n p 2 max ⁡ ( n , p ) ) O(np2max⁡(n,p)) . Therefore, the proposed algorithm is statistically consistent and computationally feasible for learning a densely connected sub-Gaussian linear SEM with large maximum degree . Numerical experiments verified that the proposed algorithm is consistent, and performs better than the state-of-the-art high-dimensional linear SEM learning HGSM, LISTEN, and TD algorithms in both sparse and dense graph settings. Also demonstrated through real data is that the proposed algorithm is well-suited to estimating the Seoul public bike usage patterns in 2019.},
  archive      = {J_CSDA},
  author       = {Semin Choi and Yesool Kim and Gunwoong Park},
  doi          = {10.1016/j.csda.2023.107691},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107691},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Densely connected sub-gaussian linear structural equation model learning via ℓ1- and ℓ2-regularized regressions},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian circular lattice filters for computationally
efficient estimation of multivariate time-varying autoregressive models.
<em>CSDA</em>, <em>181</em>, 107690. (<a
href="https://doi.org/10.1016/j.csda.2023.107690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonstationary time series data exist in various scientific disciplines, including environmental science, biology, signal processing, econometrics, among others. Many Bayesian models have been developed to handle nonstationary time series. The time-varying vector autoregressive (TV-VAR) model is a well-established model for multivariate nonstationary time series. Nevertheless, in most cases, the large number of parameters presented by the model results in a high computational burden, ultimately limiting its usage. To address this issue, a computationally efficient multivariate Bayesian Circular Lattice Filter is developed, extending the usage of the TV-VAR model to a broader class of high-dimensional problems. The fully Bayesian framework allows both the autoregressive (AR) coefficients and innovation covariance to vary over time. The proposed estimation method is based on the Bayesian lattice filter (BLF), which is extremely computationally efficient and stable in univariate cases . To illustrate the effectiveness of the proposed approach, a comprehensive comparison with other competing methods is conducted through simulation studies and finds that, in most cases, the proposed approach performs superior in terms of the average squared error between the estimated and true time-varying spectral density . Finally, the methodology is demonstrated through applications to quarterly Gross Domestic Product (GDP) data and Northern California wind data.},
  archive      = {J_CSDA},
  author       = {Yuelei Sui and Scott H. Holan and Wen-Hsi Yang},
  doi          = {10.1016/j.csda.2023.107690},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107690},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian circular lattice filters for computationally efficient estimation of multivariate time-varying autoregressive models},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient permutation testing of variable importance
measures by the example of random forests. <em>CSDA</em>, <em>181</em>,
107689. (<a href="https://doi.org/10.1016/j.csda.2022.107689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypothesis testing of variable importance measures (VIMPs) is still the subject of ongoing research. This particularly applies to random forests (RF), for which VIMPs are a popular feature. Among recent developments, heuristic approaches to parametric testing have been proposed whose distributional assumptions are based on empirical evidence. Other formal tests under regularity conditions were derived analytically. But these approaches can be computationally expensive or even practically infeasible. This problem also occurs with non-parametric permutation tests , which are, however, distribution-free and can generically be applied to any kind of prediction model and VIMP. Embracing this advantage, it is proposed to use sequential permutation tests and sequential p-value estimation to reduce the computational costs associated with conventional permutation tests. These costs can be particularly high in case of complex prediction models. Therefore, RF&#39;s popular and widely used permutation VIMP (pVIMP) serves as a practical and relevant application example. The results of simulation studies confirm the theoretical properties of the sequential tests, that is, the type-I error probability is controlled at a nominal level and a high power is maintained with considerably fewer permutations needed compared to conventional permutation testing. The numerical stability of the methods is investigated in two additional application studies. In summary, theoretically sound sequential permutation testing of VIMP is possible at greatly reduced computational costs. Recommendations for application are given. A respective implementation for RF&#39;s pVIMP is provided through the accompanying R package rfvimptest .},
  archive      = {J_CSDA},
  author       = {Alexander Hapfelmeier and Roman Hornung and Bernhard Haller},
  doi          = {10.1016/j.csda.2022.107689},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107689},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient permutation testing of variable importance measures by the example of random forests},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local inference for functional linear mixed models.
<em>CSDA</em>, <em>181</em>, 107688. (<a
href="https://doi.org/10.1016/j.csda.2022.107688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of performing inference on the parameters of a functional mixed effect model for multivariate functional data is addressed, motivated by the analysis of 3D acceleration curves of trotting horses. Inference is performed in a local perspective, i.e., defining an adjusted p -value function on the same domain as the data. Such adjusted p -value functions can be thresholded at level α to select the regions of the domain and the coordinates of functional data presenting statistically significant effects. The probability of wrongly selecting as significant a region of the domain, and/or a coordinate of functional data where the null hypothesis is true, is always lower than the pre-specified level α due to the interval-wise control of the family-wise error rate. The procedure is based on nonparametric permutation tests , based on different permutation strategies. It is shown by simulations that all strategies proposed gain in power by taking random effects into account in permutations. Finally, the procedure is applied to the acceleration curves of trotting horses for testing differences between different levels of induced lameness. The method can clearly identify group differences.},
  archive      = {J_CSDA},
  author       = {Alessia Pini and Helle Sørensen and Anders Tolver and Simone Vantini},
  doi          = {10.1016/j.csda.2022.107688},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107688},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Local inference for functional linear mixed models},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Functional classification of bitcoin addresses.
<em>CSDA</em>, <em>181</em>, 107687. (<a
href="https://doi.org/10.1016/j.csda.2022.107687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A classification model for predicting the main activity of bitcoin addresses based on their balances is proposed. Since the balances are functions of time, methods from functional data analysis are applied; more specifically, the features of the proposed classification model are the functional principal components of the data. Classifying bitcoin addresses is a relevant problem for two main reasons: to understand the composition of the bitcoin market, and to identify addresses used for illicit activities. Although other bitcoin classifiers have been proposed, they focus primarily on network analysis rather than curve behavior. The proposed approach, on the other hand, does not require any network information for prediction. Furthermore, functional features have the advantage of being straightforward to build, unlike expert-built features. Results show improvement when combining functional features with scalar features, and similar accuracy for the models using those features separately, which points to the functional model being a good alternative when domain-specific knowledge is not available.},
  archive      = {J_CSDA},
  author       = {Manuel Febrero-Bande and Wenceslao González-Manteiga and Brenda Prallon and Yuri F. Saporito},
  doi          = {10.1016/j.csda.2022.107687},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107687},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Functional classification of bitcoin addresses},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An impartial trimming algorithm for robust circle fitting.
<em>CSDA</em>, <em>181</em>, 107686. (<a
href="https://doi.org/10.1016/j.csda.2022.107686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate circle fitting can be seriously compromised by the occurrence of even few anomalous points. Then, it is proposed to resort to a robust fitting strategy based on the idea of impartial trimming. Malicious data are supposed to be deleted, whereas estimation only relies on a set of genuine observations. The procedure is impartial in that trimmed points are not decided in advance but they are detected simultaneously to parameters estimation, according to an iterative algorithm : in each step a fixed proportion of the data is trimmed after sorting their geometric distances from the current fitted circle in non decreasing order. A reweighting step is also considered to improve the quality of the fit and make it less dependent on the selected trimming level. The global robustness properties of the method are established. The finite sample behavior of the proposed estimator has been investigated according to some numerical studies and real data examples.},
  archive      = {J_CSDA},
  author       = {Luca Greco and Simona Pacillo and Piera Maresca},
  doi          = {10.1016/j.csda.2022.107686},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107686},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An impartial trimming algorithm for robust circle fitting},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gaussian copula joint model for longitudinal and
time-to-event data with random effects. <em>CSDA</em>, <em>181</em>,
107685. (<a href="https://doi.org/10.1016/j.csda.2022.107685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal and survival sub-models are two building blocks for joint modelling of longitudinal and time-to-event data. Extensive research indicates separate analysis of these two processes could result in biased outputs due to their associations. Conditional independence between measurements of biomarkers and event time process given latent classes or random effects is a conventional approach for characterising the association between the two sub-models while taking the heterogeneity among the population into account. However, this assumption is difficult to validate because of the unobservable latent variables. Thus a Gaussian copula joint model with random effects is proposed to accommodate the scenarios where the conditional independence assumption is questionable. The conventional joint model assuming conditional independence is a special case of the proposed model when the association parameters in the Gaussian copula shrink to zero. Simulation studies and real data application are carried out to evaluate the performance of the proposed model with different correlation structures . In addition, personalised dynamic predictions of survival probabilities are obtained based on the proposed model and comparisons are made to the predictions obtained under the conventional joint model.},
  archive      = {J_CSDA},
  author       = {Zili Zhang and Christiana Charalambous and Peter Foster},
  doi          = {10.1016/j.csda.2022.107685},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107685},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A gaussian copula joint model for longitudinal and time-to-event data with random effects},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the optimal binary classifier with an application.
<em>CSDA</em>, <em>181</em>, 107683. (<a
href="https://doi.org/10.1016/j.csda.2022.107683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alternative accumulated improvement curve stochastic order is a criterion for the comparison of the performance of classifiers that predict binary responses . An explicit optimal classifier for this criterion is obtained. That optimal classifier has the largest ROC and CAP curves and indexes, that is, it is also optimal for the criteria based on the comparison of such curves and indexes. An application of the results to the search of the best classifier to predict clients of a bank which will make a transaction in the future is developed.},
  archive      = {J_CSDA},
  author       = {María Concepción López-Díaz and Miguel López-Díaz and Sergio Martínez-Fernández},
  doi          = {10.1016/j.csda.2022.107683},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107683},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On the optimal binary classifier with an application},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 2nd special issue on BIOSTATISTICS. <em>CSDA</em>,
<em>181</em>, 107681. (<a
href="https://doi.org/10.1016/j.csda.2022.107681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CSDA},
  author       = {Shuangge Ma ( The special issue editor ) and Martina Mittlboeck (The special issue editor) and F. Javier Rubio (The special issue editor) and Catherine C. Liu (The special issue editor)},
  doi          = {10.1016/j.csda.2022.107681},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107681},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {2nd special issue on BIOSTATISTICS},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial for the special issue on time series analysis.
<em>CSDA</em>, <em>181</em>, 107675. (<a
href="https://doi.org/10.1016/j.csda.2022.107675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CSDA},
  author       = {Konstantinos Fokianos ( The special issue Guest Editor ) and Claudia Kirch ( The special issue Guest Editor ) and Hernando Ombao ( The special issue Guest Editor )},
  doi          = {10.1016/j.csda.2022.107675},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107675},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Editorial for the special issue on time series analysis},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of microbial features in multivariate
regression under false discovery rate control. <em>CSDA</em>,
<em>181</em>, 107621. (<a
href="https://doi.org/10.1016/j.csda.2022.107621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many microbiome studies, researchers often aim at detecting statistical associations between microbial taxa and multiple disease-related secondary phenotypes of interest, which are further investigated in downstream functional studies. Most existing approaches tackle this aim by analyzing one taxon at a time and then followed by multiple testing correction. However, the large number of microbial taxa poses a heavy multiple correction burden which often limits the power of discovery of the aforementioned individual taxon-based analyses. Moreover, complicated correlation structures among taxa poses grand challenges for multiple testing correction procedures to achieve a satisfactory performance (e.g., false discovery rate control). To address these potential limitations, a new approach is proposed to detect statistical associations between multiple responses and microbial features in a multivariate regression model, which models the correlations among responses to boost power of association discovery. By utilizing the knockoff filter technique, the proposed procedure also enjoys the property of finite-sample false discovery rate control. It is demonstrated through a comprehensive simulation study to show the validity and usefulness of our new method and apply the methodology to a data set collected from microbiome studies to gain additional biological insights.},
  archive      = {J_CSDA},
  author       = {Arun Srinivasan and Lingzhou Xue and Xiang Zhan},
  doi          = {10.1016/j.csda.2022.107621},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107621},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Identification of microbial features in multivariate regression under false discovery rate control},
  volume       = {181},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unified model-free interaction screening via CV-entropy
filter. <em>CSDA</em>, <em>180</em>, 107684. (<a
href="https://doi.org/10.1016/j.csda.2022.107684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many practical high-dimensional problems, interactions have been increasingly found to play important roles beyond main effects. A representative example is gene-gene interaction. Joint analysis, which analyzes all interactions and main effects in a single model, can be seriously challenged by high dimensionality . For high-dimensional data analysis in general, marginal screening has been established as effective for reducing computational cost, increasing stability, and improving estimation/selection performance. Most of the existing marginal screening methods are designed for the analysis of main effects only. The existing screening methods for interaction analysis are often limited by making stringent model assumptions, lacking robustness, and/or requiring predictors to be continuous (and hence lacking flexibility). A unified marginal screening approach tailored to interaction analysis is developed, which can be applied to regression, classification, and survival analysis. Predictors are allowed to be continuous and discrete. The proposed approach is built on Coefficient of Variation (CV) filters based on information entropy. Statistical properties are rigorously established. It is shown that the CV filters are almost insensitive to the distribution tails of predictors, correlation structure among predictors, and sparsity level of signals. An efficient two-stage algorithm is developed to make the proposed approach scalable to ultrahigh-dimensional data. Simulations and the analysis of TCGA LUAD data further establish the practical superiority of the proposed approach.},
  archive      = {J_CSDA},
  author       = {Wei Xiong and Yaxian Chen and Shuangge Ma},
  doi          = {10.1016/j.csda.2022.107684},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107684},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Unified model-free interaction screening via CV-entropy filter},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dimension reduction in time series under the presence of
conditional heteroscedasticity. <em>CSDA</em>, <em>180</em>, 107682. (<a
href="https://doi.org/10.1016/j.csda.2022.107682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a time series, where the conditional mean is assumed to be an unknown function of linear combinations of past p observations and the conditional variance is assumed to be an unknown function of linear combinations of past q squared residuals. The linear combinations are assumed to contain all the necessary information about the time series that is available through the conditional mean and conditional variance, respectively. Nadaraya-Watson kernel smoother is used to estimate the unknown mean and variance function and an iterative approach is proposed to estimate the parameter matrices associated with the linear combinations. The estimators are shown to be consistent. To overcome computational challenges and provide numerical stability, a novel angular representation of parameter matrices is introduced. The numerical performance of the proposed method on forecasting the conditional mean is assessed by simulations studies. A real data of Brazilian Real (BRL)/U.S. Dollar Exchange Rate is analyzed. For the BRL/USD series, the estimated linear combinations yield a better time series model than an AR-ARCH model in terms of out-of-sample forecasts.},
  archive      = {J_CSDA},
  author       = {Murilo da Silva and T.N. Sriram and Yuan Ke},
  doi          = {10.1016/j.csda.2022.107682},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107682},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dimension reduction in time series under the presence of conditional heteroscedasticity},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The multi-aspect tests in the presence of ties.
<em>CSDA</em>, <em>180</em>, 107680. (<a
href="https://doi.org/10.1016/j.csda.2022.107680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-sample problem is one of the most important topics in various fields, such as biomedical experiments and product quality maintenance. The Lepage-type test, which is the sum of squares of standardized linear rank statistics, has often been used in the location-scale shift model. Recently, the Lepage-type test has been applied to the joint location-scale and joint location-scale-shape problems. In this study, the test statistics based on the Euclidean distance and Mahalanobis distance of standardized linear rank statistics are considered in the presence of ties. The moments of these test statistics are calculated by deriving the moment-generating function of the vector of linear rank statistics. Moreover, the gamma approximation based on these moments is compared with the chi-square approximation based on the limiting null distribution. Simulation studies and data examples demonstrate the usefulness of gamma approximation in the case of small sample sizes.},
  archive      = {J_CSDA},
  author       = {Hikaru Yamaguchi and Hidetoshi Murakami},
  doi          = {10.1016/j.csda.2022.107680},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107680},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The multi-aspect tests in the presence of ties},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locally weighted minimum contrast estimation for
spatio-temporal log-gaussian cox processes. <em>CSDA</em>, <em>180</em>,
107679. (<a href="https://doi.org/10.1016/j.csda.2022.107679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A local version of spatio-temporal log-Gaussian Cox processes is proposed by using Local Indicators of Spatio-Temporal Association (LISTA) functions plugged into the minimum contrast procedure, to obtain space as well as time-varying parameters. The new procedure resorts to the joint minimum contrast fitting method to estimate the set of second-order parameters. This approach has the advantage of being suitable in both separable and non-separable parametric specifications of the correlation function of the underlying Gaussian Random Field . Simulation studies to assess the performance of the proposed fitting procedure are presented, and an application to seismic spatio-temporal point pattern data is shown.},
  archive      = {J_CSDA},
  author       = {Nicoletta D&#39;Angelo and Giada Adelfio and Jorge Mateu},
  doi          = {10.1016/j.csda.2022.107679},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107679},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Locally weighted minimum contrast estimation for spatio-temporal log-gaussian cox processes},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Systematic enumeration of two-level even-odd designs of
strength 3. <em>CSDA</em>, <em>180</em>, 107678. (<a
href="https://doi.org/10.1016/j.csda.2022.107678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first dedicated algorithm to enumerate even-odd designs of strength 3 is presented. Such designs cannot be constructed by folding over smaller designs, but they may permit simultaneous estimation of many more two-factor interactions than designs that can be constructed by folding over. In the algorithm, enumeration is restricted to the computationally convenient class of designs with at least one nonzero correlation between a two-factor and a three-factor interaction contrast vector. All such designs with up to 56 runs, all those with 64 runs and up to 13 factors, and a specific subclass of those with 64 runs and more than 13 factors have been enumerated. 1 The best ranked 64-run designs substantially improve on benchmark designs from the literature.},
  archive      = {J_CSDA},
  author       = {Pieter T. Eendebak and Eric D. Schoen and Alan R. Vazquez and Peter Goos},
  doi          = {10.1016/j.csda.2022.107678},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107678},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Systematic enumeration of two-level even-odd designs of strength 3},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and fully-automated histograms for large-scale data
sets. <em>CSDA</em>, <em>180</em>, 107668. (<a
href="https://doi.org/10.1016/j.csda.2022.107668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {G-Enum histograms are a new fast and fully automated method for irregular histogram construction. By framing histogram construction as a density estimation problem and its automation as a model selection task, these histograms leverage the Minimum Description Length principle (MDL) to derive two different model selection criteria . Several proven theoretical results about these criteria give insights about their asymptotic behaviour and are used to speed up their optimisation. These insights, combined to a greedy search heuristic , are used to construct histograms in linearithmic time rather than the polynomial time incurred by previous works. The capabilities of the proposed MDL density estimation method are illustrated with reference to other fully automated methods in the literature, both on synthetic and large real-world data sets.},
  archive      = {J_CSDA},
  author       = {Valentina Zelaya Mendizábal and Marc Boullé and Fabrice Rossi},
  doi          = {10.1016/j.csda.2022.107668},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107668},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast and fully-automated histograms for large-scale data sets},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial heterogeneity automatic detection and estimation.
<em>CSDA</em>, <em>180</em>, 107667. (<a
href="https://doi.org/10.1016/j.csda.2022.107667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial regression is widely used for modeling the relationship between a dependent variable and explanatory covariates . Oftentimes, the linear relationships vary across space, such that some covariates have location-specific effects on the response. One fundamental question is how to detect the systematic variation in the model and identify which locations share common regression coefficients and which do not. Only a correct model structure can assure unbiased estimation of coefficients and valid inferences. A new procedure is proposed, called Spatial Heterogeneity Automatic Detection and Estimation (SHADE), for automatically and simultaneously subgrouping and estimating covariate effects for spatial regression models. The SHADE employs a class of spatially-weighted fusion type penalty on all pairs of observations, with location-specific weight constructed using spatial information, to cluster coefficients into subgroups. Under certain regularity conditions , the SHADE is shown to be able to identify the true model structure with probability approaching one and estimate regression coefficients consistently. An alternating direction method of multiplier algorithm (ADMM) is developed to compute the SHADE. In numerical studies , the empirical performance of the SHADE is demonstrated by using different choices of weights and comparing their accuracy. The results suggest that spatial information can enhance subgroup structure analysis in challenging situations when the spatial variation among regression coefficients is small or the number of repeated measures is small. Finally, the SHADE is applied to find the relationship between a natural resource survey and a land cover data layer to identify spatially interpretable groups.},
  archive      = {J_CSDA},
  author       = {Xin Wang and Zhengyuan Zhu and Hao Helen Zhang},
  doi          = {10.1016/j.csda.2022.107667},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107667},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Spatial heterogeneity automatic detection and estimation},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Projection expectile regression for sufficient dimension
reduction. <em>CSDA</em>, <em>180</em>, 107666. (<a
href="https://doi.org/10.1016/j.csda.2022.107666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing sufficient dimension reduction methods are designed for regression with predictors that are elliptically distributed, which limits their application in real data analyses . Projection expectile regression (PER) is proposed as a new linear sufficient dimension reduction method for handling complex predictor structures, which includes continuous, discrete, and mixed predictor variables . PER requires the link function between the response and the predictor to be monotone, but not necessarily smooth, which makes it suitable for handling stratified response surfaces. By design, PER does not involve matrix inversion or high-dimensional smoothing. Thus, PER is ideal for controlling problems associated with multicollinearity , high dimensionality , and sparsity in the predictor. An extensive simulation study demonstrates the performance of projection expectile regression in synthetic data. A real data analysis of health insurance charges in the United States is also provided. The asymptotic properties of the PER estimator are included as well.},
  archive      = {J_CSDA},
  author       = {Abdul-Nasah Soale},
  doi          = {10.1016/j.csda.2022.107666},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107666},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Projection expectile regression for sufficient dimension reduction},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Polya tree monte carlo method. <em>CSDA</em>, <em>180</em>,
107665. (<a href="https://doi.org/10.1016/j.csda.2022.107665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markov Chain Monte Carlo (MCMC) methods have been widely used in Statistics and machine learning research. However, such methods have several limitations, including slow convergence and the inefficiency in handling multi-modal distributions. To overcome these limitations of MCMC methods, a new, efficient sampling method has been proposed and it applies to general distributions including multi-modal ones or those having complex structure. The proposed approach, called the Polya tree Monte Carlo (PTMC) method, roots in constructing a Polya tree distribution using the idea of Monte Carlo method , and then using this distribution to approximate and facilitate sampling from a target distribution that may be complex or have multiple modes. The associated convergence property of the PTMC method is established and computationally efficient sampling algorithms are developed based on the PTMC. Extensive numerical studies demonstrate the satisfactory performance of the proposed method under various settings including its superiority to the usual MCMC algorithms. The evaluation and comparison are carried out in terms of sampling efficiency, computational speed and the capacity of identifying distribution modes . Additional details about the method, proofs and simulation results are provided in the Supplementary Web Appendices online .},
  archive      = {J_CSDA},
  author       = {Haoxin Zhuang and Liqun Diao and Grace Y. Yi},
  doi          = {10.1016/j.csda.2022.107665},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107665},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Polya tree monte carlo method},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariate-modulated large-scale multiple testing under
dependence. <em>CSDA</em>, <em>180</em>, 107664. (<a
href="https://doi.org/10.1016/j.csda.2022.107664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multiple testing, which calls for conducting tens of thousands of hypothesis testings simultaneously, has been applied in many scientific fields. Most conventional multiple testing procedures often focused on the control of false discovery rate (FDR) and largely ignored covariate information and the dependence structure among tests. A FDR control procedure, termed as Covariate-Modulated Local Index of Significance (cmLIS) procedure, which not only takes into account local correlations among tests but also accommodates the covariate information by leveraging a covariate-modulated hidden Markov model (HMM), has been proposed. In the oracle case where all parameters of the covariate-modulated HMM are known, the cmLIS procedure is shown to be valid and optimal in some sense. According to whether the number of mixed components in the non-null distribution is known, two Bayesian sampling algorithms are provided for parameter estimation. Extensive simulations are conducted to demonstrate the effectiveness of the cmLIS procedure over state-of-the-art multiple testing procedures. Finally, the cmLIS procedure is applied to an RNA sequencing data and a schizophrenia (SCZ) data.},
  archive      = {J_CSDA},
  author       = {Jiangzhou Wang and Tingting Cui and Wensheng Zhu and Pengfei Wang},
  doi          = {10.1016/j.csda.2022.107664},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107664},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Covariate-modulated large-scale multiple testing under dependence},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep parameterizations of pairwise and triplet markov models
for unsupervised classification of sequential data. <em>CSDA</em>,
<em>180</em>, 107663. (<a
href="https://doi.org/10.1016/j.csda.2022.107663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models are probabilistic graphical models based on hidden and observed random variables . They are popular to address classification tasks for time series applications such as part-of-speech tagging, image segmentation , genetic sequence analysis. Direct extensions of these models, the pairwise and triplet Markov models, are considered. These models aim at relaxing the assumptions underlying the hidden Markov chain by extending the direct dependencies of the involved random variables or by considering the addition of a third latent process. While these extensions define interesting modeling capabilities that have been little explored so far, they also raise new problems such as defining the nature of their core probability distributions and their parameterization. Once the model is fixed, the unsupervised classification task ( i.e. the estimation of the parameters and next of the hidden random variables) is a critical problem. These challenges are addressed, first it is shown that it is possible to embed recent deep neural networks in these models in order to exploit their full modeling power. Second, a continuous latent process in triplet Markov chains is considered. The latter aims at estimating the nature of the joint distributions of the hidden and observed random variables, in addition to their parameters. The introduction of such a continuous auxiliary latent process also offers a new way to model continuous non-stationarity in hidden Markov models. For each model that is introduced, an original unsupervised Bayesian estimation method is proposed. In particular, it takes into account the interpretability of the hidden random variables in terms of signal processing classification. Through unsupervised classification problems on synthetic and real data, it is shown that the new models outperform hidden Markov chains and their classical extensions.},
  archive      = {J_CSDA},
  author       = {Hugo Gangloff and Katherine Morales and Yohan Petetin},
  doi          = {10.1016/j.csda.2022.107663},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107663},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Deep parameterizations of pairwise and triplet markov models for unsupervised classification of sequential data},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariate dependent beta-GOS process. <em>CSDA</em>,
<em>180</em>, 107662. (<a
href="https://doi.org/10.1016/j.csda.2022.107662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-dependent processes have been widely used in Bayesian nonparametric statistics thanks to their flexibility to incorporate covariate information and correlation among process realizations . Unlike most of the existing work that focuses on extensions of exchangeable species sampling processes such as Dirichlet process , a new class of covariate-dependent nonexchangeable priors is proposed by considering the generalization of an nonexchangeable sequence, namely the Beta-GOS model. The proposed prior has an equivalent formulation under a continuous kernel mixture. It also has a latent variable representation that leads to a natural nonexchangeable parallel with the classical dependent Dirichlet process formulation. This prior is further applied in regression and autoregressive models and it is shown that its posterior sampling algorithm enjoys the same computational complexity with that of the Beta-GOS. The excellent numerical performance of the method is demonstrated via simulation and two real data examples.},
  archive      = {J_CSDA},
  author       = {Kunzhi Chen and Weining Shen and Weixuan Zhu},
  doi          = {10.1016/j.csda.2022.107662},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107662},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Covariate dependent beta-GOS process},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust mixture regression modeling based on the normal
mean-variance mixture distributions. <em>CSDA</em>, <em>180</em>,
107661. (<a href="https://doi.org/10.1016/j.csda.2022.107661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture regression models (MRMs) are widely used to capture the heterogeneity of relationships between the response variable and one or more predictors coming from several non-homogeneous groups. Since the conventional MRMs are quite sensitive to departures from normality caused by extra skewness and possible heavy tails, various extensions built on more flexible distributions have been put forward in the last decade. The class of normal mean-variance mixture (NMVM) distributions that arise from scaling both the mean and variance of a normal random variable with a common mixing distribution encompasses many prominent (symmetric or asymmetrical) distributions as special cases. A unified approach to robustifying MRMs is proposed by considering the class of NMVM distributions for component errors. An expectation conditional maximization either (ECME) algorithm, which incorporates membership indicators and the latent scaling variables as the missing data, is developed for carrying out maximum likelihood (ML) estimation of model parameters. Four simulation studies are conducted to examine the finite-sample property of ML estimators and the robustness of the proposed model against outliers for contaminated and noisy data. The usefulness and superiority of our methodology are demonstrated through applications to two real datasets.},
  archive      = {J_CSDA},
  author       = {Mehrdad Naderi and Elham Mirfarah and Wan-Lun Wang and Tsung-I Lin},
  doi          = {10.1016/j.csda.2022.107661},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107661},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust mixture regression modeling based on the normal mean-variance mixture distributions},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjusting for unmeasured confounding in survival causal
effect using validation data. <em>CSDA</em>, <em>180</em>, 107660. (<a
href="https://doi.org/10.1016/j.csda.2022.107660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmeasured confounding is an important problem in observational studies, which brings a great challenge to eliminate or reduce bias. A large main data set with unmeasured confounders and a smaller validation data set with detailed information on these confounders are combined to estimate the survival causal effect . The initial estimator based on the small validation data set under the ignorable treatment assignment and the error-prone estimator based on the large main data set are both obtained by the doubly robust method. Then, the proposed estimator is obtained by leveraging the correlation between the initial estimator and the error-prone estimator. The large sample theory of the proposed estimator is established. Simulation studies are conducted to show the good performance of the proposed method. A real data of breast cancer from the cBio Cancer Genomics Portal is analyzed to illustrate the proposed method.},
  archive      = {J_CSDA},
  author       = {Yongxiu Cao and Jichang Yu},
  doi          = {10.1016/j.csda.2022.107660},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107660},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Adjusting for unmeasured confounding in survival causal effect using validation data},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BART-based inference for poisson processes. <em>CSDA</em>,
<em>180</em>, 107658. (<a
href="https://doi.org/10.1016/j.csda.2022.107658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of Bayesian Additive Regression Trees (BART) has been demonstrated in a variety of contexts including non-parametric regression and classification. A BART scheme for estimating the intensity of inhomogeneous Poisson processes is introduced. Poisson intensity estimation is a vital task in various applications including medical imaging , astrophysics and network traffic analysis . The new approach enables full posterior inference of the intensity in a non-parametric regression setting. The performance of the novel scheme is demonstrated through simulation studies on synthetic and real datasets up to five dimensions, and the new scheme is compared with alternative approaches.},
  archive      = {J_CSDA},
  author       = {Stamatina Lamprinakou and Mauricio Barahona and Seth Flaxman and Sarah Filippi and Axel Gandy and Emma J. McCoy},
  doi          = {10.1016/j.csda.2022.107658},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107658},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {BART-based inference for poisson processes},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Jackstraw inference for AJIVE data integration.
<em>CSDA</em>, <em>180</em>, 107649. (<a
href="https://doi.org/10.1016/j.csda.2022.107649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of big data, data integration is a critical step especially in the understanding of how diverse data types work together and work separately. Among data integration methods, the Angle-Based Joint and Individual Variation Explained (AJIVE) approach is particularly attractive because it not only studies joint behavior but also individual behavior. Typically AJIVE scores indicate important relationships between data objects, such as clusters. An important challenge is understanding which features, i.e. variables, are associated with those relationships. This challenge is addressed by the proposal of a hypothesis test for assessing statistical significance of features. The new test is inspired by the related jackstraw method developed for Principal Component Analysis . We use a high-dimensional multi-genomic cancer data set as our strong motivation and deep illustration of the methodology.},
  archive      = {J_CSDA},
  author       = {Xi Yang and Katherine A. Hoadley and Jan Hannig and J.S. Marron},
  doi          = {10.1016/j.csda.2022.107649},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107649},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Jackstraw inference for AJIVE data integration},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of predictive performance in high-dimensional
data settings using learning curves. <em>CSDA</em>, <em>180</em>,
107622. (<a href="https://doi.org/10.1016/j.csda.2022.107622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional prediction settings, it remains challenging to reliably estimate the test performance. To address this challenge, a novel performance estimation framework is presented. This framework, called Learn2Evaluate, is based on learning curves by fitting a smooth monotone curve depicting test performance as a function of the sample size. Learn2Evaluate has several advantages compared to commonly applied performance estimation methodologies. Firstly, a learning curve offers a graphical overview of a learner. This overview assists in assessing the potential benefit of adding training samples and it provides a more complete comparison between learners than performance estimates at a fixed subsample size. Secondly, a learning curve facilitates in estimating the performance at the total sample size rather than a subsample size. Thirdly, Learn2Evaluate allows the computation of a theoretically justified and useful lower confidence bound . Furthermore, this bound may be tightened by performing a bias correction. The benefits of Learn2Evaluate are illustrated by a simulation study and applications to omics data.},
  archive      = {J_CSDA},
  author       = {Jeroen M. Goedhart and Thomas Klausch and Mark A. van de Wiel},
  doi          = {10.1016/j.csda.2022.107622},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107622},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of predictive performance in high-dimensional data settings using learning curves},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized martingale difference divergence: Detecting
conditional mean independence with applications in variable screening.
<em>CSDA</em>, <em>180</em>, 107618. (<a
href="https://doi.org/10.1016/j.csda.2022.107618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Martingale difference divergence measures the departure of conditional mean independence of two random vectors. Generalized martingale difference divergence and its correlation are developed based on symmetric Lévy measures to detect such an independence. Then the proposed generalized martingale difference correlation is utilized as a marginal utility to do high-dimensional variable screening. Both simulation results and real data illustrations show the promising performance of the developed indexes.},
  archive      = {J_CSDA},
  author       = {Lu Li and Chenlu Ke and Xiangrong Yin and Zhou Yu},
  doi          = {10.1016/j.csda.2022.107618},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107618},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized martingale difference divergence: Detecting conditional mean independence with applications in variable screening},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Specification testing for ordinary differential equation
models with fixed design and applications to COVID-19 epidemic models.
<em>CSDA</em>, <em>180</em>, 107616. (<a
href="https://doi.org/10.1016/j.csda.2022.107616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Checking the models about the ongoing Coronavirus Disease 2019 (COVID-19) pandemic is an important issue. Some famous ordinary differential equation (ODE) models, such as the SIR and SEIR models have been used to describe and predict the epidemic trend. Still, in many cases, only part of the equations can be observed. A test is suggested to check possibly partially observed ODE models with a fixed design sampling scheme. The asymptotic properties of the test under the null , global and local alternative hypotheses are presented. Two new propositions about U-statistics with varying kernels based on independent but non-identical data are derived as essential tools. Some simulation studies are conducted to examine the performances of the test. Based on the available public data, it is found that the SEIR model, for modeling the data of COVID-19 infective cases in certain periods in Japan and Algeria, respectively, maybe not be appropriate by applying the proposed test.},
  archive      = {J_CSDA},
  author       = {Ran Liu and Lixing Zhu},
  doi          = {10.1016/j.csda.2022.107616},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107616},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Specification testing for ordinary differential equation models with fixed design and applications to COVID-19 epidemic models},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Empirical gittins index strategies with ε-explorations for
multi-armed bandit problems. <em>CSDA</em>, <em>180</em>, 107610. (<a
href="https://doi.org/10.1016/j.csda.2022.107610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine learning/statistics literature has so far considered largely multi-armed bandit (MAB) problems in which the rewards from every arm are assumed independent and identically distributed. For more general MAB models in which every arm evolves according to a rewarded Markov process , it is well known the optimal policy is to pull an arm with the highest Gittins index. When the underlying distributions are unknown, an empirical Gittins index rule with ε -exploration (abbreviated as empirical ε -Gittinx index rule) is proposed to solve such MAB problems. This procedure is constructed by combining the idea of ε -exploration (for exploration) and empirical Gittins indices (for exploitation) computed by applying the Largest-Remaining-Index algorithm to the estimated underlying distribution. The convergence of empirical Gittins indices to the true Gittins indices and expected discounted total rewards of the empirical ε -Gittinx index rule to those of the oracle Gittins index rule is provided. A numerical simulation study is demonstrated to show the behavior of the proposed policies, and its performance over the ε -mean reward is discussed.},
  archive      = {J_CSDA},
  author       = {Xiao Li and Yuqiang Li and Xianyi Wu},
  doi          = {10.1016/j.csda.2022.107610},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107610},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Empirical gittins index strategies with ε-explorations for multi-armed bandit problems},
  volume       = {180},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sparse bayesian hierarchical vector autoregressive model
for microbial dynamics in a wastewater treatment plant. <em>CSDA</em>,
<em>179</em>, 107659. (<a
href="https://doi.org/10.1016/j.csda.2022.107659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper function of a wastewater treatment plant (WWTP) relies on maintaining a delicate balance between a multitude of competing microorganisms. Gaining a detailed understanding of the complex network of interactions therein is essential to maximising not only current operational efficiencies, but also for the effective design of new treatment technologies. Metagenomics offers an insight into these dynamic systems through the analysis of the microbial DNA sequences present. Unique taxa are deduced through sequence clustering to form operational taxonomic units (OTUs), with per-taxa abundance estimates obtained from corresponding sequence counts. The data in this study comprise weekly OTU counts from an activated sludge (AS) tank of a WWTP along with corresponding measurements of chemical and environmental (CE) covariates . Directly fitting a model to the OTU data is incredibly challenging because of the high dimensionality and sparsity of the observations. The first step is therefore to aggregate the OTUs into twelve microbial communities or “bins” using a seasonal phase-based clustering approach . The mean abundances in the twelve bins are assumed to vary over time according to a multivariate linear regression on the CE covariates. Deviations from the mean are then modelled using a vector autoregressive (VAR) model of order one, which is a linear approximation to the commonly used generalised Lotka-Volterra (gLV) model. Sparsity is assumed in the interactions between microbial communities by carrying out inference in a hierarchical Bayesian framework which uses a shrinkage prior for the autoregressive coefficient matrix of the VAR model. Different shrinkage priors are explored by analysing simulated data sets before selecting the regularised horseshoe prior for the biological application . It is found that ammonia and chemical oxygen demand have a positive relationship with several bins and pH has a positive relationship with one bin. These results are supported by findings in the biological literature. Several negative interactions are also identified. These novel biological findings suggest OTUs in different bins may be competing for resources and that these relationships are complex. Although simpler than a gLV model, the VAR model is still able to offer valuable insight into the microbial dynamics of the WWTP.},
  archive      = {J_CSDA},
  author       = {Naomi E. Hannaford and Sarah E. Heaps and Tom M.W. Nye and Thomas P. Curtis and Ben Allen and Andrew Golightly and Darren J. Wilkinson},
  doi          = {10.1016/j.csda.2022.107659},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107659},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A sparse bayesian hierarchical vector autoregressive model for microbial dynamics in a wastewater treatment plant},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust factored principal component analysis for
matrix-valued outlier accommodation and detection. <em>CSDA</em>,
<em>179</em>, 107657. (<a
href="https://doi.org/10.1016/j.csda.2022.107657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is a popular dimension reduction technique for vector data. Factored PCA (FPCA) is a probabilistic extension of PCA for matrix data, which can substantially reduce the number of parameters in PCA while yield satisfactory performance. However, FPCA is based on the Gaussian assumption and thereby susceptible to outliers. Although the multivariate t distribution as a robust modeling tool for vector data has a very long history, its application to matrix data is very limited. The main reason is that the dimension of the vectorized matrix data is often very high, and the higher the dimension, the lower the breakdown point that measures the robustness. To solve the robustness problem suffered by FPCA and make it applicable to matrix data, in this paper a robust extension of FPCA (RFPCA) is proposed, which is built upon a t -type distribution called matrix-variate t distribution. Like the multivariate t distribution, the matrix-variate t distribution can adaptively down-weight outliers and yield robust estimates. A fast EM-type algorithm for parameter estimation is developed. Experiments on synthetic and real-world datasets reveal that (i) RFPCA is compared favorably with several closely related methods. Importantly, RFPCA has a significantly higher breakdown point than its vector-based cousin multivariate t PCA ( t PCA), which makes RFPCA more applicable to matrix data; (ii) the expected latent weights of RFPCA can be readily used for outlier detection , and they are much more reliable than those by t PCA. Such detection is rarely available with existing matrix-based methods, especially for gross matrix-valued outliers.},
  archive      = {J_CSDA},
  author       = {Xuan Ma and Jianhua Zhao and Yue Wang and Changchun Shang and Fen Jiang},
  doi          = {10.1016/j.csda.2022.107657},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107657},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust factored principal component analysis for matrix-valued outlier accommodation and detection},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extending the fellegi-sunter record linkage model for
mixed-type data with application to the french national health data
system. <em>CSDA</em>, <em>179</em>, 107656. (<a
href="https://doi.org/10.1016/j.csda.2022.107656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic record linkage is a process of combining data from different sources, when such data refer to common entities and identifying information is not available. A probabilistic record linkage framework that takes into account multiple non-identifying information that this is limited to simple binary comparison between matching variables has been previously proposed. An extension of this method is proposed for mixed-type comparison vectors. A mixture model for handling comparison values of low prevalence categorical matching variables, and a mixture of hurdle gamma distribution for handling comparison values of continuous matching variables have been developed. The parameters are estimated by means of the Expectation Conditional Maximization (ECM) algorithm. Through a Monte Carlo simulation study, both the posterior probability estimation for a record pair to be a match and the prediction of matched record pairs are evaluated. The simulation results indicate that the proposed methods outperform existing ones in most considered cases. The proposed methods are applied on a real dataset, to perform linkage between a registry of patients suffering from venous thromboembolism in the Brest district area (GETBO) and the French national health information system (SNDS).},
  archive      = {J_CSDA},
  author       = {Thanh Huan Vo and Guillaume Chauvet and André Happe and Emmanuel Oger and Stéphane Paquelet and Valérie Garès},
  doi          = {10.1016/j.csda.2022.107656},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107656},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Extending the fellegi-sunter record linkage model for mixed-type data with application to the french national health data system},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology-based goodness-of-fit tests for sliced spatial
data. <em>CSDA</em>, <em>179</em>, 107655. (<a
href="https://doi.org/10.1016/j.csda.2022.107655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In materials science and many other application domains, 3D information can often only be obtained by extrapolating from 2D slices. In topological data analysis, persistence vineyards have emerged as a powerful tool to take into account topological features stretching over several slices. It is illustrated how persistence vineyards can be used to design rigorous statistical hypothesis tests for 3D microstructure models based on data from 2D slices. More precisely, by establishing the asymptotic normality of suitable longitudinal and cross-sectional summary statistics, goodness-of-fit tests that become asymptotically exact in large sampling windows are devised. The testing methodology is illustrated through a detailed simulation study and a prototypical example from materials science is provided.},
  archive      = {J_CSDA},
  author       = {Alessandra Cipriani and Christian Hirsch and Martina Vittorietti},
  doi          = {10.1016/j.csda.2022.107655},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107655},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Topology-based goodness-of-fit tests for sliced spatial data},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and feasible inference for high-dimensional normal
copula regression models. <em>CSDA</em>, <em>179</em>, 107654. (<a
href="https://doi.org/10.1016/j.csda.2022.107654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The composite likelihood (CL) is amongst the computational methods used for the estimation of high-dimensional multivariate normal (MVN) copula models with discrete responses. Its computational advantage, as a surrogate likelihood method, is that is based on the independence likelihood for the univariate marginal regression and non-regression parameters and pairwise likelihood for the correlation parameters. Nevertheless, the efficiency of the CL method for estimating the univariate regression and non-regression marginal parameters can be low. For a high-dimensional discrete response, weighted versions of the composite likelihood estimating equations and an iterative approach to determine good weight matrices are proposed. The general methodology is applied to the MVN copula with univariate ordinal regressions as the marginals. Efficiency calculations show that the proposed method is nearly as efficient as the maximum likelihood for fully specified MVN copula models. Illustrations include simulations and real data applications regarding longitudinal (low-dimensional) and time (high-dimensional) series ordinal response data with covariates . Our studies suggest that there is a substantial gain in efficiency via the weighted CL method.},
  archive      = {J_CSDA},
  author       = {Aristidis K. Nikoloulopoulos},
  doi          = {10.1016/j.csda.2022.107654},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107654},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient and feasible inference for high-dimensional normal copula regression models},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven stabilizations of goodness-of-fit tests.
<em>CSDA</em>, <em>179</em>, 107653. (<a
href="https://doi.org/10.1016/j.csda.2022.107653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exact null distributions of goodness-of-fit test statistics are generally challenging to obtain in tractable forms. Practitioners are therefore usually obliged to rely on asymptotic null distributions or Monte Carlo methods , either in the form of a lookup table or carried out on demand, to apply a goodness-of-fit test. There exist simple and useful transformations of several classic goodness-of-fit test statistics that stabilize their exact- n critical values for varying sample sizes n . However, detail on the accuracy of these and subsequent transformations in yielding exact p -values, or even deep understanding on the derivation of several transformations, is still scarce nowadays. The latter stabilization approach is explained and automated to ( i ) expand its scope of applicability and ( ii ) yield upper-tail exact p -values, as opposed to exact critical values for fixed significance levels . Improvements on the stabilization accuracy of the exact null distributions of the Kolmogorov–Smirnov, Cramér–von Mises, Anderson–Darling, Kuiper, and Watson test statistics are shown. In addition, a parameter-dependent exact- n stabilization for several novel statistics for testing uniformity on the hypersphere of arbitrary dimension is provided. A data application in astronomy illustrates the benefits of the advocated stabilization for quickly analyzing small-to-moderate sequentially-measured samples.},
  archive      = {J_CSDA},
  author       = {Alberto Fernández-de-Marcos and Eduardo García-Portugués},
  doi          = {10.1016/j.csda.2022.107653},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107653},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Data-driven stabilizations of goodness-of-fit tests},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A latent slice sampling algorithm. <em>CSDA</em>,
<em>179</em>, 107652. (<a
href="https://doi.org/10.1016/j.csda.2022.107652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a sampling algorithm for discrete spaces, a variation of the slice sampler for continuous spaces is introduced. It utilizes latent variables and is related to Neal&#39;s slice sampler. The key difference is that the additional latent variables allow the sequential stepping out or doubling procedures, which makes the basic slice sampler difficult to use in high dimensional problems, to be avoided. On the other hand, the latent slice sampling algorithm is applicable on high dimensional problems where the variables can all be treated in a single block.},
  archive      = {J_CSDA},
  author       = {Yanxin Li and Stephen G. Walker},
  doi          = {10.1016/j.csda.2022.107652},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107652},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A latent slice sampling algorithm},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixture models with decreasing weights. <em>CSDA</em>,
<em>179</em>, 107651. (<a
href="https://doi.org/10.1016/j.csda.2022.107651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decreasing weight prior distributions for mixture models play an important role in nonparametric Bayesian inference . Various random probability measures with decreasing weights have been previously explored and it has been shown that they provide an efficient alternative to the more traditional Dirichlet process mixture model. This ordering of the weights implicitly alleviates the so-called label switching problem, as larger weights correspond to larger groups. A general procedure to define any decreasing weights model based on a characterization of a discrete random variable which also allows for an easy and generic sampling algorithm for estimating the model is provided. An exact representation for the number of expected components is given. Finally, the performance of the mixture model on simulated data sets is investigated numerically.},
  archive      = {J_CSDA},
  author       = {Spyridon J. Hatjispyros and Christos Merkatas and Stephen G. Walker},
  doi          = {10.1016/j.csda.2022.107651},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107651},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Mixture models with decreasing weights},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mean regression model for the zero-truncated poisson
distribution and its generalization. <em>CSDA</em>, <em>179</em>,
107650. (<a href="https://doi.org/10.1016/j.csda.2022.107650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-truncated count data (e.g., days of staying in hospital; survival weeks of female patients with breast cancer) often arise in various fields such as medical studies. To model such data, the zero-truncated Poisson (ZTP) distribution is commonly utilized to investigate the relationship between the response counts and a set of covariates . For existing ZTP regression models, it is very hard to explain the regression coefficients β or it is quite difficult to perform a constrained optimization to calculate the maximum likelihood estimates (MLEs) of β . This paper aims to introduce a new mean regression model for the ZTP distribution with a clear interpretation about the regression coefficients . Because of a challenge that the original Poisson mean parameter λ i λi cannot be expressed explicitly by the ZTP mean parameter μ i μi , an embedded Newton–Raphson algorithm is developed to calculate the MLEs of regression coefficients. The construction of bootstrap confidence intervals is presented and three hypothesis tests (i.e., the likelihood ratio test , the Wald test and the score test) are considered. Furthermore, the ZTP mean regression model is generalized to the mean regression model for the k -truncated Poisson distribution . Simulation studies are conducted and two real data are analyzed to illustrate the proposed model and methods.},
  archive      = {J_CSDA},
  author       = {Xun-Jian Li and Yuan Sun and Guo-Liang Tian and Jiajuan Liang and Jianhua Shi},
  doi          = {10.1016/j.csda.2022.107650},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107650},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Mean regression model for the zero-truncated poisson distribution and its generalization},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust multiscale estimation of time-average variance for
time series segmentation. <em>CSDA</em>, <em>179</em>, 107648. (<a
href="https://doi.org/10.1016/j.csda.2022.107648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exist several methods developed for the canonical change point problem of detecting multiple mean shifts, which search for changes over sections of the data at multiple scales. In such methods, estimation of the noise level is often required in order to distinguish genuine changes from random fluctuations due to the noise. When serial dependence is present, using a single estimator of the noise level may not be appropriate. Instead, it is proposed to adopt a scale-dependent time-average variance constant that depends on the length of the data section in consideration, to gauge the level of the noise therein. Accordingly, an estimator that is robust to the presence of multiple mean shifts is developed. The consistency of the proposed estimator is shown under general assumptions permitting heavy-tailedness, and its use with two widely adopted data segmentation algorithms, the moving sum and the wild binary segmentation procedures, is discussed. The performance of the proposed estimator is illustrated through extensive simulation studies and on applications to the house price index and air quality data sets.},
  archive      = {J_CSDA},
  author       = {Euan T. McGonigle and Haeran Cho},
  doi          = {10.1016/j.csda.2022.107648},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107648},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust multiscale estimation of time-average variance for time series segmentation},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of social media users with generalized
functional data analysis. <em>CSDA</em>, <em>179</em>, 107647. (<a
href="https://doi.org/10.1016/j.csda.2022.107647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancement has made possible the collection of data from social media platforms at unprecedented speed and volume. Current methods for analyzing such data lack interpretability , are computationally intensive, or require a rigid data specification. Functional data analysis enables the development of a flexible, yet interpretable, modeling framework to extract lower-dimensional relevant features of a user&#39;s posting behavior on social media, based on their posting activity over time. The extracted features can then be used to discriminate a malicious user from a genuine one. The proposed methodology can classify a binary time series in a computationally efficient manner and provides more insights into the posting behavior of social media agents. Performance of the method is illustrated numerically in simulation studies and on a motivating Twitter data set. The developed methods are applicable to other social media data , such as Facebook, Instagram, Reddit, or TikTok, or any form of digital interaction where the user&#39;s posting behavior is indicative of their user class.},
  archive      = {J_CSDA},
  author       = {Anthony Weishampel and Ana-Maria Staicu and William Rand},
  doi          = {10.1016/j.csda.2022.107647},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107647},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Classification of social media users with generalized functional data analysis},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified framework of multiply robust estimation approaches
for handling incomplete data. <em>CSDA</em>, <em>179</em>, 107646. (<a
href="https://doi.org/10.1016/j.csda.2022.107646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data occur frequently in practice. Inverse probability weighting and imputation are regarded as two important approaches for handling missing data . However, the validity of these approaches depends on underlying model assumptions. A new general framework for multiply robust estimation procedures by combining multiple nonresponse and imputation models is proposed in the paper. The proposed method can be used to estimate both smooth and non-smooth parameters defined as the solution of some estimating equations. It includes population means, quantiles , and distribution functions as special cases. The asymptotic results of the proposed methods are established. The results of a simulation study and a real data application suggest that the proposed methods perform well in terms of bias and efficiency.},
  archive      = {J_CSDA},
  author       = {Sixia Chen and David Haziza},
  doi          = {10.1016/j.csda.2022.107646},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107646},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A unified framework of multiply robust estimation approaches for handling incomplete data},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric inference on smoothed quantile regression
process. <em>CSDA</em>, <em>179</em>, 107645. (<a
href="https://doi.org/10.1016/j.csda.2022.107645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the global estimation in semiparametric quantile regression models. For estimating unknown functional parameters, an integrated quantile regression loss function with penalization is proposed. The first step is to obtain a vector-valued functional Bahadur representation of the resulting estimators, and then derive the asymptotic distribution of the proposed infinite-dimensional estimators. Furthermore, a resampling approach that generalizes the minimand perturbing technique is adopted to construct confidence intervals and to conduct hypothesis testing . Extensive simulation studies demonstrate the effectiveness of the proposed method, and applications to the real estate dataset and world happiness report data are provided.},
  archive      = {J_CSDA},
  author       = {Meiling Hao and Yuanyuan Lin and Guohao Shen and Wen Su},
  doi          = {10.1016/j.csda.2022.107645},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107645},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric inference on smoothed quantile regression process},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smoothly varying regularization. <em>CSDA</em>,
<em>179</em>, 107644. (<a
href="https://doi.org/10.1016/j.csda.2022.107644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basis expansion with regularization methods is much appealing to the flexible or robust nonlinear regression models for data with complex structures. When the underlying function has inhomogeneous smoothness, it does not perform well when one tries to estimate it with regularization methods that do not require intensive computational load. Specifically, when the underlying function has both smooth and non-smooth parts, the conventional regularization methods tend to over-fit in the smooth part or under-fit in the non-smooth part. Therefore, a new efficient basis expansion is considered by proposing a smoothly varying regularization method which is constructed by some special penalties. These should be called adaptive penalties. In the modeling, adaptive-type penalties play key roles and it has been successful in giving good estimation for inhomogeneous smoothness functions. A crucial issue in the modeling process is the choice of a suitable model among candidates. To select the suitable model, an approximated generalized information criterion (GIC) is derived. The proposed method is investigated through Monte Carlo simulations and real data analysis. Numerical results suggest that the proposed method performs well in various situations.},
  archive      = {J_CSDA},
  author       = {Daeju Kim and Shuichi Kawano and Yoshiyuki Ninomiya},
  doi          = {10.1016/j.csda.2022.107644},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107644},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Smoothly varying regularization},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive spatial designs minimizing the integrated bernoulli
variance in spatial logistic regression models - with an application to
benthic habitat mapping. <em>CSDA</em>, <em>179</em>, 107643. (<a
href="https://doi.org/10.1016/j.csda.2022.107643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing efficient spatial sampling designs is important for monitoring, understanding and characterizing environmental processes acting over large-scale spatial domain. Multiple types of devices can accomplish informative sampling in various ways, and time and computational limitations should be considered too. In-situ sampling is often accurate, but it tends to be very sparse in the vast spatial areas considered for mapping purposes, and careful constructions of experimental sampling designs are needed. A criterion for mapping spatial presence-absence variables is presented. The expected integrated Bernoulli variance criterion is used in order to explore regions with more uncertainty about the binary outcomes. This approach uses a hierarchical Bayesian logistic regression model for the binary variable and develops approximate closed form expressions for the expected integrated Bernoulli variance. The approximations are fast to compute for many designs. The expressions are extended to find adaptive designs in the setting of sequential spatial exploration, where there are limited computation resources and operational time restrictions. In simulation studies the approximations are compared with tedious Monte Carlo sampling methods. Approximations are shown to be accurate and to work for a large spatial grid. An application in benthic habitat mapping is presented. It considers a coral dataset from Australia to demonstrate the suggested sampling design approach.},
  archive      = {J_CSDA},
  author       = {Susan Anyosa and Jo Eidsvik and Oscar Pizarro},
  doi          = {10.1016/j.csda.2022.107643},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107643},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Adaptive spatial designs minimizing the integrated bernoulli variance in spatial logistic regression models - with an application to benthic habitat mapping},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balancing covariates in multi-arm trials via adaptive
randomization. <em>CSDA</em>, <em>179</em>, 107642. (<a
href="https://doi.org/10.1016/j.csda.2022.107642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-arm trials are common in medical and health research for comparing the efficacy of competing drugs and interventions, among other applications. While ensuring covariate balance is a critical issue for comparative studies to be successful, classical multi-arm trials often fail to balance covariates among multi-treatments. An adaptive randomization via Mahalanobis distance for multi-arm trials is proposed to improve the covariate balance and thus the quality of the subsequent treatment effect estimation. The investigation scope includes the implementation of the proposed method and also its theoretical properties. Both theoretical and numerical results demonstrate the proposed method can attain desirable covariate balance, and thus improving the subsequent estimation efficiency. Compared with other competing methods, the computational cost of the proposed method is also favorable. An illustrative real case analysis of the efficacy of different doses of Canagliflozin, a treatment for patients with type 2 diabetes, also proves that the proposed method has broad applicability.},
  archive      = {J_CSDA},
  author       = {Haoyu Yang and Yichen Qin and Fan Wang and Yang Li and Feifang Hu},
  doi          = {10.1016/j.csda.2022.107642},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107642},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Balancing covariates in multi-arm trials via adaptive randomization},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering multivariate count data via dirichlet-multinomial
network fusion. <em>CSDA</em>, <em>179</em>, 107634. (<a
href="https://doi.org/10.1016/j.csda.2022.107634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering of multivariate count data has widespread applications in areas such as text analysis and microbiome studies. The need to account for overdispersion generally results in a nonconvex loss function, which does not fit into the existing convex clustering framework. Moreover, prior knowledge of a network over the samples, often available from citation or similarity relationships, is not taken into account. We introduce Dirichlet-multinomial network fusion (DMNet) for clustering multivariate count data, which models the samples via Dirichlet-multinomial distributions with individual parameters and employs a weighted group L 1 L1 fusion penalty to pursue homogeneity over a prespecified network. To circumvent the nonconvexity issue, we present two exponential family approximations to the Dirichlet-multinomial distribution, which are amenable to efficient optimization and theoretical analysis. We derive an ADMM algorithm and establish nonasymptotic error bounds for the proposed methods. Our bounds involve a trade-off between the connectivity of the network and its fidelity to the true parameter. The usefulness of our methods is illustrated through simulation studies and two text clustering applications.},
  archive      = {J_CSDA},
  author       = {Xin Zhao and Jingru Zhang and Wei Lin},
  doi          = {10.1016/j.csda.2022.107634},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107634},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Clustering multivariate count data via dirichlet-multinomial network fusion},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust tests for scatter separability beyond gaussianity.
<em>CSDA</em>, <em>179</em>, 107633. (<a
href="https://doi.org/10.1016/j.csda.2022.107633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separability (a Kronecker product) of a scatter matrix is one of favorable structures when multivariate heavy-tailed data are collected in a matrix form, due to its parsimonious representation. However, little attempt has been made to test separability beyond Gaussianity . In this paper, we present nonparametric separability tests that can be applied to a larger class of multivariate distributions not only including elliptical distributions but also generalized elliptical distributions and transelliptical distributions. The proposed test statistic exploits robustness of Tyler&#39;s M (or Kendall&#39;s tau) estimator and a likelihood function of a scaled variable. Since its distribution is hard to specify, we approximate the p-value using a permutation procedure, whose unbiasedness is obtained from the permutation invariance of multivariate paired data. Our simulation study demonstrates the efficacy of our method against other alternatives, and we apply it to rhesus monkey data and corpus callosum data.},
  archive      = {J_CSDA},
  author       = {Seungkyu Kim and Seongoh Park and Johan Lim and Sang Han Lee},
  doi          = {10.1016/j.csda.2022.107633},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107633},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust tests for scatter separability beyond gaussianity},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable screening based on gaussian centered l-moments.
<em>CSDA</em>, <em>179</em>, 107632. (<a
href="https://doi.org/10.1016/j.csda.2022.107632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important challenge in big data is identification of important variables. For this purpose, methods of discovering variables with non-standard univariate marginal distributions are proposed. The conventional moments based summary statistics can be well-adopted, but their sensitivity to outliers can lead to selection based on a few outliers rather than distributional shape such as bimodality. To address this type of non-robustness, the L-moments are considered. Using these in practice, however, has a limitation since they do not take zero values at the Gaussian distributions to which the shape of a marginal distribution is most naturally compared. As a remedy, Gaussian Centered L-moments are proposed, which share advantages of the L-moments, but have zeros at the Gaussian distributions . The strength of Gaussian Centered L-moments over other conventional moments is shown in theoretical and practical aspects such as their performances in screening important genes in cancer genetics data.},
  archive      = {J_CSDA},
  author       = {Hyowon An and Kai Zhang and Hannu Oja and J.S. Marron},
  doi          = {10.1016/j.csda.2022.107632},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107632},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable screening based on gaussian centered L-moments},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster-robust estimators for multivariate mixed-effects
meta-regression. <em>CSDA</em>, <em>179</em>, 107631. (<a
href="https://doi.org/10.1016/j.csda.2022.107631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analyses frequently include trials that report multiple outcomes based on a common set of study participants. These outcomes will generally be correlated. Cluster-robust variance-covariance estimators are a fruitful approach for synthesizing dependent outcomes. However, when the number of studies is small, state-of-the-art robust estimators can yield inflated Type 1 errors. Therefore, two new cluster-robust estimators are presented, in order to improve small sample performance. For both new estimators the idea is to transform the estimated variances of the residuals using only the diagonal entries of the hat matrix . The proposals are asymptotically equivalent to previously suggested cluster-robust estimators such as the bias reduced linearization approach. The methods are applied to a dataset of 81 trials examining overall and disease-free survival in neuroblastoma patients with amplified versus normal MYC-N genes. Furthermore, their performance is compared and contrasted in an extensive simulation study. The focus is on bivariate meta-regression, although the approaches can be applied more generally.},
  archive      = {J_CSDA},
  author       = {Thilo Welz and Wolfgang Viechtbauer and Markus Pauly},
  doi          = {10.1016/j.csda.2022.107631},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107631},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Cluster-robust estimators for multivariate mixed-effects meta-regression},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Block-diagonal precision matrix regularization for
ultra-high dimensional data. <em>CSDA</em>, <em>179</em>, 107630. (<a
href="https://doi.org/10.1016/j.csda.2022.107630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method that estimates the precision matrix of multiple variables in the extreme scope of “ultrahigh dimension” and “small sample-size” is proposed. Initially, a covariance column-wise screening method is provided in order to identify a small sub-group, which are significantly correlated, from thousands and even millions of variables. Then, a regularization of block-diagonal covariance structure of the thousands or millions of variables is imposed, in which only the covariances of variables in that small sub-group are retained and all others vanish. It is further proven that under some mild conditions the vital sub-group identified by the covariance column-wise screening method is consistent. A major advantage of the proposed method is its efficiency - it produces a reliable precision matrix estimator for thousands of variables within a few of seconds while the existing methods take at least several hours and even so still yield inaccurate estimators. Empirical data studies and numerical simulations show that the proposed precision matrix estimation greatly outperforms existing methods in the sense of taking much less computing time and resulting in much more accurate estimation when dealing with ultrahigh dimensional data.},
  archive      = {J_CSDA},
  author       = {Yihe Yang and Hongsheng Dai and Jianxin Pan},
  doi          = {10.1016/j.csda.2022.107630},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107630},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Block-diagonal precision matrix regularization for ultra-high dimensional data},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian nonparametric hypothesis testing for longitudinal
data analysis. <em>CSDA</em>, <em>179</em>, 107629. (<a
href="https://doi.org/10.1016/j.csda.2022.107629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Bayesian nonparametric procedure for longitudinal data analysis is proposed. The procedure simultaneously tests for the effects in the mean structure preserving the main effects when interactions are present. The method is highly flexible in that it does not assume a particular distribution for the errors and random effects, as is usually done in longitudinal data analysis. The correlation between the repeated measurements is captured via a Markovian time-dependent Dirichlet process mixture. Specifically, when this latter is represented via a species sampling model with stick-breaking weights, the effect of predictors is driven by the underlying atoms, and the time evolution driven by time-dependent weights. The performance of the proposed method is illustrated using both simulated and real data sets .},
  archive      = {J_CSDA},
  author       = {Luz Adriana Pereira and Luis Gutiérrez and Daniel Taylor-Rodríguez and Ramsés H. Mena},
  doi          = {10.1016/j.csda.2022.107629},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107629},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian nonparametric hypothesis testing for longitudinal data analysis},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tree-based ensembles for multi-output regression: Comparing
multivariate approaches with separate univariate ones. <em>CSDA</em>,
<em>179</em>, 107628. (<a
href="https://doi.org/10.1016/j.csda.2022.107628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-based ensembles such as the Random Forest are modern classics among statistical learning methods. In particular, they are used for predicting univariate responses. In case of multiple outputs the question arises whether it is better to separately fit univariate models or directly follow a multivariate approach. For the latter, several possibilities exist that are, e.g. based on modified splitting or stopping rules for multi-output regression. These methods are compared in extensive simulations and a real data example to help in answering the primary question when to use multivariate ensemble techniques instead of univariate ones.},
  archive      = {J_CSDA},
  author       = {Lena Schmid and Alexander Gerharz and Andreas Groll and Markus Pauly},
  doi          = {10.1016/j.csda.2022.107628},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107628},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Tree-based ensembles for multi-output regression: Comparing multivariate approaches with separate univariate ones},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential estimation of temporally evolving latent space
network models. <em>CSDA</em>, <em>179</em>, 107627. (<a
href="https://doi.org/10.1016/j.csda.2022.107627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic network data describe interactions among a fixed population through time. This data type can be modelled using the latent space framework, where the probability of a connection forming is expressed as a function of low-dimensional latent coordinates associated with the nodes, and sequential estimation of model parameters can be achieved via Sequential Monte Carlo (SMC) methods. In this setting, SMC is a natural candidate for estimation which offers greater scalability than existing approaches commonly considered in the literature, allows for estimates to be conveniently updated given additional observations and facilitates both online and offline inference. A novel approach to sequentially infer parameters of dynamic latent space network models is proposed by building on techniques from the high-dimensional SMC literature. The scalability and performance of the proposed approach is explored via simulation, and the flexibility under model variants is demonstrated. Finally, a real-world dataset describing classroom contacts is analysed using the proposed methodology.},
  archive      = {J_CSDA},
  author       = {Kathryn Turnbull and Christopher Nemeth and Matthew Nunes and Tyler McCormick},
  doi          = {10.1016/j.csda.2022.107627},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107627},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sequential estimation of temporally evolving latent space network models},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online learning for the dirichlet process mixture model via
weakly conjugate approximation. <em>CSDA</em>, <em>179</em>, 107626. (<a
href="https://doi.org/10.1016/j.csda.2022.107626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dirichlet process (DP) mixture model is widely used for clustering and density estimation. The use of the DP mixture model has become computationally feasible because of the development of various Markov chain Monte Carlo algorithms. However, when analyzing large data, DP mixture models are impractical owing to their high computational costs. An online learning algorithm that processes data sequentially as they arrive is an attractive way to analyze large data. Existing online learning algorithms based on variational inference are very fast; however, their performance is unsatisfactory owing to the crude approximation of the posterior distribution . We propose a novel mini-batch online learning algorithm based on assumed density filtering, which takes full advantage of available computing resources to improve performance and achieves better performances relative to existing online algorithms based on variational inference.},
  archive      = {J_CSDA},
  author       = {Kuhwan Jeong and Minwoo Chae and Yongdai Kim},
  doi          = {10.1016/j.csda.2022.107626},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107626},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Online learning for the dirichlet process mixture model via weakly conjugate approximation},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group symmetric latin hypercube designs for symmetrical
global sensitivity analysis. <em>CSDA</em>, <em>179</em>, 107625. (<a
href="https://doi.org/10.1016/j.csda.2022.107625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symmetrical global sensitivity analysis (SGSA) is of use in studying the symmetrical structure of a model via symmetrical global sensitivity indices (SGSI). The computation of SGSI is a challenging task because the model to compute is black-box and expensive. An experimental design , called a group symmetric Latin hypercube design (GSLHD) is proposed for SGSA. It achieves maximum uniformity in univariate margins and maintains a symmetrical structure that ensures the estimability of SGSI. Based on the proposed GSLHD, SGSI can be efficiently estimated by using output values evaluated from the model. Construction methods and sampling properties of the proposed design are presented. Numerical studies on benchmark test problems demonstrate that the proposed design is effective in shrinking the estimation bias and volatility with economically feasible sample sizes.},
  archive      = {J_CSDA},
  author       = {Xiaodi Wang and Hengzhen Huang},
  doi          = {10.1016/j.csda.2022.107625},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107625},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Group symmetric latin hypercube designs for symmetrical global sensitivity analysis},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sparse additive model for high-dimensional interactions
with an exposure variable. <em>CSDA</em>, <em>179</em>, 107624. (<a
href="https://doi.org/10.1016/j.csda.2022.107624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A conceptual paradigm for onset of a new disease is often considered to be the result of changes in entire biological networks whose states are affected by a complex interaction of genetic and environmental factors. However, when modeling a relevant phenotype as a function of high dimensional measurements, power to estimate interactions is low, the number of possible interactions could be enormous and their effects may be non-linear. A method called sail for detecting non-linear interactions with a key environmental or exposure variable in high-dimensional settings which respects the strong or weak heredity constraints is proposed. It is proven that asymptotically, sail possesses the oracle property, i.e., it performs as well as if the true model were known in advance. A computationally efficient fitting algorithm with automatic tuning parameter selection, which scales to high-dimensional datasets is proposed. Simulation results show that sail outperforms existing penalized regression methods in terms of prediction accuracy and support recovery when there are non-linear interactions with an exposure variable. sail is applied to detect non-linear interactions between genes and a prenatal psychosocial intervention program on cognitive performance in children at 4 years of age. Results show that individuals who are genetically predisposed to lower educational attainment are those who stand to benefit the most from the intervention. The proposed algorithms are implemented in an R package available on CRAN ( https://cran.r-project.org/package=sail ).},
  archive      = {J_CSDA},
  author       = {Sahir R. Bhatnagar and Tianyuan Lu and Amanda Lovato and David L. Olds and Michael S. Kobor and Michael J. Meaney and Kieran O&#39;Donnell and Archer Y. Yang and Celia M.T. Greenwood},
  doi          = {10.1016/j.csda.2022.107624},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107624},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A sparse additive model for high-dimensional interactions with an exposure variable},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel mediation analysis with structured unmeasured
mediator-outcome confounding. <em>CSDA</em>, <em>179</em>, 107623. (<a
href="https://doi.org/10.1016/j.csda.2022.107623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis usually requires the assumption that there is no unmeasured mediator-outcome confounder . However, this may not hold in many social and scientific studies. Though various parametric and nonparametric mediation methods have been developed, this assumption remains instrumental, without which the causal effects are not identifiable unless alternative assumptions are imposed. To circumvent this, a multilevel parametric structural equation modeling framework is proposed to relax this no unmeasured mediator-outcome confounding assumption under a specific data setting inspired by a real experiment. Using the proposed framework, it is shown that the causal effects are identifiable and consistently estimated. Likelihood-based approaches are proposed with efficient optimization algorithms to estimate the parameters, including the unmeasured confounding effect, instead of performing sensitivity analysis. The asymptotic consistency is established. Using extensive simulations and a functional magnetic resonance imaging dataset, the improvement of the approaches over existing methods is demonstrated. The R package macc for implementation is available on CRAN.},
  archive      = {J_CSDA},
  author       = {Yi Zhao and Xi Luo},
  doi          = {10.1016/j.csda.2022.107623},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107623},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multilevel mediation analysis with structured unmeasured mediator-outcome confounding},
  volume       = {179},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate sparse laplacian shrinkage for joint estimation
of two graphical structures. <em>CSDA</em>, <em>178</em>, 107620. (<a
href="https://doi.org/10.1016/j.csda.2022.107620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate regression models are widely used in various fields for fitting multiple responses. In this paper, we proposed a sparse Laplacian shrinkage estimator for the high-dimensional multivariate regression models. We consider two graphical structures among predictors and responses. The proposed method explores the regression relationship allowing the predictors and responses derived from different multivariate normal distributions with general covariance matrices . In practice, the correlations within data are often complex and interact with each other based on the regression function . The proposed method solves this problem by building a structured penalty to encourage the shared structure between the graphs and the regression coefficients . We provide theoretical results under reasonable conditions and discuss the related algorithm. The effectiveness of the proposed method is demonstrated in a variety of simulations as well as an application to the index tracking problem in the stock market.},
  archive      = {J_CSDA},
  author       = {Yuehan Yang and Siwei Xia and Hu Yang},
  doi          = {10.1016/j.csda.2022.107620},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107620},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multivariate sparse laplacian shrinkage for joint estimation of two graphical structures},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel multiplier bootstrap tests for high-dimensional data
with applications to MANOVA. <em>CSDA</em>, <em>178</em>, 107619. (<a
href="https://doi.org/10.1016/j.csda.2022.107619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New bootstrap tests are proposed for linear hypotheses testing of high-dimensional means. In particular, they handle multiple-sample one- and two-way MANOVA tests with unequal cell sizes and unequal unknown cell covariances, as well as contrast tests in elegant and unified way. New tests are compared theoretically and on simulations studies with existing popular contemporary tests. They enjoy consistency, computational efficiency, very mild moment/tail conditions. They avoid the estimation of correlation or precision matrices , and allow the dimension to grow with sample size exponentially. Additionally, they allow the number of groups and the sparsity to grow with the sample size exponentially, thus broadening their applicability.},
  archive      = {J_CSDA},
  author       = {Nilanjan Chakraborty and Lyudmila Sakhanenko},
  doi          = {10.1016/j.csda.2022.107619},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107619},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Novel multiplier bootstrap tests for high-dimensional data with applications to MANOVA},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Applications on linear spectral statistics of
high-dimensional sample covariance matrix with divergent spectrum.
<em>CSDA</em>, <em>178</em>, 107617. (<a
href="https://doi.org/10.1016/j.csda.2022.107617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale statistical inference when the sample size n and dimension p both tend to infinity, the original central limit theorems (CLTs) produce the bounded spectral norm assumption of the covariance matrix which excludes many important applications. Recently, a new CLT (DCLT) was established for the unbounded population spectrum, allowing utilization with divergent spectral norm population models. Comparative simulations are provided in this study for the original CLT and DCLT, and applications for John&#39;s test, interval estimation , and point estimation. Numerical results document the greater performance of DCLT than the original CLT in most cases. Moreover, for the bounded population spectrum, the DCLT modifies the limiting mean and variance shift, and gains preferable theoretical results with small p and n . Real data implementation are illustrated on a radio frequency dataset.},
  archive      = {J_CSDA},
  author       = {Yangchun Zhang and Yirui Zhou and Xiaowei Liu},
  doi          = {10.1016/j.csda.2022.107617},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107617},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Applications on linear spectral statistics of high-dimensional sample covariance matrix with divergent spectrum},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal designs for semi-parametric dose-response models
under random contamination. <em>CSDA</em>, <em>178</em>, 107615. (<a
href="https://doi.org/10.1016/j.csda.2022.107615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of personalized medicine, it is more and more crucial to capture not only the dose-effect but also the effects of the prognostic factors due to individual differences in a dose-response experiment. This paper considers the design issue for predicting semi-parametric dose-response curves in the presence of linear effects of covariates . Inspired by the Neyman-Pearson paradigm, a novel design criterion, namely bias constraint optimality , is introduced to minimize the overall prediction error. The corresponding equivalence theorems are established, the characteristics of the optimal designs are shown, and an equivalent bias compound optimality criterion is proposed for practical implementation. Based on the obtained theoretical results, efficient algorithms for searching for optimal designs are developed. Numerical simulations are given to illustrate the superior performance of the obtained optimal designs.},
  archive      = {J_CSDA},
  author       = {Jun Yu and Xiran Meng and Yaping Wang},
  doi          = {10.1016/j.csda.2022.107615},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107615},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Optimal designs for semi-parametric dose-response models under random contamination},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape-constrained estimation in functional regression with
bernstein polynomials. <em>CSDA</em>, <em>178</em>, 107614. (<a
href="https://doi.org/10.1016/j.csda.2022.107614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape restrictions on functional regression coefficients such as non-negativity, monotonicity, convexity or concavity are often available in the form of a prior knowledge or required to maintain a structural consistency in functional regression models. A new estimation method is developed in shape-constrained functional regression models using Bernstein polynomials . Specifically, estimation approaches from nonparametric regression are extended to functional data, properly accounting for shape-constraints in a large class of functional regression models such as scalar-on-function regression (SOFR), function-on-scalar regression (FOSR), and function-on-function regression (FOFR). Theoretical results establish the asymptotic consistency of the constrained estimators under standard regularity conditions . A projection based approach provides point-wise asymptotic confidence intervals for the constrained estimators. A bootstrap test is developed facilitating testing of the shape constraints. Numerical analysis using simulations illustrates improvement in efficiency of the estimators from the use of the proposed method under shape constraints. Two applications include i) modeling a drug effect in a mental health study via shape-restricted FOSR and ii) modeling subject-specific quantile functions of accelerometry-estimated physical activity in the Baltimore Longitudinal Study of Aging (BLSA) as outcomes via shape-restricted quantile-function on scalar regression (QFOSR). R software implementation and illustration of the proposed estimation method and the test is provided.},
  archive      = {J_CSDA},
  author       = {Rahul Ghosal and Sujit Ghosh and Jacek Urbanek and Jennifer A. Schrack and Vadim Zipunnikov},
  doi          = {10.1016/j.csda.2022.107614},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107614},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Shape-constrained estimation in functional regression with bernstein polynomials},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical clustered multiclass discriminant analysis via
cross-validation. <em>CSDA</em>, <em>178</em>, 107613. (<a
href="https://doi.org/10.1016/j.csda.2022.107613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is a well-known method for multiclass classification and dimensionality reduction. However, in general, ordinary LDA does not achieve high prediction accuracy when observations in some classes are difficult to be classified. A novel cluster-based LDA method is proposed that significantly improves prediction accuracy. Hierarchical clustering is adopted, and the dissimilarity measure of two clusters is defined by the cross-validation (CV) value. Therefore, clusters are constructed such that the misclassification error rate is minimized. The proposed approach involves a heavy computational load because the CV value must be computed at each step of the hierarchical clustering algorithm. To address this issue, a regression formulation for LDA is developed and an efficient algorithm that computes an approximate CV value is constructed. The performance of the proposed method is investigated by applying it to both artificial and real datasets. The proposed method provides high prediction accuracy with fast computation from both numerical and theoretical viewpoints.},
  archive      = {J_CSDA},
  author       = {Kei Hirose and Kanta Miura and Atori Koie},
  doi          = {10.1016/j.csda.2022.107613},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107613},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hierarchical clustered multiclass discriminant analysis via cross-validation},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The circular quantile residual. <em>CSDA</em>, <em>178</em>,
107612. (<a href="https://doi.org/10.1016/j.csda.2022.107612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular-linear regression is often used to model the relationship between a circular dependent variable and a set of linear predictor variables . It is used in many areas such as meteorology, biology, and medicine. For checking model adequacy, it is desirable to use residuals that are approximately standard normally distributed. Most of the residuals used in circular regression models do not meet this requirement and are used especially for outlier identification. Other residuals are limited to the von Mises regression models. An asymptotically standard normally distributed residual that can be used for any parametric circular-linear regression model is introduced. Monte Carlo simulation studies suggest that the distribution of this residual is well approximated by the standard normal distribution in small samples. To study the behavior of this residual, two regression models are introduced, and two applications are used to show that the proposed residual can detect model misspecification .},
  archive      = {J_CSDA},
  author       = {Ana C.C. Andrade and Gustavo H.A. Pereira and Rinaldo Artes},
  doi          = {10.1016/j.csda.2022.107612},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107612},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The circular quantile residual},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust spline approach in partially linear additive
models. <em>CSDA</em>, <em>178</em>, 107611. (<a
href="https://doi.org/10.1016/j.csda.2022.107611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially linear additive models generalize linear regression models by assuming that the relationship between the response and a set of explanatory variables is linear on some of the covariates , while the other ones enter into the model through unknown univariate smooth functions. The harmful effect of outliers either in the residuals or in the covariates involved in the linear component has been described in the situation of partially linear models, that is, when only one nonparametric component is involved. When dealing with additive components , the problem of providing reliable estimators when atypical data arise is of practical importance motivating the need of robust procedures . Based on this fact, a family of robust estimators for partially linear additive models that combines B -splines with robust linear MM -regression estimators is proposed. Under mild assumptions, consistency results and rates of convergence for the proposed estimators are derived. Furthermore, the asymptotic normality for the linear regression estimators is obtained. A Monte Carlo study is carried out to compare, under different models and contamination schemes, the performance of the robust MM -proposal based on B -splines with its classical counterpart and also with a quantile approach. The obtained results show the benefits of using the robust MM -approach. The analysis of a real data set illustrates the usefulness of the proposed method.},
  archive      = {J_CSDA},
  author       = {Graciela Boente and Alejandra Mercedes Martínez},
  doi          = {10.1016/j.csda.2022.107611},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107611},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A robust spline approach in partially linear additive models},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smoothed tensor quantile regression estimation for
longitudinal data. <em>CSDA</em>, <em>178</em>, 107609. (<a
href="https://doi.org/10.1016/j.csda.2022.107609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As extensions of vector and matrix data with ultrahigh dimensionality and complex structures, tensor data are fast emerging in a large variety of scientific applications. In this paper, a two-stage estimation procedure for linear tensor quantile regression (QR) with longitudinal data is proposed. In the first stage, we account for within-subject correlations by using the generalized estimating equations and then impose a low-rank assumption on tensor coefficients to reduce the number of parameters by a canonical polyadic decomposition. To avoid the asymptotic analysis and computation problems caused by the non-smooth QR score function , kernel smoothing method is applied in the second stage to construct the smoothed tensor QR estimator. When the number of rank is given, a block-relaxation algorithm is proposed to estimate the regression coefficients . A modified BIC is applied to estimate the number of rank in practice and show the rank selection consistency. Further, a regularized estimator and its algorithm are investigated for better interpretation and efficiency. The asymptotic properties of the proposed estimators are established. Simulation studies and a real example on Beijing Air Quality data set are provided to show the performance of the proposed estimators.},
  archive      = {J_CSDA},
  author       = {Baofang Ke and Weihua Zhao and Lei Wang},
  doi          = {10.1016/j.csda.2022.107609},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107609},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Smoothed tensor quantile regression estimation for longitudinal data},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grouped spatial autoregressive model. <em>CSDA</em>,
<em>178</em>, 107601. (<a
href="https://doi.org/10.1016/j.csda.2022.107601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the internet, network data with replications can be collected at different time points . The spatial autoregressive panel (SARP) model is a useful tool for analyzing such network data. However, in the traditional SARP model, all individuals are assumed to be homogeneous in their network autocorrelation coefficients, while in practice, correlations could differ for the nodes in different groups. Here, a grouped spatial autoregressive (GSAR) model based on the SARP model is proposed to permit network autocorrelation heterogeneity among individuals, while analyzing network data with independent replications across different time points and strong spatial effects. Each individual in the network belongs to a latent specific group, which is characterized by a set of parameters. Two estimation methods are studied: two-step naive least-squares estimator, and two-step conditional least-squares estimator. Furthermore, their corresponding asymptotic properties and technical conditions are investigated. To demonstrate the performance of the proposed GSAR model and its corresponding estimation methods, numerical analysis was performed on simulated and real data.},
  archive      = {J_CSDA},
  author       = {Danyang Huang and Wei Hu and Bingyi Jing and Bo Zhang},
  doi          = {10.1016/j.csda.2022.107601},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107601},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Grouped spatial autoregressive model},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction in functional regression with discretely observed
and noisy covariates. <em>CSDA</em>, <em>178</em>, 107600. (<a
href="https://doi.org/10.1016/j.csda.2022.107600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider discretely sampled and noisy functional data as explanatory variables in a linear regression. If the primary goal is prediction, then it is argued that the practical gain of embedding the problem into a scalar-on-function regression is limited. Instead, the approximate factor model structure of the predictors is employed and the response is regressed on an appropriate number of factor scores. This approach is shown to be consistent under mild technical assumptions, it is numerically efficient, and it yields good practical performance in both, simulations and real data settings.},
  archive      = {J_CSDA},
  author       = {Siegfried Hörmann and Fatima Jammoul},
  doi          = {10.1016/j.csda.2022.107600},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107600},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Prediction in functional regression with discretely observed and noisy covariates},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A markov decision process for response-adaptive
randomization in clinical trials. <em>CSDA</em>, <em>178</em>, 107599.
(<a href="https://doi.org/10.1016/j.csda.2022.107599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, response-adaptive randomization (RAR) has the appealing ability to assign more subjects to better-performing treatments based on interim results. Traditional RAR strategies alter the randomization ratio on a patient-by-patient basis. An alternate approach is blocked RAR, which groups patients together in blocks and recomputes the randomization ratio in a block-wise fashion; past works show that this provides robustness against time-trend bias. However, blocked RAR poses additional questions: how many blocks should there be, and how many patients should each block contain? TrialMDP is an algorithm that designs two-armed blocked RAR clinical trials. It differs from other trial design approaches in that it optimizes the size and number of blocks in addition to their treatment allocations. More precisely, the algorithm yields an adaptive policy that chooses the size and allocation ratio of the next block, based on results seen up to that point in the trial. TrialMDP is related to past works that compute optimal trial designs via dynamic programming . The algorithm maximizes a utility function balancing (i) statistical power, (ii) patient outcomes, and (iii) the number of blocks. It attains significant improvements in utility over a suite of baseline designs , and gives useful control over the tradeoff between statistical power and patient outcomes. It is well suited for small trials that assign high cost to patient failures.},
  archive      = {J_CSDA},
  author       = {David Merrell and Thevaa Chandereng and Yeonhee Park},
  doi          = {10.1016/j.csda.2022.107599},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107599},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A markov decision process for response-adaptive randomization in clinical trials},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualization and assessment of model selection uncertainty.
<em>CSDA</em>, <em>178</em>, 107598. (<a
href="https://doi.org/10.1016/j.csda.2022.107598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although model selection is ubiquitous in scientific discovery, the stability and uncertainty of the selected model is often hard to evaluate. How to characterize the random behavior of the model selection procedure is the key to understand and quantify the model selection uncertainty. To this goal, initially several graphical tools are proposed. These include the G-plots and H-plots, to visualize the distribution of the selected model. Then the concept of model selection deviation to quantify the model selection uncertainty is introduced. Similar to the standard error of an estimator, model selection deviation measures the stability of the selected model given by a model selection procedure . For such a measure, a bootstrap estimation procedure is discussed and its desirable performance is demonstrated through simulation studies and real data analysis.},
  archive      = {J_CSDA},
  author       = {Yichen Qin and Linna Wang and Yang Li and Rong Li},
  doi          = {10.1016/j.csda.2022.107598},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107598},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Visualization and assessment of model selection uncertainty},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust prediction interval estimation for gaussian processes
by cross-validation method. <em>CSDA</em>, <em>178</em>, 107597. (<a
href="https://doi.org/10.1016/j.csda.2022.107597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic regression models typically use the Maximum Likelihood Estimation or Cross-Validation to fit parameters. These methods can give an advantage to the solutions that fit observations on average, but they do not pay attention to the coverage and the width of Prediction Intervals. A robust two-step approach is used to address the problem of adjusting and calibrating Prediction Intervals for Gaussian Processes Regression. First, the covariance hyperparameters are determined by a standard Cross-Validation or Maximum Likelihood Estimation method. A Leave-One-Out Coverage Probability is introduced as a metric to adjust the covariance hyperparameters and assess the optimal type II Coverage Probability to a nominal level. Then a relaxation method is applied to choose the hyperparameters that minimize the Wasserstein distance between the Gaussian distribution with the initial hyperparameters (obtained by Cross-Validation or Maximum Likelihood Estimation) and the proposed Gaussian distribution with the hyperparameters that achieve the desired Coverage Probability. The method gives Prediction Intervals with appropriate coverage probabilities and small widths.},
  archive      = {J_CSDA},
  author       = {Naoufal Acharki and Antoine Bertoncello and Josselin Garnier},
  doi          = {10.1016/j.csda.2022.107597},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107597},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust prediction interval estimation for gaussian processes by cross-validation method},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast bayesian inference on spectral analysis of multivariate
stationary time series. <em>CSDA</em>, <em>178</em>, 107596. (<a
href="https://doi.org/10.1016/j.csda.2022.107596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral analysis discovers trends, periodic and other characteristics of a time series by representing these features in the frequency domain. However, when multivariate time series are considered, and the number of components increases, the size of the spectral density matrix grows quadratically, making estimation and inference rather challenging. The proposed novel Bayesian framework considers a Whittle likelihood-based spectral modeling approach and imposes a discounted regularized horseshoe prior on the coefficients that define a spline representation of each of the components of a Cholesky factorization of the inverse spectral density matrix. The proposed prior structure leads to a model that provides higher posterior accuracy when compared to alternative currently available approaches. To achieve fast inference that takes advantage of the massive power of modern hardware (e.g., GPU), a stochastic gradient variational Bayes approach is proposed for the highly parallelizable posterior inference that provides computational flexibility for modeling high-dimensional time series. The accurate empirical performance of the proposed method is illustrated via extensive simulation studies and the analysis of two datasets: a wind speed data from 6 locations in California, and a 61-channel electroencephalogram data recorded on two contrasting subjects under specific experimental conditions.},
  archive      = {J_CSDA},
  author       = {Zhixiong Hu and Raquel Prado},
  doi          = {10.1016/j.csda.2022.107596},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107596},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast bayesian inference on spectral analysis of multivariate stationary time series},
  volume       = {178},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel-and-stream accelerator for computationally fast
supervised learning. <em>CSDA</em>, <em>177</em>, 107587. (<a
href="https://doi.org/10.1016/j.csda.2022.107587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two dominant distributed computing strategies have emerged to overcome the computational bottleneck of supervised learning with big data: parallel data processing in the MapReduce paradigm and serial data processing in the online streaming paradigm. Despite the two strategies&#39; common divide-and-combine approach, they differ in how they aggregate information, leading to different trade-offs between statistical and computational performance. The authors propose a new hybrid paradigm, termed a Parallel-and-Stream Accelerator (PASA) , that uses the strengths of both strategies for computationally fast and statistically efficient supervised learning. PASA&#39;s architecture nests online streaming processing into each distributed and parallelized data process in a MapReduce framework. PASA leverages the advantages and mitigates the disadvantages of both the MapReduce and online streaming approaches to deliver a more flexible paradigm satisfying practical computing needs. The authors study the analytic properties and computational complexity of PASA, and detail its implementation for two key statistical learning tasks. PASA&#39;s performance is illustrated through simulations and a large-scale data example building a prediction model for online purchases from advertising data.},
  archive      = {J_CSDA},
  author       = {Emily C. Hector and Lan Luo and Peter X.-K. Song},
  doi          = {10.1016/j.csda.2022.107587},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107587},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Parallel-and-stream accelerator for computationally fast supervised learning},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian joint modeling for causal mediation analysis with a
binary outcome and a binary mediator: Exploring the role of obesity in
the association between cranial radiation therapy for childhood acute
lymphoblastic leukemia treatment and the long-term risk of insulin
resistance. <em>CSDA</em>, <em>177</em>, 107586. (<a
href="https://doi.org/10.1016/j.csda.2022.107586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis with a binary outcome is notoriously more challenging than with a continuous outcome. A new Bayesian approach for performing causal mediation with a binary outcome and a binary mediator, named the t -link approach, is introduced. This approach relies on the Bayesian multivariate logistic regression model introduced by O&#39;Brien and Dunson (2004) and its Student- t approximation . By re-expressing the Mediation Formula, it is shown how to use this multivariate latent model for estimating the natural direct and indirect effects of an exposure on an outcome in any measure scale of interest (e.g., odds or risk ratio, risk difference). The t -link mediation approach has several valuable features which, to our knowledge, are not found together in existing binary-binary mediation analysis approaches. In particular, it allows for sensitivity analyses regarding the impact of unmeasured mediator-outcome confounders on the natural effects estimates. The proposed mediation approach was evaluated and compared with two other benchmark approaches using simulated data. Results revealed the usefulness of the t -link mediation approach when the sample size is small or moderate. Lastly, the t -link approach was applied for assessing the impact of cranial radiation therapy given to treat childhood acute lymphoblastic leukemia on the long-term risk of insulin resistance, where this effect is possibly mediated by obesity.},
  archive      = {J_CSDA},
  author       = {Miguel Caubet and Mariia Samoilenko and Simon Drouin and Daniel Sinnett and Maja Krajinovic and Caroline Laverdière and Valérie Marcil and Geneviève Lefebvre},
  doi          = {10.1016/j.csda.2022.107586},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107586},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian joint modeling for causal mediation analysis with a binary outcome and a binary mediator: Exploring the role of obesity in the association between cranial radiation therapy for childhood acute lymphoblastic leukemia treatment and the long-term risk of insulin resistance},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The dimension-wise quadrature estimation of dynamic latent
variable models for count data. <em>CSDA</em>, <em>177</em>, 107585. (<a
href="https://doi.org/10.1016/j.csda.2022.107585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When dynamic latent variable models are specified for discrete and/or mixed observations, problems related to the integration of the likelihood function arise since analytical solutions do not exist. A recently developed dimension-wise quadrature is applied to deal with these likelihoods with high-dimensional integrals. A comparison is performed with the pairwise likelihood method, one of the most often used remedies. Both a real data application and a simulation study show the superior performance of the dimension-wise quadrature with respect to the pairwise likelihood in estimating the parameters of the latent autoregressive process .},
  archive      = {J_CSDA},
  author       = {Silvia Bianconcini and Silvia Cagnone},
  doi          = {10.1016/j.csda.2022.107585},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107585},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The dimension-wise quadrature estimation of dynamic latent variable models for count data},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation for partial functional partially linear additive
model. <em>CSDA</em>, <em>177</em>, 107584. (<a
href="https://doi.org/10.1016/j.csda.2022.107584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A class of scalar-on-function regression estimating the nonparametric effects of a functional predictor and semiparametric effects of multivariate scalar predictors is investigated. The proposed model is motivated by applications considering both functional and scalar predictors with possibly nonlinear effects . A two-step estimation procedure together with functional principal components analysis allows the simultaneous estimation of nonlinear effects of both the functional and scalar predictors. The computation of the proposed estimators is efficient and does not require iterative algorithms , which is desirable for high dimensional setting. Asymptotic properties such as convergence rate and asymptotic normality have been established. Finite sample performances are studied through simulations and data analysis in functional neuroimaging and real estate analytic.},
  archive      = {J_CSDA},
  author       = {Qingguo Tang and Wei Tu and Linglong Kong},
  doi          = {10.1016/j.csda.2022.107584},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107584},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation for partial functional partially linear additive model},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric bagging clustering methods to identify latent
structures from a sequence of dependent categorical data. <em>CSDA</em>,
<em>177</em>, 107583. (<a
href="https://doi.org/10.1016/j.csda.2022.107583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonparametric bagging clustering methods are studied and compared to identify latent structures from a sequence of dependent categorical data observed along a one-dimensional (discrete) time domain. The frequency of the observed categories is assumed to be generated by a (slowly varying) latent signal, according to latent state-specific probability distributions. The bagging clustering methods use random tessellations (partitions) of the time domain and clustering of the category frequencies of the observed data in the tessellation cells to recover the latent signal, within a bagging framework. New and existing ways of generating the tessellations and clustering are discussed and combined into different bagging clustering methods. Edge tessellations and adaptive tessellations are the new proposed ways of forming partitions. Composite methods are also introduced, that are using (automated) decision rules based on entropy measures to choose among the proposed bagging clustering methods. The performance of all the methods is compared in a simulation study. From the simulation study it can be concluded that local and global entropy measures are powerful tools in improving the recovery of the latent signal, both via the adaptive tessellation strategies (local entropy) and in designing composite methods (global entropy). The composite methods are robust and overall improve performance, in particular the composite method using adaptive (edge) tessellations.},
  archive      = {J_CSDA},
  author       = {Konrad Abramowicz and Sara Sjöstedt de Luna and Johan Strandberg},
  doi          = {10.1016/j.csda.2022.107583},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107583},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric bagging clustering methods to identify latent structures from a sequence of dependent categorical data},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixture-based estimation of entropy. <em>CSDA</em>,
<em>177</em>, 107582. (<a
href="https://doi.org/10.1016/j.csda.2022.107582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The entropy is a measure of uncertainty that plays a central role in information theory . When the distribution of the data is unknown, an estimate of the entropy needs to be obtained from the data sample itself. A semi-parametric estimate is proposed based on a mixture model approximation of the distribution of interest. A Gaussian mixture model is used to illustrate the accuracy and versatility of the proposal, although the estimate can rely on any type of mixture. Performance of the proposed approach is assessed through a series of simulation studies. Two real-life data examples are also provided to illustrate its use.},
  archive      = {J_CSDA},
  author       = {Stéphane Robin and Luca Scrucca},
  doi          = {10.1016/j.csda.2022.107582},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107582},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Mixture-based estimation of entropy},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sparse spatially clustered coefficient model via adaptive
regularization. <em>CSDA</em>, <em>177</em>, 107581. (<a
href="https://doi.org/10.1016/j.csda.2022.107581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large spatial datasets with many spatial covariates have become ubiquitous in many fields in recent years. A question of interest is to identify which covariates are likely to influence a spatial response, and whether and how the effects of these covariates vary across space, including potential abrupt changes from region to region. To solve this question, a new efficient regularized spatially clustered coefficient (RSCC) regression approach is proposed, which could achieve variable selection and identify latent spatially heterogeneous covariate effects with clustered patterns simultaneously. By carefully designing the regularization term of RSCC as a chain graph guided fusion penalty plus a group lasso penalty, the RSCC model is computationally efficient for large spatial datasets while still achieving the theoretical guarantees for estimation. RSCC also adopts the idea of adaptive learning to allow for adaptive weights and adaptive graphs in its regularization terms and further improves the estimation performance. RSCC is applied to study the acceptance of COVID-19 vaccines using county-level data in the United States and discover the determinants of vaccination acceptance with varying effects across counties, revealing important within-state and across-state spatially clustered patterns of covariates effects.},
  archive      = {J_CSDA},
  author       = {Yan Zhong and Huiyan Sang and Scott J. Cook and Paul M. Kellstedt},
  doi          = {10.1016/j.csda.2022.107581},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107581},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sparse spatially clustered coefficient model via adaptive regularization},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of the differencing operator of a
non-stationary time series via testing for zeroes in the spectral
density. <em>CSDA</em>, <em>177</em>, 107580. (<a
href="https://doi.org/10.1016/j.csda.2022.107580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonparametric procedure for identifying the differencing operator of a non-stationary time series is presented and tested. Any proposed differencing operator is first applied to the time series, and the spectral density is tested for zeroes corresponding to the polynomial roots of the operator. A nonparametric tapered spectral density estimator is used, and the subsampling methodology is applied to obtain critical values. Simulations explore the effectiveness of the procedure under a variety of scenarios involving non-stationary processes.},
  archive      = {J_CSDA},
  author       = {Tucker S. McElroy and Agnieszka Jach},
  doi          = {10.1016/j.csda.2022.107580},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107580},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Identification of the differencing operator of a non-stationary time series via testing for zeroes in the spectral density},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and inference for multivariate time series of
counts based on the INGARCH scheme. <em>CSDA</em>, <em>177</em>, 107579.
(<a href="https://doi.org/10.1016/j.csda.2022.107579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling multivariate time series of counts using the integer-valued generalized autoregressive conditional heteroscedastic (INGARCH) scheme is proposed. The key idea is to model each component of the time series with a univariate INGARCH model, where the conditional distribution is modeled with a one-parameter exponential family distribution, and to use a (nonlinear) parametric function of all components to recursively produce the conditional means. It is shown that the proposed multivariate INGARCH (MINGARCH) model is strictly stationary and ergodic. For inference, the quasi-maximum likelihood estimator (QMLE) and the minimum density power divergence estimator (MDPDE) for robust estimation are adopted, and their consistency and asymptotic normality are verified. As an application, the change point test based on the QMLE and MDPDE is illustrated. The Monte Carlo simulation study and real data analysis using the number of weekly syphilis cases in the United States are conducted to confirm the validity of the proposed method.},
  archive      = {J_CSDA},
  author       = {Sangyeol Lee and Dongwon Kim and Byungsoo Kim},
  doi          = {10.1016/j.csda.2022.107579},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107579},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Modeling and inference for multivariate time series of counts based on the INGARCH scheme},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessment of the effect of constraints in a new
multivariate mixed method for statistical matching. <em>CSDA</em>,
<em>177</em>, 107569. (<a
href="https://doi.org/10.1016/j.csda.2022.107569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Multivariate Mixed method for Statistical Matching (MMSM) is proposed. The MMSM is a predictive mean matching method to impute values when integrating two datasets from the same population without overlapping units measuring several common and non-common variables. It considers the multivariate structure of the data by using multivariate Bayesian regression. The MMSM can also include auxiliary information from an additional dataset to improve the computation of intermediate values, and constraints to improve the selection of the donors. The results from a simulation study show that including information from an auxiliary dataset leads to far better results, especially in terms of bias and percentage of correct imputations. The inclusion of constraints also increases the quality of the imputations, and hence of the statistical matching.},
  archive      = {J_CSDA},
  author       = {Juan Claramunt González and Arnout van Delden and Ton de Waal},
  doi          = {10.1016/j.csda.2022.107569},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107569},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Assessment of the effect of constraints in a new multivariate mixed method for statistical matching},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shared bayesian variable shrinkage in multinomial logistic
regression. <em>CSDA</em>, <em>177</em>, 107568. (<a
href="https://doi.org/10.1016/j.csda.2022.107568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Bayesian approaches have been explored for variable selection in the multinomial regression framework. While there are a number of studies considering variable selection in the regression paradigm with a numerical response, the research is limited for a categorical response variable. The proposed approach develops a method for leveraging the features of the global-local shrinkage framework to improve variable selection in baseline categorical logistic regression by introducing new shrinkage priors that encourage similar predictors to be selected across the models for different response levels. To that end, the proposed shrinkage priors share information across response models through the local parameters that favor similar levels of shrinkage for all coefficients (log-odds ratios) of a predictor. Different shrinkage approaches are explored using the horseshoe and normal gamma priors within this setting and compared to a spike-and-slab setup and other shrinkage priors that fail to share information across models. The performance of the approach is investigated in both simulations and a real data application.},
  archive      = {J_CSDA},
  author       = {Md Nazir Uddin and Jeremy T. Gaskins},
  doi          = {10.1016/j.csda.2022.107568},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107568},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Shared bayesian variable shrinkage in multinomial logistic regression},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online non-parametric changepoint detection with application
to monitoring operational performance of network devices. <em>CSDA</em>,
<em>177</em>, 107551. (<a
href="https://doi.org/10.1016/j.csda.2022.107551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a telecommunications application where there are few computational constraints, a novel nonparametric algorithm, NUNC, is introduced to perform an online detection for changes in the distribution of data. Two variants are considered: the first, NUNC Local, detects changes within a sliding window. Conversely, NUNC Global, compares the current window of data to all of the historic information seen so far and makes use of an efficient update step so that this historic information does not need to be stored. To explore the properties of both algorithms, both real and simulated datasets are analysed. Furthermore, a theoretical result for the choice of test threshold to control the false alarm rate is presented, a result that could be applied in other binary segmentation change detection settings.},
  archive      = {J_CSDA},
  author       = {Edward Austin and Gaetano Romano and Idris A. Eckley and Paul Fearnhead},
  doi          = {10.1016/j.csda.2022.107551},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107551},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Online non-parametric changepoint detection with application to monitoring operational performance of network devices},
  volume       = {177},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
