<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TOG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tog---268">TOG - 268</h2>
<ul>
<li><details>
<summary>
(2023). Implicit surface tension for SPH fluid simulation.
<em>TOG</em>, <em>43</em>(1), 13:1–14. (<a
href="https://doi.org/10.1145/3631936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The numerical simulation of surface tension is an active area of research in many different fields of application and has been attempted using a wide range of methods. Our contribution is the derivation and implementation of an implicit cohesion force based approach for the simulation of surface tension effects using the Smoothed Particle Hydrodynamics (SPH) method. We define a continuous formulation inspired by the properties of surface tension at the molecular scale which is spatially discretized using SPH. An adapted variant of the linearized backward Euler method is used for time discretization, which we also strongly couple with an implicit viscosity model. Finally, we extend our formulation with adhesion forces for interfaces with rigid objects. Existing SPH approaches for surface tension in computer graphics are mostly based on explicit time integration, thereby lacking in stability for challenging settings. We compare our implicit surface tension method to these approaches and further evaluate our model on a wider variety of complex scenarios, showcasing its efficacy and versatility. Among others, these include but are not limited to simulations of a water crown, a dripping faucet, and a droplet toy.},
  archive      = {J_TOG},
  author       = {Stefan Rhys Jeske and Lukas Westhofen and Fabian Löschner and José Antonio Fernández-fernández and Jan Bender},
  doi          = {10.1145/3631936},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {13:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Implicit surface tension for SPH fluid simulation},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intrinsic image decomposition via ordinal shading.
<em>TOG</em>, <em>43</em>(1), 12:1–24. (<a
href="https://doi.org/10.1145/3630750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrinsic decomposition is a fundamental mid-level vision problem that plays a crucial role in various inverse rendering and computational photography pipelines. Generating highly accurate intrinsic decompositions is an inherently under-constrained task that requires precisely estimating continuous-valued shading and albedo. In this work, we achieve high-resolution intrinsic decomposition by breaking the problem into two parts. First, we present a dense ordinal shading formulation using a shift- and scale-invariant loss in order to estimate ordinal shading cues without restricting the predictions to obey the intrinsic model. We then combine low- and high-resolution ordinal estimations using a second network to generate a shading estimate with both global coherency and local details. We encourage the model to learn an accurate decomposition by computing losses on the estimated shading as well as the albedo implied by the intrinsic model. We develop a straightforward method for generating dense pseudo ground truth using our model’s predictions and multi-illumination data, enabling generalization to in-the-wild imagery. We present exhaustive qualitative and quantitative analysis of our predicted intrinsic components against state-of-the-art methods. Finally, we demonstrate the real-world applicability of our estimations by performing otherwise difficult editing tasks such as recoloring and relighting.},
  archive      = {J_TOG},
  author       = {Chris Careaga and Yağız Aksoy},
  doi          = {10.1145/3630750},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {12:1–24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Intrinsic image decomposition via ordinal shading},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disentangling structure and appearance in ViT feature space.
<em>TOG</em>, <em>43</em>(1), 11:1–16. (<a
href="https://doi.org/10.1145/3630096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for semantically transferring the visual appearance of one natural image to another. Specifically, our goal is to generate an image in which objects in a source structure image are “painted” with the visual appearance of their semantically related objects in a target appearance image. To integrate semantic information into our framework, our key idea is to leverage a pre-trained and fixed Vision Transformer (ViT) model. Specifically, we derive novel disentangled representations of structure and appearance extracted from deep ViT features. We then establish an objective function that splices the desired structure and appearance representations, interweaving them together in the space of ViT features. Based on our objective function, we propose two frameworks of semantic appearance transfer – “Splice”, which works by training a generator on a single and arbitrary pair of structure-appearance images, and “SpliceNet”, a feed-forward real-time appearance transfer model trained on a dataset of images from a specific domain . Our frameworks do not involve adversarial training, nor do they require any additional input information such as semantic segmentation or correspondences. We demonstrate high-resolution results on a variety of in-the-wild image pairs, under significant variations in the number of objects, pose, and appearance. Code and supplementary material are available in our project page: splice-vit.github.io.},
  archive      = {J_TOG},
  author       = {Narek Tumanyan and Omer Bar-Tal and Shir Amir and Shai Bagon and Tali Dekel},
  doi          = {10.1145/3630096},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {11:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Disentangling structure and appearance in ViT feature space},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Layout-aware single-image document flattening. <em>TOG</em>,
<em>43</em>(1), 9:1–17. (<a
href="https://doi.org/10.1145/3627818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image rectification of document deformation is a challenging task. Although some recent deep learning-based methods have attempted to solve this problem, they cannot achieve satisfactory results when dealing with document images with complex deformations. In this article, we propose a new efficient framework for document flattening. Our main insight is that most layout primitives in a document have rectangular outline shapes, making unwarping local layout primitives essentially homogeneous with unwarping the entire document. The former task is clearly more straightforward to solve than the latter due to the more consistent texture and relatively smooth deformation. On this basis, we propose a layout-aware deep model working in a divide-and-conquer manner. First, we employ a transformer-based segmentation module to obtain the layout information of the input document. Then a new regression module is applied to predict the global and local UV maps. Finally, we design an effective merging algorithm to correct the global prediction with local details. Both quantitative and qualitative experimental results demonstrate that our framework achieves favorable performance against state-of-the-art methods. In addition, the current publicly available document flattening datasets have limited 3D paper shapes without layout annotation and also lack a general geometric correction metric. Therefore, we build a new large-scale synthetic dataset by utilizing a fully automatic rendering method to generate deformed documents with diverse shapes and exact layout segmentation labels. We also propose a new geometric correction metric based on our paired document UV maps. Code and dataset will be released at https://github.com/BunnySoCrazy/LA-DocFlatten .},
  archive      = {J_TOG},
  author       = {Pu Li and Weize Quan and Jianwei Guo and Dong-Ming Yan},
  doi          = {10.1145/3627818},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {9:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Layout-aware single-image document flattening},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent l-systems: Transformer-based tree generator.
<em>TOG</em>, <em>43</em>(1), 7:1–16. (<a
href="https://doi.org/10.1145/3627101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show how a Transformer can encode hierarchical tree-like string structures by introducing a new deep learning-based framework for generating 3D biological tree models represented as Lindenmayer system (L-system) strings. L-systems are string-rewriting procedural systems that encode tree topology and geometry. L-systems are efficient, but creating the production rules is one of the most critical problems precluding their usage in practice. We substitute the procedural rules creation with a deep neural model. Instead of writing the rules, we train a deep neural model that produces the output strings. We train our model on 155k tree geometries that are encoded as L-strings, de-parameterized, and converted to a hierarchy of linear sequences corresponding to branches. An end-to-end deep learning model with an attention mechanism then learns the distributions of geometric operations and branches from the input, effectively replacing the L-system rewriting rule generation. The trained deep model generates new L-strings representing 3D tree models in the same way L-systems do by providing the starting string. Our model allows for the generation of a wide variety of new trees, and the deep model agrees with the input by 93.7\% in branching angles, 97.2\% in branch lengths, and 92.3\% in an extracted list of geometric features. We also validate the generated trees using perceptual metrics showing 97\% agreement with input geometric models.},
  archive      = {J_TOG},
  author       = {Jae Joong Lee and Bosheng Li and Bedrich Benes},
  doi          = {10.1145/3627101},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {7:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Latent L-systems: Transformer-based tree generator},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HAvatar: High-fidelity head avatar via facial model
conditioned neural radiance field. <em>TOG</em>, <em>43</em>(1), 6:1–16.
(<a href="https://doi.org/10.1145/3626316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of modeling an animatable 3D human head avatar under lightweight setups is of significant importance but has not been well solved. Existing 3D representations either perform well in the realism of portrait images synthesis or the accuracy of expression control, but not both. To address the problem, we introduce a novel hybrid explicit-implicit 3D representation, Facial Model Conditioned Neural Radiance Field, which integrates the expressiveness of NeRF and the prior information from the parametric template. At the core of our representation, a synthetic-renderings-based condition method is proposed to fuse the prior information from the parametric model into the implicit field without constraining its topological flexibility. Besides, based on the hybrid representation, we properly overcome the inconsistent shape issue presented in existing methods and improve the animation stability. Moreover, by adopting an overall GAN-based architecture using an image-to-image translation network, we achieve high-resolution, realistic and view-consistent synthesis of dynamic head appearance. Experiments demonstrate that our method can achieve state-of-the-art performance for 3D head avatar animation compared with previous methods.},
  archive      = {J_TOG},
  author       = {Xiaochen Zhao and Lizhen Wang and Jingxiang Sun and Hongwen Zhang and Jinli Suo and Yebin Liu},
  doi          = {10.1145/3626316},
  journal      = {ACM Transactions on Graphics},
  month        = {11},
  number       = {1},
  pages        = {6:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {HAvatar: High-fidelity head avatar via facial model conditioned neural radiance field},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SparsePoser: Real-time full-body motion reconstruction from
sparse data. <em>TOG</em>, <em>43</em>(1), 5:1–14. (<a
href="https://doi.org/10.1145/3625264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable human motion reconstruction is crucial for creating natural interactions of full-body avatars in Virtual Reality (VR) and entertainment applications. As the Metaverse and social applications gain popularity, users are seeking cost-effective solutions to create full-body animations that are comparable in quality to those produced by commercial motion capture systems. In order to provide affordable solutions though, it is important to minimize the number of sensors attached to the subject’s body. Unfortunately, reconstructing the full-body pose from sparse data is a heavily under-determined problem. Some studies that use IMU sensors face challenges in reconstructing the pose due to positional drift and ambiguity of the poses. In recent years, some mainstream VR systems have released 6-degree-of-freedom (6-DoF) tracking devices providing positional and rotational information. Nevertheless, most solutions for reconstructing full-body poses rely on traditional inverse kinematics (IK) solutions, which often produce non-continuous and unnatural poses. In this article, we introduce SparsePoser, a novel deep learning-based solution for reconstructing a full-body pose from a reduced set of six tracking devices. Our system incorporates a convolutional-based autoencoder that synthesizes high-quality continuous human poses by learning the human motion manifold from motion capture data. Then, we employ a learned IK component, made of multiple lightweight feed-forward neural networks, to adjust the hands and feet toward the corresponding trackers. We extensively evaluate our method on publicly available motion capture datasets and with real-time live demos. We show that our method outperforms state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and can be used for users with different body dimensions and proportions.},
  archive      = {J_TOG},
  author       = {Jose Luis Ponton and Haoran Yun and Andreas Aristidou and Carlos Andujar and Nuria Pelechano},
  doi          = {10.1145/3625264},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {1},
  pages        = {5:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {SparsePoser: Real-time full-body motion reconstruction from sparse data},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time reconstruction of fluid flow under unknown
disturbance. <em>TOG</em>, <em>43</em>(1), 4:1–14. (<a
href="https://doi.org/10.1145/3624011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework that captures sparse Lagrangian flow information from a volume of real liquid and reconstructs its detailed kinematic information in real time. Our framework can perform flow reconstruction even when the liquid is disturbed by an object of unknown movement and shape. Through a large dataset of liquid moving under external disturbance, an agent is trained using reinforcement learning to reproduce the target flow kinematics with only the captured sparse information as inputs while remaining oblivious to the movement and the shape of the disturbance sources. To ensure that the underlying simulation model faithfully obeys physical reality, we also optimize the viscosity parameters in Smoothed Particle Hydrodynamics (SPH) using classical fluid dynamics knowledge and gradient-based optimization. By quantitatively comparing the reconstruction results against real-world and simulated ground truth, we verified that our reconstruction method is resilient to different agitation patterns.},
  archive      = {J_TOG},
  author       = {Kinfung Chu and Jiawei Huang and Hidemasa Takana and Yoshifumi Kitamura},
  doi          = {10.1145/3624011},
  journal      = {ACM Transactions on Graphics},
  month        = {10},
  number       = {1},
  pages        = {4:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time reconstruction of fluid flow under unknown disturbance},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A function-based approach to interactive high-precision
volumetric design and fabrication. <em>TOG</em>, <em>43</em>(1), 3:1–15.
(<a href="https://doi.org/10.1145/3622934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel function representation (F-Rep) based geometric modeling kernel tailor-made to support computer aided design (CAD) and fabrication of high resolution volumetric models containing hundreds of billions of voxel grid elements. Our modeling kernel addresses existing limitations associated with evaluating, storing, and accessing volumetric data produced by F-Reps in contexts outside of rendering. The result is an F-Rep modeling kernel well suited for CAD-based applications. Our kernel utilizes a sparse volume data structure to manage F-Rep data while efficient F-Rep evaluation is achieved through a combination of interval arithmetic (IA), just-in-time (JIT) compilation of user-defined functions, and massively parallel evaluation on the GPU. We employ IA as the basis for local pruning of the function evaluation tree to minimize total function evaluations, we use a novel JIT compilation scheme to optimize function execution, and we take advantage of GPU-parallelism to enhance computational throughput. We illustrate the kernel’s effectiveness in visualizing and slicing models with complex defining functions and detailed geometry, and utilize the geometry kernel to manufacture a physical part. Additionally, we present performance metrics across multiple hardware configurations demonstrating significant performance improvements over existing F-Rep geometry kernels, and we examine how our geometry kernel scales with computing power.},
  archive      = {J_TOG},
  author       = {Christopher Uchytil and Duane Storti},
  doi          = {10.1145/3622934},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {3:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A function-based approach to interactive high-precision volumetric design and fabrication},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint UV optimization and texture baking. <em>TOG</em>,
<em>43</em>(1), 2:1–20. (<a
href="https://doi.org/10.1145/3617683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Level of detail has been widely used in interactive computer graphics. In current industrial 3D modeling pipelines, artists rely on commercial software to generate highly detailed models with UV maps and then bake textures for low-poly counterparts. In these pipelines, each step is performed separately, leading to unsatisfactory visual appearances for low polygon count models. Moreover, existing texture baking techniques assume the low-poly mesh has a small geometric difference from the high-poly, which is often not true in practice, especially with extremely low poly count models. To alleviate the visual discrepancy of the low-poly mesh, we propose to jointly optimize UV mappings during texture baking, allowing for low-poly models to faithfully replicate the appearance of the high-poly even with large geometric differences. We formulate the optimization within a differentiable rendering framework, allowing the automatic adjustment of texture regions to encode appearance information. To compensate for view parallax when two meshes have large geometric differences, we introduce a spherical harmonic parallax mapping, which uses spherical harmonic functions to modulate per-texel UV coordinates based on the view direction. We evaluate the effectiveness and robustness of our approach on a dataset composed of online downloaded models, with varying complexities and geometric discrepancies. Our method achieves superior quality over state-of-the-art techniques and commercial solutions.},
  archive      = {J_TOG},
  author       = {Julian Knodt and Zherong Pan and Kui Wu and Xifeng Gao},
  doi          = {10.1145/3617683},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {2:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Joint UV optimization and texture baking},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Light codes for fast two-way human-centric visual
communication. <em>TOG</em>, <em>43</em>(1), 1:1–14. (<a
href="https://doi.org/10.1145/3617682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual codes, such as QR codes, are widely used in several applications for conveying information to users. However, user interactions based on spatial codes (e.g., displaying codes on phone screens for exchanging contact information) are often tedious, time consuming, and prone to errors due to image corruptions such as noise, blur, saturation, and perspective distortions. We propose Light Codes (LICO), a novel method for fast and fluid exchange of information among users. Light codes are based on transmitting and receiving temporal codes (instead of spatial) using compact and low-cost transceiver devices. The resulting approach enables seamless and near instantaneous exchange of short messages among users with minimal physical and cognitive effort. We design novel coding techniques, hardware prototypes, and applications that are optimized for human-centric communication, and facilitate fast and fluid user-to-user interactions in various challenging conditions, including a range of distances, motion, and ambient illumination. We evaluate the performance of the proposed methods both via quantitative analysis and user study based comparisons with several existing approaches including display-camera links, Bluetooth, and near-field communication, which show strong preference toward Light Codes in various real-world application scenarios.},
  archive      = {J_TOG},
  author       = {Mohit Gupta and Jian Wang and Karl Bayer and Shree K. Nayar},
  doi          = {10.1145/3617682},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {1},
  pages        = {1:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Light codes for fast two-way human-centric visual communication},
  volume       = {43},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-singular-hessian: Implicit neural representation of
unoriented point clouds by enforcing singular hessian. <em>TOG</em>,
<em>42</em>(6), 274:1–14. (<a
href="https://doi.org/10.1145/3618311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural implicit representation is a promising approach for reconstructing surfaces from point clouds. Existing methods combine various regularization terms, such as the Eikonal and Laplacian energy terms, to enforce the learned neural function to possess the properties of a Signed Distance Function (SDF). However, inferring the actual topology and geometry of the underlying surface from poor-quality unoriented point clouds remains challenging. In accordance with Differential Geometry, the Hessian of the SDF is singular for points within the differential thin-shell space surrounding the surface. Our approach enforces the Hessian of the neural implicit function to have a zero determinant for points near the surface. This technique aligns the gradients for a near-surface point and its on-surface projection point, producing a rough but faithful shape within just a few iterations. By annealing the weight of the singular-Hessian term, our approach ultimately produces a high-fidelity reconstruction result. Extensive experimental results demonstrate that our approach effectively suppresses ghost geometry and recovers details from unoriented point clouds with better expressiveness than existing fitting-based methods.},
  archive      = {J_TOG},
  author       = {Zixiong Wang and Yunxiao Zhang and Rui Xu and Fan Zhang and Peng-Shuai Wang and Shuangmin Chen and Shiqing Xin and Wenping Wang and Changhe Tu},
  doi          = {10.1145/3618311},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {274:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural-singular-hessian: Implicit neural representation of unoriented point clouds by enforcing singular hessian},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconstructing close human interactions from multiple views.
<em>TOG</em>, <em>42</em>(6), 273:1–14. (<a
href="https://doi.org/10.1145/3618336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenging task of reconstructing the poses of multiple individuals engaged in close interactions, captured by multiple calibrated cameras. The difficulty arises from the noisy or false 2D keypoint detections due to inter-person occlusion, the heavy ambiguity in associating keypoints to individuals due to the close interactions, and the scarcity of training data as collecting and annotating motion data in crowded scenes is resource-intensive. We introduce a novel system to address these challenges. Our system integrates a learning-based pose estimation component and its corresponding training and inference strategies. The pose estimation component takes multi-view 2D keypoint heatmaps as input and reconstructs the pose of each individual using a 3D conditional volumetric network. As the network doesn&#39;t need images as input, we can leverage known camera parameters from test scenes and a large quantity of existing motion capture data to synthesize massive training data that mimics the real data distribution in test scenes. Extensive experiments demonstrate that our approach significantly surpasses previous approaches in terms of pose accuracy and is generalizable across various camera setups and population sizes. The code is available on our project page: https://github.com/zju3dv/CloseMoCap.},
  archive      = {J_TOG},
  author       = {Qing Shuai and Zhiyuan Yu and Zhize Zhou and Lixin Fan and Haijun Yang and Can Yang and Xiaowei Zhou},
  doi          = {10.1145/3618336},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {273:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reconstructing close human interactions from multiple views},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Slippage-preserving reshaping of human-made 3D content.
<em>TOG</em>, <em>42</em>(6), 272:1–18. (<a
href="https://doi.org/10.1145/3618391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artists often need to reshape 3D models of human-made objects by changing the relative proportions or scales of different model parts or elements while preserving the look and structure of the inputs. Manually reshaping inputs to satisfy these criteria is highly time-consuming; the edit in our teaser took an artist 5 hours to complete. However, existing methods for 3D shape editing are largely designed for other tasks and produce undesirable outputs when repurposed for reshaping. Prior work on 2D curve network reshaping suggests that in 2D settings the user-expected outcome is achieved when the reshaping edit keeps the orientations of the different model elements and when these elements scale as-locally-uniformly-as-possible (ALUP). However, our observations suggest that in 3D viewers are tolerant of non-uniform tangential scaling if and when this scaling preserves slippage and reduces changes in element size, or scale , relative to the input. Slippage preservation requires surfaces which are locally slippable with respect to a given rigid motion to retain this property post-reshaping (a motion is slippable if when applied to the surface, it slides the surface along itself without gaps). We build on these observations by first extending the 2D ALUP framework to 3D and then modifying it to allow non-uniform scaling while promoting slippage and scale preservation. Our 3D ALUP extension produces reshaped outputs better aligned with viewer expectations than prior alternatives; our slippage-aware method further improves the outcome producing results on par with manual reshaping ones. Our method does not require any user input beyond specifying control handles and their target locations. We validate our method by applying it to over one hundred diverse inputs and by comparing our results to those generated by alternative approaches and manually. Comparative study participants preferred our outputs over the best performing traditional deformation method by a 65\% margin and over our 3D ALUP extension by a 61\% margin; they judged our outputs as at least on par with manually produced ones.},
  archive      = {J_TOG},
  author       = {Chrystiano Araújo and Nicholas Vining and Silver Burla and Manuel Ruivo De Oliveira and Enrique Rosales and Alla Sheffer},
  doi          = {10.1145/3618391},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {272:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Slippage-preserving reshaping of human-made 3D content},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doppler time-of-flight rendering. <em>TOG</em>,
<em>42</em>(6), 271:1–18. (<a
href="https://doi.org/10.1145/3618335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Doppler time-of-flight (D-ToF) rendering, an extension of ToF rendering for dynamic scenes, with applications in simulating D-ToF cameras. D-ToF cameras use high-frequency modulation of illumination and exposure, and measure the Doppler frequency shift to compute the radial velocity of dynamic objects. The time-varying scene geometry and high-frequency modulation functions used in such cameras make it challenging to accurately and efficiently simulate their measurements with existing ToF rendering algorithms. We overcome these challenges in a twofold manner: To achieve accuracy, we derive path integral expressions for D-ToF measurements under global illumination and form unbiased Monte Carlo estimates of these integrals. To achieve efficiency, we develop a tailored time-path sampling technique that combines antithetic time sampling with correlated path sampling. We show experimentally that our sampling technique achieves up to two orders of magnitude lower variance compared to naive time-path sampling. We provide an open-source simulator that serves as a digital twin for D-ToF imaging systems, allowing imaging researchers, for the first time, to investigate the impact of modulation functions, material properties, and global illumination on D-ToF imaging performance.},
  archive      = {J_TOG},
  author       = {Juhyeon Kim and Wojciech Jarosz and Ioannis Gkioulekas and Adithya Pediredla},
  doi          = {10.1145/3618335},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {271:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Doppler time-of-flight rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GroomGen: A high-quality generative hair model using
hierarchical latent representations. <em>TOG</em>, <em>42</em>(6),
270:1–16. (<a href="https://doi.org/10.1145/3618309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent successes in hair acquisition that fits a high-dimensional hair model to a specific input subject, generative hair models, which establish general embedding spaces for encoding, editing, and sampling diverse hairstyles, are way less explored. In this paper, we present GroomGen , the first generative model designed for hair geometry composed of highly-detailed dense strands. Our approach is motivated by two key ideas. First, we construct hair latent spaces covering both individual strands and hairstyles. The latent spaces are compact, expressive, and well-constrained for high-quality and diverse sampling. Second, we adopt a hierarchical hair representation that parameterizes a complete hair model to three levels: single strands, sparse guide hairs, and complete dense hairs. This representation is critical to the compactness of latent spaces, the robustness of training, and the efficiency of inference. Based on this hierarchical latent representation, our proposed pipeline consists of a strand-VAE and a hairstyle-VAE that encode an individual strand and a set of guide hairs to their respective latent spaces, and a hybrid densification step that populates sparse guide hairs to a dense hair model. GroomGen not only enables novel hairstyle sampling and plausible hairstyle interpolation, but also supports interactive editing of complex hairstyles, or can serve as strong data-driven prior for hairstyle reconstruction from images. We demonstrate the superiority of our approach with qualitative examples of diverse sampled hairstyles and quantitative evaluation of generation quality regarding every single component and the entire pipeline.},
  archive      = {J_TOG},
  author       = {Yuxiao Zhou and Menglei Chai and Alessandro Pepe and Markus Gross and Thabo Beeler},
  doi          = {10.1145/3618309},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {270:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {GroomGen: A high-quality generative hair model using hierarchical latent representations},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EMS: 3D eyebrow modeling from single-view images.
<em>TOG</em>, <em>42</em>(6), 269:1–19. (<a
href="https://doi.org/10.1145/3618323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eyebrows play a critical role in facial expression and appearance. Although the 3D digitization of faces is well explored, less attention has been drawn to 3D eyebrow modeling. In this work, we propose EMS, the first learning-based framework for single-view 3D eyebrow reconstruction. Following the methods of scalp hair reconstruction, we also represent the eyebrow as a set of fiber curves and convert the reconstruction to fibers growing problem. Three modules are then carefully designed: RootFinder firstly localizes the fiber root positions which indicate where to grow; OriPredictor predicts an orientation field in the 3D space to guide the growing of fibers; FiberEnder is designed to determine when to stop the growth of each fiber. Our OriPredictor directly borrows the method used in hair reconstruction. Considering the differences between hair and eyebrows, both RootFinder and FiberEnder are newly proposed. Specifically, to cope with the challenge that the root location is severely occluded, we formulate root localization as a density map estimation task. Given the predicted density map, a density-based clustering method is further used for finding the roots. For each fiber, the growth starts from the root point and moves step by step until the ending, where each step is defined as an oriented line segment with a constant length according to the predicted orientation field. To determine when to end, a pixel-aligned RNN architecture is designed to form a binary classifier, which outputs stop or not for each growing step. To support the training of all proposed networks, we build the first 3D synthetic eyebrow dataset that contains 400 high-quality eyebrow models manually created by artists. Extensive experiments have demonstrated the effectiveness of the proposed EMS pipeline on a variety of different eyebrow styles and lengths, ranging from short and sparse to long bushy eyebrows.},
  archive      = {J_TOG},
  author       = {Chenghong Li and Leyang Jin and Yujian Zheng and Yizhou Yu and Xiaoguang Han},
  doi          = {10.1145/3618323},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {269:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {EMS: 3D eyebrow modeling from single-view images},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconstruction of machine-made shapes from bitmap sketches.
<em>TOG</em>, <em>42</em>(6), 268:1–16. (<a
href="https://doi.org/10.1145/3618361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method of reconstructing 3D machine-made shapes from bitmap sketches by separating an input image into individual patches and jointly optimizing their geometry. We rely on two main observations: (1) human observers interpret sketches of man-made shapes as a collection of simple geometric primitives, and (2) sketch strokes often indicate occlusion contours or sharp ridges between those primitives. Using these main observations we design a system that takes a single bitmap image of a shape, estimates image depth and segmentation into primitives with neural networks, then fits primitives to the predicted depth while determining occlusion contours and aligning intersections with the input drawing via optimization. Unlike previous work, our approach does not require additional input, annotation, or templates, and does not require retraining for a new category of man-made shapes. Our method produces triangular meshes that display sharp geometric features and are suitable for downstream applications, such as editing, rendering, and shading.},
  archive      = {J_TOG},
  author       = {Ivan Puhachov and Cedric Martens and Paul G. Kry and Mikhail Bessmeltsev},
  doi          = {10.1145/3618361},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {268:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Reconstruction of machine-made shapes from bitmap sketches},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural packing: From visual sensing to reinforcement
learning. <em>TOG</em>, <em>42</em>(6), 267:1–11. (<a
href="https://doi.org/10.1145/3618354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel learning framework to solve the transport-and-packing (TAP) problem in 3D. It constitutes a full solution pipeline from partial observations of input objects via RGBD sensing and recognition to final box placement, via robotic motion planning, to arrive at a compact packing in a target container. The technical core of our method is a neural network for TAP, trained via reinforcement learning (RL), to solve the NP-hard combinatorial optimization problem. Our network simultaneously selects an object to pack and determines the final packing location, based on a judicious encoding of the continuously evolving states of partially observed source objects and available spaces in the target container, using separate encoders both enabled with attention mechanisms. The encoded feature vectors are employed to compute the matching scores and feasibility masks of different pairings of box selection and available space configuration for packing strategy optimization. Extensive experiments, including ablation studies and physical packing execution by a real robot (Universal Robot UR5e), are conducted to evaluate our method in terms of its design choices, scalability, generalizability, and comparisons to baselines, including the most recent RL-based TAP solution. We also contribute the first benchmark for TAP which covers a variety of input settings and difficulty levels.},
  archive      = {J_TOG},
  author       = {Juzhan Xu and Minglun Gong and Hao Zhang and Hui Huang and Ruizhen Hu},
  doi          = {10.1145/3618354},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {267:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural packing: From visual sensing to reinforcement learning},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning based 2D irregular shape packing. <em>TOG</em>,
<em>42</em>(6), 266:1–16. (<a
href="https://doi.org/10.1145/3618348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {2D irregular shape packing is a necessary step to arrange UV patches of a 3D model within a texture atlas for memory-efficient appearance rendering in computer graphics. Being a joint, combinatorial decision-making problem involving all patch positions and orientations, this problem has well-known NP-hard complexity. Prior solutions either assume a heuristic packing order or modify the upstream mesh cut and UV mapping to simplify the problem, which either limits the packing ratio or incurs robustness or generality issues. Instead, we introduce a learning-assisted 2D irregular shape packing method that achieves a high packing quality with minimal requirements from the input. Our method iteratively selects and groups subsets of UV patches into near-rectangular super patches, essentially reducing the problem to bin-packing, based on which a joint optimization is employed to further improve the packing ratio. In order to efficiently deal with large problem instances with hundreds of patches, we train deep neural policies to predict nearly rectangular patch subsets and determine their relative poses, leading to linear time scaling with the number of patches. We demonstrate the effectiveness of our method on three datasets for UV packing, where our method achieves a higher packing ratio over several widely used baselines with competitive computational speed.},
  archive      = {J_TOG},
  author       = {Zeshi Yang and Zherong Pan and Manyi Li and Kui Wu and Xifeng Gao},
  doi          = {10.1145/3618348},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {266:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning based 2D irregular shape packing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NodeGit: Diffing and merging node graphs. <em>TOG</em>,
<em>42</em>(6), 265:1–12. (<a
href="https://doi.org/10.1145/3618343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of version control is pervasive in collaborative software projects. Version control systems are based on two primary operations: diffing two versions to compute the change between them and merging two versions edited concurrently. Recent works provide solutions to diff and merge graphics assets such as images, meshes and scenes. In this work, we present a practical algorithm to diff and merge procedural programs written as node graphs. To obtain more precise diffs, we version the graphs directly rather than their textual representations. Diffing graphs is equivalent to computing the graph edit distance, which is known to be computationally infeasible. Following prior work, we propose an approximate algorithm tailored to our problem domain. We validate the proposed algorithm by applying it both to manual edits and to a large set of randomized modifications of procedural shapes and materials. We compared our method with existing state-of-the-art algorithms, showing that our approach is the only one that reliably detects user edits.},
  archive      = {J_TOG},
  author       = {Eduardo Rinaldi and Davide Sforza and Fabio Pellacini},
  doi          = {10.1145/3618343},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {265:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {NodeGit: Diffing and merging node graphs},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SLANG.d: Fast, modular and differentiable shader
programming. <em>TOG</em>, <em>42</em>(6), 264:1–28. (<a
href="https://doi.org/10.1145/3618353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce SLANG.D, an extension to the Slang shading language that incorporates first-class automatic differentiation support. The new shading language allows us to transform a Direct3D-based path tracer to be fully differentiable with minor modifications to existing code. SLANG.D enables a shared ecosystem between machine learning frameworks and pre-existing graphics hardware API-based rendering systems, promoting the interchange of components and ideas across these two domains. Our contributions include a differentiable type system designed to ensure type safety and semantic clarity in codebases that blend differentiable and non-differentiable code, language primitives that automatically generate both forward and reverse gradient propagation methods, and a compiler architecture that generates efficient derivative propagation shader code for graphics pipelines. Our compiler supports differentiating code that involves arbitrary control-flow, dynamic dispatch, generics and higher-order differentiation, while providing developers flexible control of checkpointing and gradient aggregation strategies for best performance. Our system allows us to differentiate an existing real-time path tracer, Falcor, with minimal change to its shader code. We show that the compiler-generated derivative kernels perform as efficiently as handwritten ones. In several benchmarks, the SLANG.D code achieves significant speedup when compared to prior automatic differentiation systems.},
  archive      = {J_TOG},
  author       = {Sai Praveen Bangaru and Lifan Wu and Tzu-Mao Li and Jacob Munkberg and Gilbert Bernstein and Jonathan Ragan-Kelley and Frédo Durand and Aaron Lefohn and Yong He},
  doi          = {10.1145/3618353},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {264:1–28},
  shortjournal = {ACM Trans. Graph.},
  title        = {SLANG.D: Fast, modular and differentiable shader programming},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient hybrid zoom using camera fusion on mobile phones.
<em>TOG</em>, <em>42</em>(6), 263:1–12. (<a
href="https://doi.org/10.1145/3618362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DSLR cameras can achieve multiple zoom levels via shifting lens distances or swapping lens types. However, these techniques are not possible on smart-phone devices due to space constraints. Most smartphone manufacturers adopt a hybrid zoom system: commonly a Wide ( W ) camera at a low zoom level and a Telephoto ( T ) camera at a high zoom level. To simulate zoom levels between W and T , these systems crop and digitally upsample images from W , leading to significant detail loss. In this paper, we propose an efficient system for hybrid zoom super-resolution on mobile devices, which captures a synchronous pair of W and T shots and leverages machine learning models to align and transfer details from T to W. We further develop an adaptive blending method that accounts for depth-of-field mismatches, scene occlusion, flow uncertainty, and alignment errors. To minimize the domain gap, we design a dual-phone camera rig to capture real-world inputs and ground-truths for supervised training. Our method generates a 12-megapixel image in 500ms on a mobile platform and compares favorably against state-of-the-art methods under extensive evaluation on real-world scenarios.},
  archive      = {J_TOG},
  author       = {Xiaotong Wu and Wei-Sheng Lai and Yichang Shih and Charles Herrmann and Michael Krainin and Deqing Sun and Chia-Kai Liang},
  doi          = {10.1145/3618362},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {263:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient hybrid zoom using camera fusion on mobile phones},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decaf: Monocular deformation capture for face and hand
interactions. <em>TOG</em>, <em>42</em>(6), 262:1–16. (<a
href="https://doi.org/10.1145/3618329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for 3D tracking from monocular RGB videos predominantly consider articulated and rigid objects ( e.g. , two hands or humans interacting with rigid environments). Modelling dense non-rigid object deformations in this setting ( e.g. when hands are interacting with a face), remained largely unaddressed so far, although such effects can improve the realism of the downstream applications such as AR/VR, 3D virtual avatar communications, and character animations. This is due to the severe ill-posedness of the monocular view setting and the associated challenges ( e.g. , in acquiring a dataset for training and evaluation or obtaining the reasonable non-uniform stiffness of the deformable object). While it is possible to naïvely track multiple non-rigid objects independently using 3D templates or parametric 3D models, such an approach would suffer from multiple artefacts in the resulting 3D estimates such as depth ambiguity, unnatural intra-object collisions and missing or implausible deformations. Hence, this paper introduces the first method that addresses the fundamental challenges depicted above and that allows tracking human hands interacting with human faces in 3D from single monocular RGB videos. We model hands as articulated objects inducing non-rigid face deformations during an active interaction. Our method relies on a new hand-face motion and interaction capture dataset with realistic face deformations acquired with a markerless multi-view camera system. As a pivotal step in its creation, we process the reconstructed raw 3D shapes with position-based dynamics and an approach for non-uniform stiffness estimation of the head tissues, which results in plausible annotations of the surface deformations, hand-face contact regions and head-hand positions. At the core of our neural approach are a variational auto-encoder supplying the hand-face depth prior and modules that guide the 3D tracking by estimating the contacts and the deformations. Our final 3D hand and face reconstructions are realistic and more plausible compared to several baselines applicable in our setting, both quantitatively and qualitatively. https://vcai.mpi-inf.mpg.de/projects/Decaf},
  archive      = {J_TOG},
  author       = {Soshi Shimada and Vladislav Golyanik and Patrick Pérez and Christian Theobalt},
  doi          = {10.1145/3618329},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {262:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Decaf: Monocular deformation capture for face and hand interactions},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScaNeRF: Scalable bundle-adjusting neural radiance fields
for large-scale scene rendering. <em>TOG</em>, <em>42</em>(6), 261:1–18.
(<a href="https://doi.org/10.1145/3618369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-quality large-scale scene rendering requires a scalable representation and accurate camera poses. This research combines tile-based hybrid neural fields with parallel distributive optimization to improve bundle-adjusting neural radiance fields. The proposed method scales with a divide-and-conquer strategy. We partition scenes into tiles, each with a multi-resolution hash feature grid and shallow chained diffuse and specular multilayer perceptrons (MLPs). Tiles unify foreground and background via a spatial contraction function that allows both distant objects in outdoor scenes and planar reflections as virtual images outside the tile. Decomposing appearance with the specular MLP allows a specular-aware warping loss to provide a second optimization path for camera poses. We apply the alternating direction method of multipliers (ADMM) to achieve consensus among camera poses while maintaining parallel tile optimization. Experimental results show that our method outperforms state-of-the-art neural scene rendering method quality by 5\%--10\% in PSNR, maintaining sharp distant objects and view-dependent reflections across six indoor and outdoor scenes.},
  archive      = {J_TOG},
  author       = {Xiuchao Wu and Jiamin Xu and Xin Zhang and Hujun Bao and Qixing Huang and Yujun Shen and James Tompkin and Weiwei Xu},
  doi          = {10.1145/3618369},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {261:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {ScaNeRF: Scalable bundle-adjusting neural radiance fields for large-scale scene rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive shells for efficient neural radiance field
rendering. <em>TOG</em>, <em>42</em>(6), 260:1–15. (<a
href="https://doi.org/10.1145/3618390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images. Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization. Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel. Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity. Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To this end, we generalize the NeuS [Wang et al. 2021] formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band. At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Experiments show that our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.},
  archive      = {J_TOG},
  author       = {Zian Wang and Tianchang Shen and Merlin Nimier-David and Nicholas Sharp and Jun Gao and Alexander Keller and Sanja Fidler and Thomas Müller and Zan Gojcic},
  doi          = {10.1145/3618390},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {260:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Adaptive shells for efficient neural radiance field rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Text-guided synthesis of eulerian cinemagraphs.
<em>TOG</em>, <em>42</em>(6), 259:1–13. (<a
href="https://doi.org/10.1145/3618326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Text2Cinemagraph, a fully automated method for creating cinemagraphs from text descriptions --- an especially challenging task when prompts feature imaginary elements and artistic styles, given the complexity of interpreting the semantics and motions of these images. We focus on cinemagraphs of fluid elements, such as flowing rivers, and drifting clouds, which exhibit continuous motion and repetitive textures. Existing single-image animation methods fall short on artistic inputs, and recent text-based video methods frequently introduce temporal inconsistencies, struggling to keep certain regions static. To address these challenges, we propose an idea of synthesizing image twins from a single text prompt --- a pair of an artistic image and its pixel-aligned corresponding natural-looking twin. While the artistic image depicts the style and appearance detailed in our text prompt, the realistic counterpart greatly simplifies layout and motion analysis. Leveraging existing natural image and video datasets, we can accurately segment the realistic image and predict plausible motion given the semantic information. The predicted motion can then be transferred to the artistic image to create the final cinemagraph. Our method outperforms existing approaches in creating cinemagraphs for natural landscapes as well as artistic and other-worldly scenes, as validated by automated metrics and user studies. Finally, we demonstrate two extensions: animating existing paintings and controlling motion directions using text.},
  archive      = {J_TOG},
  author       = {Aniruddha Mahapatra and Aliaksandr Siarohin and Hsin-Ying Lee and Sergey Tulyakov and Jun-Yan Zhu},
  doi          = {10.1145/3618326},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {259:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Text-guided synthesis of eulerian cinemagraphs},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ART-owen scrambling. <em>TOG</em>, <em>42</em>(6), 258:1–11.
(<a href="https://doi.org/10.1145/3618307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel algorithm for implementing Owen-scrambling, combining the generation and distribution of the scrambling bits in a single self-contained compact process. We employ a context-free grammar to build a binary tree of symbols, and equip each symbol with a scrambling code that affects all descendant nodes. We nominate the grammar of adaptive regular tiles (ART) derived from the repetition-avoiding Thue-Morse word, and we discuss its potential advantages and shortcomings. Our algorithm has many advantages, including random access to samples, fixed time complexity, GPU friendliness, and scalability to any memory budget. Further, it provides two unique features over known methods: it admits optimization, and it is in-vertible, enabling screen-space scrambling of the high-dimensional Sobol sampler.},
  archive      = {J_TOG},
  author       = {Abdalla G. M. Ahmed and Matt Pharr and Peter Wonka},
  doi          = {10.1145/3618307},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {258:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {ART-owen scrambling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manifold path guiding for importance sampling specular
chains. <em>TOG</em>, <em>42</em>(6), 257:1–14. (<a
href="https://doi.org/10.1145/3618360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex visual effects such as caustics are often produced by light paths containing multiple consecutive specular vertices (dubbed specular chains) , which pose a challenge to unbiased estimation in Monte Carlo rendering. In this work, we study the light transport behavior within a sub-path that is comprised of a specular chain and two non-specular separators. We show that the specular manifolds formed by all the sub-paths could be exploited to provide coherence among sub-paths. By reconstructing continuous energy distributions from historical and coherent sub-paths, seed chains can be generated in the context of importance sampling and converge to admissible chains through manifold walks. We verify that importance sampling the seed chain in the continuous space reaches the goal of importance sampling the discrete admissible specular chain. Based on these observations and theoretical analyses, a progressive pipeline, manifold path guiding , is designed and implemented to importance sample challenging paths featuring long specular chains. To our best knowledge, this is the first general framework for importance sampling discrete specular chains in regular Monte Carlo rendering. Extensive experiments demonstrate that our method outperforms state-of-the-art unbiased solutions with up to 40 × variance reduction, especially in typical scenes containing long specular chains and complex visibility.},
  archive      = {J_TOG},
  author       = {Zhimin Fan and Pengpei Hong and Jie Guo and Changqing Zou and Yanwen Guo and Ling-Qi Yan},
  doi          = {10.1145/3618360},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {257:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Manifold path guiding for importance sampling specular chains},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rectifying strip patterns. <em>TOG</em>, <em>42</em>(6),
256:1–18. (<a href="https://doi.org/10.1145/3618378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Straight flat strips of inextensible material can be bent into curved strips aligned with arbitrary space curves. The large shape variety of these so-called rectifying strips makes them candidates for shape modeling, especially in applications such as architecture where simple elements are preferred for the fabrication of complex shapes. In this paper, we provide computational tools for the design of shapes from rectifying strips. They can form various patterns and fulfill constraints which are required for specific applications such as gridshells or shading systems. The methodology is based on discrete models of rectifying strips, a discrete level-set formulation and optimization-based constrained mesh design and editing. We also analyse the geometry at nodes and present remarkable quadrilateral arrangements of rectifying strips with torsion-free nodes.},
  archive      = {J_TOG},
  author       = {Bolun Wang and Hui Wang and Eike Schling and Helmut Pottmann},
  doi          = {10.1145/3618378},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {256:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rectifying strip patterns},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational barycentric coordinates. <em>TOG</em>,
<em>42</em>(6), 255:1–16. (<a
href="https://doi.org/10.1145/3618403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a variational technique to optimize for generalized barycentric coordinates that offers additional control compared to existing models. Prior work represents barycentric coordinates using meshes or closed-form formulae, in practice limiting the choice of objective function. In contrast, we directly parameterize the continuous function that maps any coordinate in a polytope&#39;s interior to its barycentric coordinates using a neural field. This formulation is enabled by our theoretical characterization of barycentric coordinates, which allows us to construct neural fields that parameterize the entire function class of valid coordinates. We demonstrate the flexibility of our model using a variety of objective functions, including multiple smoothness and deformation-aware energies; as a side contribution, we also present mathematically-justified means of measuring and minimizing objectives like total variation on discontinuous neural fields. We offer a practical acceleration strategy, present a thorough validation of our algorithm, and demonstrate several applications.},
  archive      = {J_TOG},
  author       = {Ana Dodik and Oded Stein and Vincent Sitzmann and Justin Solomon},
  doi          = {10.1145/3618403},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {255:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variational barycentric coordinates},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OpenSVBRDF: A database of measured spatially-varying
reflectance. <em>TOG</em>, <em>42</em>(6), 254:1–14. (<a
href="https://doi.org/10.1145/3618358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the first large-scale database of measured spatially-varying anisotropic reflectance, consisting of 1,000 high-quality near-planar SVBRDFs, spanning 9 material categories such as wood, fabric and metal. Each sample is captured in 15 minutes, and represented as a set of high-resolution texture maps that correspond to spatially-varying BRDF parameters and local frames. To build this database, we develop a novel integrated system for robust, high-quality and -efficiency reflectance acquisition and reconstruction. Our setup consists of 2 cameras and 16,384 LEDs. We train 64 lighting patterns for efficient acquisition, in conjunction with a network that predicts per-point reflectance in a neural representation from carefully aligned two-view measurements captured under the patterns. The intermediate results are further fine-tuned with respect to the photographs acquired under 63 effective linear lights, and finally fitted to a BRDF model. We report various statistics of the database, and demonstrate its value in the applications of material generation, classification as well as sampling. All related data, including future additions to the database, can be downloaded from https://opensvbrdf.github.io/.},
  archive      = {J_TOG},
  author       = {Xiaohe Ma and Xianmin Xu and Leyao Zhang and Kun Zhou and Hongzhi Wu},
  doi          = {10.1145/3618358},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {254:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {OpenSVBRDF: A database of measured spatially-varying reflectance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From skin to skeleton: Towards biomechanically accurate 3D
digital humans. <em>TOG</em>, <em>42</em>(6), 253:1–12. (<a
href="https://doi.org/10.1145/3618381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Great progress has been made in estimating 3D human pose and shape from images and video by training neural networks to directly regress the parameters of parametric human models like SMPL. However, existing body models have simplified kinematic structures that do not correspond to the true joint locations and articulations in the human skeletal system, limiting their potential use in biomechanics. On the other hand, methods for estimating biomechanically accurate skeletal motion typically rely on complex motion capture systems and expensive optimization methods. What is needed is a parametric 3D human model with a biomechanically accurate skeletal structure that can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL body model with a biomechanics skeleton. To enable this, we need training data of skeletons inside SMPL meshes in diverse poses. We build such a dataset by optimizing biomechanically accurate skeletons inside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL mesh vertices to the optimized joint locations and bone rotations. Finally, we re-parametrize the SMPL mesh with the new kinematic parameters. The resulting SKEL model is animatable like SMPL but with fewer, and biomechanically-realistic, degrees of freedom. We show that SKEL has more biomechanically accurate joint locations than SMPL, and the bones fit inside the body surface better than previous methods. By fitting SKEL to SMPL meshes we are able to &quot;upgrade&quot; existing human pose and shape datasets to include biomechanical parameters. SKEL provides a new tool to enable biomechanics in the wild, while also providing vision and graphics researchers with a better constrained and more realistic model of human articulation. The model, code, and data are available for research at https://skel.is.tue.mpg.de.},
  archive      = {J_TOG},
  author       = {Marilyn Keller and Keenon Werling and Soyong Shin and Scott Delp and Sergi Pujades and C. Karen Liu and Michael J. Black},
  doi          = {10.1145/3618381},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {253:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {From skin to skeleton: Towards biomechanically accurate 3D digital humans},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scene-aware activity program generation with language
guidance. <em>TOG</em>, <em>42</em>(6), 252:1–16. (<a
href="https://doi.org/10.1145/3618338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of scene-aware activity program generation, which requires decomposing a given activity task into instructions that can be sequentially performed within a target scene to complete the activity. While existing methods have shown the ability to generate rational or executable programs, generating programs with both high rationality and executability still remains a challenge. Hence, we propose a novel method where the key idea is to explicitly combine the language rationality of a powerful language model with dynamic perception of the target scene where instructions are executed, to generate programs with high rationality and executability. Our method iteratively generates instructions for the activity program. Specifically, a two-branch feature encoder operates on a language-based and graph-based representation of the current generation progress to extract language features and scene graph features, respectively. These features are then used by a predictor to generate the next instruction in the program. Subsequently, another module performs the predicted action and updates the scene for perception in the next iteration. Extensive evaluations are conducted on the VirtualHome-Env dataset, showing the advantages of our method over previous work. Key algorithmic designs are validated through ablation studies, and results on other types of inputs are also presented to show the generalizability of our method.},
  archive      = {J_TOG},
  author       = {Zejia Su and Qingnan Fan and Xuelin Chen and Oliver Van Kaick and Hui Huang and Ruizhen Hu},
  doi          = {10.1145/3618338},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {252:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Scene-aware activity program generation with language guidance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online scene CAD recomposition via autonomous scanning.
<em>TOG</em>, <em>42</em>(6), 251:1–16. (<a
href="https://doi.org/10.1145/3618339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous surface reconstruction of 3D scenes has been intensely studied in recent years, however, it is still difficult to accurately reconstruct all the surface details of complex scenes with complicated object relations and severe occlusions, which makes the reconstruction results not suitable for direct use in applications such as gaming and virtual reality. Therefore, instead of reconstructing the detailed surfaces, we aim to recompose the scene with CAD models retrieved from a given dataset to faithfully reflect the object geometry and arrangement in the given scene. Moreover, unlike most of the previous works on scene CAD recomposition requiring an offline reconstructed scene or captured video as input, which leads to significant data redundancy, we propose a novel online scene CAD recomposition method with autonomous scanning, which efficiently recomposes the scene with the guidance of automatically optimized Next-Best-View (NBV) in a single online scanning pass. Based on the key observation that spatial relation in the scene can not only constrain the object pose and layout optimization but also guide the NBV generation, our system consists of two key modules: relation-guided CAD recomposition module that uses relation-constrained global optimization to get accurate object pose and layout estimation, and relation-aware NBV generation module that makes the exploration during the autonomous scanning tailored for our composition task. Extensive experiments have been conducted to show the superiority of our method over previous methods in scanning efficiency and retrieval accuracy as well as the importance of each key component of our method.},
  archive      = {J_TOG},
  author       = {Changhao Li and Junfu Guo and Ruizhen Hu and Ligang Liu},
  doi          = {10.1145/3618339},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {251:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Online scene CAD recomposition via autonomous scanning},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interaction-driven active 3D reconstruction with object
interiors. <em>TOG</em>, <em>42</em>(6), 250:1–12. (<a
href="https://doi.org/10.1145/3618327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an active 3D reconstruction method which integrates visual perception, robot-object interaction , and 3D scanning to recover both the exterior and interior , i.e., unexposed, geometries of a target 3D object. Unlike other works in active vision which focus on optimizing camera viewpoints to better investigate the environment, the primary feature of our reconstruction is an analysis of the interactability of various parts of the target object and the ensuing part manipulation by a robot to enable scanning of occluded regions. As a result, an understanding of part articulations of the target object is obtained on top of complete geometry acquisition. Our method operates fully automatically by a Fetch robot with built-in RGBD sensors. It iterates between interaction analysis and interaction-driven reconstruction, scanning and reconstructing detected moveable parts one at a time, where both the articulated part detection and mesh reconstruction are carried out by neural networks. In the final step, all the remaining, non-articulated parts, including all the interior structures that had been exposed by prior part manipulations and subsequently scanned, are reconstructed to complete the acquisition. We demonstrate the performance of our method via qualitative and quantitative evaluation, ablation studies, comparisons to alternatives, as well as experiments in a real environment.},
  archive      = {J_TOG},
  author       = {Zihao Yan and Fubao Su and Mingyang Wang and Ruizhen Hu and Hao Zhang and Hui Huang},
  doi          = {10.1145/3618327},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {250:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Interaction-driven active 3D reconstruction with object interiors},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Thin on-sensor nanophotonic array cameras. <em>TOG</em>,
<em>42</em>(6), 249:1–18. (<a
href="https://doi.org/10.1145/3618398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today&#39;s commodity camera systems rely on compound optics to map light originating from the scene to positions on the sensor where it gets recorded as an image. To record images without optical aberrations, i.e., deviations from Gauss&#39; linear model of optics, typical lens systems introduce increasingly complex stacks of optical elements which are responsible for the height of existing commodity cameras. In this work, we investigate flat nanophotonic computational cameras as an alternative that employs an array of skewed lenslets and a learned reconstruction approach. The optical array is embedded on a metasurface that, at 700 nm height, is flat and sits on the sensor cover glass at 2.5 mm focal distance from the sensor. To tackle the highly chromatic response of a metasurface and design the array over the entire sensor, we propose a differentiable optimization method that continuously samples over the visible spectrum and factorizes the optical modulation for different incident fields into individual lenses. We reconstruct a megapixel image from our flat imager with a learned probabilistic reconstruction method that employs a generative diffusion model to sample an implicit prior. To tackle scene-dependent aberrations in broadband , we propose a method for acquiring paired captured training data in varying illumination conditions. We assess the proposed flat camera design in simulation and with an experimental prototype, validating that the method is capable of recovering images from diverse scenes in broadband with a single nanophotonic layer.},
  archive      = {J_TOG},
  author       = {Praneeth Chakravarthula and Jipeng Sun and Xiao Li and Chenyang Lei and Gene Chou and Mario Bijelic and Johannes Froesch and Arka Majumdar and Felix Heide},
  doi          = {10.1145/3618398},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {249:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Thin on-sensor nanophotonic array cameras},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fluid simulation on neural flow maps. <em>TOG</em>,
<em>42</em>(6), 248:1–21. (<a
href="https://doi.org/10.1145/3618392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Neural Flow Maps, a novel simulation method bridging the emerging paradigm of implicit neural representations with fluid simulation based on the theory of flow maps, to achieve state-of-the-art simulation of in-viscid fluid phenomena. We devise a novel hybrid neural field representation, Spatially Sparse Neural Fields (SSNF), which fuses small neural networks with a pyramid of overlapping, multi-resolution, and spatially sparse grids, to compactly represent long-term spatiotemporal velocity fields at high accuracy. With this neural velocity buffer in hand, we compute long-term, bidirectional flow maps and their Jacobians in a mechanistically symmetric manner, to facilitate drastic accuracy improvement over existing solutions. These long-range, bidirectional flow maps enable high advection accuracy with low dissipation, which in turn facilitates high-fidelity incompressible flow simulations that manifest intricate vortical structures. We demonstrate the efficacy of our neural fluid simulation in a variety of challenging simulation scenarios, including leapfrogging vortices, colliding vortices, vortex reconnections, as well as vortex generation from moving obstacles and density differences. Our examples show increased performance over existing methods in terms of energy conservation, visual complexity, adherence to experimental observations, and preservation of detailed vortical structures.},
  archive      = {J_TOG},
  author       = {Yitong Deng and Hong-Xing Yu and Diyang Zhang and Jiajun Wu and Bo Zhu},
  doi          = {10.1145/3618392},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {248:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fluid simulation on neural flow maps},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ClothCombo: Modeling inter-cloth interaction for draping
multi-layered clothes. <em>TOG</em>, <em>42</em>(6), 247:1–13. (<a
href="https://doi.org/10.1145/3618376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ClothCombo, a pipeline to drape arbitrary combinations of clothes on 3D human models with varying body shapes and poses. While existing learning-based approaches for draping clothes have shown promising results, multi-layered clothing remains challenging as it is non-trivial to model inter-cloth interaction. To this end, our method utilizes a GNN-based network to efficiently model the interaction between clothes in different layers, thus enabling multi-layered clothing. Specifically, we first create feature embedding for each cloth using a topology-agnostic network. Then, the draping network deforms all clothes to fit the target body shape and pose without considering inter-cloth interaction. Lastly, the untangling network predicts the per-vertex displacements in a way that resolves interpenetration between clothes. In experiments, the proposed model demonstrates strong performance in complex multi-layered scenarios. Being agnostic to cloth topology, our method can be readily used for layered virtual try-on of real clothes in diverse poses and combinations of clothes.},
  archive      = {J_TOG},
  author       = {Dohae Lee and Hyun Kang and In-Kwon Lee},
  doi          = {10.1145/3618376},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {247:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {ClothCombo: Modeling inter-cloth interaction for draping multi-layered clothes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MIPS-fusion: Multi-implicit-submaps for scalable and robust
online neural RGB-d reconstruction. <em>TOG</em>, <em>42</em>(6),
246:1–16. (<a href="https://doi.org/10.1145/3618363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce MIPS-Fusion, a robust and scalable online RGB-D reconstruction method based on a novel neural implicit representation - multi-implicit-submap. Different from existing neural RGB-D reconstruction methods lacking either flexibility with a single neural map or scalability due to extra storage of feature grids, we propose a pure neural representation tackling both difficulties with a divide-and-conquer design. In our method, neural submaps are incrementally allocated alongside the scanning trajectory and efficiently learned with local neural bundle adjustments. The submaps can be refined individually in a back-end optimization and optimized jointly to realize submap-level loop closure. Meanwhile, we propose a hybrid tracking approach combining randomized and gradient-based pose optimizations. For the first time, randomized optimization is made possible in neural tracking with several key designs to the learning process, enabling efficient and robust tracking even under fast camera motions. The extensive evaluation demonstrates that our method attains higher reconstruction quality than the state of the arts for large-scale scenes and under fast camera motions.},
  archive      = {J_TOG},
  author       = {Yijie Tang and Jiazhao Zhang and Zhinan Yu and He Wang and Kai Xu},
  doi          = {10.1145/3618363},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {246:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {MIPS-fusion: Multi-implicit-submaps for scalable and robust online neural RGB-D reconstruction},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust zero level-set extraction from unsigned distance
fields based on double covering. <em>TOG</em>, <em>42</em>(6), 245:1–15.
(<a href="https://doi.org/10.1145/3618314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new method, called DoubleCoverUDF, for extracting the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a learned UDF and a user-specified parameter r (a small positive real number) as input and extracts an iso-surface with an iso-value r using the conventional marching cubes algorithm. We show that the computed iso-surface is the boundary of the r -offset volume of the target zero level-set S , which is an orientable manifold, regardless of the topology of S. Next, the algorithm computes a covering map to project the boundary mesh onto S , preserving the mesh&#39;s topology and avoiding folding. If S is an orientable manifold surface, our algorithm separates the double-layered mesh into a single layer using a robust minimum-cut post-processing step. Otherwise, it keeps the double-layered mesh as the output. We validate our algorithm by reconstructing 3D surfaces of open models and demonstrate its efficacy and effectiveness on synthetic models and benchmark datasets. Our experimental results confirm that our method is robust and produces meshes with better quality in terms of both visual evaluation and quantitative measures than existing UDF-based methods. The source code is available at https://github.com/jjjkkyz/DCUDF.},
  archive      = {J_TOG},
  author       = {Fei Hou and Xuhui Chen and Wencheng Wang and Hong Qin and Ying He},
  doi          = {10.1145/3618314},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {245:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust zero level-set extraction from unsigned distance fields based on double covering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ProSpect: Prompt spectrum for attribute-aware
personalization of diffusion models. <em>TOG</em>, <em>42</em>(6),
244:1–14. (<a href="https://doi.org/10.1145/3618342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalizing generative models offers a way to guide image generation with user-provided references. Current personalization methods can invert an object or concept into the textual conditioning space and compose new natural sentences for text-to-image diffusion models. However, representing and editing specific visual attributes such as material, style, and layout remains a challenge, leading to a lack of disentanglement and editability. To address this problem, we propose a novel approach that leverages the step-by-step generation process of diffusion models, which generate images from low to high frequency information, providing a new perspective on representing, generating, and editing images. We develop the Prompt Spectrum Space P*, an expanded textual conditioning space, and a new image representation method called ProSpect. ProSpect represents an image as a collection of inverted textual token embeddings encoded from per-stage prompts, where each prompt corresponds to a specific generation stage (i.e., a group of consecutive steps) of the diffusion model. Experimental results demonstrate that P* and ProSpect offer better disentanglement and controllability compared to existing methods. We apply ProSpect in various personalized attribute-aware image generation applications, such as image-guided or text-driven manipulations of materials, style, and layout, achieving previously unattainable results from a single image input without fine-tuning the diffusion models. Our source code is available at https://github.com/zyxElsa/ProSpect.},
  archive      = {J_TOG},
  author       = {Yuxin Zhang and Weiming Dong and Fan Tang and Nisha Huang and Haibin Huang and Chongyang Ma and Tong-Yee Lee and Oliver Deussen and Changsheng Xu},
  doi          = {10.1145/3618342},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {244:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {ProSpect: Prompt spectrum for attribute-aware personalization of diffusion models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A neural space-time representation for text-to-image
personalization. <em>TOG</em>, <em>42</em>(6), 243:1–10. (<a
href="https://doi.org/10.1145/3618322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key aspect of text-to-image personalization methods is the manner in which the target concept is represented within the generative process. This choice greatly affects the visual fidelity, downstream editability, and disk space needed to store the learned concept. In this paper, we explore a new text-conditioning space that is dependent on both the denoising process timestep (time) and the denoising U-Net layers (space) and showcase its compelling properties. A single concept in the space-time representation is composed of hundreds of vectors, one for each combination of time and space , making this space challenging to optimize directly. Instead, we propose to implicitly represent a concept in this space by optimizing a small neural mapper that receives the current time and space parameters and outputs the matching token embedding. In doing so, the entire personalized concept is represented by the parameters of the learned mapper, resulting in a compact, yet expressive, representation. Similarly to other personalization methods, the output of our neural mapper resides in the input space of the text encoder. We observe that one can significantly improve the convergence and visual fidelity of the concept by introducing a textual bypass , where our neural mapper additionally outputs a residual that is added to the output of the text encoder. Finally, we show how one can impose an importance-based ordering over our implicit representation, providing users control over the reconstruction and editability of the learned concept using a single trained model. We demonstrate the effectiveness of our approach over a range of concepts and prompts, showing our method&#39;s ability to generate high-quality and controllable compositions without fine-tuning any parameters of the generative model itself.},
  archive      = {J_TOG},
  author       = {Yuval Alaluf and Elad Richardson and Gal Metzer and Daniel Cohen-Or},
  doi          = {10.1145/3618322},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {243:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {A neural space-time representation for text-to-image personalization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GeoLatent: A geometric approach to latent space design for
deformable shape generators. <em>TOG</em>, <em>42</em>(6), 242:1–20. (<a
href="https://doi.org/10.1145/3618371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study how to optimize the latent space of neural shape generators that map latent codes to 3D deformable shapes. The key focus is to look at a deformable shape generator from a differential geometry perspective. We define a Riemannian metric based on as-rigid-as-possible and as-conformal-as-possible deformation energies. Under this metric, we study two desired properties of the latent space: 1) straight-line interpolations in latent codes follow geodesic curves; 2) latent codes disentangle pose and shape variations at different scales. Strictly enforcing the geometric interpolation property, however, only applies if the metric matrix is a constant. We show how to achieve this property approximately by enforcing that geodesic interpolations are axis-aligned, i.e., interpolations along coordinate axis follow geodesic curves. In addition, we introduce a novel approach that decouples pose and shape variations via generalized eigendecomposition. We also study efficient regularization terms for learning deformable shape generators, e.g., that promote smooth interpolations. Experimental results on benchmark datasets show that our approach leads to interpretable latent codes, improves the generalizability of synthetic shapes, and enhances performance in geodesic interpolation and geodesic shooting.},
  archive      = {J_TOG},
  author       = {Haitao Yang and Bo Sun and Liyan Chen and Amy Pavel and Qixing Huang},
  doi          = {10.1145/3618371},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {242:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {GeoLatent: A geometric approach to latent space design for deformable shape generators},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concept decomposition for visual exploration and
inspiration. <em>TOG</em>, <em>42</em>(6), 241:1–13. (<a
href="https://doi.org/10.1145/3618315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A creative idea is often born from transforming, combining, and modifying ideas from existing visual examples capturing various concepts. However, one cannot simply copy the concept as a whole, and inspiration is achieved by examining certain aspects of the concept. Hence, it is often necessary to separate a concept into different aspects to provide new perspectives. In this paper, we propose a method to decompose a visual concept, represented as a set of images, into different visual aspects encoded in a hierarchical tree structure. We utilize large vision-language models and their rich latent space for concept decomposition and generation. Each node in the tree represents a sub-concept using a learned vector embedding injected into the latent space of a pretrained text-to-image model. We use a set of regularizations to guide the optimization of the embedding vectors encoded in the nodes to follow the hierarchical structure of the tree. Our method allows to explore and discover new concepts derived from the original one. The tree provides the possibility of endless visual sampling at each node, allowing the user to explore the hidden sub-concepts of the object of interest. The learned aspects in each node can be combined within and across trees to create new visual ideas, and can be used in natural language sentences to apply such aspects to new designs. Project page: https://inspirationtree.github.io/inspirationtree/},
  archive      = {J_TOG},
  author       = {Yael Vinker and Andrey Voynov and Daniel Cohen-Or and Ariel Shamir},
  doi          = {10.1145/3618315},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {241:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Concept decomposition for visual exploration and inspiration},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Power plastics: A hybrid lagrangian/eulerian solver for
mesoscale inelastic flows. <em>TOG</em>, <em>42</em>(6), 240:1–11. (<a
href="https://doi.org/10.1145/3618344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel hybrid Lagrangian/Eulerian method for simulating inelastic flows that generates high-quality particle distributions with adaptive volumes. At its core, our approach integrates an updated Lagrangian time discretization of continuum mechanics with the Power Particle-In-Cell geometric representation of deformable materials. As a result, we obtain material points described by optimized density kernels that precisely track the varying particle volumes both spatially and temporally. For efficient CFL-rate simulations, we also propose an implicit time integration for our system using a non-linear Gauss-Seidel solver inspired by X-PBD, viewing Eulerian nodal velocities as primal variables. We demonstrate the versatility of our method with simulations of mesoscale bubbles, sands, liquid, and foams.},
  archive      = {J_TOG},
  author       = {Ziyin Qu and Minchen Li and Yin Yang and Chenfanfu Jiang and Fernando De Goes},
  doi          = {10.1145/3618344},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {240:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Power plastics: A hybrid Lagrangian/Eulerian solver for mesoscale inelastic flows},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Authoring and simulating meandering rivers. <em>TOG</em>,
<em>42</em>(6), 239:1–14. (<a
href="https://doi.org/10.1145/3618350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for interactively authoring and simulating meandering river networks. Starting from a terrain with an initial low-resolution network encoded as a directed graph, we simulate the evolution of the path of the different river channels using a physically-based migration equation augmented with control terms. The curvature-based terms in the equation allow us to reproduce phenomena identified in geomorphology, such as downstream migration of bends. Control terms account for the influence of the landscape topography and user-defined river trajectory constraints. Our model implements abrupt events that shape meandering networks, such as cutoffs forming oxbow lakes and avulsions. We visually show the effectiveness of our method and compare the generated networks quantitatively to river data by analyzing sinuosity and wavelength metrics. Our vector-based model runs at interactive rates, allowing for efficient authoring of large-scale meandering networks.},
  archive      = {J_TOG},
  author       = {Axel Paris and Eric Guérin and Pauline Collon and Eric Galin},
  doi          = {10.1145/3618350},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {239:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Authoring and simulating meandering rivers},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-light image enhancement with wavelet-based diffusion
models. <em>TOG</em>, <em>42</em>(6), 238:1–14. (<a
href="https://doi.org/10.1145/3618373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have achieved promising results in image restoration tasks, yet suffer from time-consuming, excessive computational resource consumption, and unstable restoration. To address these issues, we propose a robust and efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL. Specifically, we present a wavelet-based conditional diffusion model (WCDM) that leverages the generative power of diffusion models to produce results with satisfactory perceptual fidelity. Additionally, it also takes advantage of the strengths of wavelet transformation to greatly accelerate inference and reduce computational resource usage without sacrificing information. To avoid chaotic content and diversity, we perform both forward diffusion and denoising in the training phase of WCDM, enabling the model to achieve stable denoising and reduce randomness during inference. Moreover, we further design a high-frequency restoration module (HFRM) that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration. Extensive experiments on publicly available real-world benchmarks demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and it achieves remarkable improvements in efficiency compared to previous diffusion-based methods. In addition, we empirically show that the application for low-light face detection also reveals the latent practical values of our method. Code is available at https://github.com/JianghaiSCU/Diffusion-Low-Light.},
  archive      = {J_TOG},
  author       = {Hai Jiang and Ao Luo and Haoqiang Fan and Songchen Han and Shuaicheng Liu},
  doi          = {10.1145/3618373},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {238:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Low-light image enhancement with wavelet-based diffusion models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing diffusion models with 3D perspective geometry
constraints. <em>TOG</em>, <em>42</em>(6), 237:1–15. (<a
href="https://doi.org/10.1145/3618389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While perspective is a well-studied topic in art, it is generally taken for granted in images. However, for the recent wave of high-quality image synthesis methods such as latent diffusion models, perspective accuracy is not an explicit requirement. Since these methods are capable of outputting a wide gamut of possible images, it is difficult for these synthesized images to adhere to the principles of linear perspective. We introduce a novel geometric constraint in the training process of generative models to enforce perspective accuracy. We show that outputs of models trained with this constraint both appear more realistic and improve performance of downstream models trained on generated images. Subjective human trials show that images generated with latent diffusion models trained with our constraint are preferred over images from the Stable Diffusion V2 model 70\% of the time. SOTA monocular depth estimation models such as DPT and PixelFormer, fine-tuned on our images, outperform the original models trained on real images by up to 7.03\% in RMSE and 19.3\% in SqRel on the KITTI test set for zero-shot transfer.},
  archive      = {J_TOG},
  author       = {Rishi Upadhyay and Howard Zhang and Yunhao Ba and Ethan Yang and Blake Gella and Sicheng Jiang and Alex Wong and Achuta Kadambi},
  doi          = {10.1145/3618389},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {237:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Enhancing diffusion models with 3D perspective geometry constraints},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning the geodesic embedding with graph neural networks.
<em>TOG</em>, <em>42</em>(6), 236:1–12. (<a
href="https://doi.org/10.1145/3618317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GEGNN, a learning-based method for computing the approximate geodesic distance between two arbitrary points on discrete polyhedra surfaces with constant time complexity after fast precomputation. Previous relevant methods either focus on computing the geodesic distance between a single source and all destinations, which has linear complexity at least or require a long precomputation time. Our key idea is to train a graph neural network to embed an input mesh into a high-dimensional embedding space and compute the geodesic distance between a pair of points using the corresponding embedding vectors and a lightweight decoding function. To facilitate the learning of the embedding, we propose novel graph convolution and graph pooling modules that incorporate local geodesic information and are verified to be much more effective than previous designs. After training, our method requires only one forward pass of the network per mesh as precomputation. Then, we can compute the geodesic distance between a pair of points using our decoding function, which requires only several matrix multiplications and can be massively parallelized on GPUs. We verify the efficiency and effectiveness of our method on ShapeNet and demonstrate that our method is faster than existing methods by orders of magnitude while achieving comparable or better accuracy. Additionally, our method exhibits robustness on noisy and incomplete meshes and strong generalization ability on out-of-distribution meshes. The code and pretrained model can be found on https://github.com/IntelligentGeometry/GeGnn.},
  archive      = {J_TOG},
  author       = {Bo Pang and Zhongtian Zheng and Guoping Wang and Peng-Shuai Wang},
  doi          = {10.1145/3618317},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {236:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning the geodesic embedding with graph neural networks},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient cone singularity construction for conformal
parameterizations. <em>TOG</em>, <em>42</em>(6), 235:1–13. (<a
href="https://doi.org/10.1145/3618407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an efficient method to construct sparse cone singularities under distortion-bounded constraints for conformal parameterizations. Central to our algorithm is using the technique of shape derivatives to move cones for distortion reduction without changing the number of cones. In particular, the supernodal sparse Cholesky update significantly accelerates this movement process. To satisfy the distortion-bounded constraint, we alternately move cones and add cones. The capability and feasibility of our approach are demonstrated over a data set containing 3885 models. Compared with the state-of-the-art method, we achieve an average acceleration of 15 times and slightly fewer cones for the same amount of distortion.},
  archive      = {J_TOG},
  author       = {Mo Li and Qing Fang and Zheng Zhang and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3618407},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {235:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient cone singularity construction for conformal parameterizations},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metric optimization in penner coordinates. <em>TOG</em>,
<em>42</em>(6), 234:1–19. (<a
href="https://doi.org/10.1145/3618394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many parametrization and mapping-related problems in geometry processing can be viewed as metric optimization problems, i.e., computing a metric minimizing a functional and satisfying a set of constraints, such as flatness. Penner coordinates are global coordinates on the space of metrics on meshes with a fixed vertex set and topology, but varying connectivity, making it homeomorphic to the Euclidean space of dimension equal to the number of edges in the mesh, without any additional constraints imposed. These coordinates play an important role in the theory of discrete conformal maps, enabling recent development of highly robust algorithms with convergence and solution existence guarantees for computing such maps. We demonstrate how Penner coordinates can be used to solve a general class of optimization problems involving metrics, including optimization and interpolation, while retaining the key solution existence guarantees available for discrete conformal maps.},
  archive      = {J_TOG},
  author       = {Ryan Capouellez and Denis Zorin},
  doi          = {10.1145/3618394},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {234:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Metric optimization in penner coordinates},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diffusion posterior illumination for ambiguity-aware inverse
rendering. <em>TOG</em>, <em>42</em>(6), 233:1–14. (<a
href="https://doi.org/10.1145/3618357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse rendering, the process of inferring scene properties from images, is a challenging inverse problem. The task is ill-posed, as many different scene configurations can give rise to the same image. Most existing solutions incorporate priors into the inverse-rendering pipeline to encourage plausible solutions, but they do not consider the inherent ambiguities and the multi-modal distribution of possible decompositions. In this work, we propose a novel scheme that integrates a denoising diffusion probabilistic model pre-trained on natural illumination maps into an optimization framework involving a differentiable path tracer. The proposed method allows sampling from combinations of illumination and spatially-varying surface materials that are, both, natural and explain the image observations. We further conduct an extensive comparative study of different priors on illumination used in previous work on inverse rendering. Our method excels in recovering materials and producing highly realistic and diverse environment map samples that faithfully explain the illumination of the input images.},
  archive      = {J_TOG},
  author       = {Linjie Lyu and Ayush Tewari and Marc Habermann and Shunsuke Saito and Michael Zollhöfer and Thomas Leimkühler and Christian Theobalt},
  doi          = {10.1145/3618357},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {233:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Diffusion posterior illumination for ambiguity-aware inverse rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable rendering of parametric geometry.
<em>TOG</em>, <em>42</em>(6), 232:1–18. (<a
href="https://doi.org/10.1145/3618387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an efficient method for differentiable rendering of parametric surfaces and curves, which enables their use in inverse graphics problems. Our central observation is that a representative triangle mesh can be extracted from a continuous parametric object in a differentiable and efficient way. We derive differentiable meshing operators for surfaces and curves that provide varying levels of approximation granularity. With triangle mesh approximations, we can readily leverage existing machinery for differentiable mesh rendering to handle parametric geometry. Naively combining differentiable tessellation with inverse graphics settings lacks robustness and is prone to reaching undesirable local minima. To this end, we draw a connection between our setting and the optimization of triangle meshes in inverse graphics and present a set of optimization techniques, including regularizations and coarse-to-fine schemes. We show the viability and efficiency of our method in a set of image-based computer-aided design applications.},
  archive      = {J_TOG},
  author       = {Markus Worchel and Marc Alexa},
  doi          = {10.1145/3618387},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {232:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable rendering of parametric geometry},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DR-occluder: Generating occluders using differentiable
rendering. <em>TOG</em>, <em>42</em>(6), 231:1–14. (<a
href="https://doi.org/10.1145/3618346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The target of the occluder is to use very few faces to maintain similar occlusion properties of the original 3D model. In this paper, we present DR-Occluder, a novel coarse-to-fine framework for occluder generation that leverages differentiable rendering to optimize a triangle set to an occluder. Unlike prior work, which has not utilized differentiable rendering for this task, our approach provides the ability to optimize a 3D shape to defined targets. Given a 3D model as input, our method first projects it to silhouette images, which are then processed by a convolution network to output a group of vertex offsets. These offsets are used to transform a group of distributed triangles into a preliminary occluder, which is further optimized by differentiable rendering. Finally, triangles whose area is smaller than a threshold are removed to obtain the final occluder. Our extensive experiments demonstrate that DR-Occluder significantly outperforms state-of-the-art methods in terms of occlusion quality. Furthermore, we compare the performance of our method with other approaches in a commercial engine, providing compelling evidence of its effectiveness.},
  archive      = {J_TOG},
  author       = {Jiaxian Wu and Yue Lin and Dehui Lu},
  doi          = {10.1145/3618346},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {231:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {DR-occluder: Generating occluders using differentiable rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IconShop: Text-guided vector icon synthesis with
autoregressive transformers. <em>TOG</em>, <em>42</em>(6), 230:1–14. (<a
href="https://doi.org/10.1145/3618364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scalable Vector Graphics (SVG) is a popular vector image format that offers good support for interactivity and animation. Despite its appealing characteristics, creating custom SVG content can be challenging for users due to the steep learning curve required to understand SVG grammars or get familiar with professional editing software. Recent advancements in text-to-image generation have inspired researchers to explore vector graphics synthesis using either image-based methods (i.e., text → raster image → vector graphics) combining text-to-image generation models with image vectorization, or language-based methods (i.e., text → vector graphics script) through pretrained large language models. Nevertheless, these methods suffer from limitations in terms of generation quality, diversity, and flexibility. In this paper, we introduce IconShop, a text-guided vector icon synthesis method using autoregressive transformers. The key to success of our approach is to sequentialize and tokenize SVG paths (and textual descriptions as guidance) into a uniquely decodable token sequence. With that, we are able to exploit the sequence learning power of autoregressive transformers, while enabling both unconditional and text-conditioned icon synthesis. Through standard training to predict the next token on a large-scale vector icon dataset accompanied by textural descriptions, the proposed IconShop consistently exhibits better icon synthesis capability than existing image-based and language-based methods both quantitatively (using the FID and CLIP scores) and qualitatively (through formal subjective user studies). Meanwhile, we observe a dramatic improvement in generation diversity, which is validated by the objective Uniqueness and Novelty measures. More importantly, we demonstrate the flexibility of IconShop with multiple novel icon synthesis tasks, including icon editing, icon interpolation, icon semantic combination, and icon design auto-suggestion.},
  archive      = {J_TOG},
  author       = {Ronghuan Wu and Wanchao Su and Kede Ma and Jing Liao},
  doi          = {10.1145/3618364},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {230:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {IconShop: Text-guided vector icon synthesis with autoregressive transformers},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editing motion graphics video via motion vectorization and
transformation. <em>TOG</em>, <em>42</em>(6), 229:1–13. (<a
href="https://doi.org/10.1145/3618316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion graphics videos are widely used in Web design, digital advertising, animated logos and film title sequences, to capture a viewer&#39;s attention. But editing such video is challenging because the video provides a low-level sequence of pixels and frames rather than higher-level structure such as the objects in the video with their corresponding motions and occlusions. We present a motion vectorization pipeline for converting motion graphics video into an SVG motion program that provides such structure. The resulting SVG program can be rendered using any SVG renderer (e.g. most Web browsers) and edited using any SVG editor. We also introduce a program transformation API that facilitates editing of a SVG motion program to create variations that adjust the timing, motions and/or appearances of objects. We show how the API can be used to create a variety of effects including retiming object motion to match a music beat, adding motion textures to objects, and collision preserving appearance changes.},
  archive      = {J_TOG},
  author       = {Sharon Zhang and Jiaju Ma and Jiajun Wu and Daniel Ritchie and Maneesh Agrawala},
  doi          = {10.1145/3618316},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {229:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Editing motion graphics video via motion vectorization and transformation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EXIM: A hybrid explicit-implicit representation for
text-guided 3D shape generation. <em>TOG</em>, <em>42</em>(6), 228:1–12.
(<a href="https://doi.org/10.1145/3618312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new text-guided technique for generating 3D shapes. The technique leverages a hybrid 3D shape representation, namely EXIM, combining the strengths of explicit and implicit representations. Specifically, the explicit stage controls the topology of the generated 3D shapes and enables local modifications, whereas the implicit stage refines the shape and paints it with plausible colors. Also, the hybrid approach separates the shape and color and generates color conditioned on shape to ensure shape-color consistency. Unlike the existing state-of-the-art methods, we achieve high-fidelity shape generation from natural-language descriptions without the need for time-consuming per-shape optimization or reliance on human-annotated texts during training or test-time optimization. Further, we demonstrate the applicability of our approach to generate indoor scenes with consistent styles using text-induced 3D shapes. Through extensive experiments, we demonstrate the compelling quality of our results and the high coherency of our generated shapes with the input texts, surpassing the performance of existing methods by a significant margin. Codes and models are released at https://github.com/liuzhengzhe/EXIM.},
  archive      = {J_TOG},
  author       = {Zhengzhe Liu and Jingyu Hu and Ka-Hei Hui and Xiaojuan Qi and Daniel Cohen-Or and Chi-Wing Fu},
  doi          = {10.1145/3618312},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {228:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {EXIM: A hybrid explicit-implicit representation for text-guided 3D shape generation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TwinTex: Geometry-aware texture generation for abstracted 3D
architectural models. <em>TOG</em>, <em>42</em>(6), 227:1–14. (<a
href="https://doi.org/10.1145/3618328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse architectural models are often generated at scales ranging from individual buildings to scenes for downstream applications such as Digital Twin City, Metaverse, LODs, etc. Such piece-wise planar models can be abstracted as twins from 3D dense reconstructions. However, these models typically lack realistic texture relative to the real building or scene, making them unsuitable for vivid display or direct reference. In this paper, we present TwinTex , the first automatic texture mapping framework to generate a photorealistic texture for a piece-wise planar proxy. Our method addresses most challenges occurring in such twin texture generation. Specifically, for each primitive plane, we first select a small set of photos with greedy heuristics considering photometric quality, perspective quality and facade texture completeness. Then, different levels of line features (LoLs) are extracted from the set of selected photos to generate guidance for later steps. With LoLs, we employ optimization algorithms to align texture with geometry from local to global. Finally, we fine-tune a diffusion model with a multi-mask initialization component and a new dataset to inpaint the missing region. Experimental results on many buildings, indoor scenes and man-made objects of varying complexity demonstrate the generalization ability of our algorithm. Our approach surpasses state-of-the-art texture mapping methods in terms of high-fidelity quality and reaches a human-expert production level with much less effort.},
  archive      = {J_TOG},
  author       = {Weidan Xiong and Hongqian Zhang and Botao Peng and Ziyu Hu and Yongli Wu and Jianwei Guo and Hui Huang},
  doi          = {10.1145/3618328},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {227:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {TwinTex: Geometry-aware texture generation for abstracted 3D architectural models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AvatarStudio: Text-driven editing of 3D dynamic human head
avatars. <em>TOG</em>, <em>42</em>(6), 226:1–18. (<a
href="https://doi.org/10.1145/3618368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing and editing full-head performances enables the creation of virtual characters with various applications such as extended reality and media production. The past few years witnessed a steep rise in the photorealism of human head avatars. Such avatars can be controlled through different input data modalities, including RGB, audio, depth, IMUs, and others. While these data modalities provide effective means of control, they mostly focus on editing the head movements such as the facial expressions, head pose, and/or camera viewpoint. In this paper, we propose AvatarStudio, a text-based method for editing the appearance of a dynamic full head avatar. Our approach builds on existing work to capture dynamic performances of human heads using Neural Radiance Field (NeRF) and edits this representation with a text-to-image diffusion model. Specifically, we introduce an optimization strategy for incorporating multiple keyframes representing different camera viewpoints and time stamps of a video performance into a single diffusion model. Using this personalized diffusion model, we edit the dynamic NeRF by introducing view-and-time-aware Score Distillation Sampling (VT-SDS) following a model-based guidance approach. Our method edits the full head in a canonical space and then propagates these edits to the remaining time steps via a pre-trained deformation network. We evaluate our method visually and numerically via a user study, and results show that our method outperforms existing approaches. Our experiments validate the design choices of our method and highlight that our edits are genuine, personalized, as well as 3D- and time-consistent.},
  archive      = {J_TOG},
  author       = {Mohit Mendiratta and Xingang Pan and Mohamed Elgharib and Kartik Teotia and Mallikarjun B R and Ayush Tewari and Vladislav Golyanik and Adam Kortylewski and Christian Theobalt},
  doi          = {10.1145/3618368},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {226:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {AvatarStudio: Text-driven editing of 3D dynamic human head avatars},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BakedAvatar: Baking neural fields for real-time head avatar
synthesis. <em>TOG</em>, <em>42</em>(6), 225:1–17. (<a
href="https://doi.org/10.1145/3618399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing photorealistic 4D human head avatars from videos is essential for VR/AR, telepresence, and video game applications. Although existing Neural Radiance Fields (NeRF)-based methods achieve high-fidelity results, the computational expense limits their use in real-time applications. To overcome this limitation, we introduce BakedAvatar , a novel representation for real-time neural head avatar synthesis, deployable in a standard polygon rasterization pipeline. Our approach extracts deformable multi-layer meshes from learned isosurfaces of the head and computes expression-, pose-, and view-dependent appearances that can be baked into static textures for efficient rasterization. We thus propose a three-stage pipeline for neural head avatar synthesis, which includes learning continuous deformation, manifold, and radiance fields, extracting layered meshes and textures, and fine-tuning texture details with differential rasterization. Experimental results demonstrate that our representation generates synthesis results of comparable quality to other state-of-the-art methods while significantly reducing the inference time required. We further showcase various head avatar synthesis results from monocular videos, including view synthesis, face reenactment, expression editing, and pose editing, all at interactive frame rates on commodity devices. Source codes and demos are available on our project page.},
  archive      = {J_TOG},
  author       = {Hao-Bin Duan and Miao Wang and Jin-Chuan Shi and Xu-Chuan Chen and Yan-Pei Cao},
  doi          = {10.1145/3618399},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {225:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {BakedAvatar: Baking neural fields for real-time head avatar synthesis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controllable group choreography using contrastive diffusion.
<em>TOG</em>, <em>42</em>(6), 224:1–14. (<a
href="https://doi.org/10.1145/3618356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music-driven group choreography poses a considerable challenge but holds significant potential for a wide range of industrial applications. The ability to generate synchronized and visually appealing group dance motions that are aligned with music opens up opportunities in many fields such as entertainment, advertising, and virtual performances. However, most of the recent works are not able to generate high-fidelity long-term motions, or fail to enable controllable experience. In this work, we aim to address the demand for high-quality and customizable group dance generation by effectively governing the consistency and diversity of group choreographies. In particular, we utilize a diffusion-based generative approach to enable the synthesis of flexible number of dancers and long-term group dances, while ensuring coherence to the input music. Ultimately, we introduce a Group Contrastive Diffusion (GCD) strategy to enhance the connection between dancers and their group, presenting the ability to control the consistency or diversity level of the synthesized group animation via the classifier-guidance sampling technique. Through intensive experiments and evaluation, we demonstrate the effectiveness of our approach in producing visually captivating and consistent group dance motions. The experimental results show the capability of our method to achieve the desired levels of consistency and diversity, while maintaining the overall quality of the generated group choreography.},
  archive      = {J_TOG},
  author       = {Nhat Le and Tuong Do and Khoa Do and Hien Nguyen and Erman Tjiputra and Quang D. Tran and Anh Nguyen},
  doi          = {10.1145/3618356},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {224:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Controllable group choreography using contrastive diffusion},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capturing animation-ready isotropic materials using
systematic poking. <em>TOG</em>, <em>42</em>(6), 223:1–27. (<a
href="https://doi.org/10.1145/3618406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing material properties of real-world elastic solids is both challenging and highly relevant to many applications in computer graphics, robotics and related fields. We give a non-intrusive, in-situ and inexpensive approach to measure the nonlinear elastic energy density function of man-made materials and biological tissues. We poke the elastic object with 3d-printed rigid cylinders of known radii, and use a precision force meter to record the contact force as a function of the indentation depth, which we measure using a force meter stand, or a novel unconstrained laser setup. We model the 3D elastic solid using the Finite Element Method (FEM), and elastic energy using a compressible Valanis-Landel material that generalizes Neo-Hookean materials by permitting arbitrary tensile behavior under large deformations. We then use optimization to fit the nonlinear isotropic elastic energy so that the FEM contact forces and indentations match their measured real-world counterparts. Because we use carefully designed cubic splines, our materials are accurate in a large range of stretches and robust to inversions, and are therefore &quot;animation-ready&quot; for computer graphics applications. We demonstrate how to exploit radial symmetry to convert the 3D elastostatic contact problem to the mathematically equivalent 2D problem, which vastly accelerates optimization. We also greatly improve the theory and robustness of stretch-based elastic materials, by giving a simple and elegant formula to compute the tangent stiffness matrix, with rigorous proofs and singularity handling. We also contribute the observation that volume compressibility can be estimated by poking with rigid cylinders of different radii, which avoids optical cameras and greatly simplifies experiments. We validate our method by performing full 3D simulations using the optimized materials and confirming that they match real-world forces, indentations and real deformed 3D shapes. We also validate it using a &quot;Shore 00&quot; durometer, a standard device for measuring material hardness.},
  archive      = {J_TOG},
  author       = {Huanyu Chen and Danyong Zhao and Jernej Barbič},
  doi          = {10.1145/3618406},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {223:1–27},
  shortjournal = {ACM Trans. Graph.},
  title        = {Capturing animation-ready isotropic materials using systematic poking},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MetaLayer: A meta-learned BSDF model for layered materials.
<em>TOG</em>, <em>42</em>(6), 222:1–15. (<a
href="https://doi.org/10.1145/3618365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reproducing the appearance of arbitrary layered materials has long been a critical challenge in computer graphics, with regard to the demanding requirements of both physical accuracy and low computation cost. Recent studies have demonstrated promising results by learning-based representations that implicitly encode the appearance of complex (layered) materials by neural networks. However, existing generally-learned models often struggle between strong representation ability and high runtime performance, and also lack physical parameters for material editing. To address these concerns, we introduce MetaLayer , a new methodology leveraging meta-learning for modeling and rendering layered materials. MetaLayer contains two networks: a BSDFNet that compactly encodes layered materials into implicit neural representations, and a MetaNet that establishes the mapping between the physical parameters of each material and the weights of its corresponding implicit neural representation. A new positional encoding method and a well-designed training strategy are employed to improve the performance and quality of the neural model. As a new learning-based representation, the proposed MetaLayer model provides both fast responses to material editing and high-quality results for a wide range of layered materials, outperforming existing layered BSDF models.},
  archive      = {J_TOG},
  author       = {Jie Guo and Zeru Li and Xueyan He and Beibei Wang and Wenbin Li and Yanwen Guo and Ling-Qi Yan},
  doi          = {10.1145/3618365},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {222:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {MetaLayer: A meta-learned BSDF model for layered materials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An implicit neural representation for the image stack:
Depth, all in focus, and high dynamic range. <em>TOG</em>,
<em>42</em>(6), 221:1–11. (<a
href="https://doi.org/10.1145/3618367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In everyday photography, physical limitations of camera sensors and lenses frequently lead to a variety of degradations in captured images such as saturation or defocus blur. A common approach to overcome these limitations is to resort to image stack fusion, which involves capturing multiple images with different focal distances or exposures. For instance, to obtain an all-in-focus image, a set of multi-focus images is captured. Similarly, capturing multiple exposures allows for the reconstruction of high dynamic range. In this paper, we present a novel approach that combines neural fields with an expressive camera model to achieve a unified reconstruction of an all-in-focus high-dynamic-range image from an image stack. Our approach is composed of a set of specialized implicit neural representations tailored to address specific sub-problems along our pipeline: We use neural implicits to predict flow to overcome misalignments arising from lens breathing, depth, and all-in-focus images to account for depth of field, as well as tonemapping to deal with sensor responses and saturation - all trained using a physically inspired supervision structure with a differentiable thin lens model at its core. An important benefit of our approach is its ability to handle these tasks simultaneously or independently, providing flexible post-editing capabilities such as refocusing and exposure adjustment. By sampling the three primary factors in photography within our framework (focal distance, aperture, and exposure time), we conduct a thorough exploration to gain valuable insights into their significance and impact on overall reconstruction quality. Through extensive validation, we demonstrate that our method outperforms existing approaches in both depth-from-defocus and all-in-focus image reconstruction tasks. Moreover, our approach exhibits promising results in each of these three dimensions, showcasing its potential to enhance captured image quality and provide greater control in post-processing.},
  archive      = {J_TOG},
  author       = {Chao Wang and Ana Serrano and Xingang Pan and Krzysztof Wolski and Bin Chen and Karol Myszkowski and Hans-Peter Seidel and Christian Theobalt and Thomas Leimkühler},
  doi          = {10.1145/3618367},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {221:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {An implicit neural representation for the image stack: Depth, all in focus, and high dynamic range},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The shortest route is not always the fastest:
Probability-modeled stereoscopic eye movement completion time in VR.
<em>TOG</em>, <em>42</em>(6), 220:1–14. (<a
href="https://doi.org/10.1145/3618334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speed and consistency of target-shifting play a crucial role in human ability to perform complex tasks. Shifting our gaze between objects of interest quickly and consistently requires changes both in depth and direction. Gaze changes in depth are driven by slow, inconsistent vergence movements which rotate the eyes in opposite directions, while changes in direction are driven by ballistic, consistent movements called saccades , which rotate the eyes in the same direction. In the natural world, most of our eye movements are a combination of both types. While scientific consensus on the nature of saccades exists, vergence and combined movements remain less understood and agreed upon. We eschew the lack of scientific consensus in favor of proposing an operationalized computational model which predicts the completion time of any type of gaze movement during target-shifting in 3D. To this end, we conduct a psychophysical study in a stereo VR environment to collect more than 12,000 gaze movement trials, analyze the temporal distribution of the observed gaze movements, and fit a probabilistic model to the data. We perform a series of objective measurements and user studies to validate the model. The results demonstrate its predictive accuracy, generalization, as well as applications for optimizing visual performance by altering content placement. Lastly, we leverage the model to measure differences in human target-changing time relative to the natural world, as well as suggest scene-aware projection depth. By incorporating the complexities and randomness of human oculomotor control, we hope this research will support new behavior-aware metrics for VR/AR display design, interface layout, and gaze-contingent rendering.},
  archive      = {J_TOG},
  author       = {Budmonde Duinkharjav and Benjamin Liang and Anjul Patney and Rachel Brown and Qi Sun},
  doi          = {10.1145/3618334},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {220:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {The shortest route is not always the fastest: Probability-modeled stereoscopic eye movement completion time in VR},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient visualization of light pollution for the night
sky. <em>TOG</em>, <em>42</em>(6), 219:1–11. (<a
href="https://doi.org/10.1145/3618337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial light sources make our daily life convenient, but cause a severe problem called light pollution. We propose a novel system for efficient visualization of light pollution in the night sky. Numerous methods have been proposed for rendering the sky, but most of these focus on rendering of the daytime or the sunset sky where the sun is the only, or dominant light source. For the visualization of the light pollution, however, we must consider many city light sources on the ground, resulting in excessive computational cost. We address this problem by precomputing a set of intensity distributions for the sky illuminated by city light at various locations and with different atmospheric conditions. We apply a principal component analysis and fast Fourier transform to the precomputed distributions, allowing us to efficiently visualize the extent of the light pollution. Using this method, we can achieve one to two orders of magnitudes faster computation compared to a naive approach that simply accumulates the scattered intensity for each viewing ray. Furthermore, the fast computation allows us to interactively solve the inverse problem that determines the city light intensity needed to reduce light pollution. Our system provides the user with both a forward and inverse investigation tool for the study and minimization of light pollution.},
  archive      = {J_TOG},
  author       = {Yoshinori Dobashi and Naoto Ishikawa and Kei Iwasaki},
  doi          = {10.1145/3618337},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {219:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient visualization of light pollution for the night sky},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis and synthesis of digital dyadic sequences.
<em>TOG</em>, <em>42</em>(6), 218:1–17. (<a
href="https://doi.org/10.1145/3618308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the space of matrix-generated (0, m , 2)-nets and (0, 2)-sequences in base 2, also known as digital dyadic nets and sequences. In computer graphics, they are arguably leading the competition for use in rendering. We provide a complete characterization of the design space and count the possible number of constructions with and without considering possible reorderings of the point set. Based on this analysis, we then show that every digital dyadic net can be reordered into a sequence, together with a corresponding algorithm. Finally, we present a novel family of self-similar digital dyadic sequences, to be named ξ -sequences, that spans a subspace with fewer degrees of freedom. Those ξ -sequences are extremely efficient to sample and compute, and we demonstrate their advantages over the classic Sobol (0, 2)-sequence.},
  archive      = {J_TOG},
  author       = {Abdalla G. M. Ahmed and Mikhail Skopenkov and Markus Hadwiger and Peter Wonka},
  doi          = {10.1145/3618308},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {218:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Analysis and synthesis of digital dyadic sequences},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discontinuity-aware 2D neural fields. <em>TOG</em>,
<em>42</em>(6), 217:1–11. (<a
href="https://doi.org/10.1145/3618379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural image representations offer the possibility of high fidelity, compact storage, and resolution-independent accuracy, providing an attractive alternative to traditional pixel- and grid-based representations. However, coordinate neural networks fail to capture discontinuities present in the image and tend to blur across them; we aim to address this challenge. In many cases, such as rendered images, vector graphics, diffusion curves, or solutions to partial differential equations, the locations of the discontinuities are known. We take those locations as input, represented as linear, quadratic, or cubic Bézier curves, and construct a feature field that is discontinuous across these locations and smooth everywhere else. Finally, we use a shallow multi-layer perceptron to decode the features into the signal value. To construct the feature field, we develop a new data structure based on a curved triangular mesh, with features stored on the vertices and on a subset of the edges that are marked as discontinuous. We show that our method can be used to compress a 100, 000 2 -pixel rendered image into a 25MB file; can be used as a new diffusion-curve solver by combining with Monte-Carlo-based methods or directly supervised by the diffusion-curve energy; or can be used for compressing 2D physics simulation data.},
  archive      = {J_TOG},
  author       = {Yash Belhe and Michaël Gharbi and Matthew Fisher and Iliyan Georgiev and Ravi Ramamoorthi and Tzu-Mao Li},
  doi          = {10.1145/3618379},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {217:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Discontinuity-aware 2D neural fields},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReShader: View-dependent highlights for single image
view-synthesis. <em>TOG</em>, <em>42</em>(6), 216:1–9. (<a
href="https://doi.org/10.1145/3618393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, novel view synthesis from a single image has seen significant progress thanks to the rapid advancements in 3D scene representation and image inpainting techniques. While the current approaches are able to synthesize geometrically consistent novel views, they often do not handle the view-dependent effects properly. Specifically, the highlights in their synthesized images usually appear to be glued to the surfaces, making the novel views unrealistic. To address this major problem, we make a key observation that the process of synthesizing novel views requires changing the shading of the pixels based on the novel camera, and moving them to appropriate locations. Therefore, we propose to split the view synthesis process into two independent tasks of pixel reshading and relocation. During the reshading process, we take the single image as the input and adjust its shading based on the novel camera. This reshaded image is then used as the input to an existing view synthesis method to relocate the pixels and produce the final novel view image. We propose to use a neural network to perform reshading and generate a large set of synthetic input-reshaded pairs to train our network. We demonstrate that our approach produces plausible novel view images with realistic moving highlights on a variety of real world scenes.},
  archive      = {J_TOG},
  author       = {Avinash Paliwal and Brandon G. Nguyen and Andrii Tsarov and Nima Khademi Kalantari},
  doi          = {10.1145/3618393},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {216:1–9},
  shortjournal = {ACM Trans. Graph.},
  title        = {ReShader: View-dependent highlights for single image view-synthesis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive fast-multipole-accelerated hybrid boundary
integral equation method for accurate diffusion curves. <em>TOG</em>,
<em>42</em>(6), 215:1–28. (<a
href="https://doi.org/10.1145/3618374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In theory, diffusion curves promise complex color gradations for infinite-resolution vector graphics. In practice, existing realizations suffer from poor scaling, discretization artifacts, or insufficient support for rich boundary conditions. Previous applications of the boundary element method to diffusion curves have relied on polygonal approximations, which either forfeit the high-order smoothness of Bézier curves, or, when the polygonal approximation is extremely detailed, result in large and costly systems of equations that must be solved. In this paper, we utilize the boundary integral equation method to accurately and efficiently solve the underlying partial differential equation. Given a desired resolution and viewport, we then interpolate this solution and use the boundary element method to render it. We couple this hybrid approach with the fast multipole method on a non-uniform quadtree for efficient computation. Furthermore, we introduce an adaptive strategy to enable truly scalable infinite-resolution diffusion curves.},
  archive      = {J_TOG},
  author       = {Seungbae Bang and Kirill Serkh and Oded Stein and Alec Jacobson},
  doi          = {10.1145/3618374},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {215:1–28},
  shortjournal = {ACM Trans. Graph.},
  title        = {An adaptive fast-multipole-accelerated hybrid boundary integral equation method for accurate diffusion curves},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Amortizing samples in physics-based inverse rendering using
ReSTIR. <em>TOG</em>, <em>42</em>(6), 214:1–17. (<a
href="https://doi.org/10.1145/3618331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, great progress has been made in physics-based differentiable rendering. Existing differentiable rendering techniques typically focus on static scenes, but during inverse rendering---a key application for differentiable rendering---the scene is updated dynamically by each gradient step. In this paper, we take a first step to leverage temporal data in the context of inverse direct illumination. By adopting reservoir-based spatiotemporal resampled importance resampling (ReSTIR), we introduce new Monte Carlo estimators for both interior and boundary components of differential direct illumination integrals. We also integrate ReSTIR with antithetic sampling to further improve its effectiveness. At equal frame time, our methods produce gradient estimates with up to 100× lower relative error than baseline methods. Additionally, we propose an inverse-rendering pipeline that incorporates these estimators and provides reconstructions with up to 20× lower error.},
  archive      = {J_TOG},
  author       = {Yu-Chen Wang and Chris Wyman and Lifan Wu and Shuang Zhao},
  doi          = {10.1145/3618331},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {214:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Amortizing samples in physics-based inverse rendering using ReSTIR},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Warped-area reparameterization of differential path
integrals. <em>TOG</em>, <em>42</em>(6), 213:1–18. (<a
href="https://doi.org/10.1145/3618330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-based differentiable rendering is becoming increasingly crucial for tasks in inverse rendering and machine learning pipelines. To address discontinuities caused by geometric boundaries and occlusion, two classes of methods have been proposed: 1) the edge-sampling methods that directly sample light paths at the scene discontinuity boundaries, which require nontrivial data structures and precomputation to select the edges, and 2) the reparameterization methods that avoid discontinuity sampling but are currently limited to hemispherical integrals and unidirectional path tracing. We introduce a new mathematical formulation that enjoys the benefits of both classes of methods. Unlike previous reparameterization work that focused on hemispherical integral, we derive the reparameterization in the path space. As a result, to estimate derivatives using our formulation, we can apply advanced Monte Carlo rendering methods, such as bidirectional path tracing, while avoiding explicit sampling of discontinuity boundaries. We show differentiable rendering and inverse rendering results to demonstrate the effectiveness of our method.},
  archive      = {J_TOG},
  author       = {Peiyu Xu and Sai Bangaru and Tzu-Mao Li and Shuang Zhao},
  doi          = {10.1145/3618330},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {213:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Warped-area reparameterization of differential path integrals},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Projective sampling for differentiable rendering of
geometry. <em>TOG</em>, <em>42</em>(6), 212:1–14. (<a
href="https://doi.org/10.1145/3618385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discontinuous visibility changes at object boundaries remain a persistent source of difficulty in the area of differentiable rendering. Left untreated, they bias computed gradients so severely that even basic optimization tasks fail. Prior path-space methods addressed this bias by decoupling boundaries from the interior, allowing each part to be handled using specialized Monte Carlo sampling strategies. While conceptually powerful, the full potential of this idea remains unrealized since existing methods often fail to adequately sample the boundary proportional to its contribution. This paper presents theoretical and algorithmic contributions. On the theoretical side, we transform the boundary derivative into a remarkably simple local integral that invites present and future developments. Building on this result, we propose a new strategy that projects ordinary samples produced during forward rendering onto nearby boundaries. The resulting projections establish a variance-reducing guiding distribution that accelerates convergence of the subsequent differential phase. We demonstrate the superior efficiency and versatility of our method across a variety of shape representations, including triangle meshes, implicitly defined surfaces, and cylindrical fibers based on Bézier curves.},
  archive      = {J_TOG},
  author       = {Ziyi Zhang and Nicolas Roussel and Wenzel Jakob},
  doi          = {10.1145/3618385},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {212:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Projective sampling for differentiable rendering of geometry},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology guaranteed b-spline surface/surface intersection.
<em>TOG</em>, <em>42</em>(6), 211:1–16. (<a
href="https://doi.org/10.1145/3618349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surface/surface intersection technique serves as one of the most fundamental functions in modern Computer Aided Design (CAD) systems. Despite the long research history and successful applications of surface intersection algorithms in various CAD industrial software, challenges still exist in balancing computational efficiency, accuracy, as well as topology correctness. Specifically, most practical intersection algorithms fail to guarantee the correct topology of the intersection curve(s) when two surfaces are in near-critical positions, which brings instability to CAD systems. Even in one of the most successfully used commercial geometry engines ACIS, such complicated intersection topology can still be a tough nut to crack. In this paper, we present a practical topology guaranteed algorithm for computing the intersection loci of two B-spline surfaces. Our algorithm well treats all types of common and complicated intersection topology with practical efficiency, including those intersections with multiple branches or cross singularities, contacts in several isolated singular points or highorder contacts along a curve, as well as intersections along boundary curves. We present representative examples of these hard topology situations that challenge not only the open-source geometry engine OCCT but also the commercial engine ACIS. We compare our algorithm in both efficiency and topology correctness on plenty of common and complicated models with the open-source intersection package in SISL, OCCT, and the commercial engine ACIS.},
  archive      = {J_TOG},
  author       = {Jieyin Yang and Xiaohong Jia and Dong-Ming Yan},
  doi          = {10.1145/3618349},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {211:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Topology guaranteed B-spline Surface/Surface intersection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). K-surfaces: Bézier-splines interpolating at gaussian
curvature extrema. <em>TOG</em>, <em>42</em>(6), 210:1–13. (<a
href="https://doi.org/10.1145/3618383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {K-surfaces are an interactive modeling technique for Bézier-spline surfaces. Inspired by k -curves by [Yan et al. 2017], each patch provides a single control point that is being interpolated at a local extremum of Gaussian curvature. The challenge is to solve the inverse problem of finding the center control point of a Bézier patch given the boundary control points and the handle. Unlike the situation in 2D, bi-quadratic Bézier patches may exhibit none, one, or several extrema, and finding them is non-trivial. We solve the difficult inverse problem, including the possible selection among several extrema, by learning the desired function from samples, generated by computing Gaussian curvature of random patches. This approximation provides a stable solution to the ill-defined inverse problem and is much more efficient than direct numerical optimization, facilitating the interactive modeling framework. The local solution is used in an iterative optimization incorporating continuity constraints across patches. We demonstrate that the surface varies smoothly with the handle location and that the resulting modeling system provides local and generally intuitive control. The idea of learning the inverse mapping from handles to patches may be applicable to other parametric surfaces.},
  archive      = {J_TOG},
  author       = {Tobias Djuren and Maximilian Kohlbrenner and Marc Alexa},
  doi          = {10.1145/3618383},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {210:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {K-surfaces: Bézier-splines interpolating at gaussian curvature extrema},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An unified λ-subdivision scheme for quadrilateral meshes
with optimal curvature performance in extraordinary regions.
<em>TOG</em>, <em>42</em>(6), 209:1–15. (<a
href="https://doi.org/10.1145/3618400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an unified λ -subdivision scheme with a continuous family of tuned subdivisions for quadrilateral meshes. Main subdivision stencil parameters of the unified scheme are represented as spline functions of the subdominant eigenvalue λ of respective subdivision matrices and the λ value can be selected within a wide range to produce desired properties of refined meshes and limit surfaces with optimal curvature performance in extraordinary regions. Spline representations of stencil parameters are constructed based on discrete optimized stencil coefficients obtained by a general tuning framework that optimizes eigenvectors of subdivision matrices towards curvature continuity conditions. To further improve the quality of limit surfaces, a weighting function is devised to penalize sign changes of Gauss curvatures on respective second order characteristic maps. By selecting an appropriate λ , the resulting unified subdivision scheme produces anticipated properties towards different target applications, including nice properties of several other existing tuned subdivision schemes. Comparison results also validate the advantage of the proposed scheme with higher quality surfaces for subdivision at lower λ values, a challenging task for other related tuned subdivision schemes.},
  archive      = {J_TOG},
  author       = {Weiyin Ma and Xu Wang and Yue Ma},
  doi          = {10.1145/3618400},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {209:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {An unified λ-subdivision scheme for quadrilateral meshes with optimal curvature performance in extraordinary regions},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CamP: Camera preconditioning for neural radiance fields.
<em>TOG</em>, <em>42</em>(6), 208:1–11. (<a
href="https://doi.org/10.1145/3618321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input --- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67\% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29\% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.},
  archive      = {J_TOG},
  author       = {Keunhong Park and Philipp Henzler and Ben Mildenhall and Jonathan T. Barron and Ricardo Martin-Brualla},
  doi          = {10.1145/3618321},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {208:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {CamP: Camera preconditioning for neural radiance fields},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GANeRF: Leveraging discriminators to optimize neural
radiance fields. <em>TOG</em>, <em>42</em>(6), 207:1–14. (<a
href="https://doi.org/10.1145/3618402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) have shown impressive novel view synthesis results; nonetheless, even thorough recordings yield imperfections in reconstructions, for instance due to poorly observed areas or minor lighting changes. Our goal is to mitigate these imperfections from various sources with a joint solution: we take advantage of the ability of generative adversarial networks (GANs) to produce realistic images and use them to enhance realism in 3D scene reconstruction with NeRFs. To this end, we learn the patch distribution of a scene using an adversarial discriminator, which provides feedback to the radiance field reconstruction, thus improving realism in a 3D-consistent fashion. Thereby, rendering artifacts are repaired directly in the underlying 3D representation by imposing multi-view path rendering constraints. In addition, we condition a generator with multi-resolution NeRF renderings which is adversarially trained to further improve rendering quality. We demonstrate that our approach significantly improves rendering quality, e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples.},
  archive      = {J_TOG},
  author       = {Barbara Roessle and Norman Müller and Lorenzo Porzi and Samuel Rota Bulò and Peter Kontschieder and Matthias Niessner},
  doi          = {10.1145/3618402},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {207:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {GANeRF: Leveraging discriminators to optimize neural radiance fields},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural field convolutions by repeated differentiation.
<em>TOG</em>, <em>42</em>(6), 206:1–11. (<a
href="https://doi.org/10.1145/3618340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural fields are evolving towards a general-purpose continuous representation for visual computing. Yet, despite their numerous appealing properties, they are hardly amenable to signal processing. As a remedy, we present a method to perform general continuous convolutions with general continuous signals such as neural fields. Observing that piecewise polynomial kernels reduce to a sparse set of Dirac deltas after repeated differentiation, we leverage convolution identities and train a repeated integral field to efficiently execute large-scale convolutions. We demonstrate our approach on a variety of data modalities and spatially-varying kernels.},
  archive      = {J_TOG},
  author       = {Ntumba Elie Nsampi and Adarsh Djeacoumar and Hans-Peter Seidel and Tobias Ritschel and Thomas Leimkühler},
  doi          = {10.1145/3618340},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {206:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural field convolutions by repeated differentiation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAILOR: Synergizing radiance and occupancy fields for live
human performance capture. <em>TOG</em>, <em>42</em>(6), 205:1–15. (<a
href="https://doi.org/10.1145/3618370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive user experiences in live VR/AR performances require a fast and accurate free-view rendering of the performers. Existing methods are mainly based on Pixel-aligned Implicit Functions (PIFu) or Neural Radiance Fields (NeRF). However, while PIFu-based methods usually fail to produce photorealistic view-dependent textures, NeRF-based methods typically lack local geometry accuracy and are computationally heavy ( e.g. , dense sampling of 3D points, additional fine-tuning, or pose estimation). In this work, we propose a novel generalizable method, named SAILOR, to create high-quality human free-view videos from very sparse RGBD live streams. To produce view-dependent textures while preserving locally accurate geometry, we integrate PIFu and NeRF such that they work synergistically by conditioning the PIFu on depth and then rendering view-dependent textures through NeRF. Specifically, we propose a novel network, named SRONet, for this hybrid representation. SRONet can handle unseen performers without fine-tuning. Besides, a neural blending-based ray interpolation approach, a tree-based voxel-denoising scheme, and a parallel computing pipeline are incorporated to reconstruct and render live free-view videos at 10 fps on average. To evaluate the rendering performance, we construct a real-captured RGBD benchmark from 40 performers. Experimental results show that SAILOR outperforms existing human reconstruction and performance capture methods.},
  archive      = {J_TOG},
  author       = {Zheng Dong and Ke Xu and Yaoan Gao and Qilin Sun and Hujun Bao and Weiwei Xu and Rynson W. H. Lau},
  doi          = {10.1145/3618370},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {205:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {SAILOR: Synergizing radiance and occupancy fields for live human performance capture},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FLARE: Fast learning of animatable and relightable mesh
avatars. <em>TOG</em>, <em>42</em>(6), 204:1–15. (<a
href="https://doi.org/10.1145/3618401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the prefiltered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches.},
  archive      = {J_TOG},
  author       = {Shrisha Bharadwaj and Yufeng Zheng and Otmar Hilliges and Michael J. Black and Victoria Fernandez Abrevaya},
  doi          = {10.1145/3618401},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {204:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {FLARE: Fast learning of animatable and relightable mesh avatars},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multisource holography. <em>TOG</em>, <em>42</em>(6),
203:1–14. (<a href="https://doi.org/10.1145/3618380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holographic displays promise several benefits including high quality 3D imagery, accurate accommodation cues, and compact form-factors. However, holography relies on coherent illumination which can create undesirable speckle noise in the final image. Although smooth phase holograms can be speckle-free, their non-uniform eyebox makes them impractical, and speckle mitigation with partially coherent sources also reduces resolution. Averaging sequential frames for speckle reduction requires high speed modulators and consumes temporal bandwidth that may be needed elsewhere in the system. In this work, we propose multisource holography, a novel architecture that uses an array of sources to suppress speckle in a single frame without sacrificing resolution. By using two spatial light modulators, arranged sequentially, each source in the array can be controlled almost independently to create a version of the target content with different speckle. Speckle is then suppressed when the contributions from the multiple sources are averaged at the image plane. We introduce an algorithm to calculate multisource holograms, analyze the design space, and demonstrate up to a 10 dB increase in peak signal-to-noise ratio compared to an equivalent single source system. Finally, we validate the concept with a benchtop experimental prototype by producing both 2D images and focal stacks with natural defocus cues.},
  archive      = {J_TOG},
  author       = {Grace Kuo and Florian Schiffers and Douglas Lanman and Oliver Cossairt and Nathan Matsuda},
  doi          = {10.1145/3618380},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {203:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Multisource holography},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depolarized holography with polarization-multiplexing
metasurface. <em>TOG</em>, <em>42</em>(6), 202:1–16. (<a
href="https://doi.org/10.1145/3618395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of computer-generated holography (CGH) algorithms has prompted significant improvements in the performances of holographic displays. Nonetheless, they start to encounter a limited degree of freedom in CGH optimization and physical constraints stemming from the coherent nature of holograms. To surpass the physical limitations, we consider polarization as a new degree of freedom by utilizing a novel optical platform called metasurface. Polarization-multiplexing metasurfaces enable incoherent-like behavior in holographic displays due to the mutual incoherence of orthogonal polarization states. We leverage this unique characteristic of a metasurface by integrating it into a holographic display and exploiting polarization diversity to bring an additional degree of freedom for CGH algorithms. To minimize the speckle noise while maximizing the image quality, we devise a fully differentiable optimization pipeline by taking into account the metasurface proxy model, thereby jointly optimizing spatial light modulator phase patterns and geometric parameters of metasurface nanostructures. We evaluate the metasurface-enabled depolarized holography through simulations and experiments, demonstrating its ability to reduce speckle noise and enhance image quality.},
  archive      = {J_TOG},
  author       = {Seung-Woo Nam and Youngjin Kim and Dongyeon Kim and Yoonchan Jeong},
  doi          = {10.1145/3618395},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {202:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Depolarized holography with polarization-multiplexing metasurface},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational design of LEGO® sketch art. <em>TOG</em>,
<em>42</em>(6), 201:1–15. (<a
href="https://doi.org/10.1145/3618306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents computational methods to aid the creation of LEGO ®1 sketch models from simple input images. Beyond conventional LEGO ® mosaics, we aim to improve the expressiveness of LEGO ® models by utilizing LEGO ® tiles with sloping and rounding edges, together with rectangular bricks, to reproduce smooth curves and sharp features in the input. This is a challenging task, as we have limited brick shapes to use and limited space to place bricks. Also, the search space is immense and combinatorial in nature. We approach the task by decoupling the LEGO ® construction into two steps: first approximate the shape with a LEGO ® -buildable contour then filling the contour polygon with LEGO ® bricks. Further, we formulate this contour approximation into a graph optimization with our objective and constraints and effectively solve for the contour polygon that best approximates the input shape. Further, we extend our optimization model to handle multi-color and multi-layer regions, and formulate a grid alignment process and various perceptual constraints to refine the results. We employ our method to create a large variety of LEGO ® models and compare it with humans and baseline methods to manifest its compelling quality and speed.},
  archive      = {J_TOG},
  author       = {Mingjun Zhou and Jiahao Ge and Hao Xu and Chi-Wing Fu},
  doi          = {10.1145/3618306},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {201:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of LEGO® sketch art},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards garment sewing pattern reconstruction from a single
image. <em>TOG</em>, <em>42</em>(6), 200:1–15. (<a
href="https://doi.org/10.1145/3618319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Garment sewing pattern represents the intrinsic rest shape of a garment, and is the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. Code, dataset, and pre-trained models will be released.},
  archive      = {J_TOG},
  author       = {Lijuan Liu and Xiangyu Xu and Zhijie Lin and Jiabin Liang and Shuicheng Yan},
  doi          = {10.1145/3618319},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {200:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Towards garment sewing pattern reconstruction from a single image},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GarmentCode: Programming parametric sewing patterns.
<em>TOG</em>, <em>42</em>(6), 199:1–15. (<a
href="https://doi.org/10.1145/3618351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Garment modeling is an essential task of the global apparel industry and a core part of digital human modeling. Realistic representation of garments with valid sewing patterns is key to their accurate digital simulation and eventual fabrication. However, little-to-no computational tools provide support for bridging the gap between high-level construction goals and low-level editing of pattern geometry, e.g., combining or switching garment elements, semantic editing, or design exploration that maintains the validity of a sewing pattern. We suggest the first DSL for garment modeling - GarmentCode - that applies principles of object-oriented programming to garment construction and allows designing sewing patterns in a hierarchical, component-oriented manner. The programming-based paradigm naturally provides unique advantages of component abstraction, algorithmic manipulation, and free-form design parametrization. We additionally support the construction process by automating typical low-level tasks like placing a dart at a desired location. In our prototype garment configurator, users can manipulate meaningful design parameters and body measurements, while the construction of pattern geometry is handled by garment programs implemented with GarmentCode. Our configurator enables the free exploration of rich design spaces and the creation of garments using interchangeable, parameterized components. We showcase our approach by producing a variety of garment designs and retargeting them to different body shapes using our configurator. The library and garment configurator are available at https://github.com/maria-korosteleva/GarmentCode.},
  archive      = {J_TOG},
  author       = {Maria Korosteleva and Olga Sorkine-Hornung},
  doi          = {10.1145/3618351},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {199:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {GarmentCode: Programming parametric sewing patterns},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Commonsense knowledge-driven joint reasoning approach for
object retrieval in virtual reality. <em>TOG</em>, <em>42</em>(6),
198:1–18. (<a href="https://doi.org/10.1145/3618320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI), China Retrieving out-of-reach objects is a crucial task in virtual reality (VR). One of the most commonly used approaches for this task is the gesture-based approach, which allows for bare-hand, eyes-free, and direct retrieval. However, previous work has primarily focused on assigned gesture design, neglecting the context. This can make it challenging to accurately retrieve an object from a large number of objects due to the one-to-one mapping metaphor, limitations of finger poses, and memory burdens. There is a general consensus that objects and contexts are related, which suggests that the object expected to be retrieved is related to the context, including the scene and the objects with which users interact. As such, we propose a commonsense knowledge-driven joint reasoning approach for object retrieval, where human grasping gestures and context are modeled using an And-Or graph (AOG). This approach enables users to accurately retrieve objects from a large number of candidate objects by using natural grasping gestures based on their experience of grasping physical objects. Experimental results demonstrate that our proposed approach improves retrieval accuracy. We also propose an object retrieval system based on the proposed approach. Two user studies show that our system enables efficient object retrieval in virtual environments (VEs).},
  archive      = {J_TOG},
  author       = {Haiyan Jiang and Dongdong Weng and Xiaonuo Dongye and Le Luo and Zhenliang Zhang},
  doi          = {10.1145/3618320},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {198:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Commonsense knowledge-driven joint reasoning approach for object retrieval in virtual reality},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object motion guided human motion synthesis. <em>TOG</em>,
<em>42</em>(6), 197:1–11. (<a
href="https://doi.org/10.1145/3618333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling human behaviors in contextual environments has a wide range of applications in character animation, embodied AI, VR/AR, and robotics. In real-world scenarios, humans frequently interact with the environment and manipulate various objects to complete daily tasks. In this work, we study the problem of full-body human motion synthesis for the manipulation of large-sized objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a conditional diffusion framework that can generate full-body manipulation behaviors from only the object motion. Since naively applying diffusion models fails to precisely enforce contact constraints between the hands and the object, OMOMO learns two separate denoising processes to first predict hand positions from object motion and subsequently synthesize full-body poses based on the predicted hand positions. By employing the hand positions as an intermediate representation between the two denoising processes, we can explicitly enforce contact constraints, resulting in more physically plausible manipulation motions. With the learned model, we develop a novel system that captures full-body human manipulation motions by simply attaching a smartphone to the object being manipulated. Through extensive experiments, we demonstrate the effectiveness of our proposed pipeline and its ability to generalize to unseen objects. Additionally, as high-quality human-object interaction datasets are scarce, we collect a large-scale dataset consisting of 3D object geometry, object motion, and human motion. Our dataset contains human-object interaction motion for 15 objects, with a total duration of approximately 10 hours.},
  archive      = {J_TOG},
  author       = {Jiaman Li and Jiajun Wu and C. Karen Liu},
  doi          = {10.1145/3618333},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {197:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Object motion guided human motion synthesis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locally-adaptive level-of-detail for hardware-accelerated
ray tracing. <em>TOG</em>, <em>42</em>(6), 196:1–15. (<a
href="https://doi.org/10.1145/3618359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an adaptive level-of-detail technique for ray tracing triangle meshes that aims to reduce the memory bandwidth used during ray traversal, which can be the bottleneck for rendering time with large scenes and the primary consumer of energy. We propose a specific data structure for hierarchically representing triangle meshes, allowing localized decisions for the desired mesh resolution per ray. Starting with the lowest-resolution triangle mesh level, higher-resolution levels are generated by tessellating each triangle into four via splitting its edges with arbitrarily-placed vertices. We fit the resulting mesh hierarchy into a specialized acceleration structure to perform on-the-fly tessellation level selection during ray traversal. Our structure reduces both storage cost and data movement during rendering, which are the main consumers of energy. It also allows continuous transitions between detail levels, while locally adjusting the mesh resolution per ray and preserving watertightness. We present how this structure can be used with both primary and secondary rays for reflections and shadows, which can intersect with different tessellation levels, providing consistent results. We also propose specific hardware units to cover the cost of additional compute needed for level-of-detail operations. We evaluate our method using a cycle-accurate simulation of a custom ray tracing hardware architecture. Our results show that, as compared to traditional bounding volume hierarchies, our method can provide more than an order of magnitude reduction in energy use and render time, given sufficient computational resources.},
  archive      = {J_TOG},
  author       = {Jacob Haydel and Cem Yuksel and Larry Seiler},
  doi          = {10.1145/3618359},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {196:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Locally-adaptive level-of-detail for hardware-accelerated ray tracing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal design of robotic character kinematics.
<em>TOG</em>, <em>42</em>(6), 195:1–15. (<a
href="https://doi.org/10.1145/3618404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The kinematic motion of a robotic character is defined by its mechanical joints and actuators that restrict the relative motion of its rigid components. Designing robots that perform a given target motion as closely as possible with a fixed number of actuated degrees of freedom is challenging, especially for robots that form kinematic loops. In this paper, we propose a technique that simultaneously solves for optimal design and control parameters for a robotic character whose design is parameterized with configurable joints. At the technical core of our technique is an efficient solution strategy that uses dynamic programming to solve for optimal state, control, and design parameters, together with a strategy to remove redundant constraints that commonly exist in general robot assemblies with kinematic loops. We demonstrate the efficacy of our approach by either editing the design of an existing robotic character, or by optimizing the design of a new character to perform a desired motion.},
  archive      = {J_TOG},
  author       = {Guirec Maloisel and Christian Schumacher and Espen Knoop and Ruben Grandia and Moritz Bächer},
  doi          = {10.1145/3618404},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {195:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Optimal design of robotic character kinematics},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ToRoS: A topology optimization approach for designing
robotic skins. <em>TOG</em>, <em>42</em>(6), 194:1–11. (<a
href="https://doi.org/10.1145/3618382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soft robotics offers unique advantages in manipulating fragile or deformable objects, human-robot interaction, and exploring inaccessible terrain. However, designing soft robots that produce large, targeted deformations is challenging. In this paper, we propose a new methodology for designing soft robots that combines optimization-based design with a simple and cost-efficient manufacturing process. Our approach is centered around the concept of robotic skins---thin fabrics with 3D-printed reinforcement patterns that augment and control plain silicone actuators. By decoupling shape control and actuation, our approach enables a simpler and cost-efficient manufacturing process. Unlike previous methods that rely on empirical design heuristics for generating desired deformations, our approach automatically discovers complex reinforcement patterns without any need for domain knowledge or human intervention. This is achieved by casting reinforcement design as a nonlinear constrained optimization problem and using a novel, three-field topology optimization approach tailored to fabrics with 3D-printed reinforcements. We demonstrate the potential of our approach by designing soft robotic actuators capable of various motions such as bending, contraction, twist, and combinations thereof. We also demonstrate applications of our robotic skins to robotic grasping with a soft three-finger gripper and locomotion tasks for a soft quadrupedal robot.},
  archive      = {J_TOG},
  author       = {Juan Montes Maestre and Ronan Hinchet and Stelian Coros and Bernhard Thomaszewski},
  doi          = {10.1145/3618382},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {194:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {ToRoS: A topology optimization approach for designing robotic skins},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-newtonian ViRheometry via similarity analysis.
<em>TOG</em>, <em>42</em>(6), 193:1–16. (<a
href="https://doi.org/10.1145/3618310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We estimate the three Herschel-Bulkley parameters (yield stress σ Y , power-law index n , and consistency parameter η ) for shear-dependent fluid-like materials possibly with large-scale inclusions, for which rheometers may fail to provide a useful measurement. We perform experiments using the unknown material for dam-break (or column collapse) setups and capture video footage. We then use simulations to optimize for the material parameters. For better match up with the simple shear flow encountered in a rheometer, we modify the flow rule for the elasto-viscoplastic Herschel-Bulkley model. Analyzing the loss landscape for optimization, we realize a similarity relation ; material parameters far away within this relation would result in matched simulations, making it hard to distinguish the parameters. We found that by exploiting the setup dependency of the similarity relation, we can improve on the estimation using multiple setups, which we propose by analyzing the Hessian of the similarity relation. We validate the efficacy of our method by comparing the estimations to the measurements from a rheometer (for materials without large-scale inclusions) and show applications to materials with large-scale inclusions, including various salad or pasta sauces, and congee.},
  archive      = {J_TOG},
  author       = {Mitsuki Hamamichi and Kentaro Nagasawa and Masato Okada and Ryohei Seto and Yonghao Yue},
  doi          = {10.1145/3618310},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {193:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Non-newtonian ViRheometry via similarity analysis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GARM-LS: A gradient-augmented reference-map method for
level-set fluid simulation. <em>TOG</em>, <em>42</em>(6), 192:1–20. (<a
href="https://doi.org/10.1145/3618377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TOG},
  author       = {Xingqiao Li and Xingyu Ni and Bo Zhu and Bin Wang and Baoquan Chen},
  doi          = {10.1145/3618377},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {192:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {GARM-LS: A gradient-augmented reference-map method for level-set fluid simulation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High density ratio multi-fluid simulation with peridynamics.
<em>TOG</em>, <em>42</em>(6), 191:1–14. (<a
href="https://doi.org/10.1145/3618347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple fluid simulation has raised wide research interest in recent years. Despite the impressive successes of current works, simulation of scenes containing mixing or unmixing of high-density-ratio phases using particle-based discretizations still remains a challenging task. In this paper, we propose a peridynamic mixture-model theory that stably handles high-density-ratio multi-fluid simulations. With assistance of novel scalar-valued volume flow states, a particle based discretization scheme is proposed to calculate all the terms in the multi-phase Navier-Stokes equations in an integral form, We also design a novel mass updating strategy for enhancing phase mass conservation and reducing particle volume variations under high density ratio settings in multi-fluid simulations. As a result, we achieve significantly stabler simulations in mixture-model multi-fluid simulations involving mixing and unmixing of high density ratio phases. Various experiments and comparisons demonstrate the effectiveness of our approach.},
  archive      = {J_TOG},
  author       = {Han Yan and Bo Ren},
  doi          = {10.1145/3618347},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {191:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {High density ratio multi-fluid simulation with peridynamics},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-order moment-encoded kinetic simulation of turbulent
flows. <em>TOG</em>, <em>42</em>(6), 190:1–13. (<a
href="https://doi.org/10.1145/3618341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kinetic solvers for incompressible fluid simulation were designed to run efficiently on massively parallel architectures such as GPUs. While these lattice Boltzmann solvers have recently proven much faster and more accurate than the macroscopic Navier-Stokes-based solvers traditionally used in graphics, it systematically comes at the price of a very large memory requirement: a mesoscopic discretization of statistical mechanics requires over an order of magnitude more variables per grid node than most fluid solvers in graphics. In order to open up kinetic simulation to gaming and simulation software packages on commodity hardware, we propose a HighOrder Moment-Encoded Lattice-Boltzmann-Method solver which we coined HOME-LBM, requiring only the storage of a few moments per grid node, with little to no loss of accuracy in the typical simulation scenarios encountered in graphics. We show that our lightweight and lightspeed fluid solver requires three times less memory and runs ten times faster than state-of-the-art kinetic solvers, for a nearly-identical visual output.},
  archive      = {J_TOG},
  author       = {Wei Li and Tongtong Wang and Zherong Pan and Xifeng Gao and Kui Wu and Mathieu Desbrun},
  doi          = {10.1145/3618341},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {190:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {High-order moment-encoded kinetic simulation of turbulent flows},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A parametric kinetic solver for simulating
boundary-dominated turbulent flow phenomena. <em>TOG</em>,
<em>42</em>(6), 189:1–20. (<a
href="https://doi.org/10.1145/3618313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boundary layer flow plays a very important role in shaping the entire flow feature near and behind obstacles inside fluids. Thus, boundary treatment methods are crucial for a physically consistent fluid simulation, especially when turbulence occurs at a high Reynolds number, in which accurately handling thin boundary layer becomes quite challenging. Traditional Navier-Stokes solvers usually construct multi-resolution body-fitted meshes to achieve high accuracy, often together with near-wall and sub-grid turbulence modeling. However, this could be time-consuming and computationally intensive even with GPU accelerations. An alternative and much faster approach is to switch to a kinetic solver, such as the lattice Boltzmann model, but boundary treatment has to be done in a cut-cell manner, sacrificing accuracy unless grid resolution is much increased. In this paper, we focus on simulating the boundary-dominated turbulent flow phenomena with an efficient kinetic solver. In order to significantly improve the cut-cell-based boundary treatment for higher accuracy without excessively increasing the simulation resolution, we propose a novel parametric boundary treatment model, including a semi-Lagrangian scheme at the wall for non-equilibrium distribution functions, together with a purely link-based near-wall analytical mesoscopic model by analogy with the macroscopic wall modeling approach, which is yet simple to compute. Such a new method is further extended to handle moving boundaries, showing increased accuracy. Comprehensive analyses are conducted, with a variety of simulation results that are both qualitatively and quantitatively validated with experiments and real life scenarios, and compared to existing methods, to indicate superiority of our method. We highlight that our method not only provides a more accurate way for boundary treatment, but also a valuable tool to control boundary layer behaviors. This has not been achieved and demonstrated before in computer graphics, which we believe will be very useful in practical engineering.},
  archive      = {J_TOG},
  author       = {Mengyun Liu and Xiaopei Liu},
  doi          = {10.1145/3618313},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {189:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {A parametric kinetic solver for simulating boundary-dominated turbulent flow phenomena},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VASCO: Volume and surface co-decomposition for hybrid
manufacturing. <em>TOG</em>, <em>42</em>(6), 188:1–17. (<a
href="https://doi.org/10.1145/3618324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive and subtractive hybrid manufacturing (ASHM) involves the alternating use of additive and subtractive manufacturing techniques, which provides unique advantages for fabricating complex geometries with otherwise inaccessible surfaces. However, a significant challenge lies in ensuring tool accessibility during both fabrication procedures, as the object shape may change dramatically, and different parts of the shape are interdependent. In this study, we propose a computational framework to optimize the planning of additive and subtractive sequences while ensuring tool accessibility. Our goal is to minimize the switching between additive and subtractive processes to achieve efficient fabrication while maintaining product quality. We approach the problem by formulating it as a Volume-And-Surface-CO-decomposition (VASCO) problem. First, we slice volumes into slabs and build a dynamic-directed graph to encode manufacturing constraints, with each node representing a slab and direction reflecting operation order. We introduce a novel geometry property called hybrid-fabricability for a pair of additive and subtractive procedures. Then, we propose a beam-guided top-down block decomposition algorithm to solve the VASCO problem. We apply our solution to a 5-axis hybrid manufacturing platform and evaluate various 3D shapes. Finally, we assess the performance of our approach through both physical and simulated manufacturing evaluations.},
  archive      = {J_TOG},
  author       = {Fanchao Zhong and Haisen Zhao and Haochen Li and Xin Yan and Jikai Liu and Baoquan Chen and Lin Lu},
  doi          = {10.1145/3618324},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {188:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {VASCO: Volume and surface co-decomposition for hybrid manufacturing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shrink &amp; morph: 3D-printed self-shaping shells actuated
by a shape memory effect. <em>TOG</em>, <em>42</em>(6), 187:1–13. (<a
href="https://doi.org/10.1145/3618386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While 3D printing enables the customization and home fabrication of a wide range of shapes, fabricating freeform thin-shells remains challenging. As layers misalign with the curvature, they incur structural deficiencies, while the curved shells require large support structures, typically using more material than the part itself. We present a computational framework for optimizing the internal structure of 3D printed plates such that they morph into a desired freeform shell when heated. This exploits the shrinkage effect of thermoplastics such as PLA, which store internal stresses along the deposition directions. These stresses get released when the material is heated again above its glass transition temperature, causing an anisotropic deformation that induces curvature. Our inverse design method takes as input a freeform surface and finds an optimized set of deposition trajectories in each layer such that their anisotropic shrinkage deforms the plate into the prescribed surface geometry. We optimize for a continuous vector field that varies across the plate and within its thickness. The algorithm then extracts a set of deposition trajectories from the vector field in order to fabricate the flat plates on standard FFF printers. We validate our algorithm on freeform, doubly-curved surfaces.},
  archive      = {J_TOG},
  author       = {David Jourdan and Pierre-Alexandre Hugron and Camille Schreck and Jonàs Martínez and Sylvain Lefebvre},
  doi          = {10.1145/3618386},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {187:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Shrink &amp; morph: 3D-printed self-shaping shells actuated by a shape memory effect},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural metamaterial networks for nonlinear material design.
<em>TOG</em>, <em>42</em>(6), 186:1–13. (<a
href="https://doi.org/10.1145/3618325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear metamaterials with tailored mechanical properties have applications in engineering, medicine, robotics, and beyond. While modeling their macromechanical behavior is challenging in itself, finding structure parameters that lead to ideal approximation of high-level performance goals is a challenging task. In this work, we propose Neural Metamaterial Networks (NMN)---smooth neural representations that encode the nonlinear mechanics of entire metamaterial families. Given structure parameters as input, NMN return continuously differentiable strain energy density functions, thus guaranteeing conservative forces by construction. Though trained on simulation data, NMN do not inherit the discontinuities resulting from topo-logical changes in finite element meshes. They instead provide a smooth map from parameter to performance space that is fully differentiable and thus well-suited for gradient-based optimization. On this basis, we formulate inverse material design as a nonlinear programming problem that leverages neural networks for both objective functions and constraints. We use this approach to automatically design materials with desired strain-stress curves, prescribed directional stiffness and Poisson ratio profiles. We furthermore conduct ablation studies on network nonlinearities and show the advantages of our approach compared to native-scale optimization.},
  archive      = {J_TOG},
  author       = {Yue Li and Stelian Coros and Bernhard Thomaszewski},
  doi          = {10.1145/3618325},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {186:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural metamaterial networks for nonlinear material design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational design of flexible planar microstructures.
<em>TOG</em>, <em>42</em>(6), 185:1–16. (<a
href="https://doi.org/10.1145/3618396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanical metamaterials enable customizing the elastic properties of physical objects by altering their fine-scale structure. A broad gamut of effective material properties can be produced even from a single fabrication material by optimizing the geometry of a periodic microstructure tiling. Past work has extensively studied the capabilities of microstructures in the small-displacement regime, where periodic homogenization of linear elasticity yields computationally efficient optimal design algorithms. However, many applications involve flexible structures undergoing large deformations for which the accuracy of linear elasticity rapidly deteriorates due to geometric nonlinearities. Design of microstructures at finite strains involves a massive increase in computation and is much less explored; no computational tool yet exists to design metamaterials emulating target hyperelastic laws over finite regions of strain space. We make an initial step in this direction, developing algorithms to accelerate homogenization and metamaterial design for nonlinear elasticity and building a complete framework for the optimal design of planar metamaterials. Our nonlinear homogenization method works by efficiently constructing an accurate interpolant of a microstructure&#39;s deformation over a finite space of macroscopic strains likely to be endured by the metamaterial. From this interpolant, the homogenized energy density, stress, and tangent elasticity tensor describing the microstructure&#39;s effective properties can be inexpensively computed at any strain. Our design tool then fits the effective material properties to a target constitutive law over a region of strain space using a parametric shape optimization approach, producing a directly manufacturable geometry. We systematically test our framework by designing a catalog of materials fitting isotropic Hooke&#39;s laws as closely as possible. We demonstrate significantly improved accuracy over traditional linear metamaterial design techniques by fabricating and testing physical prototypes.},
  archive      = {J_TOG},
  author       = {Zhan Zhang and Christopher Brandt and Jean Jouve and Yue Wang and Tian Chen and Mark Pauly and Julian Panetta},
  doi          = {10.1145/3618396},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {185:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational design of flexible planar microstructures},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meshes with spherical faces. <em>TOG</em>, <em>42</em>(6),
184:1–19. (<a href="https://doi.org/10.1145/3618345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete surfaces with spherical faces are interesting from a simplified manufacturing viewpoint when compared to other double curved face shapes. Furthermore, by the nature of their definition they are also appealing from the theoretical side leading to a Möbius invariant discrete surface theory. We therefore systematically describe so called sphere meshes with spherical faces and circular arcs as edges where the Möbius transformation group acts on all of its elements. Driven by aspects important for manufacturing, we provide the means to cluster spherical panels by their radii. We investigate the generation of sphere meshes which allow for a geometric support structure and characterize all such meshes with triangular combinatorics in terms of non-Euclidean geometries. We generate sphere meshes with hexagonal combinatorics by intersecting tangential spheres of a reference surface and let them evolve - guided by the surface curvature - to visually convex hexagons, even in negatively curved areas. Furthermore, we extend meshes with circular faces of all combinatorics to sphere meshes by filling its circles with suitable spherical caps and provide a remeshing scheme to obtain quadrilateral sphere meshes with support structure from given sphere congruences. By broadening polyhedral meshes to sphere meshes we exploit the additional degrees of freedom to minimize intersection angles of neighboring spheres enabling the use of spherical panels that provide a softer perception of the overall surface.},
  archive      = {J_TOG},
  author       = {Martin Kilian and Anthony S Ramos Cisneros and Christian Müller and Helmut Pottmann},
  doi          = {10.1145/3618345},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {184:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Meshes with spherical faces},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developable quad meshes and contact element nets.
<em>TOG</em>, <em>42</em>(6), 183:1–13. (<a
href="https://doi.org/10.1145/3618355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The property of a surface being developable can be expressed in different equivalent ways, by vanishing Gauss curvature, or by the existence of isometric mappings to planar domains. Computational contributions to this topic range from special parametrizations to discrete-isometric mappings. However, so far a local criterion expressing developability of general quad meshes has been lacking. In this paper, we propose a new and efficient discrete developability criterion that is applied to quad meshes equipped with vertex weights, and which is motivated by a well-known characterization in differential geometry, namely a rank-deficient second fundamental form. We assign contact elements to the faces of meshes and ruling vectors to the edges, which in combination yield a developability condition per face. Using standard optimization procedures, we are able to perform interactive design and developable lofting. The meshes we employ are combinatorially regular quad meshes with isolated singularities but are otherwise not required to follow any special curves on a developable surface. They are thus easily embedded into a design workflow involving standard operations like remeshing, trimming, and merging operations. An important feature is that we can directly derive a watertight, rational bi-quadratic spline surface from our meshes. Remarkably, it occurs as the limit of weighted Doo-Sabin subdivision, which acts in an interpolatory manner on contact elements.},
  archive      = {J_TOG},
  author       = {Victor Ceballos Inza and Florian Rist and Johannes Wallner and Helmut Pottmann},
  doi          = {10.1145/3618355},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {183:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Developable quad meshes and contact element nets},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D bézier guarding: Boundary-conforming curved tetrahedral
meshing. <em>TOG</em>, <em>42</em>(6), 182:1–19. (<a
href="https://doi.org/10.1145/3618332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for the generation of higher-order tetrahedral meshes. In contrast to previous methods, the curved tetrahedral elements are guaranteed to be free of degeneracies and inversions while conforming exactly to prescribed piecewise polynomial surfaces, such as domain boundaries or material interfaces. Arbitrary polynomial order is supported. Algorithmically, the polynomial input surfaces are first covered by a single layer of carefully constructed curved elements using a recursive refinement procedure that provably avoids degeneracies and inversions. These tetrahedral elements are designed such that the remaining space is bounded piecewise linearly. In this way, our method effectively reduces the curved meshing problem to the classical problem of linear mesh generation (for the remaining space).},
  archive      = {J_TOG},
  author       = {Payam Khanteimouri and Marcel Campen},
  doi          = {10.1145/3618332},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {182:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {3D bézier guarding: Boundary-conforming curved tetrahedral meshing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constrained delaunay tetrahedrization: A robust and
practical approach. <em>TOG</em>, <em>42</em>(6), 181:1–15. (<a
href="https://doi.org/10.1145/3618352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a numerically robust algorithm for computing the constrained Delaunay tetrahedrization (CDT) of a piecewise-linear complex, which has a 100\% success rate on the 4408 valid models in the Thingi10k dataset. We build on the underlying theory of the well-known tetgen software, but use a floating-point implementation based on indirect geometric predicates to implicitly represent Steiner points: this new approach dramatically simplifies the implementation, removing the need for ad-hoc tolerances in geometric operations. Our approach leads to a robust and parameter-free implementation, with an empirically manageable number of added Steiner points. Furthermore, our algorithm addresses a major gap in tetgen&#39;s theory which may lead to algorithmic failure on valid models, even when assuming perfect precision in the calculations. Our output tetrahedrization conforms with the input geometry without approximations. We can further round our output to floating-point coordinates for downstream applications, which almost always results in valid floating-point meshes unless the input triangulation is very close to being degenerate.},
  archive      = {J_TOG},
  author       = {Lorenzo Diazzi and Daniele Panozzo and Amir Vaxman and Marco Attene},
  doi          = {10.1145/3618352},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {181:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Constrained delaunay tetrahedrization: A robust and practical approach},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collapsing embedded cell complexes for safer hexahedral
meshing. <em>TOG</em>, <em>42</em>(6), 180:1–24. (<a
href="https://doi.org/10.1145/3618384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a set of operators to perform modifications, in particular collapses and splits, in volumetric cell complexes which are discretely embedded in a background mesh. Topological integrity and geometric embedding validity are carefully maintained. We apply these operators strategically to volumetric block decompositions, so-called T-meshes or base complexes, in the context of hexahedral mesh generation. This allows circumventing the expensive and unreliable global volumetric remapping step in the versatile meshing pipeline based on 3D integer-grid maps. In essence, we reduce this step to simpler local cube mapping problems, for which reliable solutions are available. As a consequence, the robustness of the mesh generation process is increased, especially when targeting coarse or block-structured hexahedral meshes. We furthermore extend this pipeline to support feature alignment constraints, and systematically respect these throughout, enabling the generation of meshes that align to points, curves, and surfaces of special interest, whether on the boundary or in the interior of the domain.},
  archive      = {J_TOG},
  author       = {Hendrik Brückler and Marcel Campen},
  doi          = {10.1145/3618384},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {180:1–24},
  shortjournal = {ACM Trans. Graph.},
  title        = {Collapsing embedded cell complexes for safer hexahedral meshing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DiffFR: Differentiable SPH-based fluid-rigid coupling for
rigid body control. <em>TOG</em>, <em>42</em>(6), 179:1–17. (<a
href="https://doi.org/10.1145/3618318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable physics simulation has shown its efficacy in inverse design problems. Given the pervasiveness of the diverse interactions between fluids and solids in life, a differentiable simulator for the inverse design of the motion of rigid objects in two-way fluid-rigid coupling is also demanded. There are two main challenges to develop a differentiable two-way fluid-solid coupling simulator for rigid body control tasks: the ubiquitous, discontinuous contacts in fluid-solid interactions, and the high computational cost of gradient formulation due to the large number of degrees of freedom (DoF) of fluid dynamics. In this work, we propose a novel differentiable SPH-based two-way fluid-rigid coupling simulator to address these challenges. Our purpose is to provide a differentiable simulator for SPH which incorporates a unified representation for both fluids and solids using particles. However, naively differentiating the forward simulation of the particle system encounters gradient explosion issues. We investigate the instability in differentiating the SPH-based fluid-rigid coupling simulator and present a feasible gradient computation scheme to address its differentiability. In addition, we also propose an efficient method to compute the gradient of fluid-rigid coupling without incurring the high computational cost of differentiating the entire high-DoF fluid system. We show the efficacy, scalability, and extensibility of our method in various challenging rigid body control tasks with diverse fluid-rigid interactions and multi-rigid contacts, achieving up to an order of magnitude speedup in optimization compared to baseline methods in experiments.},
  archive      = {J_TOG},
  author       = {Zhehao Li and Qingyu Xu and Xiaohan Ye and Bo Ren and Ligang Liu},
  doi          = {10.1145/3618318},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {179:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {DiffFR: Differentiable SPH-based fluid-rigid coupling for rigid body control},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural categorical priors for physics-based character
control. <em>TOG</em>, <em>42</em>(6), 178:1–16. (<a
href="https://doi.org/10.1145/3618397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with improved motion quality and diversity over existing methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder&#39;s output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.},
  archive      = {J_TOG},
  author       = {Qingxu Zhu and He Zhang and Mengting Lan and Lei Han},
  doi          = {10.1145/3618397},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {178:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural categorical priors for physics-based character control},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdaptNet: Policy adaptation for physics-based character
control. <em>TOG</em>, <em>42</em>(6), 177:1–17. (<a
href="https://doi.org/10.1145/3618375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by humans&#39; ability to adapt skills in the learning of new ones, this paper presents AdaptNet, an approach for modifying the latent space of existing policies to allow new behaviors to be quickly learned from like tasks in comparison to learning from scratch. Building on top of a given reinforcement learning controller, AdaptNet uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network layers to make more substantive changes. The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies. Code is available at https://motion-lab.github.io/AdaptNet .},
  archive      = {J_TOG},
  author       = {Pei Xu and Kaixiang Xie and Sheldon Andrews and Paul G. Kry and Michael Neff and Morgan Mcguire and Ioannis Karamouzas and Victor Zordan},
  doi          = {10.1145/3618375},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {177:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {AdaptNet: Policy adaptation for physics-based character control},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive shell qasistatics for unstructured meshes.
<em>TOG</em>, <em>42</em>(6), 176:1–17. (<a
href="https://doi.org/10.1145/3618388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thin shell structures exhibit complex behaviors critical for modeling and design across wide-ranging applications. Capturing their mechanical response requires finely detailed, high-resolution meshes. Corresponding simulations for predicting equilibria with these meshes are expensive, whereas coarse-mesh simulations can be fast but generate unacceptable artifacts and inaccuracies. The recently proposed progressive simulation framework [Zhang et al. 2022] offers a promising avenue to address these limitations with consistent and progressively improving simulation over a hierarchy of increasingly higher-resolution models. Unfortunately, it is currently severely limited in application to meshes and shapes generated via Loop subdivision. We propose Progressive Shells Quasistatics to extend progressive simulation to the high-fidelity modeling and design of all input shell (and plate) geometries with unstructured (as well as structured) triangle meshes. To do so, we construct a fine-to-coarse hierarchy with a novel nonlinear prolongation operator custom-suited for curved-surface simulation that is rest-shape preserving, supports complex curved boundaries, and enables the reconstruction of detailed geometries from coarse-level meshes. Then, to enable convergent, high-quality solutions with robust contact handling, we propose a new, safe, and efficient shape-preserving upsampling method that ensures non-intersection and strain limits during refinement. With these core contributions, Progressive Shell Quasistatics enables, for the first time, wide generality for progressive simulation, including support for arbitrary curved-shell geometries, progressive collision objects, curved boundaries, and unstructured triangle meshes - all while ensuring that preview and final solutions remain free of intersections. We demonstrate these features across a wide range of stress-tests where progressive simulation captures the wrinkling, folding, twisting, and buckling behaviors of frictionally contacting thin shells with orders-of-magnitude speed-up in examples over direct fine-resolution simulation.},
  archive      = {J_TOG},
  author       = {Jiayi Eris Zhang and Jérémie Dumas and Yun (Raymond) Fei and Alec Jacobson and Doug L. James and Danny M. Kaufman},
  doi          = {10.1145/3618388},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {176:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Progressive shell qasistatics for unstructured meshes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable discrete bending by analytic eigensystem and adaptive
orthotropic geometric stiffness. <em>TOG</em>, <em>42</em>(6), 175:1–16.
(<a href="https://doi.org/10.1145/3618372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address two limitations of dihedral angle based discrete bending (DAB) models, i.e. the indefiniteness of their energy Hessian and their vulnerability to geometry degeneracies. To tackle the indefiniteness issue, we present novel analytic expressions for the eigensystem of a DAB energy Hessian. Our expressions reveal that DAB models typically have positive, negative, and zero eigenvalues, with four of each, respectively. By using these expressions, we can efficiently project an indefinite DAB energy Hessian as positive semi-definite analytically. To enhance the stability of DAB models at degenerate geometries, we propose rectifying their indefinite geometric stiffness matrix by using orthotropic geometric stiffness matrices with adaptive parameters calculated from our analytic eigensystem. Among the twelve motion modes of a dihedral element, our resulting Hessian for DAB models retains only the desirable bending modes, compared to the undesirable altitude-changing modes of the exact Hessian with original geometric stiffness, all modes of the Gauss-Newton approximation without geometric stiffness, and no modes of the projected Hessians with inappropriate geometric stiffness. Additionally, we suggest adjusting the compression stiffness according to the Kirchhoff-Love thin plate theory to avoid over-compression. Our method not only ensures the positive semidefiniteness but also avoids instability caused by large bending forces at degenerate geometries. To demonstrate the benefit of our approaches, we show comparisons against existing methods on the simulation of cloth and thin plates in challenging examples.},
  archive      = {J_TOG},
  author       = {Zhendong Wang and Yin Yang and Huamin Wang},
  doi          = {10.1145/3618372},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {175:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Stable discrete bending by analytic eigensystem and adaptive orthotropic geometric stiffness},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kirchhoff-love shells with arbitrary hyperelastic materials.
<em>TOG</em>, <em>42</em>(6), 174:1–15. (<a
href="https://doi.org/10.1145/3618405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kirchhoff-Love shells are commonly used in many branches of engineering, including in computer graphics, but have so far been simulated only under limited nonlinear material options. We derive the Kirchhoff-Love thin-shell mechanical energy for an arbitrary 3D volumetric hyperelastic material, including isotropic materials, anisotropic materials, and materials whereby the energy includes both even and odd powers of the principal stretches. We do this by starting with any 3D hyperelastic material, and then analytically computing the corresponding thin-shell energy limit. This explicitly identifies and separates in-plane stretching and bending terms, and avoids numerical quadrature. Thus, in-plane stretching and bending are shown to originate from one and the same process (volumetric elasticity of thin objects), as opposed to from two separate processes as done traditionally in cloth simulation. Because we can simulate materials that include both even and odd powers of stretches, we can accommodate standard mesh distortion energies previously employed for 3D solid simulations, such as Symmetric ARAP and Co-rotational materials. We relate the terms of our energy to those of prior work on Kirchhoff-Love thin-shells in computer graphics that assumed small in-plane stretches, and demonstrate the visual difference due to the presence of our exact stretching and bending terms. Furthermore, our formulation allows us to categorize all distinct hyperelastic Kirchhoff-Love thin-shell energies. Specifically, we prove that for Kirchhoff-Love thin-shells, the space of all hyperelastic materials collapses to two-dimensional hyperelastic materials. This observation enables us to create an interface for the design of thin-shell Kirchhoff-Love mechanical energies, which in turn enables us to create thin-shell materials that exhibit arbitrary stiffness profiles under large deformations.},
  archive      = {J_TOG},
  author       = {Jiahao Wen and Jernej Barbič},
  doi          = {10.1145/3618405},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {174:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Kirchhoff-love shells with arbitrary hyperelastic materials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). C-shells: Deployable gridshells with curved beams.
<em>TOG</em>, <em>42</em>(6), 173:1–17. (<a
href="https://doi.org/10.1145/3618366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a computational pipeline for simulating and designing C-shells , a new class of planar-to-spatial deployable linkage structures. A C-shell is composed of curved flexible beams connected at rotational joints that can be assembled in a stress-free planar configuration. When actuated, the elastic beams deform and the assembly deploys towards the target 3D shape. We propose two alternative computational design approaches for C-shells: (i) Forward exploration simulates the deployed shape from a planar beam layout provided by the user. Once a satisfactory overall shape is found, a subsequent design optimization adapts the beam geometry to reduce the elastic energy of the linkage while preserving the target shape. (ii) Inverse design is facilitated by a new geometric flattening method that takes a design surface as input and computes an initial layout of piecewise straight linkage beams. Our design optimization algorithm then calculates the smooth curved beams to best reproduce the target shape at minimal elastic energy. We find that C-shells offer a rich space for design and show several studies that highlight new shape topologies that cannot be achieved with existing deployable linkage structures.},
  archive      = {J_TOG},
  author       = {Quentin Becker and Seiichi Suzuki and Yingying Ren and Davide Pellis and Julian Panetta and Mark Pauly},
  doi          = {10.1145/3618366},
  journal      = {ACM Transactions on Graphics},
  month        = {12},
  number       = {6},
  pages        = {173:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {C-shells: Deployable gridshells with curved beams},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The design space of kirchhoff rods. <em>TOG</em>,
<em>42</em>(5), 171:1–20. (<a
href="https://doi.org/10.1145/3606033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kirchhoff rod model describes the bending and twisting of slender elastic rods in three dimensions and has been widely studied to enable the prediction of how a rod will deform, given its geometry and boundary conditions. In this work, we study a number of inverse problems with the goal of computing the geometry of a straight rod that will automatically deform to match a curved target shape after attaching its endpoints to a support structure. Our solution lets us finely control the static equilibrium state of a rod by varying the cross-sectional profiles along its length. We also show that the set of physically realizable equilibrium states admits a concise geometric description in terms of linear line complexes, which leads to very efficient computational design algorithms. Implemented in an interactive software tool, they allow us to convert three-dimensional hand-drawn spline curves to elastic rods and give feedback about the feasibility and practicality of a design in real time. We demonstrate the efficacy of our method by designing and manufacturing several physical prototypes with applications to interior design and soft robotics.},
  archive      = {J_TOG},
  author       = {Christian Hafner and Bernd Bickel},
  doi          = {10.1145/3606033},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {171:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {The design space of kirchhoff rods},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The method of moving frames for surface global
parametrization. <em>TOG</em>, <em>42</em>(5), 166:1–18. (<a
href="https://doi.org/10.1145/3604282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new representation of surface global parametrization based on Cartan’s method of moving frames . We show that a system of structure equations , characterizing the local coordinates changes with respect to a local frame system, completely characterizes the set of possible cone parametrizations. The discretization of this system provably provides necessary and sufficient conditions for the existence of a valid mapping. We are able to derive a versatile algorithm for surface parametrization, allowing feature constraints and singularities. As the first structure equation is independent of the global coordinate system, we do not require prior knowledge of cuts or cone positions. So, a single non-linear least-square problem is enough to place quantized cones while minimizing a given distortion energy. We are therefore able to take full advantage of the link between the parametrization geometry and the topology of its cone metric to solve challenging constrained parametrization problems.},
  archive      = {J_TOG},
  author       = {Guillaume Coiffier and Etienne Corman},
  doi          = {10.1145/3604282},
  journal      = {ACM Transactions on Graphics},
  month        = {9},
  number       = {5},
  pages        = {166:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {The method of moving frames for surface global parametrization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLIP-guided StyleGAN inversion for text-driven real image
editing. <em>TOG</em>, <em>42</em>(5), 172:1–18. (<a
href="https://doi.org/10.1145/3610287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have recently begun exploring the use of StyleGAN-based models for real image editing. One particularly interesting application is using natural language descriptions to guide the editing process. Existing approaches for editing images using language either resort to instance-level latent code optimization or map predefined text prompts to some editing directions in the latent space. However, these approaches have inherent limitations. The former is not very efficient, while the latter often struggles to effectively handle multi-attribute changes. To address these weaknesses, we present CLIPInverter, a new text-driven image editing approach that is able to efficiently and reliably perform multi-attribute changes. The core of our method is the use of novel, lightweight text-conditioned adapter layers integrated into pretrained GAN-inversion networks. We demonstrate that by conditioning the initial inversion step on the Contrastive Language-Image Pre-training (CLIP) embedding of the target description, we are able to obtain more successful edit directions. Additionally, we use a CLIP-guided refinement step to make corrections in the resulting residual latent codes, which further improves the alignment with the text prompt. Our method outperforms competing approaches in terms of manipulation accuracy and photo-realism on various domains including human faces, cats, and birds, as shown by our qualitative and quantitative results.},
  archive      = {J_TOG},
  author       = {Ahmet Canberk Baykal and Abdul Basit Anees and Duygu Ceylan and Erkut Erdem and Aykut Erdem and Deniz Yuret},
  doi          = {10.1145/3610287},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {172:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {CLIP-guided StyleGAN inversion for text-driven real image editing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-resolution volumetric reconstruction for clothed
humans. <em>TOG</em>, <em>42</em>(5), 170:1–15. (<a
href="https://doi.org/10.1145/3606032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method for reconstructing clothed humans from a sparse set of, e.g., 1–6 RGB images. Despite impressive results from recent works employing deep implicit representation, we revisit the volumetric approach and demonstrate that better performance can be achieved with proper system design. The volumetric representation offers significant advantages in leveraging 3D spatial context through 3D convolutions, and the notorious quantization error is largely negligible with a reasonably large yet affordable volume resolution, e.g., 512. To handle memory and computation costs, we propose a sophisticated coarse-to-fine strategy with voxel culling and subspace sparse convolution. Our method starts with a discretized visual hull to compute a coarse shape and then focuses on a narrow band nearby the coarse shape for refinement. Once the shape is reconstructed, we adopt an image-based rendering approach, which computes the colors of surface points by blending input images with learned weights. Extensive experimental results show that our method significantly reduces the mean point-to-surface (P2S) precision of state-of-the-art methods by more than 50\% to achieve approximately 2mm accuracy with a 512 volume resolution. Additionally, images rendered from our textured model achieve a higher peak signal-to-noise ratio (PSNR) compared to state-of-the-art methods.},
  archive      = {J_TOG},
  author       = {Sicong Tang and Guangyuan Wang and Qing Ran and Lingzhi Li and Li Shen and Ping Tan},
  doi          = {10.1145/3606032},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {170:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {High-resolution volumetric reconstruction for clothed humans},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seamless parametrization with cone and partial loop control.
<em>TOG</em>, <em>42</em>(5), 164:1–22. (<a
href="https://doi.org/10.1145/3600087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for constructing seamless parametrization for surfaces of any genus that can handle any feasible cone configuration with any type of cones. The mapping is guaranteed to be locally injective, which is due to careful construction of a simple domain boundary polygon. The polygon’s complexity depends on the cones in the field, and it is independent of mesh geometry. The result is a small polygon that can be optimized prior to the interior mapping, which contributes to the robustness of the pipeline. For a surface of genus &gt;0, non-contractible loops play an important role, and their holonomies significantly affect mapping quality. We enable holonomy prescription, where local injectivity is guaranteed. Our prescription, however, is limited and cannot handle all feasible holonomies due to monotonicity constraints that keep our polygon simple. Yet this work is an important step toward fully solving the holonomy prescription problem.},
  archive      = {J_TOG},
  author       = {Zohar Levi},
  doi          = {10.1145/3600087},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {164:1–22},
  shortjournal = {ACM Trans. Graph.},
  title        = {Seamless parametrization with cone and partial loop control},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeLT: Object-oriented neural light transfer. <em>TOG</em>,
<em>42</em>(5), 163:1–16. (<a
href="https://doi.org/10.1145/3596491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents object-oriented neural light transfer (NeLT), a novel neural representation of the dynamic light transportation between an object and the environment. Our method disentangles the global illumination of a scene into individual objects’ light transportation represented via neural networks, then composes them explicitly. It therefore enables flexible rendering with dynamic lighting, cameras, materials, and objects. Our rendering features various important global illumination effects, such as diffuse illumination, glossy illumination, dynamic shadowing, and indirect illumination, which completes the capability of existing neural object representation. Experiments show that NeLT does not require path tracing or shading results as input but achieves rendering quality comparable to state-of-the-art rendering frameworks, including the recent deep learning based denoisers.},
  archive      = {J_TOG},
  author       = {Chuankun Zheng and Yuchi Huo and Shaohua Mo and Zhihua Zhong and Zhizhen Wu and Wei Hua and Rui Wang and Hujun Bao},
  doi          = {10.1145/3596491},
  journal      = {ACM Transactions on Graphics},
  month        = {8},
  number       = {5},
  pages        = {163:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeLT: Object-oriented neural light transfer},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified arbitrary style transfer framework via adaptive
contrastive learning. <em>TOG</em>, <em>42</em>(5), 169:1–16. (<a
href="https://doi.org/10.1145/3605548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents Unified Contrastive Arbitrary Style Transfer (UCAST), a novel style representation learning and transfer framework, that can fit in most existing arbitrary image style transfer models, such as CNN-based, ViT-based, and flow-based methods. As the key component in image style transfer tasks, a suitable style representation is essential to achieve satisfactory results. Existing approaches based on deep neural networks typically use second-order statistics to generate the output. However, these hand-crafted features computed from a single image cannot leverage style information sufficiently, which leads to artifacts such as local distortions and style inconsistency. To address these issues, we learn style representation directly from a large number of images based on contrastive learning by considering the relationships between specific styles and the holistic style distribution. Specifically, we present an adaptive contrastive learning scheme for style transfer by introducing an input-dependent temperature. Our framework consists of three key components: a parallel contrastive learning scheme for style representation and transfer, a domain enhancement (DE) module for effective learning of style distribution, and a generative network for style transfer. Qualitative and quantitative evaluations show the results of our approach are superior to those obtained via state-of-the-art methods. The code is available at https://github.com/zyxElsa/CAST_pytorch .},
  archive      = {J_TOG},
  author       = {Yuxin Zhang and Fan Tang and Weiming Dong and Haibin Huang and Chongyang Ma and Tong-Yee Lee and Changsheng Xu},
  doi          = {10.1145/3605548},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {5},
  pages        = {169:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A unified arbitrary style transfer framework via adaptive contrastive learning},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Procedural metamaterials: A unified procedural graph for
metamaterial design. <em>TOG</em>, <em>42</em>(5), 168:1–19. (<a
href="https://doi.org/10.1145/3605389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a compact, intuitive procedural graph representation for cellular metamaterials, which are small-scale, tileable structures that can be architected to exhibit many useful material properties. Because the structures’ “architectures” vary widely—with elements such as beams, thin shells, and solid bulks—it is difficult to explore them using existing representations. Generic approaches like voxel grids are versatile, but it is cumbersome to represent and edit individual structures; architecture-specific approaches address these issues, but are incompatible with one another. By contrast, our procedural graph succinctly represents the construction process for any structure using a simple skeleton annotated with spatially varying thickness. To express the highly constrained triply periodic minimal surfaces (TPMS) in this manner, we present the first fully automated version of the conjugate surface construction method, which allows novices to create complex TPMS from intuitive input. We demonstrate our representation’s expressiveness, accuracy, and compactness by constructing a wide range of established structures and hundreds of novel structures with diverse architectures and material properties. We also conduct a user study to verify our representation’s ease-of-use and ability to expand engineers’ capacity for exploration.},
  archive      = {J_TOG},
  author       = {Liane Makatura and Bohan Wang and Yi-Lu Chen and Bolei Deng and Chris Wojtan and Bernd Bickel and Wojciech Matusik},
  doi          = {10.1145/3605389},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {5},
  pages        = {168:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Procedural metamaterials: A unified procedural graph for metamaterial design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast GPU-based two-way continuous collision handling.
<em>TOG</em>, <em>42</em>(5), 167:1–15. (<a
href="https://doi.org/10.1145/3604551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Step-and-project is a popular method to simulate non-penetrating deformable bodies in physically based animation. The strategy is to first integrate the system in time without considering contacts and then resolve potential intersections, striking a good balance between plausibility and efficiency. However, existing methods can be defective and unsafe when using large time steps, taking risks of failure or demanding repetitive collision testing and resolving that severely degrade performance. In this article, we propose a novel two-way method for fast and reliable continuous collision handling. Our method launches an optimization from both ends of the intermediate time-integrated state and the previous intersection-free state. It progressively generates a piecewise linear path and eventually obtains a feasible solution for the next time step. The algorithm efficiently alternates between a forward step and a backward step until the result is conditionally converged. Thanks to a set of unified volume-based contact constraints, our method offers flexible and reliable handling of various codimensional deformable bodies, including volumetric bodies, cloth, hair, and sand. Experimental results demonstrate the safety, robustness, physical fidelity, and numerical efficiency of our method, making it particularly suitable for scenarios involving large deformations or large time steps.},
  archive      = {J_TOG},
  author       = {Tianyu Wang and Jiong Chen and Dongping Li and Xiaowei Liu and Huamin Wang and Kun Zhou},
  doi          = {10.1145/3604551},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {5},
  pages        = {167:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast GPU-based two-way continuous collision handling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning physically realizable skills for online packing of
general 3D shapes. <em>TOG</em>, <em>42</em>(5), 165:1–21. (<a
href="https://doi.org/10.1145/3603544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of learning online packing skills for irregular 3D shapes , which is arguably the most challenging setting of bin packing problems. The goal is to consecutively move a sequence of 3D objects with arbitrary shapes into a designated container with only partial observations of the object sequence. We take physical realizability into account, involving physics dynamics and constraints of a placement. The packing policy should understand the 3D geometry of the object to be packed and make effective decisions to accommodate it in the container in a physically realizable way. We propose a Reinforcement Learning (RL) pipeline to learn the policy. The complex irregular geometry and imperfect object placement together lead to huge solution space. Direct training in such space is prohibitively data intensive. We instead propose a theoretically provable method for candidate action generation to reduce the action space of RL and the learning burden. A parameterized policy is then learned to select the best placement from the candidates. Equipped with an efficient method of asynchronous RL acceleration and a data preparation process of simulation-ready training sequences, a mature packing policy can be trained in a physics-based environment within 48 hours. Through extensive evaluation on a variety of real-life shape datasets and comparisons with state-of-the-art baselines, we demonstrate that our method outperforms the best-performing baseline on all datasets by at least 12.8\% in terms of packing utility. We also release our datasets and source code to support further research in this direction. 1},
  archive      = {J_TOG},
  author       = {Hang Zhao and Zherong Pan and Yang Yu and Kai Xu},
  doi          = {10.1145/3603544},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {5},
  pages        = {165:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning physically realizable skills for online packing of general 3D shapes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale terrain authoring through interactive erosion
simulation. <em>TOG</em>, <em>42</em>(5), 162:1–15. (<a
href="https://doi.org/10.1145/3592787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale terrains are essential in the definition of virtual worlds. Given the diversity of landforms and the geomorphological complexity, there is a need for authoring techniques offering hydrological consistency without sacrificing user control. In this article, we bridge the gap between large-scale erosion simulation and authoring into an efficient framework. We set aside modeling in the elevation domain in favour of the uplift domain and compute emerging reliefs by simulating the stream power erosion. Our simulation relies on a fast yet accurate approximation of drainage area and flow routing to compute the erosion interactively, which allows for incremental authoring. Our model provides landscape artists with tools for shaping mountain ranges and valleys, such as copy-and-paste operations; warping for imitating folds and faults; and point and curve elevation constraints to precisely sculpt ridges or carve river networks. It also lends itself to inverse procedural modeling by reconstructing the uplift from an input digital elevation model and allows hydrologically consistent blending between terrain patches.},
  archive      = {J_TOG},
  author       = {Hugo Schott and Axel Paris and Lucie Fournier and Eric Guérin and Eric Galin},
  doi          = {10.1145/3592787},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {5},
  pages        = {162:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Large-scale terrain authoring through interactive erosion simulation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeRSemble: Multi-view radiance field reconstruction of human
heads. <em>TOG</em>, <em>42</em>(4), 161:1–14. (<a
href="https://doi.org/10.1145/3592455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on reconstructing high-fidelity radiance fields of human heads, capturing their animations over time, and synthesizing re-renderings from novel viewpoints at arbitrary time steps. To this end, we propose a new multi-view capture setup composed of 16 calibrated machine vision cameras that record time-synchronized images at 7.1 MP resolution and 73 frames per second. With our setup, we collect a new dataset of over 4700 high-resolution, high-framerate sequences of more than 220 human heads, from which we introduce a new human head reconstruction benchmark 1 . The recorded sequences cover a wide range of facial dynamics, including head motions, natural expressions, emotions, and spoken language. In order to reconstruct high-fidelity human heads, we propose Dynamic Neural Radiance Fields using Hash Ensembles (NeRSemble). We represent scene dynamics by combining a deformation field and an ensemble of 3D multi-resolution hash encodings. The deformation field allows for precise modeling of simple scene movements, while the ensemble of hash encodings helps to represent complex dynamics. As a result, we obtain radiance field representations of human heads that capture motion over time and facilitate re-rendering of arbitrary novel viewpoints. In a series of experiments, we explore the design choices of our method and demonstrate that our approach outperforms state-of-the-art dynamic radiance field approaches by a significant margin.},
  archive      = {J_TOG},
  author       = {Tobias Kirschstein and Shenhan Qian and Simon Giebenhain and Tim Walter and Matthias Nießner},
  doi          = {10.1145/3592455},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {161:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeRSemble: Multi-view radiance field reconstruction of human heads},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HumanRF: High-fidelity neural radiance fields for humans in
motion. <em>TOG</em>, <em>42</em>(4), 160:1–12. (<a
href="https://doi.org/10.1145/3592415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF 1 , a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions 2 . We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.},
  archive      = {J_TOG},
  author       = {Mustafa Işık and Martin Rünz and Markos Georgopoulos and Taras Khakhulin and Jonathan Starck and Lourdes Agapito and Matthias Nießner},
  doi          = {10.1145/3592415},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {160:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {HumanRF: High-fidelity neural radiance fields for humans in motion},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SketchFaceNeRF: Sketch-based facial generation and editing
in neural radiance fields. <em>TOG</em>, <em>42</em>(4), 159:1–17. (<a
href="https://doi.org/10.1145/3592100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic 3D facial generation based on Neural Radiance Fields (NeRFs) from 2D sketches benefits various applications. Despite the high realism of free-view rendering results of NeRFs, it is tedious and difficult for artists to achieve detailed 3D control and manipulation. Meanwhile, due to its conciseness and expressiveness, sketching has been widely used for 2D facial image generation and editing. Applying sketching to NeRFs is challenging due to the inherent uncertainty for 3D generation with 2D constraints, a significant gap in content richness when generating faces from sparse sketches, and potential inconsistencies for sequential multi-view editing given only 2D sketch inputs. To address these challenges, we present SketchFaceNeRF, a novel sketch-based 3D facial NeRF generation and editing method, to produce free-view photo-realistic images. To solve the challenge of sketch sparsity, we introduce a Sketch Tri-plane Prediction net to first inject the appearance into sketches, thus generating features given reference images to allow color and texture control. Such features are then lifted into compact 3D tri-planes to supplement the absent 3D information, which is important for improving robustness and faithfulness. However, during editing, consistency for unseen or unedited 3D regions is difficult to maintain due to limited spatial hints in sketches. We thus adopt a Mask Fusion module to transform free-view 2D masks (inferred from sketch editing operations) into the tri-plane space as 3D masks, which guide the fusion of the original and sketch-based generated faces to synthesize edited faces. We further design an optimization approach with a novel space loss to improve identity retention and editing faithfulness. Our pipeline enables users to flexibly manipulate faces from different viewpoints in 3D space, easily designing desirable facial models. Extensive experiments validate that our approach is superior to the state-of-the-art 2D sketch-based image generation and editing approaches in realism and faithfulness.},
  archive      = {J_TOG},
  author       = {Lin Gao and Feng-Lin Liu and Shu-Yu Chen and Kaiwen Jiang and Chun-Peng Li and Yu-Kun Lai and Hongbo Fu},
  doi          = {10.1145/3592100},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {159:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {SketchFaceNeRF: Sketch-based facial generation and editing in neural radiance fields},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AvatarReX: Real-time expressive full-body avatars.
<em>TOG</em>, <em>42</em>(4), 158:1–19. (<a
href="https://doi.org/10.1145/3592101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present AvatarReX, a new method for learning NeRF-based full-body avatars from video data. The learnt avatar not only provides expressive control of the body, hands and the face together, but also supports real-time animation and rendering. To this end, we propose a compositional avatar representation, where the body, hands and the face are separately modeled in a way that the structural prior from parametric mesh templates is properly utilized without compromising representation flexibility. Furthermore, we disentangle the geometry and appearance for each part. With these technical designs, we propose a dedicated deferred rendering pipeline, which can be executed at a real-time framerate to synthesize high-quality free-view images. The disentanglement of geometry and appearance also allows us to design a two-pass training strategy that combines volume rendering and surface rendering for network training. In this way, patch-level supervision can be applied to force the network to learn sharp appearance details on the basis of geometry estimation. Overall, our method enables automatic construction of expressive full-body avatars with real-time rendering capability, and can generate photo-realistic images with dynamic details for novel body motions and facial expressions.},
  archive      = {J_TOG},
  author       = {Zerong Zheng and Xiaochen Zhao and Hongwen Zhang and Boning Liu and Yebin Liu},
  doi          = {10.1145/3592101},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {158:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {AvatarReX: Real-time expressive full-body avatars},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ScanBot: Autonomous reconstruction via deep reinforcement
learning. <em>TOG</em>, <em>42</em>(4), 157:1–16. (<a
href="https://doi.org/10.1145/3592113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoscanning of an unknown environment is the key to many AR/VR and robotic applications. However, autonomous reconstruction with both high efficiency and quality remains a challenging problem. In this work, we propose a reconstruction-oriented autoscanning approach, called ScanBot, which utilizes hierarchical deep reinforcement learning techniques for global region-of-interest (ROI) planning to improve the scanning efficiency and local next-best-view (NBV) planning to enhance the reconstruction quality. Given the partially reconstructed scene, the global policy designates an ROI with insufficient exploration or reconstruction. The local policy is then applied to refine the reconstruction quality of objects in this region by planning and scanning a series of NBVs. A novel mixed 2D-3D representation is designed for these policies, where a 2D quality map with tailored quality channels encoding the scanning progress is consumed by the global policy, and a coarse-to-fine 3D volumetric representation that embodies both local environment and object completeness is fed to the local policy. These two policies iterate until the whole scene has been completely explored and scanned. To speed up the learning of complex environmental dynamics and enhance the agent&#39;s memory for spatial-temporal inference, we further introduce two novel auxiliary learning tasks to guide the training of our global policy. Thorough evaluations and comparisons are carried out to show the feasibility of our proposed approach and its advantages over previous methods. Code and data are available at https://github.com/HezhiCao/Scanbot.},
  archive      = {J_TOG},
  author       = {Hezhi Cao and Xia Xi and Guan Wu and Ruizhen Hu and Ligang Liu},
  doi          = {10.1145/3592113},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {157:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {ScanBot: Autonomous reconstruction via deep reinforcement learning},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dictionary fields: Learning a neural basis decomposition.
<em>TOG</em>, <em>42</em>(4), 156:1–12. (<a
href="https://doi.org/10.1145/3592135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Dictionary Fields, a novel neural representation which decomposes a signal into a product of factors, each represented by a classical or neural field representation, operating on transformed input coordinates. More specifically, we factorize a signal into a coefficient field and a basis field, and exploit periodic coordinate transformations to apply the same basis functions across multiple locations and scales. Our experiments show that Dictionary Fields lead to improvements in approximation quality, compactness, and training time when compared to previous fast reconstruction methods. Experimentally, our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when reconstructing 3D signed distance fields, and higher compactness for radiance field reconstruction tasks. Furthermore, Dictionary Fields enable generalization to unseen images/3D scenes by sharing bases across signals during training which greatly benefits use cases such as image regression from partial observations and few-shot radiance field reconstruction.},
  archive      = {J_TOG},
  author       = {Anpei Chen and Zexiang Xu and Xinyue Wei and Siyu Tang and Hao Su and Andreas Geiger},
  doi          = {10.1145/3592135},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {156:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dictionary fields: Learning a neural basis decomposition},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OctFormer: Octree-based transformers for 3D point clouds.
<em>TOG</em>, <em>42</em>(4), 155:1–11. (<a
href="https://doi.org/10.1145/3592131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200 k. Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D semantic segmentation and 3D object detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are available at https://wang-ps.github.io/octformer.},
  archive      = {J_TOG},
  author       = {Peng-Shuai Wang},
  doi          = {10.1145/3592131},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {155:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {OctFormer: Octree-based transformers for 3D point clouds},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Materialistic: Selecting similar materials in images.
<em>TOG</em>, <em>42</em>(4), 154:1–14. (<a
href="https://doi.org/10.1145/3592390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based on a user-provided image location. In particular, we propose to leverage the unsupervised DINO [Caron et al. 2021] features coupled with a proposed Cross-Similarity Feature Weighting module and an MLP head to extract material similarities in an image. We train our model on a new synthetic image dataset, that we release. We show that our method generalizes well to real-world images. We carefully analyze our model&#39;s behavior on varying material properties and lighting. Additionally, we evaluate it against a hand-annotated benchmark of 50 real photographs. We further demonstrate our model on a set of applications, including material editing, in-video selection, and retrieval of object photographs with similar materials.},
  archive      = {J_TOG},
  author       = {Prafull Sharma and Julien Philip and Michaël Gharbi and Bill Freeman and Fredo Durand and Valentin Deschaintre},
  doi          = {10.1145/3592390},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {154:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Materialistic: Selecting similar materials in images},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end procedural material capture with proxy-free
mixed-integer optimization. <em>TOG</em>, <em>42</em>(4), 153:1–15. (<a
href="https://doi.org/10.1145/3592132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node-graph-based procedural materials are vital to 3D content creation within the computer graphics industry. Leveraging the expressive representation of procedural materials, artists can effortlessly generate diverse appearances by altering the graph structure or node parameters. However, manually reproducing a specific appearance is a challenging task that demands extensive domain knowledge and labor. Previous research has sought to automate this process by converting artist-created material graphs into differentiable programs and optimizing node parameters against a photographed material appearance using gradient descent. These methods involve implementing differentiable filter nodes [Shi et al. 2020] and training differentiable neural proxies for generator nodes to optimize continuous and discrete node parameters [Hu et al. 2022a] jointly. Nevertheless, Neural Proxies exhibits critical limitations, such as long training times, inaccuracies, fixed resolutions, and confined parameter ranges, which hinder their scalability towards the broad spectrum of production-grade material graphs. These constraints fundamentally stem from the absence of faithful and efficient implementations of generic noise and pattern generator nodes, both differentiable and non-differentiable. Such deficiency prevents the direct optimization of continuous and discrete generator node parameters without relying on surrogate models. We present Diffmat v2 , an improved differentiable procedural material library, along with a fully-automated, end-to-end procedural material capture framework that combines gradient-based optimization and gradient-free parameter search to match existing production-grade procedural materials against user-taken flash photos. Diffmat v2 expands the range of differentiable material graph nodes in Diffmat [Shi et al. 2020] by adding generic noise/pattern generator nodes and user-customizable per-pixel filter nodes. This allows for the complete translation and optimization of procedural materials across various categories without the need for external proprietary tools or pre-cached noise patterns. Consequently, our method can capture a considerably broader array of materials, encompassing those with highly regular or stochastic geometries. We demonstrate that our end-to-end approach yields a closer match to the target than MATch [Shi et al. 2020] and Neural Proxies [Hu et al. 2022a] when starting from initially unmatched continuous and discrete parameters.},
  archive      = {J_TOG},
  author       = {Beichen Li and Liang Shi and Wojciech Matusik},
  doi          = {10.1145/3592132},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {153:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {End-to-end procedural material capture with proxy-free mixed-integer optimization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards material digitization with a dual-scale optical
system. <em>TOG</em>, <em>42</em>(4), 152:1–13. (<a
href="https://doi.org/10.1145/3592147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing devices for measuring material appearance in spatially-varying samples are limited to a single scale, either micro or mesoscopic. This is a practical limitation when the material has a complex multi-scale structure. In this paper, we present a system and methods to digitize materials at two scales, designed to include high-resolution data in spatially-varying representations at larger scales. We design and build a hemispherical light dome able to digitize flat material samples up to 11x11cm. We estimate geometric properties, anisotropic reflectance and transmittance at the microscopic level using polarized directional lighting with a single orthogonal camera. Then, we propagate this structured information to the mesoscale, using a neural network trained with the data acquired by the device and image-to-image translation methods. To maximize the compatibility of our digitization, we leverage standard BSDF models commonly adopted in the industry. Through extensive experiments, we demonstrate the precision of our device and the quality of our digitization process using a set of challenging real-world material samples and validation scenes. Further, we demonstrate the optical resolution and potential of our device for acquiring more complex material representations by capturing microscopic attributes which affect the global appearance: we characterize the properties of textile materials such as the yarn twist or the shape of individual fly-out fibers. We also release the SEDDIDOME dataset of materials, including raw data captured by the machine and optimized parameteres.},
  archive      = {J_TOG},
  author       = {Elena Garces and Victor Arellano and Carlos Rodriguez-Pardo and David Pascual-Hernandez and Sergio Suja and Jorge Lopez-Moreno},
  doi          = {10.1145/3592147},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {152:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Towards material digitization with a dual-scale optical system},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Word-as-image for semantic typography. <em>TOG</em>,
<em>42</em>(4), 151:1–11. (<a
href="https://doi.org/10.1145/3592123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques. Code and demo will be available at our project page.},
  archive      = {J_TOG},
  author       = {Shir Iluz and Yael Vinker and Amir Hertz and Daniel Berio and Daniel Cohen-Or and Ariel Shamir},
  doi          = {10.1145/3592123},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {151:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Word-as-image for semantic typography},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Encoder-based domain tuning for fast personalization of
text-to-image models. <em>TOG</em>, <em>42</em>(4), 150:1–13. (<a
href="https://doi.org/10.1145/3592133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image personalization aims to teach a pre-trained diffusion model to reason about novel, user provided concepts, embedding them into new scenes guided by natural language prompts. However, current personalization approaches struggle with lengthy training times, high storage requirements or loss of identity. To overcome these limitations, we propose an encoder-based domain-tuning approach. Our key insight is that by underfitting on a large set of concepts from a given domain, we can improve generalization and create a model that is more amenable to quickly adding novel concepts from the same domain. Specifically, we employ two components: First, an encoder that takes as an input a single image of a target concept from a given domain, e.g. a specific face, and learns to map it into a word-embedding representing the concept. Second, a set of regularized weight-offsets for the text-to-image model that learn how to effectively injest additional concepts. Together, these components are used to guide the learning of unseen concepts, allowing us to personalize a model using only a single image and as few as 5 training steps --- accelerating personalization from dozens of minutes to seconds , while preserving quality. Code and trained encoders will be available at our project page.},
  archive      = {J_TOG},
  author       = {Rinon Gal and Moab Arar and Yuval Atzmon and Amit H. Bermano and Gal Chechik and Daniel Cohen-Or},
  doi          = {10.1145/3592133},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {150:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Encoder-based domain tuning for fast personalization of text-to-image models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blended latent diffusion. <em>TOG</em>, <em>42</em>(4),
149:1–11. (<a href="https://doi.org/10.1145/3592450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space and eliminating the need for resource-intensive CLIP gradient calculations at each diffusion step. We first enable LDM to perform local image edits by blending the latents at each step, similarly to Blended Diffusion. Next we propose an optimization-based solution for the inherent inability of LDM to accurately reconstruct images. Finally, we address the scenario of performing local edits using thin masks. We evaluate our method against the available baselines both qualitatively and quantitatively and demonstrate that in addition to being faster, it produces more precise results.},
  archive      = {J_TOG},
  author       = {Omri Avrahami and Ohad Fried and Dani Lischinski},
  doi          = {10.1145/3592450},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {149:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Blended latent diffusion},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attend-and-excite: Attention-based semantic guidance for
text-to-image diffusion models. <em>TOG</em>, <em>42</em>(4), 148:1–10.
(<a href="https://doi.org/10.1145/3592116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect , where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes ( e.g. , colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN) , where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite , we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen --- or excite --- their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite.github.io/Attend-and-Excite/.},
  archive      = {J_TOG},
  author       = {Hila Chefer and Yuval Alaluf and Yael Vinker and Lior Wolf and Daniel Cohen-Or},
  doi          = {10.1145/3592116},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {148:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). P2M: A fast solver for querying distance from point to mesh
surface. <em>TOG</em>, <em>42</em>(4), 147:1–13. (<a
href="https://doi.org/10.1145/3592439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing point-to-mesh distance query solvers, such as Proximity Query Package (PQP), Embree and Fast Closest Point Query (FCPW), are based on bounding volume hierarchy (BVH). The hierarchical organizational structure enables one to eliminate the vast majority of triangles that do not help find the closest point. In this paper, we develop a totally different algorithmic paradigm, named P2M , to speed up point-to-mesh distance queries. Our original intention is to precompute a KD tree (KDT) of mesh vertices to approximately encode the geometry of a mesh surface containing vertices, edges and faces. However, it is very likely that the closest primitive to the query point is an edge e (resp., a face f ), but the KDT reports a mesh vertex υ instead. We call υ an interceptor of e (resp., f ). The main contribution of this paper is to invent a simple yet effective interception inspection rule and an efficient flooding interception inspection algorithm for quickly finding out all the interception pairs. Once the KDT and the interception table are precomputed, the query stage proceeds by first searching the KDT and then looking up the interception table to retrieve the closest geometric primitive. Statistics show that our query algorithm runs many times faster than the state-of-the-art solvers.},
  archive      = {J_TOG},
  author       = {Chen Zong and Jiacheng Xu and Jiantao Song and Shuangmin Chen and Shiqing Xin and Wenping Wang and Changhe Tu},
  doi          = {10.1145/3592439},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {147:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {P2M: A fast solver for querying distance from point to mesh surface},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shortest path to boundary for self-intersecting meshes.
<em>TOG</em>, <em>42</em>(4), 146:1–15. (<a
href="https://doi.org/10.1145/3592136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a method for efficiently computing the exact shortest path to the boundary of a mesh from a given internal point in the presence of self-intersections. We provide a formal definition of shortest boundary paths for self-intersecting objects and present a robust algorithm for computing the actual shortest boundary path. The resulting method offers an effective solution for collision and self-collision handling while simulating deformable volumetric objects, using fast simulation techniques that provide no guarantees on collision resolution. Our evaluation includes complex self-collision scenarios with a large number of active contacts, showing that our method can successfully handle them by introducing a relatively minor computational overhead.},
  archive      = {J_TOG},
  author       = {He Chen and Elie Diaz and Cem Yuksel},
  doi          = {10.1145/3592136},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {146:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Shortest path to boundary for self-intersecting meshes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-timestep remeshing for contacting elastodynamics.
<em>TOG</em>, <em>42</em>(4), 145:1–15. (<a
href="https://doi.org/10.1145/3592428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose In-Timestep Remeshing, a fully coupled, adaptive meshing algorithm for contacting elastodynamics where remeshing steps are tightly integrated, implicitly, within the timestep solve. Our algorithm refines and coarsens the domain automatically by measuring physical energy changes within each ongoing timestep solve. This provides consistent, degree-of-freedom-efficient, productive remeshing that, by construction, is physics-aware and so avoids the errors, over-refinements, artifacts, per-example hand-tuning, and instabilities commonly encountered when remeshing with timestepping methods. Our in-timestep computation then ensures that each simulation step&#39;s output is both a converged stable solution on the updated mesh and a temporally consistent trajectory with respect to the model and solution of the last timestep. At the same time, the output is guaranteed safe (intersection- and inversion-free) across all operations. We demonstrate applications across a wide range of extreme stress tests with challenging contacts, sharp geometries, extreme compressions, large timesteps, and wide material stiffness ranges - all scenarios well-appreciated to challenge existing remeshing methods.},
  archive      = {J_TOG},
  author       = {Zachary Ferguson and Teseo Schneider and Danny Kaufman and Daniele Panozzo},
  doi          = {10.1145/3592428},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {145:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {In-timestep remeshing for contacting elastodynamics},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A temporal coherent topology optimization approach for
assembly planning of bespoke frame structures. <em>TOG</em>,
<em>42</em>(4), 144:1–13. (<a
href="https://doi.org/10.1145/3592102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a computational framework for planning the assembly sequence of bespoke frame structures. Frame structures are one of the most commonly used structural systems in modern architecture, providing resistance to gravitational and external loads. Building frame structures requires traversing through several partially built states. If the assembly sequence is planned poorly, these partial assemblies can exhibit substantial deformation due to self-weight, slowing down or jeopardizing the assembly process. Finding a good assembly sequence that minimizes intermediate deformations is an interesting yet challenging combinatorial problem that is usually solved by heuristic search algorithms. In this paper, we propose a new optimization-based approach that models sequence planning using a series of topology optimization problems. Our key insight is that enforcing temporal coherent constraints in the topology optimization can lead to sub-structures with small deformations while staying consistent with each other to form an assembly sequence. We benchmark our algorithm on a large data set and show improvements in both performance and computational time over greedy search algorithms. In addition, we demonstrate that our algorithm can be extended to handle assembly with static or dynamic supports. We further validate our approach by generating a series of results in multiple scales, including a real-world prototype with a mixed reality assistant using our computed sequence and a simulated example demonstrating a multi-robot assembly application.},
  archive      = {J_TOG},
  author       = {Ziqi Wang and Florian Kennel-Maushart and Yijiang Huang and Bernhard Thomaszewski and Stelian Coros},
  doi          = {10.1145/3592102},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {144:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {A temporal coherent topology optimization approach for assembly planning of bespoke frame structures},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantics and scheduling for machine knitting compilers.
<em>TOG</em>, <em>42</em>(4), 143:1–26. (<a
href="https://doi.org/10.1145/3592449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine knitting is a well-established fabrication technique for complex soft objects, and both companies and researchers have developed tools for generating machine knitting patterns. However, existing representations for machine knitted objects are incomplete (do not cover the complete domain of machine knittable objects) or overly specific (do not account for symmetries and equivalences among knitting instruction sequences). This makes it difficult to define correctness in machine knitting, let alone verify the correctness of a given program or program transformation. The major contribution of this work is a formal semantics for knitout, a low-level Domain Specific Language for knitting machines. We accomplish this by using what we call the fenced tangle , which extends concepts from knot theory to allow for a mathematical definition of knitting program equivalence that matches the intuition behind knit objects. Finally, using this formal representation, we prove the correctness of a sequence of rewrite rules; and demonstrate how these rewrite rules can form the foundation for higher-level tasks such as compiling a program for a specific machine and optimizing for time/reliability, all while provably generating the same knit object under our proposed semantics. By establishing formal definitions of correctness, this work provides a strong foundation for compiling and optimizing knit programs.},
  archive      = {J_TOG},
  author       = {Jenny Lin and Vidya Narayanan and Yuka Ikarashi and Jonathan Ragan-Kelley and Gilbert Bernstein and James Mccann},
  doi          = {10.1145/3592449},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {143:1–26},
  shortjournal = {ACM Trans. Graph.},
  title        = {Semantics and scheduling for machine knitting compilers},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PCBend: Light up your 3D shapes with foldable circuit
boards. <em>TOG</em>, <em>42</em>(4), 142:1–16. (<a
href="https://doi.org/10.1145/3592411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a computational design approach for covering a surface with individually addressable RGB LEDs, effectively forming a low-resolution surface screen. To achieve a low-cost and scalable approach, we propose creating designs from flat PCB panels bent in-place along the surface of a 3D printed core. Working with standard rigid PCBs enables the use of established PCB manufacturing services, allowing the fabrication of designs with several hundred LEDs. Our approach optimizes the PCB geometry for folding, and then jointly optimizes the LED packing, circuit and routing, solving a challenging layout problem under strict manufacturing requirements. Unlike paper, PCBs cannot bend beyond a certain point without breaking. Therefore, we introduce parametric cut patterns acting as hinges, designed to allow bending while remaining compact. To tackle the joint optimization of placement, circuit and routing, we propose a specialized algorithm that splits the global problem into one sub-problem per triangle, which is then individually solved. Our technique generates PCB blueprints in a completely automated way. After being fabricated by a PCB manufacturing service, the boards are bent and glued by the user onto the 3D printed support. We demonstrate our technique on a range of physical models and virtual examples, creating intricate surface light patterns from hundreds of LEDs. The code and data for this paper are available at https://github.com/mfremer/pcbend.},
  archive      = {J_TOG},
  author       = {Marco Freire and Manas Bhargava and Camille Schreck and Pierre-Alexandre Hugron and Bernd Bickel and Sylvain Lefebvre},
  doi          = {10.1145/3592411},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {142:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {PCBend: Light up your 3D shapes with foldable circuit boards},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense, interlocking-free and scalable spectral packing of
generic 3D objects. <em>TOG</em>, <em>42</em>(4), 141:1–14. (<a
href="https://doi.org/10.1145/3592126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Packing 3D objects into a known container is a very common task in many industries such as packaging, transportation, and manufacturing. This important problem is known to be NP-hard and even approximate solutions are challenging. This is due to the difficulty of handling interactions between objects with arbitrary 3D geometries and a vast combinatorial search space. Moreover, the packing must be interlocking-free for real-world applications. In this work, we first introduce a novel packing algorithm to search for placement locations given an object. Our method leverages a discrete voxel representation. We formulate collisions between objects as correlations of functions computed efficiently using Fast Fourier Transform (FFT). To determine the best placements, we utilize a novel cost function, which is also computed efficiently using FFT. Finally, we show how interlocking detection and correction can be addressed in the same framework resulting in interlocking-free packing. We propose a challenging benchmark with thousands of 3D objects to evaluate our algorithm. Our method demonstrates state-of-the-art performance on the benchmark when compared to existing methods in both density and speed.},
  archive      = {J_TOG},
  author       = {Qiaodong Cui and Victor Rong and Desai Chen and Wojciech Matusik},
  doi          = {10.1145/3592126},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {141:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Dense, interlocking-free and scalable spectral packing of generic 3D objects},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual mirrors: Non-line-of-sight imaging beyond the third
bounce. <em>TOG</em>, <em>42</em>(4), 140:1–15. (<a
href="https://doi.org/10.1145/3592429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-line-of-sight (NLOS) imaging methods are capable of reconstructing complex scenes that are not visible to an observer using indirect illumination. However, they assume only third-bounce illumination, so they are currently limited to single-corner configurations, and present limited visibility when imaging surfaces at certain orientations. To reason about and tackle these limitations, we make the key observation that planar diffuse surfaces behave specularly at wavelengths used in the computational wave-based NLOS imaging domain. We call such surfaces virtual mirrors. We leverage this observation to expand the capabilities of NLOS imaging using illumination beyond the third bounce, addressing two problems: imaging single-corner objects at limited visibility angles, and imaging objects hidden behind two corners. To image objects at limited visibility angles, we first analyze the reflections of the known illuminated point on surfaces of the scene as an estimator of the position and orientation of objects with limited visibility. We then image those limited visibility objects by computationally building secondary apertures at other surfaces that observe the target object from a direct visibility perspective. Beyond single-corner NLOS imaging, we exploit the specular behavior of virtual mirrors to image objects hidden behind a second corner by imaging the space behind such virtual mirrors, where the mirror image of objects hidden around two corners is formed. No specular surfaces were involved in the making of this paper.},
  archive      = {J_TOG},
  author       = {Diego Royo and Talha Sultan and Adolfo Muñoz and Khadijeh Masumnia-Bisheh and Eric Brandt and Diego Gutierrez and Andreas Velten and Julio Marco},
  doi          = {10.1145/3592429},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {140:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Virtual mirrors: Non-line-of-sight imaging beyond the third bounce},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D gaussian splatting for real-time radiance field
rendering. <em>TOG</em>, <em>42</em>(4), 139:1–14. (<a
href="https://doi.org/10.1145/3592433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  archive      = {J_TOG},
  author       = {Bernhard Kerbl and Georgios Kopanas and Thomas Leimkuehler and George Drettakis},
  doi          = {10.1145/3592433},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {139:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {3D gaussian splatting for real-time radiance field rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DreamFace: Progressive generation of animatable 3D faces
under text guidance. <em>TOG</em>, <em>42</em>(4), 138:1–16. (<a
href="https://doi.org/10.1145/3592094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Metaverse applications demand accessible, accurate and easy-to-use tools for 3D digital human creations in order to depict different cultures and societies as if in the physical world. Recent large-scale vision-language advances pave the way for novices to conveniently customize 3D content. However, the generated CG-friendly assets still cannot represent the desired facial traits for human characteristics. In this paper, we present Dream-Face, a progressive scheme to generate personalized 3D faces under text guidance. It enables layman users to naturally customize 3D facial assets that are compatible with CG pipelines, with desired shapes, textures and fine-grained animation capabilities. From a text input to describe the facial traits, we first introduce a coarse-to-fine scheme to generate the neutral facial geometry with a unified topology. We employ a selection strategy in the CLIP embedding space to generate coarse geometry, and subsequently optimize both the detailed displacements and normals using Score Distillation Sampling (SDS) from the generic Latent Diffusion Model (LDM). Then, for neutral appearance generation, we introduce a dual-path mechanism, which combines the generic LDM with a novel texture LDM to ensure both the diversity and textural specification in the UV space. We also employ a two-stage optimization to perform SDS in both the latent and image spaces to significantly provide compact priors for fine-grained synthesis. It also enables learning the mapping from the compact latent space into physically-based textures (diffuse albedo, specular intensity, normal maps, etc.). Our generated neutral assets naturally support blendshapes-based facial animations, thanks to the unified geometric topology. We further improve the animation ability with personalized deformation characteristics. To this end, we learn the universal expression prior in a latent space with neutral asset conditioning using the cross-identity hypernetwork, we subsequently train a neural facial tracker from video input space into the pre-trained expression space for personalized fine-grained animation. Extensive qualitative and quantitative experiments validate the effectiveness and generalizability of DreamFace. Notably, DreamFace can generate realistic 3D facial assets with physically-based rendering quality and rich animation ability from video footage, even for fashion icons or exotic characters in cartoons and fiction movies.},
  archive      = {J_TOG},
  author       = {Longwen Zhang and Qiwei Qiu and Hongyang Lin and Qixuan Zhang and Cheng Shi and Wei Yang and Ye Shi and Sibei Yang and Lan Xu and Jingyi Yu},
  doi          = {10.1145/3592094},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {138:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {DreamFace: Progressive generation of animatable 3D faces under text guidance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GREIL-crowds: Crowd simulation with deep reinforcement
learning and examples. <em>TOG</em>, <em>42</em>(4), 137:1–15. (<a
href="https://doi.org/10.1145/3592459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simulating crowds with realistic behaviors is a difficult but very important task for a variety of applications. Quantifying how a person balances between different conflicting criteria such as goal seeking, collision avoidance and moving within a group is not intuitive, especially if we consider that behaviors differ largely between people. Inspired by recent advances in Deep Reinforcement Learning, we propose Guided REinforcement Learning (GREIL) Crowds, a method that learns a model for pedestrian behaviors which is guided by reference crowd data. The model successfully captures behaviors such as goal seeking, being part of consistent groups without the need to define explicit relationships and wandering around seemingly without a specific purpose. Two fundamental concepts are important in achieving these results: (a) the per agent state representation and (b) the reward function. The agent state is a temporal representation of the situation around each agent. The reward function is based on the idea that people try to move in situations/states in which they feel comfortable in. Therefore, in order for agents to stay in a comfortable state space, we first obtain a distribution of states extracted from real crowd data; then we evaluate states based on how much of an outlier they are compared to such a distribution. We demonstrate that our system can capture and simulate many complex and subtle crowd interactions in varied scenarios. Additionally, the proposed method generalizes to unseen situations, generates consistent behaviors and does not suffer from the limitations of other data-driven and reinforcement learning approaches.},
  archive      = {J_TOG},
  author       = {Panayiotis Charalambous and Julien Pettre and Vassilis Vassiliades and Yiorgos Chrysanthou and Nuria Pelechano},
  doi          = {10.1145/3592459},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {137:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {GREIL-crowds: Crowd simulation with deep reinforcement learning and examples},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating activity snippets by learning human-scene
interactions. <em>TOG</em>, <em>42</em>(4), 136:1–15. (<a
href="https://doi.org/10.1145/3592096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach to generate virtual activity snippets, which comprise sequenced keyframes of multi-character, multi-object interaction scenarios in 3D environments, by learning from recordings of human-scene interactions. The generation consists of two stages. First, we use a sequential deep graph generative model with a temporal module to iteratively generate keyframe descriptions, which represent abstract interactions using graphs, while preserving spatial-temporal relations through the activities. Second, we devise an optimization framework to instantiate the activity snippets in virtual 3D environments guided by the generated keyframe descriptions. Our approach optimizes the poses of character and object instances encoded by the graph nodes to satisfy the relations and constraints encoded by the graph edges. The instantiation process includes a coarse 2D optimization followed by a fine 3D optimization to effectively explore the complex solution space for placing and posing the instances. Through experiments and a perceptual study, we applied our approach to generate plausible activity snippets under different settings.},
  archive      = {J_TOG},
  author       = {Changyang Li and Lap-Fai Yu},
  doi          = {10.1145/3592096},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {136:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generating activity snippets by learning human-scene interactions},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time radiance fields for single-image portrait view
synthesis. <em>TOG</em>, <em>42</em>(4), 135:1–15. (<a
href="https://doi.org/10.1145/3592460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ), but our algorithm can also be applied in the future to other categories with a 3D-aware image generator.},
  archive      = {J_TOG},
  author       = {Alex Trevithick and Matthew Chan and Michael Stengel and Eric Chan and Chao Liu and Zhiding Yu and Sameh Khamis and Manmohan Chandraker and Ravi Ramamoorthi and Koki Nagano},
  doi          = {10.1145/3592460},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {135:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Real-time radiance fields for single-image portrait view synthesis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal set inversion for animated implicits. <em>TOG</em>,
<em>42</em>(4), 134:1–18. (<a
href="https://doi.org/10.1145/3592448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We exploit the temporal coherence of closed-form animated implicit surfaces by locally re-evaluating an octree-like discretization of the implicit field only as and where is necessary to rigorously maintain a global error invariant over time, thereby saving resources in static or slowly-evolving areas far from the motion where per-frame updates are not necessary. We treat implicit surface rendering as a special case of the continuous constraint satisfaction problem of set inversion, which seeks preimages of arbitrary sets under vector-valued functions. From this perspective, we formalize a temporally-coherent set inversion algorithm that localizes changes in the field by range-bounding its time derivatives using interval arithmetic. We implement our algorithm on the GPU using persistent thread scheduling and apply it to the scalar case of implicit surface and swept volume rendering where we achieve significant speedups in complex scenes with localized deformations like those found in games and modelling applications where interactivity is required and bounded-error approximation is acceptable.},
  archive      = {J_TOG},
  author       = {Kavosh Jazar and Paul G. Kry},
  doi          = {10.1145/3592448},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {134:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Temporal set inversion for animated implicits},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ETER: Elastic tessellation for real-time pixel-accurate
rendering of large-scale NURBS models. <em>TOG</em>, <em>42</em>(4),
133:1–13. (<a href="https://doi.org/10.1145/3592419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present ETER, an elastic tessellation framework for rendering large-scale NURBS models with pixel-accurate and crack-free quality at real-time frame rates. We propose a highly parallel adaptive tessellation algorithm to achieve pixel accuracy, measured by the screen space error between the exact surface and its triangulation. To resolve a bottleneck in NURBS rendering, we present a novel evaluation method based on uniform sampling grids and accelerated by GPU Tensor Cores. Compared to evaluation based on hardware tessellation, our method has achieved a significant speedup of 2.9 to 16.2 times depending on the degrees of the patches. We develop an efficient crack-filling algorithm based on conservative rasterization and visibility buffer to fill the tessellation-induced cracks while greatly reducing the jagged effect introduced by conservative rasterization. We integrate all our novel algorithms, implemented in CUDA, into a GPU NURBS rendering pipeline based on Mesh Shaders and hybrid software/hardware rasterization. Our performance data on a commodity GPU show that the rendering pipeline based on ETER is capable of rendering up to 3.7 million patches (0.25 billion tessellated triangles) in real-time (30FPS). With its advantages in performance, scalability, and visual quality in rendering large-scale NURBS models, a real-time tessellation solution based on ETER can be a powerful alternative or even a potential replacement for the existing pre-tessellation solution in CAD systems.},
  archive      = {J_TOG},
  author       = {Ruicheng Xiong and Yang Lu and Cong Chen and Jiaming Zhu and Yajun Zeng and Ligang Liu},
  doi          = {10.1145/3592419},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {133:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {ETER: Elastic tessellation for real-time pixel-accurate rendering of large-scale NURBS models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning of robust spectral shape matching.
<em>TOG</em>, <em>42</em>(4), 132:1–15. (<a
href="https://doi.org/10.1145/3592107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel learning-based approach for robust 3D shape matching. Our method builds upon deep functional maps and can be trained in a fully unsupervised manner. Previous deep functional map methods mainly focus on predicting optimised functional maps alone, and then rely on off-the-shelf post-processing to obtain accurate point-wise maps during inference. However, this two-stage procedure for obtaining point-wise maps often yields sub-optimal performance. In contrast, building upon recent insights about the relation between functional maps and point-wise maps, we propose a novel unsupervised loss to couple the functional maps and point-wise maps, and thereby directly obtain point-wise maps without any post-processing. Our approach obtains accurate correspondences not only for near-isometric shapes, but also for more challenging non-isometric shapes and partial shapes, as well as shapes with different discretisation or topological noise. Using a total of nine diverse datasets, we extensively evaluate the performance and demonstrate that our method substantially outperforms previous state-of-the-art methods, even compared to recent supervised methods. Our code is available at https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.},
  archive      = {J_TOG},
  author       = {Dongliang Cao and Paul Roetzer and Florian Bernard},
  doi          = {10.1145/3592107},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {132:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Unsupervised learning of robust spectral shape matching},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expansion cones: A progressive volumetric mapping framework.
<em>TOG</em>, <em>42</em>(4), 131:1–19. (<a
href="https://doi.org/10.1145/3592421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volumetric mapping is a ubiquitous and difficult problem in Geometry Processing and has been the subject of research in numerous and various directions. While several methods show encouraging results, the field still lacks a general approach with guarantees regarding map bijectivity. Through this work, we aim at opening the door to a new family of methods by providing a novel framework based on the concept of progressive expansion. Starting from an initial map of a tetrahedral mesh whose image may contain degeneracies but no inversions, we incrementally adjust vertex images to expand degenerate elements. By restricting movement to so-called expansion cones , it is done in such a way that the number of degenerate elements decreases in a strictly monotonic manner, without ever introducing any inversion. Adaptive local refinement of the mesh is performed to facilitate this process. We describe a prototype algorithm in the realm of this framework for the computation of maps from ball-topology tetrahedral meshes to convex or star-shaped domains. This algorithm is evaluated and compared to state-of-the-art methods, demonstrating its benefits in terms of bijectivity. We also discuss the associated cost in terms of sometimes significant mesh refinement to obtain the necessary degrees of freedom required for establishing a valid mapping. Our conclusions include that while this algorithm is only of limited immediate practical utility due to efficiency concerns, the general framework has the potential to inspire a range of novel methods improving on the efficiency aspect.},
  archive      = {J_TOG},
  author       = {Valentin Zénon Nigolian and Marcel Campen and David Bommes},
  doi          = {10.1145/3592421},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {131:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Expansion cones: A progressive volumetric mapping framework},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational quasi-harmonic maps for computing
diffeomorphisms. <em>TOG</em>, <em>42</em>(4), 130:1–26. (<a
href="https://doi.org/10.1145/3592105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation of injective (or inversion-free) maps is a key task in geometry processing, physical simulation, and shape optimization. Despite being a longstanding problem, it remains challenging due to its highly nonconvex and combinatoric nature. We propose computation of variational quasi-harmonic maps to obtain smooth inversion-free maps. Our work is built on a key observation about inversion-free maps: A planar map is a diffeomorphism if and only if it is quasi-harmonic and satisfies a special Cauchy boundary condition. We hence equate the inversion-free mapping problem to an optimal control problem derived from our theoretical result, in which we search in the space of parameters that define an elliptic PDE. We show that this problem can be solved by minimizing within a family of functionals. Similarly, our discretized functionals admit exactly injective maps as the minimizers, empirically producing inversion-free discrete maps of triangle meshes. We design efficient numerical procedures for our problem that prioritize robust convergence paths. Experiments show that on challenging examples our methods can achieve up to orders of magnitude improvement over state-of-the-art, in terms of speed or quality. Moreover, we demonstrate how to optimize a generic energy in our framework while restricting to quasi-harmonic maps.},
  archive      = {J_TOG},
  author       = {Yu Wang and Minghao Guo and Justin Solomon},
  doi          = {10.1145/3592105},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {130:1–26},
  shortjournal = {ACM Trans. Graph.},
  title        = {Variational quasi-harmonic maps for computing diffeomorphisms},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Galaxy maps: Localized foliations for bijective volumetric
mapping. <em>TOG</em>, <em>42</em>(4), 129:1–16. (<a
href="https://doi.org/10.1145/3592410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A method is presented to compute volumetric maps and parametrizations of objects over 3D domains. As a key feature, continuity and bijectivity are ensured by construction. Arbitrary objects of ball topology, represented as tetrahedral meshes, are supported. Arbitrary convex as well as star-shaped domains are supported. Full control over the boundary mapping is provided. The method is based on the technique of simplicial foliations, generalized to a broader class of domain shapes and applied adaptively in a novel localized manner. This increases flexibility as well as efficiency over the state of the art, while maintaining reliability in guaranteeing map bijectivity.},
  archive      = {J_TOG},
  author       = {Steffen Hinderink and Marcel Campen},
  doi          = {10.1145/3592410},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {129:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Galaxy maps: Localized foliations for bijective volumetric mapping},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UniTune: Text-driven image editing by fine tuning a
diffusion model on a single image. <em>TOG</em>, <em>42</em>(4),
128:1–10. (<a href="https://doi.org/10.1145/3592451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit while maintaining high fidelity to the input image. UniTune does not require additional inputs, like masks or sketches, and can perform multiple edits on the same image without retraining. We test our method using the Imagen model in a range of different use cases. We demonstrate that it is broadly applicable and can perform a surprisingly wide range of expressive editing operations, including those requiring significant visual changes that were previously impossible.},
  archive      = {J_TOG},
  author       = {Dani Valevski and Matan Kalman and Eyal Molad and Eyal Segalis and Yossi Matias and Yaniv Leviathan},
  doi          = {10.1145/3592451},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {128:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {UniTune: Text-driven image editing by fine tuning a diffusion model on a single image},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved water sound synthesis using coupled bubbles.
<em>TOG</em>, <em>42</em>(4), 127:1–13. (<a
href="https://doi.org/10.1145/3592424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a practical framework for synthesizing bubble-based water sounds that captures the rich inter-bubble coupling effects responsible for low-frequency acoustic emissions from bubble clouds. We propose coupled-bubble oscillator models with regularized singularities, and techniques to reduce the computational cost of time stepping with dense, time-varying mass matrices. Airborne acoustic emissions are estimated using finite-difference time-domain (FDTD) methods. We propose a simple, analytical surface-acceleration model, and a sample-and-hold GPU wavesolver that is simple and faster than prior CPU wavesolvers. Sound synthesis results are demonstrated using bubbly flows from incompressible, two-phase simulations, as well as procedurally generated examples using single-phase FLIP fluid animations. Our results demonstrate sound simulations with hundreds of thousands of bubbles, and perceptually significant frequency transformations with fuller low-frequency content.},
  archive      = {J_TOG},
  author       = {Kangrui Xue and Ryan M. Aronson and Jui-Hsien Wang and Timothy R. Langlois and Doug L. James},
  doi          = {10.1145/3592424},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {127:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Improved water sound synthesis using coupled bubbles},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fluid cohomology. <em>TOG</em>, <em>42</em>(4), 126:1–25.
(<a href="https://doi.org/10.1145/3592402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vorticity-streamfunction formulation for incompressible inviscid fluids is the basis for many fluid simulation methods in computer graphics, including vortex methods, streamfunction solvers, spectral methods, and Monte Carlo methods. We point out that current setups in the vorticity-streamfunction formulation are insufficient at simulating fluids on general non-simply-connected domains. This issue is critical in practice, as obstacles, periodic boundaries, and nonzero genus can all make the fluid domain multiply connected. These scenarios introduce nontrivial cohomology components to the flow in the form of harmonic fields. The dynamics of these harmonic fields have been previously overlooked. In this paper, we derive the missing equations of motion for the fluid cohomology components. We elucidate the physical laws associated with the new equations, and show their importance in reproducing physically correct behaviors of fluid flows on domains with general topology.},
  archive      = {J_TOG},
  author       = {Hang Yin and Mohammad Sina Nabizadeh and Baichuan Wu and Stephanie Wang and Albert Chern},
  doi          = {10.1145/3592402},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {126:1–25},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fluid cohomology},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building a virtual weakly-compressible wind tunnel testing
facility. <em>TOG</em>, <em>42</em>(4), 125:1–20. (<a
href="https://doi.org/10.1145/3592394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual wind tunnel testing is a key ingredient in the engineering design process for the automotive and aeronautical industries as well as for urban planning: through visualization and analysis of the simulation data, it helps optimize lift and drag coefficients, increase peak speed, detect high pressure zones, and reduce wind noise at low cost prior to manufacturing. In this paper, we develop an efficient and accurate virtual wind tunnel system based on recent contributions from both computer graphics and computational fluid dynamics in high-performance kinetic solvers. Running on one or multiple GPUs, our massively-parallel lattice Boltzmann model meets industry standards for accuracy and consistency while exceeding current mainstream industrial solutions in terms of efficiency --- especially for unsteady turbulent flow simulation at very high Reynolds number (on the order of 10 7 ) --- due to key contributions in improved collision modeling and boundary treatment, automatic construction of multiresolution grids for complex models, as well as performance optimization. We demonstrate the efficacy and reliability of our virtual wind tunnel testing facility through comparisons of our results to multiple benchmark tests, showing an increase in both accuracy and efficiency compared to state-of-the-art industrial solutions. We also illustrate the fine turbulence structures that our system can capture, indicating the relevance of our solver for both VFX and industrial product design.},
  archive      = {J_TOG},
  author       = {Chaoyang Lyu and Kai Bai and Yiheng Wu and Mathieu Desbrun and Changxi Zheng and Xiaopei Liu},
  doi          = {10.1145/3592394},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {125:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Building a virtual weakly-compressible wind tunnel testing facility},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PolyStokes: A polynomial model reduction method for viscous
fluid simulation. <em>TOG</em>, <em>42</em>(4), 124:1–13. (<a
href="https://doi.org/10.1145/3592146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard liquid simulators apply operator splitting to independently solve for pressure and viscous stresses, a decoupling that induces incorrect free surface boundary conditions. Such methods are unable to simulate fluid phenomena reliant on the balance of pressure and viscous stresses, such as the liquid rope coil instability exhibited by honey. By contrast, unsteady Stokes solvers retain coupling between pressure and viscosity, thus resolving these phenomena, albeit using a much larger and thus more computationally expensive linear system compared to the decoupled approach. To accelerate solving the unsteady Stokes problem, we propose a reduced fluid model wherein interior regions are represented with incompressible polynomial vector fields. Sets of standard grid cells are consolidated into super-cells, each of which are modelled using a quadratic field of 26 degrees of freedom. We demonstrate that the reduced field must necessarily be at least quadratic, with the affine model being unable to correctly capture viscous forces. We reproduce the liquid rope coiling instability, as well as other simulated examples, to show that our reduced model is able to reproduce the same fluid phenomena at a smaller computational cost. Futhermore, we performed a crowdsourced user survey to verify that our method produces imperceptible differences compared to the full unsteady Stokes method.},
  archive      = {J_TOG},
  author       = {Jonathan Panuelos and Ryan Goldade and Eitan Grinspun and David Levin and Christopher Batty},
  doi          = {10.1145/3592146},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {124:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {PolyStokes: A polynomial model reduction method for viscous fluid simulation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fluid-solid coupling in kinetic two-phase flow simulation.
<em>TOG</em>, <em>42</em>(4), 123:1–14. (<a
href="https://doi.org/10.1145/3592138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-life flows exhibit complex and visually appealing behaviors such as bubbling, splashing, glugging and wetting that simulation techniques in graphics have attempted to capture for years. While early approaches were not capable of reproducing multiphase flow phenomena due to their excessive numerical viscosity and low accuracy, kinetic solvers based on the lattice Boltzmann method have recently demonstrated the ability to simulate water-air interaction at high Reynolds numbers in a massively-parallel fashion. However, robust and accurate handling of fluid-solid coupling has remained elusive: be it for CG or CFD solvers, as soon as the motion of immersed objects is too fast or too sudden, pressures near boundaries and interfacial forces exhibit spurious oscillations leading to blowups. Built upon a phase-field and velocity-distribution based lattice-Boltzmann solver for multiphase flows, this paper spells out a series of numerical improvements in momentum exchange, interfacial forces, and two-way coupling to drastically reduce these typical artifacts, thus significantly expanding the types of fluid-solid coupling that we can efficiently simulate. We highlight the numerical benefits of our solver through various challenging simulation results, including comparisons to previous work and real footage.},
  archive      = {J_TOG},
  author       = {Wei Li and Mathieu Desbrun},
  doi          = {10.1145/3592138},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {123:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fluid-solid coupling in kinetic two-phase flow simulation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A contact proxy splitting method for lagrangian solid-fluid
coupling. <em>TOG</em>, <em>42</em>(4), 122:1–14. (<a
href="https://doi.org/10.1145/3592115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a robust and efficient method for simulating Lagrangian solid-fluid coupling based on a new operator splitting strategy. We use variational formulations to approximate fluid properties and solid-fluid interactions, and introduce a unified two-way coupling formulation for SPH fluids and FEM solids using interior point barrier-based frictional contact. We split the resulting optimization problem into a fluid phase and a solid-coupling phase using a novel time-splitting approach with augmented contact proxies , and propose efficient custom linear solvers. Our technique accounts for fluids interaction with nonlinear hyperelastic objects of different geometries and codimensions, while maintaining an algorithmically guaranteed non-penetrating criterion. Comprehensive benchmarks and experiments demonstrate the efficacy of our method.},
  archive      = {J_TOG},
  author       = {Tianyi Xie and Minchen Li and Yin Yang and Chenfanfu Jiang},
  doi          = {10.1145/3592115},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {122:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {A contact proxy splitting method for lagrangian solid-fluid coupling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Micro-mesh construction. <em>TOG</em>, <em>42</em>(4),
121:1–18. (<a href="https://doi.org/10.1145/3592440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-meshes (μ-meshes) are a new structured graphics primitive supporting a large increase in geometric fidelity, without commensurate memory and run-time processing costs, consisting of a base mesh enriched by a displacement map. A new generation of GPUs supports this structure with native hardware μ-mesh ray-tracing, that leverages a self-bounding, compressed displacement mapping scheme to achieve these efficiencies. In this paper, we present anautomatic method to convert an existing multi-million triangle mesh into this compact format, unlocking the advantages of the data representation for a large number of scenarios. We identify the requirements for high-quality μ-meshes, and show how existing re-meshing and displacement-map baking tools are ill-suited for their generation. Our method is based on a simplification scheme tailored to the generation of high-quality base meshes , optimized for tessellation and displacement sampling, in conjunction with algorithms for determining displacement vectors to control the direction and range of displacements. We also explore the optimization of μ-meshes for texture maps and the representation of boundaries. We demonstrate our method with extensive batch processing, converting an existing collection of high-resolution scanned models to the micro-mesh representation, providing an open-source reference implementation, and, as additional material, the data and an inspection tool.},
  archive      = {J_TOG},
  author       = {Andrea Maggiordomo and Henry Moreton and Marco Tarini},
  doi          = {10.1145/3592440},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {121:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Micro-mesh construction},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolutionary piecewise developable approximations.
<em>TOG</em>, <em>42</em>(4), 120:1–14. (<a
href="https://doi.org/10.1145/3592140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method to compute high-quality piecewise developable approximations for triangular meshes. Central to our approach is an evolutionary genetic algorithm for optimizing the combinatorial and discontinuous fitness function, including the approximation error, the number of patches, the patch boundary length, and the penalty for small patches and narrow regions within patches. The genetic algorithm&#39;s operations (i.e., initialization, selection, mutation, and crossover) are explicitly designed to minimize the fitness function. The main challenge is evaluating the fitness function&#39;s approximation error as it requires developable patches, which are difficult or time-consuming to obtain. Resolving the challenge is based on a critical observation: the approximation error and the mapping distortion between an input surface and its developable approximation are positively correlated empirically. To efficiently measure distortion without explicitly generating developable shapes, we creatively use conformal mapping techniques. Then, we control the mapping distortion at a relatively low level to achieve high shape similarity in the genetic algorithm. The feasibility and effectiveness of our method are demonstrated over 240 complex examples. Compared with the state-of-the-art methods, our results have much smaller approximation errors, fewer patches, shorter patch boundaries, and fewer small patches and narrow regions.},
  archive      = {J_TOG},
  author       = {Zheng-Yu Zhao and Mo Li and Zheng Zhang and Qing Fang and Ligang Liu and Xiao-Ming Fu},
  doi          = {10.1145/3592140},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {120:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Evolutionary piecewise developable approximations},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust low-poly meshing for general 3D models. <em>TOG</em>,
<em>42</em>(4), 119:1–20. (<a
href="https://doi.org/10.1145/3592396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust re-meshing approach that can automatically generate visual-preserving low-poly meshes for any high-poly models found in the wild. Our method can be seamlessly integrated into current mesh-based 3D asset production pipelines. Given an input high-poly, our method proceeds in two stages: 1) Robustly extracting an offset surface mesh that is feature-preserving, and guaranteed to be watertight, manifold, and self-intersection free; 2) Progressively simplifying and flowing the offset mesh to bring it close to the input. The simplicity and the visual-preservation of the generated low-poly is controlled by a user-required target screen size of the input: decreasing the screen size reduces the element count of the low-poly but enlarges its visual difference from the input. We have evaluated our method on a subset of the Thingi10K dataset that contains models created by practitioners in different domains, with varying topological and geometric complexities. Compared to state-of-the-art approaches and widely used software, our method demonstrates its superiority in terms of the element count, visual preservation, geometry, and topology guarantees of the generated low-polys.},
  archive      = {J_TOG},
  author       = {Zhen Chen and Zherong Pan and Kui Wu and Etienne Vouga and Xifeng Gao},
  doi          = {10.1145/3592396},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {119:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Robust low-poly meshing for general 3D models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surface simplification using intrinsic error metrics.
<em>TOG</em>, <em>42</em>(4), 118:1–17. (<a
href="https://doi.org/10.1145/3592403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a method for fast simplification of surface meshes. Whereas past methods focus on visual appearance, our goal is to solve equations on the surface. Hence, rather than approximate the extrinsic geometry, we construct a coarse intrinsic triangulation of the input domain. In the spirit of the quadric error metric (QEM) , we perform greedy decimation while agglomerating global information about approximation error. In lieu of extrinsic quadrics, however, we store intrinsic tangent vectors that track how far curvature &quot;drifts&quot; during simplification. This process also yields a bijective map between the fine and coarse mesh, and prolongation operators for both scalar- and vector-valued data. Moreover, we obtain hard guarantees on element quality via intrinsic retriangulation---a feature unique to the intrinsic setting. The overall payoff is a &quot;black box&quot; approach to geometry processing, which decouples mesh resolution from the size of matrices used to solve equations. We show how our method benefits several fundamental tasks, including geometric multigrid, all-pairs geodesic distance, mean curvature flow, geodesic Voronoi diagrams, and the discrete exponential map.},
  archive      = {J_TOG},
  author       = {Hsueh-Ti Derek Liu and Mark Gillespie and Benjamin Chislett and Nicholas Sharp and Alec Jacobson and Keenan Crane},
  doi          = {10.1145/3592403},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {118:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Surface simplification using intrinsic error metrics},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inkjet 4D print: Self-folding tessellated origami objects by
inkjet UV printing. <em>TOG</em>, <em>42</em>(4), 117:1–13. (<a
href="https://doi.org/10.1145/3592409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Inkjet 4D Print, a self-folding fabrication method of 3D origami tessellations by printing 2D patterns on both sides of a heat-shrinkable base sheet, using a commercialized inkjet ultraviolet (UV) printer. Compared to the previous folding-based 4D printing approach using fused deposition modeling (FDM) 3D printers [An et al. 2018], our method has merits in (1) more than 1200 times higher resolution in terms of the number of self-foldable facets, (2) 2.8 times faster printing speed, and (3) optional full-color decoration. This paper describes the material selection, the folding mechanism, the heating condition, and the printing patterns to self-fold both known and freeform tessellations. We also evaluated the self-folding resolution, the printing and transformation speed, and the shape accuracy of our method. Finally, we demonstrated applications enabled by our self-foldable tessellated objects.},
  archive      = {J_TOG},
  author       = {Koya Narumi and Kazuki Koyama and Kai Suto and Yuta Noma and Hiroki Sato and Tomohiro Tachi and Masaaki Sugimoto and Takeo Igarashi and Yoshihiro Kawahara},
  doi          = {10.1145/3592409},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {117:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Inkjet 4D print: Self-folding tessellated origami objects by inkjet UV printing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative design of sheet metal structures. <em>TOG</em>,
<em>42</em>(4), 116:1–13. (<a
href="https://doi.org/10.1145/3592444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sheet Metal (SM) fabrication is perhaps one of the most common metalworking technique. Despite its prevalence, SM design is manual and costly, with rigorous practices that restrict the search space, yielding suboptimal results. In contrast, we present a framework for the first automatic design of SM parts. Focusing on load bearing applications, our novel system generates a high-performing manufacturable SM that adheres to the numerous constraints that SM design entails: The resulting part minimizes manufacturing costs while adhering to structural, spatial, and manufacturing constraints. In other words, the part should be strong enough, not disturb the environment, and adhere to the manufacturing process. These desiderata sum up to an elaborate, sparse, and expensive search space. Our generative approach is a carefully designed exploration process, comprising two steps. In Segment Discovery connections from the input load to attachable regions are accumulated, and during Segment Composition the most performing valid combination is searched for. For Discovery, we define a slim grammar, and sample it for parts using a Markov-Chain Monte Carlo (MCMC) approach, ran in intercommunicating instances (i.e, chains) for diversity. This, followed by a short continuous optimization, enables building a diverse and high-quality library of substructures. During Composition, a valid and minimal cost combination of the curated substructures is selected. To improve compliance significantly without additional manufacturing costs, we reinforce candidate parts onto themselves --- a unique SM capability called self-riveting. we provide our code and data in https://github.com/amir90/AutoSheetMetal. We show our generative approach produces viable parts for numerous scenarios. We compare our system against a human expert and observe improvements in both part quality and design time. We further analyze our pipeline&#39;s steps with respect to resulting quality, and have fabricated some results for validation. We hope our system will stretch the field of SM design, replacing costly expert hours with minutes of standard CPU, making this cheap and reliable manufacturing method accessible to anyone.},
  archive      = {J_TOG},
  author       = {Amir Barda and Guy Tevet and Adriana Schulz and Amit Haim Bermano},
  doi          = {10.1145/3592444},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {116:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generative design of sheet metal structures},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Masonry shell structures with discrete equivalence classes.
<em>TOG</em>, <em>42</em>(4), 115:1–12. (<a
href="https://doi.org/10.1145/3592095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a method to model masonry shell structures where the shell elements fall into a set of discrete equivalence classes. Such shell structure can reduce the fabrication cost and simplify the physical construction due to reuse of a few template shell elements. Given a freeform surface, our goal is to generate a small set of template shell elements that can be reused to produce a seamless and buildable structure that closely resembles the surface. The major technical challenge in this process is balancing the desire for high reusability of template elements with the need for a seamless and buildable final structure. To address the challenge, we define three error metrics to measure the seamlessness and buildability of shell structures made from discrete equivalence classes and develop a hierarchical cluster-and-optimize approach to generate a small set of template elements that produce a structure closely approximating the surface with low error metrics. We demonstrate the feasibility of our approach on various freeform surfaces and geometric patterns, and validate buildability of our results with four physical prototypes. Code and data of this paper are at https://github.com/Linsanity81/TileableShell.},
  archive      = {J_TOG},
  author       = {Rulin Chen and Pengyun Qiu and Peng Song and Bailin Deng and Ziqi Wang and Ying He},
  doi          = {10.1145/3592095},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {115:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Masonry shell structures with discrete equivalence classes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeRO: Neural geometry and BRDF reconstruction of reflective
objects from multiview images. <em>TOG</em>, <em>42</em>(4), 114:1–22.
(<a href="https://doi.org/10.1145/3592134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a neural rendering-based method called NeRO for reconstructing the geometry and the BRDF of reflective objects from multiview images captured in an unknown environment. Multiview reconstruction of reflective objects is extremely challenging because specular reflections are view-dependent and thus violate the multiview consistency, which is the cornerstone for most multiview reconstruction methods. Recent neural rendering techniques can model the interaction between environment lights and the object surfaces to fit the view-dependent reflections, thus making it possible to reconstruct reflective objects from multiview images. However, accurately modeling environment lights in the neural rendering is intractable, especially when the geometry is unknown. Most existing neural rendering methods, which can model environment lights, only consider direct lights and rely on object masks to reconstruct objects with weak specular reflections. Therefore, these methods fail to reconstruct reflective objects, especially when the object mask is not available and the object is illuminated by indirect lights. We propose a two-step approach to tackle this problem. First, by applying the split-sum approximation and the integrated directional encoding to approximate the shading effects of both direct and indirect lights, we are able to accurately reconstruct the geometry of reflective objects without any object masks. Then, with the object geometry fixed, we use more accurate sampling to recover the environment lights and the BRDF of the object. Extensive experiments demonstrate that our method is capable of accurately reconstructing the geometry and the BRDF of reflective objects from only posed RGB images without knowing the environment lights and the object masks. Codes and datasets are available at https://github.com/liuyuan-pal/NeRO.},
  archive      = {J_TOG},
  author       = {Yuan Liu and Peng Wang and Cheng Lin and Xiaoxiao Long and Jiepeng Wang and Lingjie Liu and Taku Komura and Wenping Wang},
  doi          = {10.1145/3592134},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {114:1–22},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeRO: Neural geometry and BRDF reconstruction of reflective objects from multiview images},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural volumetric reconstruction for coherent synthetic
aperture sonar. <em>TOG</em>, <em>42</em>(4), 113:1–20. (<a
href="https://doi.org/10.1145/3592141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic aperture sonar (SAS) measures a scene from multiple views in order to increase the resolution of reconstructed imagery. Image reconstruction methods for SAS coherently combine measurements to focus acoustic energy onto the scene. However, image formation is typically under-constrained due to a limited number of measurements and bandlimited hardware, which limits the capabilities of existing reconstruction methods. To help meet these challenges, we design an analysis-by-synthesis optimization that leverages recent advances in neural rendering to perform coherent SAS imaging. Our optimization enables us to incorporate physics-based constraints and scene priors into the image formation process. We validate our method on simulation and experimental results captured in both air and water. We demonstrate both quantitatively and qualitatively that our method typically produces superior reconstructions than existing approaches. We share code and data for reproducibility.},
  archive      = {J_TOG},
  author       = {Albert Reed and Juhyeon Kim and Thomas Blanford and Adithya Pediredla and Daniel Brown and Suren Jayasuriya},
  doi          = {10.1145/3592141},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {113:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural volumetric reconstruction for coherent synthetic aperture sonar},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locally meshable frame fields. <em>TOG</em>, <em>42</em>(4),
112:1–20. (<a href="https://doi.org/10.1145/3592457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main robustness issue of state-of-the-art frame field based hexahedral mesh generation algorithms originates from non-meshable topological configurations, which do not admit the construction of an integer-grid map but frequently occur in smooth frame fields. In this article, we investigate the topology of frame fields and derive conditions on their meshability, which are the basis for a novel algorithm to automatically turn a given non-meshable frame field into a similar but locally meshable one. Despite local meshability is only a necessary but not sufficient condition for the stronger requirement of meshability, our algorithm increases the 2\% success rate of generating valid integer-grid maps with state-of-the-art methods to 58\%, when compared on the challenging HexMe dataset [Beaufort et al. 2022]. The source code of our implementation and the data of our experiments are available at https://lib.algohex.eu.},
  archive      = {J_TOG},
  author       = {Heng Liu and David Bommes},
  doi          = {10.1145/3592457},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {112:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Locally meshable frame fields},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally consistent normal orientation for point clouds by
regularizing the winding-number field. <em>TOG</em>, <em>42</em>(4),
111:1–15. (<a href="https://doi.org/10.1145/3592129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating normals with globally consistent orientations for a raw point cloud has many downstream geometry processing applications. Despite tremendous efforts in the past decades, it remains challenging to deal with an unoriented point cloud with various imperfections, particularly in the presence of data sparsity coupled with nearby gaps or thin-walled structures. In this paper, we propose a smooth objective function to characterize the requirements of an acceptable winding-number field, which allows one to find the globally consistent normal orientations starting from a set of completely random normals. By taking the vertices of the Voronoi diagram of the point cloud as examination points, we consider the following three requirements: (1) the winding number is either 0 or 1, (2) the occurrences of 1 and the occurrences of 0 are balanced around the point cloud, and (3) the normals align with the outside Voronoi poles as much as possible. Extensive experimental results show that our method outperforms the existing approaches, especially in handling sparse and noisy point clouds, as well as shapes with complex geometry/topology.},
  archive      = {J_TOG},
  author       = {Rui Xu and Zhiyang Dou and Ningna Wang and Shiqing Xin and Shuangmin Chen and Mingyan Jiang and Xiaohu Guo and Wenping Wang and Changhe Tu},
  doi          = {10.1145/3592129},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {111:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Globally consistent normal orientation for point clouds by regularizing the winding-number field},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pyramid texture filtering. <em>TOG</em>, <em>42</em>(4),
110:1–11. (<a href="https://doi.org/10.1145/3592120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a simple but effective technique to smooth out textures while preserving the prominent structures. Our method is built upon a key observation---the coarsest level in a Gaussian pyramid often naturally eliminates textures and summarizes the main image structures. This inspires our central idea for texture filtering, which is to progressively upsample the very low-resolution coarsest Gaussian pyramid level to a full-resolution texture smoothing result with well-preserved structures, under the guidance of each fine-scale Gaussian pyramid level and its associated Laplacian pyramid level. We show that our approach is effective to separate structure from texture of different scales, local contrasts, and forms, without degrading structures or introducing visual artifacts. We also demonstrate the applicability of our method on various applications including detail enhancement, image abstraction, HDR tone mapping, inverse halftoning, and LDR image enhancement. Code is available at https://rewindl.github.io/pyramid_texture_filtering/.},
  archive      = {J_TOG},
  author       = {Qing Zhang and Hao Jiang and Yongwei Nie and Wei-Shi Zheng},
  doi          = {10.1145/3592120},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {110:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Pyramid texture filtering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A full-wave reference simulator for computing surface
reflectance. <em>TOG</em>, <em>42</em>(4), 109:1–17. (<a
href="https://doi.org/10.1145/3592414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing light reflection from rough surfaces is an important topic in computer graphics. Reflection models developed based on geometric optics fail to capture wave effects such as diffraction and interference, while existing models based on physical optics approximations give erroneous predictions under many circumstances (e.g. when multiple scattering from the surface cannot be ignored). We present a scalable 3D full-wave simulator for computing reference solutions to surface scattering problems, which can be used to evaluate and guide the development of approximate models for rendering. We investigate the range of validity for some existing wave optics based reflection models; our results confirm these models for low-roughness surfaces but also show that prior rendering methods do not accurately predict the scattering behavior of some types of surfaces. Our simulator is based on the boundary element method (BEM) and accelerated using the adaptive integral method (AIM), and is implemented to execute on modern GPUs. We demonstrate the simulator on domains up to 60 × 60 × 10 wavelengths, involving surface samples with significant height variations. Furthermore, we propose a new system for efficiently computing BRDF values for large numbers of incident and outgoing directions at once, by combining small simulations to characterize larger areas. Our simulator will be released as an open-source toolkit for computing surface scattering.},
  archive      = {J_TOG},
  author       = {Yunchen Yu and Mengqi Xia and Bruce Walter and Eric Michielssen and Steve Marschner},
  doi          = {10.1145/3592414},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {109:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {A full-wave reference simulator for computing surface reflectance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Second-order stencil descent for interior-point
hyperelasticity. <em>TOG</em>, <em>42</em>(4), 108:1–16. (<a
href="https://doi.org/10.1145/3592104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a GPU algorithm for finite element hyperelastic simulation. We show that the interior-point method, known to be effective for robust collision resolution, can be coupled with non-Newton procedures and be massively sped up on the GPU. Newton&#39;s method has been widely chosen for the interior-point family, which fully solves a linear system at each step. After that, the active set associated with collision/contact constraints is updated. Mimicking this routine using a non-Newton optimization (like gradient descent or ADMM) unfortunately does not deliver expected accelerations. This is because the barrier functions employed in an interior-point method need to be updated at every iteration to strictly confine the search to the feasible region. The associated cost (e.g., per-iteration CCD) quickly overweights the benefit brought by the GPU, and a new parallelism modality is needed. Our algorithm is inspired by the domain decomposition method and designed to move interior-point-related computations to local domains as much as possible. We minimize the size of each domain (i.e., a stencil) by restricting it to a single element, so as to fully exploit the capacity of modern GPUs. The stencil-level results are integrated into a global update using a novel hybrid sweep scheme. Our algorithm is locally second-order offering better convergence. It enables simulation acceleration of up to two orders over its CPU counterpart. We demonstrate the scalability, robustness, efficiency, and quality of our algorithm in a variety of simulation scenarios with complex and detailed collision geometries.},
  archive      = {J_TOG},
  author       = {Lei Lan and Minchen Li and Chenfanfu Jiang and Huamin Wang and Yin Yang},
  doi          = {10.1145/3592104},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {108:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Second-order stencil descent for interior-point hyperelasticity},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motion from shape change. <em>TOG</em>, <em>42</em>(4),
107:1–11. (<a href="https://doi.org/10.1145/3592417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider motion effected by shape change. Such motions are ubiquitous in nature and the human made environment, ranging from single cells to platform divers and jellyfish. The shapes may be immersed in various media ranging from the very viscous to air and nearly inviscid fluids. In the absence of external forces these settings are characterized by constant momentum. We exploit this in an algorithm which takes a sequence of changing shapes, say, as modeled by an animator, as input and produces corresponding motion in world coordinates. Our method is based on the geometry of shape change and an appropriate variational principle. The corresponding Euler-Lagrange equations are first order ODEs in the unknown rotations and translations and the resulting time stepping algorithm applies to all these settings without modification as we demonstrate with a broad set of examples.},
  archive      = {J_TOG},
  author       = {Oliver Gross and Yousuf Soliman and Marcel Padilla and Felix Knöppel and Ulrich Pinkall and Peter Schröder},
  doi          = {10.1145/3592417},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {107:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Motion from shape change},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast complementary dynamics via skinning eigenmodes.
<em>TOG</em>, <em>42</em>(4), 106:1–21. (<a
href="https://doi.org/10.1145/3592404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a reduced-space elastodynamic solver that is well suited for augmenting rigged character animations with secondary motion. At the core of our method is a novel deformation subspace based on Linear Blend Skinning that overcomes many of the shortcomings prior subspace methods face. Our skinning subspace is parameterized entirely by a set of scalar weights, which we can obtain through a small, material-aware and rig-sensitive generalized eigenvalue problem. The resulting subspace can easily capture rotational motion and guarantees that the resulting simulation is rotation equivariant. We further propose a simple local-global solver for linear co-rotational elasticity and propose a clustering method to aggregate per-tetrahedra nonlinear energetic quantities. The result is a compact simulation that is fully decoupled from the complexity of the mesh.},
  archive      = {J_TOG},
  author       = {Otman Benchekroun and Jiayi Eris Zhang and Siddartha Chaudhuri and Eitan Grinspun and Yi Zhou and Alec Jacobson},
  doi          = {10.1145/3592404},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {106:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {Fast complementary dynamics via skinning eigenmodes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ∇-prox: Differentiable proximal algorithm modeling for
large-scale optimization. <em>TOG</em>, <em>42</em>(4), 105:1–19. (<a
href="https://doi.org/10.1145/3592144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tasks across diverse application domains can be posed as large-scale optimization problems, these include graphics, vision, machine learning, imaging, health, scheduling, planning, and energy system forecasting. Independently of the application domain, proximal algorithms have emerged as a formal optimization method that successfully solves a wide array of existing problems, often exploiting problem-specific structures in the optimization. Although model-based formal optimization provides a principled approach to problem modeling with convergence guarantees, at first glance, this seems to be at odds with black-box deep learning methods. A recent line of work shows that, when combined with learning-based ingredients, model-based optimization methods are effective, interpretable, and allow for generalization to a wide spectrum of applications with little or no extra training data. However, experimenting with such hybrid approaches for different tasks by hand requires domain expertise in both proximal optimization and deep learning, which is often error-prone and time-consuming. Moreover, naively unrolling these iterative methods produces lengthy compute graphs, which when differentiated via autograd techniques results in exploding memory consumption, making batch-based training challenging. In this work, we introduce ∇-Prox, a domain-specific modeling language and compiler for large-scale optimization problems using differentiable proximal algorithms. ∇-Prox allows users to specify optimization objective functions of unknowns concisely at a high level, and intelligently compiles the problem into compute and memory-efficient differentiable solvers. One of the core features of ∇-Prox is its full differentiability, which supports hybrid model- and learning-based solvers integrating proximal optimization with neural network pipelines. Example applications of this methodology include learning-based priors and/or sample-dependent inner-loop optimization schedulers, learned with deep equilibrium learning or deep reinforcement learning. With a few lines of code, we show ∇-Prox can generate performant solvers for a range of image optimization problems, including end-to-end computational optics, image deraining, and compressive magnetic resonance imaging. We also demonstrate ∇-Prox can be used in a completely orthogonal application domain of energy system planning, an essential task in the energy crisis and the clean energy transition, where it outperforms state-of-the-art CVXPY and commercial Gurobi solvers.},
  archive      = {J_TOG},
  author       = {Zeqiang Lai and Kaixuan Wei and Ying Fu and Philipp Härtel and Felix Heide},
  doi          = {10.1145/3592144},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {105:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {∇-prox: Differentiable proximal algorithm modeling for large-scale optimization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). B-rep matching for collaborating across CAD systems.
<em>TOG</em>, <em>42</em>(4), 104:1–13. (<a
href="https://doi.org/10.1145/3592125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Computer-Aided Design (CAD) projects usually require collaboration across many different CAD systems as well as applications that interoperate with them for manufacturing, visualization, or simulation. A fundamental barrier to such collaborations is the ability to refer to parts of the geometry (such as a specific face) robustly under geometric and/or topological changes to the model. Persistent referencing schemes are a fundamental aspect of most CAD tools, but models that are shared across systems cannot generally make use of these internal referencing mechanisms, creating a challenge for collaboration. In this work, we address this issue by developing a novel learning-based algorithm that can automatically find correspondences between two CAD models using the standard representation used for sharing models across CAD systems: the Boundary-Representation (B-rep). Because our method works directly on B-reps it can be generalized across different CAD applications enabling collaboration.},
  archive      = {J_TOG},
  author       = {Benjamin Jones and James Noeckel and Milin Kodnongbua and Ilya Baran and Adriana Schulz},
  doi          = {10.1145/3592125},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {104:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {B-rep matching for collaborating across CAD systems},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deployable strip structures. <em>TOG</em>, <em>42</em>(4),
103:1–16. (<a href="https://doi.org/10.1145/3592393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the new concept of C-mesh to capture kinetic structures that can be deployed from a collapsed state. Quadrilateral C-meshes enjoy rich geometry and surprising relations with differential geometry: A structure that collapses onto a flat and straight strip corresponds to a Chebyshev net of curves on a surface of constant Gaussian curvature, while structures collapsing onto a circular strip follow surfaces which enjoy the linear-Weingarten property. Interestingly, allowing more general collapses actually leads to a smaller class of shapes. Hexagonal C-meshes have more degrees of freedom, but a local analysis suggests that there is no such direct relation to smooth surfaces. Besides theory, this paper provides tools for exploring the shape space of C-meshes and for their design. We also present an application for freeform architectural skins, namely paneling with spherical panels of constant radius, which is an important fabrication-related constraint.},
  archive      = {J_TOG},
  author       = {Daoming Liu and Davide Pellis and Yu-Chou Chiang and Florian Rist and Johannes Wallner and Helmut Pottmann},
  doi          = {10.1145/3592393},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {103:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deployable strip structures},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable stripe patterns for inverse design of
structured surfaces. <em>TOG</em>, <em>42</em>(4), 102:1–14. (<a
href="https://doi.org/10.1145/3592114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stripe patterns are ubiquitous in nature and everyday life. While the synthesis of these patterns has been thoroughly studied in the literature, their potential to control the mechanics of structured materials remains largely unexplored. In this work, we introduce Differentiable Stripe Patterns---a computational approach for automated design of physical surfaces structured with stripe-shaped bi-material distributions. Our method builds on the work by Knöppel and colleagues [2015] for generating globally-continuous and equally-spaced stripe patterns. To unlock the full potential of this design space, we propose a gradient-based optimization tool to automatically compute stripe patterns that best approximate macromechanical performance goals. Specifically, we propose a computational model that combines solid shell finite elements with XFEM for accurate and fully-differentiable modeling of elastic bi-material surfaces. To resolve non-uniqueness problems in the original method, we furthermore propose a robust formulation that yields unique and differentiable stripe patterns. We combine these components with equilibrium state derivatives into an end-to-end differentiable pipeline that enables inverse design of mechanical stripe patterns. We demonstrate our method on a diverse set of examples that illustrate the potential of stripe patterns as a design space for structured materials. Our simulation results are experimentally validated on physical prototypes.},
  archive      = {J_TOG},
  author       = {Juan Sebastian Montes Maestre and Yinwei Du and Ronan Hinchet and Stelian Coros and Bernhard Thomaszewski},
  doi          = {10.1145/3592114},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {102:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differentiable stripe patterns for inverse design of structured surfaces},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Language-based photo color adjustment for graphic designs.
<em>TOG</em>, <em>42</em>(4), 101:1–16. (<a
href="https://doi.org/10.1145/3592111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjusting the photo color to associate with some design elements is an essential way for a graphic design to effectively deliver its message and make it aesthetically pleasing. However, existing tools and previous works face a dilemma between the ease of use and level of expressiveness. To this end, we introduce an interactive language-based approach for photo recoloring, which provides an intuitive system that can assist both experts and novices on graphic design. Given a graphic design containing a photo that needs to be recolored, our model can predict the source colors and the target regions, and then recolor the target regions with the source colors based on the given language-based instruction. The multi-granularity of the instruction allows diverse user intentions. The proposed novel task faces several unique challenges, including: 1) color accuracy for recoloring with exactly the same color from the target design element as specified by the user; 2) multi-granularity instructions for parsing instructions correctly to generate a specific result or multiple plausible ones; and 3) locality for recoloring in semantically meaningful local regions to preserve original image semantics. To address these challenges, we propose a model called LangRecol with two main components: the language-based source color prediction module and the semantic-palette-based photo recoloring module. We also introduce an approach for generating a synthetic graphic design dataset with instructions to enable model training. We evaluate our model via extensive experiments and user studies. We also discuss several practical applications, showing the effectiveness and practicality of our approach. Please find the code and data at https://zhenwwang.github.io/langrecol.},
  archive      = {J_TOG},
  author       = {Zhenwei Wang and Nanxuan Zhao and Gerhard Hancke and Rynson W. H. Lau},
  doi          = {10.1145/3592111},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {101:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Language-based photo color adjustment for graphic designs},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guided linear upsampling. <em>TOG</em>, <em>42</em>(4),
100:1–12. (<a href="https://doi.org/10.1145/3592453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Guided upsampling is an effective approach for accelerating high-resolution image processing. In this paper, we propose a simple yet effective guided upsampling method. Each pixel in the high-resolution image is represented as a linear interpolation of two low-resolution pixels, whose indices and weights are optimized to minimize the upsampling error. The downsampling can be jointly optimized in order to prevent missing small isolated regions. Our method can be derived from the color line model and local color transformations. Compared to previous methods, our method can better preserve detail effects while suppressing artifacts such as bleeding and blurring. It is efficient, easy to implement, and free of sensitive parameters. We evaluate the proposed method with a wide range of image operators, and show its advantages through quantitative and qualitative analysis. We demonstrate the advantages of our method for both interactive image editing and real-time high-resolution video processing. In particular, for interactive editing, the joint optimization can be precomputed, thus allowing for instant feedback without hardware acceleration.},
  archive      = {J_TOG},
  author       = {Shuangbing Song and Fan Zhong and Tianju Wang and Xueying Qin and Changhe Tu},
  doi          = {10.1145/3592453},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {100:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Guided linear upsampling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seeing photons in color. <em>TOG</em>, <em>42</em>(4),
99:1–16. (<a href="https://doi.org/10.1145/3592438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Megapixel single-photon avalanche diode (SPAD) arrays have been developed recently, opening up the possibility of deploying SPADs as generalpurpose passive cameras for photography and computer vision. However, most previous work on SPADs has been limited to monochrome imaging. We propose a computational photography technique that reconstructs high-quality color images from mosaicked binary frames captured by a SPAD array, even for high-dyanamic-range (HDR) scenes with complex and rapid motion. Inspired by conventional burst photography approaches, we design algorithms that jointly denoise and demosaick single-photon image sequences. Based on the observation that motion effectively increases the color sample rate, we design a blue-noise pseudorandom RGBW color filter array for SPADs, which is tailored for imaging dark, dynamic scenes. Results on simulated data, as well as real data captured with a fabricated color SPAD hardware prototype shows that the proposed method can reconstruct high-quality images with minimal color artifacts even for challenging low-light, HDR and fast-moving scenes. We hope that this paper, by adding color to computational single-photon imaging, spurs rapid adoption of SPADs for real-world passive imaging applications.},
  archive      = {J_TOG},
  author       = {Sizhuo Ma and Varun Sundar and Paul Mos and Claudio Bruschini and Edoardo Charbon and Mohit Gupta},
  doi          = {10.1145/3592438},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {99:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Seeing photons in color},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ColorfulCurves: Palette-aware lightness control and color
editing via sparse optimization. <em>TOG</em>, <em>42</em>(4), 98:1–12.
(<a href="https://doi.org/10.1145/3592405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color editing in images often consists of two main tasks: changing hue and saturation, and editing lightness or tone curves. State-of-the-art palette-based recoloring approaches entangle these two tasks. A user&#39;s only lightness control is changing the lightness of individual palette colors. This is inferior to state-of-the-art commercial software, where lightness editing is based on flexible tone curves that remap lightness. However, tone curves are only provided globally or per color channel (e.g., RGB). They are unrelated to the image content. Neither tone curves nor palette-based approaches support direct image-space edits---changing a specific pixel to a desired hue, saturation, and lightness. ColorfulCurves solves both of these problems by uniting palette-based and tone curve editing. In ColorfulCurves , users directly edit palette colors&#39; hue and saturation, per-palette tone curves, or image pixels (hue, saturation, and lightness). ColorfulCurves solves an L 2,1 optimization problem in real-time to find a sparse edit that satisfies all user constraints. Our expert study found overwhelming support for ColorfulCurves over experts&#39; preferred tools.},
  archive      = {J_TOG},
  author       = {Cheng-Kang Ted Chao and Jason Klein and Jianchao Tan and Jose Echevarria and Yotam Gingold},
  doi          = {10.1145/3592405},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {98:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {ColorfulCurves: Palette-aware lightness control and color editing via sparse optimization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image vectorization and editing via linear gradient layer
decomposition. <em>TOG</em>, <em>42</em>(4), 97:1–13. (<a
href="https://doi.org/10.1145/3592128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key advantage of vector graphics over raster graphics is their editability. For example, linear gradients define a spatially varying color fill with a few intuitive parameters, which are ubiquitously supported in standard vector graphics formats and libraries. By layering regions filled with linear gradients, complex appearances can be created. We propose an automatic method to convert a raster image into layered regions of linear gradients. Given an input raster image segmented into regions, our approach decomposes the resulting regions into opaque and semi-transparent linear gradient fills. Our approach is fully automatic (e.g., users do not identify a background as in previous approaches) and exhaustively considers all possible decompositions that satisfy perceptual cues. Experiments on a variety of images demonstrate that our method is robust and effective.},
  archive      = {J_TOG},
  author       = {Zheng-Jun Du and Liang-Fu Kang and Jianchao Tan and Yotam Gingold and Kun Xu},
  doi          = {10.1145/3592128},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {97:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Image vectorization and editing via linear gradient layer decomposition},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DOC: Differentiable optimal control for retargeting motions
onto legged robots. <em>TOG</em>, <em>42</em>(4), 96:1–14. (<a
href="https://doi.org/10.1145/3592454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legged robots are designed to perform highly dynamic motions. However, it remains challenging for users to retarget expressive motions onto these complex systems. In this paper, we present a Differentiable Optimal Control (DOC) framework that facilitates the transfer of rich motions from either animals or animations onto these robots. Interfacing with either motion capture or animation data, we formulate retargeting objectives whose parameters make them agnostic to differences in proportions and numbers of degrees of freedom between input and robot. Optimizing these parameters over the manifold spanned by optimal state and control trajectories, we minimize the retargeting error. We demonstrate the utility and efficacy of our modeling by applying DOC to a Model-Predictive Control (MPC) formulation, showing retargeting results for a family of robots of varying proportions and mass distribution. With a hardware deployment, we further show that the retargeted motions are physically feasible, while MPC ensures that the robots retain their capability to react to unexpected disturbances.},
  archive      = {J_TOG},
  author       = {Ruben Grandia and Farbod Farshidian and Espen Knoop and Christian Schumacher and Marco Hutter and Moritz Bächer},
  doi          = {10.1145/3592454},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {96:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {DOC: Differentiable optimal control for retargeting motions onto legged robots},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning physically simulated tennis skills from broadcast
videos. <em>TOG</em>, <em>42</em>(4), 95:1–14. (<a
href="https://doi.org/10.1145/3592408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a system that learns diverse, physically simulated tennis skills from large-scale demonstrations of tennis play harvested from broadcast videos. Our approach is built upon hierarchical models, combining a low-level imitation policy and a high-level motion planning policy to steer the character in a motion embedding learned from broadcast videos. When deployed at scale on large video collections that encompass a vast set of examples of real-world tennis play, our approach can learn complex tennis shotmaking skills and realistically chain together multiple shots into extended rallies, using only simple rewards and without explicit annotations of stroke types. To address the low quality of motions extracted from broadcast videos, we correct estimated motion with physics-based imitation, and use a hybrid control policy that overrides erroneous aspects of the learned motion embedding with corrections predicted by the high-level policy. We demonstrate that our system produces controllers for physically-simulated tennis players that can hit the incoming ball to target positions accurately using a diverse array of strokes (serves, forehands, and backhands), spins (topspins and slices), and playing styles (one/two-handed backhands, left/right-handed play). Overall, our system can synthesize two physically simulated characters playing extended tennis rallies with simulated racket and ball dynamics. Code and data for this work is available at https://research.nvidia.com/labs/toronto-ai/vid2player3d/.},
  archive      = {J_TOG},
  author       = {Haotian Zhang and Ye Yuan and Viktor Makoviychuk and Yunrong Guo and Sanja Fidler and Xue Bin Peng and Kayvon Fatahalian},
  doi          = {10.1145/3592408},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {95:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Learning physically simulated tennis skills from broadcast videos},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Example-based motion synthesis via generative motion
matching. <em>TOG</em>, <em>42</em>(4), 94:1–12. (<a
href="https://doi.org/10.1145/3592395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GenMM, a generative model that &quot;mines&quot; as many diverse motions as possible from a single or few example sequences. In stark contrast to existing data-driven methods, which typically require long offline training time, are prone to visual artifacts, and tend to fail on large and complex skeletons, GenMM inherits the training-free nature and the superior quality of the well-known Motion Matching method. GenMM can synthesize a high-quality motion within a fraction of a second, even with highly complex and large skeletal structures. At the heart of our generative framework lies the generative motion matching module, which utilizes the bidirectional visual similarity as a generative cost function to motion matching, and operates in a multi-stage framework to progressively refine a random guess using exemplar motion matches. In addition to diverse motion generation, we show the versatility of our generative framework by extending it to a number of scenarios that are not possible with motion matching alone, including motion completion, key frame-guided generation, infinite looping, and motion reassembly.},
  archive      = {J_TOG},
  author       = {Weiyu Li and Xuelin Chen and Peizhuo Li and Olga Sorkine-Hornung and Baoquan Chen},
  doi          = {10.1145/3592395},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {94:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Example-based motion synthesis via generative motion matching},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Composite motion learning with task control. <em>TOG</em>,
<em>42</em>(4), 93:1–16. (<a
href="https://doi.org/10.1145/3592447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behaviors, we introduce a sample-efficient method for training composite control policies in an incremental manner, where we reuse a pre-trained policy as the meta policy and train a cooperative policy that adapts the meta one for new composite tasks. We show the applicability of our approach on a variety of challenging multi-objective tasks involving both composite motion imitation and multiple goal-directed control. Code is available at https://motion-lab.github.io/CompositeMotion .},
  archive      = {J_TOG},
  author       = {Pei Xu and Xiumin Shang and Victor Zordan and Ioannis Karamouzas},
  doi          = {10.1145/3592447},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {93:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Composite motion learning with task control},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3DShape2VecSet: A 3D shape representation for neural fields
and generative diffusion models. <em>TOG</em>, <em>42</em>(4), 92:1–16.
(<a href="https://doi.org/10.1145/3592442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation, and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation. Code: https://1zb.github.io/3DShape2VecSet/.},
  archive      = {J_TOG},
  author       = {Biao Zhang and Jiapeng Tang and Matthias Nießner and Peter Wonka},
  doi          = {10.1145/3592442},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {92:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {3DShape2VecSet: A 3D shape representation for neural fields and generative diffusion models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locally attentional SDF diffusion for controllable 3D shape
generation. <em>TOG</em>, <em>42</em>(4), 91:1–13. (<a
href="https://doi.org/10.1145/3592103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the recent rapid evolution of 3D generative neural networks greatly improves 3D shape generation, it is still not convenient for ordinary users to create 3D shapes and control the local geometry of generated shapes. To address these challenges, we propose a diffusion-based 3D generation framework --- locally attentional SDF diffusion , to model plausible 3D shapes, via 2D sketch image input. Our method is built on a two-stage diffusion model. The first stage, named occupancy-diffusion , aims to generate a low-resolution occupancy field to approximate the shape shell. The second stage, named SDF-diffusion , synthesizes a high-resolution signed distance field within the occupied voxels determined by the first stage to extract fine geometry. Our model is empowered by a novel view-aware local attention mechanism for image-conditioned shape generation, which takes advantage of 2D image patch features to guide 3D voxel feature learning, greatly improving local controllability and model generalizability. Through extensive experiments in sketch-conditioned and category-conditioned 3D shape generation tasks, we validate and demonstrate the ability of our method to provide plausible and diverse 3D shapes, as well as its superior controllability and generalizability over existing work.},
  archive      = {J_TOG},
  author       = {Xin-Yang Zheng and Hao Pan and Peng-Shuai Wang and Xin Tong and Yang Liu and Heung-Yeung Shum},
  doi          = {10.1145/3592103},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {91:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Locally attentional SDF diffusion for controllable 3D shape generation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An extensible, data-oriented architecture for
high-performance, many-world simulation. <em>TOG</em>, <em>42</em>(4),
90:1–13. (<a href="https://doi.org/10.1145/3592427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training AI agents to perform complex tasks in simulated worlds requires millions to billions of steps of experience. To achieve high performance, today&#39;s fastest simulators for training AI agents adopt the idea of batch simulation: using a single simulation engine to simultaneously step many environments in parallel. We introduce a framework for productively authoring novel training environments (including custom logic for environment generation, environment time stepping, and generating agent observations and rewards) that execute as high-performance, GPU-accelerated batched simulators. Our key observation is that the entity-component-system (ECS) design pattern, popular for expressing CPU-side game logic today, is also well-suited for providing the structure needed for high-performance batched simulators. We contribute the first fully-GPU accelerated ECS implementation that natively supports batch environment simulation. We demonstrate how ECS abstractions impose structure on a training environment&#39;s logic and state that allows the system to efficiently manage state, amortize work, and identify GPU-friendly coherent parallel computations within and across different environments. We implement several learning environments in this framework, and demonstrate GPU speedups of two to three orders of magnitude over open source CPU baselines and 5-33× over strong baselines running on a 32-thread CPU. An implementation of the OpenAI hide and seek 3D environment written in our framework, which performs rigid body physics and ray tracing in each simulator step, achieves over 1.9 million environment steps per second on a single GPU.},
  archive      = {J_TOG},
  author       = {Brennan Shacklett and Luc Guy Rosenzweig and Zhiqiang Xie and Bidipta Sarkar and Andrew Szot and Erik Wijmans and Vladlen Koltun and Dhruv Batra and Kayvon Fatahalian},
  doi          = {10.1145/3592427},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {90:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {An extensible, data-oriented architecture for high-performance, many-world simulation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MERF: Memory-efficient radiance fields for real-time view
synthesis in unbounded scenes. <em>TOG</em>, <em>42</em>(4), 89:1–12.
(<a href="https://doi.org/10.1145/3592426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.},
  archive      = {J_TOG},
  author       = {Christian Reiser and Rick Szeliski and Dor Verbin and Pratul Srinivasan and Ben Mildenhall and Andreas Geiger and Jon Barron and Peter Hedman},
  doi          = {10.1145/3592426},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {89:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {MERF: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random-access neural compression of material textures.
<em>TOG</em>, <em>42</em>(4), 88:1–25. (<a
href="https://doi.org/10.1145/3592407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The continuous advancement of photorealism in rendering is accompanied by a growth in texture data and, consequently, increasing storage and memory demands. To address this issue, we propose a novel neural compression technique specifically designed for material textures. We unlock two more levels of detail, i.e., 16× more texels, using low bitrate compression, with image quality that is better than advanced image compression techniques, such as AVIF and JPEG XL. At the same time, our method allows on-demand, real-time decompression with random access similar to block texture compression on GPUs, enabling compression on disk and memory. The key idea behind our approach is compressing multiple material textures and their mipmap chains together, and using a small neural network, that is optimized for each material, to decompress them. Finally, we use a custom training implementation to achieve practical compression speeds, whose performance surpasses that of general frameworks, like PyTorch, by an order of magnitude.},
  archive      = {J_TOG},
  author       = {Karthik Vaidyanathan and Marco Salvi and Bartlomiej Wronski and Tomas Akenine-Moller and Pontus Ebelin and Aaron Lefohn},
  doi          = {10.1145/3592407},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {88:1–25},
  shortjournal = {ACM Trans. Graph.},
  title        = {Random-access neural compression of material textures},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effect-based multi-viewer caching for cloud-native
rendering. <em>TOG</em>, <em>42</em>(4), 87:1–16. (<a
href="https://doi.org/10.1145/3592431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With cloud computing becoming ubiquitous, it appears as virtually everything can be offered as-a-service. However, real-time rendering in the cloud forms a notable exception, where the cloud adoption stops at running individual game instances in compute centers. In this paper, we explore whether a cloud-native rendering architecture is viable and scales to multi-client rendering scenarios. To this end, we propose world-space and on-surface caches to share rendering computations among viewers placed in the same virtual world. We discuss how caches can be utilized on an effect-basis and demonstrate that a large amount of computations can be saved as the number of viewers in a scene increases. Caches can easily be set up for various effects, including ambient occlusion, direct illumination, and diffuse global illumination. Our results underline that the image quality using cached rendering is on par with screen-space rendering and due to its simplicity and inherent coherence, cached rendering may even have advantages in single viewer setups. Analyzing the runtime and communication costs, we show that cached rendering is already viable in multi-GPU systems. Building on top of our research, cloud-native rendering may be just around the corner.},
  archive      = {J_TOG},
  author       = {Alexander Weinrauch and Wolfgang Tatzgern and Pascal Stadlbauer and Alexis Crickx and Jozef Hladky and Arno Coomans and Martin Winter and Joerg H. Mueller and Markus Steinberger},
  doi          = {10.1145/3592431},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {87:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Effect-based multi-viewer caching for cloud-native rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Potentially visible hidden-volume rendering for multi-view
warping. <em>TOG</em>, <em>42</em>(4), 86:1–11. (<a
href="https://doi.org/10.1145/3592108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the model and rendering algorithm of Potentially Visible Hidden Volumes (PVHVs) for multi-view image warping. PVHVs are 3D volumes that are occluded at a known source view, but potentially visible at novel views. Given a bound of novel views, we define PVHVs using the edges of foreground fragments from the known view and the bound of novel views. PVHVs can be used to batch-test the visibilities of source fragments without iterating individual novel views in multi-fragment rendering, and thereby, cull redundant fragments prior to warping. We realize the model of PVHVs in Depth Peeling (DP). Our Effective Depth Peeling (EDP) can reduce the number of completely hidden fragments, capture important fragments early, and reduce warping cost. We demonstrate the benefit of our PVHVs and EDP in terms of memory, quality, and performance in multi-view warping.},
  archive      = {J_TOG},
  author       = {Janghun Kim and Sungkil Lee},
  doi          = {10.1145/3592108},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {86:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Potentially visible hidden-volume rendering for multi-view warping},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trim regions for online computation of from-region
potentially visible sets. <em>TOG</em>, <em>42</em>(4), 85:1–15. (<a
href="https://doi.org/10.1145/3592434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visibility computation is a key element in computer graphics applications. More specifically, a from-region potentially visible set (PVS) is an established tool in rendering acceleration, but its high computational cost means a from-region PVS is almost always precomputed. Precomputation restricts the use of PVS to static scenes and leads to high storage cost, in particular, if we need fine-grained regions. For dynamic applications, such as streaming content over a variable-bandwidth network, online PVS computation with configurable region size is required. We address this need with trim regions, a new method for generating from-region PVS for arbitrary scenes in real time. Trim regions perform controlled erosion of object silhouettes in image space, implicitly applying the shrinking theorem known from previous work. Our algorithm is the first that applies automatic shrinking to unconstrained 3D scenes, including non-manifold meshes, and does so in real time using an efficient GPU execution model. We demonstrate that our algorithm generates a tight PVS for complex scenes and outperforms previous online methods for from-viewpoint and from-region PVS. It runs at 60 Hz for realistic game scenes consisting of millions of triangles and computes PVS with a tightness matching or surpassing existing approaches.},
  archive      = {J_TOG},
  author       = {Philip Voglreiter and Bernhard Kerbl and Alexander Weinrauch and Joerg Hermann Mueller and Thomas Neff and Markus Steinberger and Dieter Schmalstieg},
  doi          = {10.1145/3592434},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {85:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Trim regions for online computation of from-region potentially visible sets},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Beyond chainmail: Computational modeling of discrete
interlocking materials. <em>TOG</em>, <em>42</em>(4), 84:1–12. (<a
href="https://doi.org/10.1145/3592112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for computational modeling, mechanical characterization, and macro-scale simulation of discrete interlocking materials (DIM)---3D-printed chainmail fabrics made of quasi-rigid interlocking elements. Unlike conventional elastic materials for which deformation and restoring force are directly coupled, the mechanics of DIM are governed by contacts between individual elements that give rise to anisotropic deformation constraints. To model the mechanical behavior of these materials, we propose a computational approach that builds on three key components. ( a ): we explore the space of feasible deformations using native-scale simulations at the per-element level. ( b ): based on this simulation data, we introduce the concept of strain-space boundaries to represent deformation limits for in- and out-of-plane deformations, and ( c ): we use the strain-space boundaries to drive an efficient macro-scale simulation model based on homogenized deformation constraints. We evaluate our method on a set of representative discrete interlocking materials and validate our findings against measurements on physical prototypes.},
  archive      = {J_TOG},
  author       = {Pengbin Tang and Stelian Coros and Bernhard Thomaszewski},
  doi          = {10.1145/3592112},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {84:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Beyond chainmail: Computational modeling of discrete interlocking materials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing shallow water simulations with dispersive
surface waves. <em>TOG</em>, <em>42</em>(4), 83:1–12. (<a
href="https://doi.org/10.1145/3592098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel method for simulating large bodies of water as a height field. At the start of each time step, we partition the waves into a bulk flow (which approximately satisfies the assumptions of the shallow water equations) and surface waves (which approximately satisfy the assumptions of Airy wave theory). We then solve the two wave regimes separately using appropriate state-of-the-art techniques, and re-combine the resulting wave velocities at the end of each step. This strategy leads to the first heightfield wave model capable of simulating complex interactions between both deep and shallow water effects, like the waves from a boat wake sloshing up onto a beach, or a dam break producing wave interference patterns and eddies. We also analyze the numerical dispersion created by our method and derive an exact correction factor for waves at a constant water depth, giving us a numerically perfect re-creation of theoretical water wave dispersion patterns.},
  archive      = {J_TOG},
  author       = {Stefan Jeschke and Chris Wojtan},
  doi          = {10.1145/3592098},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {83:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Generalizing shallow water simulations with dispersive surface waves},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boundary value caching for walk on spheres. <em>TOG</em>,
<em>42</em>(4), 82:1–11. (<a
href="https://doi.org/10.1145/3592400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid-free Monte Carlo methods such as walk on spheres can be used to solve elliptic partial differential equations without mesh generation or global solves. However, such methods independently estimate the solution at every point, and hence do not take advantage of the high spatial regularity of solutions to elliptic problems. We propose a fast caching strategy which first estimates solution values and derivatives at randomly sampled points along the boundary of the domain (or a local region of interest). These cached values then provide cheap, output-sensitive evaluation of the solution (or its gradient) at interior points, via a boundary integral formulation. Unlike classic boundary integral methods, our caching scheme introduces zero statistical bias and does not require a dense global solve. Moreover we can handle imperfect geometry (e.g., with self-intersections) and detailed boundary/source terms without repairing or resampling the boundary representation. Overall, our scheme is similar in spirit to virtual point light methods from photorealistic rendering: it suppresses the typical salt-and-pepper noise characteristic of independent Monte Carlo estimates, while still retaining the many advantages of Monte Carlo solvers: progressive evaluation, trivial parallelization, geometric robustness, etc. We validate our approach using test problems from visual and geometric computing.},
  archive      = {J_TOG},
  author       = {Bailey Miller and Rohan Sawhney and Keenan Crane and Ioannis Gkioulekas},
  doi          = {10.1145/3592400},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {82:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Boundary value caching for walk on spheres},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A practical walk-on-boundary method for boundary value
problems. <em>TOG</em>, <em>42</em>(4), 81:1–16. (<a
href="https://doi.org/10.1145/3592109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the walk-on-boundary (WoB) method for solving boundary value problems to computer graphics. WoB is a grid-free Monte Carlo solver for certain classes of second order partial differential equations. A similar Monte Carlo solver, the walk-on-spheres (WoS) method, has been recently popularized in computer graphics due to its advantages over traditional spatial discretization-based alternatives. We show that WoB&#39;s intrinsic properties yield further advantages beyond those of WoS. Unlike WoS, WoB naturally supports various boundary conditions (Dirichlet, Neumann, Robin, and mixed) for both interior and exterior domains. WoB builds upon boundary integral formulations, and it is mathematically more similar to light transport simulation in rendering than the random walk formulation of WoS. This similarity between WoB and rendering allows us to implement WoB on top of Monte Carlo ray tracing, and to incorporate advanced rendering techniques (e.g., bidirectional estimators with multiple importance sampling, the virtual point lights method, and Markov chain Monte Carlo) into WoB. WoB does not suffer from the intrinsic bias of WoS near the boundary and can estimate solutions precisely on the boundary. Our numerical results highlight the advantages of WoB over WoS as an attractive alternative to solve boundary value problems based on Monte Carlo.},
  archive      = {J_TOG},
  author       = {Ryusuke Sugimoto and Terry Chen and Yiti Jiang and Christopher Batty and Toshiya Hachisuka},
  doi          = {10.1145/3592109},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {81:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {A practical walk-on-boundary method for boundary value problems},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Walk on stars: A grid-free monte carlo method for PDEs with
neumann boundary conditions. <em>TOG</em>, <em>42</em>(4), 80:1–20. (<a
href="https://doi.org/10.1145/3592398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grid-free Monte Carlo methods based on the walk on spheres (WoS) algorithm solve fundamental partial differential equations (PDEs) like the Poisson equation without discretizing the problem domain or approximating functions in a finite basis. Such methods hence avoid aliasing in the solution, and evade the many challenges of mesh generation. Yet for problems with complex geometry, practical grid-free methods have been largely limited to basic Dirichlet boundary conditions. We introduce the walk on stars (WoSt) algorithm, which solves linear elliptic PDEs with arbitrary mixed Neumann and Dirichlet boundary conditions. The key insight is that one can efficiently simulate reflecting Brownian motion (which models Neumann conditions) by replacing the balls used by WoS with star-shaped domains. We identify such domains via the closest point on the visibility silhouette, by simply augmenting a standard bounding volume hierarchy with normal information. Overall, WoSt is an easy modification of WoS, and retains the many attractive features of grid-free Monte Carlo methods such as progressive and view-dependent evaluation, trivial parallelization, and sublinear scaling to increasing geometric detail.},
  archive      = {J_TOG},
  author       = {Rohan Sawhney and Bailey Miller and Ioannis Gkioulekas and Keenan Crane},
  doi          = {10.1145/3592398},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {80:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Walk on stars: A grid-free monte carlo method for PDEs with neumann boundary conditions},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coupling conduction, convection and radiative transfer in a
single path-space: Application to infrared rendering. <em>TOG</em>,
<em>42</em>(4), 79:1–20. (<a
href="https://doi.org/10.1145/3592121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decades, Monte Carlo methods have shown their ability to solve PDEs, independently of the dimensionality of the integration domain and for different use-cases (e.g. light transport, geometry processing, physics simulation). Specifically, the path-space formulation of transport equations is a key ingredient to define tractable and scalable solvers, and we observe nowadays a strong interest in the definition of simulation systems based on Monte Carlo algorithms. We also observe that, when simulating combined physics (e.g. thermal rendering from a heat transfer simulation), there is a lack of coupled Monte Carlo algorithms allowing to solve all the physics at once, in the same path space, rather than combining several independent MC estimators, a combination that would make the global solver critically sensitive to the complexity of each simulation space. This brings to our proposal: a coupled, single path-space, Monte Carlo algorithm for efficient multi-physics problems solving. In this work, we combine our understanding and knowledge of Physics and Computer Graphics to demonstrate how to formulate and arrange different simulation spaces into a single path space. We define a tractable formalism for coupled heat transfer simulation using Monte Carlo, and we leverage the path-space construction to interactively compute multiple simulations with different conditions in the same scene, in terms of boundary conditions and observation time. We validate our proposal in the context of infrared rendering with different thermal simulation scenarios: e.g., room temperature simulation, visualization of heat paths within materials (detection of thermal bridges), heat diffusion capacity of thermal exchanger. We expect that our theoretical framework will foster collaboration and multidisciplinary studies. The perspectives this framework opens are detailed and we suggest a research agenda towards the resolution of coupled PDEs at the interface of Physics and Computer Graphics.},
  archive      = {J_TOG},
  author       = {Mégane Bati and Stéphane Blanco and Christophe Coustet and Vincent Eymet and Vincent Forest and Richard Fournier and Jacques Gautrais and Nicolas Mellado and Mathias Paulin and Benjamin Piaud},
  doi          = {10.1145/3592121},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {79:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Coupling conduction, convection and radiative transfer in a single path-space: Application to infrared rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural prefiltering for correlation-aware levels of detail.
<em>TOG</em>, <em>42</em>(4), 78:1–16. (<a
href="https://doi.org/10.1145/3592443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a practical general-purpose neural appearance filtering pipeline for physically-based rendering. We tackle the previously difficult challenge of aggregating visibility across many levels of detail from local information only, without relying on learning visibility for the entire scene. The high adaptivity of neural representations allows us to retain geometric correlations along rays and thus avoid light leaks. Common approaches to prefiltering decompose the appearance of a scene into volumetric representations with physically-motivated parameters, where the inflexibility of the fitted models limits rendering accuracy. We avoid assumptions on particular types of geometry or materials, bypassing any special-case decompositions. Instead, we directly learn a compressed representation of the intra-voxel light transport. For such high-dimensional functions, neural networks have proven to be useful representations. To satisfy the opposing constraints of prefiltered appearance and correlation-preserving point-to-point visibility, we use two small independent networks on a sparse multi-level voxel grid. Each network requires 10--20 minutes of training to learn the appearance of an asset across levels of detail. Our method achieves 70--95\% compression ratios and around 25\% of quality improvements over previous work. We reach interactive to real-time framerates, depending on the level of detail.},
  archive      = {J_TOG},
  author       = {Philippe Weier and Tobias Zirr and Anton Kaplanyan and Ling-Qi Yan and Philipp Slusallek},
  doi          = {10.1145/3592443},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {78:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Neural prefiltering for correlation-aware levels of detail},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards attention–aware foveated rendering. <em>TOG</em>,
<em>42</em>(4), 77:1–10. (<a
href="https://doi.org/10.1145/3592406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foveated graphics is a promising approach to solving the bandwidth challenges of immersive virtual and augmented reality displays by exploiting the falloff in spatial acuity in the periphery of the visual field. However, the perceptual models used in these applications neglect the effects of higherlevel cognitive processing, namely the allocation of visual attention, and are thus overestimating sensitivity in the periphery in many scenarios. Here, we introduce the first attention-aware model of contrast sensitivity. We conduct user studies to measure contrast sensitivity under different attention distributions and show that sensitivity in the periphery drops significantly when the user is required to allocate attention to the fovea. We motivate the development of future foveation models with another user study and demonstrate that tolerance for foveation in the periphery is significantly higher when the user is concentrating on a task in the fovea. Analysis of our model predicts significant bandwidth savings over those afforded by current models. As such, our work forms the foundation for attention-aware foveated graphics techniques.},
  archive      = {J_TOG},
  author       = {Brooke Krajancich and Petr Kellnhofer and Gordon Wetzstein},
  doi          = {10.1145/3592406},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {77:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Towards attention–aware foveated rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EgoLocate: Real-time motion capture, localization, and
mapping with sparse body-mounted sensors. <em>TOG</em>, <em>42</em>(4),
76:1–17. (<a href="https://doi.org/10.1145/3592099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human and environment sensing are two important topics in Computer Vision and Graphics. Human motion is often captured by inertial sensors, while the environment is mostly reconstructed using cameras. We integrate the two techniques together in EgoLocate, a system that simultaneously performs human motion capture (mocap), localization, and mapping in real time from sparse body-mounted sensors, including 6 inertial measurement units (IMUs) and a monocular phone camera. On one hand, inertial mocap suffers from large translation drift due to the lack of the global positioning signal. EgoLo-cate leverages image-based simultaneous localization and mapping (SLAM) techniquesto locate the human in the reconstructed scene. Onthe other hand, SLAM often fails when the visual feature is poor. EgoLocate involves inertial mocap to provide a strong prior for the camera motion. Experiments show that localization, a key challenge for both two fields, is largely improved by our technique, compared with the state of the art of the two fields. Our codes are available for research at https://xinyu-yi.github.io/EgoLocate/.},
  archive      = {J_TOG},
  author       = {Xinyu Yi and Yuxiao Zhou and Marc Habermann and Vladislav Golyanik and Shaohua Pan and Christian Theobalt and Feng Xu},
  doi          = {10.1145/3592099},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {76:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {EgoLocate: Real-time motion capture, localization, and mapping with sparse body-mounted sensors},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CT2Hair: High-fidelity 3D hair modeling using computed
tomography. <em>TOG</em>, <em>42</em>(4), 75:1–13. (<a
href="https://doi.org/10.1145/3592106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce CT2Hair, a fully automatic framework for creating high-fidelity 3D hair models that are suitable for use in downstream graphics applications. Our approach utilizes real-world hair wigs as input, and is able to reconstruct hair strands for a wide range of hair styles. Our method leverages computed tomography (CT) to create density volumes of the hair regions, allowing us to see through the hair unlike image-based approaches which are limited to reconstructing the visible surface. To address the noise and limited resolution of the input density volumes, we employ a coarse-to-fine approach. This process first recovers guide strands with estimated 3D orientation fields, and then populates dense strands through a novel neural interpolation of the guide strands. The generated strands are then refined to conform to the input density volumes. We demonstrate the robustness of our approach by presenting results on a wide variety of hair styles and conducting thorough evaluations on both real-world and synthetic datasets. Code and data for this paper are at github.com/facebookresearch/CT2Hair.},
  archive      = {J_TOG},
  author       = {Yuefan Shen and Shunsuke Saito and Ziyan Wang and Olivier Maury and Chenglei Wu and Jessica Hodgins and Youyi Zheng and Giljoo Nam},
  doi          = {10.1145/3592106},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {75:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {CT2Hair: High-fidelity 3D hair modeling using computed tomography},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sag-free initialization for strand-based hybrid hair
simulation. <em>TOG</em>, <em>42</em>(4), 74:1–14. (<a
href="https://doi.org/10.1145/3592143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lagrangian/Eulerian hybrid strand-based hair simulation techniques have quickly become a popular approach in VFX and real-time graphics applications. With Lagrangian hair dynamics, the inter-hair contacts are resolved in the Eulerian grid using the continuum method, i.e., the MPM scheme with the granular Drucker-Prager rheology, to avoid expensive collision detection and handling. This fuzzy collision handling makes the authoring process significantly easier. However, although current hair grooming tools provide a wide range of strand-based modeling tools for this simulation approach, the crucial sag-free initialization functionality remains often ignored. Thus, when the simulation starts, gravity would cause any artistic hairstyle to sag and deform into unintended and undesirable shapes. This paper proposes a novel four-stage sag-free initialization framework to solve stable quasistatic configurations for hybrid strand-based hair dynamic systems. These four stages are split into two global-local pairs. The first one ensures static equilibrium at every Eulerian grid node with additional inequality constraints to prevent stress from exiting the yielding surface. We then derive several associated closed-form solutions in the local stage to compute segment rest lengths, orientations, and particle deformation gradients in parallel. The second global-local step solves along each hair strand to ensure all the bend and twist constraints produce zero net torque on every hair segment, followed by a local step to adjust the rest Darboux vectors to a unit quaternion. We also introduce an essential modification for the Darboux vector to eliminate the ambiguity of the Cosserat rod rest pose in both initialization and simulation. We evaluate our method on a wide range of hairstyles, and our approach can only take a few seconds to minutes to get the rest quasistatic configurations for hundreds of hair strands. Our results show that our method successfully prevents sagging and has minimal impact on the hair motion during simulation.},
  archive      = {J_TOG},
  author       = {Jerry Hsu and Tongtong Wang and Zherong Pan and Xifeng Gao and Cem Yuksel and Kui Wu},
  doi          = {10.1145/3592143},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {74:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Sag-free initialization for strand-based hybrid hair simulation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational exploration of multistable elastic knots.
<em>TOG</em>, <em>42</em>(4), 73:1–16. (<a
href="https://doi.org/10.1145/3592399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithmic approach to discover, study, and design multistable elastic knots. Elastic knots are physical realizations of closed curves embedded in 3-space. When endowed with the material thickness and bending resistance of a physical wire, these knots settle into equilibrium states that balance the forces induced by elastic deformation and self-contacts of the wire. In general, elastic knots can have many distinct equilibrium states, i.e. they are multistable mechanical systems. We propose a computational pipeline that combines randomized spatial sampling and physics simulation to efficiently find stable equilibrium states of elastic knots. Leveraging results from knot theory, we run our pipeline on thousands of different topological knot types to create an extensive data set of multistable knots. By applying a series of filters to this data, we discover new transformable knots with interesting geometric and physical properties. A further analysis across knot types reveals geometric and topological patterns, yielding constructive principles that generalize beyond the currently tabulated knot types. We show how multistable elastic knots can be used to design novel deployable structures and engaging recreational puzzles. Several physical prototypes at different scales highlight these applications and validate our simulation.},
  archive      = {J_TOG},
  author       = {Michele Vidulis and Yingying Ren and Julian Panetta and Eitan Grinspun and Mark Pauly},
  doi          = {10.1145/3592399},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {73:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational exploration of multistable elastic knots},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex wrinkle field evolution. <em>TOG</em>,
<em>42</em>(4), 72:1–19. (<a
href="https://doi.org/10.1145/3592397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach for representing wrinkles, designed to capture complex and detailed wrinkle behavior on coarse triangle meshes, called Complex Wrinkle Fields. Complex Wrinkle Fields consist of an almost-everywhere-unit complex-valued phase function over the surface; a frequency one-form; and an amplitude scalar, with a soft compatibility condition coupling the frequency and phase. We develop algorithms for interpolating between two such wrinkle fields, for visualizing them as displacements of a Loop-subdivided refinement of the base mesh, and for making smooth local edits to the wrinkle amplitude, frequency, and/or orientation. These algorithms make it possible, for the first time, to create and edit animations of wrinkles on triangle meshes that are smooth in space, evolve smoothly through time, include singularities along with their complex interactions, and that represent frequencies far finer than the surface resolution.},
  archive      = {J_TOG},
  author       = {Zhen Chen and Danny Kaufman and Mélina Skouras and Etienne Vouga},
  doi          = {10.1145/3592397},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {72:1–19},
  shortjournal = {ACM Trans. Graph.},
  title        = {Complex wrinkle field evolution},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient embeddings in exact arithmetic. <em>TOG</em>,
<em>42</em>(4), 71:1–17. (<a
href="https://doi.org/10.1145/3592445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a set of tools for generating planar embeddings of triangulated topological spheres. The algorithms make use of Schnyder labelings and realizers. A new representation of the realizer based on dual trees leads to a simple linear time algorithm mapping from weights per triangle to barycentric coordinates and, more importantly, also in the reverse direction. The algorithms can be implemented so that all coefficients involved are 1 or -1. This enables integer computation, making all computations exact. Being a Schnyder realizer, mapping from positive triangle weights guarantees that the barycentric coordinates form an embedding. The reverse direction enables an algorithm for fixing flipped triangles in planar realizations, by mapping from coordinates to weights and adjusting the weights (without forcing them to be positive). In a range of experiments, we demonstrate that all algorithms are orders of magnitude faster than existing robust approaches.},
  archive      = {J_TOG},
  author       = {Ugo Finnendahl and Dimitrios Bogiokas and Pablo Robles Cervantes and Marc Alexa},
  doi          = {10.1145/3592445},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {71:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Efficient embeddings in exact arithmetic},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Min-deviation-flow in bi-directed graphs for t-mesh
quantization. <em>TOG</em>, <em>42</em>(4), 70:1–25. (<a
href="https://doi.org/10.1145/3592437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subdividing non-conforming T-mesh layouts into conforming quadrangular meshes is a core component of state-of-the-art (re-)meshing methods. Typically, the required constrained assignment of integer lengths to T-Mesh edges is left to generic branch-and-cut solvers, greedy heuristics, or a combination of the two. This either does not scale well with input complexity or delivers suboptimal result quality. We introduce the Minimum-Deviation-Flow Problem in bi-directed networks (Bi-MDF) and demonstrate its use in modeling and efficiently solving a variety of T-Mesh quantization problems. We develop a fast approximate solver as well as an iterative refinement algorithm based on matching in graphs that solves Bi-MDF exactly. Compared to the state-of-the-art QuadWild [Pietroni et al. 2021] implementation on the authors&#39; 300 dataset, our exact solver finishes after only 0.49\% (total 17.06s) of their runtime (3491s) and achieves 11\% lower energy while an approximation is computed after 0.09\% (3.19s) of their runtime at the cost of 24\% increased energy. A novel half-arc-based T-Mesh quantization formulation extends the feasible solution space to include previously unattainable quad meshes. The Bi-MDF problem is more general than our application in layout quantization, potentially enabling similar speedups for other optimization problems that fit into the scheme, such as quad mesh refinement.},
  archive      = {J_TOG},
  author       = {Martin Heistermann and Jethro Warnett and David Bommes},
  doi          = {10.1145/3592437},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {70:1–25},
  shortjournal = {ACM Trans. Graph.},
  title        = {Min-deviation-flow in bi-directed graphs for T-mesh quantization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential operators on sketches via alpha contours.
<em>TOG</em>, <em>42</em>(4), 69:1–15. (<a
href="https://doi.org/10.1145/3592420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vector sketch is a popular and natural geometry representation depicting a 2D shape. When viewed from afar, the disconnected vector strokes of a sketch and the empty space around them visually merge into positive space and negative space , respectively. Positive and negative spaces are the key elements in the composition of a sketch and define what we perceive as the shape. Nevertheless, the notion of positive or negative space is mathematically ambiguous: While the strokes unambiguously indicate the interior or boundary of a 2D shape, the empty space may or may not belong to the shape&#39;s exterior. For standard discrete geometry representations, such as meshes or point clouds, some of the most robust pipelines rely on discretizations of differential operators, such as Laplace-Beltrami. Such discretizations are not available for vector sketches; defining them may enable numerous applications of classical methods on vector sketches. However, to do so, one needs to define the positive space of a vector sketch, or the sketch shape. Even though extracting this 2D sketch shape is mathematically ambiguous, we propose a robust algorithm, Alpha Contours , constructing its conservative estimate: a 2D shape containing all the input strokes, which lie in its interior or on its boundary, and aligning tightly to a sketch. This allows us to define popular differential operators on vector sketches, such as Laplacian and Steklov operators. We demonstrate that our construction enables robust tools for vector sketches, such as As-Rigid-As-Possible sketch deformation and functional maps between sketches, as well as solving partial differential equations on a vector sketch.},
  archive      = {J_TOG},
  author       = {Mariia Myronova and William Neveu and Mikhail Bessmeltsev},
  doi          = {10.1145/3592420},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {69:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Differential operators on sketches via alpha contours},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orientable dense cyclic infill for anisotropic appearance
fabrication. <em>TOG</em>, <em>42</em>(4), 68:1–13. (<a
href="https://doi.org/10.1145/3592412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method to 3D print surfaces exhibiting a prescribed varying field of anisotropic appearance using only standard fused filament fabrication printers. This enables the fabrication of patterns triggering reflections similar to that of brushed metal with direct control over the directionality of the reflections. Our key insight, on which we ground the method, is that the direction of the deposition paths leads to a certain degree of surface roughness, which yields a visual anisotropic appearance. Therefore, generating dense cyclic infills aligned with a line field allows us to grade the anisotropic appearance of the printed surface. To achieve this, we introduce a highly parallelizable algorithm for optimizing oriented, cyclic paths. Our algorithm outperforms existing approaches regarding efficiency, robustness, and result quality. We demonstrate the effectiveness of our technique in conveying an anisotropic appearance on several challenging test cases, ranging from patterns to photographs reinterpreted as anisotropic appearances.},
  archive      = {J_TOG},
  author       = {Xavier Chermain and Cédric Zanni and Jonàs Martínez and Pierre-Alexandre Hugron and Sylvain Lefebvre},
  doi          = {10.1145/3592412},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {68:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Orientable dense cyclic infill for anisotropic appearance fabrication},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Skin-screen: A computational fabrication framework for color
tattoos. <em>TOG</em>, <em>42</em>(4), 67:1–13. (<a
href="https://doi.org/10.1145/3592432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tattoos are a highly popular medium, with both artistic and medical applications. Although the mechanical process of tattoo application has evolved historically, the results are reliant on the artisanal skill of the artist. This can be especially challenging for some skin tones, or in cases where artists lack experience. We provide the first systematic overview of tattooing as a computational fabrication technique. We built an automated tattooing rig and a recipe for the creation of silicone sheets mimicking realistic skin tones, which allowed us to create an accurate model predicting tattoo appearance. This enables several exciting applications including tattoo previewing, color retargeting, novel ink spectra optimization, color-accurate prosthetics, and more.},
  archive      = {J_TOG},
  author       = {Michal Piovarci and Alexandre Chapiro and Bernd Bickel},
  doi          = {10.1145/3592432},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {67:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Skin-screen: A computational fabrication framework for color tattoos},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meso-facets for goniochromatic 3D printing. <em>TOG</em>,
<em>42</em>(4), 66:1–12. (<a
href="https://doi.org/10.1145/3592137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Goniochromatic materials and objects appear to have different colors depending on viewing direction. This occurs in nature, such as in wood or minerals, and in human-made objects such as metal and effect pigments. In this paper, we propose algorithms to control multi-material 3D printers to produce goniochromatic effects on arbitrary surfaces by procedurally augmenting the input surface with meso-facets, which allow distinct colors to be assigned to different viewing directions of the input surface while introducing minimal changes to that surface. Previous works apply only to 2D or 2.5D surfaces, require multiple fabrication technologies, or make considerable changes to the input surface and require special post-processing, whereas our approach requires a single fabrication technology and no special post-processing. Our framework is general, allowing different generating functions for both the shape and color of the facets. Working with implicit representations allows us to generate geometric features at the limit of device resolution without tessellation. We evaluate our approach for performance, showing negligible overhead compared to baseline color 3D print processing, and for goniochromatic quality.},
  archive      = {J_TOG},
  author       = {Lubna Abu Rmaileh and Alan Brunton},
  doi          = {10.1145/3592137},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {66:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Meso-facets for goniochromatic 3D printing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scratch-based reflection art via differentiable rendering.
<em>TOG</em>, <em>42</em>(4), 65:1–12. (<a
href="https://doi.org/10.1145/3592142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D visual optical arts create fascinating special effects by carefully designing interactions between objects and light sources. One of the essential types is 3D reflection art, which aims to create reflectors that can display different images when viewed from different directions. Existing works produce impressive visual effects. Unfortunately, previous works discretize the reflector surface with regular grids/facets, leading to a large parameter space and a high optimization time cost. In this paper, we introduce a new type of 3D reflection art - scratch-based reflection art , which allows for a more compact parameter space, easier fabrication, and computationally efficient optimization. To design a 3D reflection art with scratches, we formulate it as a multi-view optimization problem and introduce differentiable rendering to enable efficient gradient-based optimizers. For that, we propose an analytical scratch rendering approach, together with a high-performance rendering pipeline, allowing efficient differentiable rendering. As a consequence, we could display multiple images on a single metallic board with only several minutes for optimization. We demonstrate our work by showing virtual objects and manufacturing our designed reflectors with a carving machine.},
  archive      = {J_TOG},
  author       = {Pengfei Shen and Ruizeng Li and Beibei Wang and Ligang Liu},
  doi          = {10.1145/3592142},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {65:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Scratch-based reflection art via differentiable rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting controlled mixture sampling for rendering
applications. <em>TOG</em>, <em>42</em>(4), 64:1–13. (<a
href="https://doi.org/10.1145/3592435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte Carlo rendering makes heavy use of mixture sampling and multiple importance sampling (MIS). Previous work has shown that control variates can be used to make such mixtures more efficient and more robust. However, the existing approaches failed to yield practical applications, chiefly because their underlying theory is based on the unrealistic assumption that a single mixture is optimized for a single integral. This is in stark contrast with rendering reality, where millions of integrals are computed---one per pixel---and each is infinitely recursive. We adapt and extend the theory introduced by previous work to tackle the challenges of real-world rendering applications. We achieve robust mixture sampling and (approximately) optimal MIS weighting for common applications such as light selection, BSDF sampling, and path guiding.},
  archive      = {J_TOG},
  author       = {Qingqin Hua and Pascal Grittmann and Philipp Slusallek},
  doi          = {10.1145/3592435},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {64:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Revisiting controlled mixture sampling for rendering applications},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Film grain rendering and parameter estimation. <em>TOG</em>,
<em>42</em>(4), 63:1–14. (<a
href="https://doi.org/10.1145/3592127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a realistic film grain rendering algorithm based on statistics derived analytically from a physics-based Boolean model that Newson et al. adopted for Monte Carlo simulations of film grain. We also propose formulas for estimation of the model parameters from scanned film grain images. The proposed rendering is computationally efficient and can be used for real-time film grain simulation for a wide range of film grain parameters when the individual film grains are not visible. Experimental results demonstrate the effectiveness of the proposed approach for both constant and real-world images, for a six orders of magnitude speed-up compared with the Monte Carlo simulations of the Newson et al. approach.},
  archive      = {J_TOG},
  author       = {Kaixuan Zhang and Jingxian Wang and Daizong Tian and Thrasyvoulos N. Pappas},
  doi          = {10.1145/3592127},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {63:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Film grain rendering and parameter estimation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recursive control variates for inverse rendering.
<em>TOG</em>, <em>42</em>(4), 62:1–13. (<a
href="https://doi.org/10.1145/3592139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for reducing errors---variance and bias---in physically based differentiable rendering (PBDR). Typical applications of PBDR repeatedly render a scene as part of an optimization loop involving gradient descent. The actual change introduced by each gradient descent step is often relatively small, causing a significant degree of redundancy in this computation. We exploit this redundancy by formulating a gradient estimator that employs a recursive control variate , which leverages information from previous optimization steps. The control variate reduces variance in gradients, and, perhaps more importantly, alleviates issues that arise from differentiating loss functions with respect to noisy inputs, a common cause of drift to bad local minima or divergent optimizations. We experimentally evaluate our approach on a variety of path-traced scenes containing surfaces and volumes and observe that primal rendering efficiency improves by a factor of up to 10.},
  archive      = {J_TOG},
  author       = {Baptiste Nicolet and Fabrice Rousselle and Jan Novak and Alexander Keller and Wenzel Jakob and Thomas Müller},
  doi          = {10.1145/3592139},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {62:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Recursive control variates for inverse rendering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forming terrains by glacial erosion. <em>TOG</em>,
<em>42</em>(4), 61:1–14. (<a
href="https://doi.org/10.1145/3592422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the first solution for simulating the formation and evolution of glaciers, together with their attendant erosive effects, for periods covering the combination of glacial and inter-glacial cycles. Our efficient solution includes both a fast yet accurate deep learning-based estimation of highorder ice flows and a new, multi-scale advection scheme enabling us to account for the distinct time scales at which glaciers reach equilibrium compared to eroding the terrain. We combine the resulting glacial erosion model with finer-scale erosive phenomena to account for the transport of debris flowing from cliffs. This enables us to model the formation of terrain shapes not previously adequately modeled in Computer Graphics, ranging from U-shaped and hanging valleys to fjords and glacial lakes.},
  archive      = {J_TOG},
  author       = {Guillaume Cordonnier and Guillaume Jouvet and Adrien Peytavie and Jean Braun and Marie-Paule Cani and Bedrich Benes and Eric Galin and Eric Guérin and James Gain},
  doi          = {10.1145/3592422},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {61:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Forming terrains by glacial erosion},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Example-based procedural modeling using graph grammars.
<em>TOG</em>, <em>42</em>(4), 60:1–16. (<a
href="https://doi.org/10.1145/3592119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for automatically generating polygonal shapes from an example using a graph grammar. Most procedural modeling techniques use grammars with manually created rules, but our method can create them automatically from an example. Our graph grammars generate graphs that are locally similar to a given example. We disassemble the input into small pieces called primitives and then reassemble the primitives into new graphs. We organize all possible locally similar graphs into a hierarchy and find matching graphs within the hierarchy. These matches are used to create a graph grammar that can construct every locally similar graph. Our method generates graphs using the grammar and then converts them into a planar graph drawing to produce the final shape.},
  archive      = {J_TOG},
  author       = {Paul Merrell},
  doi          = {10.1145/3592119},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {60:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Example-based procedural modeling using graph grammars},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rhizomorph: The coordinated function of shoots and roots.
<em>TOG</em>, <em>42</em>(4), 59:1–16. (<a
href="https://doi.org/10.1145/3592145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer graphics has dedicated a considerable amount of effort to generating realistic models of trees and plants. Many existing methods leverage procedural modeling algorithms - that often consider biological findings - to generate branching structures of individual trees. While the realism of tree models generated by these algorithms steadily increases, most approaches neglect to model the root system of trees. However, the root system not only adds to the visual realism of tree models but also plays an important role in the development of trees. In this paper, we advance tree modeling in the following ways: First, we define a physically-plausible soil model to simulate resource gradients, such as water and nutrients. Second, we propose a novel developmental procedural model for tree roots that enables us to emergently develop root systems that adapt to various soil types. Third, we define long-distance signaling to coordinate the development of shoots and roots. We show that our advanced procedural model of tree development enables - for the first time - the generation of trees with their root systems.},
  archive      = {J_TOG},
  author       = {Bosheng Li and Jonathan Klein and Dominik L. Michels and Bedrich Benes and Sören Pirk and Wojtek Pałubicki},
  doi          = {10.1145/3592145},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {59:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Rhizomorph: The coordinated function of shoots and roots},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Étendue expansion in holographic near eye displays through
sparse eye-box generation using lens array eyepiece. <em>TOG</em>,
<em>42</em>(4), 58:1–13. (<a
href="https://doi.org/10.1145/3592441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel method the étendue expansion of near-eye holographic displays through the generation of a sparse eye-box. Conventional holographic near-eye displays have suffered from narrow field of view or narrow eye-box due to the limited étendue supported by a spatial light modulator. We focus on the fact that these displays typically form a dense eye-box, which could be an excessive investment of the limited étendue. By rearranging the eye-box in a sparse manner, the practical étendue can be extended. With a properly designed sparse eye-box shape, it can provide the 3D holographic images and ensure continuous light entrance to the pupil. To create a sparse eye-box, we utilize a lens array as an eyepiece lens. We optimize the spatial light modulator&#39;s phase profile for the proposed system and analyze the impact of the use of the lens array eyepiece on the holographic image quality. In particular, we focus on the lens array specification of the lenslet pitch and the focal length, deriving feasible specifications based on our analysis. We experimentally demonstrate a near eye see-through display using the proposed system and verify the étendue expansion.},
  archive      = {J_TOG},
  author       = {Minseok Chae and Kiseung Bang and Dongheon Yoo and Yoonchan Jeong},
  doi          = {10.1145/3592441},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {58:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {Étendue expansion in holographic near eye displays through sparse eye-box generation using lens array eyepiece},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Split-lohmann multifocal displays. <em>TOG</em>,
<em>42</em>(4), 57:1–18. (<a
href="https://doi.org/10.1145/3592110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work provides the design of a multifocal display that can create a dense stack of focal planes in a single shot. We achieve this using a novel computational lens that provides spatial selectivity in its focal length, i.e, the lens appears to have different focal lengths across points on a display behind it. This enables a multifocal display via an appropriate selection of the spatially-varying focal length, thereby avoiding time multiplexing techniques that are associated with traditional focus tunable lenses. The idea central to this design is a modification of a Lohmann lens, a focus tunable lens created with two cubic phase plates that translate relative to each other. Using optical relays and a phase spatial light modulator, we replace the physical translation of the cubic plates with an optical one, while simultaneously allowing for different pixels on the display to undergo different amounts of translations and, consequently, different focal lengths. We refer to this design as a Split-Lohmann multifocal display. Split-Lohmann displays provide a large étendue as well as high spatial and depth resolutions; the absence of time multiplexing and the extremely light computational footprint for content processing makes it suitable for video and interactive experiences. Using a lab prototype, we show results over a wide range of static, dynamic, and interactive 3D scenes, showcasing high visual quality over a large working range.},
  archive      = {J_TOG},
  author       = {Yingsi Qin and Wei-Yu Chen and Matthew O&#39;Toole and Aswin C. Sankaranarayanan},
  doi          = {10.1145/3592110},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {57:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Split-lohmann multifocal displays},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised reference-based sketch extraction using a
contrastive learning framework. <em>TOG</em>, <em>42</em>(4), 56:1–12.
(<a href="https://doi.org/10.1145/3592392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketches reflect the drawing style of individual artists; therefore, it is important to consider their unique styles when extracting sketches from color images for various applications. Unfortunately, most existing sketch extraction methods are designed to extract sketches of a single style. Although there have been some attempts to generate various style sketches, the methods generally suffer from two limitations: low quality results and difficulty in training the model due to the requirement of a paired dataset. In this paper, we propose a novel multi-modal sketch extraction method that can imitate the style of a given reference sketch with unpaired data training in a semi-supervised manner. Our method outperforms state-of-the-art sketch extraction methods and unpaired image translation methods in both quantitative and qualitative evaluations.},
  archive      = {J_TOG},
  author       = {Chang Wook Seo and Amirsaman Ashtari and Junyong Noh},
  doi          = {10.1145/3592392},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {56:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Semi-supervised reference-based sketch extraction using a contrastive learning framework},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StripMaker: Perception-driven learned vector sketch
consolidation. <em>TOG</em>, <em>42</em>(4), 55:1–15. (<a
href="https://doi.org/10.1145/3592130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artist sketches often use multiple overdrawn strokes to depict a single intended curve. Humans effortlessly mentally consolidate such sketches by detecting groups of overdrawn strokes and replacing them with the corresponding intended curves. While this mental process is near instantaneous, manually annotating or retracing sketches to communicate this intended mental image is highly time consuming; yet most sketch applications are not designed to handle overdrawing and can only operate on overdrawing-free, consolidated sketches. We propose StripMaker , a new and robust learning based method for automatic consolidation of raw vector sketches. We avoid the need for an unsustainably large manually annotated learning corpus by leveraging observations about artist workflow and perceptual cues viewers employ when mentally consolidating sketches. We train two perception-aware classifiers that assess the likelihood that a pair of stroke groups jointly depicts the same intended curve: our first classifier is purely local and only accounts for the properties of the evaluated strokes; our second classifier incorporates global context and is designed to operate on approximately consolidated sketches. We embed these classifiers within a consolidation framework that leverages artist workflow: we first process strokes in the order they were drawn and use our local classifier to arrive at an approximate consolidation output, then use the contextual classifier to refine this output and finalize the consolidated result. We validate StripMaker by comparing its results to manual consolidation outputs and algorithmic alternatives. StripMaker achieves comparable performance to manual consolidation. In a comparative study participants preferred our results by a 53\% margin over those of the closest algorithmic alternative (67\% versus 14\%, other/neither 19\%).},
  archive      = {J_TOG},
  author       = {Chenxi Liu and Toshiki Aoki and Mikhail Bessmeltsev and Alla Sheffer},
  doi          = {10.1145/3592130},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {55:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {StripMaker: Perception-driven learned vector sketch consolidation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VideoDoodles: Hand-drawn animations on videos with
scene-aware canvases. <em>TOG</em>, <em>42</em>(4), 54:1–12. (<a
href="https://doi.org/10.1145/3592413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an interactive system to ease the creation of so-called video doodles - videos on which artists insert hand-drawn animations for entertainment or educational purposes. Video doodles are challenging to create because to be convincing, the inserted drawings must appear as if they were part of the captured scene. In particular, the drawings should undergo tracking, perspective deformations and occlusions as they move with respect to the camera and to other objects in the scene - visual effects that are difficult to reproduce with existing 2D video editing software. Our system supports these effects by relying on planar canvases that users position in a 3D scene reconstructed from the video. Furthermore, we present a custom tracking algorithm that allows users to anchor canvases to static or dynamic objects in the scene, such that the canvases move and rotate to follow the position and direction of these objects. When testing our system, novices could create a variety of short animated clips in a dozen of minutes, while professionals praised its speed and ease of use compared to existing tools.},
  archive      = {J_TOG},
  author       = {Emilie Yu and Kevin Blackburn-Matzen and Cuong Nguyen and Oliver Wang and Rubaiat Habib Kazi and Adrien Bousseau},
  doi          = {10.1145/3592413},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {54:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {VideoDoodles: Hand-drawn animations on videos with scene-aware canvases},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Patternshop: Editing point patterns by image manipulation.
<em>TOG</em>, <em>42</em>(4), 53:1–14. (<a
href="https://doi.org/10.1145/3592418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point patterns are characterized by their density and correlation. While spatial variation of density is well-understood, analysis and synthesis of spatially-varying correlation is an open challenge. No tools are available to intuitively edit such point patterns, primarily due to the lack of a compact representation for spatially varying correlation. We propose a low-dimensional perceptual embedding for point correlations. This embedding can map point patterns to common three-channel raster images, enabling manipulation with off-the-shelf image editing software. To synthesize back point patterns, we propose a novel edge-aware objective that carefully handles sharp variations in density and correlation. The resulting framework allows intuitive and backward-compatible manipulation of point patterns, such as recoloring, relighting to even texture synthesis that have not been available to 2D point pattern design before. Effectiveness of our approach is tested in several user experiments. Code is available at https://github.com/xchhuang/patternshop.},
  archive      = {J_TOG},
  author       = {Xingchang Huang and Tobias Ritschel and Hans-Peter Seidel and Pooran Memari and Gurprit Singh},
  doi          = {10.1145/3592418},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {53:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Patternshop: Editing point patterns by image manipulation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Juxtaform: Interactive visual summarization for exploratory
shape design. <em>TOG</em>, <em>42</em>(4), 52:1–14. (<a
href="https://doi.org/10.1145/3592436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present juxtaform , a novel approach to the interactive summarization of large shape collections for conceptual shape design. We conduct a formative study to ascertain design goals for creative shape exploration tools. Motivated by a mathematical formulation of these design goals, juxtaform integrates the exploration, analysis, selection, and refinement of large shape collections to support an interactive divergence-convergence shape design workflow. We exploit sparse, segmented sketch-stroke visual abstractions of shape and a novel visual summarization algorithm to balance the needs of shape understanding, in-situ shape juxtaposition, and visual clutter. Our evaluation is three-fold: we show that existing shape and stroke clustering algorithms do not address our design goals compared to our proposed shape corpus summarization algorithm; we compare juxtaform against a structured image gallery interface for various shape design and analysis tasks; and we present multiple compelling 2D/3D applications using juxtaform.},
  archive      = {J_TOG},
  author       = {Karran Pandey and Fanny Chevalier and Karan Singh},
  doi          = {10.1145/3592436},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {52:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Juxtaform: Interactive visual summarization for exploratory shape design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ArrangementNet: Learning scene arrangements for vectorized
indoor scene modeling. <em>TOG</em>, <em>42</em>(4), 51:1–15. (<a
href="https://doi.org/10.1145/3592122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel vectorized indoor modeling approach that converts point clouds into building information models (BIM) with concise and semantically segmented polygonal meshes. Existing methods detect planar shapes and connect them to complete the scene. Some focus on floor plan reconstruction as a simplified problem to better analyze connectivity between planes of floors and walls. However, the connectivity analysis is still challenging and ill-posed with incomplete point clouds as input. We propose ArrangementNet to estimate scene arrangements from an incomplete point cloud, which we can easily convert into a BIM model. ArrangementNet is a novel graph neural network that consumes noisy over-partitioned initial arrangements extracted through non-learning techniques and outputs high-quality scene arrangement. The core of ArrangementNet is an extended graph convolution that leverages co-linear and co-face relationships in the arrangement and improves the quality of prediction in complex scenes. We apply ArrangementNet to improve floor plan and ceiling arrangements and enrich them with semantic objects as scene arrangements for scene generation. Our approach faithfully models challenging scenes obtained from laser scans or multiview stereo and shows significant improvement in BIM model reconstruction compared to the state-of-the-art. Our code is available at https://github.com/zssjh/ArrangementNet.},
  archive      = {J_TOG},
  author       = {Jingwei Huang and Shanshan Zhang and Bo Duan and Yanfeng Zhang and Xiaoyang Guo and Mingwei Sun and Li Yi},
  doi          = {10.1145/3592122},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {51:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {ArrangementNet: Learning scene arrangements for vectorized indoor scene modeling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The visual language of fabrics. <em>TOG</em>,
<em>42</em>(4), 50:1–15. (<a
href="https://doi.org/10.1145/3592391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce text2fabric, a novel dataset that links free-text descriptions to various fabric materials. The dataset comprises 15,000 natural language descriptions associated to 3,000 corresponding images of fabric materials. Traditionally, material descriptions come in the form of tags/keywords, which limits their expressivity, induces pre-existing knowledge of the appropriate vocabulary, and ultimately leads to a chopped description system. Therefore, we study the use of free-text as a more appropriate way to describe material appearance, taking the use case of fabrics as a common item that non-experts may often deal with. Based on the analysis of the dataset, we identify a compact lexicon, set of attributes and key structure that emerge from the descriptions. This allows us to accurately understand how people describe fabrics and draw directions for generalization to other types of materials. We also show that our dataset enables specializing large vision-language models such as CLIP, creating a meaningful latent space for fabric appearance, and significantly improving applications such as fine-grained material retrieval and automatic captioning.},
  archive      = {J_TOG},
  author       = {Valentin Deschaintre and Julia Guerrero-Viu and Diego Gutierrez and Tamy Boubekeur and Belen Masia},
  doi          = {10.1145/3592391},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {50:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {The visual language of fabrics},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ShapeCoder: Discovering abstractions for visual programs
from unstructured primitives. <em>TOG</em>, <em>42</em>(4), 49:1–17. (<a
href="https://doi.org/10.1145/3592416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across a dataset, so that programs rewritten with these abstractions are more compact, and suppress spurious degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stringent input assumptions. This is principally made possible by two methodological advancements: (a) a shape-to-program recognition network that learns to solve sub-problems and (b) the use of e-graphs, augmented with a conditional rewrite scheme, to determine when abstractions with complex parametric expressions can be applied, in a tractable manner. We evaluate ShapeCoder on multiple datasets of 3D shapes, where primitive decompositions are either parsed from manual annotations or produced by an unsupervised cuboid abstraction method. In all domains, ShapeCoder discovers a library of abstractions that captures high-level relationships, removes extraneous degrees of freedom, and achieves better dataset compression compared with alternative approaches. Finally, we investigate how programs rewritten to use discovered abstractions prove useful for downstream tasks.},
  archive      = {J_TOG},
  author       = {R. Kenny Jones and Paul Guerrero and Niloy J. Mitra and Daniel Ritchie},
  doi          = {10.1145/3592416},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {49:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {ShapeCoder: Discovering abstractions for visual programs from unstructured primitives},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational long exposure mobile photography.
<em>TOG</em>, <em>42</em>(4), 48:1–15. (<a
href="https://doi.org/10.1145/3592124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the scene motion over multiple frames and align the images in order to preserve desired sharpness and to produce aesthetically pleasing motion streaks. We capture an under-exposed burst and select the subset of input frames that will produce blur trails of controlled length, regardless of scene or camera motion velocity. We predict inter-frame motion and synthesize motion-blur to fill the temporal gaps between the input frames. Finally, we composite the blurred image with the sharp regular exposure to protect the sharpness of faces or areas of the scene that are barely moving, and produce a final high resolution and high dynamic range (HDR) photograph. Our system democratizes a capability previously reserved to professionals, and makes this creative style accessible to most casual photographers.},
  archive      = {J_TOG},
  author       = {Eric Tabellion and Nikhil Karnad and Noa Glaser and Ben Weiss and David E. Jacobs and Yael Pritch},
  doi          = {10.1145/3592124},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {48:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Computational long exposure mobile photography},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FactorMatte: Redefining video matting for re-composition
tasks. <em>TOG</em>, <em>42</em>(4), 47:1–14. (<a
href="https://doi.org/10.1145/3592423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Factor Matting , an alternative formulation of the video matting problem in terms of counterfactual video synthesis that is better suited for re-composition tasks. The goal of factor matting is to separate the contents of a video into independent components, each representing a counterfactual version of the scene where the contents of other components have been removed. We show that factor matting maps well to a more general Bayesian framing of the matting problem that accounts for complex conditional interactions between layers. Based on this observation, we present a method for solving the factor matting problem that learns augmented patch-based appearance priors to produce useful decompositions even for video with complex cross-layer interactions like splashes, shadows, and reflections. Our method is trained per-video and does not require external training data or any knowledge about the 3D structure of the scene. Through extensive experiments, we show that it is able to produce useful decompositions of scenes with such complex interactions while performing competitively on classical matting tasks as well. We also demonstrate the benefits of our approach on a wide range of downstream video editing tasks. Our project website is at: https://factormatte.github.io/.},
  archive      = {J_TOG},
  author       = {Zeqi Gu and Wenqi Xian and Noah Snavely and Abe Davis},
  doi          = {10.1145/3592423},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {47:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {FactorMatte: Redefining video matting for re-composition tasks},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Eventfulness for interactive video alignment. <em>TOG</em>,
<em>42</em>(4), 46:1–10. (<a
href="https://doi.org/10.1145/3592118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are remarkably sensitive to the alignment of visual events with other stimuli, which makes synchronization one of the hardest tasks in video editing. A key observation of our work is that most of the alignment we do involves salient localizable events that occur sparsely in time. By learning how to recognize these events, we can greatly reduce the space of possible synchronizations that an editor or algorithm has to consider. Furthermore, by learning descriptors of these events that capture additional properties of visible motion, we can build active tools that adapt their notion of eventfulness to a given task as they are being used. Rather than learning an automatic solution to one specific problem, our goal is to make a much broader class of interactive alignment tasks significantly easier and less time-consuming. We show that a suitable visual event descriptor can be learned entirely from stochastically-generated synthetic video. We then demonstrate the usefulness of learned and adaptive eventfulness by integrating it in novel interactive tools for applications including audio-driven time warping of video and the extraction and application of sound effects across different videos.},
  archive      = {J_TOG},
  author       = {Jiatian Sun and Longxiulin Deng and Triantafyllos Afouras and Andrew Owens and Abe Davis},
  doi          = {10.1145/3592118},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {46:1–10},
  shortjournal = {ACM Trans. Graph.},
  title        = {Eventfulness for interactive video alignment},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contact edit: Artist tools for intuitive modeling of
hand-object interactions. <em>TOG</em>, <em>42</em>(4), 45:1–20. (<a
href="https://doi.org/10.1145/3592117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Posing high-contact interactions is challenging and time-consuming, with hand-object interactions being especially difficult due to the large number of degrees of freedom (DOF) of the hand and the fact that humans are experts at judging hand poses. This paper addresses this challenge by elevating contact areas to first-class primitives. We provide end-to-end art-directable (EAD) tools to model interactions based on contact areas, directly manipulate contact areas, and compute corresponding poses automatically. To make these operations intuitive and fast, we present a novel axis-based contact model that supports real-time approximately isometry-preserving operations on triangulated surfaces, permits movement between surfaces, and is both robust and scalable to large areas. We show that use of our contact model facilitates high quality posing even for unconstrained, high-DOF custom rigs intended for traditional keyframe-based animation pipelines. We additionally evaluate our approach with comparisons to prior art, ablation studies, user studies, qualitative assessments, and extensions to full-body interaction.},
  archive      = {J_TOG},
  author       = {Arjun Sriram Lakshmipathy and Nicole Feng and Yu Xi Lee and Moshe Mahler and Nancy Pollard},
  doi          = {10.1145/3592117},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {45:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Contact edit: Artist tools for intuitive modeling of hand-object interactions},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Listen, denoise, action! Audio-driven motion synthesis with
diffusion models. <em>TOG</em>, <em>42</em>(4), 44:1–20. (<a
href="https://doi.org/10.1145/3592458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest.},
  archive      = {J_TOG},
  author       = {Simon Alexanderson and Rajmund Nagy and Jonas Beskow and Gustav Eje Henter},
  doi          = {10.1145/3592458},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {44:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Listen, denoise, action! audio-driven motion synthesis with diffusion models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bodyformer: Semantics-guided 3D body gesture synthesis with
transformer. <em>TOG</em>, <em>42</em>(4), 43:1–12. (<a
href="https://doi.org/10.1145/3592456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system is trained with either the Trinity speech-gesture dataset or the Talking With Hands 16.2M dataset. The results show that our system can produce more realistic, appropriate, and diverse body gestures compared to existing state-of-the-art approaches.},
  archive      = {J_TOG},
  author       = {Kunkun Pang and Dafei Qin and Yingruo Fan and Julian Habekost and Takaaki Shiratori and Junichi Yamagishi and Taku Komura},
  doi          = {10.1145/3592456},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {43:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Bodyformer: Semantics-guided 3D body gesture synthesis with transformer},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GestureDiffuCLIP: Gesture diffusion model with CLIP latents.
<em>TOG</em>, <em>42</em>(4), 42:1–18. (<a
href="https://doi.org/10.1145/3592097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with flexible style control. We leverage the power of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and present a novel CLIP-guided mechanism that extracts efficient style representations from multiple input modalities, such as a piece of text, an example motion clip, or a video. Our system learns a latent diffusion model to generate high-quality gestures and infuses the CLIP representations of style into the generator via an adaptive instance normalization (AdaIN) layer. We further devise a gesture-transcript alignment mechanism that ensures a semantically correct gesture generation based on contrastive learning. Our system can also be extended to allow fine-grained style control of individual body parts. We demonstrate an extensive set of examples showing the flexibility and generalizability of our model to a variety of style descriptions. In a user study, we show that our system outperforms the state-of-the-art approaches regarding human likeness, appropriateness, and style correctness.},
  archive      = {J_TOG},
  author       = {Tenglong Ao and Zeyi Zhang and Libin Liu},
  doi          = {10.1145/3592097},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {42:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {GestureDiffuCLIP: Gesture diffusion model with CLIP latents},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HACK: Learning a parametric head and neck model for
high-fidelity animation. <em>TOG</em>, <em>42</em>(4), 41:1–20. (<a
href="https://doi.org/10.1145/3592093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant advancements have been made in developing parametric models for digital humans, with various approaches concentrating on parts such as the human body, hand, or face. Nevertheless, connectors such as the neck have been overlooked in these models, with rich anatomical priors often unutilized. In this paper, we introduce HACK (Head-And-neCK), a novel parametric model for constructing the head and cervical region of digital humans. Our model seeks to disentangle the full spectrum of neck and larynx motions, facial expressions, and appearance variations, providing personalized and anatomically consistent controls, particularly for the neck regions. To build our HACK model, we acquire a comprehensive multi-modal dataset of the head and neck under various facial expressions. We employ a 3D ultrasound imaging scheme to extract the inner biomechanical structures, namely the precise 3D rotation information of the seven vertebrae of the cervical spine. We then adopt a multi-view photometric approach to capture the geometry and physically-based textures of diverse subjects, who exhibit a diverse range of static expressions as well as sequential head-and-neck movements. Using the multi-modal dataset, we train the parametric HACK model by separating the 3D head and neck depiction into various shape, pose, expression, and larynx blendshapes from the neutral expression and the rest skeletal pose. We adopt an anatomically-consistent skeletal design for the cervical region, and the expression is linked to facial action units for artist-friendly controls. We also propose to optimize the mapping from the identical shape space to the PCA spaces of personalized blendshapes to augment the pose and expression blendshapes, providing personalized properties within the framework of the generic model. Furthermore, we use larynx blendshapes to accurately control the larynx deformation and force the larynx slicing motions along the vertical direction in the UV-space for precise modeling of the larynx beneath the neck skin. HACK addresses the head and neck as a unified entity, offering more accurate and expressive controls, with a new level of realism, particularly for the neck regions. This approach has significant benefits for numerous applications, including geometric fitting and animation, and enables inter-correlation analysis between head and neck for fine-grained motion synthesis and transfer.},
  archive      = {J_TOG},
  author       = {Longwen Zhang and Zijun Zhao and Xinzhou Cong and Qixuan Zhang and Shuqi Gu and Yuchong Gao and Rui Zheng and Wei Yang and Lan Xu and Jingyi Yu},
  doi          = {10.1145/3592093},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {41:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {HACK: Learning a parametric head and neck model for high-fidelity animation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anatomically detailed simulation of human torso.
<em>TOG</em>, <em>42</em>(4), 40:1–11. (<a
href="https://doi.org/10.1145/3592425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing digital human models approximate the human skeletal system using rigid bodies connected by rotational joints. While the simplification is considered acceptable for legs and arms, it significantly lacks fidelity to model rich torso movements in common activities such as dancing, Yoga, and various sports. Research from biomechanics provides more detailed modeling for parts of the torso, but their models often operate in isolation and are not fast and robust enough to support computationally heavy applications and large-scale data generation for full-body digital humans. This paper proposes a new torso model that aims to achieve high fidelity both in perception and in functionality, while being computationally feasible for simulation and optimal control tasks. We build a detailed human torso model consisting of various anatomical components, including facets, ligaments, and intervertebral discs, by coupling efficient finite-element and rigid-body simulations. Given an existing motion capture sequence without dense markers placed on the torso, our new model is able to recover the underlying torso bone movements. Our method is remarkably robust that it can be used to automatically &quot;retrofit&quot; the entire Mixamo motion database of highly diverse human motions without user intervention. We also show that our model is computationally efficient for solving trajectory optimization of highly dynamic full-body movements, without relying any reference motion. Physiological validity of the model is validated against established literature.},
  archive      = {J_TOG},
  author       = {Seunghwan Lee and Yifeng Jiang and C. Karen Liu},
  doi          = {10.1145/3592425},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {40:1–11},
  shortjournal = {ACM Trans. Graph.},
  title        = {Anatomically detailed simulation of human torso},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A practical wave optics reflection model for hair and fur.
<em>TOG</em>, <em>42</em>(4), 39:1–15. (<a
href="https://doi.org/10.1145/3592446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional fiber scattering models, based on ray optics, are missing some important visual aspects of fiber appearance. Previous work [Xia et al. 2020] on wave scattering from ideal extrusions demonstrated that diffraction produces strong forward scattering and colorful effects that are missing from ray-based models. However, that work was unable to include some important surface characteristics such as surface roughness and tilted cuticle scales, which are known to be important for fiber appearance. In this work, we take an important step to study wave effects from rough fibers with arbitrary 3D microgeometry. While the full-wave simulation of realistic 3D fibers remains intractable, we developed a 3D wave optics simulator based on a physical optics approximation, using a GPU-based hierarchical algorithm to greatly accelerate the calculation. It simulates surface reflection and diffractive scattering, which are present in all fibers and typically dominate for darkly pigmented fibers. The simulation provides a detailed picture of first order scattering, but it is not practical to use for production rendering as this would require tabulation per fiber geometry. To practically handle geometry variations in the scene, we propose a model based on wavelet noise, capturing the important statistical features in the simulation results that are relevant for rendering. Both our simulation and practical model show similar granular patterns to those observed in optical measurement. Our compact noise model can be easily combined with existing scattering models to render hair and fur of various colors, introducing visually important colorful glints that were missing from all previous models.},
  archive      = {J_TOG},
  author       = {Mengqi Xia and Bruce Walter and Christophe Hery and Olivier Maury and Eric Michielssen and Steve Marschner},
  doi          = {10.1145/3592446},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {39:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A practical wave optics reflection model for hair and fur},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology driven approximation to rational surface-surface
intersection via interval algebraic topology analysis. <em>TOG</em>,
<em>42</em>(4), 38:1–16. (<a
href="https://doi.org/10.1145/3592452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing the intersection between two parametric surfaces (SSI) is one of the most fundamental problems in geometric and solid modeling. Maintaining the SSI topology is critical to its computation robustness. We propose a topology-driven hybrid symbolic-numeric framework to approximate rational parametric surface-surface intersection (SSI) based on a concept of interval algebraic topology analysis (IATA) , which configures within a 4D interval box the SSI topology. We map the SSI topology to an algebraic system&#39;s solutions within the framework, classify and enumerate all topological cases as a mixture of four fundamental cases (or their specific sub-cases). Various complicated topological situations are covered, such as cusp points or curves, tangent points (isolated or not) or curves, tiny loops, self-intersections, or their mixtures. The theoretical formulation is also implemented numerically using advanced real solution isolation techniques, and computed within a topology-driven framework which maximally utilizes the advantages of the topology maintenance of algebraic analysis, the robustness of iterative subdivision, and the efficiency of forward marching. The approach demonstrates improved robustness under benchmark topological cases when compared with available open-source and commercial solutions, including IRIT, SISL, and Parasolid.},
  archive      = {J_TOG},
  author       = {Jin-San Cheng and Bingwei Zhang and Yikun Xiao and Ming Li},
  doi          = {10.1145/3592452},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {38:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Topology driven approximation to rational surface-surface intersection via interval algebraic topology analysis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible isosurface extraction for gradient-based mesh
optimization. <em>TOG</em>, <em>42</em>(4), 37:1–16. (<a
href="https://doi.org/10.1145/3592430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers gradient-based mesh optimization, where we iteratively optimize for a 3D surface mesh by representing it as the isosurface of a scalar field, an increasingly common paradigm in applications including photogrammetry, generative modeling, and inverse physics. Existing implementations adapt classic isosurface extraction algorithms like Marching Cubes or Dual Contouring; these techniques were designed to extract meshes from fixed, known fields, and in the optimization setting they lack the degrees of freedom to represent high-quality feature-preserving meshes, or suffer from numerical instabilities. We introduce FlexiCubes, an isosurface representation specifically designed for optimizing an unknown mesh with respect to geometric, visual, or even physical objectives. Our main insight is to introduce additional carefully-chosen parameters into the representation, which allow local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task. We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.},
  archive      = {J_TOG},
  author       = {Tianchang Shen and Jacob Munkberg and Jon Hasselgren and Kangxue Yin and Zian Wang and Wenzheng Chen and Zan Gojcic and Sanja Fidler and Nicholas Sharp and Jun Gao},
  doi          = {10.1145/3592430},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {37:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {Flexible isosurface extraction for gradient-based mesh optimization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Winding numbers on discrete surfaces. <em>TOG</em>,
<em>42</em>(4), 36:1–17. (<a
href="https://doi.org/10.1145/3592401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the plane, the winding number is the number of times a curve wraps around a given point. Winding numbers are a basic component of geometric algorithms such as point-in-polygon tests, and their generalization to data with noise or topological errors has proven valuable for geometry processing tasks ranging from surface reconstruction to mesh booleans. However, standard definitions do not immediately apply on surfaces, where not all curves bound regions. We develop a meaningful generalization, starting with the well-known relationship between winding numbers and harmonic functions. By processing the derivatives of such functions, we can robustly filter out components of the input that do not bound any region. Ultimately, our algorithm yields (i) a closed, completed version of the input curves, (ii) integer labels for regions that are meaningfully bounded by these curves, and (iii) the complementary curves that do not bound any region. The main computational cost is solving a standard Poisson equation, or for surfaces with nontrivial topology, a sparse linear program. We also introduce special basis functions to represent singularities that naturally occur at endpoints of open curves.},
  archive      = {J_TOG},
  author       = {Nicole Feng and Mark Gillespie and Keenan Crane},
  doi          = {10.1145/3592401},
  journal      = {ACM Transactions on Graphics},
  month        = {7},
  number       = {4},
  pages        = {36:1–17},
  shortjournal = {ACM Trans. Graph.},
  title        = {Winding numbers on discrete surfaces},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeRFFaceLighting: Implicit and disentangled face lighting
representation leveraging generative prior in neural radiance fields.
<em>TOG</em>, <em>42</em>(3), 35:1–18. (<a
href="https://doi.org/10.1145/3597300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D-aware portrait lighting control is an emerging and promising domain, thanks to the recent advance of generative adversarial networks and neural radiance fields. Existing solutions typically try to decouple the lighting from the geometry and appearance for disentangled control with an explicit lighting representation (e.g., Lambertian or Phong). However, they either are limited to a constrained lighting condition (e.g., directional light) or demand a tricky-to-fetch dataset as supervision for the intrinsic compositions (e.g., the albedo). We propose NeRFFaceLighting to explore an implicit representation for portrait lighting based on the pretrained tri-plane representation to address the above limitations. We approach this disentangled lighting-control problem by distilling the shading from the original fused representation of both appearance and lighting (i.e., one tri-plane) to their disentangled representations (i.e., two tri-planes) with the conditional discriminator to supervise the lighting effects. We further carefully design the regularization to reduce the ambiguity of such decomposition and enhance the ability of generalization to unseen lighting conditions. Moreover, our method can be extended to enable 3D-aware real portrait relighting. Through extensive quantitative and qualitative evaluations, we demonstrate the superior 3D-aware lighting control ability of our model compared to alternative and existing solutions.},
  archive      = {J_TOG},
  author       = {Kaiwen Jiang and Shu-Yu Chen and Hongbo Fu and Lin Gao},
  doi          = {10.1145/3597300},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {35:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {NeRFFaceLighting: Implicit and disentangled face lighting representation leveraging generative prior in neural radiance fields},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatiotemporally consistent HDR indoor lighting estimation.
<em>TOG</em>, <em>42</em>(3), 34:1–15. (<a
href="https://doi.org/10.1145/3595921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a physically motivated deep learning framework to solve a general version of the challenging indoor lighting estimation problem. Given a single LDR image with a depth map, our method predicts spatially consistent lighting at any given image position. Particularly, when the input is an LDR video sequence, our framework not only progressively refines the lighting prediction as it sees more regions, but also preserves temporal consistency by keeping the refinement smooth. Our framework reconstructs a spherical Gaussian lighting volume (SGLV) through a tailored 3D encoder-decoder, which enables spatially consistent lighting prediction through volume ray tracing, a hybrid blending network for detailed environment maps, an in-network Monte Carlo rendering layer to enhance photorealism for virtual object insertion, and recurrent neural networks (RNN) to achieve temporally consistent lighting prediction with a video sequence as the input. For training, we significantly enhance the OpenRooms public dataset of photorealistic synthetic indoor scenes with around 360k HDR environment maps of much higher resolution and 38k video sequences, rendered with GPU-based path tracing. Experiments show that our framework achieves lighting prediction with higher quality compared to state-of-the-art single-image or video-based methods, leading to photorealistic AR applications such as object insertion.},
  archive      = {J_TOG},
  author       = {Zhengqin Li and Li Yu and Mikhail Okunev and Manmohan Chandraker and Zhao Dong},
  doi          = {10.1145/3595921},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {34:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Spatiotemporally consistent HDR indoor lighting estimation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultra-high resolution SVBRDF recovery from a single image.
<em>TOG</em>, <em>42</em>(3), 33:1–14. (<a
href="https://doi.org/10.1145/3593798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing convolutional neural networks have achieved great success in recovering Spatially Varying Bidirectional Surface Reflectance Distribution Function (SVBRDF) maps from a single image. However, they mainly focus on handling low-resolution (e.g., 256 × 256) inputs. Ultra-High Resolution (UHR) material maps are notoriously difficult to acquire by existing networks because (1) finite computational resources set bounds for input receptive fields and output resolutions, and (2) convolutional layers operate locally and lack the ability to capture long-range structural dependencies in UHR images. We propose an implicit neural reflectance model and a divide-and-conquer solution to address these two challenges simultaneously. We first crop a UHR image into low-resolution patches, each of which are processed by a local feature extractor to extract important details. To fully exploit long-range spatial dependency and ensure global coherency, we incorporate a global feature extractor and several coordinate-aware feature assembly modules into our pipeline. The global feature extractor contains several lightweight material vision transformers that have a global receptive field at each scale and have the ability to infer long-term relationships in the material. After decoding globally coherent feature maps assembled by coordinate-aware feature assembly modules, the proposed end-to-end method is able to generate UHR SVBRDF maps from a single image with fine spatial details and consistent global structures.},
  archive      = {J_TOG},
  author       = {Jie Guo and Shuichang Lai and Qinghao Tu and Chengzhi Tao and Changqing Zou and Yanwen Guo},
  doi          = {10.1145/3593798},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {33:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Ultra-high resolution SVBRDF recovery from a single image},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A method for animating children’s drawings of the human
figure. <em>TOG</em>, <em>42</em>(3), 32:1–15. (<a
href="https://doi.org/10.1145/3592788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children’s drawings have a wonderful inventiveness, creativity, and variety to them. We present a system that automatically animates children’s drawings of the human figure, is robust to the variance inherent in these depictions, and is simple and straightforward enough for anyone to use. We demonstrate the value and broad appeal of our approach by building and releasing the Animated Drawings Demo, a freely available public website that has been used by millions of people around the world. We present a set of experiments exploring the amount of training data needed for fine-tuning, as well as a perceptual study demonstrating the appeal of a novel twisted perspective retargeting technique. Finally, we introduce the Amateur Drawings Dataset, a first-of-its-kind annotated dataset, collected via the public demo, containing over 178,000 amateur drawings and corresponding user-accepted character bounding boxes, segmentation masks, and joint location annotations.},
  archive      = {J_TOG},
  author       = {Harrison Jesse Smith and Qingyuan Zheng and Yifei Li and Somya Jain and Jessica K. Hodgins},
  doi          = {10.1145/3592788},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {32:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {A method for animating children’s drawings of the human figure},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Textured mesh quality assessment: Large-scale dataset and
deep learning-based quality metric. <em>TOG</em>, <em>42</em>(3),
31:1–20. (<a href="https://doi.org/10.1145/3592786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, three-dimensional (3D) graphics have become highly detailed to mimic the real world, exploding their size and complexity. Certain applications and device constraints necessitate their simplification and/or lossy compression, which can degrade their visual quality. Thus, to ensure the best Quality of Experience, it is important to evaluate the visual quality to accurately drive the compression and find the right compromise between visual quality and data size. In this work, we focus on subjective and objective quality assessment of textured 3D meshes. We first establish a large-scale dataset, which includes 55 source models quantitatively characterized in terms of geometric, color, and semantic complexity, and corrupted by combinations of five types of compression-based distortions applied on the geometry, texture mapping, and texture image of the meshes. This dataset contains over 343k distorted stimuli. We propose an approach to select a challenging subset of 3,000 stimuli for which we collected 148,929 quality judgments from over 4,500 participants in a large-scale crowdsourced subjective experiment. Leveraging our subject-rated dataset, a learning-based quality metric for 3D graphics was proposed. Our metric demonstrates state-of-the-art results on our dataset of textured meshes and on a dataset of distorted meshes with vertex colors. Finally, we present an application of our metric and dataset to explore the influence of distortion interactions and content characteristics on the perceived quality of compressed textured meshes.},
  archive      = {J_TOG},
  author       = {Yana Nehmé and Johanna Delanoy and Florent Dupont and Jean-Philippe Farrugia and Patrick Le Callet and Guillaume Lavoué},
  doi          = {10.1145/3592786},
  journal      = {ACM Transactions on Graphics},
  month        = {6},
  number       = {3},
  pages        = {31:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Textured mesh quality assessment: Large-scale dataset and deep learning-based quality metric},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How important are detailed hand motions for communication
for a virtual character through the lens of charades? <em>TOG</em>,
<em>42</em>(3), 27:1–16. (<a
href="https://doi.org/10.1145/3578575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detailed hand motions play an important role in face-to-face communication to emphasize points, describe objects, clarify concepts, or replace words altogether. While shared virtual reality (VR) spaces are becoming more popular, these spaces do not, in most cases, capture and display accurate hand motions. In this article, we investigate the consequences of such errors in hand and finger motions on comprehension, character perception, social presence, and user comfort. We conduct three perceptual experiments where participants guess words and movie titles based on motion captured movements. We introduce errors and alterations to the hand movements and apply techniques to synthesize or correct hand motions. We collect data from more than 1000 Amazon Mechanical Turk participants in two large experiments, and conduct a third experiment in VR. As results might differ depending on the virtual character used, we investigate all effects on two virtual characters of different levels of realism. We furthermore investigate the effects of clip length in our experiments. Amongst other results, we show that the absence of finger motion significantly reduces comprehension and negatively affects people’s perception of a virtual character and their social presence. Adding some hand motions, even random ones, does attenuate some of these effects when it comes to the perception of the virtual character or social presence, but it does not necessarily improve comprehension. Slightly inaccurate or erroneous hand motions are sufficient to achieve the same level of comprehension as with accurate hand motions. They might however still affect the viewers’ impression of a character. Finally, jittering hand motions should be avoided as they significantly decrease user comfort.},
  archive      = {J_TOG},
  author       = {Alex Adkins and Aline Normoyle and Lorraine Lin and Yu Sun and Yuting Ye and Massimiliano Di Luca and Sophie Jörg},
  doi          = {10.1145/3578575},
  journal      = {ACM Transactions on Graphics},
  month        = {5},
  number       = {3},
  pages        = {27:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {How important are detailed hand motions for communication for a virtual character through the lens of charades?},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parsing-conditioned anime translation: A new dataset and
method. <em>TOG</em>, <em>42</em>(3), 30:1–14. (<a
href="https://doi.org/10.1145/3585002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anime is an abstract art form that is substantially different from the human portrait, leading to a challenging misaligned image translation problem that is beyond the capability of existing methods. This can be boiled down to a highly ambiguous unconstrained translation between two domains. To this end, we design a new anime translation framework by deriving the prior knowledge of a pre-trained StyleGAN model. We introduce disentangled encoders to separately embed structure and appearance information into the same latent code, governed by four tailored losses. Moreover, we develop a FaceBank aggregation method that leverages the generated data of the StyleGAN, anchoring the prediction to produce in-domain animes. To empower our model and promote the research of anime translation, we propose the first anime portrait parsing dataset, Danbooru-Parsing , containing 4,921 densely labeled images across 17 classes. This dataset connects the face semantics with appearances, enabling our new constrained translation setting. We further show the editability of our results, and extend our method to manga images, by generating the first manga parsing pseudo data. Extensive experiments demonstrate the values of our new dataset and method, resulting in the first feasible solution on anime translation.},
  archive      = {J_TOG},
  author       = {Zhansheng Li and Yangyang Xu and Nanxuan Zhao and Yang Zhou and Yongtuo Liu and Dahua Lin and Shengfeng He},
  doi          = {10.1145/3585002},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {30:1–14},
  shortjournal = {ACM Trans. Graph.},
  title        = {Parsing-conditioned anime translation: A new dataset and method},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric optimisation via spectral shifting. <em>TOG</em>,
<em>42</em>(3), 29:1–15. (<a
href="https://doi.org/10.1145/3585003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a geometric optimisation framework that can recover fold-over free maps from non-injective initial states using popular flip-preventing distortion energies. Since flip-preventing energies are infinite for folded configurations, we propose a new regularisation scheme that shifts the singular values of the deformation gradient. This allow us to re-use many existing algorithms, especially locally injective methods for initially folded maps. Our regularisation is suitable for both singular value- and invariant-based formulations, and systematically contributes multiple stabilisers to the Hessian. In contrast to proxy-based techniques, we maintain second-order convergence. Compact expressions for the energy eigensystems can be obtained for our extended stretch invariants, enabling the use of fast projected Newton solvers. Although spectral shifting in general has no theoretical guarantees that the global minimum is an injection, extensive experiments show that our framework is fast and extremely robust in practice, and capable of generating high-quality maps from severely distorted, degenerate and folded initialisations.},
  archive      = {J_TOG},
  author       = {Roman Poya and Rogelio Ortigosa and Theodore Kim},
  doi          = {10.1145/3585003},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {29:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {Geometric optimisation via spectral shifting},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Symmetric volume maps: Order-invariant volumetric mesh
correspondence with free boundary. <em>TOG</em>, <em>42</em>(3),
25:1–20. (<a href="https://doi.org/10.1145/3572897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although shape correspondence is a central problem in geometry processing, most methods for this task apply only to two-dimensional surfaces. The neglected task of volumetric correspondence—a natural extension relevant to shapes extracted from simulation, medical imaging, and volume rendering—presents unique challenges that do not appear in the two-dimensional case. In this work, we propose a method for mapping between volumes represented as tetrahedral meshes. Our formulation minimizes a distortion energy designed to extract maps symmetrically, i.e., without dependence on the ordering of the source and target domains. We accompany our method with theoretical discussion describing the consequences of this symmetry assumption, leading us to select a symmetrized ARAP energy that favors isometric correspondences. Our final formulation optimizes for near-isometry while matching the boundary. We demonstrate our method on a diverse geometric dataset, producing low-distortion matchings that align closely to the boundary.},
  archive      = {J_TOG},
  author       = {S. Mazdak Abulnaga and Oded Stein and Polina Golland and Justin Solomon},
  doi          = {10.1145/3572897},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {3},
  pages        = {25:1–20},
  shortjournal = {ACM Trans. Graph.},
  title        = {Symmetric volume maps: Order-invariant volumetric mesh correspondence with free boundary},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven digital lighting design for residential indoor
spaces. <em>TOG</em>, <em>42</em>(3), 28:1–18. (<a
href="https://doi.org/10.1145/3582001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventionally, interior lighting design is technically complex yet challenging and requires professional knowledge and aesthetic disciplines of designers. This article presents a new digital lighting design framework for virtual interior scenes, which allows novice users to automatically obtain lighting layouts and interior rendering images with visually pleasing lighting effects. The proposed framework utilizes neural networks to retrieve and learn underlying design guidelines and the principles beneath the existing lighting designs, e.g., a newly constructed dataset of 6k 3D interior scenes from professional designers with dense annotations of lights. With a 3D furniture-populated indoor scene as the input, the framework takes two stages to perform lighting design: (1) lights are iteratively placed in the room; (2) the colors and intensities of the lights are optimized by an adversarial scheme, resulting in lighting designs with aesthetic lighting effects. Quantitative and qualitative experiments show that the proposed framework effectively learns the guidelines and principles and generates lighting designs that are preferred over the rule-based baseline and comparable to those of professional human designers.},
  archive      = {J_TOG},
  author       = {Haocheng Ren and Hangming Fan and Rui Wang and Yuchi Huo and Rui Tang and Lei Wang and Hujun Bao},
  doi          = {10.1145/3582001},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {28:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Data-driven digital lighting design for residential indoor spaces},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). As-continuous-as-possible extrusion-based fabrication of
surface models. <em>TOG</em>, <em>42</em>(3), 26:1–16. (<a
href="https://doi.org/10.1145/3575859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a computational framework for optimizing the continuity of the toolpath in fabricating surface models on an extrusion-based 3D printer. Toolpath continuity is a critical issue that influences both the quality and the efficiency of extrusion-based fabrication. Transfer moves lead to rough and bumpy surfaces, where this phenomenon worsens for materials with large viscosity, like clay. The effects of continuity on the surface models are even more severe in terms of the quality of the surface and the stability of the model. We introduce a criterion called the one–path patch (OPP) to represent a patch on the surface of the shell that can be traversed along one path by considering the constraints on fabrication. We study the properties of the OPPs and their merging operations to propose a bottom-up OPP merging procedure to decompose the given shell surface into a minimal number of OPPs, and to generate the “as-continuous-as-possible” (ACAP) toolpath. Furthermore, we augment the path planning algorithm with a curved-layer printing scheme that reduces staircase defects and improves the continuity of the toolpath by connecting multiple segments. We evaluated the ACAP algorithm on ceramic and thermoplastic materials, and the results showed that it improves the fabrication of surface models in terms of both efficiency and surface quality.},
  archive      = {J_TOG},
  author       = {Fanchao Zhong and Yonglai Xu and Haisen Zhao and Lin Lu},
  doi          = {10.1145/3575859},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {3},
  pages        = {26:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {As-continuous-as-possible extrusion-based fabrication of surface models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OpenMPD: A low-level presentation engine for multimodal
particle-based displays. <em>TOG</em>, <em>42</em>(2), 24:1–13. (<a
href="https://doi.org/10.1145/3572896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phased arrays of transducers have been quickly evolving in terms of software and hardware with applications in haptics (acoustic vibrations), display (levitation), and audio. Most recently, Multimodal Particle-based Displays (MPDs) have even demonstrated volumetric content that can be seen, heard, and felt simultaneously, without additional instrumentation. However, current software tools only support individual modalities and they do not address the integration and exploitation of the multi-modal potential of MPDs. This is because there is no standardized presentation pipeline tackling the challenges related to presenting such kind of multi-modal content (e.g., multi-modal support, multi-rate synchronization at 10 KHz, visual rendering or synchronization and continuity). This article presents OpenMPD, a low-level presentation engine that deals with these challenges and allows structured exploitation of any type of MPD content (i.e., visual, tactile, audio). We characterize OpenMPD’s performance and illustrate how it can be integrated into higher-level development tools (i.e., Unity game engine). We then illustrate its ability to enable novel presentation capabilities, such as support of multiple MPD contents, dexterous manipulations of fast-moving particles, or novel swept-volume MPD content.},
  archive      = {J_TOG},
  author       = {Roberto Montano-Murillo and Ryuji Hirayama and Diego Martinez Plasencia},
  doi          = {10.1145/3572896},
  journal      = {ACM Transactions on Graphics},
  month        = {4},
  number       = {2},
  pages        = {24:1–13},
  shortjournal = {ACM Trans. Graph.},
  title        = {OpenMPD: A low-level presentation engine for multimodal particle-based displays},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep appearance prefiltering. <em>TOG</em>, <em>42</em>(2),
23:1–23. (<a href="https://doi.org/10.1145/3570327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically based rendering of complex scenes can be prohibitively costly with a potentially unbounded and uneven distribution of complexity across the rendered image. The goal of an ideal level of detail (LoD) method is to make rendering costs independent of the 3D scene complexity, while preserving the appearance of the scene. However, current prefiltering LoD methods are limited in the appearances they can support due to their reliance of approximate models and other heuristics. We propose the first comprehensive multi-scale LoD framework for prefiltering 3D environments with complex geometry and materials (e.g., the Disney BRDF), while maintaining the appearance with respect to the ray-traced reference. Using a multi-scale hierarchy of the scene, we perform a data-driven prefiltering step to obtain an appearance phase function and directional coverage mask at each scale. At the heart of our approach is a novel neural representation that encodes this information into a compact latent form that is easy to decode inside a physically based renderer. Once a scene is baked out, our method requires no original geometry, materials, or textures at render time. We demonstrate that our approach compares favorably to state-of-the-art prefiltering methods and achieves considerable savings in memory for complex scenes.},
  archive      = {J_TOG},
  author       = {Steve Bako and Pradeep Sen and Anton Kaplanyan},
  doi          = {10.1145/3570327},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {23:1–23},
  shortjournal = {ACM Trans. Graph.},
  title        = {Deep appearance prefiltering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sparse distributed gigascale resolution material point
method. <em>TOG</em>, <em>42</em>(2), 22:1–21. (<a
href="https://doi.org/10.1145/3570160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a four-layer distributed simulation system and its adaptation to the Material Point Method (MPM). The system is built upon a performance portable C++ programming model targeting major High-Performance-Computing (HPC) platforms. A key ingredient of our system is a hierarchical block-tile-cell sparse grid data structure that is distributable to an arbitrary number of Message Passing Interface (MPI) ranks. We additionally propose strategies for efficient dynamic load balance optimization to maximize the efficiency of MPI tasks. Our simulation pipeline can easily switch among backend programming models, including OpenMP and CUDA, and can be effortlessly dispatched onto supercomputers and the cloud. Finally, we construct benchmark experiments and ablation studies on supercomputers and consumer workstations in a local network to evaluate the scalability and load balancing criteria. We demonstrate massively parallel, highly scalable, and gigascale resolution MPM simulations of up to 1.01 billion particles for less than 323.25 seconds per frame with 8 OpenSSH-connected workstations.},
  archive      = {J_TOG},
  author       = {Yuxing Qiu and Samuel Temple Reeve and Minchen Li and Yin Yang and Stuart Ryan Slattery and Chenfanfu Jiang},
  doi          = {10.1145/3570160},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {2},
  pages        = {22:1–21},
  shortjournal = {ACM Trans. Graph.},
  title        = {A sparse distributed gigascale resolution material point method},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Breaking good: Fracture modes for realtime destruction.
<em>TOG</em>, <em>42</em>(1), 10:1–12. (<a
href="https://doi.org/10.1145/3549540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drawing a direct analogy with the well-studied vibration or elastic modes, we introduce an object’s fracture modes , which constitute its preferred or most natural ways of breaking. We formulate a sparsified eigenvalue problem, which we solve iteratively to obtain the n lowest-energy modes. These can be precomputed for a given shape to obtain a prefracture pattern that can substitute the state of the art for realtime applications at no runtime cost but significantly greater realism. Furthermore, any realtime impact can be projected onto our modes to obtain impact-dependent fracture patterns without the need for any online crack propagation simulation. We not only introduce this theoretically novel concept, but also show its fundamental and practical advantages in a diverse set of examples and contexts.},
  archive      = {J_TOG},
  author       = {Silvia Sellán and Jack Luong and Leticia Mattos Da Silva and Aravind Ramakrishnan and Yuchuan Yang and Alec Jacobson},
  doi          = {10.1145/3549540},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {1},
  pages        = {10:1–12},
  shortjournal = {ACM Trans. Graph.},
  title        = {Breaking good: Fracture modes for realtime destruction},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Centimeter-wave free-space neural time-of-flight imaging.
<em>TOG</em>, <em>42</em>(1), 3:1–18. (<a
href="https://doi.org/10.1145/3522671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth sensors have emerged as a cornerstone sensor modality with diverse applications in personal hand-held devices, robotics, scientific imaging, autonomous vehicles, and more. In particular, correlation Time-of-Flight (ToF) sensors have found widespread adoption for meter-scale indoor applications such as object tracking and pose estimation. While they offer high depth resolution at competitive costs, the precision of these indirect ToF sensors is fundamentally limited by their modulation contrast, which is in turn limited by the effects of photo-conversion noise. In contrast, optical interferometric methods can leverage short illumination modulation wavelengths to achieve depth precision three orders of magnitude greater than ToF, but typically find their range is restricted to the sub-centimeter. In this work, we merge concepts from both correlation ToF design and interferometric imaging; a step towards bridging the gap between these methods. We propose a computational ToF imaging method that optically computes the GHz ToF correlation signal in free space before photo-conversion. To acquire a depth map, we scan a scene point-wise and computationally unwrap the collected correlation measurements. Specifically, we repurpose electro-optical modulators used in optical communication for ToF imaging with centimeter-wave signals, and achieve all-optical correlation at 7.15 GHz and 14.32 GHz modulation frequencies. While GHz modulation frequencies increase depth precision, these high modulation rates also pose a technical challenge. They result in dozens of wraps per meter which cannot be estimated robustly by existing phase unwrapping methods. We tackle this problem with a proposed segmentation-inspired phase unwrapping network , which exploits the correlation of adjacent GHz phase measurements to classify regions into their respective wrap counts. We validate this method in simulation and experimentally, and demonstrate precise depth sensing using centimeter wave modulation that is robust to surface texture and ambient light. Compared to existing analog demodulation methods, the proposed system outperforms all of them across all tested scenarios. CCS Concepts: • Computing methodologies ;},
  archive      = {J_TOG},
  author       = {Seung-Hwan Baek and Noah Walsh and Ilya Chugunov and Zheng Shi and Felix Heide},
  doi          = {10.1145/3522671},
  journal      = {ACM Transactions on Graphics},
  month        = {3},
  number       = {1},
  pages        = {3:1–18},
  shortjournal = {ACM Trans. Graph.},
  title        = {Centimeter-wave free-space neural time-of-flight imaging},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The statistics of eye movements and binocular disparities
during VR gaming: Implications for headset design. <em>TOG</em>,
<em>42</em>(1), 7:1–15. (<a
href="https://doi.org/10.1145/3549529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human visual system evolved in environments with statistical regularities. Binocular vision is adapted to these such that depth perception and eye movements are more precise, faster, and performed comfortably in environments consistent with the regularities. We measured the statistics of eye movements and binocular disparities in virtual-reality (VR) -gaming environments and found that they are quite different from those in the natural environment. Fixation distance and direction are more restricted in VR, and fixation distance is farther. The pattern of disparity across the visual field is less regular in VR and does not conform to a prominent property of naturally occurring disparities. From this we predict that double vision is more likely in VR than in the natural environment. We also determined the optimal screen distance to minimize discomfort due to the vergence-accommodation conflict, and the optimal nasal-temporal positioning of head-mounted display (HMD) screens to maximize binocular field of view. Finally, in a user study we investigated how VR content affects comfort and performance. Content that is more consistent with the statistics of the natural world yields less discomfort than content that is not. Furthermore, consistent content yields slightly better performance than inconsistent content.},
  archive      = {J_TOG},
  author       = {Avi M. Aizenman and George A. Koulieris and Agostino Gibaldi and Vibhor Sehgal and Dennis M. Levi and Martin S. Banks},
  doi          = {10.1145/3549529},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {1},
  pages        = {7:1–15},
  shortjournal = {ACM Trans. Graph.},
  title        = {The statistics of eye movements and binocular disparities during VR gaming: Implications for headset design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ConTesse: Accurate occluding contours for subdivision
surfaces. <em>TOG</em>, <em>42</em>(1), 5:1–16. (<a
href="https://doi.org/10.1145/3544778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a method for computing the visible occluding contours of subdivision surfaces. The article first introduces new theory for contour visibility of smooth surfaces. Necessary and sufficient conditions are introduced for when a sampled occluding contour is valid, that is, when it may be assigned consistent visibility. Previous methods do not guarantee these conditions, which helps explain why smooth contour visibility has been such a challenging problem in the past. The article then proposes an algorithm that, given a subdivision surface, finds sampled contours satisfying these conditions, and then generates a new triangle mesh matching the given occluding contours. The contours of the output triangle mesh may then be rendered with standard non-photorealistic rendering algorithms, using the mesh for visibility computation. The method can be applied to any triangle mesh, by treating it as the base mesh of a subdivision surface.},
  archive      = {J_TOG},
  author       = {Chenxi Liu and Pierre Bénard and Aaron Hertzmann and Shayan Hoshyari},
  doi          = {10.1145/3544778},
  journal      = {ACM Transactions on Graphics},
  month        = {1},
  number       = {1},
  pages        = {5:1–16},
  shortjournal = {ACM Trans. Graph.},
  title        = {ConTesse: Accurate occluding contours for subdivision surfaces},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
