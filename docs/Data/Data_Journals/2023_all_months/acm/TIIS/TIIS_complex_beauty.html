<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIIS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tiis---31">TIIS - 31</h2>
<ul>
<li><details>
<summary>
(2023). Directive explanations for actionable explainability in
machine learning applications. <em>TIIS</em>, <em>13</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3579363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we show that explanations of decisions made by machine learning systems can be improved by not only explaining why a decision was made but also explaining how an individual could obtain their desired outcome. We formally define the concept of directive explanations (those that offer specific actions an individual could take to achieve their desired outcome), introduce two forms of directive explanations (directive-specific and directive-generic), and describe how these can be generated computationally. We investigate people’s preference for and perception toward directive explanations through two online studies, one quantitative and the other qualitative, each covering two domains (the credit scoring domain and the employee satisfaction domain). We find a significant preference for both forms of directive explanations compared to non-directive counterfactual explanations. However, we also find that preferences are affected by many aspects, including individual preferences and social factors. We conclude that deciding what type of explanation to provide requires information about the recipients and other contextual information. This reinforces the need for a human-centered and context-specific approach to explainable AI.},
  archive      = {J_TIIS},
  doi          = {10.1145/3579363},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Directive explanations for actionable explainability in machine learning applications},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-design of human-centered, explainable AI for clinical
decision support. <em>TIIS</em>, <em>13</em>(4), 1–35. (<a
href="https://doi.org/10.1145/3587271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback with a two-fold outcome: First, we obtain evidence that explanations increase users’ trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so we can re-design a better, more human-centered explanation interface.},
  archive      = {J_TIIS},
  doi          = {10.1145/3587271},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-35},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Co-design of human-centered, explainable AI for clinical decision support},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual analytics of neuron vulnerability to adversarial
attacks on convolutional neural networks. <em>TIIS</em>, <em>13</em>(4),
1–26. (<a href="https://doi.org/10.1145/3587470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks on a convolutional neural network (CNN)—injecting human-imperceptible perturbations into an input image—could fool a high-performance CNN into making incorrect predictions. The success of adversarial attacks raises serious concerns about the robustness of CNNs, and prevents them from being used in safety-critical applications, such as medical diagnosis and autonomous driving. Our work introduces a visual analytics approach to understanding adversarial attacks by answering two questions: (1) Which neurons are more vulnerable to attacks? and (2) Which image features do these vulnerable neurons capture during the prediction? For the first question, we introduce multiple perturbation-based measures to break down the attacking magnitude into individual CNN neurons and rank the neurons by their vulnerability levels. For the second, we identify image features (e.g., cat ears) that highly stimulate a user-selected neuron to augment and validate the neuron’s responsibility. Furthermore, we support an interactive exploration of a large number of neurons by aiding with hierarchical clustering based on the neurons’ roles in the prediction. To this end, a visual analytics system is designed to incorporate visual reasoning for interpreting adversarial attacks. We validate the effectiveness of our system through multiple case studies as well as feedback from domain experts.},
  archive      = {J_TIIS},
  doi          = {10.1145/3587470},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Visual analytics of neuron vulnerability to adversarial attacks on convolutional neural networks},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effects of AI and logic-style explanations on users’
decisions under different levels of uncertainty. <em>TIIS</em>,
<em>13</em>(4), 1–42. (<a
href="https://doi.org/10.1145/3588320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing eXplainable Artificial Intelligence (XAI) techniques support people in interpreting AI advice. However, although previous work evaluates the users’ understanding of explanations, factors influencing the decision support are largely overlooked in the literature. This article addresses this gap by studying the impact of user uncertainty , AI correctness , and the interaction between AI uncertainty and explanation logic-styles for classification tasks. We conducted two separate studies: one requesting participants to recognize handwritten digits and one to classify the sentiment of reviews. To assess the decision making, we analyzed the task performance, agreement with the AI suggestion, and the user’s reliance on the XAI interface elements. Participants make their decision relying on three pieces of information in the XAI interface (image or text instance, AI prediction, and explanation). Participants were shown one explanation style (between-participants design) according to three styles of logical reasoning (inductive, deductive, and abductive). This allowed us to study how different levels of AI uncertainty influence the effectiveness of different explanation styles. The results show that user uncertainty and AI correctness on predictions significantly affected users’ classification decisions considering the analyzed metrics. In both domains (images and text), users relied mainly on the instance to decide. Users were usually overconfident about their choices, and this evidence was more pronounced for text. Furthermore, the inductive style explanations led to overreliance on the AI advice in both domains—it was the most persuasive, even when the AI was incorrect. The abductive and deductive styles have complex effects depending on the domain and the AI uncertainty levels.},
  archive      = {J_TIIS},
  doi          = {10.1145/3588320},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-42},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Effects of AI and logic-style explanations on users’ decisions under different levels of uncertainty},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How do users experience traceability of AI systems?
Examining subjective information processing awareness in automated
insulin delivery (AID) systems. <em>TIIS</em>, <em>13</em>(4), 1–34. (<a
href="https://doi.org/10.1145/3588594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When interacting with artificial intelligence (AI) in the medical domain, users frequently face automated information processing, which can remain opaque to them. For example, users with diabetes may interact daily with automated insulin delivery (AID). However, effective AID therapy requires traceability of automated decisions for diverse users. Grounded in research on human-automation interaction, we study Subjective Information Processing Awareness (SIPA) as a key construct to research users’ experience of explainable AI. The objective of the present research was to examine how users experience differing levels of traceability of an AI algorithm. We developed a basic AID simulation to create realistic scenarios for an experiment with N = 80, where we examined the effect of three levels of information disclosure on SIPA and performance. Attributes serving as the basis for insulin needs calculation were shown to users, who predicted the AID system’s calculation after over 60 observations. Results showed a difference in SIPA after repeated observations, associated with a general decline of SIPA ratings over time. Supporting scale validity, SIPA was strongly correlated with trust and satisfaction with explanations. The present research indicates that the effect of different levels of information disclosure may need several repetitions before it manifests. Additionally, high levels of information disclosure may lead to a miscalibration between SIPA and performance in predicting the system’s results. The results indicate that for a responsible design of XAI, system designers could utilize prediction tasks in order to calibrate experienced traceability.},
  archive      = {J_TIIS},
  doi          = {10.1145/3588594},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-34},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {How do users experience traceability of AI systems? examining subjective information processing awareness in automated insulin delivery (AID) systems},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LIMEADE: From AI explanations to advice taking.
<em>TIIS</em>, <em>13</em>(4), 1–29. (<a
href="https://doi.org/10.1145/3589345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in human-centered AI has shown the benefits of systems that can explain their predictions. Methods that allow AI to take advice from humans in response to explanations are similarly useful. While both capabilities are well developed for transparent learning models (e.g., linear models and GA 2 Ms) and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, little attention has been given to advice methods for opaque models. This article introduces LIMEADE, the first general framework that translates both positive and negative advice (expressed using high-level vocabulary such as that employed by post hoc explanations) into an update to an arbitrary, underlying opaque model. We demonstrate the generality of our approach with case studies on 70 real-world models across two broad domains: image classification and text recommendation. We show that our method improves accuracy compared to a rigorous baseline on the image classification domains. For the text modality, we apply our framework to a neural recommender system for scientific papers on a public website; our user study shows that our framework leads to significantly higher perceived user control, trust, and satisfaction.},
  archive      = {J_TIIS},
  doi          = {10.1145/3589345},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {LIMEADE: From AI explanations to advice taking},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of explainable AI in the research field of AI
ethics. <em>TIIS</em>, <em>13</em>(4), 1–39. (<a
href="https://doi.org/10.1145/3599974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ethics of Artificial Intelligence (AI) is a growing research field that has emerged in response to the challenges related to AI. Transparency poses a key challenge for implementing AI ethics in practice. One solution to transparency issues is AI systems that can explain their decisions. Explainable AI (XAI) refers to AI systems that are interpretable or understandable to humans. The research fields of AI ethics and XAI lack a common framework and conceptualization. There is no clarity of the field’s depth and versatility. A systematic approach to understanding the corpus is needed. A systematic review offers an opportunity to detect research gaps and focus points. This article presents the results of a systematic mapping study (SMS) of the research field of the Ethics of AI. The focus is on understanding the role of XAI and how the topic has been studied empirically. An SMS is a tool for performing a repeatable and continuable literature search. This article contributes to the research field with a Systematic Map that visualizes what, how, when, and why XAI has been studied empirically in the field of AI ethics. The mapping reveals research gaps in the area. Empirical contributions are drawn from the analysis. The contributions are reflected on in regards to theoretical and practical implications. As the scope of the SMS is a broader research area of AI ethics, the collected dataset opens possibilities to continue the mapping process in other directions.},
  archive      = {J_TIIS},
  doi          = {10.1145/3599974},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-39},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {The role of explainable AI in the research field of AI ethics},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Does this explanation help? Designing local model-agnostic
explanation representations and an experimental evaluation using
eye-tracking technology. <em>TIIS</em>, <em>13</em>(4), 1–47. (<a
href="https://doi.org/10.1145/3607145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Explainable Artificial Intelligence (XAI) research, various local model-agnostic methods have been proposed to explain individual predictions to users in order to increase the transparency of the underlying Artificial Intelligence (AI) systems. However, the user perspective has received less attention in XAI research, leading to a (1) lack of involvement of users in the design process of local model-agnostic explanations representations and (2) a limited understanding of how users visually attend them. Against this backdrop, we refined representations of local explanations from four well-established model-agnostic XAI methods in an iterative design process with users. Moreover, we evaluated the refined explanation representations in a laboratory experiment using eye-tracking technology as well as self-reports and interviews. Our results show that users do not necessarily prefer simple explanations and that their individual characteristics, such as gender and previous experience with AI systems, strongly influence their preferences. In addition, users find that some explanations are only useful in certain scenarios making the selection of an appropriate explanation highly dependent on context. With our work, we contribute to ongoing research to improve transparency in AI.},
  archive      = {J_TIIS},
  doi          = {10.1145/3607145},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-47},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Does this explanation help? designing local model-agnostic explanation representations and an experimental evaluation using eye-tracking technology},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). XAutoML: A visual analytics tool for understanding and
validating automated machine learning. <em>TIIS</em>, <em>13</em>(4),
1–39. (<a href="https://doi.org/10.1145/3625240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last 10 years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 36 domain experts, data scientists, and AutoML researchers from different professions with vastly different expertise in ML, we collect detailed informational needs for AutoML. We propose XAutoML , an interactive visual analytics tool for explaining arbitrary AutoML optimization procedures and ML pipelines constructed by AutoML. XAutoML combines interactive visualizations with established techniques from explainable artificial intelligence (XAI) to make the complete AutoML procedure transparent and explainable. By integrating XAutoML with JupyterLab , experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from XAutoML . We validate our approach in a user study with the same diverse user group from the requirements analysis. All participants were able to extract useful information from XAutoML , leading to a significantly increased understanding of ML pipelines produced by AutoML and the AutoML optimization itself.},
  archive      = {J_TIIS},
  doi          = {10.1145/3625240},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-39},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {XAutoML: A visual analytics tool for understanding and validating automated machine learning},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable activity recognition in videos using deep
learning and tractable probabilistic models. <em>TIIS</em>,
<em>13</em>(4), 1–32. (<a
href="https://doi.org/10.1145/3626961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the following video activity recognition (VAR) task: given a video, infer the set of activities being performed in the video and assign each frame to an activity. Although VAR can be solved accurately using existing deep learning techniques, deep networks are neither interpretable nor explainable and as a result their use is problematic in high stakes decision-making applications (in healthcare, experimental Biology, aviation, law, etc.). In such applications, failure may lead to disastrous consequences and therefore it is necessary that the user is able to either understand the inner workings of the model or probe it to understand its reasoning patterns for a given decision. We address these limitations of deep networks by proposing a new approach that feeds the output of a deep model into a tractable, interpretable probabilistic model called a dynamic conditional cutset network that is defined over the explanatory and output variables and then performing joint inference over the combined model. The two key benefits of using cutset networks are: (a) they explicitly model the relationship between the output and explanatory variables and as a result, the combined model is likely to be more accurate than the vanilla deep model and (b) they can answer reasoning queries in polynomial time and as a result, they can derive meaningful explanations by efficiently answering explanation queries. We demonstrate the efficacy of our approach on two datasets, Textually Annotated Cooking Scenes (TACoS), and wet lab, using conventional evaluation measures such as the Jaccard Index and Hamming Loss, as well as a human-subjects study.},
  archive      = {J_TIIS},
  doi          = {10.1145/3626961},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-32},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Explainable activity recognition in videos using deep learning and tractable probabilistic models},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meaningful explanation effect on user’s trust in an AI
medical system: Designing explanations for non-expert users.
<em>TIIS</em>, <em>13</em>(4), 1–39. (<a
href="https://doi.org/10.1145/3631614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Whereas most research in AI system explanation for healthcare applications looks at developing algorithmic explanations targeted at AI experts or medical professionals, the question we raise is: How do we build meaningful explanations for laypeople? And how does a meaningful explanation affect user’s trust perceptions? Our research investigates how the key factors affecting human-AI trust change in the light of human expertise, and how to design explanations specifically targeted at non-experts. By means of a stage-based design method, we map the ways laypeople understand AI explanations in a User Explanation Model. We also map both medical professionals and AI experts’ practice in an Expert Explanation Model. A Target Explanation Model is then proposed, which represents how experts’ practice and layperson’s understanding can be combined to design meaningful explanations. Design guidelines for meaningful AI explanations are proposed, and a prototype of AI system explanation for non-expert users in a breast cancer scenario is presented and assessed on how it affect users’ trust perceptions.},
  archive      = {J_TIIS},
  doi          = {10.1145/3631614},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {12},
  number       = {4},
  pages        = {1-39},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Meaningful explanation effect on user’s trust in an AI medical system: Designing explanations for non-expert users},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning and understanding user interface semantics from
heterogeneous networks with multimodal and positional attributes.
<em>TIIS</em>, <em>13</em>(3), 1–31. (<a
href="https://doi.org/10.1145/3578522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User interfaces (UI) of desktop, web, and mobile applications involve a hierarchy of objects (e.g., applications, screens, view class, and other types of design objects) with multimodal (e.g., textual and visual) and positional (e.g., spatial location, sequence order, and hierarchy level) attributes. We can therefore represent a set of application UIs as a heterogeneous network with multimodal and positional attributes. Such a network not only represents how users understand the visual layout of UIs but also influences how users would interact with applications through these UIs. To model the UI semantics well for different UI annotation, search, and evaluation tasks, this article proposes the novel Heterogeneous Attention-based Multimodal Positional (HAMP) graph neural network model. HAMP combines graph neural networks with the scaled dot-product attention used in transformers to learn the embeddings of heterogeneous nodes and associated multimodal and positional attributes in a unified manner. HAMP is evaluated with classification and regression tasks conducted on three distinct real-world datasets. Our experiments demonstrate that HAMP significantly out-performs other state-of-the-art models on such tasks. To further provide interpretations of the contribution of heterogeneous network information for understanding the relationships between the UI structure and prediction tasks, we propose Adaptive HAMP (AHAMP), which adaptively learns the importance of different edges linking different UI objects. Our experiments demonstrate AHAMP’s superior performance over HAMP on a number of tasks, and its ability to provide interpretations of the contribution of multimodal and positional attributes, as well as heterogeneous network information to different tasks.},
  archive      = {J_TIIS},
  doi          = {10.1145/3578522},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-31},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Learning and understanding user interface semantics from heterogeneous networks with multimodal and positional attributes},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The impact of intelligent pedagogical agents’ interventions
on student behavior and performance in open-ended game design
environments. <em>TIIS</em>, <em>13</em>(3), 1–29. (<a
href="https://doi.org/10.1145/3578523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research has shown that free-form Game-Design (GD) environments can be very effective in fostering Computational Thinking (CT) skills at a young age. However, some students can still need some guidance during the learning process due to the highly open-ended nature of these environments. Intelligent Pedagogical Agents (IPAs) can be used to provide personalized assistance in real-time to alleviate this challenge. This paper presents our results in evaluating such an agent deployed in a real-word free-form GD learning environment to foster CT in the early K-12 education, Unity-CT. We focus on the effect of repetition by comparing student behaviors between no intervention, 1-shot, and repeated intervention groups for two different errors that are known to be challenging in the online lessons of Unity-CT. Our findings showed that the agent was perceived very positively by the students and the repeated intervention showed promising results in terms of helping students make fewer errors and more correct behaviors, albeit only for one of the two target errors. Building from these results, we provide insights on how to provide IPA interventions in free-form GD environments.},
  archive      = {J_TIIS},
  doi          = {10.1145/3578523},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-29},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {The impact of intelligent pedagogical agents’ interventions on student behavior and performance in open-ended game design environments},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling efficient web data-record interaction for people
with visual impairments via proxy interfaces. <em>TIIS</em>,
<em>13</em>(3), 1–27. (<a
href="https://doi.org/10.1145/3579364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web data records are usually accompanied by auxiliary webpage segments, such as filters, sort options, search form, and multi-page links, to enhance interaction efficiency and convenience for end users. However, blind and visually impaired (BVI) persons are presently unable to fully exploit the auxiliary segments like their sighted peers, since these segments are scattered all across the screen, and as such assistive technologies used by BVI users, i.e., screen reader and screen magnifier, are not geared for efficient interaction with such scattered content. Specifically, for blind screen reader users, content navigation is predominantly one-dimensional despite the support for skipping content, and therefore navigating to-and-fro between different parts of the webpage is tedious and frustrating. Similarly, low vision screen magnifier users have to continuously pan back-and-forth between different portions of a webpage, given that only a portion of the screen is viewable at any instant due to content enlargement. The extant techniques to overcome inefficient web interaction for BVI users have mostly focused on general web-browsing activities, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting – activities that are equally important for facilitating quick and easy access to desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom machine learning-based algorithms to automatically extract auxiliary segments on any webpage containing data records; and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted auxiliary segments using either basic keyboard shortcuts or mouse actions. Evaluation studies with 14 blind participants and 16 low vision participants showed significant improvement in web usability with InSupport, driven by increased reduction in interaction time and user effort, compared to the state-of-the-art solutions.},
  archive      = {J_TIIS},
  doi          = {10.1145/3579364},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Enabling efficient web data-record interaction for people with visual impairments via proxy interfaces},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conversational context-sensitive ad generation with a few
core-queries. <em>TIIS</em>, <em>13</em>(3), 1–37. (<a
href="https://doi.org/10.1145/3588578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When people are talking together in front of digital signage, advertisements that are aware of the context of the dialogue will work the most effectively. However, it has been challenging for computer systems to retrieve the appropriate advertisement from among the many options presented in large databases. Our proposed system, the Conversational Context-sensitive Advertisement generator (CoCoA), is the first attempt to apply masked word prediction to web information retrieval that takes into account the dialogue context. The novelty of CoCoA is that advertisers simply need to prepare a few abstract phrases, called Core-Queries, and then CoCoA automatically generates a context-sensitive expression as a complete search query by utilizing a masked word prediction technique that adds a word related to the dialogue context to one of the prepared Core-Queries. This automatic generation frees the advertisers from having to come up with context-sensitive phrases to attract users’ attention. Another unique point is that the modified Core-Query offers users speaking in front of the CoCoA system a list of context-sensitive advertisements. CoCoA was evaluated by crowd workers regarding the context-sensitivity of the generated search queries against the dialogue text of multiple domains prepared in advance. The results indicated that CoCoA could present more contextual and practical advertisements than other web-retrieval systems. Moreover, CoCoA acquired a higher evaluation in a particular conversation that included many travel topics to which the Core-Queries were designated, implying that it succeeded in adapting the Core-Queries for the specific ongoing context better than the compared method without any effort on the part of the advertisers. In addition, case studies with users and advertisers revealed that the context-sensitive advertisements generated by CoCoA also had an effect on the content of the ongoing dialogue. Specifically, since pairs unfamiliar with each other more frequently referred to the advertisement CoCoA displayed, the advertisements had an effect on the topics about which the pairs spoke. Moreover, participants of an advertiser role recognized that some of the search queries generated by CoCoA fit the context of a conversation and that CoCoA improved the effect of the advertisement. In particular, they learned how to design of designing a good Core-Query at ease by observing the users’ response to the advertisements retrieved with the generated search queries.},
  archive      = {J_TIIS},
  doi          = {10.1145/3588578},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-37},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Conversational context-sensitive ad generation with a few core-queries},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowdsourcing thumbnail captions: Data collection and
validation. <em>TIIS</em>, <em>13</em>(3), 1–28. (<a
href="https://doi.org/10.1145/3589346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech interfaces, such as personal assistants and screen readers, read image captions to users. Typically, however, only one caption is available per image, which may not be adequate for all situations (e.g., browsing large quantities of images). Long captions provide a deeper understanding of an image but require more time to listen to, whereas shorter captions may not allow for such thorough comprehension yet have the advantage of being faster to consume. We explore how to effectively collect both thumbnail captions—succinct image descriptions meant to be consumed quickly—and comprehensive captions—which allow individuals to understand visual content in greater detail. We consider text-based instructions and time-constrained methods to collect descriptions at these two levels of detail and find that a time-constrained method is the most effective for collecting thumbnail captions while preserving caption accuracy. Additionally, we verify that caption authors using this time-constrained method are still able to focus on the most important regions of an image by tracking their eye gaze. We evaluate our collected captions along human-rated axes—correctness, fluency, amount of detail, and mentions of important concepts—and discuss the potential for model-based metrics to perform large-scale automatic evaluations in the future.},
  archive      = {J_TIIS},
  doi          = {10.1145/3589346},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Crowdsourcing thumbnail captions: Data collection and validation},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RadarSense: Accurate recognition of mid-air hand gestures
with radar sensing and few training examples. <em>TIIS</em>,
<em>13</em>(3), 1–45. (<a
href="https://doi.org/10.1145/3589645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microwave radars bring many benefits to mid-air gesture sensing due to their large field of view and independence from environmental conditions, such as ambient light and occlusion. However, radar signals are highly dimensional and usually require complex deep learning approaches. To understand this landscape, we report results from a systematic literature review of ( N =118) scientific papers on radar sensing, unveiling a large variety of radar technology of different operating frequencies and bandwidths and antenna configurations but also various gesture recognition techniques. Although highly accurate, these techniques require a large amount of training data that depend on the type of radar. Therefore, the training results cannot be easily transferred to other radars. To address this aspect, we introduce a new gesture recognition pipeline that implements advanced full-wave electromagnetic modeling and inversion to retrieve physical characteristics of gestures that are radar independent, i.e., independent of the source, antennas, and radar-hand interactions. Inversion of radar signals further reduces the size of the dataset by several orders of magnitude, while preserving the essential information. This approach is compatible with conventional gesture recognizers, such as those based on template matching, which only need a few training examples to deliver high recognition accuracy rates. To evaluate our gesture recognition pipeline, we conducted user-dependent and user-independent evaluations on a dataset of 16 gesture types collected with the Walabot, a low-cost off-the-shelf array radar. We contrast these results with those obtained for the same gesture types collected with an ultra-wideband radar made of a vector network analyzer with a single horn antenna and with a computer vision sensor, respectively. Based on our findings, we suggest some design implications to support future development in radar-based gesture recognition.},
  archive      = {J_TIIS},
  doi          = {10.1145/3589645},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-45},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {RadarSense: Accurate recognition of mid-air hand gestures with radar sensing and few training examples},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). When biased humans meet debiased AI: A case study in college
major recommendation. <em>TIIS</em>, <em>13</em>(3), 1–28. (<a
href="https://doi.org/10.1145/3611313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g., along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when humans and fair AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer career recommendations without sacrificing its accuracy in prediction. Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that perceived gender disparity is a determining factor for the acceptance of a recommendation. In other words, we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. We conducted a follow-up survey to gain additional insights into the effectiveness of various design options that can help participants to overcome their own biases. Our results suggest that making fair AI explainable is crucial for increasing its adoption in the real world.},
  archive      = {J_TIIS},
  doi          = {10.1145/3611313},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {When biased humans meet debiased AI: A case study in college major recommendation},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 2022 TiiS best paper announcement. <em>TIIS</em>,
<em>13</em>(3), 1. (<a href="https://doi.org/10.1145/3615590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIIS},
  doi          = {10.1145/3615590},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {2022 TiiS best paper announcement},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalisable dialogue-based approach for active learning of
activities of daily living. <em>TIIS</em>, <em>13</em>(3), 1–37. (<a
href="https://doi.org/10.1145/3616017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Human Activity Recognition systems may benefit from Active Learning by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in activity recognition systems, which utilises semantic similarity measures and a dataset of natural language descriptions of common activities (which we make publicly available). Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply this approach to two active learning scenarios: (i) using an existing CASAS dataset, demonstrating long-term usage; and (ii) using an online activity recognition system, which tackles the issue of online segmentation and labelling. We demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) quantitatively, as an efficient way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to compare our approach and an established method. Results show the potential of our approach as a hands-free interface for annotation of sensor data as part of an active learning system. We provide insights into the challenges of active learning for activity recognition under real-world conditions and identify potential ways to address them.},
  archive      = {J_TIIS},
  doi          = {10.1145/3616017},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {9},
  number       = {3},
  pages        = {1-37},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Generalisable dialogue-based approach for active learning of activities of daily living},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual analytics of co-occurrences to discover subspaces in
structured data. <em>TIIS</em>, <em>13</em>(2), 1–49. (<a
href="https://doi.org/10.1145/3579031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach that shows all relevant subspaces of categorical data condensed in a single picture. We model the categorical values of the attributes as co-occurrences with data partitions generated from structured data using pattern mining. We show that these co-occurrences are a-priori , allowing us to greatly reduce the search space, effectively generating the condensed picture where conventional approaches filter out several subspaces as these are deemed insignificant. The task of identifying interesting subspaces is common but difficult due to exponential search spaces and the curse of dimensionality. One application of such a task might be identifying a cohort of patients defined by attributes such as gender, age, and diabetes type that share a common patient history, which is modeled as event sequences. Filtering the data by these attributes is common but cumbersome and often does not allow a comparison of subspaces. We contribute a powerful multi-dimensional pattern exploration approach (MDPE-approach) agnostic to the structured data type that models multiple attributes and their characteristics as co-occurrences, allowing the user to identify and compare thousands of subspaces of interest in a single picture. In our MDPE-approach, we introduce two methods to dramatically reduce the search space, outputting only the boundaries of the search space in the form of two tables. We implement the MDPE-approach in an interactive visual interface (MDPE-vis) that provides a scalable, pixel-based visualization design allowing the identification, comparison, and sense-making of subspaces in structured data. Our case studies using a gold-standard dataset and external domain experts confirm our approach’s and implementation’s applicability. A third use case sheds light on the scalability of our approach and a user study with 15 participants underlines its usefulness and power.},
  archive      = {J_TIIS},
  doi          = {10.1145/3579031},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {6},
  number       = {2},
  pages        = {1-49},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Visual analytics of co-occurrences to discover subspaces in structured data},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable activity recognition for smart home systems.
<em>TIIS</em>, <em>13</em>(2), 1–39. (<a
href="https://doi.org/10.1145/3561533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate, and therefore inconsistencies in smart home operations can lead users reliant on smart home predictions to wonder “Why did the smart home do that?” In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques and introduce an explainable activity recognition framework in which we leverage leading XAI methods (Local Interpretable Model-agnostic Explanations, SHapley Additive exPlanations (SHAP), Anchors) to generate natural language explanations that explain what about an activity led to the given classification. We evaluate our framework in the context of a commonly targeted smart home scenario: autonomous remote caregiver monitoring for individuals who are living alone or need assistance. Within the context of remote caregiver monitoring, we perform a two-step evaluation: (a) utilize Machine Learning experts to assess the sensibility of explanations and (b) recruit non-experts in two user remote caregiver monitoring scenarios, synchronous and asynchronous, to assess the effectiveness of explanations generated via our framework. Our results show that the XAI approach, SHAP, has a 92% success rate in generating sensible explanations. Moreover, in 83% of sampled scenarios users preferred natural language explanations over a simple activity label, underscoring the need for explainable activity recognition systems. Finally, we show that explanations generated by some XAI methods can lead users to lose confidence in the accuracy of the underlying activity recognition model, while others lead users to gain confidence. Taking all studied factors into consideration, we make a recommendation regarding which existing XAI method leads to the best performance in the domain of smart home automation and discuss a range of topics for future work to further improve explainable activity recognition.},
  archive      = {J_TIIS},
  doi          = {10.1145/3561533},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-39},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Explainable activity recognition for smart home systems},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining the projective consciousness model and virtual
humans for immersive psychological research: A proof-of-concept
simulating a ToM assessment. <em>TIIS</em>, <em>13</em>(2), 1–31. (<a
href="https://doi.org/10.1145/3583886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relating explicit psychological mechanisms and observable behaviours is a central aim of psychological and behavioural science. One of the challenges is to understand and model the role of consciousness and, in particular, its subjective perspective as an internal level of representation (including for social cognition) in the governance of behaviour. Toward this aim, we implemented the principles of the Projective Consciousness Model (PCM) into artificial agents embodied as virtual humans, extending a previous implementation of the model. Our goal was to offer a proof-of-concept, based purely on simulations, as a basis for a future methodological framework. Its overarching aim is to be able to assess hidden psychological parameters in human participants, based on a model relevant to consciousness research, in the context of experiments in virtual reality. As an illustration of the approach, we focused on simulating the role of Theory of Mind (ToM) in the choice of strategic behaviours of approach and avoidance to optimise the satisfaction of agents’ preferences. We designed a main experiment in a virtual environment that could be used with real humans, allowing us to classify behaviours as a function of order of ToM, up to the second order. We show that agents using the PCM demonstrated expected behaviours with consistent parameters of ToM in this experiment. We also show that the agents could be used to estimate correctly each other’s order of ToM. Furthermore, in a supplementary experiment, we demonstrated how the agents could simultaneously estimate order of ToM and preferences attributed to others to optimize behavioural outcomes. Future studies will empirically assess and fine tune the framework with real humans in virtual reality experiments.},
  archive      = {J_TIIS},
  doi          = {10.1145/3583886},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-31},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Combining the projective consciousness model and virtual humans for immersive psychological research: A proof-of-concept simulating a ToM assessment},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRAFS: Graphical faceted search system to support conceptual
understanding in exploratory search. <em>TIIS</em>, <em>13</em>(2),
1–36. (<a href="https://doi.org/10.1145/3588319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When people search for information about a new topic within large document collections, they implicitly construct a mental model of the unfamiliar information space to represent what they currently know and guide their exploration into the unknown. Building this mental model can be challenging as it requires not only finding relevant documents but also synthesizing important concepts and the relationships that connect those concepts both within and across documents. This article describes a novel interactive approach designed to help users construct a mental model of an unfamiliar information space during exploratory search. We propose a new semantic search system to organize and visualize important concepts and their relations for a set of search results. A user study ( n =20) was conducted to compare the proposed approach against a baseline faceted search system on exploratory literature search tasks. Experimental results show that the proposed approach is more effective in helping users recognize relationships between key concepts, leading to a more sophisticated understanding of the search topic while maintaining similar functionality and usability as a faceted search system.},
  archive      = {J_TIIS},
  doi          = {10.1145/3588319},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {5},
  number       = {2},
  pages        = {1-36},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {GRAFS: Graphical faceted search system to support conceptual understanding in exploratory search},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining recommendations through conversations: Dialog
model and the effects of interface type and degree of interactivity.
<em>TIIS</em>, <em>13</em>(2), 1–47. (<a
href="https://doi.org/10.1145/3579541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explaining system-generated recommendations based on user reviews can foster users’ understanding and assessment of the recommended items and the recommender system (RS) as a whole. While up to now explanations have mostly been static, shown in a single presentation unit, some interactive explanatory approaches have emerged in explainable artificial intelligence (XAI), making it easier for users to examine system decisions and to explore arguments according to their information needs. However, little is known about how interactive interfaces should be conceptualized and designed to meet the explanatory aims of transparency, effectiveness, and trust in RS. Thus, we investigate the potential of interactive, conversational explanations in review-based RS and propose an explanation approach inspired by dialog models and formal argument structures. In particular, we investigate users’ perception of two different interface types for presenting explanations, a graphical user interface (GUI)-based dialog consisting of a sequence of explanatory steps, and a chatbot-like natural-language interface. Since providing explanations by means of natural language conversation is a novel approach, there is a lack of understanding how users would formulate their questions with a corresponding lack of datasets. We thus propose an intent model for explanatory queries and describe the development of ConvEx-DS, a dataset containing intent annotations of 1,806 user questions in the domain of hotels, that can be used to to train intent detection methods as part of the development of conversational agents for explainable RS. We validate the model by measuring user-perceived helpfulness of answers given based on the implemented intent detection. Finally, we report on a user study investigating users’ evaluation of the two types of interactive explanations proposed (GUI and chatbot), and to test the effect of varying degrees of interactivity that result in greater or lesser access to explanatory information. By using Structural Equation Modeling, we reveal details on the relationships between the perceived quality of an explanation and the explanatory objectives of transparency, trust, and effectiveness. Our results show that providing interactive options for scrutinizing explanatory arguments has a significant positive influence on the evaluation by users (compared to low interactive alternatives). Results also suggest that user characteristics such as decision-making style may have a significant influence on the evaluation of different types of interactive explanation interfaces.},
  archive      = {J_TIIS},
  doi          = {10.1145/3579541},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {4},
  number       = {2},
  pages        = {1-47},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Explaining recommendations through conversations: Dialog model and the effects of interface type and degree of interactivity},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EDAssistant: Supporting exploratory data analysis in
computational notebooks with in situ code search and recommendation.
<em>TIIS</em>, <em>13</em>(1), 1–27. (<a
href="https://doi.org/10.1145/3545995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using computational notebooks (e.g., Jupyter Notebook), data scientists rationalize their exploratory data analysis (EDA) based on their prior experience and external knowledge, such as online examples. For novices or data scientists who lack specific knowledge about the dataset or problem to investigate, effectively obtaining and understanding the external information is critical to carrying out EDA. This article presents EDAssistant, a JupyterLab extension that supports EDA with in situ search of example notebooks and recommendation of useful APIs, powered by novel interactive visualization of search results. The code search and recommendation are enabled by advanced machine learning models, trained on a large corpus of EDA notebooks collected online. A user study is conducted to investigate both EDAssistant and data scientists’ current practice (i.e., using external search engines). The results demonstrate the effectiveness and usefulness of EDAssistant, and participants appreciated its smooth and in-context support of EDA. We also report several design implications regarding code recommendation tools.},
  archive      = {J_TIIS},
  doi          = {10.1145/3545995},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {EDAssistant: Supporting exploratory data analysis in computational notebooks with in situ code search and recommendation},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The influence of personality traits on user interaction with
recommendation interfaces. <em>TIIS</em>, <em>13</em>(1), 1–39. (<a
href="https://doi.org/10.1145/3558772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users’ personality traits can take an active role in affecting their behavior when they interact with a computer interface. However, in the area of recommender systems (RS) , though personality-based RS has been extensively studied, most works focus on algorithm design, with little attention paid to studying whether and how the personality may influence users’ interaction with the recommendation interface. In this manuscript, we report the results of a user study (with 108 participants) that not only measured the influence of users’ personality traits on their perception and performance when using the recommendation interface but also employed an eye-tracker to in-depth reveal how personality may influence users’ eye-movement behavior. Moreover, being different from related work that has mainly been conducted in a single product domain, our user study was performed in three typical application domains (i.e., electronics like smartphones, entertainment like movies, and tourism like hotels). Our results show that mainly three personality traits, i.e., Openness to experience , Conscientiousness , and Agreeableness , significantly influence users’ perception and eye-movement behavior, but the exact influences vary across the domains. Finally, we provide a set of guidelines that might be constructive for designing a more effective recommendation interface based on user personality.},
  archive      = {J_TIIS},
  doi          = {10.1145/3558772},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-39},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {The influence of personality traits on user interaction with recommendation interfaces},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthesizing game levels for collaborative gameplay in a
shared virtual environment. <em>TIIS</em>, <em>13</em>(1), 1–36. (<a
href="https://doi.org/10.1145/3558773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We developed a method to synthesize game levels that accounts for the degree of collaboration required by two players to finish a given game level. We first asked a game level designer to create playable game level chunks. Then, two artificial intelligence (AI) virtual agents driven by behavior trees played each game level chunk. We recorded the degree of collaboration required to accomplish each game level chunk by the AI virtual agents and used it to characterize each game level chunk. To synthesize a game level, we assigned to the total cost function cost terms that encode both the degree of collaboration and game level design decisions. Then, we used a Markov-chain Monte Carlo optimization method, called simulated annealing, to solve the total cost function and proposed a design for a game level. We synthesized three game levels (low, medium, and high degrees of collaboration game levels) to evaluate our implementation. We then recruited groups of participants to play the game levels to explore whether they would experience a certain degree of collaboration and validate whether the AI virtual agents provided sufficient data that described the collaborative behavior of players in each game level chunk. By collecting both in-game objective measurements and self-reported subjective ratings, we found that the three game levels indeed impacted the collaboration gameplay behavior of our participants. Moreover, by analyzing our collected data, we found moderate and strong correlations between the participants and the AI virtual agents. These results show that game developers can consider AI virtual agents as an alternative method for evaluating the degree of collaboration required to finish a game level.},
  archive      = {J_TIIS},
  doi          = {10.1145/3558773},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-36},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Synthesizing game levels for collaborative gameplay in a shared virtual environment},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A personalized interaction mechanism framework for
micro-moment recommender systems. <em>TIIS</em>, <em>13</em>(1), 1–28.
(<a href="https://doi.org/10.1145/3569586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the micro-moment concept highlights the influence of context; recommender system design should reflect this trend. In response to different contexts, a micro-moment recommender system (MMRS) requires an effective interaction mechanism that allows users to easily interact with the system in a way that supports autonomy and promotes the creation and expression of self. We study four types of interaction mechanisms to understand which personalization approach is the most suitable design for MMRSs. We assume that designs that support micro-moment needs well are those that give users more control over the system and constitute a lighter user burden. We test our hypothesis via a two-week between-subject field study in which participants used our system and provided feedback. User-initiated and mix-initiated intention mechanisms show higher perceived active control, and the additional controls do not add to user burdens. Therefore, these two designs suit the MMRS interaction mechanism.},
  archive      = {J_TIIS},
  doi          = {10.1145/3569586},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {A personalized interaction mechanism framework for micro-moment recommender systems},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualization and visual analytics approaches for image and
video datasets: A survey. <em>TIIS</em>, <em>13</em>(1), 1–41. (<a
href="https://doi.org/10.1145/3576935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image and video data analysis has become an increasingly important research area with applications in different domains such as security surveillance, healthcare, augmented and virtual reality, video and image editing, activity analysis and recognition, synthetic content generation, distance education, telepresence, remote sensing, sports analytics, art, non-photorealistic rendering, search engines, and social media. Recent advances in Artificial Intelligence (AI) and particularly deep learning have sparked new research challenges and led to significant advancements, especially in image and video analysis. These advancements have also resulted in significant research and development in other areas such as visualization and visual analytics, and have created new opportunities for future lines of research. In this survey article, we present the current state of the art at the intersection of visualization and visual analytics, and image and video data analysis. We categorize the visualization articles included in our survey based on different taxonomies used in visualization and visual analytics research. We review these articles in terms of task requirements, tools, datasets, and application areas. We also discuss insights based on our survey results, trends and patterns, the current focus of visualization research, and opportunities for future research.},
  archive      = {J_TIIS},
  doi          = {10.1145/3576935},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {3},
  number       = {1},
  pages        = {1-41},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Visualization and visual analytics approaches for image and video datasets: A survey},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on highlights of IUI 2021: introduction.
<em>TIIS</em>, <em>12</em>(4), 1–4. (<a
href="https://doi.org/10.1145/3561516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TIIS},
  doi          = {10.1145/3561516},
  journal      = {ACM Transactions on Interactive Intelligent Systems},
  month        = {2},
  number       = {4},
  pages        = {1-4},
  shortjournal = {ACM Trans. Interact. Intell. Syst.},
  title        = {Special issue on highlights of IUI 2021: Introduction},
  volume       = {12},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
