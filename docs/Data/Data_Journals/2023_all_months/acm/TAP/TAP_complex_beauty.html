<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tap---20">TAP - 20</h2>
<ul>
<li><details>
<summary>
(2023). The effect of interocular contrast differences on the
appearance of augmented reality imagery. <em>TAP</em>, <em>21</em>(1),
1–23. (<a href="https://doi.org/10.1145/3617684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) devices seek to create compelling visual experiences that merge virtual imagery with the natural world. These devices often rely on wearable near-eye display systems that can optically overlay digital images to the left and right eyes of the user separately. Ideally, the two eyes should be shown images with minimal radiometric differences (e.g., the same overall luminance, contrast, and color in both eyes), but achieving this binocular equality can be challenging in wearable systems with stringent demands on weight and size. Basic vision research has shown that a spectrum of potentially detrimental perceptual effects can be elicited by imagery with radiometric differences between the eyes, but it is not clear whether and how these findings apply to the experience of modern AR devices. In this work, we first develop a testing paradigm for assessing multiple aspects of visual appearance at once, and characterize five key perceptual factors when participants viewed stimuli with interocular contrast differences. In a second experiment, we simulate optical see-through AR imagery using conventional desktop LCD monitors and use the same paradigm to evaluate the multi-faceted perceptual implications when the AR display luminance differs between the two eyes. We also include simulations of monocular AR systems (i.e., systems in which only one eye sees the displayed image). Our results suggest that interocular contrast differences can drive several potentially detrimental perceptual effects in binocular AR systems, such as binocular luster, rivalry, and spurious depth differences. In addition, monocular AR displays tend to have more artifacts than binocular displays with a large contrast difference in the two eyes. A better understanding of the range and likelihood of these perceptual phenomena can help inform design choices that support high-quality user experiences in AR.},
  archive      = {J_TAP},
  doi          = {10.1145/3617684},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The effect of interocular contrast differences on the appearance of augmented reality imagery},
  volume       = {21},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The influence of the other-race effect on susceptibility to
face morphing attacks. <em>TAP</em>, <em>21</em>(1), 1–13. (<a
href="https://doi.org/10.1145/3618113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial morphs created between two identities resemble both of the faces used to create the morph. Consequently, humans and machines are prone to mistake morphs made from two identities for either of the faces used to create the morph. This vulnerability has been exploited in “morph attacks” in security scenarios. Here, we asked whether the “other-race effect” (ORE)—the human advantage for identifying own- vs. other-race faces—exacerbates morph attack susceptibility for humans. We also asked whether face-identification performance in a deep convolutional neural network (DCNN) is affected by the race of morphed faces. Caucasian (CA) and East-Asian (EA) participants performed a face-identity matching task on pairs of CA and EA face images in two conditions. In the morph condition, different-identity pairs consisted of an image of identity “A” and a 50/50 morph between images of identity “A” and “B”. In the baseline condition, morphs of different identities never appeared. As expected, morphs were identified mistakenly more often than original face images. Of primary interest, morph identification was substantially worse for cross-race faces than for own-race faces. Similar to humans, the DCNN performed more accurately for original face images than for morphed image pairs. Notably, the deep network proved substantially more accurate than humans in both cases. The results point to the possibility that DCNNs might be useful for improving face identification accuracy when morphed faces are presented. They also indicate the significance of the race of a face in morph attack susceptibility in applied settings.},
  archive      = {J_TAP},
  doi          = {10.1145/3618113},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-13},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The influence of the other-race effect on susceptibility to face morphing attacks},
  volume       = {21},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The haptic intensity order illusion is caused by amplitude
changes. <em>TAP</em>, <em>21</em>(1), 1–18. (<a
href="https://doi.org/10.1145/3626237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When two brief vibrotactile stimulations are sequentially applied to observers’ lower back, there is systematic mislocalization of the stimulation: if the second stimulation is of higher intensity than the first one, observers tend to respond that the second stimulation was above the first one, and vice versa when weak intensity stimulation follows a strong one. This haptic mislocalization effect has been called the intensity order illusion . In the original demonstration of the illusion, frequency and amplitude of the stimulation were inextricably linked so that changes in amplitude also resulted in changes in frequency. It is therefore unknown whether the illusion is caused by changes in frequency, amplitude or both. To test this, we performed a multifactorial experiment, where we used L5 actuators that allow independent manipulation of frequency and amplitude. This approach enabled us to investigate the effects of stimulus amplitude, frequency and location, and to assess any potential interactions among these factors. We report four main findings: (1) we were able to replicate the intensity order illusion with the L5 tactors; (2) the illusion mainly occurred in the upwards direction, or in other words, when strong stimulation following a weaker one occurred above or in the same location as the first stimulation; (3) the illusion did not occur when similar stimulation patterns were applied in the horizontal direction; and (4) the illusion was solely due to changes in amplitude, whereas changes in frequency (100 Hz vs 200 Hz) had no effect.},
  archive      = {J_TAP},
  doi          = {10.1145/3626237},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-18},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {The haptic intensity order illusion is caused by amplitude changes},
  volume       = {21},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring the relative effects of body position and
locomotion method on presence and cybersickness when navigating a
virtual environment. <em>TAP</em>, <em>21</em>(1), 1–25. (<a
href="https://doi.org/10.1145/3627706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary goals of this research are to strengthen the understanding of the mechanisms underlying presence and cybersickness in relation to the body position and locomotion method when navigating a virtual environment (VE). In this regard, we compared two body positions (standing and sitting) and four locomotion methods (steering + embodied control [EC], steering + instrumental control [IC], teleportation + EC, and teleportation + IC) to examine the association between body position, locomotion method, presence, and cybersickness in VR. The results of a two-way ANOVA revealed a main effect of locomotion method on presence, with the sense of presence significantly lower for the steering + IC condition. However, there was no main effect of body position on presence, nor was there an interaction between body position and locomotion method. For cybersickness, nonparametric tests were used due to non-normality. The results of Mann-Whitney U tests indicated a statistically significant effect of body position on cybersickness. In particular, the level of cybersickness was significantly higher for a standing position than for a sitting position. In addition, the results of Kruskal-Wallis tests revealed that the locomotion method had a meaningful effect on cybersickness, with participants in the steering conditions feeling stronger symptoms of cybersickness than those in the teleportation conditions. Overall, this study confirmed the relationship between body position, locomotion method, presence, and cybersickness when navigating a VE.},
  archive      = {J_TAP},
  doi          = {10.1145/3627706},
  journal      = {ACM Transactions on Applied Perception},
  month        = {12},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Exploring the relative effects of body position and locomotion method on presence and cybersickness when navigating a virtual environment},
  volume       = {21},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the perception of mid-air tactile shapes with
spatio-temporally-modulated tactile pointers. <em>TAP</em>,
<em>20</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3611388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound mid-air haptic (UMH) devices can remotely render vibrotactile shapes on the skin of unequipped users, e.g., to draw haptic icons or render virtual object shapes. Spatio-temporal modulation (STM), the state-of-the-art UMH shape-rendering method, provides large freedom in shape design and produces the strongest possible stimuli for this technology. Yet, STM shapes are often reported to be blurry, complicating shape identification. Dynamic tactile pointers (DTP) were recently introduced as a technique to overcome this issue. By tracing a contour with an amplitude-modulated focal point, they significantly improve shape identification accuracy over STM, but at the cost of much lower stimulus intensity. Building upon this, we propose spatio-temporally-modulated Tactile Pointers (STP), a novel method for rendering clearer and sharper UMH shapes while at the same time producing strong vibrotactile sensations. We ran two human participant experiments, which show that STP shapes are perceived as significantly stronger than DTP shapes, while shape identification accuracy is significantly improved over STM and on par with that obtained with DTP. Our work has implications for effective shape rendering with UMH and provides insights that could inform future psychophysical investigation into vibrotactile shape perception in UMH.},
  archive      = {J_TAP},
  doi          = {10.1145/3611388},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Improving the perception of mid-air tactile shapes with spatio-temporally-modulated tactile pointers},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibrated passability perception in virtual reality
transfers to augmented reality. <em>TAP</em>, <em>20</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3613450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As applications for virtual reality (VR) and augmented reality (AR) technology increase, it will be important to understand how users perceive their action capabilities in virtual environments. Feedback about actions may help to calibrate perception for action opportunities (affordances) so that action judgments in VR and AR mirror actors’ real abilities. Previous work indicates that walking through a virtual doorway while wielding an object can calibrate the perception of one’s passability through feedback from collisions. In the current study, we aimed to replicate this calibration through feedback using a different paradigm in VR while also testing whether this calibration transfers to AR. Participants held a pole at 45°and made passability judgments in AR (pretest phase). Then, they made passability judgments in VR and received feedback on those judgments by walking through a virtual doorway while holding the pole (calibration phase). Participants then returned to AR to make posttest passability judgments. Results indicate that feedback calibrated participants’ judgments in VR. Moreover, this calibration transferred to the AR environment. In other words, after experiencing feedback in VR, passability judgments in VR and in AR became closer to an actor’s actual ability, which could make training applications in these technologies more effective.},
  archive      = {J_TAP},
  doi          = {10.1145/3613450},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Calibrated passability perception in virtual reality transfers to augmented reality},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On human-like biases in convolutional neural networks for
the perception of slant from texture. <em>TAP</em>, <em>20</em>(4),
1–18. (<a href="https://doi.org/10.1145/3613451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation is fundamental to 3D perception, and humans are known to have biased estimates of depth. This study investigates whether convolutional neural networks (CNNs) can be biased when predicting the sign of curvature and depth of surfaces of textured surfaces under different viewing conditions (field of view) and surface parameters (slant and texture irregularity). This hypothesis is drawn from the idea that texture gradients described by local neighborhoods—a cue identified in human vision literature—are also representable within convolutional neural networks. To this end, we trained both unsupervised and supervised CNN models on the renderings of slanted surfaces with random Polka dot patterns and analyzed their internal latent representations. The results show that the unsupervised models have similar prediction biases as humans across all experiments, while supervised CNN models do not exhibit similar biases. The latent spaces of the unsupervised models can be linearly separated into axes representing field of view and optical slant. For supervised models, this ability varies substantially with model architecture and the kind of supervision (continuous slant vs. sign of slant). Even though this study says nothing of any shared mechanism, these findings suggest that unsupervised CNN models can share similar predictions to the human visual system. Code: github.com/brownvc/Slant-CNN-Biases.},
  archive      = {J_TAP},
  doi          = {10.1145/3613451},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-18},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {On human-like biases in convolutional neural networks for the perception of slant from texture},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Changes in navigation over time: A comparison of
teleportation and joystick-based locomotion. <em>TAP</em>,
<em>20</em>(4), 1–16. (<a
href="https://doi.org/10.1145/3613902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Little research has studied how people use Virtual Reality (VR) changes as they experience VR. This article reports the results of an experiment investigating how users’ behavior with two locomotion methods changed over 4 weeks: teleportation and joystick-based locomotion. Twenty novice VR users (with no more than 1 hour prior experience with any form of walking in VR) were recruited. They loaned an Oculus Quest for 4 weeks on their own time, including an activity we provided them with. Results showed that the time required to complete the navigation task decreased faster for joystick-based locomotion. Spatial memory improved with time, particularly when using teleportation (which starts disadvantaged to joystick-based locomotion). In addition, overall cybersickness decreased slightly over time; however, two dimensions of cybersickness (nausea and disorientation) increased notably over time using joystick-based navigation.},
  archive      = {J_TAP},
  doi          = {10.1145/3613902},
  journal      = {ACM Transactions on Applied Perception},
  month        = {10},
  number       = {4},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Changes in navigation over time: A comparison of teleportation and joystick-based locomotion},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Introduction to the SAP 2023 special issue. <em>TAP</em>,
<em>20</em>(4), 1–2. (<a href="https://doi.org/10.1145/3629977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAP},
  doi          = {10.1145/3629977},
  journal      = {ACM Transactions on Applied Perception},
  month        = {2},
  number       = {4},
  pages        = {1-2},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Introduction to the SAP 2023 special issue},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effect of subthreshold electrotactile stimulation on the
perception of electrovibration. <em>TAP</em>, <em>20</em>(3), 1–16. (<a
href="https://doi.org/10.1145/3599970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrovibration is used in touch enabled devices to render different textures. Tactile sub-modal stimuli can enhance texture perception when presented along with electrovibration stimuli. Perception of texture depends on the threshold of electrovibration. In the current study, we have conducted a psychophysical experiment on 13 participants to investigate the effect of introducing a subthreshold electrotactile stimulus (SES) to the perception of electrovibration. Interaction of tactile sub-modal stimuli causes masking of a stimulus in the presence of another stimulus. This study explored the occurrence of tactile masking of electrovibration by electrotactile stimulus. The results indicate the reduction of electrovibration threshold by 12.46% and 6.75% when the electrotactile stimulus was at 90% and 80% of its perception threshold, respectively. This method was tested over a wide range of frequencies from 20 Hz to 320 Hz in the tuning curve, and the variation in percentage reduction with frequency is reported. Another experiment was conducted to measure the perception of combined stimuli on the Likert scale. The results showed that the perception was more inclined towards the electrovibration at 80% of SES and was indifferent at 90% of SES. The reduction in the threshold of electrovibration reveals that the effect of tactile masking by electrotactile stimulus was not prevalent under subthreshold conditions. This study provides significant insights into developing a texture rendering algorithm based on tactile sub-modal stimuli in the future.},
  archive      = {J_TAP},
  doi          = {10.1145/3599970},
  journal      = {ACM Transactions on Applied Perception},
  month        = {9},
  number       = {3},
  pages        = {1-16},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Effect of subthreshold electrotactile stimulation on the perception of electrovibration},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Twin identification over viewpoint change: A deep
convolutional neural network surpasses humans. <em>TAP</em>,
<em>20</em>(3), 1–15. (<a
href="https://doi.org/10.1145/3609224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (DCNNs) have achieved human-level accuracy in face identification (Phillips et al., 2018), though it is unclear how accurately they discriminate highly-similar faces. Here, humans and a DCNN performed a challenging face-identity matching task that included identical twins. Participants ( N = 87) viewed pairs of face images of three types: same-identity, general imposters (different identities from similar demographic groups), and twin imposters (identical twin siblings). The task was to determine whether the pairs showed the same person or different people. Identity comparisons were tested in three viewpoint-disparity conditions: frontal to frontal, frontal to 45° profile, and frontal to 90° profile. Accuracy for discriminating matched-identity pairs from twin-imposter pairs and general-imposter pairs was assessed in each viewpoint-disparity condition. Humans were more accurate for general-imposter pairs than twin-imposter pairs, and accuracy declined with increased viewpoint disparity between the images in a pair. A DCNN trained for face identification (Ranjan et al., 2018) was tested on the same image pairs presented to humans. Machine performance mirrored the pattern of human accuracy, but with performance at or above all humans in all but one condition. Human and machine similarity scores were compared across all image-pair types. This item-level analysis showed that human and machine similarity ratings correlated significantly in six of nine image-pair types [range r = 0.38 to r = 0.63], suggesting general accord between the perception of face similarity by humans and the DCNN. These findings also contribute to our understanding of DCNN performance for discriminating high-resemblance faces, demonstrate that the DCNN performs at a level at or above humans, and suggest a degree of parity between the features used by humans and the DCNN.},
  archive      = {J_TAP},
  doi          = {10.1145/3609224},
  journal      = {ACM Transactions on Applied Perception},
  month        = {9},
  number       = {3},
  pages        = {1-15},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Twin identification over viewpoint change: A deep convolutional neural network surpasses humans},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Participatory design of virtual humans for mental health
support among north american computer science students: Voice,
appearance, and the similarity-attraction effect. <em>TAP</em>,
<em>20</em>(3), 1–27. (<a
href="https://doi.org/10.1145/3613961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual humans (VHs) have the potential to support mental wellness among college computer science (CS) students. However, designing effective VHs for counseling purposes requires a clear understanding of students’ demographics, backgrounds, and expectations. To this end, we conducted two user studies with 216 CS students from a major university in North America. In the first study, we explored how students co-designed VHs to support mental wellness conversations and found that the VHs’ demographics, appearance, and voice closely resembled the characteristics of their designers. In the second study, we investigated how the interplay between the VH’s appearance and voice impacted the agent’s effectiveness in promoting CS students’ intentions toward gratitude journaling. Our findings suggest that the active participation of CS students in VH design leads to the creation of agents that closely resemble their designers. Moreover, we found that the interplay between the VH’s appearance and voice impacts the agent’s effectiveness in promoting CS students’ intentions toward mental wellness techniques.},
  archive      = {J_TAP},
  doi          = {10.1145/3613961},
  journal      = {ACM Transactions on Applied Perception},
  month        = {9},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Participatory design of virtual humans for mental health support among north american computer science students: Voice, appearance, and the similarity-attraction effect},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying lines and interpreting vertical jumps in eye
tracking studies of reading text and code. <em>TAP</em>, <em>20</em>(2),
1–20. (<a href="https://doi.org/10.1145/3579357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking studies have shown that reading code, in contradistinction to reading text, includes many vertical jumps. As different lines of code may have quite different functions (e.g., variable definition, flow control, or computation), it is important to accurately identify the lines being read. We design experiments that require a specific line of text to be scrutinized. Using the distribution of gazes around this line, we then calculate how the precision with which we can identify the line being read depends on the font size and spacing. The results indicate that, even after correcting for systematic bias, unnaturally large fonts and spacing may be required for reliable line identification. Interestingly, during the experiments, the participants also repeatedly re-checked their task and if they were looking at the correct line, leading to vertical jumps similar to those observed when reading code. This suggests that observed reading patterns may be “inefficient,” in the sense that participants feel the need to repeat actions beyond the minimal number apparently required for the task. This may have implications regarding the interpretation of reading patterns. In particular, reading does not reflect only the extraction of information from the text or code. Rather, reading patterns may also reflect other types of activities, such as getting a general orientation, and searching for specific locations in the context of performing a particular task.},
  archive      = {J_TAP},
  doi          = {10.1145/3579357},
  journal      = {ACM Transactions on Applied Perception},
  month        = {4},
  number       = {2},
  pages        = {1-20},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Identifying lines and interpreting vertical jumps in eye tracking studies of reading text and code},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning GAN-based foveated reconstruction to recover
perceptually important image features. <em>TAP</em>, <em>20</em>(2),
1–23. (<a href="https://doi.org/10.1145/3583072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A foveated image can be entirely reconstructed from a sparse set of samples distributed according to the retinal sensitivity of the human visual system, which rapidly decreases with increasing eccentricity. The use of generative adversarial networks (GANs) has recently been shown to be a promising solution for such a task, as they can successfully hallucinate missing image information. As in the case of other supervised learning approaches, the definition of the loss function and the training strategy heavily influence the quality of the output. In this work,we consider the problem of efficiently guiding the training of foveated reconstruction techniques such that they are more aware of the capabilities and limitations of the human visual system, and thus can reconstruct visually important image features. Our primary goal is to make the training procedure less sensitive to distortions that humans cannot detect and focus on penalizing perceptually important artifacts. Given the nature of GAN-based solutions, we focus on the sensitivity of human vision to hallucination in case of input samples with different densities. We propose psychophysical experiments, a dataset, and a procedure for training foveated image reconstruction. The proposed strategy renders the generator network flexible by penalizing only perceptually important deviations in the output. As a result, the method emphasized the recovery of perceptually important image features. We evaluated our strategy and compared it with alternative solutions by using a newly trained objective metric, a recent foveated video quality metric, and user experiments. Our evaluations revealed significant improvements in the perceived image reconstruction quality compared with the standard GAN-based training approach.},
  archive      = {J_TAP},
  doi          = {10.1145/3583072},
  journal      = {ACM Transactions on Applied Perception},
  month        = {4},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Learning GAN-based foveated reconstruction to recover perceptually important image features},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Salient-centeredness and saliency size in computational
aesthetics. <em>TAP</em>, <em>20</em>(2), 1–23. (<a
href="https://doi.org/10.1145/3588317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the optimal aesthetic location and size of a single dominant salient region in a photographic image. Existing algorithms for photographic composition do not take full account of the spatial positioning or sizes of these salient regions. We present a set of experiments to assess aesthetic preferences, inspired by theories of centeredness, principal lines, and Rule-of-Thirds. Our experimental results show a clear preference for the salient region to be centered in the image and that there is a preferred size of non-salient border around this salient region. We thus propose a novel image cropping mechanism for images containing a single salient region to achieve the best aesthetic balance. Our results show that the Rule-of-Thirds guideline is not generally valid but also allow us to hypothesize in which situations it is useful and in which it is inappropriate.},
  archive      = {J_TAP},
  doi          = {10.1145/3588317},
  journal      = {ACM Transactions on Applied Perception},
  month        = {4},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. Appl. Perc.},
  title        = {Salient-centeredness and saliency size in computational aesthetics},
  volume       = {20},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient dataflow modeling of peripheral encoding in the
human visual system. <em>ACM Transactions on Applied Perceptions</em>,
<em>20</em>(1), 1–22. (<a
href="https://doi.org/10.1145/3564605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computer graphics seeks to deliver compelling images, generated within a computing budget, targeted at a specific display device, and ultimately viewed by an individual user. The foveated nature of human vision offers an opportunity to efficiently allocate computation and compression to appropriate areas of the viewer’s visual field, of particular importance with the rise of high-resolution and wide field-of-view display devices. However, while variations in acuity and contrast sensitivity across the field of view have been well-studied and modeled, a more consequential variation concerns peripheral vision’s degradation in the face of clutter, known as crowding. Understanding of peripheral crowding has greatly advanced in recent years, in terms of both phenomenology and modeling. Accurately leveraging this knowledge is critical for many applications, as peripheral vision covers a majority of pixels in the image. We advance computational models for peripheral vision aimed toward their eventual use in computer graphics. In particular, researchers have recently developed high-performing models of peripheral crowding, known as “pooling” models, which predict a wide range of phenomena but are computationally inefficient. We reformulate the problem as a dataflow computation, which enables faster processing and operating on larger images. Further, we account for the explicit encoding of “end stopped” features in the image, which was missing from previous methods. We evaluate our model in the context of perception of textures in the periphery, including a novel texture dataset and updated textural descriptors. Our improved computational framework may simplify development and testing of more sophisticated, complete models in more robust and realistic settings relevant to computer graphics.},
  archive  = {J},
  doi      = {10.1145/3564605},
  journal  = {ACM Transactions on Applied Perceptions},
  month    = {1},
  number   = {1},
  pages    = {1-22},
  title    = {Efficient dataflow modeling of peripheral encoding in the human visual system},
  volume   = {20},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A content-adaptive visibility predictor for perceptually
optimized image blending. <em>ACM Transactions on Applied
Perceptions</em>, <em>20</em>(1), 1–29. (<a
href="https://doi.org/10.1145/3565972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The visibility of an image semi-transparently overlaid on another image varies significantly, depending on the content of the images. This makes it difficult to maintain the desired visibility level when the image content changes. To tackle this problem, we developed a perceptual model to predict the visibility of the blended results of arbitrarily combined images. Conventional visibility models cannot reflect the dependence of the suprathreshold visibility of the blended images on the appearance of the pre-blended image content. Therefore, we have proposed a visibility model with a content-adaptive feature aggregation mechanism, which integrates the visibility for each image feature (i.e., such as spatial frequency and colors) after applying weights that are adaptively determined according to the appearance of the input image. We conducted a large-scale psychophysical experiment to develop the visibility predictor model. Ablation studies revealed the importance of the adaptive weighting mechanism in accurately predicting the visibility of blended images. We have also proposed a technique for optimizing the image opacity such that users can set the visibility of the target image to an arbitrary level. Our evaluation revealed that the proposed perceptually optimized image blending was effective under practical conditions.},
  archive  = {J},
  doi      = {10.1145/3565972},
  journal  = {ACM Transactions on Applied Perceptions},
  month    = {1},
  number   = {1},
  pages    = {1-29},
  title    = {A content-adaptive visibility predictor for perceptually optimized image blending},
  volume   = {20},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical saccade prediction for head-mounted displays:
Towards a comprehensive model. <em>ACM Transactions on Applied
Perceptions</em>, <em>20</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3568311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Eye-tracking technology has started to become an integral component of new display devices such as virtual and augmented reality headsets. Applications of gaze information range from new interaction techniques that exploit eye patterns to gaze-contingent digital content creation. However, system latency is still a significant issue in many of these applications because it breaks the synchronization between the current and measured gaze positions. Consequently, it may lead to unwanted visual artifacts and degradation of the user experience. In this work, we focus on foveated rendering applications where the quality of an image is reduced towards the periphery for computational savings. In foveated rendering, the presence of system latency leads to delayed updates to the rendered frame, making the quality degradation visible to the user. To address this issue and to combat system latency, recent work proposes using saccade landing position prediction to extrapolate gaze information from delayed eye tracking samples. Although the benefits of such a strategy have already been demonstrated, the solutions range from simple and efficient ones, which make several assumptions about the saccadic eye movements, to more complex and costly ones, which use machine learning techniques. However, it is unclear to what extent the prediction can benefit from accounting for additional factors and how more complex predictions can be performed efficiently to respect the latency requirements. This paper presents a series of experiments investigating the importance of different factors for saccades prediction in common virtual and augmented reality applications. In particular, we investigate the effects of saccade orientation in 3D space and smooth pursuit eye-motion (SPEM) and how their influence compares to the variability across users. We also present a simple, yet efficient post-hoc correction method that adapts existing saccade prediction methods to handle these factors without performing extensive data collection. Furthermore, our investigation and the correction technique may also help future developments of machine-learning-based techniques by limiting the required amount of training data.},
  archive  = {J},
  doi      = {10.1145/3568311},
  journal  = {ACM Transactions on Applied Perceptions},
  month    = {1},
  number   = {1},
  pages    = {1-23},
  title    = {Practical saccade prediction for head-mounted displays: Towards a comprehensive model},
  volume   = {20},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gap detection in pairs of ultrasound mid-air vibrotactile
stimuli. <em>ACM Transactions on Applied Perceptions</em>,
<em>20</em>(1), 1–17. (<a
href="https://doi.org/10.1145/3570904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Ultrasound mid-air haptic (UMH) devices are a novel tool for haptic feedback, capable of providing localized vibrotactile stimuli to users at a distance. UMH applications largely rely on generating tactile shape outlines on the users’ skin. Here we investigate how to achieve sensations of continuity or gaps within such two-dimensional curves by studying the perception of pairs of amplitude-modulated focused ultrasound stimuli. On the one hand, we aim to investigate perceptual effects that may arise from providing simultaneous UMH stimuli. On the other hand, we wish to provide perception-based rendering guidelines for generating continuous or discontinuous sensations of tactile shapes. Finally, we hope to contribute toward a measure of the perceptually achievable resolution of UMH interfaces. We performed a user study to identify how far apart two focal points need to be to elicit a perceptual experience of two distinct stimuli separated by a gap. Mean gap detection thresholds were found at 32.3-mm spacing between focal points, but a high within- and between-subject variability was observed. Pairs spaced below 15 mm were consistently (&gt;95%) perceived as a single stimulus, while pairs spaced 45 mm apart were consistently (84%) perceived as two separate stimuli. To investigate the observed variability, we resort to acoustic simulations of the resulting pressure fields. These show a non-linear evolution of actual peak pressure spacing as a function of nominal focal point spacing. Beyond an initial threshold in spacing (between 15 and 18 mm), which we believe to be related to the perceived size of a focal point, the probability of detecting a gap between focal points appears to linearly increase with spacing. Our work highlights physical interactions and perceptual effects to consider when designing or investigating the perception of UMH shapes.},
  archive  = {J},
  doi      = {10.1145/3570904},
  journal  = {ACM Transactions on Applied Perceptions},
  month    = {1},
  number   = {1},
  pages    = {1-17},
  title    = {Gap detection in pairs of ultrasound mid-air vibrotactile stimuli},
  volume   = {20},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Virtual big heads in extended reality: Estimation of ideal
head scales and perceptual thresholds for comfort and facial cues.
<em>ACM Transactions on Applied Perceptions</em>, <em>20</em>(1), 1–31.
(<a href="https://doi.org/10.1145/3571074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Extended reality (XR) technologies, such as virtual reality (VR) and augmented reality (AR), provide users, their avatars, and embodied agents a shared platform to collaborate in a spatial context. Although traditional face-to-face communication is limited by users’ proximity, meaning that another human’s non-verbal embodied cues become more difficult to perceive the farther one is away from that person, researchers and practitioners have started to look into ways to accentuate or amplify such embodied cues and signals to counteract the effects of distance with XR technologies. In this article, we describe and evaluate the Big Head technique, in which a human’s head in VR/AR is scaled up relative to their distance from the observer as a mechanism for enhancing the visibility of non-verbal facial cues, such as facial expressions or eye gaze. To better understand and explore this technique, we present two complimentary human-subject experiments in this article. In our first experiment, we conducted a VR study with a head-mounted display to understand the impact of increased or decreased head scales on participants’ ability to perceive facial expressions as well as their sense of comfort and feeling of “uncannniness” over distances of up to 10 m. We explored two different scaling methods and compared perceptual thresholds and user preferences. Our second experiment was performed in an outdoor AR environment with an optical see-through head-mounted display. Participants were asked to estimate facial expressions and eye gaze, and identify a virtual human over large distances of 30, 60, and 90 m. In both experiments, our results show significant differences in minimum, maximum, and ideal head scales for different distances and tasks related to perceiving faces, facial expressions, and eye gaze, and we also found that participants were more comfortable with slightly bigger heads at larger distances. We discuss our findings with respect to the technologies used, and we discuss implications and guidelines for practical applications that aim to leverage XR-enhanced facial cues.},
  archive  = {J},
  doi      = {10.1145/3571074},
  journal  = {ACM Transactions on Applied Perceptions},
  month    = {1},
  number   = {1},
  pages    = {1-31},
  title    = {Virtual big heads in extended reality: Estimation of ideal head scales and perceptual thresholds for comfort and facial cues},
  volume   = {20},
  year     = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
