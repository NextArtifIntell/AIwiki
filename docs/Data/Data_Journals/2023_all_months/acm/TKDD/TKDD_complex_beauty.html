<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDD_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkdd---188">TKDD - 188</h2>
<ul>
<li><details>
<summary>
(2023). History-enhanced and uncertainty-aware trajectory recovery
via attentive neural network. <em>TKDD</em>, <em>18</em>(3), 1–22. (<a
href="https://doi.org/10.1145/3615660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A considerable amount of mobility data has been accumulated due to the proliferation of location-based services. Nevertheless, compared with mobility data from transportation systems like the GPS module in taxis, this kind of data is commonly sparse in terms of individual trajectories in the sense that users do not access mobile services and contribute their data all the time. Consequently, the sparsity inevitably weakens the practical value of the data even if it has a high user penetration rate. To solve this problem, we propose a novel attentional neural network-based model, named AttnMove, to densify individual trajectories by recovering unobserved locations at a fine-grained spatial-temporal resolution. To tackle the challenges posed by sparsity, we design various intra- and inter- trajectory attention mechanisms to better model the mobility regularity of users and fully exploit the periodical pattern from long-term history. In addition, to guarantee the robustness of the generated trajectories to avoid harming downstream applications, we also exploit the Bayesian approximate neural network to estimate the uncertainty of each imputation. As a result, locations generated by the model with high uncertainty will be excluded. We evaluate our model on two real-world datasets, and extensive results demonstrate the performance gain compared with the state-of-the-art methods. In-depth analyses of each design of our model have been conducted to understand their contribution. We also show that, by providing high-quality mobility data, our model can benefit a variety of mobility-oriented downstream applications.},
  archive      = {J_TKDD},
  doi          = {10.1145/3615660},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {History-enhanced and uncertainty-aware trajectory recovery via attentive neural network},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair and private data preprocessing through
microaggregation. <em>TKDD</em>, <em>18</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3617377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy protection for personal data and fairness in automated decisions are fundamental requirements for responsible Machine Learning. Both may be enforced through data preprocessing and share a common target: data should remain useful for a task, while becoming uninformative of the sensitive information. The intrinsic connection between privacy and fairness implies that modifications performed to guarantee one of these goals, may have an effect on the other, e.g., hiding a sensitive attribute from a classification algorithm might prevent a biased decision rule having such attribute as a criterion. This work resides at the intersection of algorithmic fairness and privacy. We show how the two goals are compatible, and may be simultaneously achieved, with a small loss in predictive performance. Our results are competitive with both state-of-the-art fairness correcting algorithms and hybrid privacy-fairness methods. Experiments were performed on three widely used benchmark datasets: Adult Income , COMPAS, and German Credit .},
  archive      = {J_TKDD},
  doi          = {10.1145/3617377},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fair and private data preprocessing through microaggregation},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From asset flow to status, action, and intention discovery:
Early malice detection in cryptocurrency. <em>TKDD</em>, <em>18</em>(3),
1–27. (<a href="https://doi.org/10.1145/3626102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency has been subject to illicit activities probably more often than traditional financial assets due to the pseudo-anonymous nature of its transacting entities. An ideal detection model is expected to achieve all three critical properties of early detection, good interpretability, and versatility for various illicit activities. However, existing solutions cannot meet all these requirements, as most of them heavily rely on deep learning without interpretability and are only available for retrospective analysis of a specific illicit type. To tackle all these challenges, we propose Intention Monitor for early malice detection in Bitcoin, where the on-chain record data for a certain address are much scarcer than other cryptocurrency platforms. We first define asset transfer paths with the Decision Tree based feature Selection and Complement to build different feature sets for different malice types. Then, the Status/Action Proposal module and the Intention-VAE module generate the status, action, intent-snippet, and hidden intent-snippet embedding. With all these modules, our model is highly interpretable and can detect various illegal activities. Moreover, well-designed loss functions further enhance the prediction speed and the model’s interpretability. Extensive experiments on three real-world datasets demonstrate that our proposed algorithm outperforms the state-of-the-art methods. Furthermore, additional case studies justify that our model not only explains existing illicit patterns but also can find new suspicious characters.},
  archive      = {J_TKDD},
  doi          = {10.1145/3626102},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {From asset flow to status, action, and intention discovery: Early malice detection in cryptocurrency},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EFMVFL: An efficient and flexible multi-party vertical
federated learning without a third party. <em>TKDD</em>, <em>18</em>(3),
1–20. (<a href="https://doi.org/10.1145/3627993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a machine learning setting which allows multiple participants collaboratively to train a model under the orchestration of a server without disclosing their local data. Vertical federated learning (VFL) is a special structure in FL. It handles the situation where participants have the same ID space but different feature spaces. In order to guarantee the security and privacy of the local data of each participant, homomorphic encryption (HE) is often used to transmit intermediate parameters or data during the training process. In most VFL frameworks, a trusted third-party server is necessary because the plaintexts of the parameters need to be revealed for the computation. However, it is hard to find such a credible entity in the real world. Existing methods for solving this problem are either communication-intensive or unsuitable for multi-party scenarios. By combining secret sharing (SS) and HE, we propose a novel VFL framework without any trusted third parties called EFMVFL. It allows intermediate parameters to be transmitted among multiple parties without revealing the plaintexts. EFMVFL is applicable to generalized linear models (GLMs) and supports flexible expansion to multiple participants. Extensive experiments under Logistic Regression and Poisson Regression show that our framework is outstanding in communication (reduced by 3.2×– 6.8×) and efficiency (accelerated by 1.6×– 3.1×).},
  archive      = {J_TKDD},
  doi          = {10.1145/3627993},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {EFMVFL: An efficient and flexible multi-party vertical federated learning without a third party},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling interference for individual treatment effect
estimation from networked observational data. <em>TKDD</em>,
<em>18</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3628449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating individual treatment effect (ITE) from observational data has attracted great interest in recent years, which plays a crucial role in decision-making across many high-impact domains such as economics, medicine, and e-commerce. Most existing studies of ITE estimation assume that different units at play are independent and do not influence each other. However, many social science experiments have shown that there often exist different levels of interactions between units in observational data, especially in a networked environment. As a result, the treatment assignment of one unit can affect the outcome of other units connected to it in the network, which is referred to as the interference or spillover effect . In this article, we study an important problem of ITE estimation from networked observational data by modeling the interference between different units and provide a principled framework to support such study. Methodologically, we propose a novel framework, SPNet , that first captures the influence of hidden confounders with the aid of graph convolutional network and then models the interference by introducing an environment summary variable and developing a masked attention mechanism. Experimental evaluations on several semi-synthetic datasets based on real-world networks corroborate the superiority of our proposed framework over state-of-the-art individual treatment effect estimation methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3628449},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling interference for individual treatment effect estimation from networked observational data},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SILVAN: Estimating betweenness centralities with progressive
sampling and non-uniform rademacher bounds. <em>TKDD</em>,
<em>18</em>(3), 1–55. (<a
href="https://doi.org/10.1145/3628601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Sim Sala Bim!” —Silvan, https://en.wikipedia.org/wiki/Silvan_(illusionist) Betweenness centrality is a popular centrality measure with applications in several domains and whose exact computation is impractical for modern-sized networks. We present SILVAN , a novel, efficient algorithm to compute, with high probability, accurate estimates of the betweenness centrality of all nodes of a graph and a high-quality approximation of the top- k betweenness centralities. SILVAN follows a progressive sampling approach and builds on novel bounds based on Monte Carlo Empirical Rademacher Averages, a powerful and flexible tool from statistical learning theory. SILVAN relies on a novel estimation scheme providing non-uniform bounds on the deviation of the estimates of the betweenness centrality of all the nodes from their true values and a refined characterisation of the number of samples required to obtain a high-quality approximation. Our extensive experimental evaluation shows that SILVAN extracts high-quality approximations while outperforming, in terms of number of samples and accuracy, the state-of-the-art approximation algorithm with comparable quality guarantees.},
  archive      = {J_TKDD},
  doi          = {10.1145/3628601},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-55},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SILVAN: Estimating betweenness centralities with progressive sampling and non-uniform rademacher bounds},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on bid optimization in real-time bidding display
advertising. <em>TKDD</em>, <em>18</em>(3), 1–31. (<a
href="https://doi.org/10.1145/3628603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-Time Bidding (RTB) is one of the most important forms of online advertising, where an auction is hosted in real time to sell the individual ad impression. How to design an automated bidding strategy in response to the dynamic auction environment is crucial for improving user experience, protecting the interests of advertisers, and promoting the long-term development of the advertising platform. As an exciting topic in the real-world industry, it has attracted great research interest from several disciplines, most notably data science. There have been abundant studies on bidding strategy design which are based on the large volume of historical ad requests. Despite its popularity and significance, few works provide a summary for bid optimization. In this survey, we present the latest overview of the recent works to shed light on the optimization techniques where most of them are validated in practice. We first explore the optimization problem in different works, explaining how these different settings affect the bidding strategy designs. Then, some forms of bidding functions and specific optimization techniques are illustrated. Further, we specifically discuss a new trend about bidding in first-price auctions, which have gradually become popular in recent years. From this survey, both practitioners and researchers can gain insights of the challenges and future prospects of bid optimization in RTB.},
  archive      = {J_TKDD},
  doi          = {10.1145/3628603},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey on bid optimization in real-time bidding display advertising},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Put your voice on stage: Personalized headline generation
for news articles. <em>TKDD</em>, <em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3629168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the problem of personalized news headline generation, which aims to produce not only concise and fact-consistent titles for news articles but also decorate these titles as personalized irresistible reading invitations by incorporating readers’ preferences. We propose an approach named PNG ( P ersonalized N ews headline G enerator) by utilizing distant supervision in readers’ past click behaviors to resolve. First, user preference representations are learned through a knowledge-aware user encoder that comprehensively captures the genuine, sequential, and flash interests of users reflected in their historical clicked news. Then, a user-perturbed pointer-generator network is devised to accomplish the headline generation in which the learned user representations implicitly affect the word prediction. The proposed model is optimized by reinforcement learning solvers where indicators on factual, personalized, and linguistic aspects of the generated headline are regarded as rewards. Extensive experiments are conducted on the real-world dataset PENS, 1 which is a large-scale benchmark collected from Microsoft News. Both the quantitative and qualitative results validate the effectiveness of our approach.},
  archive      = {J_TKDD},
  doi          = {10.1145/3629168},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Put your voice on stage: Personalized headline generation for news articles},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive adversarial contrastive learning for cross-domain
recommendation. <em>TKDD</em>, <em>18</em>(3), 1–34. (<a
href="https://doi.org/10.1145/3630259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based cross-domain recommendations (CDRs) are useful for suggesting appropriate items because of their promising ability to extract features from user–item interactions and transfer knowledge across domains. Thus, the model can effectively alleviate cold start and data sparsity issues. Although the graph-based CDRs can capture valuable information, they still have some limitations. First, embeddings are highly vulnerable to noisy interactions, because the message aggregation in the graph convolutional network can further enlarge the impact. Second, because of the property of graph-structured data, the influence of high-degree nodes on representation learning is more than that of the long-tail items, and this can cause a poor recommendation performance. In this study, we devised a novel A daptive A dversarial C ontrastive L earning framework for graph-based C ross- D omain R ecommendation ( ACLCDR ). The ACLCDR introduces reinforcement learning to generate adaptive augmented samples for contrastive learning tasks. Then, we leveraged a multitask training strategy to jointly optimize the model with auxiliary tasks. Finally, we verified the effectiveness of the ACLCDR through nine real-world cross-domain tasks adopted from Amazon and Douban. We observed that ACLCDR exceeded the best state-of-the-art baseline by 25%, 42.5%, 16.3%, and 23.8% in terms of HR@ 10 and NDCG@10 for the Music &amp; Movie task from Amazon.},
  archive      = {J_TKDD},
  doi          = {10.1145/3630259},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Adaptive adversarial contrastive learning for cross-domain recommendation},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group-aware graph neural network for nationwide city air
quality forecasting. <em>TKDD</em>, <em>18</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3631713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of air pollution threatens public health. Air quality forecasting can provide the air quality index hours or even days later, which can help the public to prevent air pollution in advance. Previous works focus on citywide air quality forecasting and cannot solve nationwide city forecasting problems, whose difficulties lie in capturing the latent dependencies between geographically distant but highly correlated cities. In this article, we propose the group-aware graph neural network (GAGNN), a hierarchical model for nationwide city air quality forecasting. The model constructs a city graph and a city group graph to model the spatial and latent dependencies between cities, respectively. GAGNN introduces a differentiable grouping network to discover the latent dependencies among cities and generate city groups. Based on the generated city groups, a group correlation encoding module is introduced to learn the correlations between them, which can effectively capture the dependencies between city groups. After the graph construction, GAGNN implements message passing mechanism to model the dependencies between cities and city groups. The evaluation experiments on two real-world nationwide city air quality datasets, including the China dataset and the US dataset, indicate that our GAGNN outperforms existing forecasting models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3631713},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Group-aware graph neural network for nationwide city air quality forecasting},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning entangled interactions of complex causality via
self-paced contrastive learning. <em>TKDD</em>, <em>18</em>(3), 1–24.
(<a href="https://doi.org/10.1145/3632406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning causality from large-scale text corpora is an important task with numerous applications—for example, in finance, biology, medicine, and scientific discovery. Prior studies have focused mainly on simple causality, which only includes one cause-effect pair. However, causality is notoriously difficult to understand and analyze because of multiple cause spans and their entangled interactions. To detect complex causality, we propose a self-paced contrastive learning model, namely N2NCause, to learn entangled interactions between multiple spans. Specifically, N2NCause introduces data enhancement operations to convert implicit expressions into explicit expressions with the most rational causal connectives for the synthesis of positive samples and to invert the directed connection between a cause-effect pair for the synthesis of negative samples. To learn the semantic dependency and causal direction of positive and negative samples, self-paced contrastive learning is proposed to learn the entangled interactions among spans, including the interaction direction and interaction field. We evaluated the performance of N2NCause in three cause-effect detection tasks. The experimental results show that, with the least data annotation efforts, N2NCause demonstrates competitive performance in detecting simple cause-effect relations, and it is superior to existing solutions for the detection of complex causality.},
  archive      = {J_TKDD},
  doi          = {10.1145/3632406},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {12},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning entangled interactions of complex causality via self-paced contrastive learning},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Open-world graph active learning for node classification.
<em>TKDD</em>, <em>18</em>(2), 1–20. (<a
href="https://doi.org/10.1145/3607144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great power of Graph Neural Networks (GNNs) relies on a large number of labeled training data, but obtaining the labels can be costly in many cases. Graph Active Learning (GAL) is proposed to reduce such annotation costs, but the existing methods mainly focus on improving labeling efficiency with fixed classes, and are limited to handle the emergence of novel classes. We term the problem as Open-World Graph Active Learning (OWGAL) and propose a framework of the same name. The key is to recognize novel-class as well as informative nodes in a unified framework. Instead of a fully connected neural network classifier, OWGAL employs prototype learning and label propagation to assign high uncertainty scores to the targeted nodes in the representation and topology space, respectively. Weighted sampling further suppresses the impact of unimportant classes by weighing both the node and class importance. Experimental results on four large-scale datasets demonstrate that our framework achieves a substantial improvement of 5.97% to 16.57% on Macro-F1 over state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3607144},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Open-world graph active learning for node classification},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised heterogeneous graph learning with
multi-level data augmentation. <em>TKDD</em>, <em>18</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3608953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, semi-supervised graph learning with data augmentation (DA) has been the most commonly used and best-performing method to improve model robustness in sparse scenarios with few labeled samples. However, most existing DA methods are based on the homogeneous graph, but none are specific for the heterogeneous graph. Differing from the homogeneous graph, DA in the heterogeneous graph faces greater challenges: heterogeneity of information requires DA strategies to effectively handle heterogeneous relations, which considers the information contribution of different types of neighbors and edges to the target nodes. Furthermore, over-squashing of information is caused by the negative curvature formed by the non-uniformity distribution and the strong clustering in a complex graph. To address these challenges, this article presents a novel method named HG-MDA (Semi-Supervised Heterogeneous Graph Learning with Multi-Level Data Augmentation). For the problem of heterogeneity of information in DA, node and topology augmentation strategies are proposed for the characteristics of the heterogeneous graph. Additionally, meta-relation-based attention is applied as one of the indexes for selecting augmented nodes and edges. For the problem of over-squashing of information, triangle-based edge adding and removing are designed to alleviate the negative curvature and bring the gain of topology. Finally, the loss function consists of the cross-entropy loss for labeled data and the consistency regularization for unlabeled data. To effectively fuse the prediction results of various DA strategies, sharpening is used. Existing experiments on public datasets (i.e., ACM, DBLP, and OGB) and the industry dataset MB show that HG-MDA outperforms current SOTA models. Additionally, HG-MDA is applied to user identification in internet finance scenarios, helping the business to add 30% key users, and increase loans and balances by 3.6%, 11.1%, and 9.8%.},
  archive      = {J_TKDD},
  doi          = {10.1145/3608953},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Semi-supervised heterogeneous graph learning with multi-level data augmentation},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph mining for cybersecurity: A survey. <em>TKDD</em>,
<em>18</em>(2), 1–52. (<a
href="https://doi.org/10.1145/3610228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth of cyber attacks today, such as malware, spam, and intrusions, has caused severe consequences on society. Securing cyberspace has become a great concern for organizations and governments. Traditional machine learning based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers have investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this work, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybersecurity tasks. For each task, we probe into relevant methods and highlight the graph types, graph approaches, and task levels in their modeling. Furthermore, we collect open datasets and toolkits for graph-based cybersecurity. Finally, we present an outlook on the potential directions of this field for future research.},
  archive      = {J_TKDD},
  doi          = {10.1145/3610228},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-52},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Graph mining for cybersecurity: A survey},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting conversation-branch-tweet HyperGraph structure to
detect misinformation on social media. <em>TKDD</em>, <em>18</em>(2),
1–20. (<a href="https://doi.org/10.1145/3610297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of misinformation on social media is a serious issue that can have negative consequences for public health and political stability. While detecting and identifying misinformation can be challenging, many attempts have been made to address this problem. However, traditional models that focus on pairwise relationships on misinformation propagation paths may not be effective in capturing the underlying connections among multiple tweets. To address this limitation, the proposed “Conversation-Branch-Tweet” hypergraph convolutional network (CBT-HGCN) uses a hypergraph to represent the internal structure and content of tweet data, with tweets and their replies viewed as nodes and hyperedges, respectively. The model first pre-processes the tweets of a conversation and then uses a pre-trained model as an encoder to extract node information. Finally, a hypergraph convolution network is used as an information fuser for classification. Experimental results on three benchmark datasets (Twitter15, Twitter16, and Pheme) show that the proposed model outperforms several strong baseline models and achieves state-of-the-art performance. This indicates that the CBT-HGCN approach is effective in detecting and identifying misinformation on social media by capturing the underlying connections among multiple tweets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3610297},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Exploiting conversation-branch-tweet HyperGraph structure to detect misinformation on social media},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversary for social good: Leveraging adversarial attacks to
protect personal attribute privacy. <em>TKDD</em>, <em>18</em>(2), 1–24.
(<a href="https://doi.org/10.1145/3614098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media has drastically reshaped the world that allows billions of people to engage in such interactive environments to conveniently create and share content with the public. Among them, text data (e.g., tweets, blogs) maintains the basic yet important social activities and generates a rich source of user-oriented information. While those explicit sensitive user data like credentials have been significantly protected by all means, personal private attribute (e.g., age, gender, location) disclosure due to inference attacks is somehow challenging to avoid, especially when powerful natural language processing (NLP) techniques have been effectively deployed to automate attribute inferences from implicit text data. This puts users’ attribute privacy at risk. To address this challenge, in this article, we leverage the inherent vulnerability of machine learning to adversarial attacks, and design a novel text-space Adv ersarial attack for S ocial G ood, called Adv4SG . In other words, we cast the problem of protecting personal attribute privacy as an adversarial attack formulation problem over the social media text data to defend against NLP-based attribute inference attacks. More specifically, Adv4SG proceeds with a sequence of word perturbations under given constraints such that the probed attribute cannot be identified correctly. Different from the prior works, we advance Adv4SG by considering social media property, and introducing cost-effective mechanisms to expedite attribute obfuscation over text data under the black-box setting. Extensive experiments on real-world social media datasets have demonstrated that our method can effectively degrade the inference accuracy with less computational cost over different attribute settings, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection.},
  archive      = {J_TKDD},
  doi          = {10.1145/3614098},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Adversary for social good: Leveraging adversarial attacks to protect personal attribute privacy},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resisting the edge-type disturbance for link prediction in
heterogeneous networks. <em>TKDD</em>, <em>18</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3614099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of heterogeneous networks has proposed new challenges to the long-standing link prediction problem. Existing models trained on the verified edge samples from different types usually learn type-specific knowledge, and their type-specific predictions may be contradictory for unverified edge samples with uncertain types. This challenge is termed edge-type disturbance in link prediction in heterogeneous networks. To address this challenge, we develop a disturbance-resilient prediction method ( DRPM ) comprising a structural characterizer, a type differentiator, and a resilient predictor. The structural characterizer is responsible for learning edge representations for link prediction. Concurrently, the type differentiator distinguishes type-specific edge representations to generate diverse type experts while maximizing their link prediction performances on specific types. Furthermore, the resilient predictor evaluates the reliability weights of different type experts to develop a resilient prediction mechanism to aggregate discriminable predictions. Extensive experiments conducted on various real-world datasets demonstrate the importance of the explainable introduction of the edge-type disturbance and the superiority of DRPM over state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3614099},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Resisting the edge-type disturbance for link prediction in heterogeneous networks},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attacking shortest paths by cutting edges. <em>TKDD</em>,
<em>18</em>(2), 1–42. (<a
href="https://doi.org/10.1145/3622941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying shortest paths between nodes in a network is a common graph analysis problem that is important for many applications involving routing of resources. An adversary that can manipulate the graph structure could alter traffic patterns to gain some benefit (e.g., make more money by directing traffic to a toll road). This article presents the Force Path Cut problem, in which an adversary removes edges from a graph to make a particular path the shortest between its terminal nodes. We prove that the optimization version of this problem is APX-hard but introduce PATHATTACK , a polynomial-time approximation algorithm that guarantees a solution within a logarithmic factor of the optimal value. In addition, we introduce the Force Edge Cut and Force Node Cut problems, in which the adversary targets a particular edge or node, respectively, rather than an entire path. We derive a nonconvex optimization formulation for these problems and derive a heuristic algorithm that uses PATHATTACK as a subroutine. We demonstrate all of these algorithms on a diverse set of real and synthetic networks, illustrating where the proposed algorithms provide the greatest improvement over baseline methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3622941},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-42},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Attacking shortest paths by cutting edges},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedEgo: Privacy-preserving personalized federated graph
learning with ego-graphs. <em>TKDD</em>, <em>18</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3624017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As special information carriers containing both structure and feature information, graphs are widely used in graph mining, e.g., Graph Neural Networks (GNNs). However, graph data are stored separately in multiple distributed parties in some practical scenarios, which may not be directly shared due to conflicts of interest. Hence, federated graph neural networks are proposed to address such data silo issues while preserving each party’s privacy (or client). Nevertheless, different graph data distributions of various parties, which is known as the statistical heterogeneity, may degrade the performance of naive federated learning algorithms like FedAvg. In this article, we propose FedEgo, a federated graph learning framework based on ego-graphs to tackle the challenges above, in which each client will train their local models while also contributing to the training of a global model. FedEgo applies GraphSAGE over ego-graphs to make full use of the structure information and utilizes Mixup for privacy concerns. To deal with the statistical heterogeneity, we integrate personalization into learning and propose an adaptive mixing coefficient strategy that enables clients to achieve their optimal personalization. Extensive experimental results and in-depth analysis demonstrate the effectiveness of FedEgo.},
  archive      = {J_TKDD},
  doi          = {10.1145/3624017},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {FedEgo: Privacy-preserving personalized federated graph learning with ego-graphs},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised graph-level representation learning with
adversarial contrastive learning. <em>TKDD</em>, <em>18</em>(2), 1–23.
(<a href="https://doi.org/10.1145/3624018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently developed unsupervised graph representation learning approaches apply contrastive learning into graph-structured data and achieve promising performance. However, these methods mainly focus on graph augmentation for positive samples, while the negative mining strategies for graph contrastive learning are less explored, leading to sub-optimal performance. To tackle this issue, we propose a Graph Adversarial Contrastive Learning (GraphACL) scheme that learns a bank of negative samples for effective self-supervised whole-graph representation learning. Our GraphACL consists of (i) a graph encoding branch that generates the representations of positive samples and (ii) an adversarial generation branch that produces a bank of negative samples. To generate more powerful hard negative samples, our method minimizes the contrastive loss during encoding updating while maximizing the contrastive loss adversarially over the negative samples for providing the challenging contrastive task. Moreover, the quality of representations produced by the adversarial generation branch is enhanced through the regularization of carefully designed bank divergence loss and bank orthogonality loss. We optimize the parameters of the graph encoding branch and adversarial generation branch alternately. Extensive experiments on 14 real-world benchmarks on both graph classification and transfer learning tasks demonstrate the effectiveness of the proposed approach over existing graph self-supervised representation learning methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3624018},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Self-supervised graph-level representation learning with adversarial contrastive learning},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature selection for efficient local-to-global bayesian
network structure learning. <em>TKDD</em>, <em>18</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3624479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local-to-global learning approach plays an essential role in Bayesian network (BN) structure learning. Existing local-to-global learning algorithms first construct the skeleton of a DAG (directed acyclic graph) by learning the MB (Markov blanket) or PC (parents and children) of each variable in a dataset, then orient edges in the skeleton. However, existing MB or PC learning methods are often computationally expensive especially with a large-sized BN, resulting in inefficient local-to-global learning algorithms. To tackle the problem, in this article, we link feature selection with local BN structure learning and develop an efficient local-to-global learning approach using filtering feature selection. Specifically, we first analyze the rationale of the well-known Minimum-Redundancy and Maximum-Relevance (MRMR) feature selection approach for learning a PC set of a variable. Based on the analysis, we propose an efficient F2SL (feature selection-based structure learning) approach to local-to-global BN structure learning. The F2SL approach first employs the MRMR approach to learn the skeleton of a DAG, then orients edges in the skeleton. Employing independence tests or score functions for orienting edges, we instantiate the F2SL approach into two new algorithms, F2SL-c (using independence tests) and F2SL-s (using score functions). Compared to the state-of-the-art local-to-global BN learning algorithms, the experiments validated that the proposed algorithms in this article are more efficient and provide competitive structure learning quality than the compared algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3624479},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Feature selection for efficient local-to-global bayesian network structure learning},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Understanding any time series classifier with a
subsequence-based explainer. <em>TKDD</em>, <em>18</em>(2), 1–34. (<a
href="https://doi.org/10.1145/3624480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing availability of time series data has increased the usage of classifiers for this data type. Unfortunately, state-of-the-art time series classifiers are black-box models and, therefore, not usable in critical domains such as healthcare or finance, where explainability can be a crucial requirement. This paper presents a framework to explain the predictions of any black-box classifier for univariate and multivariate time series. The provided explanation is composed of three parts. First, a saliency map highlighting the most important parts of the time series for the classification. Second, an instance-based explanation exemplifies the black-box’s decision by providing a set of prototypical and counterfactual time series. Third, a factual and counterfactual rule-based explanation, revealing the reasons for the classification through logical conditions based on subsequences that must, or must not, be contained in the time series. Experiments and benchmarks show that the proposed method provides faithful, meaningful, stable, and interpretable explanations.},
  archive      = {J_TKDD},
  doi          = {10.1145/3624480},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Understanding any time series classifier with a subsequence-based explainer},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multisource data fusion-based heterogeneous graph
attention network for competitor prediction. <em>TKDD</em>,
<em>18</em>(2), 1–20. (<a
href="https://doi.org/10.1145/3625101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitor identification is an essential component of corporate strategy. With the rapid development of artificial intelligence, various data-mining methodologies and frameworks have emerged to identify competitors. In general, the competitiveness among companies is determined by both market commonality and resource similarity. However, because resource information is more difficult to obtain than market information, existing studies primarily identify competitors via market commonality. To address this limitation, we introduce multisource company descriptions as well as heterogeneous business relationships, and we propose a novel method for simultaneously mining the market commonality and resource similarity. First, we use multisource company descriptions to represent companies and transform the heterogeneous business relationships into a heterogeneous business network. Then, we propose a novel multisource data fusion-based heterogeneous graph attention network (MHGAT) to learn the pairwise competitive relationships between companies. Specifically, a graph neural network-based model is proposed to learn the embeddings of companies by preserving their competition, and a multilevel attention framework is designed to integrate the embeddings from neighboring company level, heterogeneous relationship level, and multisource description level. Finally, experiments on a real-world dataset verify the effectiveness of our proposed MHGAT and demonstrate the usefulness of company descriptions and business relationships in competitor identification.},
  archive      = {J_TKDD},
  doi          = {10.1145/3625101},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A multisource data fusion-based heterogeneous graph attention network for competitor prediction},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-training-teaching: A robust semi-supervised framework for
review-aware rating regression. <em>TKDD</em>, <em>18</em>(2), 1–16. (<a
href="https://doi.org/10.1145/3625391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Review-aware Rating Regression (RaRR) suffers the severe challenge of extreme data sparsity as the multi-modality interactions of ratings accompanied by reviews are costly to obtain. Although some studies of semi-supervised rating regression are proposed to mitigate the impact of sparse data, they bear the risk of learning from noisy pseudo-labeled data. In this article, we propose a simple yet effective paradigm, called co-training-teaching ( CoT 2 ), for integrating the merits of both co-training and co-teaching toward robust semi-supervised RaRR. CoT 2 employs two predictors trained with different feature sets of textual reviews, each of which functions as both “labeler” and “validator.” Specifically, one predictor (labeler) first labels unlabeled data for its peer predictor (validator); after that, the validator samples reliable instances from the noisy pseudo-labeled data it received and sends them back to the labeler for updating. By exchanging and validating pseudo-labeled instances, the two predictors are reinforced by each other in an iterative learning process. The final prediction is made by averaging the outputs of both the refined predictors. Extensive experiments show that our CoT 2 considerably outperforms the state-of-the-art recommendation techniques in the RaRR task, especially when the training data is severely insufficient.},
  archive      = {J_TKDD},
  doi          = {10.1145/3625391},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-16},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Co-training-teaching: A robust semi-supervised framework for review-aware rating regression},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximizing the diversity of exposure in online social
networks by identifying users with increased susceptibility to
persuasion. <em>TKDD</em>, <em>18</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3625826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals may have a range of opinions on controversial topics. However, the ease of making friendships in online social networks tends to create groups of like-minded individuals, who propagate messages that reinforce existing opinions and ignore messages expressing opposite opinions. This creates a situation where there is a decrease in the diversity of messages to which users are exposed ( diversity of exposure ). This means that users do not easily get the chance to be exposed to messages containing alternative viewpoints; it is even more unlikely that they forward such messages to their friends. Increasing the chance that such messages are propagated implies that an individuals’ susceptibility to persuasion is increased, something that may ultimately increase the diversity of messages to which users are exposed. This article formulates a novel problem which aims to identify a small set of users for whom increasing susceptibility to persuasion maximizes the diversity of exposure of all users in the network. We study the properties of this problem and develop a method to find a solution with an approximation guarantee. For this, we first prove that the problem is neither submodular nor supermodular and then we develop submodular bounds for it. These bounds are used in the Sandwich framework to propose a method which approximates the solution using reverse sampling. The proposed method is validated using four real-world datasets. The obtained results demonstrate the superiority of the proposed method compared to baseline approaches.},
  archive      = {J_TKDD},
  doi          = {10.1145/3625826},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {11},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Maximizing the diversity of exposure in online social networks by identifying users with increased susceptibility to persuasion},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A clique-querying mining framework for discovering high
utility co-location patterns without generating candidates.
<em>TKDD</em>, <em>18</em>(1), 1–42. (<a
href="https://doi.org/10.1145/3617378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Groups of spatial features whose instances frequently appear together in nearby areas are regarded as prevalent co-location patterns (PCPs). Traditional PCP mining ignores the significance of instances and features. However, in reality, these instances and features have different significance, the traditional PCPs may not sufficiently expose knowledge from spatial data. This study focuses on discovering high utility co-location patterns (HUCPs) in which each instance is assigned a utility to reflect its significance. To filter HUCPs, an adaptive utility participation index (UPI) is designed. Unfortunately, the UPI does not hold the downward closure property. The performance of mining HUCPs is very inefficient since unnecessary candidates cannot be early pruned. Thus, an efficient clique-querying mining framework is devised without generating candidates. This framework first divides neighboring instances into cliques, then compacts these cliques into a hash table structure. Next, the adaptive UPI of any patterns can be quickly calculated based on their participating instances that are obtained by executing a querying scheme on the hash table. Finally, HUCPs are filtered efficiently. The effectiveness and efficiency of the proposed method are proved in both theory and experiments to make a promise that the patterns mined are more meaningful and the mining performance is significantly improved compared to the previous methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3617378},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {1},
  pages        = {1-42},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A clique-querying mining framework for discovering high utility co-location patterns without generating candidates},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MEGA: Meta-graph augmented pre-training model for knowledge
graph completion. <em>TKDD</em>, <em>18</em>(1), 1–24. (<a
href="https://doi.org/10.1145/3617379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, a large number of Knowledge Graph Completion (KGC) methods have been proposed by using embedding based manners, to overcome the incompleteness problem faced with knowledge graph (KG). One important recent innovation in Natural Language Processing (NLP) domain is the employ of deep neural models that make the most of pre-training, culminating in BERT, the most popular example of this line of approaches today. Recently, a series of new KGC methods introducing a pre-trained language model, such as KG-BERT, have been developed and released compelling performance. However, previous pre-training based KGC methods usually train the model by using simple training task and only utilize one-hop relational signals in KG, which leads that they cannot model high-order semantic contexts and multi-hop complex relatedness. To overcome this problem, this article presents a novel pre-training framework for KGC task, which especially consists of both one-hop relation level task (low-order) and multi-hop meta-graph level task (high-order). Hence, the proposed method can capture not only the elaborate sub-graph structure but also the subtle semantic information on the given KG. The empirical results show the efficiency of the proposed method on the widely used real-world datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3617379},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {1},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {MEGA: Meta-graph augmented pre-training model for knowledge graph completion},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling users’ curiosity in recommender systems.
<em>TKDD</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3617598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today’s recommender systems are criticized for recommending items that are too obvious to arouse users’ interests. Therefore, the research community has advocated some “beyond accuracy” evaluation metrics such as novelty, diversity, and serendipity with the hope of promoting information discovery and sustaining users’ interests over a long period of time. While bringing in new perspectives, most of these evaluation metrics have not considered individual users’ differences in their capacity to experience those “beyond accuracy” items. Open-minded users may embrace a wider range of recommendations than conservative users. In this article, we proposed to use curiosity traits to capture such individual users’ differences. We developed a model to approximate an individual’s curiosity distribution over different stimulus levels. We used an item’s surprise level to estimate the stimulus level and whether such a level is in the range of the user’s appetite for stimulus, called Comfort Zone . We then proposed a recommender system framework that considers both user preference and their Comfort Zone where the curiosity is maximally aroused. Our framework differs from a typical recommender system in that it leverages human’s Comfort Zone for stimuli to promote engagement with the system. A series of evaluation experiments have been conducted to show that our framework is able to rank higher the items with not only high ratings but also high curiosity stimulation. The recommendation list generated by our algorithm has a higher potential of inspiring user curiosity compared to the state-of-the-art deep learning approaches. The personalization factor for assessing the surprise stimulus levels further helps the recommender model achieve smaller (better) inter-user similarity.},
  archive      = {J_TKDD},
  doi          = {10.1145/3617598},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling users’ curiosity in recommender systems},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering interesting patterns from hypergraphs.
<em>TKDD</em>, <em>18</em>(1), 1–34. (<a
href="https://doi.org/10.1145/3622940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A hypergraph is a complex data structure capable of expressing associations among any number of data entities. Overcoming the limitations of traditional graphs, hypergraphs are useful to model real-life problems. Frequent pattern mining is one of the most popular problems in data mining with a lot of applications. To the best of our knowledge, there exists no flexible pattern mining framework for hypergraph databases decomposing associations among data entities. In this article, we propose a flexible and complete framework for mining frequent patterns from a collection of hypergraphs. To discover more interesting patterns beyond the traditional frequent patterns, we propose frameworks for weighted and uncertain hypergraph mining also. We develop three algorithms for mining frequent, weighted, and uncertain hypergraph patterns efficiently by introducing a canonical labeling technique for isomorphic hypergraphs. Extensive experiments have been conducted on real-life hypergraph databases to show both the effectiveness and efficiency of our proposed frameworks and algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3622940},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {1},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Discovering interesting patterns from hypergraphs},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure-driven representation learning for deep
clustering. <em>TKDD</em>, <em>18</em>(1), 1–25. (<a
href="https://doi.org/10.1145/3623400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important branch of unsupervised learning methods, clustering makes a wide contribution in the area of data mining. It is well known that capturing the group-discriminative properties of each sample for clustering is crucial. Among them, deep clustering delivers promising results due to the strong representational power of neural networks. However, most of them adopt sample-level learning strategies, and the standalone data point barely captures its holistic cluster’s context and may undergo sub-optimal cluster assignment. To tackle this issue, we propose a Structure-driven Representation Learning (SRL) method by introducing latent structure information into the representation learning process at both the local and global levels. Specifically, a local-structure-driven sample representation strategy is proposed to approximate the estimation of data distribution, which models the neighborhood distribution of samples with potential structure information and exploits statistical dependencies between them to improve cluster consistency. A global-structure-driven cluster representation strategy is designed, where the context of each cluster is sufficiently encoded according to its samples (exemplar-theory) and corresponding prototype (prototype-theory). In this case, each cluster can only be related to its most similar samples, and different clusters are separated as much as possible. These two models are seamlessly combined into a joint optimization problem, which can be efficiently solved. Experiments on six widely-used datasets demonstrate the superiority of SRL over state-of-the-art clustering methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3623400},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {10},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Structure-driven representation learning for deep clustering},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting the role of heterophily in graph representation
learning: An edge classification perspective. <em>TKDD</em>,
<em>18</em>(1), 1–17. (<a
href="https://doi.org/10.1145/3603378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning aims at integrating node contents with graph structure to learn nodes/graph representations. Nevertheless, it is found that many existing graph learning methods do not work well on data with high heterophily level that accounts for a large proportion of edges between different class labels. Recent efforts to this problem focus on improving the message passing mechanism. However, it remains unclear whether heterophily truly does harm to the performance of graph neural networks (GNNs). The key is to unfold the relationship between a node and its immediate neighbors, e.g., are they heterophilous or homophilious? From this perspective, here we study the role of heterophily in graph representation learning before/after the relationships between connected nodes are disclosed. In particular, we propose an end-to-end framework that both learns the type of edges (i.e., heterophilous/homophilious) and leverage edge type information to improve the expressiveness of graph neural networks. We implement this framework in two different ways. Specifically, to avoid messages passing through heterophilous edges, we can optimize the graph structure to be homophilious by dropping heterophilous edges identified by an edge classifier. Alternatively, it is possible to exploit the information about the presence of heterophilous neighbors for feature learning, so a hybrid message passing approach is devised to aggregate homophilious neighbors and diversify heterophilous neighbors based on edge classification. Extensive experiments demonstrate the remarkable performance improvement of GNNs with the proposed framework on multiple datasets across the full spectrum of homophily level.},
  archive      = {J_TKDD},
  doi          = {10.1145/3603378},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Revisiting the role of heterophily in graph representation learning: An edge classification perspective},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Same or different? Diff-vectors for authorship analysis.
<em>TKDD</em>, <em>18</em>(1), 1–36. (<a
href="https://doi.org/10.1145/3609226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we investigate the effects on authorship identification tasks (including authorship verification, closed-set authorship attribution, and closed-set and open-set same-author verification) of a fundamental shift in how to conceive the vectorial representations of documents that are given as input to a supervised learner. In “classic” authorship analysis, a feature vector represents a document, the value of a feature represents (an increasing function of) the relative frequency of the feature in the document, and the class label represents the author of the document. We instead investigate the situation in which a feature vector represents an unordered pair of documents, the value of a feature represents the absolute difference in the relative frequencies (or increasing functions thereof) of the feature in the two documents, and the class label indicates whether the two documents are from the same author or not. This latter (learner-independent) type of representation has been occasionally used before, but has never been studied systematically. We argue that it is advantageous, and that, in some cases (e.g., authorship verification), it provides a much larger quantity of information to the training process than the standard representation. The experiments that we carry out on several publicly available datasets (among which one that we here make available for the first time) show that feature vectors representing pairs of documents (that we here call Diff-Vectors ) bring about systematic improvements in the effectiveness of authorship identification tasks, and especially so when training data are scarce (as it is often the case in real-life authorship identification scenarios). Our experiments tackle same-author verification, authorship verification, and closed-set authorship attribution; while DVs are naturally geared for solving the 1st, we also provide two novel methods for solving the 2nd and 3rd that use a solver for the 1st as a building block. The code to reproduce our experiments is open-source and available online. 1},
  archive      = {J_TKDD},
  doi          = {10.1145/3609226},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-36},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Same or different? diff-vectors for authorship analysis},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on explainable anomaly detection. <em>TKDD</em>,
<em>18</em>(1), 1–54. (<a
href="https://doi.org/10.1145/3609333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past two decades, most research on anomaly detection has focused on improving the accuracy of the detection, while largely ignoring the explainability of the corresponding methods and thus leaving the explanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-critical domains, providing explanations for the high-stakes decisions made in those domains has become an ethical and regulatory requirement. Therefore, this work provides a comprehensive and structured survey on state-of-the-art explainable anomaly detection techniques. We propose a taxonomy based on the main aspects that characterise each explainable anomaly detection technique, aiming to help practitioners and researchers find the explainable anomaly detection method that best suits their needs.},
  archive      = {J_TKDD},
  doi          = {10.1145/3609333},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-54},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey on explainable anomaly detection},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TDAN: Transferable domain adversarial network for link
prediction in heterogeneous social networks. <em>TKDD</em>,
<em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1145/3610229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction has received increased attention in social network analysis. One of the unique challenges in heterogeneous social networks is link prediction in new link types without verified link information, such as recommending products to new overseas groups. Existing link prediction models tend to learn type-specific knowledge on specific link types and predict missing or future links on the same link types. However, because of the uncertainty of new link types in the evolving process of social networks, it is difficult to collect sufficient verified link information in new link types. Therefore, we propose the Transferable Domain Adversarial Network ( TDAN ) based on transfer learning to handle the challenge. TDAN exploits transferable type-shared knowledge in historical link types to help predict the unobserved links in new link types. TDAN mainly comprises a structural encoder, a domain discriminator, and an optimization decoder. The structural encoder learns the link representations in a heterogeneous social network. Subsequently, to learn transferable type-shared knowledge, the domain discriminator distinguishes link representations into different link types while minimizing the differences between type-specific knowledge in adversarial training. Inspired by the denoising auto-encoder, the optimization decoder reconstructs the learned type-shared knowledge to eliminate the noise generated during the adversarial training. Extensive experiments on Facebook and YouTube show that TDAN can outperform the state-of-the-art models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3610229},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {TDAN: Transferable domain adversarial network for link prediction in heterogeneous social networks},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving node classification accuracy of GNN through input
and output intervention. <em>TKDD</em>, <em>18</em>(1), 1–31. (<a
href="https://doi.org/10.1145/3610535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) are a popular machine learning framework for solving various graph processing applications. This framework exploits both the graph topology and the feature vectors of the nodes. One of the important applications of GNN is in the semi-supervised node classification task. The accuracy of the node classification using GNN depends on (i) the number and (ii) the choice of the training nodes. In this article, we demonstrate that increasing the training nodes by selecting nodes from the same class that are spread out across non-contiguous subgraphs, can significantly improve the accuracy. We accomplish this by presenting a novel input intervention technique that can be used in conjunction with different GNN classification methods to increase the non-contiguous training nodes and, thereby, improve the accuracy. We also present an output intervention technique to identify misclassified nodes and relabel them with their potentially correct labels. We demonstrate on real-world networks that our proposed methods, both individually and collectively, significantly improve the accuracy in comparison to the baseline GNN algorithms. Both our methods are agnostic. Apart from the initial set of training nodes generated by the baseline GNN methods, our techniques do not need any other extra knowledge about the classes of the nodes. Thus, our methods are modular and can be used as pre-and post-processing steps with many of the currently available GNN methods to improve their accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3610535},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Improving node classification accuracy of GNN through input and output intervention},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepCPR: Deep path reasoning using sequence of
user-preferred attributes for conversational recommendation.
<em>TKDD</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1145/3610775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommender systems (CRS) have garnered significant attention in academia and industry because of their ability to capture user preferences via system questions and user responses. Typically, in a CRS, reinforcement learning (RL) is utilized to determine the optimal timing for requesting attribute information or suggesting items. However, existing methods consider user-preferred attributes independently and ignore that attributes may be of different importance to the same user, in the attribute and item selection phases, which limits the accuracy and interpretability of CRS. Inspired by this, we propose deep conversational path reasoning (DeepCPR), which involves constructing a reasoning path on a graph with a series of user-favored attributes. It utilizes the attention mechanism to thoroughly examine the connections between these attributes and provide improved explanations for which attributes to inquire about or which items to recommend. In DeepCPR, two deep-learning-based modules are proposed to realize attribute and item selection. In the first module, the sequence of attributes confirmed by the user in conversation is encoded with a gated graph neural network to obtain the user’s long-term preference using a self-attention mechanism for the selection of candidate attributes. In the second module, a self-attention approach with more appropriate strategies is developed to dynamically select candidate items. In addition, to achieve fine-grained user preference modeling, a recurrent neural network is employed to aggregate the sequence of attributes that interact with the users. Numerous experimental evaluations conducted on four real CRS datasets show that the proposed method significantly outperforms existing advanced methods in terms of conversational recommendations.},
  archive      = {J_TKDD},
  doi          = {10.1145/3610775},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DeepCPR: Deep path reasoning using sequence of user-preferred attributes for conversational recommendation},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Criterion-based heterogeneous collaborative filtering for
multi-behavior implicit recommendation. <em>TKDD</em>, <em>18</em>(1),
1–26. (<a href="https://doi.org/10.1145/3611310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the explosive growth of interaction behaviors in multimedia information systems, where multi-behavior recommender systems have received increasing attention by leveraging data from various auxiliary behaviors such as tip and collect. Among various multi-behavior recommendation methods, non-sampling methods have shown superiority over negative sampling methods. However, two observations are usually ignored in existing state-of-the-art non-sampling methods based on binary regression: (1) users have different preference strengths for different items, so they cannot be measured simply by binary implicit data; (2) the dependency across multiple behaviors varies for different users and items. To tackle the above issue, we propose a novel non-sampling learning framework named C riterion-guided H eterogeneous C ollaborative F iltering (CHCF). CHCF introduces both upper and lower thresholds to indicate selection criteria, which will guide user preference learning. Besides, CHCF integrates criterion learning and user preference learning into a unified framework, which can be trained jointly for the interaction prediction of the target behavior. We further theoretically demonstrate that the optimization of Collaborative Metric Learning can be approximately achieved by the CHCF learning framework in a non-sampling form effectively. Extensive experiments on three real-world datasets show the effectiveness of CHCF in heterogeneous scenarios.},
  archive      = {J_TKDD},
  doi          = {10.1145/3611310},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Criterion-based heterogeneous collaborative filtering for multi-behavior implicit recommendation},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic attributes-driven graph attention network modeling
on behavioral finance for stock prediction. <em>TKDD</em>,
<em>18</em>(1), 1–29. (<a
href="https://doi.org/10.1145/3611311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock prediction is a challenging task due to multiple influencing factors and complex market dependencies. Traditional solutions are based on a single type of information. With the success of multi-source information in different fields, the combination of different types of information such as numerical and textual information has become a promising option. Although multi-source information provides rich multi-view information, how to mine and construct structured relationships from them is a difficult problem. Specifically, most existing methods usually extract features from commonly used multi-source information as predictive information sources, without further pre-constructing stock relationship graphs with dependencies using broader information. More importantly, they typically treat each stock as an isolated forecasting, or employ stock market correlations based on a fixed predefined graph structure, but current methods are not sensitive enough to aggregate the attribute features extracted from multi-source information and stock relationship graph, to obtain the dynamic update of market relations and relationship strength. The stock market is highly temporally, and the attributes of nodes are affected by the time perception of other attributes, which is not fully considered. To address these problems, we propose a novel dynamic attributes-driven graph attention networks incorporating sentiment (DGATS) information, transaction data, and text data. Inspired by behavioral finance, we separately extract sentiment information as a factor of technical indicators, and further realize the early fusion of technical indicators and textual data through Kronecker product-based tensor fusion. In particular, by LSTM and temporal attention network, the short-term and long-term transition features are gradually grasped from the local composition of the fused stock trading sequence. Furthermore, real-time intra-market dependencies and key attributes information are captured with graph networks, enabling dynamic updates of relationships and relationship strengths in predefined graphs. Experiments on the real datasets show that the architecture can outperform the previous methods in prediction performance.},
  archive      = {J_TKDD},
  doi          = {10.1145/3611311},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-29},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A dynamic attributes-driven graph attention network modeling on behavioral finance for stock prediction},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient transfer learning method with auxiliary
information. <em>TKDD</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3612930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning (TL) is an information reuse learning tool, which can help us learn better classification effect than traditional single task learning, because transfer learning can share information within the task-to-task model. Most TL algorithms are studied in the field of data improvement, doing some data extraction and transformation. However, it ignores that existing the additional information to improve the model’s accuracy, like Universum samples in the training data with privileged information. In this article, we focus on considering prior data to improve the TL algorithm, and the additional features also called privileged information are incorporated into the learning to improve the learning paradigm. In addition, we also carry out the Universum samples which do not belong to any indicated categories into the transfer learning paradigm to improve the utilization of prior knowledge. We propose a new TL Model (PU-TLSVM), in which each task with corresponding privileged features and Universum data is considered in the proposed model, so as to apply tasks with a priori data to the training stage. Then, we use Lagrange duality theorem to optimize our model to obtain the optimal discriminant for target task classification. Finally, we make a lot of predictions and tests to compare the actual effectiveness of the proposed method with the previous methods. The experiment results indicate that the proposed method is more effective and robust than other baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3612930},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {An efficient transfer learning method with auxiliary information},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised dynamic graph representation learning via
temporal subgraph contrast. <em>TKDD</em>, <em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3612931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning on graphs has recently drawn a lot of attention due to its independence from labels and its robustness in representation. Current studies on this topic mainly use static information such as graph structures but cannot well capture dynamic information such as timestamps of edges. Realistic graphs are often dynamic, which means the interaction between nodes occurs at a specific time. This article proposes a self-supervised dynamic graph representation learning framework DySubC, which defines a temporal subgraph contrastive learning task to simultaneously learn the structural and evolutional features of a dynamic graph. Specifically, a novel temporal subgraph sampling strategy is firstly proposed, which takes each node of the dynamic graph as the central node and uses both neighborhood structures and edge timestamps to sample the corresponding temporal subgraph. The subgraph representation function is then designed according to the influence of neighborhood nodes on the central node after encoding the nodes in each subgraph. Finally, the structural and temporal contrastive loss are defined to maximize the mutual information between node representation and temporal subgraph representation. Experiments on five real-world datasets demonstrate that (1) DySubC performs better than the related baselines including two graph contrastive learning models and five dynamic graph representation learning models, especially in the link prediction task, and (2) the use of temporal information cannot only sample more effective subgraphs, but also learn better representation by temporal contrastive loss.},
  archive      = {J_TKDD},
  doi          = {10.1145/3612931},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Self-supervised dynamic graph representation learning via temporal subgraph contrast},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Laplacian-based cluster-contractive t-SNE for
high-dimensional data visualization. <em>TKDD</em>, <em>18</em>(1),
1–22. (<a href="https://doi.org/10.1145/3612932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction techniques aim at representing high-dimensional data in low-dimensional spaces to extract hidden and useful information or facilitate visual understanding and interpretation of the data. However, few of them take into consideration the potential cluster information contained implicitly in the high-dimensional data. In this article, we propose Lap tSNE, a new graph-layout nonlinear dimensionality reduction method based on t-SNE, one of the best techniques for visualizing high-dimensional data as 2D scatter plots. Specifically, Lap tSNE leverages the eigenvalue information of the graph Laplacian to shrink the potential clusters in the low-dimensional embedding when learning to preserve the local and global structure from high-dimensional space to low-dimensional space. It is nontrivial to solve the proposed model because the eigenvalues of normalized symmetric Laplacian are functions of the decision variable. We provide a majorization-minimization algorithm with convergence guarantee to solve the optimization problem of Lap tSNE and show how to calculate the gradient analytically, which may be of broad interest when considering optimization with Laplacian-composited objective. We evaluate our method by a formal comparison with state-of-the-art methods on seven benchmark datasets, both visually and via established quantitative measurements. The results demonstrate the superiority of our method over baselines such as t-SNE and UMAP. We also provide out-of-sample extension, large-scale extension, and mini-batch extension for our Lap tSNE to facilitate dimensionality reduction in various scenarios.},
  archive      = {J_TKDD},
  doi          = {10.1145/3612932},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Laplacian-based cluster-contractive t-SNE for high-dimensional data visualization},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed cooperative coevolution of data publishing
privacy and transparency. <em>TKDD</em>, <em>18</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3613962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data transparency is beneficial to data participants’ awareness, users’ fairness, and research work’s reproducibility. However, when addressing transparency requirements, we cannot ignore data privacy. This article defines the multi-objective data publishing (MODP) problem, optimizing data privacy and transparency at the same time. Accordingly, we propose a distributed cooperative coevolutionary genetic algorithm (DCCGA) to optimize the MODP problem. In the population of DCCGA, each individual represents an anonymization solution to MODP. Three modules in DCCGA, i.e., grouping module, cooperative coevolutionary module, and evolving module, are proposed for distributed sub-population update and evaluation, improving DCCGA’s optimization performance and parallel efficiency. Moreover, a matrix-based crossover operator and a matrix-based mutation operator are designed to exchange and adjust anonymization information in the individuals efficiently. Experimental results demonstrate that the proposed DCCGA outperforms the competitors with respect to solution accuracy, convergence speed, and scalability. Besides, we verify the effectiveness of all the proposed components in DCCGA.},
  archive      = {J_TKDD},
  doi          = {10.1145/3613962},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Distributed cooperative coevolution of data publishing privacy and transparency},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adapting knowledge inference algorithms to measure geometry
competencies through a puzzle game. <em>TKDD</em>, <em>18</em>(1), 1–23.
(<a href="https://doi.org/10.1145/3614436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid technological evolution of the last years has motivated students to develop capabilities that will prepare them for an unknown future in the 21st century. In this context, many teachers intend to optimise the learning process, making it more dynamic and exciting through the introduction of gamification. Thus, this article focuses on a data-driven assessment of geometry competencies, which are essential for developing problem-solving and higher-order thinking skills. Our main goal is to adapt, evaluate and compare Bayesian Knowledge Tracing (BKT), Performance Factor Analysis (PFA), Elo, and Deep Knowledge Tracing (DKT) algorithms applied to the data of a geometry game named Shadowspect, in order to predict students’ performance by means of several classifier metrics. We analysed two algorithmic configurations, with and without prioritisation of Knowledge Components (KCs) – the skills needed to complete a puzzle successfully, and we found Elo to be the algorithm with the best prediction power with the ability to model the real knowledge of students. However, the best results are achieved without KCs because it is a challenging task to differentiate between KCs effectively in game environments. Our results prove that the above-mentioned algorithms can be applied in formal education to improve teaching, learning, and organisational efficiency.},
  archive      = {J_TKDD},
  doi          = {10.1145/3614436},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {9},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Adapting knowledge inference algorithms to measure geometry competencies through a puzzle game},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HUSP-SP: Faster utility mining on sequence data.
<em>TKDD</em>, <em>18</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3597935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-utility sequential pattern mining (HUSPM) has emerged as an important topic due to its wide application and considerable popularity. However, due to the combinatorial explosion of the search space when the HUSPM problem encounters a low-utility threshold or large-scale data, it may be time-consuming and memory-costly to address the HUSPM problem. Several algorithms have been proposed for addressing this problem, but they still cost a lot in terms of running time and memory usage. In this article, to further solve this problem efficiently, we design a compact structure called sequence projection (seqPro) and propose an efficient algorithm, namely, discovering high-utility sequential patterns with the seqPro structure (HUSP-SP). HUSP-SP utilizes the compact seq-array to store the necessary information in a sequence database. The seqPro structure is designed to efficiently calculate candidate patterns’ utilities and upper-bound values. Furthermore, a new upper bound on utility, namely, tighter reduced sequence utility and two pruning strategies in search space, are utilized to improve the mining performance of HUSP-SP. Experimental results on both synthetic and real-life datasets show that HUSP-SP can significantly outperform the state-of-the-art algorithms in terms of running time, memory usage, search space pruning efficiency, and scalability.},
  archive      = {J_TKDD},
  doi          = {10.1145/3597935},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {HUSP-SP: Faster utility mining on sequence data},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness in recommender systems: Evaluation approaches and
assurance strategies. <em>TKDD</em>, <em>18</em>(1), 1–37. (<a
href="https://doi.org/10.1145/3604558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide application of recommender systems, the potential impacts of recommender systems on customers, item providers and other parties have attracted increasing attention. Fairness, which is the quality of treating people equally, is also becoming important in recommender system evaluation and algorithm design. Therefore, in the past years, there has been a growing interest in fairness measurement and assurance in recommender systems. Although there are several reviews on related topics, such as fairness in machine learning and debias in recommender systems, they do not present a systematic view on fairness in recommender systems, which is context aware and has a multi-sided meaning. Therefore, in this review, the concept of fairness is discussed in detail in the various contexts of recommender systems. Specifically, a comprehensive framework to classify fairness metrics is proposed from four dimensions, i.e., Fairness for Whom , Demographic Unit , Time Frame , and Quantification Method . Then the strategies for eliminating unfairness in recommendations, fairness in different recommendation tasks and datasets are reviewed and summarized. Finally, the challenges and future work are discussed.},
  archive      = {J_TKDD},
  doi          = {10.1145/3604558},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-37},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fairness in recommender systems: Evaluation approaches and assurance strategies},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential and graphical cross-domain recommendations with a
multi-view hierarchical transfer gate. <em>TKDD</em>, <em>18</em>(1),
1–28. (<a href="https://doi.org/10.1145/3604615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain recommender systems could potentially improve the recommendation performance by means of transferring abundant knowledge from the auxiliary domain to the target domain. They could help address some key challenges in recommender systems, such as data sparsity and cold start. However, most existing cross-domain recommendation approaches represent the user preferences based on a single kind of user’s feature or behavior and fail to explore the hidden interaction effects of different kinds of features or behaviors. In this article, we propose the S equential and G raphical Cross -Domain Recommendations with a Multi-View Hierarchical Transfer Gate (SGCross) to transfer user representations from multiple perspectives. The SGCross model constructs a user profile by learning the personal preference from a personal view, the dynamic preference from a temporal view, as well as the collaborative preference from a collaborative view. Specifically, a Multi-view Hierarchical Gate (MHG) is designed to transfer the informative representations of user knowledge on different views from the auxiliary domain separately, aiming to enhance the user representations. Furthermore, a two-stage attentive fusion module is designed to integrate transferred information at two levels: the domain level and the view level. Extensive experiments on the Amazon dataset and the Douban dataset have demonstrated that SGCross effectively improves the accuracy of cross-domain recommendations and outperforms the state-of-the-art baseline models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3604615},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Sequential and graphical cross-domain recommendations with a multi-view hierarchical transfer gate},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Server-client collaborative distillation for federated
reinforcement learning. <em>TKDD</em>, <em>18</em>(1), 1–22. (<a
href="https://doi.org/10.1145/3604939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) learns a global model in a distributional manner, which does not require local clients to share private data. Such merit has drawn lots of attention in the interaction scenarios, where Federated Reinforcement Learning (FRL) emerges as a cross-field research direction focusing on the robust training of agents. Different from FL, the heterogeneity problem in FRL is more challenging because the data depends on the policy of agents and the environment dynamics. FRL learns to interact under the non-stationary environment feedback, while the typical FL methods aim at handling the constant data heterogeneity. In this article, we are among the first attempts to analyze the heterogeneity problem in FRL and propose an off-policy FRL framework. Specifically, a student–teacher–student model learning and fusion method, termed as Server-Client Collaborative Distillation (SCCD), is introduced. Unlike the traditional FL, we distill all local models on the server side for model fusion. To reduce the variance of the training, a local distillation is also conducted every time the agent receives the global model. Experimentally, we compare SCCD with a range of straightforward combinations between FL methods and RL. The results demonstrate that SCCD has a superior performance in four classical continuous control tasks with non-IID environments.},
  archive      = {J_TKDD},
  doi          = {10.1145/3604939},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Server-client collaborative distillation for federated reinforcement learning},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TTS-norm: Forecasting tensor time series via multi-way
normalization. <em>TKDD</em>, <em>18</em>(1), 1–25. (<a
href="https://doi.org/10.1145/3605894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor time series (TTS) data, a generalization of one-dimensional time series on a high-dimensional space, is ubiquitous in real-world applications. Compared to modeling time series or multivariate time series, which has received much attention and achieved tremendous progress in recent years, tensor time series has been paid less effort. However, properly coping with the TTS is a much more challenging task, due to its high-dimensional and complex inner structure. In this article, we start by revealing the structure of TTS data from afn statistical view of point. Then, in line with this analysis, we perform T ensor T ime S eries forecasting via a proposed Multi-way Norm alization ( TTS-Norm ), which effectively disentangles multiple heterogeneous low-dimensional substructures from the original high-dimensional structure. Finally, we design a novel objective function for TTS forecasting, accounting for the numerical heterogeneity among different low-dimensional subspaces of TTS. Extensive experiments on two real-world datasets verify the superior performance of our proposed model. 1},
  archive      = {J_TKDD},
  doi          = {10.1145/3605894},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {TTS-norm: Forecasting tensor time series via multi-way normalization},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HGV4Risk: Hierarchical global view-guided sequence
representation learning for risk prediction. <em>TKDD</em>,
<em>18</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3605895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk prediction, usually achieved by learning representations from patient’s physiological sequence or user’s behavioral sequence data, and has been widely applied in healthcare and finance. Despite that, some recent time-aware deep learning methods have led to superior performances in such sequence representation learning tasks, such improvement is limited due to a lack of guidance from hierarchical global view. To address this issue, we propose a novel end-to-end H ierarchical G lobal V iew-guided (HGV) sequence representation learning framework. Specifically, the Global Graph Embedding (GGE) module is proposed to learn sequential clip-aware representations from temporal correlation graph (TCG) at instance level. Furthermore, following the way of key-query attention, the harmonic β-attention (β-Attn) is also developed for making a global tradeoff between time-aware decay and observation significance at channel level adaptively. Moreover, the hierarchical representations at both instance level and channel level can be coordinated by the heterogeneous information aggregation under the guidance of global view. Experimental results on both healthcare risk prediction benchmark and SMEs credit overdue risk prediction task from the real-world industrial scenario in MYBank, Ant Group, have illustrated that the proposed model can achieve competitive prediction performance compared with other known baselines. The code has been released public available at: https://github.com/LiYouru0228/HGV.},
  archive      = {J_TKDD},
  doi          = {10.1145/3605895},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {HGV4Risk: Hierarchical global view-guided sequence representation learning for risk prediction},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-label quantification. <em>TKDD</em>, <em>18</em>(1),
1–36. (<a href="https://doi.org/10.1145/3606264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantification, variously called supervised prevalence estimation or learning to quantify , is the supervised learning task of generating predictors of the relative frequencies (a.k.a. prevalence values ) of the classes of interest in unlabelled data samples. While many quantification methods have been proposed in the past for binary problems and, to a lesser extent, single-label multiclass problems, the multi-label setting (i.e., the scenario in which the classes of interest are not mutually exclusive) remains by and large unexplored. A straightforward solution to the multi-label quantification problem could simply consist of recasting the problem as a set of independent binary quantification problems. Such a solution is simple but naïve, since the independence assumption upon which it rests is, in most cases, not satisfied. In these cases, knowing the relative frequency of one class could be of help in determining the prevalence of other related classes. We propose the first truly multi-label quantification methods, i.e., methods for inferring estimators of class prevalence values that strive to leverage the stochastic dependencies among the classes of interest in order to predict their relative frequencies more accurately. We show empirical evidence that natively multi-label solutions outperform the naïve approaches by a large margin. The code to reproduce all our experiments is available online.},
  archive      = {J_TKDD},
  doi          = {10.1145/3606264},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-36},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-label quantification},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient density-peaks clustering algorithms on static and
dynamic data in euclidean space. <em>TKDD</em>, <em>18</em>(1), 1–27.
(<a href="https://doi.org/10.1145/3607873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering multi-dimensional points is a fundamental task in many fields, and density-based clustering supports many applications because it can discover clusters of arbitrary shapes. This article addresses the problem of Density-Peaks Clustering (DPC) in Euclidean space. DPC already has many applications, but its straightforward implementation incurs O ( n 2 ) time, where n is the number of points, thereby does not scale to large datasets. To enable DPC on large datasets, we first propose empirically efficient exact DPC algorithm, Ex-DPC. Although this algorithm is much faster than the straightforward implementation, it still suffers from O ( n 2 ) time theoretically. We hence propose a new exact algorithm, Ex-DPC++, that runs in o ( n 2 ) time. We accelerate their efficiencies by leveraging multi-threading. Moreover, real-world datasets may have arbitrary updates (point insertions and deletions). It is hence important to support efficient cluster updates. To this end, we propose D-DPC for fully dynamic DPC. We conduct extensive experiments using real datasets, and our experimental results demonstrate that our algorithms are efficient and scalable.},
  archive      = {J_TKDD},
  doi          = {10.1145/3607873},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Efficient density-peaks clustering algorithms on static and dynamic data in euclidean space},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view graph convolutional networks with differentiable
node selection. <em>TKDD</em>, <em>18</em>(1), 1–21. (<a
href="https://doi.org/10.1145/3608954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data containing complementary and consensus information can facilitate representation learning by exploiting the intact integration of multi-view features. Because most objects in the real world often have underlying connections, organizing multi-view data as heterogeneous graphs is beneficial to extracting latent information among different objects. Due to the powerful capability to gather information of neighborhood nodes, in this article, we apply Graph Convolutional Network (GCN) to cope with heterogeneous graph data originating from multi-view data, which is still under-explored in the field of GCN. In order to improve the quality of network topology and alleviate the interference of noises yielded by graph fusion, some methods undertake sorting operations before the graph convolution procedure. These GCN-based methods generally sort and select the most confident neighborhood nodes for each vertex, such as picking the top- k nodes according to pre-defined confidence values. Nonetheless, this is problematic due to the non-differentiable sorting operators and inflexible graph embedding learning, which may result in blocked gradient computations and undesired performance. To cope with these issues, we propose a joint framework dubbed Multi-view Graph Convolutional Network with Differentiable Node Selection (MGCN-DNS), which is constituted of an adaptive graph fusion layer, a graph learning module, and a differentiable node selection schema. MGCN-DNS accepts multi-channel graph-structural data as inputs and aims to learn more robust graph fusion through a differentiable neural network. The effectiveness of the proposed method is verified by rigorous comparisons with considerable state-of-the-art approaches in terms of multi-view semi-supervised classification tasks, and the experimental results indicate that MGCN-DNS achieves pleasurable performance on several benchmark multi-view datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3608954},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-view graph convolutional networks with differentiable node selection},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete listwise content-aware recommendation.
<em>TKDD</em>, <em>18</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3609334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To perform online inference efficiently, hashing techniques, devoted to encoding model parameters as binary codes, play a key role in reducing the computational cost of content-aware recommendation (CAR), particularly on devices with limited computation resource. However, current hashing methods for CAR fail to align their learning objectives (e.g., squared loss) with the ranking-based metrics (e.g., Normalized Discounted Cumulative Gain (NDCG)), resulting in suboptimal recommendation accuracy. In this article, we propose a novel ranking-based CAR hashing method based on Factorization Machine (FM), called Discrete Listwise FM (DLFM), for fast and accurate recommendation. Concretely, our DLFM is to optimize NDCG in the Hamming space for preserving the listwise user-item relationships. We devise an efficient algorithm to resolve the challenging DLFM problem, which can directly learn binary parameters in a relaxed continuous solution space, without additional quantization. Particularly, our theoretical analysis shows that the optimal solution to the relaxed continuous optimization problem is approximately the same as that of the original discrete optimization problem. Through extensive experiments on two real-world datasets, we show that DLFM consistently outperforms state-of-the-art hashing-based recommendation techniques.},
  archive      = {J_TKDD},
  doi          = {10.1145/3609334},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Discrete listwise content-aware recommendation},
  volume       = {18},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-label feature selection via adaptive label correlation
estimation. <em>TKDD</em>, <em>17</em>(9), 1–28. (<a
href="https://doi.org/10.1145/3604560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-label learning, each instance is associated with multiple labels simultaneously. Multi-label data often have noisy, irrelevant, and redundant features of high dimensionality. Multi-label feature selection has received considerable attention as an effective means for dealing with high-dimensional multi-label data. Many multi-label feature selection methods exploit label correlations to help select features. However, finding label correlations and selecting features in existing multi-label feature selection methods are often two separate processes, the existence of noises and outliers in training data makes the label correlations exploited from label space less reliable. Therefore, the learned label correlations may mislead the feature selection process and result in the selection of less informative features. This article proposes a novel algorithm named ROAD, i.e., multi-label featuRe selectiOn via ADaptive label correlation estimation. ROAD jointly performs adaptive label correlation exploration and feature selection with alternating optimization to obtain reliable estimation of label correlations, which can more effectively reveal the intrinsic manifold structure among labels and lead to the selection of a more proper feature subset. Comprehensive experiments on several frequently used datasets validate the superiority of ROAD against the state-of-the-art multi-label feature selection algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3604560},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {9},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-label feature selection via adaptive label correlation estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoupledGT: Coupled geospatial-temporal data modeling for air
quality prediction. <em>TKDD</em>, <em>17</em>(9), 1–21. (<a
href="https://doi.org/10.1145/3604616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution seriously affects public health, while effective air quality prediction remains a challenging problem since the complex spatial-temporal couplings exist in multi-area monitoring data of the city. Current approaches rarely consider relative geographical locations when capturing spatial-temporal relations, instead the latent inter-dependencies (i.e., implicit spatial relations) of data as a replacement. However, such relations cannot necessarily reflect the diffusion of air pollutants in the real world, and genuine location-related information could be lost during the implicit relation learning process. In this article, we introduce a new concept, geospatial-temporal data, and propose a novel deep neural network architecture, CoupledGT, to learn the geospatial-temporal couplings within data for air quality prediction. Specifically, the asymmetric diffusion relation of air quality data between two areas is first explicitly represented by the newly developed planar Gaussian diffusion (PGD) equation. And then, a geospatial couplings diffuser (GCD) is designed to parameterize the PGD equation and learn multi-areas diffusion mutually affected geospatial couplings. Besides, the RNN is employed to capture temporal couplings of each area, and incorporated with GCD to learn both shared and unique characteristics of the geospatial-temporal data simultaneously, which empowers the generalization and efficiency of the model. Extensive experiments on two real-world datasets demonstrate our method is robust and outperforms existing baseline methods in air quality prediction tasks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3604616},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {8},
  number       = {9},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {CoupledGT: Coupled geospatial-temporal data modeling for air quality prediction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multifaceted relation-aware meta-learning with dual
customization for user cold-start recommendation. <em>TKDD</em>,
<em>17</em>(9), 1–27. (<a
href="https://doi.org/10.1145/3597458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User cold-start scenarios pose great challenges to recommendation systems in accurately capturing user preferences with sparse interaction records. Besides incorporating auxiliary information to enrich user/item representations, recent studies under the schema of meta-learning focus on quickly adapting personalized recommendation models based on cold-start users’ scarce interactions. The majority of meta-learning based recommendation methods follow a bi-level optimization paradigm and learn globally shared initialization across all cold-start recommendation tasks. In addition, to further facilitate the ability of fast adaptation, existing methods have made efforts to tailor task-specific prior knowledge by identifying the individual characteristics of each task. However, we argue that multi-view commonalities between existing users and cold-start users are also essential for precisely distinguishing new tasks, but not comprehensively modeled in previous studies. In this article, we propose a multifaceted relation-aware meta-learning approach namely MeCM for user cold-start recommendation, which enhances task-adaptive initialization customization by extracting multiple views of task relevance. We design a dual customization framework consisting of two successive phases including cluster-level customization and task-level customization. Specifically, MeCM first extracts multifaceted semantic relations between tasks and refines task commonalities into task clusters maintained with memory networks (MNs). Globally learned fast weights corresponding to task clusters are queried to perform cluster-level customization. Then task-level customization is triggered based on contextual information of the target task via interaction-wise encoding. Extensive experiments on real-world datasets demonstrate the superior performance of our model over state-of-the-art meta-learning-based recommendation methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3597458},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {9},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multifaceted relation-aware meta-learning with dual customization for user cold-start recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An information theory based method for quantifying the
predictability of human mobility. <em>TKDD</em>, <em>17</em>(9), 1–19.
(<a href="https://doi.org/10.1145/3597500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on human mobility drives the development of economy and society. How to predict when and where one will go accurately is one of the core research questions. Existing work is mainly concerned with performance of mobility prediction models. Since accuracy of predict models does not indicate whether or not one’s mobility is inherently easy to predict, there has not been a definite conclusion about that to what extent can our predictions of human mobility be accurate. To help solve this problem, we describe the formalized definition of predictability of human mobility, propose a model based on additive Markov chain to measure the probability of exploration, and further develop an information theory based method for quantifying the predictability considering exploration of human mobility. Then, we extend our method by using mutual information in order to measure the predictability considering external influencing factors, which has not been studied before. Experiments on simulation data and three real-world datasets show that our method yields a tighter upper bound on predictability of human mobility than previous work, and that predictability increased slightly when considering external factors such as weather and temperature.},
  archive      = {J_TKDD},
  doi          = {10.1145/3597500},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {9},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {An information theory based method for quantifying the predictability of human mobility},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint inference of diffusion and structure in partially
observed social networks using coupled matrix factorization.
<em>TKDD</em>, <em>17</em>(9), 1–28. (<a
href="https://doi.org/10.1145/3599237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access to complete data in large-scale networks is often infeasible. Therefore, the problem of missing data is a crucial and unavoidable issue in the analysis and modeling of real-world social networks. However, most of the research on different aspects of social networks does not consider this limitation. One effective way to solve this problem is to recover the missing data as a pre-processing step. In this paper, a model is learned from partially observed data to infer unobserved diffusion and structure networks. To jointly discover omitted diffusion activities and hidden network structures, we develop a probabilistic generative model called “DiffStru.” The interrelations among links of nodes and cascade processes are utilized in the proposed method via learning coupled with low-dimensional latent factors. Besides inferring unseen data, latent factors such as community detection may also aid in network classification problems. We tested different missing data scenarios on simulated independent cascades over LFR networks and real datasets, including Twitter and Memetracker. Experiments on these synthetic and real-world datasets show that the proposed method successfully detects invisible social behaviors, predicts links, and identifies latent features.},
  archive      = {J_TKDD},
  doi          = {10.1145/3599237},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {9},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Joint inference of diffusion and structure in partially observed social networks using coupled matrix factorization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on influence maximization: From an ML-based
combinatorial optimization. <em>TKDD</em>, <em>17</em>(9), 1–50. (<a
href="https://doi.org/10.1145/3604559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence Maximization (IM) is a classical combinatorial optimization problem, which can be widely used in mobile networks, social computing, and recommendation systems. It aims at selecting a small number of users such that maximizing the influence spread across the online social network. Because of its potential commercial and academic value, there are a lot of researchers focusing on studying the IM problem from different perspectives. The main challenge comes from the NP-hardness of the IM problem and #P-hardness of estimating the influence spread, thus traditional algorithms for overcoming them can be categorized into two classes: heuristic algorithms and approximation algorithms. However, there is no theoretical guarantee for heuristic algorithms, and the theoretical design is close to the limit. Therefore, it is almost impossible to further optimize and improve their performance. With the rapid development of artificial intelligence, technologies based on Machine Learning (ML) have achieved remarkable achievements in many fields. In view of this, in recent years, a number of new methods have emerged to solve combinatorial optimization problems by using ML-based techniques. These methods have the advantages of fast solving speed and strong generalization ability to unknown graphs, which provide a brand-new direction for solving combinatorial optimization problems. Therefore, we abandon the traditional algorithms based on iterative search and review the recent development of ML-based methods, especially Deep Reinforcement Learning, to solve the IM problem and other variants in social networks. We focus on summarizing the relevant background knowledge, basic principles, common methods, and applied research. Finally, the challenges that need to be solved urgently in future IM research are pointed out.},
  archive      = {J_TKDD},
  doi          = {10.1145/3604559},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {7},
  number       = {9},
  pages        = {1-50},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey on influence maximization: From an ML-based combinatorial optimization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank representation with adaptive dimensionality
reduction via manifold optimization for clustering. <em>TKDD</em>,
<em>17</em>(9), 1–18. (<a
href="https://doi.org/10.1145/3589767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dimensionality reduction techniques are often used to reduce data dimensionality for computational efficiency or other purposes in existing low-rank representation (LRR)-based methods. However, the two steps of dimensionality reduction and learning low-rank representation coefficients are implemented in an independent way; thus, the adaptability of representation coefficients to the original data space may not be guaranteed. This article proposes a novel model, i.e., low-rank representation with adaptive dimensionality reduction (LRRARD) via manifold optimization for clustering, where dimensionality reduction and learning low-rank representation coefficients are integrated into a unified framework. This model introduces a low-dimensional projection matrix to find the projection that best fits the original data space. And the low-dimensional projection matrix and the low-rank representation coefficients interact with each other to simultaneously obtain the best projection matrix and representation coefficients. In addition, a manifold optimization method is employed to obtain the optimal projection matrix, which is an unconstrained optimization method in a constrained search space. The experimental results on several real datasets demonstrate the superiority of our proposed method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3589767},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Low-rank representation with adaptive dimensionality reduction via manifold optimization for clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Road network representation learning: A dual graph-based
approach. <em>TKDD</em>, <em>17</em>(9), 1–25. (<a
href="https://doi.org/10.1145/3592859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road network is a critical infrastructure powering many applications including transportation, mobility and logistics in real life. To leverage the input of a road network across these different applications, it is necessary to learn the representations of the roads in the form of vectors, which is named road network representation learning (RNRL). While several models have been proposed for RNRL, they capture the pairwise relationships/connections among roads only (i.e., as a simple graph), and fail to capture among roads the high-order relationships (e.g., those roads that jointly form a local region usually have similar features such as speed limit) and long-range relationships (e.g., some roads that are far apart may have similar semantics such as being roads in residential areas). Motivated by this, we propose to construct a hypergraph , where each hyperedge corresponds to a set of multiple roads forming a region. The constructed hypergraph would naturally capture the high-order relationships among roads with hyperedges. We then allow information propagation via both the edges in the simple graph and the hyperedges in the hypergraph in a graph neural network context. In addition, we introduce different pretext tasks based on both the simple graph (i.e., graph reconstruction) and the hypergraph (including hypergraph reconstruction and hyperedge classification) for optimizing the representations of roads. The graph reconstruction and hypergraph reconstruction tasks are conventional ones and can capture structural information. The hyperedge classification task can capture long-range relationships between pairs of roads that belong to hyperedges with the same label. We call the resulting model HyperRoad . We further extend HyperRoad to problem settings when additional inputs of road attributes and/or trajectories that are generated on the roads are available. We conduct extensive experiments on two real datasets, for five downstream tasks, and under four problem settings, which demonstrate that our model achieves impressive improvements compared with existing baselines across datasets, tasks, problem settings, and performance metrics. CCS Concepts: • Information systems → Data mining ; • Urban computing ; • Spatial-temporal systems ;},
  archive      = {J_TKDD},
  doi          = {10.1145/3592859},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Road network representation learning: A dual graph-based approach},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervision for tabular data by learning to predict
additive homoskedastic gaussian noise as pretext. <em>TKDD</em>,
<em>17</em>(9), 1–17. (<a
href="https://doi.org/10.1145/3594720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of scalability of data annotation translates to the need to decrease dependency on labels. Self-supervision offers a solution with data training themselves. However, it has received relatively less attention on tabular data, data that drive a large proportion of business and application domains. This work, which we name the Statistical Self-Supervisor (SSS), proposes a method for self-supervision on tabular data by defining a continuous perturbation as pretext. It enables a neural network to learn representations by learning to predict the level of additive isotropic Gaussian noise added to inputs. The choice of the pretext transformation is motivated by intrinsic characteristics of a neural network fundamentally performing linear fits under the widely adopted assumption of Gaussianity in its fitting error and the preservation of locality of a data example on the data manifold in the presence of small random perturbations. The transform condenses information in the generated representations, making them better employable for further task-specific prediction as evidenced by performance improvement of the downstream classifier. To evaluate the persistence of performance under low-annotation settings, SSS is evaluated against different levels of label availability to the downstream classifier (1% to 100%) and benchmarked against self- and semi-supervised methods. At the most label-constrained, 1% setting, we report a maximum increase of at least 2.5% against the next-best semi-supervised competing method. We report an increase of more than 1.5% against self-supervised state of the art. Ablation studies also reveal that increasing label availability from 0% to 1% results in a maximum increase of up to 50% on either of the five performance metrics and up to 15% thereafter, indicating diminishing returns in additional annotation.},
  archive      = {J_TKDD},
  doi          = {10.1145/3594720},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Self-supervision for tabular data by learning to predict additive homoskedastic gaussian noise as pretext},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid continuous-time dynamic graph representation
learning model by exploring both temporal and repetitive information.
<em>TKDD</em>, <em>17</em>(9), 1–22. (<a
href="https://doi.org/10.1145/3596447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, dynamic graph representation learning has attracted more and more attention from both academic and industrial communities due to its capabilities of capturing different real-world phenomena. For a dynamic graph represented as a sequence of timestamped events, there are two kinds of evolutionary essences: temporal and repetitive information. At present, the temporal information of interactions (e.g., timestamps) have been deeply explored. However, as another vital nature of dynamic graphs, the repetitive information of interactions between two nodes is neglected, which may lead to inaccurate node representation. To address this issue, we propose a novel continuous-time dynamic graph representation learning model, which consists of a node-level-memory based module, a historical high-order neighborhood based vertical aggregation module and a repetitive-topological information based horizontal aggregation module. In particular, to characterize the evolving pattern of the repetitive information of interactions between a pair of nodes, we put forward a repetitive-interaction based attention mechanism to integrate the two key attributes (i.e., the content and the number of interactions) of repetitive interactions at different moments, based on the insight that the repetitive behaviors of nodes are widespread and essential. We conduct extensive experiments including future link prediction tasks (for transductive and inductive learning) and dynamic node classification task, and results on three real-life dynamic graph datasets demonstrate that the proposed method significantly outperforms state-of-the-art baselines, for both observed nodes and new ones.},
  archive      = {J_TKDD},
  doi          = {10.1145/3596447},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A hybrid continuous-time dynamic graph representation learning model by exploring both temporal and repetitive information},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STA-TCN: Spatial-temporal attention over temporal
convolutional network for next point-of-interest recommendation.
<em>TKDD</em>, <em>17</em>(9), 1–19. (<a
href="https://doi.org/10.1145/3596497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a vastly increasing popularity of location-based social networks (LBSNs), which facilitates studies on the next Point-of-Interest (POI) recommendation problem. A user’s POI visiting behavior shows the sequential transition correlation with previous successive check-ins and the global spatial-temporal correlation with those check-ins that happened a long time ago at a similar time of day and in geographically close areas. Although previous POI recommendation methods attempted to capture these two correlations, several limitations remain to be solved: (1) RNNs are widely adopted to capture the sequential transition correlation, whereas training an RNN is rather time-consuming given the long input check-in sequence. (2) The pairwise proximities on time of day and geographical area of check-ins are crucial for global spatial-temporal correlation learning, but have not been comprehensively considered by previous methods. To tackle these issues, we propose a novel next POI recommendation framework named STA-TCN. Specifically, instead of RNNs, STA-TCN augments the Temporal Convolutional Network with gated input injection to learn sequential transition correlation. Furthermore, STA-TCN fuses two novel grid-difference and time-sensitivity learning mechanisms with attention network to learn the pairwise spatial-temporal proximities among a user’s check-ins. Extensive experiments are conducted on two large-scale real-world LBSN datasets, and the results show that STA-TCN outperforms the best state-of-the-art baseline with an average improvement of 9.71% and 7.88% on hit rate and normalized discounted cumulative gain, respectively.},
  archive      = {J_TKDD},
  doi          = {10.1145/3596497},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {STA-TCN: Spatial-temporal attention over temporal convolutional network for next point-of-interest recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TechPat: Technical phrase extraction for patent mining.
<em>TKDD</em>, <em>17</em>(9), 1–31. (<a
href="https://doi.org/10.1145/3596603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, due to the explosive growth of patent applications, patent mining has drawn extensive attention and interest. An important issue of patent mining is that of recognizing the technologies contained in patents, which serves as a fundamental preparation for deeper analysis. To this end, in this article, we make a focused study on constructing a technology portrait for each patent, i.e., to recognize technical phrases concerned in it, which can summarize and represent patents from a technical perspective. Along this line, a critical challenge is how to analyze the unique characteristics of technical phrases and illustrate them with definite descriptions. Therefore, we first generate the detailed descriptions about the technical phrases existing in extensive patents based on different criteria, including various previous works, practical experience, and statistical analyses. Then, considering the unique characteristics of technical phrases and the complex structure of patent documents, such as multi-aspect semantics and multi-level relevances, we further propose a novel unsupervised model, namely TechPat, which can not only automatically recognize technical phrases from massive patents but also avoid the need for expensive human labeling. After that, we evaluate the extraction results from various aspects. Specifically, we propose a novel evaluation metric called Information Retrieval Efficiency (IRE) to quantify the performance of extracted technical phrases from a new perspective. Extensive experiments on real-world patent data demonstrate that the TechPat model can effectively discriminate technical phrases in patents and greatly outperform existing methods. We further apply extracted technical phrases to two practical application tasks, namely patent search and patent classification, where the experimental results confirm the wide application prospects of technical phrases. Finally, we discuss the generalization ability of our proposed methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3596603},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {TechPat: Technical phrase extraction for patent mining},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards a better tradeoff between quality and efficiency of
community detection: An inductive embedding method across graphs.
<em>TKDD</em>, <em>17</em>(9), 1–34. (<a
href="https://doi.org/10.1145/3596605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many network applications can be formulated as NP-hard combinatorial optimization problems of community detection (CD) that partitions nodes of a graph into several groups with dense linkage. Most existing CD methods are transductive , which independently optimized their models for each single graph, and can only ensure either high quality or efficiency of CD by respectively using advanced machine learning techniques or fast heuristic approximation. In this study, we consider the CD task and aims to alleviate its NP-hard challenge. Motivated by the efficient inductive inference of graph neural networks (GNNs), we explore the possibility to achieve a better tradeoff between the quality and efficiency of CD via an inductive embedding scheme across multiple graphs of a system and propose a novel inductive community detection (ICD) method. Concretely, ICD first conducts the offline training of an adversarial dual GNN structure on historical graphs to capture key properties of a system. The trained model is then directly generalized to new graphs of the same system for online CD without additional optimization, where a better tradeoff between quality and efficiency can be achieved. Compared with existing inductive approaches, we develop a novel feature extraction module based on graph coarsening, which can efficiently extract informative feature inputs for GNNs. Moreover, our original designs of adversarial dual GNN and clustering regularization loss further enable ICD to capture permutation-invariant community labels in the offline training and help derive community-preserved embedding to support the high-quality online CD. Experiments on a set of benchmarks demonstrate that ICD can achieve a significant tradeoff between quality and efficiency over various baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3596605},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Towards a better tradeoff between quality and efficiency of community detection: An inductive embedding method across graphs},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accurate open-set recognition for memory workload.
<em>TKDD</em>, <em>17</em>(9), 1–14. (<a
href="https://doi.org/10.1145/3597027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we accurately identify new memory workloads while classifying known memory workloads? Verifying DRAM (Dynamic Random Access Memory) using various workloads is an important task to guarantee the quality of DRAM. A crucial component in the process is open-set recognition which aims to detect new workloads not seen in the training phase. Despite its importance, however, existing open-set recognition methods are unsatisfactory in terms of accuracy since they fail to exploit the characteristics of workload sequences. In this article, we propose Acorn , an accurate open-set recognition method capturing the characteristics of workload sequences. Acorn extracts two types of feature vectors to capture sequential patterns and spatial locality patterns in memory access. Acorn then uses the feature vectors to accurately classify a subsequence into one of the known classes or identify it as the unknown class. Experiments show that Acorn achieves state-of-the-art accuracy, giving up to 37% points higher unknown class detection accuracy while achieving comparable known class classification accuracy than existing methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3597027},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-14},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Accurate open-set recognition for memory workload},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling long- and short-term user preferences via
self-supervised learning for next POI recommendation. <em>TKDD</em>,
<em>17</em>(9), 1–20. (<a
href="https://doi.org/10.1145/3597211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the accumulation of check-in data from location-based services, next Point-of-Interest (POI) recommendations are gaining increasing attention. It is well known that the spatio-temporal contextual information of user check-in behavior plays a crucial role in handling vital and inherent challenges in next POI recommendation, including capture of user dynamic preferences and the sparsity problem of check-in data. However, many studies either ignore or simply stack the context features with the embedding of POIs while relying only on POI recommendation loss to optimize the entire model, therefore failing to take full advantage of the potential information in contexts. Additionally, users’ interests are usually unstable and evolve over time, and accordingly recent studies have proposed various approaches to predict users’ next POIs by incorporating contextual information and modeling both their long- and short-term preferences, respectively. Yet many studies overemphasize the final POI recommendation performance, and the association between POI sequences and contextual information is not well embodied in data representations. In this article, we focus on the preceding problems and propose a unified attention framework for next POI recommendation by modeling users’ Long- and Short-term Preferences via Self-supervised Learning (LSPSL). Specifically, based on the self-attention network and two self-supervised optimization objectives, LSPSL first deeply exploits the intrinsic correlations between POI sequences and contextual information through pre-training, which strengthens data representations. Then, supported by pre-trained contextualized embeddings, LSPSL models and fuses users’ complex long- and short-term preferences in a unified way. Extensive experiments on real-world datasets demonstrate the superiority of our model compared with other state-of-the-art approaches.},
  archive      = {J_TKDD},
  doi          = {10.1145/3597211},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {9},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling long- and short-term user preferences via self-supervised learning for next POI recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contrastive learning based graph convolution network for
social recommendation. <em>TKDD</em>, <em>17</em>(8), 1–21. (<a
href="https://doi.org/10.1145/3587268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting social networks is expected to enhance the performance of recommender systems when interaction information is sparse. Existing social recommendation models focus on modeling multi-graph structures and then aggregating the information from these multiple graphs to learn potential user preferences. However, these methods often employ complex models and redundant parameters to get a slight performance improvement. Contrastive learning has been widely researched as an effective paradigm in the area of recommendation. Most existing contrastive learning-based models usually focus on constructing multi-graph structures to perform graph augmentation for contrastive learning. However, the effect of graph augmentation on contrastive learning is inconclusive. In view of these challenges, in this work, we propose a contrastive learning based graph convolution network for social recommendation (CLSR), which integrates information from both the social graph and the interaction graph. First, we propose a fusion-simplified method to combine the social graph and the interaction graph. Technically, on the basis of exploring users’ interests by interaction graph, we further exploit social connections to alleviate data sparsity. By combining the user embeddings learned through two graphs in a certain proportion, we can obtain user representation at a finer granularity. Meanwhile, we introduce a contrastive learning framework for multi-graph network modeling, where we explore the feasibility of constructing positive and negative samples of contrastive learning by conducting data augmentation on embedding representations. Extensive experiments verify the superiority of CLSR’s contrastive learning framework and fusion-simplified method of integrating social relations.},
  archive      = {J_TKDD},
  doi          = {10.1145/3587268},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Contrastive learning based graph convolution network for social recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive collaborative soft label learning for unsupervised
multi-view feature selection. <em>TKDD</em>, <em>17</em>(8), 1–25. (<a
href="https://doi.org/10.1145/3591467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised multi-view feature selection aims to select informative features with multi-view features and unsupervised learning. It is a challenging problem due to the absence of explicit semantic supervision. Recently, graph theory and hard pseudo-label learning have been adopted to solve multi-view feature selection problems under the unsupervised learning paradigm. However, graph-based methods are difficult to support large-scale real scenarios due to the high computational complexity of graph construction. Moreover, existing methods based on hard pseudo-label learning generally result in significant information loss. In this article, we propose an Adaptive Collaborative Soft Label Learning (ACSLL) model for unsupervised multi-view feature selection. In this model, collaborative soft label learning and multi-view feature selection are integrated into a unified framework. Specifically, we learn the pseudo soft labels from each view feature by a simple and efficient method and fuse them with an adaptive weighting strategy into a joint soft label matrix. This matrix is further used for guiding the feature selection process to identify valuable features. An effective optimization strategy guaranteed with proven convergence is derived to iteratively solve this problem. Experiments demonstrate the superiority of the proposed method in both feature selection accuracy and efficiency.},
  archive      = {J_TKDD},
  doi          = {10.1145/3591467},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Adaptive collaborative soft label learning for unsupervised multi-view feature selection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel classification technique based on formal methods.
<em>TKDD</em>, <em>17</em>(8), 1–30. (<a
href="https://doi.org/10.1145/3592796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In last years, we are witnessing a growing interest in the application of supervised machine learning techniques in the most disparate fields. One winning factor of machine learning is represented by its ability to easily create models, as it does not require prior knowledge about the application domain. Complementary to machine learning are formal methods, that intrinsically offer safeness check and mechanism for reasoning on failures. Considering the weaknesses of machine learning, a new challenge could be represented by the use of formal methods. However, formal methods require the expertise of the domain, knowledge about modeling language with its semantic and mathematical rigour to specify properties. In this article, we propose a novel learning technique based on the adoption of formal methods for classification thanks to the automatic generation both of the formula and of the model. In this way the proposed method does not require any human intervention and thus it can be applied also to complex/large datasets. This leads to less effort both in using formal methods and in a better explainability and reasoning about the obtained results. Through a set of case studies from different real-world domains (i.e., driver detection, scada attack identification, arrhythmia characterization, mobile malware detection, and radiomics for lung cancer analysis), we demonstrate the usefulness of the proposed method, by showing that we are able to overcome the performances obtained from widespread classification algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3592796},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A novel classification technique based on formal methods},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling regime shifts in multiple time series.
<em>TKDD</em>, <em>17</em>(8), 1–31. (<a
href="https://doi.org/10.1145/3592857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the problem of discovering and modeling regime shifts in an ecosystem comprising multiple time series known as co-evolving time series. Regime shifts refer to the changing behaviors exhibited by series at different time intervals. Learning these changing behaviors is a key step toward time series forecasting. While advances have been made, existing methods suffer from one or more of the following shortcomings: (1) failure to take relationships between time series into consideration for discovering regimes in multiple time series; (2) lack of an effective approach that models time-dependent behaviors exhibited by series; (3) difficulties in handling data discontinuities which may be informative. Most of the existing methods are unable to handle all of these three issues in a unified framework. This, therefore, motivates our effort to devise a principled approach for modeling interactions and time-dependency in co-evolving time series. Specifically, we model an ecosystem of multiple time series by summarizing the heavy ensemble of time series into a lighter and more meaningful structure called a mapping grid . By using the mapping grid, our model first learns time series behavioral dependencies through a dynamic network representation, then learns the regime transition mechanism via a full time-dependent Cox regression model. The originality of our approach lies in modeling interactions between time series in regime identification and in modeling time-dependent regime transition probabilities, usually assumed to be static in existing work.},
  archive      = {J_TKDD},
  doi          = {10.1145/3592857},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling regime shifts in multiple time series},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view graph representation learning beyond homophily.
<em>TKDD</em>, <em>17</em>(8), 1–21. (<a
href="https://doi.org/10.1145/3592858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised graph representation learning (GRL) aims at distilling diverse graph information into task-agnostic embeddings without label supervision. Due to a lack of support from labels, recent representation learning methods usually adopt self-supervised learning, and embeddings are learned by solving a handcrafted auxiliary task (so-called pretext task). However, partially due to the irregular non-Euclidean data in graphs, the pretext tasks are generally designed under homophily assumptions and cornered in the low-frequency signals, which results in significant loss of other signals, especially high-frequency signals widespread in graphs with heterophily. Motivated by this limitation, we propose a multi-view perspective and the usage of diverse pretext tasks to capture different signals in graphs into embeddings. A novel framework, denoted as Multi-view Graph Encoder (MVGE), is proposed, and a set of key designs are identified. More specifically, a set of new pretext tasks are designed to encode different types of signals, and a straightforward operation is proposed to maintain both the commodity and personalization in both the attribute and the structural levels. Extensive experiments on synthetic and real-world network datasets show that the node representations learned with MVGE achieve significant performance improvements in three different downstream tasks, especially on graphs with heterophily.},
  archive      = {J_TKDD},
  doi          = {10.1145/3592858},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-view graph representation learning beyond homophily},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional independence test based on residual similarity.
<em>TKDD</em>, <em>17</em>(8), 1–18. (<a
href="https://doi.org/10.1145/3593810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many regression-based conditional independence (CI) test methods have been proposed to solve the problem of causal discovery. These methods provide alternatives to test CI of x,y given Z by first removing the information of the controlling set Z from x and y , and then testing the independence between the two residuals R x,Z and R y,Z . When the residuals are linearly uncorrelated, the independence test between them is nontrivial. With the ability to calculate inner product in high-dimensional space, kernel-based methods are usually used to achieve this goal, but they are considerably time-consuming. In this paper, we test the independence between two linear combinations under linear structural equation model. We show that the dependence between the two residuals can be captured by the difference between the similarity of R x,Z and R y,Z and that of R x,Z and R r ( R r is an independent copy of R y,Z ) in high-dimensional space. With this result, we provide a new way to test CI based on the similarity between residuals, which is called SCIT — the abbreviation of Similarity-based CI Testing. Furthermore, we develop two versions of the proposal, called Kernel-SCIT and Neural-SCIT, respectively. Kernel-SCIT calculates the similarity by using kernel functions, while Neural-SCIT approximates the upper bound of the similarity by using deep neural networks. In both algorithms, random permutation tests are performed to control Type I error rate. The proposed tests are evaluated on (conditional) independence test and causal discovery with both synthetic and real datasets. Experimental results show that Kernel-SCIT is simpler yet more efficient and effective than the typical existing kernel-based methods HSIC and KCIT in the cases of small sample size, and Neural-SCIT can significantly boost the performance of CI testing when sufficient samples are available. The source code is available at https://github.com/xyw5vplus1/SCIT .},
  archive      = {J_TKDD},
  doi          = {10.1145/3593810},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Conditional independence test based on residual similarity},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Community-based influence maximization using network
embedding in dynamic heterogeneous social networks. <em>TKDD</em>,
<em>17</em>(8), 1–21. (<a
href="https://doi.org/10.1145/3594544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) is a very important issue in social network diffusion analysis. The topology of real social network is large-scale, dynamic, and heterogeneous. The heterogeneity, and continuous expansion and evolution of social network pose a challenge to find influential users. Existing IM algorithms usually assume that social networks are static or dynamic but homogeneous to simplify the complexity of the IM problem. We propose a community-based influence maximization algorithm using network embedding in dynamic heterogeneous social networks. We use DyHATR algorithm to obtain the propagation feature vectors of network nodes, and execute k -means cluster algorithm to transform the original network into a coarse granularity network (CGN). On CGN, we propose a community-based three-hop independent cascade model and construct the objective function of IM problem. We design a greedy heuristics algorithm to solve the IM problem with \((1-\frac{1}{e})-\) approximation guarantee and use community structure to quickly identify seed users and estimate their influence value. Experimental results on real social networks demonstrated that compared with existing IM algorithms, our proposed algorithm had better comprehensive performance with respect to the influence value, more less execution time and memory consumption, and better scalability.},
  archive      = {J_TKDD},
  doi          = {10.1145/3594544},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Community-based influence maximization using network embedding in dynamic heterogeneous social networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instrumental variable-driven domain generalization with
unobserved confounders. <em>TKDD</em>, <em>17</em>(8), 1–21. (<a
href="https://doi.org/10.1145/3595380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization (DG) aims to learn from multiple source domains a model that can generalize well on unseen target domains. Existing DG methods mainly learn the representations with invariant marginal distribution of the input features, however, the invariance of the conditional distribution of the labels given the input features is more essential for unknown domain prediction. Meanwhile, the existing of unobserved confounders which affect the input features and labels simultaneously cause spurious correlation and hinder the learning of the invariant relationship contained in the conditional distribution. Interestingly, with a causal view on the data generating process, we find that the input features of one domain are valid instrumental variables for other domains. Inspired by this finding, we propose an instrumental variable-driven DG method (IV-DG) by removing the bias of the unobserved confounders with two-stage learning. In the first stage, it learns the conditional distribution of the input features of one domain given input features of another domain. In the second stage, it estimates the relationship by predicting labels with the learned conditional distribution. Theoretical analyses and simulation experiments show that it accurately captures the invariant relationship. Extensive experiments on real-world datasets demonstrate that IV-DG method yields state-of-the-art results.},
  archive      = {J_TKDD},
  doi          = {10.1145/3595380},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {6},
  number       = {8},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Instrumental variable-driven domain generalization with unobserved confounders},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining diverse meta-features to accurately identify
recurring concept drift in data streams. <em>TKDD</em>, <em>17</em>(8),
1–36. (<a href="https://doi.org/10.1145/3587098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from streaming data is challenging as the distribution of incoming data may change over time, a phenomenon known as concept drift. The predictive patterns, or experience learned under one distribution may become irrelevant as conditions change under concept drift, but may become relevant once again when conditions reoccur. Adaptive learning methods adapt a classifier to concept drift by identifying which distribution, or concept , is currently present in order to determine which experience is relevant. Identifying a concept requires some representation to be stored for comparison, with the quality of the representation being key to accurate identification. Existing concept representations are based on meta-features, efficient univariate summaries of a concept. However, no single meta-feature can fully represent a concept, leading to severe accuracy loss when existing representations cannot describe concept drift. To avoid these failure cases, we propose the first general framework for combining a diverse range of meta-features into a single representation. We solve two main challenges, first presenting a method of efficiently computing, storing, and querying an arbitrary set of meta-features as a single representation, showing that a combination of meta-features may successfully avoid failure cases seen with existing methods. Second, we present the first method for dynamically learning which meta-features distinguish concepts in any given dataset, significantly improving performance. Our proposed approach enables state-of-the-art feature selection methods, such as mutual information, to be applied to concept representation meta-features for the first time. We investigate tradeoffs between memory budget and classification performance, observing accuracy increases of up to 16% by dynamically weighting the contribution of each meta-feature.},
  archive      = {J_TKDD},
  doi          = {10.1145/3587098},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {8},
  pages        = {1-36},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Combining diverse meta-features to accurately identify recurring concept drift in data streams},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SMONE: A session-based recommendation model based on
neighbor sessions with similar probabilistic intentions. <em>TKDD</em>,
<em>17</em>(8), 1–22. (<a
href="https://doi.org/10.1145/3587099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A session-based recommendation system (SRS) tries to predict the next possible choice of anonymous users. In recent years, graph neural network (GNN) models have been successfully applied to SRSs and have achieved great success. Using GNN models in SRSs, each session graph is processed successively to obtain the embedding of the node (i.e, each action on an item), which is then imported into the prediction module to generate recommendation results. However, solely depending on the session graph to obtain the node embeddings is not sufficient because each session only involves a few items. Therefore, neighbor sessions have been used to extend the session graph to learn more informative node representations. In this paper, we introduce a S ession-based recommendation MO del based on N eighbor sessions with similar probabilistic int E ntions(SMONE). SMONE models the intentions behind sessions in a probabilistic way and retrieves the neighbor sessions with similar intentions. After the neighbor sessions are found, the target session and its neighbor sessions are modeled as a hypyergraph to learn the contextualized embeddings, which are combined with item embeddings through GNN to produce the final item recommendations. Experiments on real-world datasets prove the effectiveness and superiority of SMONE.},
  archive      = {J_TKDD},
  doi          = {10.1145/3587099},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {8},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SMONE: A session-based recommendation model based on neighbor sessions with similar probabilistic intentions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ADATIME: A benchmarking suite for domain adaptation on time
series data. <em>TKDD</em>, <em>17</em>(8), 1–18. (<a
href="https://doi.org/10.1145/3587937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation methods aim at generalizing well on unlabeled test data that may have a different (shifted) distribution from the training data. Such methods are typically developed on image data, and their application to time series data is less explored. Existing works on time series domain adaptation suffer from inconsistencies in evaluation schemes, datasets, and backbone neural network architectures. Moreover, labeled target data are often used for model selection, which violates the fundamental assumption of unsupervised domain adaptation. To address these issues, we develop a benchmarking evaluation suite ( AdaTime ) to systematically and fairly evaluate different domain adaptation methods on time series data. Specifically, we standardize the backbone neural network architectures and benchmarking datasets, while also exploring more realistic model selection approaches that can work with no labeled data or just a few labeled samples. Our evaluation includes adapting state-of-the-art visual domain adaptation methods to time series data as well as the recent methods specifically developed for time series data. We conduct extensive experiments to evaluate 11 state-of-the-art methods on five representative datasets spanning 50 cross-domain scenarios. Our results suggest that with careful selection of hyper-parameters, visual domain adaptation methods are competitive with methods proposed for time series domain adaptation. In addition, we find that hyper-parameters could be selected based on realistic model selection approaches. Our work unveils practical insights for applying domain adaptation methods on time series data and builds a solid foundation for future works in the field. The code is available at github.com/emadeldeen24/AdaTime .},
  archive      = {J_TKDD},
  doi          = {10.1145/3587937},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {8},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {ADATIME: A benchmarking suite for domain adaptation on time series data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kernel fisher dictionary transfer learning. <em>TKDD</em>,
<em>17</em>(8), 1–17. (<a
href="https://doi.org/10.1145/3588575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dictionary learning is an efficient knowledge representation method that can learn the essential features of data. Traditional dictionary learning methods are difficult to obtain nonlinear information when processing large-scale and high-dimensional datasets. While most dictionary learning algorithms are based on the assumption that the training data and test data have the same feature distribution, which is not always true in practical applications. To address the above problems, we propose the Kernel Fisher Dictionary Transfer Learning (KFDTL) algorithm. First, we map each sample to high-dimensional space through kernel mapping and use any dictionary learning algorithm to learn the essential features. Then, the feature-based transfer learning method is performed to predict the labels of the target samples. This method includes three main contributions: (1) KFDTL constructs a discriminative Fisher embedding model to make the same class samples have similar coding coefficients; (2) Based on the relationship between profiles and atoms, KFDTL constructs an adaptive model that adapts source domain samples to target domain samples; (3) The kernel method is used to efficiently solve nonlinear problems. Experiments on a large number of public image datasets have proved the effectiveness of the proposed method. The source code of the proposed method is available at https://github.com/zzfan3/KFDTL .},
  archive      = {J_TKDD},
  doi          = {10.1145/3588575},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {8},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Kernel fisher dictionary transfer learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph neural networks with motisf-aware for tenuous subgraph
finding. <em>TKDD</em>, <em>17</em>(8), 1–19. (<a
href="https://doi.org/10.1145/3589643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tenuous subgraph finding aims to detect a subgraph with few social interactions and weak relationships among nodes. Despite significant efforts made on this task, they are mostly carried out in view of graph-structured data. These methods depend on calculating the shortest path and need to enumerate all the paths between nodes, which suffer the combinatorial explosion. Moreover, they all lack the integration of neighborhood information. To this end, we propose a novel model named Graph Neural Network with Motif-aware for tenuous subgraph finding (GNNM), a neighborhood aggregation-based GNN framework that can capture the latent relationship between nodes. We design a GNN module to project nodes into a low-dimensional vector combining the higher-order correlation within nodes based on a motif-aware module. Then we design greedy algorithms in vector space to obtain a tenuous subgraph whose size is greater than a specified constraint. Particularly, considering that existing evaluation indicators cannot capture the latent friendship between nodes, we introduce a novel Potential Friend concept to measure the tenuity of a graph from a new perspective. Experimental results on the real-world and synthetic datasets demonstrate that our proposed method GNNM outperforms existing algorithms in efficiency and subgraph quality.},
  archive      = {J_TKDD},
  doi          = {10.1145/3589643},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {8},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Graph neural networks with motisf-aware for tenuous subgraph finding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning the explainable semantic relations via unified
graph topic-disentangled neural networks. <em>TKDD</em>, <em>17</em>(8),
1–23. (<a href="https://doi.org/10.1145/3589964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) such as Graph Convolutional Networks (GCNs) can effectively learn node representations via aggregating neighbors based on the relation graph. However, despite a few exceptions, most of the previous work in this line does not consider the topical semantics underlying the edges, making the node representations less effective and the learned relation between nodes hard to explain. For instance, the current GNNs make us usually don’t know what is the reason for the connection of network nodes, such as the specific research topics cited in this article and the concerns among friends on social platforms. Some methods have begun to explore the extraction of relation semantics in recent related literature, but existing studies generally face two bottlenecks, i.e., either being unable to explain the mined latent relations to ensure their reasonableness and independence, or demanding the textual content of edges which is unavailable in most real-world datasets. Actually, these two issues are both crucial in practical use. In our work, we propose a novel Topic-Disentangled Graph Neural Network (TDG) to address the above two issues at the same time, which explores the relation topics from the perspective of node contents. We design an optimized graph topic module to handle node features to construct independent and explainable semantic subspaces, then the reasonable relation topics that correspond to these subspaces are assigned to each graph relation via a neighborhood routing mechanism. Our proposed model can be easily combined with related graph tasks to form an end-to-end model, to avoid the risk of deviation between node representation space and task space. To evaluate the efficiency of our model, sufficient node-related tasks are conducted on three public datasets in the experimental section. The results show the obvious superiority of TDG compared with the state-of-the-art models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3589964},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {8},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning the explainable semantic relations via unified graph topic-disentangled neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computing graph descriptors on edge streams. <em>TKDD</em>,
<em>17</em>(8), 1–25. (<a
href="https://doi.org/10.1145/3591468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction is an essential task in graph analytics. These feature vectors, called graph descriptors, are used in downstream vector-space-based graph analysis models. This idea has proved fruitful in the past, with spectral-based graph descriptors providing state-of-the-art classification accuracy. However, known algorithms to compute meaningful descriptors do not scale to large graphs since: (1) they require storing the entire graph in memory, and (2) the end-user has no control over the algorithm’s runtime. In this article, we present streaming algorithms to approximately compute three different graph descriptors capturing the essential structure of graphs. Operating on edge streams allows us to avoid storing the entire graph in memory, and controlling the sample size enables us to keep the runtime of our algorithms within desired bounds. We demonstrate the efficacy of the proposed descriptors by analyzing the approximation error and classification accuracy. Our scalable algorithms compute descriptors of graphs with millions of edges within minutes. Moreover, these descriptors yield predictive accuracy comparable to the state-of-the-art methods but can be computed using only 25% as much memory.},
  archive      = {J_TKDD},
  doi          = {10.1145/3591468},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {8},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Computing graph descriptors on edge streams},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards informative and diverse dialogue systems over
hierarchical crowd intelligence knowledge graph. <em>TKDD</em>,
<em>17</em>(7), 1–25. (<a
href="https://doi.org/10.1145/3583758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-enhanced dialogue systems aim at generating factually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog , a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of “crowd intelligence knowledge” extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the C rowd I ntelligence K nowledge G raph ( CIKG ) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the G ated F usion with D ynamic Knowledge- D ependent ( GFDD ) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.},
  archive      = {J_TKDD},
  doi          = {10.1145/3583758},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {7},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fitting imbalanced uncertainties in multi-output time series
forecasting. <em>TKDD</em>, <em>17</em>(7), 1–23. (<a
href="https://doi.org/10.1145/3584704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on multi-step ahead time series forecasting with the multi-output strategy. From the perspective of multi-task learning (MTL), we recognize imbalanced uncertainties between prediction tasks of different future time steps. Unexpectedly, trained by the standard summed Mean Squared Error (MSE) loss, existing multi-output forecasting models may suffer from performance drops due to the inconsistency between the loss function and the imbalance structure. To address this problem, we reformulate each prediction task as a distinct Gaussian Mixture Model (GMM) and derive a multi-level Gaussian mixture loss function to better fit imbalanced uncertainties in multi-output time series forecasting. Instead of using the two-step Expectation-Maximization (EM) algorithm, we apply the self-attention mechanism on the task-specific parameters to learn the correlations between different prediction tasks and generate the weight distribution for each GMM component. In this way, our method jointly optimizes the parameters of the forecasting model and the mixture model simultaneously in an end-to-end fashion, avoiding the need of two-step optimization. Experiments on three real-world datasets demonstrate the effectiveness of our multi-level Gaussian mixture loss compared to models trained with the standard summed MSE loss function. All the experimental data and source code are available at https://github.com/smallGum/GMM-FNN .},
  archive      = {J_TKDD},
  doi          = {10.1145/3584704},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {7},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fitting imbalanced uncertainties in multi-output time series forecasting},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generalized deep learning clustering algorithm based on
non-negative matrix factorization. <em>TKDD</em>, <em>17</em>(7), 1–20.
(<a href="https://doi.org/10.1145/3584862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a popular research topic in the field of data mining, in which the clustering method based on non-negative matrix factorization (NMF) has been widely employed. However, in the update process of NMF, there is no learning rate to guide the update as well as the update depends on the data itself, which leads to slow convergence and low clustering accuracy. To solve these problems, a generalized deep learning clustering (GDLC) algorithm based on NMF is proposed in this article. Firstly, a nonlinear constrained NMF (NNMF) algorithm is constructed to achieve sequential updates of the elements in the matrix guided by the learning rate. Then, the gradient values corresponding to the element update are transformed into generalized weights and generalized biases, by inputting the elements as well as their corresponding generalized weights and generalized biases into the nonlinear activation function to construct the GDLC algorithm. In addition, for improving the understanding of the GDLC algorithm, its detailed inference procedure and algorithm design are provided. Finally, the experimental results on eight datasets show that the GDLC algorithm has efficient performance.},
  archive      = {J_TKDD},
  doi          = {10.1145/3584862},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {7},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A generalized deep learning clustering algorithm based on non-negative matrix factorization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust influence maximization under both aleatory and
epistemic uncertainty. <em>TKDD</em>, <em>17</em>(7), 1–27. (<a
href="https://doi.org/10.1145/3587100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncertainty is ubiquitous in almost every real-life optimization problem, which must be effectively managed to get a robust outcome. This is also true for the Influence Maximization (IM) problem, which entails locating a set of influential users within a social network. However, most of the existing IM approaches have overlooked the uncertain factors in finding the optimal solution, which often leads to subpar performance in reality. A few recent studies have considered only the epistemic uncertainty (i.e., arises from the imprecise data), while ignoring completely the aleatory uncertainty (i.e., arises from natural or physical variability). In this article, we propose a formulation and a novel algorithm for the Robust Influence Maximization (RIM) problem under both types of uncertainties. First, we develop a robust influence spread function under aleatory uncertainty that, in contrast to the existing IM theory, is no longer monotone and submodular. Thereafter, we expand our RIM formulation to incorporate epistemic uncertainty aiming to maximize the robust ratio between the selected worst-case solution and the best-case optimal solution, adopting a conservative approach. Furthermore, using a chance-constraint-based method, we investigated feasibility robustness by accounting for the uncertainties related to constraint functions. Finally, an Evolutionary Algorithm (named EA-RIM) is designed to solve the proposed formulation of the RIM problem. Experimental evaluation results on four empirical datasets show that our proposed formulation and algorithm are more effective in dealing with uncertainties and finding an optimal solution for the RIM problem.},
  archive      = {J_TKDD},
  doi          = {10.1145/3587100},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {7},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Robust influence maximization under both aleatory and epistemic uncertainty},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-aware domain mining and cross-aware supervision for
weakly-supervised semantic segmentation. <em>TKDD</em>, <em>17</em>(7),
1–19. (<a href="https://doi.org/10.1145/3589343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly Supervised Semantic Segmentation with image-level annotation uses localization maps from the classifier to generate pseudo labels. However, such localization maps focus only on sparse salient object regions, it is difficult to generate high-quality segmentation labels, which deviates from the requirement of semantic segmentation. To address this issue, we propose a dual-aware domain mining and cross-aware supervision (DDMCAS) method for weakly-supervised semantic segmentation. Specifically, we propose a dual-aware domain mining (DDM) module consisting of graph-based global reasoning unit and salient-region extension controller, which produces dense localization maps by exploring object features in salient regions and adjacent non-salient regions simultaneously. In order to further bridge the gap between salient regions and adjacent non-salient regions to generate more refined localization maps, we propose a cross-aware supervision (CAS) strategy to recover missing parts of the target objects and enhance weak attention in adjacent non-salient regions, leading to pseudo labels of higher quality for training the segmentation network. Based on the generated pseudo-labels, extensive experiments on PASCAL VOC 2012 dataset demonstrate that our method outperforms state-of-the-art methods using image-level labels for weakly supervised semantic segmentation.},
  archive      = {J_TKDD},
  doi          = {10.1145/3589343},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dual-aware domain mining and cross-aware supervision for weakly-supervised semantic segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DuCape: Dual quaternion and capsule network–based temporal
knowledge graph embedding. <em>TKDD</em>, <em>17</em>(7), 1–19. (<a
href="https://doi.org/10.1145/3589644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the development of temporal knowledge graph technology, more and more Temporal Knowledge Graph Embedded (TKGE) models have been developed. The effectiveness of TKGE largely depends on the ability to model intrinsic relation patterns and capture specific information about entities and relations. However, existing approaches can capture only some of them with insufficient modeling capacity, and none has a “deep” architecture for modeling the entries in a quadruple at the same dimension. In this article, we propose a more powerful KGE framework named DuCape , which combines a dual quaternion and capsule network in modeling for the first time to make up for the defects of existing TKGE models. In dual quaternion vector space, the head entity learns a k -dimensional rigid transformation parametrized by relation and time, falling near its corresponding tail entity. Further, we employ the embeddings of entities, relations, and time trained from dual quaternion vector space as the input to capsule networks. Experimental results on several basic datasets show that the DuCape model constructed in this article is superior to existing state-of-the-art models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3589644},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DuCape: Dual quaternion and capsule Network–Based temporal knowledge graph embedding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view ensemble clustering via low-rank and sparse
decomposition: From matrix to tensor. <em>TKDD</em>, <em>17</em>(7),
1–19. (<a href="https://doi.org/10.1145/3589768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a significant extension of classical clustering methods, ensemble clustering first generates multiple basic clusterings and then fuses them into one consensus partition by solving a problem concerning graph partition with respect to the co-association matrix. Although the collaborative cluster structure among basic clusterings can be well discovered by ensemble clustering, most advanced ensemble clustering utilizes the self-representation strategy with the constraint of low-rank to explore a shared consensus representation matrix in multiple views. However, they still encounter two challenges: (1) high computational cost caused by both the matrix inversion operation and singular value decomposition of large-scale square matrices; (2) less considerable attention on high-order correlation attributed to the pursue of the two-dimensional pair-wise relationship matrix. In this article, based on low-rank and sparse decomposition from both matrix and tensor perspectives, we propose two novel multi-view ensemble clustering methods, which tangibly decrease computational complexity. Specifically, our first method utilizes low-rank and sparse matrix decomposition to learn one common co-association matrix, while our last method constructs all co-association matrices into one third-order tensor to investigate the high-order correlation among multiple views by low-rank and sparse tensor decomposition. We adopt the alternating direction method of multipliers to solve two convex models by dividing them into several subproblems with closed-form solution. Experimental results on ten real-world datasets prove the effectiveness and efficiency of the proposed two multi-view ensemble clustering methods by comparing them with other advanced ensemble clustering methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3589768},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {5},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-view ensemble clustering via low-rank and sparse decomposition: From matrix to tensor},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Review of clustering methods for functional data.
<em>TKDD</em>, <em>17</em>(7), 1–34. (<a
href="https://doi.org/10.1145/3581789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data clustering is to identify heterogeneous morphological patterns in the continuous functions underlying the discrete measurements/observations. Application of functional data clustering has appeared in many publications across various fields of sciences, including but not limited to biology, (bio)chemistry, engineering, environmental science, medical science, psychology, social science, and so on. The phenomenal growth of the application of functional data clustering indicates the urgent need for a systematic approach to develop efficient clustering methods and scalable algorithmic implementations. On the other hand, there is abundant literature on the cluster analysis of time series, trajectory data, spatio-temporal data, and so on, which are all related to functional data. Therefore, an overarching structure of existing functional data clustering methods will enable the cross-pollination of ideas across various research fields. We here conduct a comprehensive review of original clustering methods for functional data. We propose a systematic taxonomy that explores the connections and differences among the existing functional data clustering methods and relates them to the conventional multivariate clustering methods. The structure of the taxonomy is built on three main attributes of a functional data clustering method and therefore is more reliable than existing categorizations. The review aims to bridge the gap between the functional data analysis community and the clustering community and to generate new principles for functional data clustering.},
  archive      = {J_TKDD},
  doi          = {10.1145/3581789},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Review of clustering methods for functional data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-level visual similarity based personalized tourist
attraction recommendation using geo-tagged photos. <em>TKDD</em>,
<em>17</em>(7), 1–18. (<a
href="https://doi.org/10.1145/3582015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geo-tagged photo-based tourist attraction recommendation can discover users’ travel preferences from their taken photos, so as to recommend suitable tourist attractions to them. However, existing visual content-based methods cannot fully exploit the user and tourist attraction information of photos to extract visual features, and do not differentiate the significance of different photos. In this article, we propose multi-level visual similarity-based personalized tourist attraction recommendation using geo-tagged photos (MEAL). MEAL utilizes the visual contents of photos and interaction behavior data to obtain the final embeddings of users and tourist attractions, which are then used to predict the visit probabilities. Specifically, by crossing the user and tourist attraction information of photos, we define four visual similarity levels and introduce a corresponding quintuplet loss to embed the visual contents of photos. In addition, to capture the significance of different photos, we exploit the self-attention mechanism to obtain the visual representations of users and tourist attractions. We conducted experiments on two datasets crawled from Flickr, and the experimental results proved the advantage of this method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3582015},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-level visual similarity based personalized tourist attraction recommendation using geo-tagged photos},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal disentanglement for implicit recommendations with
network information. <em>TKDD</em>, <em>17</em>(7), 1–18. (<a
href="https://doi.org/10.1145/3582435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online user engagement is highly influenced by various machine learning models, such as recommender systems. These systems recommend new items to the user based on the user’s historical interactions. Implicit recommender systems reflect a binary setting showing whether a user interacted (e.g., clicked on) with an item or not. However, the observed clicks may be due to various causes such as user’s interest, item’s popularity, and social influence factors. Traditional recommender systems consider these causes under a unified representation, which may lead to the emergence and amplification of various biases in recommendations. However, recent work indicates that by disentangling the unified representations, one can mitigate bias (e.g., popularity bias) in recommender systems and help improve recommendation performance. Yet, prior work in causal disentanglement in recommendations does not consider a crucial factor, that is, social influence. Social theories such as homophily and social influence provide evidence that a user’s decision can be highly influenced by the user’s social relations. Thus, accounting for the social relations while disentangling leads to less biased recommendations. To this end, we identify three separate causes behind an effect (e.g., clicks): (a) user’s interest, (b) item’s popularity, and (c) user’s social influence. Our approach seeks to causally disentangle the user and item latent features to mitigate popularity bias in implicit feedback–based social recommender systems. To achieve this goal, we draw from causal inference theories and social network theories and propose a causality-aware disentanglement method that leverages both the user–item interaction network and auxiliary social network information. Experiments on real-world datasets against various state-of-the-art baselines validate the effectiveness of the proposed model for mitigating popularity bias and generating de-biased recommendations.},
  archive      = {J_TKDD},
  doi          = {10.1145/3582435},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Causal disentanglement for implicit recommendations with network information},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal scale-free small-world graphs with minimum scaling
of cover time. <em>TKDD</em>, <em>17</em>(7), 1–19. (<a
href="https://doi.org/10.1145/3583691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cover time of random walks on a graph has found wide practical applications in different fields of computer science, such as crawling and searching on the World Wide Web and query processing in sensor networks, with the application effects dependent on the behavior of the cover time: the smaller the cover time, the better the application performance. It was proved that over all graphs with N nodes, complete graphs have the minimum cover time N log N . However, complete graphs cannot mimic real-world networks with small average degree and scale-free small-world properties, for which the cover time has not been examined carefully, and its behavior is still not well understood. In this article, we first experimentally evaluate the cover time for various real-world networks with scale-free small-world properties, which scales as N log N . To better understand the behavior of the cover time for real-world networks, we then study the cover time of three scale-free small-world model networks by using the connection between cover time and resistance diameter. For all the three networks, their cover time also behaves as N log N . This work indicates that sparse networks with scale-free and small-world topology are favorable architectures with optimal scaling of cover time. Our results deepen understanding the behavior of cover time in real-world networks with scale-free small-world structure, and have potential implications in the design of efficient algorithms related to cover time.},
  archive      = {J_TKDD},
  doi          = {10.1145/3583691},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Optimal scale-free small-world graphs with minimum scaling of cover time},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evolving social media background representation with
frequency weights and co-occurrence graphs. <em>TKDD</em>,
<em>17</em>(7), 1–17. (<a
href="https://doi.org/10.1145/3585389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media as a background information source has been utilized in many practical computational tasks, such as stock price prediction, epidemic tracking, and product recommendation. However, proper representation of an evolving social media background is still in an early research stage. In this article, we propose a representation method that considers temporal novelties as well as the fine details of word inter-dependencies. Our method is based on the tf-idf and graph embedding techniques. The proposed method has superiority over other representation methods because it takes the advantage of both the temporal aspect of tf-idf and the semantic aspect of graph embeddings. We compare our method with a variety of baselines in two practical application scenarios using real-world data. In tweet popularity prediction, our representation achieves 5.7% less error and 12.8% higher correlation compared to the best baseline. In e-commerce product recommendation, our representation achieves 17% higher hit-rate and 20% higher NDCG compared to the best baseline.},
  archive      = {J_TKDD},
  doi          = {10.1145/3585389},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Evolving social media background representation with frequency weights and co-occurrence graphs},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DMGF-net: An efficient dynamic multi-graph fusion network
for traffic prediction. <em>TKDD</em>, <em>17</em>(7), 1–19. (<a
href="https://doi.org/10.1145/3586164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is the core task of intelligent transportation system (ITS) and accurate traffic prediction can greatly improve the utilization of public resources. Dynamic interaction of multiple spatial relationships will influence the accuracy of traffic prediction. However, many existing methods only consider static spatial relationships, which restricts the accuracy of the prediction. To address the above problem, in this article, we propose the Dynamic Multi-Graph Fusion Network (DMGF-Net) to model the spatial-temporal correlations in traffic network. In the DMGF-Net, the fusion graph is designed to leverage and extract the various spatial correlations between different regions by fusing spatial graph, semantic graph, and spatial-semantic graph. Further, to dynamically learn the importance of different neighbors, we design the Dynamic Spatial-Temporal Unit (DSTU), which can adjust the aggregation weights of different neighbors by combining the convolution operation and the attention mechanism. It can selectively aggregate spatial-temporal features from different neighbors. Extensive experiments on three datasets demonstrate that effectiveness of our model, especially on PEMS08, our model achieves an increase of about 8.55% and 7.55% in terms of MAE and RMSE than the static model STGCN.},
  archive      = {J_TKDD},
  doi          = {10.1145/3586164},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DMGF-net: An efficient dynamic multi-graph fusion network for traffic prediction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal discovery via causal star graphs. <em>TKDD</em>,
<em>17</em>(7), 1–24. (<a
href="https://doi.org/10.1145/3586997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovering causal relationships among observed variables is an important research focus in data mining. Existing causal discovery approaches are mainly based on constraint-based methods and functional causal models (FCMs). However, the constraint-based method cannot identify the Markov equivalence class and the functional causal models cannot identify the complex interrelationships when multiple variables affect one variable. To address the two aforementioned problems, we propose a new graph structure Causal Star Graph (CSG) and a corresponding framework Causal Discovery via Causal Star Graphs (CD-CSG) to divide a causal directed acyclic graph into multiple CSGs for causal discovery. In this framework, we also propose a generalized learning in CSGs based on a variational approach to learn the representative intermediate variable of CSG’s non-central variables. Through the generalized learning in CSGs, the asymmetry in the forward and backward model of CD-CSG can be found to identify the causal directions in the directed acyclic graphs. We further divide the CSGs into three categories and provide the causal identification principle under each category in our proposed framework. Experiments using synthetic data show that the causal relationships between variables can be effectively identified with CD-CSG and the accuracy of CD-CSG is higher than the best existing model. By applying CD-CSG to real-world data, our proposed method can greatly augment the applicability and effectiveness of causal discovery.},
  archive      = {J_TKDD},
  doi          = {10.1145/3586997},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Causal discovery via causal star graphs},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neighborhood weighted voting-based noise correction for
crowdsourcing. <em>TKDD</em>, <em>17</em>(7), 1–18. (<a
href="https://doi.org/10.1145/3586998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowdsourcing scenarios, we can obtain each instance’s multiple noisy labels set from different crowd workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, a certain level of noise still remains in the integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, however, nearly all existing noise correction algorithms only exploit each instance’s own multiple noisy label sets but ignore the multiple noisy label sets of its neighbors. Here neighbors refer to the nearest instances found in the feature space based on the distance metric learning. In this article, we propose neighborhood weighted voting-based noise correction (NWVNC). In NWVNC, we at first take advantage of the multiple noisy label sets of each instance’s neighbors (including itself) to estimate the probability that it belongs to its integrated label. Then, we use the estimated probability to identify and filter noise instances and thus obtain a clean set and a noise set. Finally, we train three heterogeneous classifiers on the clean set and correct the noise instances by the consensus voting of three trained classifiers. The experimental results on 34 simulated and two real-world crowdsourced datasets show that NWVNC significantly outperforms all the other state-of-the-art noise correction algorithms used for comparison.},
  archive      = {J_TKDD},
  doi          = {10.1145/3586998},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {7},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Neighborhood weighted voting-based noise correction for crowdsourcing},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving personalized fitness recommender system
P3FitRec: A multi-level deep learning approach. <em>TKDD</em>,
<em>17</em>(6), 1–24. (<a
href="https://doi.org/10.1145/3572899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users’ privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this article, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable Internet of Things (IoT) devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data, minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, and weight. Our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared with similar studies. 1 Furthermore, our approach is novel compared with existing studies, as it does not require collecting and using users’ sensitive information. Thus, it preserves the users’ privacy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3572899},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Privacy-preserving personalized fitness recommender system P3FitRec: A multi-level deep learning approach},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge-enhanced global disentangled graph neural network for
sequential recommendation. <em>TKDD</em>, <em>17</em>(6), 1–22. (<a
href="https://doi.org/10.1145/3577928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation has been a widely popular topic of recommender systems. Existing works have contributed to enhancing the prediction ability of sequential recommendation systems based on various methods, such as recurrent networks and self-attention mechanisms. However, they fail to discover and distinguish various relationships between items, which could be underlying factors which motivate user behaviors. In this article, we propose an Edge-Enhanced Global Disentangled Graph Neural Network (EGD-GNN) model to capture the relation information between items for global item representation and local user intention learning. At the global level, we build a global-link graph over all sequences to model item relationships. Then a channel-aware disentangled learning layer is designed to decompose edge information into different channels, which can be aggregated to represent the target item from its neighbors. At the local level, we apply a variational auto-encoder framework to learn user intention over the current sequence. We evaluate our proposed method on three real-world datasets. Experimental results show that our model can get a crucial improvement over state-of-the-art baselines and is able to distinguish item features.},
  archive      = {J_TKDD},
  doi          = {10.1145/3577928},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Edge-enhanced global disentangled graph neural network for sequential recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bavarian: Betweenness centrality approximation with
variance-aware rademacher averages. <em>TKDD</em>, <em>17</em>(6), 1–47.
(<a href="https://doi.org/10.1145/3577021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“[A]llain Gersten, Hopfen, und Wasser” — 1516 Reinheitsgebot We present Bavarian , a collection of sampling-based algorithms for approximating the Betweenness Centrality (BC) of all vertices in a graph. Our algorithms use Monte-Carlo Empirical Rademacher Averages (MCERAs), a concept from statistical learning theory, to efficiently compute tight bounds on the maximum deviation of the estimates from the exact values. The MCERAs provide a sample-dependent approximation guarantee much stronger than the state-of-the-art, thanks to its use of variance-aware probabilistic tail bounds. The flexibility of the MCERAs allows us to introduce a unifying framework that can be instantiated with existing sampling-based estimators of BC, thus allowing a fair comparison between them, decoupled from the sample-complexity results with which they were originally introduced. Additionally, we prove novel sample-complexity results showing that, for all estimators, the sample size sufficient to achieve a desired approximation guarantee depends on the vertex-diameter of the graph, an easy-to-bound characteristic quantity. We also show progressive-sampling algorithms and extensions to other centrality measures, such as percolation centrality. Our extensive experimental evaluation of Bavarian shows the improvement over the state-of-the-art made possible by the MCERAs (2–4× reduction in the error bound), and it allows us to assess the different trade-offs between sample size and accuracy guarantees offered by the different estimators.},
  archive      = {J_TKDD},
  doi          = {10.1145/3577021},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {6},
  pages        = {1-47},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Bavarian: Betweenness centrality approximation with variance-aware rademacher averages},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and effective academic expert finding on
heterogeneous graphs through (k, 𝒫)-core based embedding. <em>TKDD</em>,
<em>17</em>(6), 1–35. (<a
href="https://doi.org/10.1145/3578365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expert finding is crucial for a wealth of applications in both academia and industry. Given a user query and trove of academic papers, expert finding aims at retrieving the most relevant experts for the query, from the academic papers. Existing studies focus on embedding-based solutions that consider academic papers’ textual semantic similarities to a query via document representation and extract the top- n experts from the most similar papers. Beyond implicit textual semantics, however, papers’ explicit relationships (e.g., co-authorship) in a heterogeneous graph (e.g., DBLP) are critical for expert finding, because they help improve the representation quality. Despite their importance, the explicit relationships of papers generally have been ignored in the literature. In this article, we study expert finding on heterogeneous graphs by considering both the explicit relationships and implicit textual semantics of papers in one model. Specifically, we define the cohesive ( k , 𝒫)-core community of papers w.r.t. a meta-path 𝒫 (i.e., relationship) and propose a ( k , 𝒫)-core based document embedding model to enhance the representation quality. Based on this, we design a proximity graph-based index (PG-Index) of papers and present a threshold algorithm (TA)-based method to efficiently extract top- n experts from papers returned by PG-Index. We further optimize our approach in two ways: (1) we boost effectiveness by considering the ( k , 𝒫)-core community of experts and the diversity of experts’ research interests, to achieve high-quality expert representation from paper representation; and (2) we streamline expert finding, going from “extract top- n experts from top- m ( m&gt; n ) semantically similar papers” to “directly return top- n experts”. The process of returning a large number of top- m papers as intermediate data is avoided, thereby improving the efficiency. Extensive experiments using real-world datasets demonstrate our approach’s superiority.},
  archive      = {J_TKDD},
  doi          = {10.1145/3578365},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {6},
  pages        = {1-35},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Efficient and effective academic expert finding on heterogeneous graphs through (k, 𝒫)-core based embedding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GEO: A computational design framework for automotive
exterior facelift. <em>TKDD</em>, <em>17</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3578521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exterior facelift has become an effective method for automakers to boost the consumers’ interest in an existing car model before it is redesigned. To support the automotive facelift design process, this study develops a novel computational framework – Generator, Evaluator, Optimiser (GEO) , which comprises three components: a StyleGAN2-based design generator that creates different facelift designs; a convolutional neural network (CNN) -based evaluator that assesses designs from the aesthetics perspective; and a recurrent neural network (RNN) -based decision optimiser that selects designs to maximise the predicted profit of the targeted car model over time. We validate the GEO framework in experiments with real-world datasets and describe some resulting managerial implications for automotive facelift. Our study makes both methodological and application contributions. First, the generator’s mapping network and projection methods are carefully tailored to facelift where only minor changes are performed without affecting the family signature of the automobile brands. Second, two evaluation metrics are proposed to assess the generated designs. Third, profit maximisation is taken into account in the design selection. From a high-level perspective, our study contributes to the recent use of machine learning and data mining in marketing and design studies. To the best of our knowledge, this is the first study that uses deep generative models for automotive regional design upgrading and that provides an end-to-end decision-support solution for automakers and designers.},
  archive      = {J_TKDD},
  doi          = {10.1145/3578521},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {GEO: A computational design framework for automotive exterior facelift},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Summarizing user-item matrix by group utility maximization.
<em>TKDD</em>, <em>17</em>(6), 1–22. (<a
href="https://doi.org/10.1145/3578586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A user-item utility matrix represents the utility (or preference) associated with each (user, item) pair, such as citation counts, rating/vote on items or locations, and clicks on items. A high utility value indicates a strong association of the pair. In this work, we consider the problem of summarizing strong association for a large user-item matrix using a small summary size. Traditional techniques fail to distinguish user groups associated with different items (such as top- l item selection) or fail to focus on high utility (such as similarity- based subspace clustering and biclustering). We formulate a new problem, called Group Utility Maximization (GUM), to summarize the entire user population through k user groups and l items for each group; the goal is to maximize the total utility of selected items over all groups collectively. We show this problem is NP-hard even for l =1. We present two algorithms. One greedily finds the next group, called Greedy algorithm, and the other iteratively refines existing k groups, called k -max algorithm. Greedy algorithm provides the \((1-\frac{1}{e})\) approximation guarantee for a nonnegative utility matrix, whereas k -max algorithm is more efficient for large datasets. We evaluate these algorithms on real-life datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3578586},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {6},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Summarizing user-item matrix by group utility maximization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Misinformation blocking problem in virtual and real
interconversion social networks. <em>TKDD</em>, <em>17</em>(6), 1–25.
(<a href="https://doi.org/10.1145/3578936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the in-depth development of intelligent media technology, online and offline fusion, reality and virtual entanglement, information content generalization, the boundary between positive and negative information is blurred, all kinds of misinformation in the social network fission spread, and cyberspace governance has become a global consensus. In this article, we comprehensively consider the spread of misinformation in location-based interpersonal social network and online social network, and systematically tackle the novel problem of minimizing the influence of misinformation under individual protection strategies. We first analyze the complexity and modularity of the problem. Then, we leverage the Lovász extension to devise a nonsubmodular set function continuity approximate convex relaxation method, and develop an approximate projected subgradient procedure to obtain a solution with a factor approximate guarantee. Finally, experiments on three assembled real-world datasets demonstrate the effectiveness and feasibility of our designed method and developed the algorithm.},
  archive      = {J_TKDD},
  doi          = {10.1145/3578936},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {6},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Misinformation blocking problem in virtual and real interconversion social networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Slack-factor-based fuzzy support vector machine for class
imbalance problems. <em>TKDD</em>, <em>17</em>(6), 1–26. (<a
href="https://doi.org/10.1145/3579050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance and noisy data widely exist in real-world problems, and the support vector machine (SVM) is hard to construct good classifiers on these data. Fuzzy SVMs (FSVMs), as variants of SVM, use a fuzzy membership function both to reflect the samples’ importance and to remove the impact of noises, and employ cost-sensitive technology to address the class imbalance. They can handle the noise and class imbalance problems in many cases; however, the fuzzy membership functions are often affected by the class imbalance data, leading to inaccurate measures for samples’ performance and affecting the performance of FSVMs. To solve this problem, we design a new fuzzy membership function and combine it with cost-sensitive learning to deal with the class imbalance problem with noisy data, named Slack-Factor-based FSVM (SFFSVM). In SFFSVM, the relative distances between samples and an estimated hyperplane, called slack factors, are used to define the fuzzy membership function. To eliminate the impact of class imbalance on the function and gain more accurate samples’ importance, we rectify the importance according to the positional relationship between the estimated hyperplane and the optimal hyperplane of the problem, and the slack factors of samples. Comprehensive experiments on artificial and real-world datasets demonstrate that SFFSVM outperforms other comparative methods on F1, MCC, and AUC-PR metrics.},
  archive      = {J_TKDD},
  doi          = {10.1145/3579050},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Slack-factor-based fuzzy support vector machine for class imbalance problems},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three-way preference completion via preference graph.
<em>TKDD</em>, <em>17</em>(6), 1–19. (<a
href="https://doi.org/10.1145/3580368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the personal partial rankings from agents over a subset of alternatives, the goal of preference completion is to infer the agent’s personalized preference over all alternatives including those the agent has not yet handled from uncertain preference of third parties. By combining the partial rankings of the target agent and the partial rankings from third parties to settle some disagreement with three-way preference completion, which includes a general strategy, an optimal strategy, and a pessimistic strategy, it forms the weighted preference graph. Technically, to settle the disagreement and obtain the completed preference of the target agent in the weighted preference graph, maximum likelihood estimation (MLE) under Mallows is proposed and validated theoretically by removing edges with the minimum weight in the weighted preference graph. However, it is not easy to locate the edges with the minimum weight efficiently in a big graph. Hence, an optimal MLE algorithm and three greedy MLE algorithms are proposed to process the MLE. Furthermore, these proposed algorithms are experimentally validated and compared with each other by both the synthetic dataset and the Flixter dataset.},
  archive      = {J_TKDD},
  doi          = {10.1145/3580368},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {6},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Three-way preference completion via preference graph},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Triadic closure sensitive influence maximization.
<em>TKDD</em>, <em>17</em>(6), 1–26. (<a
href="https://doi.org/10.1145/3573011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The influence are not linked to any footnote in the text. Please check and suggest. maximization problem aims at selecting the k most influential nodes (i.e., seed nodes) from a social network, where the nodes can maximize the number of influenced nodes activated by a certain propagation model. However, the widely used Independent Cascade model shares the same propagation probability among substantial adjacent node pairs, which is too idealistic and unreasonable in practice. In addition, most heuristic algorithms for influence maximization need to update the expected influence of the remaining nodes in the seed selection process, resulting in high computation cost. To address these non-trivial problems, we propose a novel edge propagation probability calculation method. The method first utilizes the triadic closure structure of social networks to precisely measure the closeness between nodes and assigns different propagation probabilities to each edge, deriving a Triadic Closure-based Independent Cascade (TC-IC) model. Then, we further propose a heuristic influence maximization algorithm named Triadic Closure-based Influence Maximization (TC-IM). The algorithm evaluates the expected influence of a node by integrating the triadic closure weighted propagation probability and the triadic closure weighted degree. Especially, in the seed selection process, only the most influential node that has not been updated in the current round needs to be updated, which significantly improves the efficiency. Besides, we further provide theoretical proofs to guarantee the correctness of this updating strategy. Experimental results on nine real datasets and three propagation models demonstrate that: (1) The TC-IC model can set a proper propagation probability for each node pair, where the IM algorithms could easily identify influential nodes; (2) The TC-IM algorithm can significantly reduce the complexity through an efficient updating strategy with a comparable influence spread to the approximation IM algorithms; (3) Besides, the TC-IM algorithm also exhibits stable performance under other IC models including UIC and WIC, exhibiting good stability and generality.},
  archive      = {J_TKDD},
  doi          = {10.1145/3573011},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Triadic closure sensitive influence maximization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical dense pattern detection in tensors.
<em>TKDD</em>, <em>17</em>(6), 1–29. (<a
href="https://doi.org/10.1145/3577022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense subtensor detection gains remarkable success in spotting anomalies and fraudulent behaviors for multi-aspect data (i.e., tensors), like in social media and event streams. Existing methods detect the densest subtensors flatly and separately, with the underlying assumption that those subtensors are exclusive. However, many real-world tensors usually present hierarchical properties, e.g., the core-periphery structure and dynamic communities in networks. It is also unexplored how to fuse the prior knowledge into dense pattern detection to capture the local behavior. In this article, we propose CatchCore , a novel framework to efficiently find the hierarchical dense subtensors. We first design a unified metric for dense subtensor detection, which can be optimized with gradient-based methods. With the proposed metric, CatchCore detects hierarchical dense subtensors through the hierarchy-wise alternative optimization and finds local dense patterns concerning some items in a query manner. Finally, we utilize the minimum description length principle to measure the quality of detection results and select the optimal hierarchical dense subtensors. Extensive experiments on synthetic and real-world datasets demonstrate that CatchCore outperforms the top competitors in accuracy for detecting dense subtensors and anomaly patterns, like network attacks. Additionally, CatchCore successfully identifies a hierarchical researcher co-authorship group with intense interactions in the DBLP dataset; it can also capture core collaboration and multi-hop relations around some query objects. Meanwhile, CatchCore also scales linearly with all aspects of tensors.},
  archive      = {J_TKDD},
  doi          = {10.1145/3577022},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {6},
  pages        = {1-29},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Hierarchical dense pattern detection in tensors},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness of information flow in social networks.
<em>TKDD</em>, <em>17</em>(6), 1–26. (<a
href="https://doi.org/10.1145/3578268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks form a major parts of people’s lives, and individuals often make important life decisions based on information that spreads through these networks. For this reason, it is important to know whether individuals from different protected groups have equal access to information flowing through a network. In this article, we define the Information Unfairness (IUF) metric, which quantifies inequality in access to information across protected groups. We then introduce MinIUF , an algorithm for reducing inequalities in information flow by adding edges to the network. Finally, we provide an in-depth analysis of information flow with respect to an attribute of interest, such as gender, across different types of networks to evaluate whether the structure of these networks allows groups to equally access information flowing in the network. Moreover, we investigate the causes of unfairness in such networks and how it can be improved.},
  archive      = {J_TKDD},
  doi          = {10.1145/3578268},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {6},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Fairness of information flow in social networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonnegative matrix factorization based on node centrality
for community detection. <em>TKDD</em>, <em>17</em>(6), 1–21. (<a
href="https://doi.org/10.1145/3578520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection is an important topic in network analysis, and recently many community detection methods have been developed on top of the Nonnegative Matrix Factorization (NMF) technique. Most NMF-based community detection methods only utilize the first-order proximity information in the adjacency matrix, which has some limitations. Besides, many NMF-based community detection methods involve sparse regularizations to promote clearer community memberships. However, in most of these regularizations, different nodes are treated equally, which seems unreasonable. To dismiss the above limitations, this article proposes a community detection method based on node centrality under the framework of NMF. Specifically, we design a new similarity measure which considers the proximity of higher-order neighbors to form a more informative graph regularization mechanism, so as to better refine the detected communities. Besides, we introduce the node centrality and Gini impurity to measure the importance of nodes and sparseness of the community memberships, respectively. Then, we propose a novel sparse regularization mechanism which forces nodes with higher node centrality to have smaller Gini impurity. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over thirteen state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3578520},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {6},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Nonnegative matrix factorization based on node centrality for community detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-stage machine learning model for hierarchical tie
valence prediction. <em>TKDD</em>, <em>17</em>(6), 1–20. (<a
href="https://doi.org/10.1145/3579096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals interacting in organizational settings involving varying levels of formal hierarchy naturally form a complex network of social ties having different tie valences (e.g., positive and negative connections). Social ties critically affect employees’ satisfaction, behaviors, cognition, and outcomes—yet identifying them solely through survey data is challenging because of the large size of some organizations or the often hidden nature of these ties and their valences. We present a novel deep learning model encompassing NLP and graph neural network techniques that identifies positive and negative ties in a hierarchical network. The proposed model uses human resource attributes as node information and web-logged work conversation data as link information. Our findings suggest that the presence of conversation data improves the tie valence classification by 8.91% compared to employing user attributes alone. This gain came from accurately distinguishing positive ties, particularly for male, non-minority, and older employee groups. We also show a substantial difference in conversation patterns for positive and negative ties with positive ties being associated with more messages exchanged on weekends, and lower use of words related to anger and sadness. These findings have broad implications for facilitating collaboration and managing conflict within organizational and other social networks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3579096},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {6},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-stage machine learning model for hierarchical tie valence prediction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private release of heterogeneous network for
managing healthcare data. <em>TKDD</em>, <em>17</em>(6), 1–30. (<a
href="https://doi.org/10.1145/3580367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing adoption of digital health platforms through mobile apps and online services, people have greater flexibility connecting with medical practitioners, pharmacists, and laboratories and accessing resources to manage their own health-related concerns. Many healthcare institutions are connecting with each other to facilitate the exchange of healthcare data, with the goal of effective healthcare data management. The contents generated over these platforms are often shared with third parties for a variety of purposes. However, sharing healthcare data comes with the potential risk of exposing patients’ sensitive information to privacy threats. In this article, we address the challenge of sharing healthcare data while protecting patients’ privacy. We first model a complex healthcare dataset using a heterogeneous information network that consists of multi-type entities and their relationships. We then propose DiffHetNet , an edge-based differentially private algorithm, to protect the sensitive links of patients from inbound and outbound attacks in the heterogeneous health network. We evaluate the performance of our proposed method in terms of information utility and efficiency on different types of real-life datasets that can be modeled as networks. Experimental results suggest that DiffHetNet generally yields less information loss and is significantly more efficient in terms of runtime in comparison with existing network anonymization methods. Furthermore, DiffHetNet is scalable to large network datasets.},
  archive      = {J_TKDD},
  doi          = {10.1145/3580367},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {6},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Differentially private release of heterogeneous network for managing healthcare data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypergraph transformer neural networks. <em>TKDD</em>,
<em>17</em>(5), 1–22. (<a
href="https://doi.org/10.1145/3565028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been widely used for graph structure learning and achieved excellent performance in tasks such as node classification and link prediction. Real-world graph networks imply complex and various semantic information and are often referred to as heterogeneous information networks (HINs). Previous GNNs have laboriously modeled heterogeneous graph networks with pairwise relations, in which the semantic information representation for learning is incomplete and severely hinders node embedded learning. Therefore, the conventional graph structure cannot satisfy the demand for information discovery in HINs. In this article, we propose an end-to-end hypergraph transformer neural network (HGTN) that exploits the communication abilities between different types of nodes and hyperedges to learn higher-order relations and discover semantic information. Specifically, attention mechanisms weigh the importance of semantic information hidden in original HINs to generate useful meta-paths. Meanwhile, our method develops a multi-scale attention module to aggregate node embeddings in higher-order neighborhoods. We evaluate the proposed model with node classification tasks on six datasets: DBLP, ACM, IBDM, Reuters, STUD-BJUT, and Citeseer. Experiments on a large number of benchmarks show the advantages of HGTN.},
  archive      = {J_TKDD},
  doi          = {10.1145/3565028},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {5},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Hypergraph transformer neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective and scalable manifold ranking-based image
retrieval with output bound. <em>TKDD</em>, <em>17</em>(5), 1–31. (<a
href="https://doi.org/10.1145/3565574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image retrieval keeps attracting a lot of attention from both academic and industry over past years due to its variety of useful applications. Due to the rapid growth of deep learning approaches, more better feature vectors of images could be discovered for improving image retrieval. However, most (if not all) existing deep learning approaches consider the similarity between two images locally without considering the similarity among a group of similar images globally , and thus could not return accurate results. In this article, we study the image retrieval with manifold ranking (MR) which considers both the local similarity and the global similarity, which could give more accurate results. However, existing best-known algorithms have one of the following issues: (1) they require to build a bulky index, (2) some of them do not have any theoretical bound on the output, and (3) some of them are time-consuming. Motivated by this, we propose two algorithms, namely Monte Carlo-based MR ( MCMR ) and MCMR+ , for image retrieval, which do not have the above issues. We are the first one to propose an index-free manifold ranking image retrieval with the output theoretical bound. More importantly, our algorithms give the first best-known time complexity result of \(O(n \log n)\) where \(n\) is the total number of images in the database compared with the existing best-known result of \(O(n^2)\) in the literature of computing the exact top- \(k\) results with quality guarantee. Lastly, our experimental result shows that MCMR+ outperforms existing algorithms by up to four orders of magnitude in terms of query time.},
  archive      = {J_TKDD},
  doi          = {10.1145/3565574},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {5},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Effective and scalable manifold ranking-based image retrieval with output bound},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ada-MIP: Adaptive self-supervised graph representation
learning via mutual information and proximity optimization.
<em>TKDD</em>, <em>17</em>(5), 1–23. (<a
href="https://doi.org/10.1145/3568165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised graph-level representation learning has recently received considerable attention. Given varied input distributions, jointly learning graphs’ unique and common features is vital to downstream tasks. Inspired by graph contrastive learning (GCL), which targets maximizing the agreement between graph representations from different views, we propose an Ada ptive self-supervised framework, Ada-MIP, considering both M utual I nformation between views (unique features) and inter-graph P roximity (common features). Specifically, Ada-MIP learns graphs’ unique information through a learnable and probably injective augmenter, which can acquire more adaptive views compared to the augmentation strategies applied by existing GCL methods; to learn graphs’ common information, we employ graph kernels to calculate graphs’ proximity and learn graph representations among which the precomputed proximity is preserved. By sharing a global encoder, graphs’ unique and common information can be well integrated into the graph representations learned by Ada-MIP. Ada-MIP is also extendable to semi-supervised scenarios, with our experiments confirming its superior performance in both unsupervised and semi-supervised tasks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3568165},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {5},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Ada-MIP: Adaptive self-supervised graph representation learning via mutual information and proximity optimization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto-STGCN: Autonomous spatial-temporal graph convolutional
network search. <em>TKDD</em>, <em>17</em>(5), 1–21. (<a
href="https://doi.org/10.1145/3571285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many spatial-temporal graph convolutional network (STGCN) models are proposed to deal with the spatial-temporal network data forecasting problem. These STGCN models have their own advantages, i.e., each of them puts forward many effective operations and achieves good prediction results in the real applications. If users can effectively utilize and combine these excellent operations integrating the advantages of existing models, then they may obtain more effective STGCN models thus create greater value using existing work. However, they fail to do so due to the lack of domain knowledge, and there is lack of automated system to help users to achieve this goal. In this article, we fill this gap and propose Auto-STGCN algorithm, which makes use of existing models to automatically explore high-performance STGCN model for specific scenarios. Specifically, we design Unified-STGCN framework, which summarizes the operations of existing architectures, and use parameters to control the usage and characteristic attributes of each operation, so as to realize the parameterized representation of the STGCN architecture and the reorganization and fusion of advantages. Then, we present Auto-STGCN, an optimization method based on reinforcement learning, to quickly search the parameter search space provided by Unified-STGCN, and generate optimal STGCN models automatically. Extensive experiments on real-world benchmark datasets show that our Auto-STGCN can find STGCN models superior to existing STGCN models used for search space construction, which demonstrates the effectiveness of our proposed method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3571285},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Auto-STGCN: Autonomous spatial-temporal graph convolutional network search},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic flow forecasting in the COVID-19: A deep
spatial-temporal model based on discrete wavelet transformation.
<em>TKDD</em>, <em>17</em>(5), 1–28. (<a
href="https://doi.org/10.1145/3564753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow prediction has always been the focus of research in the field of Intelligent Transportation Systems, which is conducive to the more reasonable allocation of basic transportation resources and formulation of transportation policies. The spread of COVID-19 has seriously affected the normal order in the transportation sector. With the increase in the number of infected people and the government&#39;s anti-epidemic policy, human outgoing activities have gradually decreased, resulting in increasingly obvious discreteness and irregularities in traffic flow data. This article proposes a deep-space time traffic flow prediction model based on discrete wavelet transform (DSTM-DWT) to overcome the highly discrete and irregular nature of the new crown epidemic. First, DSTM-DWT decomposes traffic flow into discrete attributes, such as flow trend, discrete amplitude, and discrete baseline. Second, we design the spatial relationship of the transportation network as a graph and integrate the new crown pneumonia epidemic data into the characteristics of each transportation node. Then, we use the graph convolutional network to calculate the spatial correlation of each node, and the temporal convolutional network to calculate the temporal correlation of the data. In order to solve the problem of high discreteness of traffic flow data during the epidemic, this article proposes a graph memory network (GMN), which is used to convert discrete magnitudes separated by discrete wavelet transform into high-dimensional discrete features. Finally, use DWT to segment the predicted traffic data, and then perform the inverse discrete wavelet transform between the newly segmented traffic trend and discrete baseline and the discrete model predicted by GMN to obtain the final traffic flow prediction result. In simulation experiments, this work was compared with the existing advanced baselines to verify the superiority of DSTM-DWT.},
  archive      = {J_TKDD},
  doi          = {10.1145/3564753},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Traffic flow forecasting in the COVID-19: A deep spatial-temporal model based on discrete wavelet transformation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterizing and forecasting urban vibrancy evolution: A
multi-view graph mining perspective. <em>TKDD</em>, <em>17</em>(5),
1–24. (<a href="https://doi.org/10.1145/3568683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban vibrancy describes the prosperity, diversity, and accessibility of urban areas, which is vital to a city’s socio-economic development and sustainability. While many efforts have been made for statically measuring and evaluating urban vibrancy, there are few studies on the evolutionary process of urban vibrancy, yet we know little about the relationship between urban vibrancy evolution and sophisticated spatiotemporal dynamics. In this article, we make use of multi-sourced urban data to develop a data-driven framework, U-Evolve , to investigate urban vibrancy evolution. Specifically, we first exploit the spatiotemporal characteristics of urban areas to create multi-view time-dependent graphs. Then, we analyze the contextual features and graph patterns of multi-view time-dependent graphs in terms of informing future urban vibrancy variations. Our analysis validates the informativeness of multi-view time-dependent graphs for characterizing and informing future urban vibrancy evolution. After that, we construct a feature based model to forecast future urban vibrancy evolution and quantify each feature’s importance. Moreover, to further enhance the forecasting effectiveness, we propose a graph learning based model to capture spatiotemporal autocorrelation of urban areas based on multi-view time-dependent graphs in an end-to-end manner. Finally, extensive experiments on two metropolises, Beijing and Shanghai, demonstrate the effectiveness of our forecasting models. The U-Evolve framework has also been deployed in the production environment to deliver real-world urban development and planning insights for various cities in China.},
  archive      = {J_TKDD},
  doi          = {10.1145/3568683},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Characterizing and forecasting urban vibrancy evolution: A multi-view graph mining perspective},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ONION: Joint unsupervised feature selection and robust
subspace extraction for graph-based multi-view clustering.
<em>TKDD</em>, <em>17</em>(5), 1–23. (<a
href="https://doi.org/10.1145/3568684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based Multi-View Clustering (GMVC) has received extensive attention due to its ability to capture the neighborhood relationship among data points from diverse views. However, most existing approaches construct similarity graphs from the original multi-view data, the accuracy of which heavily and implicitly relies on the quality of the original multiple features. Moreover, previous methods either focus on mining the multi-view commonality or emphasize on exploring the multi-view individuality, making the rich information contained in multiple features cannot be effectively exploited. In this work, we design a novel GMVC framework via c O mmo N ality and I ndividuality disc O vering in late N t subspace ( ONION ), seeking for a robust and discriminative subspace representation compatible across multiple features for GMVC. To be specific, our method simultaneously formulates the unsupervised sparse feature selection and the robust subspace extraction, as well as the target graph learning in a unified optimization model, which can help the learning of the discriminative subspace representation and the target graph in a mutual reinforcement manner. Meanwhile, we manipulate the target graph by an explicit structural penalty, rendering the connected components in the graph directly reveal clusters. Experimental results on seven benchmark datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3568684},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {ONION: Joint unsupervised feature selection and robust subspace extraction for graph-based multi-view clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A weighted ensemble classification algorithm based on
nearest neighbors for multi-label data stream. <em>TKDD</em>,
<em>17</em>(5), 1–21. (<a
href="https://doi.org/10.1145/3570960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of data stream, multi-label algorithms for mining dynamic data become more and more important. At the same time, when data distribution changes, concept drift will occur, which will make the existing classification models lose effectiveness. Ensemble methods have been used for multi-label classification, but few methods consider both the accuracy and diversity of base classifiers. To address the above-mentioned problem, a Weighted Ensemble classification algorithm based on Nearest Neighbors for Multi-Label data stream (WENNML) is proposed. WENNML uses data blocks to train Active candidate Ensemble Classifiers (AEC) and Passive candidate Ensemble Classifiers (PEC). The base classifiers of AEC and PEC are dynamically updated using geometric and diversity weighting methods. When the difference value between the number of current instances and the number of warning instances reaches the passive warning value, the algorithm selects the optimal base classifiers from AEC and PEC according to the subset accuracy and hamming score and puts them into the predictive ensemble classifiers. Experiments are carried out on 12 kinds of datasets with 9 comparison algorithms. The results show that WENNML achieves the best average rankings among the four evaluation metrics.},
  archive      = {J_TKDD},
  doi          = {10.1145/3570960},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {5},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A weighted ensemble classification algorithm based on nearest neighbors for multi-label data stream},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-paced adaptive bipartite graph learning for consensus
clustering. <em>TKDD</em>, <em>17</em>(5), 1–35. (<a
href="https://doi.org/10.1145/3564701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consensus clustering provides an elegant framework to aggregate multiple weak clustering results to learn a consensus one that is more robust and stable than a single result. However, most of the existing methods usually use all data for consensus learning, whereas ignoring the side effects caused by some unreliable or difficult data. To address this issue, in this article, we propose a novel self-paced consensus clustering method with adaptive bipartite graph learning to gradually involve data from more reliable to less reliable ones in consensus learning. At first, we construct an initial bipartite graph from the base results, where the nodes represent the clusters and instances, and the edges indicate that an instance belongs to a cluster. Then, we adaptively learn a structured bipartite graph from this initial one by self-paced learning, i.e., we automatically determine the reliability of each edge with adaptive cluster similarity measuring and involve the edges in bipartite graph learning in order of their reliability. At last, we obtain the final consensus result from the learned structured bipartite graph. We conduct extensive experiments on both toy and benchmark datasets, and the results show the effectiveness and superiority of our method. The codes of this article are released in http://Doctor-Nobody.github.io/codes/code_SCCABG.zip.},
  archive      = {J_TKDD},
  doi          = {10.1145/3564701},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-35},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Self-paced adaptive bipartite graph learning for consensus clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowdsourcing truth inference via reliability-driven
multi-view graph embedding. <em>TKDD</em>, <em>17</em>(5), 1–26. (<a
href="https://doi.org/10.1145/3565576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing truth inference aims to assign a correct answer to each task from candidate answers that are provided by crowdsourced workers. A common approach is to generate workers’ reliabilities to represent the quality of answers. Although crowdsourced triples can be converted into various crowdsourced relationships, the available related methods are not effective in capturing these relationships to alleviate the harm to inference that is caused by conflicting answers. In this research, we propose a Re liability-driven M ulti-view G raph E mbedding framework for T ruth i nference (TiReMGE), which explores multiple crowdsourced relationships by organically integrating worker reliabilities into a graph space that is constructed from crowdsourced triples. Specifically, to create an interactive environment, we propose a reliability-driven initialization criterion for initializing vectors of tasks and workers as interactive carriers of reliabilities. From the perspective of multiple crowdsourced relationships, a multi-view graph embedding framework is proposed for reliability information interaction on a task-worker graph, which encodes latent crowdsourced relationships into vectors of workers and tasks for reliability update and truth inference. A heritable reliability updating method based on the Lagrange multiplier method is proposed to obtain reliabilities that match the quality of workers for interaction by a novel constraint law. Our ultimate goal is to minimize the Euclidean distance between the encoded task vector and the answer that is provided by a worker with high reliability. Extensive experimental results on nine real-world datasets demonstrate that TiReMGE significantly outperforms the nine state-of-the-art baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3565576},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Crowdsourcing truth inference via reliability-driven multi-view graph embedding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncovering the local hidden community structure in social
networks. <em>TKDD</em>, <em>17</em>(5), 1–25. (<a
href="https://doi.org/10.1145/3567597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden community is a useful concept proposed recently for social network analysis. Hidden communities indicate some weak communities whose most members also belong to other stronger dominant communities. Dominant communities could form a layer that partitions all the individuals of a network, and hidden communities could form other layer(s) underneath. These layers could be natural structures in the real-world networks like students grouped by major, minor, hometown, and so on. To handle the rapid growth of network scale, in this work, we explore the detection of hidden communities from the local perspective, and propose a new method that detects and boosts each layer iteratively on a subgraph sampled from the original network. We first expand the seed set from a single seed node based on our modified local spectral method and detect an initial dominant local community. Then we temporarily remove the members of this community as well as their connections to other nodes, and detect all the neighborhood communities in the remaining subgraph, including some “broken communities” that only contain a fraction of members in the original network. The local community and neighborhood communities form a dominant layer, and by reducing the edge weights inside these communities, we weaken this layer’s structure to reveal the hidden layers. Eventually, we repeat the whole process, and all communities containing the seed node can be detected and boosted iteratively. We theoretically show that our method can avoid some situations that a broken community and the local community are regarded as one community in the subgraph, leading to the inaccuracy of detection which can be caused by global hidden community detection methods. Extensive experiments show that our method could significantly outperform the state-of-the-art baselines designed for either global hidden community detection or multiple local community detection.},
  archive      = {J_TKDD},
  doi          = {10.1145/3567597},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Uncovering the local hidden community structure in social networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Static and streaming tucker decomposition for dense tensors.
<em>TKDD</em>, <em>17</em>(5), 1–34. (<a
href="https://doi.org/10.1145/3568682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a dense tensor, how can we efficiently discover hidden relations and patterns in static and online streaming settings? Tucker decomposition is a fundamental tool to analyze multidimensional arrays in the form of tensors. However, existing Tucker decomposition methods in both static and online streaming settings have limitations of efficiency since they directly deal with large dense tensors for the result of Tucker decomposition. In a static setting, although few static methods have tried to reduce their time cost by sampling tensors, sketching tensors, and efficient matrix operations, there remains a need for an efficient method. Moreover, streaming versions of Tucker decomposition are still time-consuming to deal with newly arrived tensors. We propose D-Tucker and D-TuckerO, efficient Tucker decomposition methods for large dense tensors in static and online streaming settings, respectively. By decomposing a given large dense tensor with randomized singular value decomposition, avoiding the reconstruction from SVD results, and carefully determining the order of operations, D-Tucker and D-TuckerO efficiently obtain factor matrices and core tensor. Experimental results show that D-Tucker achieves up to 38.4 × faster running times, and requires up to 17.2 × less space than existing methods while having similar accuracy. Furthermore, D-TuckerO is up to 6.1× faster than existing streaming methods for each newly arrived tensor while its running time is proportional to the size of the newly arrived tensor, not the accumulated tensor.},
  archive      = {J_TKDD},
  doi          = {10.1145/3568682},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-34},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Static and streaming tucker decomposition for dense tensors},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised sentiment classification and emotion
distribution learning across domains. <em>TKDD</em>, <em>17</em>(5),
1–30. (<a href="https://doi.org/10.1145/3571736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, sentiment classification and emotion distribution learning across domains are both formulated as a semi-supervised domain adaptation problem, which utilizes a small amount of labeled documents in the target domain for model training. By introducing a shared matrix that captures the stable association between document clusters and word clusters, non-negative matrix tri-factorization (NMTF) is robust to the labeled target domain data and has shown remarkable performance in cross-domain text classification. However, the existing NMTF-based models ignore the incompatible relationship of sentiment polarities and the relatedness among emotions. Besides, their applications on large-scale datasets are limited by the high computation complexity. To address these issues, we propose a semi-supervised NMTF framework for sentiment classification and emotion distribution learning across domains. Based on a many-to-many mapping between document clusters and sentiment polarities (or emotions), we first incorporate the prior information of label dependency to improve the model performance. Then, we develop a parallel algorithm based on message passing interface (MPI) to further enhance the model scalability. Extensive experiments on real-world datasets validate the effectiveness of our method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3571736},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Semi-supervised sentiment classification and emotion distribution learning across domains},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STAD-GAN: Unsupervised anomaly detection on multivariate
time series with self-training generative adversarial networks.
<em>TKDD</em>, <em>17</em>(5), 1–18. (<a
href="https://doi.org/10.1145/3572780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection on multivariate time series (MTS) is an important research topic in data mining, which has a wide range of applications in information technology, financial management, manufacturing system, and so on. However, the state-of-the-art unsupervised deep learning models for MTS anomaly detection are vulnerable to noise and have poor performance on the training data containing anomalies. In this article, we propose a novel Self-Training based Anomaly Detection with Generative Adversarial Network (GAN) model called STAD-GAN to address the practical challenge. The STAD-GAN model consists of a generator-discriminator structure for adversarial learning and a neural network classifier for anomaly classification. The generator is learned to capture the normal data distribution, and the discriminator is learned to amplify the reconstruction error of abnormal data for better recognition. The proposed model is optimized with a self-training teacher-student framework, where a teacher model generates reliable high-quality pseudo-labels to train a student model iteratively with a refined dataset so that the performance of the anomaly classifier can be gradually improved. Extensive experiments based on six open MTS datasets show that STAD-GAN is robust to noise and achieves significant performance improvement compared to the state-of-the-art.},
  archive      = {J_TKDD},
  doi          = {10.1145/3572780},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {STAD-GAN: Unsupervised anomaly detection on multivariate time series with self-training generative adversarial networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diffuse and smooth: Beyond truncated receptive field for
scalable and adaptive graph representation learning. <em>TKDD</em>,
<em>17</em>(5), 1–25. (<a
href="https://doi.org/10.1145/3572781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the scope of receptive field and the depth of Graph Neural Networks (GNNs) are two completely orthogonal aspects for graph learning, existing GNNs often have shallow layers with truncated-receptive field and far from achieving satisfactory performance. In this article, we follow the idea of decoupling graph convolution into propagation and transformation processes, which generates representations over a sequence of increasingly larger neighborhoods. Though this manner can enlarge the receptive field, it has two critical problems unsolved: how to find the suitable receptive field to avoid under-smoothing or over-smoothing? and how to balance different diffusion operators for better capturing the local and global dependencies? We tackle these challenges and propose a S calable, A daptive G raph C onvolutional N etworks ( SAGCN ) with Transformer architecture. Concretely, we propose a novel non-heuristic metric method that quickly finds the suitable number of diffusing iterations and produces smoothed local embeddings that enable the truncated receptive field to become scalable and independent of prior experience. Furthermore, we devise smooth2seq and diffusion-based position schemes introduced into Transformer architecture for better capturing local and global information among embeddings. Experimental results show that SAGCN enjoys high accuracy, scalability and efficiency on various open benchmarks and is competitive with other state-of-the-art competitors.},
  archive      = {J_TKDD},
  doi          = {10.1145/3572781},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {5},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Diffuse and smooth: Beyond truncated receptive field for scalable and adaptive graph representation learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LoSAC: An efficient local stochastic average control method
for federated optimization. <em>TKDD</em>, <em>17</em>(4), 1–28. (<a
href="https://doi.org/10.1145/3566128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated optimization (FedOpt), which targets at collaboratively training a learning model across a large number of distributed clients, is vital for federated learning. The primary concerns in FedOpt can be attributed to the model divergence and communication efficiency, which significantly affect the performance. In this article, we propose a new method, i.e., LoSAC, to learn from heterogeneous distributed data more efficiently. Its key algorithmic insight is to locally update the estimate for the global full gradient after each regular local model update. Thus, LoSAC can keep clients’ information refreshed in a more compact way. In particular, we have studied the convergence result for LoSAC. Besides, the bonus of LoSAC is the ability to defend the information leakage from the recent technique Deep Leakage Gradients (DLG). Finally, experiments have verified the superiority of LoSAC comparing with state-of-the-art FedOpt algorithms. Specifically, LoSAC significantly improves communication efficiency by more than 100% on average, mitigates the model divergence problem, and equips with the defense ability against DLG.},
  archive      = {J_TKDD},
  doi          = {10.1145/3566128},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {4},
  number       = {4},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {LoSAC: An efficient local stochastic average control method for federated optimization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized federated learning on non-IID data via
group-based meta-learning. <em>TKDD</em>, <em>17</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3558005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (PFL) has emerged as a paradigm to provide a personalized model that can fit the local data distribution of each client. One natural choice for PFL is to leverage the fast adaptation capability of meta-learning, where it first obtains a single global model, and each client achieves a personalized model by fine-tuning the global one with its local data. However, existing meta-learning-based approaches implicitly assume that the data distribution among different clients is similar, which may not be applicable due to the property of data heterogeneity in federated learning. In this work, we propose a Group-based Federated Meta-Learning framework, called G-FML , which adaptively divides the clients into groups based on the similarity of their data distribution, and the personalized models are obtained with meta-learning within each group. In particular, we develop a simple yet effective grouping mechanism to adaptively partition the clients into multiple groups. Our mechanism ensures that each group is formed by the clients with similar data distribution such that the group-wise meta-model can achieve “personalization” at large. By doing so, our framework can be generalized to a highly heterogeneous environment. We evaluate the effectiveness of our proposed G-FML framework on three heterogeneous benchmarking datasets. The experimental results show that our framework improves the model accuracy by up to 13.15% relative to the state-of-the-art federated meta-learning.},
  archive      = {J_TKDD},
  doi          = {10.1145/3558005},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Personalized federated learning on non-IID data via group-based meta-learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowdsourcing truth inference based on label confidence
clustering. <em>TKDD</em>, <em>17</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3556545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Truth inference can help solve some difficult problems of data integration in crowdsourcing. Crowdsourced workers are not experts and their labeling ability varies greatly; therefore, in practical applications, it is difficult to determine whether the labels collected from a crowdsourcing platform are correct. This article proposes a novel algorithm called truth inference based on label confidence clustering (TILCC) to improve the quality of integrated labels for the single-choice classification problem in crowdsourcing labeling tasks. We obtain the label confidence via worker reliability, which is calculated from multiple noise labels using a truth discovery method, and then we generate the clustering features and use the K-means algorithm to cluster all the tasks into K different clusters. Each cluster corresponds to a specific class, and the tasks in the cluster are assigned a label. Compared with the performances of six state-of-the-art methods, MV, ZenCrowd, PM, CATD, GLAD, and GTIC, on 12 randomly selected real-world datasets, the performance of our algorithm showed many advantages: no need to set complex parameters, faster running speed, and significantly higher accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3556545},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Crowdsourcing truth inference based on label confidence clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CrowdAtlas: Estimating crowd distribution within the urban
rail transit system. <em>TKDD</em>, <em>17</em>(4), 1–24. (<a
href="https://doi.org/10.1145/3558521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While urban rail transit systems are playing an increasingly important role in meeting the transportation demands of people, precise awareness of how the human crowd is distributed within such a system is highly necessary, which serves a range of important applications including emergency response, transit recommendation, and commercial valuation. Most rail transit systems are closed systems where once entered the passengers are free to move around all stations and are difficult to track. In this article, we attempt to estimate the crowd distribution based only on the tap-in and tap-out records of all the rail riders. Specifically, we study Singapore MRT (Mass Rapid Transit) as a vehicle and leverage EZ-Link transit card records to estimate the crowd distribution. Guided by a key observation that the passenger inflows and arrival flows at different MRT stations and time are spatio-temporally correlated due to behavioral consistency of MRT riders, we design and implement a machine learning-based solution, CrowdAtlas, that captures MRT riders’ transition probabilities among stations and across time, and based on that accurately estimates the crowd distribution within the MRT system. Our comprehensive performance evaluations with both trace-driven studies and real-world experiments in MRT disruption cases demonstrate the effectiveness of CrowdAtlas.},
  archive      = {J_TKDD},
  doi          = {10.1145/3558521},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {CrowdAtlas: Estimating crowd distribution within the urban rail transit system},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DiVA: A scalable, interactive and customizable visual
analytics platform for information diffusion on large networks.
<em>TKDD</em>, <em>17</em>(4), 1–33. (<a
href="https://doi.org/10.1145/3558771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an increasing outreach of digital platforms in our lives, researchers have taken a keen interest in studying different facets of social interactions. Analyzing the spread of information ( aka diffusion) has brought forth multiple research areas such as modelling user engagement, determining emerging topics, forecasting the virality of online posts and predicting information cascades. Despite such ever-increasing interest, there remains a vacuum among easy-to-use interfaces for large-scale visualization of diffusion models. In this article, we introduce DiVA — Di ffusion V isualization and A nalysis, a tool that provides a scalable web interface and extendable APIs to analyze various diffusion trends on networks. DiVA uniquely offers support for simultaneous comparison of two competing diffusion models and even the comparison with the ground-truth results, which help develop a coherent understanding of real-world scenarios. Along with performing an exhaustive feature comparison and system evaluation of DiVA against publicly-available web interfaces for information diffusion, we conducted a user study to understand the strengths and limitations of DiVA . We noticed that evaluators had a seamless user experience, especially when analyzing diffusion on large networks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3558771},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-33},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DiVA: A scalable, interactive and customizable visual analytics platform for information diffusion on large networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRASP: Scalable graph alignment by spectral corresponding
functions. <em>TKDD</em>, <em>17</em>(4), 1–26. (<a
href="https://doi.org/10.1145/3561058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What is the best way to match the nodes of two graphs? This graph alignment problem generalizes graph isomorphism and arises in applications from social network analysis to bioinformatics. Some solutions assume that auxiliary information on known matches or node or edge attributes is available, or utilize arbitrary graph features. Such methods fare poorly in the pure form of the problem, in which only graph structures are given. Other proposals translate the problem to one of aligning node embeddings, yet, by doing so, provide only a single-scale view of the graph. In this article, we transfer the shape-analysis concept of functional maps from the continuous to the discrete case, and treat the graph alignment problem as a special case of the problem of finding a mapping between functions on graphs. We present GRASP, a method that first establishes a correspondence between functions derived from Laplacian matrix eigenvectors, which capture multiscale structural characteristics, and then exploits this correspondence to align nodes. We enhance the basic form of GRASP by altering two of its components, namely the embedding method and the assignment procedure it employs, leveraging its modular, hence adaptable design. Our experimental study, featuring noise levels higher than anything used in previous studies, shows that the enhanced form of GRASP outperforms scalable state-of-the-art methods for graph alignment across noise levels and graph types, and performs competitively with respect to the best non-scalable ones. We include in our study another modular graph alignment algorithm, CONE, which is also adaptable thanks to its modular nature, and show it can manage graphs with skewed power-law degree distributions.},
  archive      = {J_TKDD},
  doi          = {10.1145/3561058},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {GRASP: Scalable graph alignment by spectral corresponding functions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random walk sampling in social networks involving private
nodes. <em>TKDD</em>, <em>17</em>(4), 1–28. (<a
href="https://doi.org/10.1145/3561388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis of social networks with limited data access is challenging for third parties. To address this challenge, a number of studies have developed algorithms that estimate properties of social networks via a simple random walk. However, most existing algorithms do not assume private nodes that do not publish their neighbors’ data when they are queried in empirical social networks. Here we propose a practical framework for estimating properties via random walk-based sampling in social networks involving private nodes. First, we develop a sampling algorithm by extending a simple random walk to the case of social networks involving private nodes. Then, we propose estimators with reduced biases induced by private nodes for the network size, average degree, and density of the node label. Our results show that the proposed estimators reduce biases induced by private nodes in the existing estimators by up to 92.6% on social network datasets involving private nodes.},
  archive      = {J_TKDD},
  doi          = {10.1145/3561388},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Random walk sampling in social networks involving private nodes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lifelong online learning from accumulated knowledge.
<em>TKDD</em>, <em>17</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3563947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we formulate lifelong learning as an online transfer learning procedure over consecutive tasks, where learning a given task depends on the accumulated knowledge. We propose a novel theoretical principled framework, lifelong online learning, where the learning process for each task is in an incremental manner. Specifically, our framework is composed of two-level predictions: the prediction information that is solely from the current task; and the prediction from the knowledge base by previous tasks. Moreover, this article tackled several fundamental challenges: arbitrary or even non-stationary task generation process, an unknown number of instances in each task, and constructing an efficient accumulated knowledge base. Notably, we provide a provable bound of the proposed algorithm, which offers insights on the how the accumulated knowledge improves the predictions. Finally, empirical evaluations on both synthetic and real datasets validate the effectiveness of the proposed algorithm.},
  archive      = {J_TKDD},
  doi          = {10.1145/3563947},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Lifelong online learning from accumulated knowledge},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MIRROR: Mining implicit relationships via structure-enhanced
graph convolutional networks. <em>TKDD</em>, <em>17</em>(4), 1–24. (<a
href="https://doi.org/10.1145/3564531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data explosion in the information society drives people to develop more effective ways to extract meaningful information. Extracting semantic information and relational information has emerged as a key mining primitive in a wide variety of practical applications. Existing research on relation mining has primarily focused on explicit connections and ignored underlying information, e.g., the latent entity relations. Exploring such information (defined as implicit relationships in this article) provides an opportunity to reveal connotative knowledge and potential rules. In this article, we propose a novel research topic, i.e., how to identify implicit relationships across heterogeneous networks. Specially, we first give a clear and generic definition of implicit relationships. Then, we formalize the problem and propose an efficient solution, namely MIRROR, a graph convolutional network (GCN) model to infer implicit ties under explicit connections. MIRROR captures rich information in learning node-level representations by incorporating attributes from heterogeneous neighbors. Furthermore, MIRROR is tolerant of missing node attribute information because it is able to utilize network structure. We empirically evaluate MIRROR on four different genres of networks, achieving state-of-the-art performance for target relations mining. The underlying information revealed by MIRROR contributes to enriching existing knowledge and leading to novel domain insights.},
  archive      = {J_TKDD},
  doi          = {10.1145/3564531},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {MIRROR: Mining implicit relationships via structure-enhanced graph convolutional networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TAP: Traffic accident profiling via multi-task
spatio-temporal graph representation learning. <em>TKDD</em>,
<em>17</em>(4), 1–25. (<a
href="https://doi.org/10.1145/3564594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting traffic accidents can help traffic management departments respond to sudden traffic situations promptly, improve drivers’ vigilance, and reduce losses caused by traffic accidents. However, the causality of traffic accidents is complex and difficult to analyze. Most existing traffic accident prediction methods do not consider the dynamic spatio-temporal correlation of traffic data, which leads to unsatisfactory prediction accuracy. To address this issue, we propose a multi-task learning framework (TAP) based on the Spatio-temporal Variational Graph Auto-Encoders (ST-VGAE) for traffic accident profiling. We firstly capture the dynamic spatio-temporal correlation of traffic conditions through a spatio-temporal graph convolutional encoder and embed it as a low-latitude vector. Then, we use a multi-task learning scheme to combine external factors to generate the traffic accident profiling. Furthermore, we propose a traffic accident profiling application framework based on edge computing. This method increases the speed of calculation by offloading the calculation of traffic accident profiling to edge nodes. Finally, the experimental results on real datasets demonstrate that TAP outperforms other state-of-the-art baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3564594},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {TAP: Traffic accident profiling via multi-task spatio-temporal graph representation learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trip reinforcement recommendation with graph-based
representation learning. <em>TKDD</em>, <em>17</em>(4), 1–20. (<a
href="https://doi.org/10.1145/3564609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tourism is an important industry and a popular leisure activity involving billions of tourists per annum. One challenging problem tourists face is identifying attractive Places-of-Interest (POIs) and planning the personalized trip with time constraints. Most of the existing trip recommendation methods mainly consider POI popularity and user preferences, and focus on the last visited POI when choosing the next POI. However, the visit patterns and their asymmetry property have not been fully exploited. To this end, in this article, we present a GRM-RTrip (short for G raph-based R epresentation M ethod for R einforce Trip Recommendation) framework. GRM-RTrip learns POI representations from incoming and outgoing views to obtain asymmetric POI-POI transition probability via POI-POI graph networks, and then fuses the trained POI representation into a user-POI graph network to estimate user preferences. Finally, after formulating the personalized trip recommendation as a Markov Decision Process (MDP), we utilize a reinforcement learning algorithm for generating a personalized trip with maximal user travel experience. Extensive experiments are performed on the public datasets and the results demonstrate the superiority of GRM-RTrip compared with the state-of-the-art trip recommendation methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3564609},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Trip reinforcement recommendation with graph-based representation learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic multi-view graph neural networks for citywide
traffic inference. <em>TKDD</em>, <em>17</em>(4), 1–22. (<a
href="https://doi.org/10.1145/3564754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate citywide traffic inference is critical for improving intelligent transportation systems with smart city applications. However, this task is very challenging given the limited training data, due to the high cost of sensor installment and maintenance across the entire urban space. A more practical scenario to study the citywide traffic inference is effectively modeling the spatial and temporal traffic patterns with limited historical traffic observations. In this work, we propose a dynamic multi-view graph neural network for citywide traffic inference with the method CTVI+. Specifically, for the temporal dimension, we propose a temporal self-attention mechanism that is capable of learning the dynamics of traffic data with the time-evolving traffic volume variations. For spatial dimension, we build a multi-view graph neural network, employing the road-wise message passing scheme to capture the region dependencies. With the designed spatial-temporal learning paradigms, we enable our traffic inference model to encode the dynamism from both spatial and temporal traffic patterns, which is reflective of intra- and inter-road traffic correlations. In our evaluation, CTVI+ achieves consistent better performance compared with different baselines on real-world traffic volume datasets. Further ablation study validates the effectiveness of key components in CTVI+. We release the model implementation at https://github.com/dsj96/TKDD.},
  archive      = {J_TKDD},
  doi          = {10.1145/3564754},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dynamic multi-view graph neural networks for citywide traffic inference},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning shared representations for recommendation with
dynamic heterogeneous graph convolutional networks. <em>TKDD</em>,
<em>17</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3565575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have been widely used for collaborative filtering, due to their effectiveness in exploiting high-order collaborative signals. However, two issues have not been well addressed by existing studies. First, usually only one kind of information is utilized, i.e., user preference in user-item graphs or item dependency in item-item graphs. Second, they usually adopt static graphs, which cannot retain the temporal evolution of the information. These can limit the recommendation quality. To address these limitations, we propose to mine three kinds of information (user preference, item dependency, and user behavior similarity) and their temporal evolution by constructing multiple discrete dynamic heterogeneous graphs (i.e., a user-item dynamic graph, an item-item dynamic graph, and a user-subseq dynamic graph) from interaction data. A novel network (PDGCN) is proposed to learn the representations of users and items in these dynamic graphs. Moreover, we designed a structural neighbor aggregation module with novel pooling and convolution operations to aggregate the features of structural neighbors. We also design a temporal neighbor aggregation module based on self-attention mechanism to aggregate the features of temporal neighbors. We conduct extensive experiments on four real-world datasets. The results indicate that our approach outperforms several competing methods in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). Dynamic graphs are also shown to be effective in improving recommendation performance.},
  archive      = {J_TKDD},
  doi          = {10.1145/3565575},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning shared representations for recommendation with dynamic heterogeneous graph convolutional networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SPAP: Simultaneous demand prediction and planning for
electric vehicle chargers in a new city. <em>TKDD</em>, <em>17</em>(4),
1–25. (<a href="https://doi.org/10.1145/3565577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a new city that is committed to promoting Electric Vehicles (EVs), it is significant to plan the public charging infrastructure where charging demands are high. However, it is difficult to predict charging demands before the actual deployment of EV chargers for lack of operational data, resulting in a deadlock. A direct idea is to leverage the urban transfer learning paradigm to learn the knowledge from a source city, then exploit it to predict charging demands, and meanwhile determine locations and amounts of slow/fast chargers for charging stations in the target city. However, the demand prediction and charger planning depend on each other, and it is required to re-train the prediction model to eliminate the negative transfer between cities for each varied charger plan, leading to the unacceptable time complexity. To this end, we design an effective solution of S imultaneous Demand P rediction A nd P lanning ( SPAP ): discriminative features are extracted from multi-source data, and fed into an Attention-based Spatial-Temporal City Domain Adaptation Network ( AST-CDAN ) for cross-city demand prediction; a novel Transfer Iterative Optimization ( TIO ) algorithm is designed for charger planning by iteratively utilizing AST-CDAN and a charger plan fine-tuning algorithm. Extensive experiments on real-world datasets collected from three cities in China validate the effectiveness and efficiency of SPAP . Specially, SPAP improves at most 72.5% revenue compared with the real-world charger deployment.},
  archive      = {J_TKDD},
  doi          = {10.1145/3565577},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SPAP: Simultaneous demand prediction and planning for electric vehicle chargers in a new city},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STHAN: Transportation demand forecasting with compound
spatio-temporal relationships. <em>TKDD</em>, <em>17</em>(4), 1–23. (<a
href="https://doi.org/10.1145/3565578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation demand forecasting is a critical precondition of optimal online transportation dispatch, which will greatly reduce drivers’ wasted mileage and customers’ waiting time, contributing to economic and environmental sustainability. Though various methods have been developed, the core spatio-temporal complexity remains challenging from three perspectives: (1) Compound spatial relationships. According to our empirical analysis, these relationships widely exist. Previous studies focus on capturing different spatial relationships using multi-homogeneous graphs. However, the information flow across various spatial relationships is not modeled explicitly. (2) Heterogeneity in spatial relationships. A region’s neighbors under the same spatial relationship may have different weights for this region. Meanwhile, different relationships may also weigh differently. (3) Synchronicity between compound spatial relationships and temporal relationships. Previous research considers synchronous influences from spatial and temporal relationships in a homogeneous fashion while compound spatial relationships are not captured for this synchronicity. To address the aforementioned perspectives, we propose the S patio- T emporal H eterogeneous graph A ttention N etwork (STHAN), where the key intuition is capturing the compound spatial relationships via meta-paths explicitly. We first construct a spatio-temporal heterogeneous graph including multiple spatial relationships and temporal relationships and use meta-paths to depict compound spatial relationships. To capture the heterogeneity, we use hierarchical attention, which contains node level attention and meta-path level attention. The synchronicity between temporal relationships and spatial relationships, including compound ones, is modeled in meta-path-level attention. Our framework outperforms state-of-the-art models by reducing 6.58%, 4.57%, and 4.20% of WMAPE in experiments on three real-world datasets, respectively.},
  archive      = {J_TKDD},
  doi          = {10.1145/3565578},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {4},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {STHAN: Transportation demand forecasting with compound spatio-temporal relationships},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learnable graph-regularization for matrix decomposition.
<em>TKDD</em>, <em>17</em>(3), 1–20. (<a
href="https://doi.org/10.1145/3544781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank approximation models of data matrices have become important machine learning and data mining tools in many fields, including computer vision, text mining, bioinformatics, and many others. They allow for embedding high-dimensional data into low-dimensional spaces, which mitigates the effects of noise and uncovers latent relations. In order to make the learned representations inherit the structures in the original data, graph-regularization terms are often added to the loss function. However, the prior graph construction often fails to reflect the true network connectivity and the intrinsic relationships. In addition, many graph-regularized methods fail to take the dual spaces into account. Probabilistic models are often used to model the distribution of the representations, but most of previous methods often assume that the hidden variables are independent and identically distributed for simplicity. To this end, we propose a learnable graph-regularization model for matrix decomposition (LGMD), which builds a bridge between graph-regularized methods and probabilistic matrix decomposition models for the first time. LGMD incorporates two graphical structures (i.e., two precision matrices) learned in an iterative manner via sparse precision matrix estimation and is more robust to noise and missing entries. Extensive numerical results and comparison with competing methods demonstrate its effectiveness.},
  archive      = {J_TKDD},
  doi          = {10.1145/3544781},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {3},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learnable graph-regularization for matrix decomposition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-processing modeling techniques for machine learning
fairness: A survey. <em>TKDD</em>, <em>17</em>(3), 1–27. (<a
href="https://doi.org/10.1145/3551390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models are becoming pervasive in high-stakes applications. Despite their clear benefits in terms of performance, the models could show discrimination against minority groups and result in fairness issues in a decision-making process, leading to severe negative impacts on the individuals and the society. In recent years, various techniques have been developed to mitigate the unfairness for machine learning models. Among them, in-processing methods have drawn increasing attention from the community, where fairness is directly taken into consideration during model design to induce intrinsically fair models and fundamentally mitigate fairness issues in outputs and representations. In this survey, we review the current progress of in-processing fairness mitigation techniques. Based on where the fairness is achieved in the model, we categorize them into explicit and implicit methods, where the former directly incorporates fairness metrics in training objectives, and the latter focuses on refining latent representation learning. Finally, we conclude the survey with a discussion of the research challenges in this community to motivate future exploration.},
  archive      = {J_TKDD},
  doi          = {10.1145/3551390},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {3},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {In-processing modeling techniques for machine learning fairness: A survey},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variational graph autoencoder with adversarial mutual
information learning for network representation learning. <em>TKDD</em>,
<em>17</em>(3), 1–18. (<a
href="https://doi.org/10.1145/3555809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of Graph Neural Network (GNN) in network data, some GNN-based representation learning methods for networks have emerged recently. Variational Graph Autoencoder (VGAE) is a basic GNN framework for network representation. Its purpose is to well preserve the topology and node attribute information of the network to learn node representation, but it only reconstructs network topology, and does not consider the reconstruction of node features. This strategy will make node representation can not well reserve node features information, impairing the ability of the VGAE method to learn higher quality representations. To solve this problem, we arise a new network representation method to improve the VGAE method for well retaining both node features and network structure information. The method utilizes adversarial mutual information learning to maximize the mutual information (MI) of node features and node representations during the encoding process of the variational autoencoder, which forces the variational encoder to get the representation containing the most informative node features. The method consists of three parts: a variational graph autoencoder includes a variational encoder (MI generator (G)) and a decoder, a positive MI sample module (maximizing MI module), and an MI discriminator (D). Furthermore, we explain why maximizing MI between node features and node representation can reconstruct node attributes. Finally, we conduct experiments on seven public representative datasets for nodes classification, nodes clustering, and graph visualization tasks. Experimental results demonstrate that the proposed algorithm significantly outperforms current popular network representation algorithms on these tasks. The best improvement is 17.13% than the VGAE method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3555809},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {3},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Variational graph autoencoder with adversarial mutual information learning for network representation learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRACE: A general graph convolution framework for attributed
graph clustering. <em>TKDD</em>, <em>17</em>(3), 1–31. (<a
href="https://doi.org/10.1145/3544977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering (AGC) is an important problem in graph mining as more and more complex data in real-world have been represented in graphs with attributed nodes. While it is a common practice to leverage both attribute and structure information for improved clustering performance, most existing AGC algorithms consider only a specific type of relations, which hinders their applicability to integrate various complex relations into node attributes for AGC. In this article, we propose GRACE, an extended graph convolution framework for AGC tasks. Our framework provides a general and interpretative solution for clustering many different types of attributed graphs, including undirected, directed, heterogeneous and hyper attributed graphs. By building suitable graph Laplacians for each of the aforementioned graph types, GRACE can seamlessly perform graph convolution on node attributes to fuse all available information for clustering. We conduct extensive experiments on 14 real-world datasets of four different graph types. The experimental results show that GRACE outperforms the state-of-the-art AGC methods on the different graph types in terms of clustering quality, time, and memory usage.},
  archive      = {J_TKDD},
  doi          = {10.1145/3544977},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-31},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {GRACE: A general graph convolution framework for attributed graph clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contact tracing and epidemic intervention via deep
reinforcement learning. <em>TKDD</em>, <em>17</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3546870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent outbreak of COVID-19 poses a serious threat to people’s lives. Epidemic control strategies have also caused damage to the economy by cutting off humans’ daily commute. In this article, we develop an Individual-based Reinforcement Learning Epidemic Control Agent (IDRLECA) to search for smart epidemic control strategies that can simultaneously minimize infections and the cost of mobility intervention. IDRLECA first hires an infection probability model to calculate the current infection probability of each individual. Then, the infection probabilities together with individuals’ health status and movement information are fed to a novel GNN to estimate the spread of the virus through human contacts. The estimated risks are used to further support an RL agent to select individual-level epidemic-control actions. The training of IDRLECA is guided by a specially designed reward function considering both the cost of mobility intervention and the effectiveness of epidemic control. Moreover, we design a constraint for control-action selection that eases its difficulty and further improve exploring efficiency. Extensive experimental results demonstrate that IDRLECA can suppress infections at a very low level and retain more than 95% of human mobility.},
  archive      = {J_TKDD},
  doi          = {10.1145/3546870},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Contact tracing and epidemic intervention via deep reinforcement learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for practical express systems with
mixed deliveries and pickups. <em>TKDD</em>, <em>17</em>(3), 1–19. (<a
href="https://doi.org/10.1145/3546952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world express systems, couriers need to satisfy not only the delivery demands but also the pick-up demands of customers. Delivery and pickup tasks are usually mixed together within integrated routing plans. Such a mixed routing problem can be abstracted and formulated as Vehicle Routing Problem with Mixed Delivery and Pickup (VRPMDP), which is an NP-hard combinatorial optimization problem. To solve VRPMDP, there are three major challenges as below. (a) Even though successive pickup and delivery tasks are independent to accomplish, the inter-influence between choosing pickup task or delivery task to deal with still exists. (b) Due to the two-way flow of goods between the depot and customers, the loading rate of vehicles leaving the depot affects routing decisions. (c) The proportion of deliveries and pickups will change due to the complex demand situation in real-world scenarios, which requires robustness of the algorithm. To solve the challenges above, we design an encoder-decoder based framework to generate high-quality and robust VRPMDP solutions. First, we consider a VRPMDP instance as a graph and utilize a GNN encoder to extract the feature of the instance effectively. The detailed routing solutions are further decoded as a sequence by the decoder with attention mechanism. Second, we propose a Coordinated Decision of Loading and Routing (CDLR) mechanism to determine the loading rate dynamically after the vehicle returns to the depot, thus avoiding the influence of improper loading rate settings. Finally, the model equipped with a GNN encoder and CDLR simultaneously can adapt to the changes in the proportion of deliveries and pickups. We conduct the experiments to demonstrate the effectiveness of our model. The experiments show that our method achieves desirable results and generalization ability.},
  archive      = {J_TKDD},
  doi          = {10.1145/3546952},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Reinforcement learning for practical express systems with mixed deliveries and pickups},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ONP-miner: One-off negative sequential pattern mining.
<em>TKDD</em>, <em>17</em>(3), 1–24. (<a
href="https://doi.org/10.1145/3549940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative sequential pattern mining (SPM) is an important SPM research topic. Unlike positive SPM, negative SPM can discover events that should have occurred but have not occurred, and it can be used for financial risk management and fraud detection. However, existing methods generally ignore the repetitions of the pattern and do not consider gap constraints, which can lead to mining results containing a large number of patterns that users are not interested in. To solve this problem, this article discovers frequent one-off negative sequential patterns (ONPs). This problem has the following two characteristics. First, the support is calculated under the one-off condition, which means that any character in the sequence can only be used once at most. Second, the gap constraint can be given by the user. To efficiently mine patterns, this article proposes the ONP-Miner algorithm, which employs depth-first and backtracking strategies to calculate the support. Therefore, ONP-Miner can effectively avoid creating redundant nodes and parent-child relationships. Moreover, to effectively reduce the number of candidate patterns, ONP-Miner uses pattern join and pruning strategies to generate and further prune the candidate patterns, respectively. Experimental results show that ONP-Miner not only improves the mining efficiency but also has better mining performance than the state-of-the-art algorithms. More importantly, ONP mining can find more interesting patterns in traffic volume data to predict future traffic.},
  archive      = {J_TKDD},
  doi          = {10.1145/3549940},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {ONP-miner: One-off negative sequential pattern mining},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Methods and applications of clusterwise linear regression: A
survey and comparison. <em>TKDD</em>, <em>17</em>(3), 1–54. (<a
href="https://doi.org/10.1145/3550074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clusterwise linear regression (CLR) is a well-known technique for approximating a data using more than one linear function. It is based on the combination of clustering and multiple linear regression methods. This article provides a comprehensive survey and comparative assessments of CLR including model formulations, description of algorithms, and their performance on small to large-scale synthetic and real-world datasets. Some applications of the CLR algorithms and possible future research directions are also discussed.},
  archive      = {J_TKDD},
  doi          = {10.1145/3550074},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-54},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Methods and applications of clusterwise linear regression: A survey and comparison},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable integration of social media background in a
dynamic neural recommender. <em>TKDD</em>, <em>17</em>(3), 1–14. (<a
href="https://doi.org/10.1145/3550279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems nowadays are commonly deployed in e-commerce platforms to help customers making purchase decisions. Dynamic recommender considers not only static user-item interaction data, but the temporal information at the time of recommendation. Previous researches have suggested to incorporate social media as the temporal information in dynamic neural recommenders after transforming them into embeddings. While such an approach can potentially improve recommendation performance, the effectiveness is difficult to explain. In this article, we propose an explainable method to integrate social media in a dynamic neural recommender. Our method applies association rule mining, which can generate human-understandable behavior patterns from social media and e-commerce platforms. With real-world social media and e-commerce data, we show that the integration can improve accuracy by up to 14% while using the same data. Moreover, we can explain the positive cases by examining relevant association rules.},
  archive      = {J_TKDD},
  doi          = {10.1145/3550279},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-14},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Explainable integration of social media background in a dynamic neural recommender},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L2MM: Learning to map matching with deep models for
low-quality GPS trajectory data. <em>TKDD</em>, <em>17</em>(3), 1–25.
(<a href="https://doi.org/10.1145/3550486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Map matching is a fundamental research topic with the objective of aligning GPS trajectories to paths on the road network. However, existing models fail to achieve satisfactory performance for low-quality (i.e., noisy, low-frequency, and non-uniform) trajectory data. To this end, we propose a general and robust deep learning-based model, L2MM , to tackle these issues at all. First, high-quality representations of low-quality trajectories are learned by two representation enhancement methods, i.e., enhancement with high-frequency trajectories and enhancement with the data distribution . The former employs high-frequency trajectories to enhance the expressive capability of representations, while the latter regularizes the representation distribution over the latent space to improve the generalization ability of representations. Secondly, to embrace more heuristic clues, typical mobility patterns are recognized in the latent space and further incorporated into the map matching task. Finally, based on the available representations and patterns, a mapping from trajectories to corresponding paths is constructed through a joint optimization method. Extensive experiments are conducted based on a range of datasets, which demonstrate the superiority of L2MM and validate the significance of high-quality representations as well as mobility patterns.},
  archive      = {J_TKDD},
  doi          = {10.1145/3550486},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {L2MM: Learning to map matching with deep models for low-quality GPS trajectory data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient node PageRank improvement via link-building using
geometric deep learning. <em>TKDD</em>, <em>17</em>(3), 1–22. (<a
href="https://doi.org/10.1145/3551642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centrality is a relevant topic in the field of network research, due to its various theoretical and practical implications. In general, all centrality metrics aim at measuring the importance of nodes (according to some definition of importance), and such importance scores are used to rank the nodes in the network, therefore the rank improvement is a strictly related topic. In a given network, the rank improvement is achieved by establishing new links, therefore the question shifts to which and how many links should be collected to get a desired rank. This problem, also known as link-building has been shown to be NP-hard, and most heuristics developed failed in obtaining good performance with acceptable computational complexity. In this article, we present LB–GDM , a novel approach that leverages Geometric Deep Learning to tackle the link-building problem. To validate our proposal, 31 real-world networks were considered; tests show that LB–GDM performs significantly better than the state-of-the-art heuristics, while having a comparable or even lower computational complexity, which allows it to scale well even to large networks.},
  archive      = {J_TKDD},
  doi          = {10.1145/3551642},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Efficient node PageRank improvement via link-building using geometric deep learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple imputation ensembles for time series (MIE-TS).
<em>TKDD</em>, <em>17</em>(3), 1–28. (<a
href="https://doi.org/10.1145/3551643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification has become an interesting field of research, thanks to the extensive studies conducted in the past two decades. Time series may have missing data, which may affect both the representation and also modeling of time series. Thus, recovering missing data using appropriate time series-based imputation methods is an essential step. Multiple imputation is a data recovery method where it produced multiple imputed data. The method proves its usefulness in terms of reflecting the uncertainty inherit in missing data; however, it is under-researched in time series problems. In this article, we propose two multiple imputation approaches for time series. The first is a multiple imputation method based on interpolation. The second is a multiple imputation and ensemble method. First, we simulate missing consecutive sub-sequences under a Missing Completely at Random mechanism; then, we use single/multiple imputation methods. The imputed data are used to build bagging and stacking ensembles. We build ensembles using standard classification algorithms as well as time series classifiers. The standard classifiers involve Random Forest, Support Vector Machines, K-Nearest Neighbour, C4.5, and PART while TSCHIEF, Proximity Forest, Time Series Forest, RISE, and BOSS are chosen as time series classifiers. Our findings show that the combination of multiple imputation and ensemble improves the performance of the majority of classifiers tested in this study, often above the performance obtained from the complete data, even under increasing missing data scenarios. This may be because the diversity injected by multiple imputation has a very favourable and stabilising effect on the classifier performance, which is a very important finding.},
  archive      = {J_TKDD},
  doi          = {10.1145/3551643},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multiple imputation ensembles for time series (MIE-TS)},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DNformer: Temporal link prediction with transfer learning in
dynamic networks. <em>TKDD</em>, <em>17</em>(3), 1–21. (<a
href="https://doi.org/10.1145/3551892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal link prediction (TLP) is among the most important graph learning tasks, capable of predicting dynamic, time-varying links within networks. The key problem of TLP is how to explore potential link-evolving tendency from the increasing number of links over time. There exist three major challenges toward solving this problem: temporal nonlinear sparsity, weak serial correlation, and discontinuous structural dynamics. In this article, we propose a novel transfer learning model, called DNformer, to predict temporal link sequence in dynamic networks. The structural dynamic evolution is sequenced into consecutive links one by one over time to inhibit temporal nonlinear sparsity. The self-attention of the model is used to capture the serial correlation between the input and output link sequences. Moreover, our structural encoding is designed to obtain changing structures from the consecutive links and to learn the mapping between link sequences. This structural encoding consists of two parts: the node clustering encoding of each link and the link similarity encoding between links. These encodings enable the model to perceive the importance and correlation of links. Furthermore, we introduce a measurement of structural similarity in the loss function for the structural differences of link sequences. The experimental results demonstrate that our model outperforms other state-of-the-art TLP methods such as Transformer, TGAT, and EvolveGCN. It achieves the three highest AUC and four highest precision scores in five different representative dynamic networks problems.},
  archive      = {J_TKDD},
  doi          = {10.1145/3551892},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DNformer: Temporal link prediction with transfer learning in dynamic networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Microblog retrieval based on concept-enhanced pre-training
model. <em>TKDD</em>, <em>17</em>(3), 1–32. (<a
href="https://doi.org/10.1145/3552311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have mostly been applied to conventional ad-hoc retrieval tasks over web pages and newswire articles. This article proposes a concept-enhanced pre-training model for microblog retrieval task, leveraging Semantic Matching Model (SMM) objective and Concept Correlation Model (CCM) objective. The proposed model is a novel neural ranking model specifically designed for ranking short-text microblog, which could merge the advantage of pre-training methodology for generating valid contextualized embedding with the superiority of the prior lexical knowledge (e.g., concept knowledge) for understanding short-text language semantic. We conduct experiments on widely used real-world datasets, and the experimental results demonstrate the efficiency of the proposed model, even compared with latest state-of-the-art neural-based models and pre-training based models.},
  archive      = {J_TKDD},
  doi          = {10.1145/3552311},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-32},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Microblog retrieval based on concept-enhanced pre-training model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual subgraph-based graph neural network for friendship
prediction in location-based social networks. <em>TKDD</em>,
<em>17</em>(3), 1–28. (<a
href="https://doi.org/10.1145/3554981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide use of Location-Based Social Networks (LBSNs), predicting user friendship from online social relations and offline trajectory data is of great value to improve the platform service quality and user satisfaction. Existing methods mainly focus on some hand-crafted features or graph embedding models based on the user-location bipartite graph, which cannot precisely capture the latent mobility similarity for the majority of users who have no explicit co-visit behaviors and also fail to balance the tradeoff between social features and mobility features for friendship prediction. In this regard, we propose a dual subgraph-based pairwise graph neural network (DSGNN) for friendship prediction in LBSNs, which extracts a pairwise social subgraph and a trajectory subgraph to model the social proximity and mobility similarity, respectively. Specifically, to overcome the co-visit data sparsity, we design an entropy-based random walk to construct a location graph that captures the high-level correlation between locations. Based on this, we characterize the pairwise mobility similarity from trajectory level instead of location level, which is modeled by a graph neural network (GNN) on a labeled trajectory subgraph composed of the two trajectories of the target user pair. Besides, we also utilize another GNN to extract social proximity based on social subgraph of the target user pair. Finally, we propose a gate layer to adaptively balance the fusion of the social and mobility features for friendship prediction. We conduct extensive experiments on the real-world datasets and demonstrate the superiority of our approach, which outperforms other state-of-the-art methods. In particular, the comparative experiments on the trajectory level mobility similarity further validate the effectiveness of the designed trajectory subgraph-based method, which can extract predictive mobility features.},
  archive      = {J_TKDD},
  doi          = {10.1145/3554981},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {3},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dual subgraph-based graph neural network for friendship prediction in location-based social networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CausalSE: Understanding varied spatial effects with missing
data toward adding new bike-sharing stations. <em>TKDD</em>,
<em>17</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3536427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the growing bike-sharing demands and make people’s travel convenient, the companies need to add new stations at locations where demands exceed supply. Before making reliable decisions on adding new stations, it is required to understand the spatial effects of new stations on the station network. In this article, we study the deployment of the new station by estimating its varied causal effects on the demands of nearby stations, e.g., how does adding a new station (treatment) causally influence the demands (outcome) of nearby stations? When working with observational data, we should control hidden confounders, which cause spurious relations between treatments and outcomes. However, previous studies use historical data of the individual unit (e.g., the station’s historical demands) to approximate its hidden confounders, which cannot deal with the lack of historical data for new stations. And the conventional methods overlook the differences between units, which cannot be applied to our problem. To overcome the challenges, we propose a novel model (CausalSE) to estimate the varied effects of new stations on nearby stations, which uses the shared knowledge (i.e., similar traveling patterns among stations) to approximate hidden confounders. Experimental results on real-world datasets show that CausalSE outperforms six state-of-the-art methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3536427},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {CausalSE: Understanding varied spatial effects with missing data toward adding new bike-sharing stations},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scheduling hyperparameters to improve generalization: From
centralized SGD to asynchronous SGD. <em>TKDD</em>, <em>17</em>(2),
1–37. (<a href="https://doi.org/10.1145/3544782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article 1 studies how to schedule hyperparameters to improve generalization of both centralized single-machine stochastic gradient descent (SGD) and distributed asynchronous SGD (ASGD). SGD augmented with momentum variants (e.g., heavy ball momentum (SHB) and Nesterov’s accelerated gradient (NAG)) has been the default optimizer for many tasks, in both centralized and distributed environments. However, many advanced momentum variants, despite empirical advantage over classical SHB/NAG, introduce extra hyperparameters to tune. The error-prone tuning is the main barrier for AutoML. Centralized SGD: We first focus on centralized single-machine SGD and show how to efficiently schedule the hyperparameters of a large class of momentum variants to improve generalization. We propose a unified framework called multistage quasi-hyperbolic momentum (Multistage QHM), which covers a large family of momentum variants as its special cases (e.g., vanilla SGD/SHB/NAG). Existing works mainly focus on only scheduling learning rate α’s decay, while multistage QHM allows additional varying hyperparameters (e.g., momentum factor), and demonstrates better generalization than only tuning α. We show the convergence of multistage QHM for general non-convex objectives. Distributed SGD: We then extend our theory to distributed asynchronous SGD (ASGD), in which a parameter server distributes data batches to several worker machines and updates parameters via aggregating batch gradients from workers. We quantify the asynchrony between different workers (i.e., gradient staleness), model the dynamics of asynchronous iterations via a stochastic differential equation (SDE), and then derive a PAC-Bayesian generalization bound for ASGD. As a byproduct, we show how a moderately large learning rate helps ASGD to generalize better. Our tuning strategies have rigorous justifications rather than a blind trial-and-error as we theoretically prove why our tuning strategies could decrease our derived generalization errors in both cases. Our strategies simplify the tuning process and beat competitive optimizers in test accuracy empirically. Our codes are publicly available https://github.com/jsycsjh/centralized-asynchronous-tuning .},
  archive      = {J_TKDD},
  doi          = {10.1145/3544782},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {2},
  pages        = {1-37},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Scheduling hyperparameters to improve generalization: From centralized SGD to asynchronous SGD},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeltaShield: Information theory for human- trafficking
detection. <em>TKDD</em>, <em>17</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3563040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a million escort advertisements, how can we spot near-duplicates? Such micro-clusters of ads are usually signals of human trafficking (HT). How can we summarize them to convince law enforcement to act? Spotting micro-clusters of near-duplicate documents is useful in multiple, additional settings, including spam-bot detection in Twitter ads, plagiarism, and more. We present InfoShield , which makes the following contributions: practical , being scalable and effective on real data; parameter-free and principled , requiring no user-defined parameters; interpretable , finding a document to be the cluster representative, highlighting all the common phrases, and automatically detecting “slots” (i.e., phrases that differ in every document); and generalizable , beating or matching domain-specific methods in Twitter bot detection and HT detection, respectively, as well as being language independent. Interpretability is particularly important for the anti-HT domain, where law enforcement must visually inspect ads. Our experiments on real data show that InfoShield correctly identifies Twitter bots with an F1 score over 90% and detects HT ads with 84% precision. Moreover, it is scalable, requiring about 8 hours for 4 million documents on a stock laptop. Our incremental version, DeltaShield , allows for fast, incremental updates, with minor loss of accuracy.},
  archive      = {J_TKDD},
  doi          = {10.1145/3563040},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {DeltaShield: Information theory for human- trafficking detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting anomalous graphs in labeled multi-graph databases.
<em>TKDD</em>, <em>17</em>(2), 1–25. (<a
href="https://doi.org/10.1145/3533770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within a large database 𝒢 containing graphs with labeled nodes and directed, multi-edges; how can we detect the anomalous graphs? Most existing work are designed for plain (unlabeled) and/or simple (unweighted) graphs. We introduce CODEtect , the first approach that addresses the anomaly detection task for graph databases with such complex nature. To this end, it identifies a small representative set 𝒮 of structural patterns (i.e., node-labeled network motifs) that losslessly compress database 𝒢 as concisely as possible. Graphs that do not compress well are flagged as anomalous. CODEtect exhibits two novel building blocks: (i) a motif-based lossless graph encoding scheme, and (ii) fast memory-efficient search algorithms for 𝒮. We show the effectiveness of CODEtect on transaction graph databases from three different corporations and statistically similar synthetic datasets, where existing baselines adjusted for the task fall behind significantly, across different types of anomalies and performance metrics.},
  archive      = {J_TKDD},
  doi          = {10.1145/3533770},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-25},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Detecting anomalous graphs in labeled multi-graph databases},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure diversity-induced anchor graph fusion for
multi-view clustering. <em>TKDD</em>, <em>17</em>(2), 1–18. (<a
href="https://doi.org/10.1145/3534931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The anchor graph structure has been widely used to speed up large-scale multi-view clustering and exhibited promising performance. How to effectively integrate the anchor graphs on multiple views to achieve enhanced clustering performance still remains a challenging task. Existing fusing strategies ignore the structure diversity among anchor graphs and restrict the anchor generation to be same on different views, which degenerates the representation ability of corresponding fused consensus graph. To overcome these drawbacks, we propose a novel structural fusion framework to integrate the multi-view anchor graphs for clustering. Different from traditional integration strategies, we merge the anchors and edges of all the view-specific anchor graphs into a single graph for the structural optimal graph learning. Benefiting from the structural fusion strategy, the anchor generation of each view is not forced to be same, which greatly improves the representation capability of the target structural optimal graph, since the anchors of each view capture the diverse structure of different views. By leveraging the potential structural consistency among each anchor graph, a connectivity constraint is imposed on the target graph to indicate clusters directly without any post-processing such as k -means in classical spectral clustering. Substantial experiments on real-world datasets are conducted to verify the superiority of the proposed method, as compared with the state-of-the-arts over the clustering performance and time expenditure.},
  archive      = {J_TKDD},
  doi          = {10.1145/3534931},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Structure diversity-induced anchor graph fusion for multi-view clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On equivalence of anomaly detection algorithms.
<em>TKDD</em>, <em>17</em>(2), 1–26. (<a
href="https://doi.org/10.1145/3536428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most domains, anomaly detection is typically cast as an unsupervised learning problem because of the infeasibility of labeling large datasets. In this setup, the evaluation and comparison of different anomaly detection algorithms is difficult. Although some work has been published in this field, they fail to account that different algorithms can detect different kinds of anomalies. More precisely, the literature on this topic has focused on defining criteria to determine which algorithm is better, while ignoring the fact that such criteria are meaningful only if the algorithms being compared are detecting the same kind of anomalies. Therefore, in this article, we propose an equivalence criterion for anomaly detection algorithms that measures to what degree two anomaly detection algorithms detect the same kind of anomalies. First, we lay out a set of desirable properties that such an equivalence criterion should have and why; second, we propose Gaussian Equivalence Criterion (GEC) as equivalence criterion and show mathematically that it has the desirable properties previously mentioned. Finally, we empirically validate these properties using a simulated and a real-world dataset. For the real-world dataset, we show how GEC can provide insight about the anomaly detection algorithms as well as the dataset.},
  archive      = {J_TKDD},
  doi          = {10.1145/3536428},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {On equivalence of anomaly detection algorithms},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distance-preserving embedding adaptive bipartite graph
multi-view learning with application to multi-label classification.
<em>TKDD</em>, <em>17</em>(2), 1–21. (<a
href="https://doi.org/10.1145/3537900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based multi-view learning has attracted much attention due to the efficacy of fusing the information from different views. However, most of them exhibit high computational complexity. We propose an anchor-based bipartite graph embedding approach to accelerate the learning process. Specifically, different from existing anchor-based methods where anchors are obtained from key samples by clustering or weighted averaging strategies, in this article, the anchors are learned in a principled fashion which aims at constructing a distance-preserving embedding for each view from samples to their representations, whose elements are the weights of the edges linking corresponding samples and anchors. In addition, the consistency among different views can be explored by imposing a low-rank constraint on the concatenated embedding representations. We further design a concise yet effective feature collinearity guided feature selection scheme to learn tight multi-label classifiers. The objective function is optimized in an alternating optimization fashion. Both theoretical analysis and experimental results on different multi-label image datasets verify the effectiveness and efficiency of the proposed method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3537900},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Distance-preserving embedding adaptive bipartite graph multi-view learning with application to multi-label classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable embedding and visualization of compressed
data. <em>TKDD</em>, <em>17</em>(2), 1–22. (<a
href="https://doi.org/10.1145/3537901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional embedding methodologies, also known as dimensionality reduction techniques, assume the availability of exact pairwise distances between the high-dimensional objects that will be embedded in a lower dimensionality. In this article, we propose an embedding that overcomes this limitation and can operate on pairwise distances that are represented as a range of lower and upper bounds. Such bounds are typically estimated when objects are compressed in a lossy manner, so our approach is highly applicable in the case of big compressed datasets. Our methodology can preserve multiple aspects of the original data relationships: distances, correlations, and object scores/ranks, whereas existing techniques typically preserve only distances. Comparative experiments with prevalent embedding methodologies (ISOMAP, t-SNE, MDS, UMAP) illustrate that our approach can provide fidelitous preservation of multiple object relationships, even in the presence of inexact distance information. Our visualization method is also easily interpretable.},
  archive      = {J_TKDD},
  doi          = {10.1145/3537901},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Interpretable embedding and visualization of compressed data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-information fusion of hierarchical semantics dependency
and graph structure for structured text classification. <em>TKDD</em>,
<em>17</em>(2), 1–18. (<a
href="https://doi.org/10.1145/3537971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured text with plentiful hierarchical structure information is an important part in real-world complex texts. Structured text classification is attracting more attention in natural language processing due to the increasing complexity of application scenarios. Most existing methods treat structured text from a local hierarchy perspective, focusing on the semantics dependency and the graph structure of the structured text independently. However, structured text has global hierarchical structures with sophisticated dependency when compared to unstructured text. According to the variety of structured texts, it is not appropriate to use the existing methods directly. The function of distinction information within semantics dependency and graph structure for structured text, referred to as meta-information, should be stated more precisely. In this article, we propose HGMETA, a novel meta-information embedding frame network for structured text classification, to obtain the fusion embedding of hierarchical semantics dependency and graph structure in a structured text, and to distill the meta-information from fusion characteristics. To integrate the global hierarchical features with fused structured text information, we design a hierarchical LDA module and a structured text embedding module. Specially, we employ a multi-hop message passing mechanism to explicitly incorporate complex dependency into a meta-graph. The meta-information is constructed from meta-graph via neighborhood-based propagation to distill redundant information. Furthermore, using an attention-based network, we investigate the complementarity of semantics dependency and graph structure based on global hierarchical characteristics and meta-information. Finally, the fusion embedding and the meta-information can be straightforwardly incorporated for structured text classification. Experiments conducted on three real-world datasets show the effectiveness of meta-information and demonstrate the superiority of our method.},
  archive      = {J_TKDD},
  doi          = {10.1145/3537971},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-18},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Meta-information fusion of hierarchical semantics dependency and graph structure for structured text classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel graph indexing approach for uncovering potential
COVID-19 transmission clusters. <em>TKDD</em>, <em>17</em>(2), 1–24. (<a
href="https://doi.org/10.1145/3538492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has caused the society lockdowns and a large number of deaths in many countries. Potential transmission cluster discovery is to find all suspected users with infections, which is greatly needed to fast discover virus transmission chains so as to prevent an outbreak of COVID-19 as early as possible. In this article, we study the problem of potential transmission cluster discovery based on the spatio-temporal logs. Given a query of patient user q and a timestamp of confirmed infection t q , the problem is to find all potential infected users who have close social contacts to user q before time t q . We motivate and formulate the potential transmission cluster model, equipped with a detailed analysis of transmission cluster property and particular model usability. To identify potential clusters, one straightforward method is to compute all close contacts on-the-fly, which is simple but inefficient caused by scanning spatio-temporal logs many times. To accelerate the efficiency, we propose two indexing algorithms by constructing a multigraph index and an advanced BCG-index. Leveraging two well-designed techniques of spatio-temporal compression and graph partition on bipartite contact graphs, our BCG-index approach achieves a good balance of index construction and online query processing to fast discover potential transmission cluster. We theoretically analyze and compare the algorithm complexity of three proposed approaches. Extensive experiments on real-world check-in datasets and COVID-19 confirmed cases in the United States validate the effectiveness and efficiency of our potential transmission cluster model and algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3538492},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-24},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A novel graph indexing approach for uncovering potential COVID-19 transmission clusters},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative multi-label correlation learning. <em>TKDD</em>,
<em>17</em>(2), 1–19. (<a
href="https://doi.org/10.1145/3538708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, a single instance could have more than one label. To solve this task, multi-label learning methods emerged in recent years. It is a more challenging problem for many reasons, such as complex label correlation, long-tail label distribution, and data shortage. In general, overcoming these challenges and bettering learning performance could be achieved by utilizing more training samples and including label correlations. However, these solutions are expensive and inflexible. Large-scale, well-labeled datasets are difficult to obtain, and building label correlation maps requires task-specific semantic information as prior knowledge. To address these limitations, we propose a general and compact Multi-Label Correlation Learning (MUCO) framework. MUCO explicitly and effectively learns the latent label correlations by updating a label correlation tensor, which provides highly accurate and interpretable prediction results. In addition, a multi-label generative strategy is deployed to handle the long-tail label distribution challenge. It borrows the visual clues from limited samples and synthesizes more diverse samples. All networks in our model are optimized simultaneously. Extensive experiments illustrate the effectiveness and efficiency of MUCO. Ablation studies further prove the effectiveness of all the modules.},
  archive      = {J_TKDD},
  doi          = {10.1145/3538708},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Generative multi-label correlation learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stratification of children with autism spectrum disorder
through fusion of temporal information in eye-gaze scan-paths.
<em>TKDD</em>, <em>17</em>(2), 1–20. (<a
href="https://doi.org/10.1145/3539226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Looking pattern differences are shown to separate individuals with Autism Spectrum Disorder (ASD) and Typically Developing (TD) controls. Recent studies have shown that, in children with ASD, these patterns change with intellectual and social impairments, suggesting that patterns of social attention provide indices of clinically meaningful variation in ASD. Method: We conducted a naturalistic study of children with ASD (n = 55) and typical development (TD, n = 32). A battery of eye-tracking video stimuli was used in the study, including Activity Monitoring (AM), Social Referencing (SR), Theory of Mind (ToM), and Dyadic Bid (DB) tasks. This work reports on the feasibility of spatial and spatiotemporal scanpaths generated from eye-gaze patterns of these paradigms in stratifying ASD and TD groups. Algorithm: This article presents an approach for automatically identifying clinically meaningful information contained within the raw eye-tracking data of children with ASD and TD. The proposed mechanism utilizes combinations of eye-gaze scan-paths (spatial information), fused with temporal information and pupil velocity data and Convolutional Neural Network (CNN) for stratification of diagnosis (ASD or TD). Results: Spatial eye-gaze representations in the form of scanpaths in stratifying ASD and TD (ASD vs. TD: DNN: 74.4%) are feasible. These spatial eye-gaze features, e.g., scan-paths, are shown to be sensitive to factors mediating heterogeneity in ASD: age (ASD: 2–4 y/old vs. 10–17 y/old CNN: 80.5%), gender (Male vs. Female ASD: DNN: 78.0%) and the mixture of age and gender (5–9 y/old Male vs. 5–9 y/old Female ASD: DNN:98.8%). Limiting scan-path representations temporally increased variance in stratification performance, attesting to the importance of the temporal dimension of eye-gaze data. Spatio-Temporal scan-paths that incorporate velocity of eye movement in their images of eye-gaze are shown to outperform other feature representation methods achieving classification accuracy of 80.25%. Conclusion: The results indicate the feasibility of scan-path images to stratify ASD and TD diagnosis in children of varying ages and gender. Infusion of temporal information and velocity data improves the classification performance of our deep learning models. Such novel velocity fused spatio-temporal scan-path features are shown to be able to capture eye gaze patterns that reflect age, gender, and the mixed effect of age and gender, factors that are associated with heterogeneity in ASD and difficulty in identifying robust biomarkers for ASD.},
  archive      = {J_TKDD},
  doi          = {10.1145/3539226},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Stratification of children with autism spectrum disorder through fusion of temporal information in eye-gaze scan-paths},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised contrastive learning for interpretable long-form
document matching. <em>TKDD</em>, <em>17</em>(2), 1–17. (<a
href="https://doi.org/10.1145/3542822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning techniques have transformed the area of semantic text matching (STM). However, most state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, and so on. These models have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) the presence of different contexts for the same word throughout the document, (ii) small sections of contextually similar text between two documents, but dissimilar text in the remaining parts (this defies the basic understanding of “similarity”), and (iii) the coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this article, we describe CoLDE : Co ntrastive L ong D ocument E ncoder—a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a supervised contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and across other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document matching task, CoLDE is also robust to changes in document length and text perturbations and provides interpretable results. The code for the proposed model is publicly available at https://github.com/InterDigitalInc/CoLDE .},
  archive      = {J_TKDD},
  doi          = {10.1145/3542822},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-17},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Supervised contrastive learning for interpretable long-form document matching},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph deep factors for probabilistic time-series
forecasting. <em>TKDD</em>, <em>17</em>(2), 1–30. (<a
href="https://doi.org/10.1145/3543511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective time-series forecasting methods are of significant importance to solve a broad spectrum of research problems. Deep probabilistic forecasting techniques have recently been proposed for modeling large collections of time-series. However, these techniques explicitly assume either complete independence (local model) or complete dependence (global model) between time-series in the collection. This corresponds to the two extreme cases where every time-series is disconnected from every other time-series in the collection or likewise, that every time-series is related to every other time-series resulting in a completely connected graph. In this work, we propose a deep hybrid probabilistic graph-based forecasting framework called Graph Deep Factors (GraphDF) that goes beyond these two extremes by allowing nodes and their time-series to be connected to others in an arbitrary fashion. GraphDF is a hybrid forecasting framework that consists of a relational global and relational local model. In particular, a relational global model learns complex non-linear time-series patterns globally using the structure of the graph to improve both forecasting accuracy and computational efficiency. Similarly, instead of modeling every time-series independently, a relational local model not only considers its individual time-series but also the time-series of nodes that are connected in the graph. The experiments demonstrate the effectiveness of the proposed deep hybrid graph-based forecasting model compared to the state-of-the-art methods in terms of its forecasting accuracy, runtime, and scalability. Our case study reveals that GraphDF can successfully generate cloud usage forecasts and opportunistically schedule workloads to increase cloud cluster utilization by 47.5% on average. Furthermore, we target addressing the common nature of many time-series forecasting applications where time-series are provided in a streaming version; however, most methods fail to leverage the newly incoming time-series values and result in worse performance over time. In this article, we propose an online incremental learning framework for probabilistic forecasting. The framework is theoretically proven to have lower time and space complexity. The framework can be universally applied to many other machine learning-based methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3543511},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Graph deep factors for probabilistic time-series forecasting},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On dynamically pricing crowdsourcing tasks. <em>TKDD</em>,
<em>17</em>(2), 1–27. (<a
href="https://doi.org/10.1145/3544018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourcing techniques have been extensively explored in the past decade, including task allocation, quality assessment, and so on. Most of professional crowdsourcing platforms adopt the fixed pricing scheme to offer a fixed price for crowd tasks. It is neither incentive for crowd workers to produce good performance, nor profitable for the requester to gain high utility with low budget. In this article, we study the problem of pricing crowdsourcing tasks with optional bonuses. We propose a dynamic pricing mechanism, named CrowdPricer for incentively delivering bonuses to the crowd workers of completing tasks, in addition to offering a base payment for completing a task. We leverage a deep time sequence model to learn the effect of bonuses on workers’ quality for crowd tasks. CrowdPricer makes decisions on whether to provide bonuses on workers, so as to maximize the requester’s utility in expectation. We present an efficient bonus delivery algorithm under the help of beam search technique, in order to efficiently solve the decision making problem. Extensive experiments using both a real crowdsourcing platform and simulations demonstrate that CrowdPricer yields the higher utility for the requester. It also obtains more correct crowd answers than the state-of-the-art pricing methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3544018},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {2},
  pages        = {1-27},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {On dynamically pricing crowdsourcing tasks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-representation method with local similarity
preserving for fast multi-view outlier detection. <em>TKDD</em>,
<em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3532191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapidly growing attention to multi-view data in recent years, multi-view outlier detection has become a rising field with intense research. These researches have made some success, but still exist some issues that need to be solved. First, many multi-view outlier detection methods can only handle datasets that conform to the cluster structure but are powerless for complex data distributions such as manifold structures. This overly restrictive data assumption limits the applicability of these methods. In addition, almost the majority of multi-view outlier detection algorithms cannot solve the online detection problem of multi-view outliers. To address these issues, we propose a new detection method based on the local similarity relation and data reconstruction, i.e., the Self-Representation Method with Local Similarity Preserving for fast multi-view outlier detection (SRLSP). By using the local similarity structure, the proposed method fully utilizes the characteristics of outliers and detects outliers with an applicable objective function. Besides, a well-designed optimization algorithm is proposed, which completes each iteration with linear time complexity and can calculate each instance parallelly. Also, the optimization algorithm can be easily extended to the online version, which is more suitable for practical production environments. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method on both performance and time complexity.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532191},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {3},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A self-representation method with local similarity preserving for fast multi-view outlier detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning aspect-aware high-order representations from
ratings and reviews for recommendation. <em>TKDD</em>, <em>17</em>(1),
1–22. (<a href="https://doi.org/10.1145/3532188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textual reviews contain rich semantic information that is useful for making better recommendation, as such semantic information may indicate more fine-grained preferences of users. Recent efforts make considerable improvement on recommendation by integrating textual reviews in rating-based recommendations. However, there still exist major challenges on integrating textual reviews for recommendation. On the one hand, most existing works focus on learning a single representation from reviews but ignoring complex relations between users (or items) and reviews, which may fail to capture user preferences and item attributes together. On the other hand, these works independently learn latent representations from ratings and reviews while omitting correlations between rating-based features and review-based features, which may harm recommendation performance. In this article, we capture the aspect-aware relations by constructing heterogeneous graphs from reviews. Furthermore, we propose a new recommendation model, namely AHOR, to jointly distill rating-based features and review-based features, which are derived from ratings and reviews, respectively. To explore the multi-hop connectivity information between users, items, and aspects, a novel graph neural network is introduced to learn aspect-aware high-order representations. Experiments based on public datasets show that our approach outperforms state-of-the-art methods. We also provide detailed analysis on the high-order signals and the aspect importance to show the interpretability of our proposed model.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532188},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Learning aspect-aware high-order representations from ratings and reviews for recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrating global and local feature selection for
multi-label learning. <em>TKDD</em>, <em>17</em>(1), 1–37. (<a
href="https://doi.org/10.1145/3532190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning deals with the problem where an instance is associated with multiple labels simultaneously. Multi-label data is often of high dimensionality and has many noisy, irrelevant, and redundant features. As an important machine learning task, multi-label feature selection has received considerable attention in recent years due to its promising performance in dealing with high-dimensional multi-label data. Existing multi-label feature selection methods typically select the global features which are shared by all instances in a dataset. However, these multi-label feature selection methods may be suboptimal since they do not consider the specific characteristics of instances. In this paper, we propose a novel algorithm that integrates Global and Local Feature Selection (GLFS) to exploit both the global features and a subset of discriminative features shared only locally by a subgroup of instances in a multi-label dataset. Specifically, GLFS employs linear regression and ℓ 2,1 -norm on the regression parameters to achieve simultaneous global and local feature selection. Moreover, the proposed algorithm has an effective mechanism for utilizing label correlations to improve the feature selection. Experiments on real-world multi-label datasets show the superiority of GLFS over the state-of-the-art multi-label feature selection methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532190},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-37},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Integrating global and local feature selection for multi-label learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling cross-session information with multi-interest graph
neural networks for the next-item recommendation. <em>TKDD</em>,
<em>17</em>(1), 1–28. (<a
href="https://doi.org/10.1145/3532192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-item recommendation involves predicting the next item of interest of a given user from their past behavior. Users tend to browse and purchase various items on e-commerce websites according to their varied interests and needs, as reflected in their purchasing history. Most existing next-item recommendation methods aim at extracting the main point of interest in each browsing session and encapsulate it in a single representation. However, past behavior sequences reflect the multiple interests of a single user, which cannot be captured by methods that focus on single-interest contexts. Indeed, multiple interests cannot be captured in a single representation, and doing so results in missing information. Therefore, we propose a model with a multi-interest structure for capturing the various interests of users from their behavior sequence. Moreover, we adopted a method based on a graph neural network to construct interest graphs based on the historical and current behavior sequences of users. These graphs can capture complex item transition patterns related to different interests. In experiments, the proposed method outperforms state-of-the-art session-based recommendation systems on three real-world datasets, achieving 4% improvement of Recall over the SOTAs on Jdata dataset.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532192},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-28},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Modeling cross-session information with multi-interest graph neural networks for the next-item recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SigGAN: Adversarial model for learning signed relationships
in networks. <em>TKDD</em>, <em>17</em>(1), 1–20. (<a
href="https://doi.org/10.1145/3532610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signed link prediction in graphs is an important problem that has applications in diverse domains. It is a binary classification problem that predicts whether an edge between a pair of nodes is positive or negative. Existing approaches for link prediction in unsigned networks cannot be directly applied for signed link prediction due to their inherent differences. Furthermore, signed link prediction must consider the inherent characteristics of signed networks, such as structural balance theory. Recent signed link prediction approaches generate node representations using either generative models or discriminative models. Inspired by the recent success of Generative Adversarial Network (GAN) based models in several applications, we propose a GAN based model for signed networks, SigGAN. It considers the inherent characteristics of signed networks, such as integration of information from negative edges, high imbalance in number of positive and negative edges, and structural balance theory. Comparing the performance with state-of-the-art techniques on five real-world datasets validates the effectiveness of SigGAN.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532610},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-20},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {SigGAN: Adversarial model for learning signed relationships in networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic graph convolutional recurrent network for traffic
prediction: Benchmark and solution. <em>TKDD</em>, <em>17</em>(1), 1–21.
(<a href="https://doi.org/10.1145/3532611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is the cornerstone of intelligent transportation system. Accurate traffic forecasting is essential for the applications of smart cities, i.e., intelligent traffic management and urban planning. Although various methods are proposed for spatio-temporal modeling, they ignore the dynamic characteristics of correlations among locations on road network. Meanwhile, most Recurrent Neural Network based works are not efficient enough due to their recurrent operations. Additionally, there is a severe lack of fair comparison among different methods on the same datasets. To address the above challenges, in this article, we propose a novel traffic prediction framework, named Dynamic Graph Convolutional Recurrent Network (DGCRN). In DGCRN, hyper-networks are designed to leverage and extract dynamic characteristics from node attributes, while the parameters of dynamic filters are generated at each time step. We filter the node embeddings and then use them to generate dynamic graph, which is integrated with pre-defined static graph. As far as we know, we are first to employ a generation method to model fine topology of dynamic graph at each time step. Furthermore, to enhance efficiency and performance, we employ a training strategy for DGCRN by restricting the iteration number of decoder during forward and backward propagation. Finally, a reproducible standardized benchmark and a brand new representative traffic dataset are opened for fair comparison and further research. Extensive experiments on three datasets demonstrate that our model outperforms 15 baselines consistently. Source codes are available at https://github.com/tsinghua-fib-lab/Traffic-Benchmark .},
  archive      = {J_TKDD},
  doi          = {10.1145/3532611},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Dynamic graph convolutional recurrent network for traffic prediction: Benchmark and solution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individuality meets commonality: A unified graph learning
framework for multi-view clustering. <em>TKDD</em>, <em>17</em>(1),
1–21. (<a href="https://doi.org/10.1145/3532612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which aims at boosting the clustering performance by leveraging the individual information and the common information of multi-view data, has gained extensive consideration in recent years. However, most existing multi-view clustering algorithms either focus on extracting the multi-view individuality or emphasize on exploring the multi-view commonality, neither of which can fully utilize the comprehensive information from multiple views. To this end, we propose a novel algorithm named V iew-specific and C onsensus G raph A lignment (VCGA) for multi-view clustering, which simultaneously formulates the multi-view individuality and the multi-view commonality into a unified framework to effectively partition data points. To be specific, the VCGA model constructs the view-specific graphs and the shared graph from original multi-view data and hidden latent representation, respectively. Furthermore, the view-specific graphs of different views and the consensus graph are aligned into an informative target graph, which is employed as a crucial input to the standard spectral clustering method for clustering. Extensive experimental results on six benchmark datasets demonstrate the superiority of our method against other state-of-the-art clustering algorithms.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532612},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-21},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Individuality meets commonality: A unified graph learning framework for multi-view clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). US-rule: Discovering utility-driven sequential rules.
<em>TKDD</em>, <em>17</em>(1), 1–22. (<a
href="https://doi.org/10.1145/3532613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utility-driven mining is an important task in data science and has many applications in real life. High-utility sequential pattern mining (HUSPM) is one kind of utility-driven mining. It aims at discovering all sequential patterns with high utility. However, the existing algorithms of HUSPM can not provide a relatively accurate probability to deal with some scenarios for prediction or recommendation. High-utility sequential rule mining (HUSRM) is proposed to discover all sequential rules with high utility and high confidence. There is only one algorithm proposed for HUSRM, which is not efficient enough. In this article, we propose a faster algorithm called US-Rule, to efficiently mine high-utility sequential rules. It utilizes the rule estimated utility co-occurrence pruning strategy (REUCP) to avoid meaningless computations. Moreover, to improve its efficiency on dense and long sequence datasets, four tighter upper bounds (LEEU, REEU, LERSU, and RERSU) and corresponding pruning strategies (LEEUP, REEUP, LERSUP, and RERSUP) are designed. US-Rule also proposes the rule estimated utility recomputing pruning strategy (REURP) to deal with sparse datasets. Finally, a large number of experiments on different datasets compared to the state-of-the-art algorithm demonstrate that US-Rule can achieve better performance in terms of execution time, memory consumption, and scalability.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532613},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {US-rule: Discovering utility-driven sequential rules},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient aggregation method for the symbolic
representation of temporal data. <em>TKDD</em>, <em>17</em>(1), 1–22.
(<a href="https://doi.org/10.1145/3532622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbolic representations are a useful tool for the dimension reduction of temporal data, allowing for the efficient storage of and information retrieval from time series. They can also enhance the training of machine learning algorithms on time series data through noise reduction and reduced sensitivity to hyperparameters. The adaptive Brownian bridge-based aggregation (ABBA) method is one such effective and robust symbolic representation, demonstrated to accurately capture important trends and shapes in time series. However, in its current form, the method struggles to process very large time series. Here, we present a new variant of the ABBA method, called fABBA. This variant utilizes a new aggregation approach tailored to the piecewise representation of time series. By replacing the k-means clustering used in ABBA with a sorting-based aggregation technique, and thereby avoiding repeated sum-of-squares error computations, the computational complexity is significantly reduced. In contrast to the original method, the new approach does not require the number of time series symbols to be specified in advance. Through extensive tests, we demonstrate that the new method significantly outperforms ABBA with a considerable reduction in runtime while also outperforming the popular SAX and 1d-SAX representations in terms of reconstruction accuracy. We further demonstrate that fABBA can compress other data types such as images.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532622},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-22},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {An efficient aggregation method for the symbolic representation of temporal data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised graph pattern matching and rematching for
expert community location. <em>TKDD</em>, <em>17</em>(1), 1–26. (<a
href="https://doi.org/10.1145/3532623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph pattern matching (GPM) is widely used in social network analysis, such as expert finding, social group query, and social position detection. Technically, GPM is to find matched subgraphs that meet the requirements of pattern graphs in big social networks. In the application of expert community location, the nodes in the pattern graph and data graph represent expert entities, and the edges represent previous cooperations between them. However, the existing GPM methods focus on shortening the matching time and without considering the preference of the decision maker (DM), which makes it difficult for the DM to find ideal teams from numerous matches to complete the assigned task. In this article, as for the process of graph pattern matching and rematching, with a preferred expert set, i.e., the DM hopes that one or more experts in this set will appear in matched subgraphs, we propose a Dual Simulation-based Edge Sequencing-oriented Semi-Supervised GPM method (DsEs-ssGPM). In addition, considering a preferred expert set and a dispreferred expert set together, the DM hopes that experts in the dispreferred expert set will not appear in final matches, so we have the DsEs-ssGPM+ method. Technically, these DsEs-ssGPM methods conduct the matching process from the preferred expert set during dual simulation-based edge sequencing, and based on the edge sequence, these edges are searched recursively. Especially, as for the rematching process, when the preferred and/or the dispreferred expert sets change continuously, to process the GPM again is unnecessary and it is possible to revise the previous matched results partially with DsEs-ssGPM methods. Experiments on four large datasets demonstrate the effectiveness, efficiency and stability of our proposed DsEs-ssGPM methods, and the necessity of introducing an edge sequencing mechanism.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532623},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-26},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Semi-supervised graph pattern matching and rematching for expert community location},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on deep hashing methods. <em>TKDD</em>,
<em>17</em>(1), 1–50. (<a
href="https://doi.org/10.1145/3532624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearest neighbor search aims at obtaining the samples in the database with the smallest distances from them to the queries, which is a basic task in a range of fields, including computer vision and data mining. Hashing is one of the most widely used methods for its computational and storage efficiency. With the development of deep learning, deep hashing methods show more advantages than traditional methods. In this survey, we detailedly investigate current deep hashing algorithms including deep supervised hashing and deep unsupervised hashing. Specifically, we categorize deep supervised hashing methods into pairwise methods, ranking-based methods, pointwise methods as well as quantization according to how measuring the similarities of the learned hash codes. Moreover, deep unsupervised hashing is categorized into similarity reconstruction-based methods, pseudo-label-based methods, and prediction-free self-supervised learning-based methods based on their semantic learning manners. We also introduce three related important topics including semi-supervised deep hashing, domain adaption deep hashing, and multi-modal deep hashing. Meanwhile, we present some commonly used public datasets and the scheme to measure the performance of deep hashing algorithms. Finally, we discuss some potential research directions in conclusion.},
  archive      = {J_TKDD},
  doi          = {10.1145/3532624},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-50},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {A survey on deep hashing methods},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised graph-based entity resolution for complex
entities. <em>TKDD</em>, <em>17</em>(1), 1–30. (<a
href="https://doi.org/10.1145/3533016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity resolution (ER) is the process of linking records that refer to the same entity. Traditionally, this process compares attribute values of records to calculate similarities and then classifies pairs of records as referring to the same entity or not based on these similarities. Recently developed graph-based ER approaches combine relationships between records with attribute similarities to improve linkage quality. Most of these approaches only consider databases containing basic entities that have static attribute values and static relationships, such as publications in bibliographic databases. In contrast, temporal record linkage addresses the problem where attribute values of entities can change over time. However, neither existing graph-based ER nor temporal record linkage can achieve high linkage quality on databases with complex entities , where an entity (such as a person) can change its attribute values over time while having different relationships with other entities at different points in time. In this article, we propose an unsupervised graph-based ER framework that is aimed at linking records of complex entities. Our framework provides five key contributions. First, we propagate positive evidence encountered when linking records to use in subsequent links by propagating attribute values that have changed. Second, we employ negative evidence by applying temporal and link constraints to restrict which candidate record pairs to consider for linking. Third, we leverage the ambiguity of attribute values to disambiguate similar records that, however, belong to different entities. Fourth, we adaptively exploit the structure of relationships to link records that have different relationships. Fifth, using graph measures, we refine matched clusters of records by removing likely wrong links between records. We conduct extensive experiments on seven real-world datasets from different domains showing that on average our unsupervised graph-based ER framework can improve precision by up to 25% and recall by up to 29% compared to several state-of-the-art ER techniques.},
  archive      = {J_TKDD},
  doi          = {10.1145/3533016},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-30},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Unsupervised graph-based entity resolution for complex entities},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-concept representation learning for knowledge graph
completion. <em>TKDD</em>, <em>17</em>(1), 1–19. (<a
href="https://doi.org/10.1145/3533017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge Graph Completion (KGC) aims at inferring missing entities or relations by embedding them in a low-dimensional space. However, most existing KGC methods generally fail to handle the complex concepts hidden in triplets, so the learned embeddings of entities or relations may deviate from the true situation. In this article, we propose a novel M ulti- c oncept R epresentation L earning (McRL) method for the KGC task, which mainly consists of a multi-concept representation module, a deep residual attention module, and an interaction embedding module. Specifically, instead of the single-feature representation, the multi-concept representation module projects each entity or relation to multiple vectors to capture the complex conceptual information hidden in them. The deep residual attention module simultaneously explores the inter- and intra-connection between entities and relations to enhance the entity and relation embeddings corresponding to the current contextual situation. Moreover, the interaction embedding module further weakens the noise and ambiguity to obtain the optimal and robust embeddings. We conduct the link prediction experiment to evaluate the proposed method on several standard datasets, and experimental results show that the proposed method outperforms existing state-of-the-art KGC methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3533017},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-19},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Multi-concept representation learning for knowledge graph completion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainability-based mix-up approach for text data
augmentation. <em>TKDD</em>, <em>17</em>(1), 1–14. (<a
href="https://doi.org/10.1145/3533048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text augmentation is a strategy for increasing the diversity of training examples without explicitly collecting new data. Owing to the efficiency and effectiveness of text augmentation, numerous augmentation methodologies have been proposed. Among them, the method based on modification, particularly the mix-up method of swapping words between two or more sentences, is widely used because it can be applied simply and shows good levels of performance. However, the existing mix-up approaches are limited; they do not reflect the importance of the manipulated word. That is, even if a word that has a critical effect on the classification result is manipulated, it is not considered significant in labeling the augmented data. Therefore, in this study, we propose an effective text augmentation technique that explicitly derives the importance of manipulated words and reflects this importance in the labeling of augmented data. The importance of each word, in other words, explainability, is calculated, and this is explicitly reflected in the labeling process of the augmented data. The results of the experiment confirmed that when the importance of the manipulated word was reflected in the labeling, the performance was significantly higher than that of the existing methods.},
  archive      = {J_TKDD},
  doi          = {10.1145/3533048},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Explainability-based mix-up approach for text data augmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Be causal: De-biasing social network confounding in
recommendation. <em>TKDD</em>, <em>17</em>(1), 1–23. (<a
href="https://doi.org/10.1145/3533725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommendation systems, the existence of the missing-not-at-random (MNAR) problem results in the selection bias issue, degrading the recommendation performance ultimately. A common practice to address MNAR is to treat missing entries from the so-called “exposure” perspective, i.e., modeling how an item is exposed (provided) to a user. Most of the existing approaches use heuristic models or re-weighting strategy on observed ratings to mimic the missing-at-random setting. However, little research has been done to reveal how the ratings are missing from a causal perspective. To bridge the gap, we propose an unbiased and robust method called DENC ( De-Bias Network Confounding in Recommendation ), inspired by confounder analysis in causal inference. In general, DENC provides a causal analysis on MNAR from both the inherent factors (e.g., latent user or item factors) and auxiliary network’s perspective. Particularly, the proposed exposure model in DENC can control the social network confounder meanwhile preserve the observed exposure information. We also develop a deconfounding model through the balanced representation learning to retain the primary user and item features, which enables DENC generalize well on the rating prediction. Extensive experiments on three datasets validate that our proposed model outperforms the state-of-the-art baselines.},
  archive      = {J_TKDD},
  doi          = {10.1145/3533725},
  journal      = {ACM Transactions on Knowledge Discovery from Data},
  month        = {2},
  number       = {1},
  pages        = {1-23},
  shortjournal = {ACM Trans. knowl. Discov. Data},
  title        = {Be causal: De-biasing social network confounding in recommendation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
