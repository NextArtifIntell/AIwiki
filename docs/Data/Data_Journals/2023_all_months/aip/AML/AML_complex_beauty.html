<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AML_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aml---72">AML - 72</h2>
<ul>
<li><details>
<summary>
(2023). Seizure detection using dynamic memristor-based reservoir
computing and leaky integrate-and-fire neuron for post-processing.
<em>AML</em>, <em>1</em>(4), 046123. (<a
href="https://doi.org/10.1063/5.0171274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a prevalent neurological disorder, rendering the development of automated seizure detection systems imperative. While complex machine learning models are powerful, their training and hardware deployment remain challenging. The reservoir computing system offers a low-cost solution in terms of both hardware requirements and training. In this paper, we introduce a compact reservoir computing system for seizure detection, based on the Î±-In 2 Se 3 dynamic memristors. Leaky integrate-and-fire neurons are used for post-processing the output of the system, and experimental results indicate their effectiveness in suppressing erroneous outputs, where both accuracy and specificity are enhanced by over 2.5\%. The optimized compact reservoir system achieves 96.40\% accuracy, 86.34\% sensitivity, and 96.56\% specificity in seizure detection tasks. This work demonstrates the feasibility of using reservoir computing for seizure detection and shows its potential for future application in extreme edge devices.},
  archive      = {J_AML},
  author       = {Yang, Zhiyu and Liu, Keqin and Yuan, Rui and Wu, Xulei and Cai, Lei and Zhang, Teng and Tao, Yaoyu and Jin, Yufeng and Yang, Yuchao},
  doi          = {10.1063/5.0171274},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046123},
  shortjournal = {APL Mach. Learn.},
  title        = {Seizure detection using dynamic memristor-based reservoir computing and leaky integrate-and-fire neuron for post-processing},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experiment-based deep learning approach for power allocation
with a programmable metasurface. <em>AML</em>, <em>1</em>(4), 046122.
(<a href="https://doi.org/10.1063/5.0184328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metasurfaces designed with deep learning approaches have emerged as efficient tools for manipulating electromagnetic waves to achieve beam steering and power allocation objectives. However, the effects of complex environmental factors like obstacle blocking and other unavoidable scattering need to be sufficiently considered for practical applications. In this work, we employ an experiment-based deep learning approach for programmable metasurface design to control powers delivered to specific locations generally with obstacle blocking. Without prior physical knowledge of the complex system, large sets of experimental data can be efficiently collected with a programmable metasurface to train a deep neural network (DNN). The experimental data can inherently incorporate complex factors that are difficult to include if only simulation data are used for training. Moreover, the DNN can be updated by collecting new experimental data on-site to adapt to changes in the environment. Our proposed experiment-based DNN demonstrates significant potential for intelligent wireless communication, imaging, sensing, and quiet-zone control for practical applications.},
  archive      = {J_AML},
  author       = {Zhang, Jingxin and Xi, Jiawei and Li, Peixing and Cheung, Ray C. C. and Wong, Alex M. H. and Li, Jensen},
  doi          = {10.1063/5.0184328},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046122},
  shortjournal = {APL Mach. Learn.},
  title        = {Experiment-based deep learning approach for power allocation with a programmable metasurface},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibration in machine learning uncertainty quantification:
Beyond consistency to target adaptivity. <em>AML</em>, <em>1</em>(4),
046121. (<a href="https://doi.org/10.1063/5.0174943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods for testing the conditional calibration with respect to uncertainty , i.e., consistency . Consistency is assessed mostly by so-called reliability diagrams. There exists, however, another way beyond average calibration, which is conditional calibration with respect to input features , i.e., adaptivity . In practice, adaptivity is the main concern of the final users of the ML-UQ method, seeking the reliability of predictions and uncertainties for any point in the feature space. This article aims to show that consistency and adaptivity are complementary validation targets and that good consistency does not imply good adaptivity. An integrated validation framework is proposed and illustrated with a representative example.},
  archive      = {J_AML},
  author       = {Pernot, Pascal},
  doi          = {10.1063/5.0174943},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046121},
  shortjournal = {APL Mach. Learn.},
  title        = {Calibration in machine learning uncertainty quantification: Beyond consistency to target adaptivity},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective generative design of three-dimensional
material structures. <em>AML</em>, <em>1</em>(4), 046120. (<a
href="https://doi.org/10.1063/5.0169432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative design for materials has recently gained significant attention due to the rapid evolution of generative deep learning models. There have been a few successful generative design demonstrations of molecular-level structures with the help of graph neural networks. However, in the realm of macroscale material structures, most of the works are targeting two-dimensional, ungoverned structure generations. Hindered by the complexity of 3D structures, it is hard to extract customized structures with multiple desired properties from a large, unexplored design space. Here we report a novel framework, a multi-objective driven Wasserstein generative adversarial network (WGAN), to implement inverse designs of 3D structures according to given geometrical, structural, and mechanical requirements. Our framework consists of a WGAN-based network that generates 3D structures possessing geometrical and structural features learned from the target dataset. Besides, multiple objectives are introduced to our framework for the control of mechanical property and isotropy of the structures. An accurate surrogate model is incorporated into the framework to perform efficient prediction on the properties of generated structures in training iterations. With multiple objectives combined by their weight and the 3D WGAN acting as a soft constraint to regulate features that are hard to define by the traditional method, our framework has proven to be capable of tuning the properties of the generated structures in multiple aspects while keeping the selected structural features. The feasibility of a small dataset and the scalability of the objectives of other properties make our work an effective approach to provide fast and automated structure designs for various functional materials.},
  archive      = {J_AML},
  author       = {Zhang, Zhengyang and Fang, Han and Xu, Zhao and Lv, Jiajie and Shen, Yao and Wang, Yanming},
  doi          = {10.1063/5.0169432},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046120},
  shortjournal = {APL Mach. Learn.},
  title        = {Multi-objective generative design of three-dimensional material structures},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Manifold projection image segmentation for nano-XANES
imaging. <em>AML</em>, <em>1</em>(4), 046119. (<a
href="https://doi.org/10.1063/5.0167584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As spectral imaging techniques are becoming more prominent in science, advanced image segmentation algorithms are required to identify appropriate domains in these images. We present a version of image segmentation called manifold projection image segmentation (MPIS) that is generally applicable to a broad range of systems without the need for training because MPIS uses unsupervised machine learning with a few physically motivated hyperparameters. We apply MPIS to nanoscale x-ray absorption near edge structure (XANES) imaging, where XANES spectra are collected with nanometer spatial resolution. We show the superiority of manifold projection over linear transformations, such as the commonly used principal component analysis (PCA). Moreover, MPIS maintains accuracy while reducing computation time and sensitivity to noise compared to the standard nano-XANES imaging analysis procedure. Finally, we demonstrate how multimodal information, such as x-ray fluorescence data and spatial location of pixels, can be incorporated into the MPIS framework. We propose that MPIS is adaptable for any spectral imaging technique, including scanning transmission x-ray microscopy, where the length scale of domains is larger than the resolution of the experiment.},
  archive      = {J_AML},
  author       = {Tetef, Samantha and Pattammattel, Ajith and Chu, Yong S. and Chan, Maria K. Y. and Seidler, Gerald T.},
  doi          = {10.1063/5.0167584},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046119},
  shortjournal = {APL Mach. Learn.},
  title        = {Manifold projection image segmentation for nano-XANES imaging},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantifying the impact of uninformative features on the
performance of supervised classification and dimensionality reduction
algorithms. <em>AML</em>, <em>1</em>(4), 046118. (<a
href="https://doi.org/10.1063/5.0170229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning approaches have become critical tools in data mining and knowledge discovery, especially when attempting to uncover relationships in high-dimensional data. However, researchers have noticed that a large fraction of features in high-dimensional datasets are commonly uninformative (too noisy or irrelevant). Because optimal feature selection is an NP-hard task, it is essential to understand how uninformative features impact the performance of machine learning algorithms. Here, we conduct systematic experiments on algorithms from a wide range of taxonomy families using synthetic datasets with different numbers of uninformative features and different numbers of patterns to be learned. Upon visual inspection, we classify these algorithms into four groups with varying robustness against uninformative features. For the algorithms in three of the groups, we find that when the number of uninformative features exceeds the number of data instances per pattern to be learned, the algorithms fail to learn the patterns. Finally, we investigate whether increasing the distinguishability of patterns or adding training instances can mitigate the effect of uninformative features. Surprisingly, we find that uninformative features still cause algorithms to suffer big losses in performance, even when patterns should be easily distinguishable. Analyses of real-world data show that our conclusions hold beyond the synthetic datasets we study systematically.},
  archive      = {J_AML},
  author       = {Lei, Weihua and Zanchettin, Cleber and Ho, Zoey E. and Nunes Amaral, LuÃ­s A.},
  doi          = {10.1063/5.0170229},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046118},
  shortjournal = {APL Mach. Learn.},
  title        = {Quantifying the impact of uninformative features on the performance of supervised classification and dimensionality reduction algorithms},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven design for enhanced efficiency of sn-based
perovskite solar cells using machine learning. <em>AML</em>,
<em>1</em>(4), 046117. (<a
href="https://doi.org/10.1063/5.0177271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel three-step learning-based machine learning (ML) methodology is developed utilizing 26â000 experimental records from The Perovskite Database Project. A comprehensive set of 29 features encompassing both categorical and numerical data was utilized to train various ML models for various solar cell performance metrics, including open-circuit voltage (V OC ), short-circuit current (J SC ), fill factor (FF), and power conversion efficiency (PCE). The model accuracy was assessed using four key metrics: mean absolute error, mean square error, root mean square error, and R 2 score. Among the constructed models, random forest (RF) emerged as the standout performer, boasting an R 2 score of 0.70 for PCE. This RF model was then used for prediction on the large, optimized design pool of Sn-based perovskite data with intent to probe a viable non-toxic substitute to the standard Pb-based absorber. A three-step algorithm was tailored, which led to the discovery of a new set of feature combinations, showcasing a PCE improvement over the existing peak performance of Sn-based devices. The key aspects identified were device architecture, dimensionality, and deposition procedures for essential layers, including the electron transport layer, the hole transport layer, the perovskite absorber layer, and the back-contact. Through consideration of these features, an impressive increase in PCE was achieved. There was a 28.35\% increase in PCE from 12.24\% to 15.71\% for architecture optimization and a 24.6\% increase in PCE from 12.24\% to 15.25\% for deposition method optimization. This study additionally addresses the effective implementation of target encoding applied to a diverse set of categorical feature labels. The data-driven methodology proposed in this study allows scientists to efficiently identify an optimal architecture and deposition parameters for non-toxic Sn-based perovskite materials with a much higher anticipated device PCE compared to traditional trial-and-error analyses. Further exploration and exploitation of the current investigation is expected to lead to successful and sustainable development of highly efficient Sn-based perovskite solar cells.},
  archive      = {J_AML},
  author       = {Rumman, Abdul Hamid and Sahriar, Miah Abdullah and Islam, Md Tohidul and Shorowordi, Kazi Md and Carbonara, Joaquin and Broderick, Scott and Ahmed, Saquib},
  doi          = {10.1063/5.0177271},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046117},
  shortjournal = {APL Mach. Learn.},
  title        = {Data-driven design for enhanced efficiency of sn-based perovskite solar cells using machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention hybrid variational net for accelerated MRI
reconstruction. <em>AML</em>, <em>1</em>(4), 046116. (<a
href="https://doi.org/10.1063/5.0165485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of compressed sensing (CS)-enabled data reconstruction for accelerating magnetic resonance imaging (MRI) remains a challenging problem. This is due to the fact that the information lost in k-space from the acceleration mask makes it difficult to reconstruct an image similar to the quality of a fully sampled image. Multiple deep learning-based structures have been proposed for MRI reconstruction using CS, in both the k-space and image domains, and using unrolled optimization methods. However, the drawback of these structures is that they are not fully utilizing the information from both domains (k-space and image). Herein, we propose a deep learning-based attention hybrid variational network that performs learning in both the k-space and image domains. We evaluate our method on a well-known open-source MRI dataset (652 brain cases and 1172 knee cases) and a clinical MRI dataset of 243 patients diagnosed with strokes from our institution to demonstrate the performance of our network. Our model achieves an overall peak signal-to-noise ratio/structural similarity of 40.92 Â± 0.29/0.9577 Â± 0.0025 (fourfold) and 37.03 Â± 0.25/0.9365 Â± 0.0029 (eightfold) for the brain dataset, 31.09 Â± 0.25/0.6901 Â± 0.0094 (fourfold) and 29.49 Â± 0.22/0.6197 Â± 0.0106 (eightfold) for the knee dataset, and 36.32 Â± 0.16/0.9199 Â± 0.0029 (20-fold) and 33.70 Â± 0.15/0.8882 Â± 0.0035 (30-fold) for the stroke dataset. In addition to quantitative evaluation, we undertook a blinded comparison of image quality across networks performed by a subspecialty trained radiologist. Overall, we demonstrate that our network achieves a superior performance among others under multiple reconstruction tasks.},
  archive      = {J_AML},
  author       = {Shen, Guoyao and Hao, Boran and Li, Mengyu and Farris, Chad W. and Paschalidis, Ioannis Ch. and Anderson, Stephan W. and Zhang, Xin},
  doi          = {10.1063/5.0165485},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046116},
  shortjournal = {APL Mach. Learn.},
  title        = {Attention hybrid variational net for accelerated MRI reconstruction},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of novel organic polar materials: A machine
learning study with importance sampling. <em>AML</em>, <em>1</em>(4),
046115. (<a href="https://doi.org/10.1063/5.0162380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in the synthesis of polar molecular materials have produced practical alternatives to ferroelectric ceramics, opening up exciting new avenues for their incorporation into modern electronic devices. However, in order to realize the full potential of polar polymer and molecular crystals for modern technological applications, it is paramount to assemble and evaluate all the available data for such compounds, identifying descriptors that could be associated with an emergence of ferroelectricity. In this paper, we utilized data-driven approaches to judiciously shortlist candidate materials from a wide chemical space that could possess ferroelectric functionalities. A machine learning study with importance sampling was employed to address the challenge of having a limited amount of available data on already-known organic ferroelectrics. Sets of molecular- and crystal-level descriptors were combined with a Random Forest Regression algorithm in order to predict the spontaneous polarization of the shortlisted compounds. First-principles simulations were performed to further validate the predictions obtained from the machine learning model.},
  archive      = {J_AML},
  author       = {Ghosh, Ayana and Trujillo, Dennis P. and Hazarika, Subhashis and Schiesser, Elizabeth and Swamynathan, M. J. and Ghosh, Saurabh and Zhu, Jian-Xin and Nakhmanson, Serge},
  doi          = {10.1063/5.0162380},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046115},
  shortjournal = {APL Mach. Learn.},
  title        = {Identification of novel organic polar materials: A machine learning study with importance sampling},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyena neural operator for partial differential equations.
<em>AML</em>, <em>1</em>(4), 046114. (<a
href="https://doi.org/10.1063/5.0177276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerically solving partial differential equations typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving partial differential equations that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and enjoys a global receptive field at the meantime. This mechanism enhances the modelâs comprehension of the inputâs context and enables data-dependent weight for different partial differential equation instances. To measure how effective the layers are in solving partial differential equations, we conduct experiments on the diffusionâreaction equation and NavierâStokes equation and compare it with the Fourier neural operator. Our findings indicate that the Hyena neural operator can serve as an efficient and accurate model for learning the partial differential equation solution operator. The data and code used can be found at https://github.com/Saupatil07/Hyena-Neural-Operator .},
  archive      = {J_AML},
  author       = {Patil, Saurabh and Li, Zijie and Barati Farimani, Amir},
  doi          = {10.1063/5.0177276},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046114},
  shortjournal = {APL Mach. Learn.},
  title        = {Hyena neural operator for partial differential equations},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Advancing magnetic material discovery through machine
learning: Unveiling new manganese-based materials. <em>AML</em>,
<em>1</em>(4), 046113. (<a
href="https://doi.org/10.1063/5.0171320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic materials are used in a variety of applications, such as electric generators, speakers, hard drives, MRI machines, etc. Discovery of new magnetic materials with desirable properties is essential for advancement in these applications. In this research article, we describe the development and validation of a machine-learning model to discover new manganese-based stable magnetic materials. The machine learning model is trained on the input data from the Materials Project database to predict the magnetization and formation energy of the materials. New hypothetical structures are made using the substitution method, and the properties are predicted using the machine learning model to select the materials with desired properties. Harnessing the power of machine learning allows us to intelligently narrow down the vast pool of potential candidates. By doing so, we deftly reduce the number of materials that warrant in-depth examination using density functional theory, rendering the task more manageable and efficient. The selected materials, seemingly promising with their magnetic potential, undergo a meticulous validation process using the Vienna Ab initio Simulation Package, grounded in density functional theory. Our results underscore the paramount significance of input data in the efficacy of the machine learning model. Particularly in the realm of magnetic materials, the proper initialization of atomic magnetic spins holds the key to converging upon the true magnetic state of each material.},
  archive      = {J_AML},
  author       = {Khatri, Yogesh and Kashyap, Arti},
  doi          = {10.1063/5.0171320},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046113},
  shortjournal = {APL Mach. Learn.},
  title        = {Advancing magnetic material discovery through machine learning: Unveiling new manganese-based materials},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spontaneous muscle activity classification with delay-based
reservoir computing. <em>AML</em>, <em>1</em>(4), 046112. (<a
href="https://doi.org/10.1063/5.0160927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromuscular disorders (NMDs) affect various parts of a motor unit, such as the motor neuron, neuromuscular junction, and muscle fibers. Abnormal spontaneous activity (SA) is detected with electromyography (EMG) as an essential hallmark in diagnosing NMD, which causes fatigue, pain, and muscle weakness. Monitoring the effects of NMD calls for new smart devices to collect and classify EMG. Delay-based Reservoir Computing (DRC) is a neuromorphic algorithm with high efficiency in classifying sequential data. This work proposes a new DRC-based algorithm that provides a reference for medical education and training and a second opinion to clinicians to verify NMD diagnoses by detecting SA in muscles. With a sampling frequency of Fs = 64Â kHz, we have classified SA with EMG signals of 1Â s of muscle recordings. Furthermore, the DRC model of size N = 600 nodes has successfully detected SA signals against normal muscle activity with an accuracy of up to 90.7\%. The potential of using neuromorphic processing approaches in point-of-care diagnostics, alongside the supervision of a clinician, provides a more comprehensive and reliable clinical profile. Our developed model benefits from the potential to be implemented in physical hardware to provide near-sensor edge computing.},
  archive      = {J_AML},
  author       = {Pavlidou, Antonia and Liang, Xiangpeng and Arekhloo, Negin Ghahremani and Li, Haobo and Marquetand, Justus and Heidari, Hadi},
  doi          = {10.1063/5.0160927},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046112},
  shortjournal = {APL Mach. Learn.},
  title        = {Spontaneous muscle activity classification with delay-based reservoir computing},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of TbCo composition from local-minimum-energy
magnetic images taken by magneto-optical kerr effect microscope by using
machine learning. <em>AML</em>, <em>1</em>(4), 046111. (<a
href="https://doi.org/10.1063/5.0160970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the incorporation of machine learning (ML) has heralded significant advancements in materials science. For instance, in spintronics, it has been shown that magnetic parameters, such as the DzyaloshinskiiâMoriya interaction, can be estimated from magnetic domain images using ML. Magnetic materials exhibit hysteresis, leading to numerous magnetic states with locally minimized energy (LME) even within a single sample. However, it remains uncertain whether these parameters can be derived from LME states. In our research, we explored the estimation of material parameters from an LME magnetic state using a convolutional neural network. We introduced a technique to manipulate LME magnetic states, combining the ac demagnetizing method with the magneto-optical Kerr effect. By applying this method, we generated multiple LME magnetic states from a single sample and successfully estimated its material composition. Our findings suggest that ML emphasizes not the global domain structures that are readily perceived by humans but the more subtle local domain structures that are often overlooked. Adopting this approach could potentially facilitate the estimation of magnetic parameters from any state observed in experiments, streamlining experimental processes in spintronics.},
  archive      = {J_AML},
  author       = {Kuno, Shiori and Deguchi, Shinji and Sumi, Satoshi and Awano, Hiroyuki and Tanabe, Kenji},
  doi          = {10.1063/5.0160970},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046111},
  shortjournal = {APL Mach. Learn.},
  title        = {Estimation of TbCo composition from local-minimum-energy magnetic images taken by magneto-optical kerr effect microscope by using machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A physics-based predictive model for pulse design to realize
high-performance memristive neural networks. <em>AML</em>,
<em>1</em>(4), 046110. (<a
href="https://doi.org/10.1063/5.0180346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memristive neural networks have extensively been investigated for their capability in handling various artificial intelligence tasks. The training performance of memristive neural networks depends on the pulse scheme applied to the constituent memristors. However, the design of the pulse scheme in most previous studies was approached in an empirical manner or through a trial-and-error method. Here, we choose ferroelectric tunnel junction (FTJ) as a model memristor and demonstrate a physics-based predictive model for the pulse design to achieve high training performance. This predictive model comprises a physical model for FTJ that can adequately describe the polarization switching and memristive switching behaviors of the FTJ and an FTJ-based neural network that uses the long-term potentiation (LTP)/long-term depression (LTD) characteristics of the FTJ for the weight update. Simulation results based on the predictive model demonstrate that the LTP/LTD characteristics with a good trade-off between ON/OFF ratio, nonlinearity, and asymmetry can lead to high training accuracies for the FTJ-based neural network. Moreover, it is revealed that an amplitude-increasing pulse scheme may be the most favorable pulse scheme as it offers the widest ranges of pulse amplitudes and widths for achieving high accuracies. This study may provide useful guidance for the pulse design in the experimental development of high-performance memristive neural networks.},
  archive      = {J_AML},
  author       = {Deng, Haoyue and Fan, Zhen and Dong, Shuai and Chen, Zhiwei and Li, Wenjie and Chen, Yihong and Liu, Kun and Tao, Ruiqiang and Tian, Guo and Chen, Deyang and Qin, Minghui and Zeng, Min and Lu, Xubing and Zhou, Guofu and Gao, Xingsen and Liu, Jun-Ming},
  doi          = {10.1063/5.0180346},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046110},
  shortjournal = {APL Mach. Learn.},
  title        = {A physics-based predictive model for pulse design to realize high-performance memristive neural networks},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging graph neural networks and neural operator
techniques for high-fidelity mesh-based physics simulations.
<em>AML</em>, <em>1</em>(4), 046109. (<a
href="https://doi.org/10.1063/5.0167014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing fast and accurate computational models to simulate intricate physical phenomena has been a persistent research challenge. Recent studies have demonstrated remarkable capabilities in predicting various physical outcomes through machine learning-assisted approaches. However, it remains challenging to generalize current methods, usually crafted for a specific problem, to other more complex or broader scenarios. To address this challenge, we developed graph neural network (GNN) models with enhanced generalizability derived from the distinct GNN architecture and neural operator techniques. As a proof of concept, we employ our GNN models to predict finite element (FE) simulation results for three-dimensional solid mechanics problems with varying boundary conditions. Results show that our GNN model achieves accurate and robust performance in predicting the stress and deformation profiles of structures compared with FE simulations. Furthermore, the neural operator embedded GNN approach enables learning and predicting various solid mechanics problems in a generalizable fashion, making it a promising approach for surrogate modeling.},
  archive      = {J_AML},
  author       = {Jin, Zeqing and Zheng, Bowen and Kim, Changgon and Gu, Grace X.},
  doi          = {10.1063/5.0167014},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046109},
  shortjournal = {APL Mach. Learn.},
  title        = {Leveraging graph neural networks and neural operator techniques for high-fidelity mesh-based physics simulations},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cloud platform for sharing and automated analysis of raw
data from high throughput polymer MD simulations. <em>AML</em>,
<em>1</em>(4), 046108. (<a
href="https://doi.org/10.1063/5.0160937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open material databases storing thousands of material structures and their properties have become the cornerstone of modern computational materials science. Yet, the raw simulation outputs are generally not shared due to their huge size. In this work, we describe a cloud-based platform to enable fast post-processing of the trajectories and to facilitate sharing of the raw data. As an initial demonstration, our database includes 6286 molecular dynamics trajectories for amorphous polymer electrolytes (5.7 terabytes of data). We create a public analysis library at https://github.com/TRI-AMDD/htp_md to extract ion transport properties from the raw data using expert-designed functions and machine learning models. The analysis is run automatically on the cloud, and the results are uploaded onto an open database. Our platform encourages users to contribute both new trajectory data and analysis functions via public interfaces. Finally, we create a front-end user interface at https://www.htpmd.matr.io/ for browsing and visualization of our data. We envision the platform to be a new way of sharing raw data and new insights for the materials science community.},
  archive      = {J_AML},
  author       = {Xie, Tian and Kwon, Ha-Kyung and Schweigert, Daniel and Gong, Sheng and France-Lanord, Arthur and Khajeh, Arash and Crabb, Emily and Puzon, Michael and Fajardo, Chris and Powelson, Will and Shao-Horn, Yang and Grossman, Jeffrey C.},
  doi          = {10.1063/5.0160937},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046108},
  shortjournal = {APL Mach. Learn.},
  title        = {A cloud platform for sharing and automated analysis of raw data from high throughput polymer MD simulations},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Unsupervised machine learning to analyze corneal tissue
surfaces. <em>AML</em>, <em>1</em>(4), 046107. (<a
href="https://doi.org/10.1063/5.0159502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying/classifying damage features on soft materials, such as tissues, is much more challenging than on classical, hard materialsâbut nevertheless important, especially in the field of bio-tribology. For instance, cartilage samples from osteoarthritic patients exhibit surface damage even at early stages of tissue degeneration, and corneal tissues can be damaged by contact lenses when the ocular lubrication system fails. Here, we employ unsupervised machine learning (ML) methods to assess the surface condition of a soft tissue by detecting and classifying different wear morphologies as well as the severity of surface damage they represent. We show that different clustering methods, especially a k -means clustering algorithm, can indeed achieve aâfrom a material science point of viewâmeaningful classification of those tissue samples. Our study pinpoints the ability of unsupervised ML models to guide or even replace human decision processes for the analysis of complex surfaces and topographical datasets thatâeither owing to their complexity or the sample sizeâexceed the capability of the human brain.},
  archive      = {J_AML},
  author       = {Rickert, Carolin A. and Henkel, Fabio and Lieleg, Oliver},
  doi          = {10.1063/5.0159502},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046107},
  shortjournal = {APL Mach. Learn.},
  title        = {Unsupervised machine learning to analyze corneal tissue surfaces},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian optimization approach to quantify the effect of
input parameter uncertainty on predictions of numerical physics
simulations. <em>AML</em>, <em>1</em>(4), 046106. (<a
href="https://doi.org/10.1063/5.0151747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An understanding of how input parameter uncertainty in the numerical simulation of physical models leads to simulation output uncertainty is a challenging task. Common methods for quantifying output uncertainty, such as performing a grid or random search over the model input space, are computationally intractable for a large number of input parameters represented by a high-dimensional input space. It is, therefore, generally unclear as to whether a numerical simulation can reproduce a particular outcome (e.g., a set of experimental results) with a plausible set of model input parameters. Here, we present a method for efficiently searching the input space using Bayesian optimization to minimize the difference between the simulation output and a set of experimental results. Our method allows explicit evaluation of the probability that the simulation can reproduce the measured experimental results in the region of input space defined by the uncertainty in each input parameter. We apply this method to the simulation of charge-carrier dynamics in the perovskite semiconductor methyl-ammonium lead iodide (MAPbI 3 ), which has attracted attention as a light harvesting material in solar cells. From our analysis, we conclude that the formation of large polarons, quasiparticles created by the coupling of excess electrons or holes with ionic vibrations, cannot explain the experimentally observed temperature dependence of electron mobility.},
  archive      = {J_AML},
  author       = {McCallum, Samuel G. and LerpiniÃ¨re, James E. and Jensen, Kjeld O. and Friederich, Pascal and Walker, Alison B.},
  doi          = {10.1063/5.0151747},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046106},
  shortjournal = {APL Mach. Learn.},
  title        = {Bayesian optimization approach to quantify the effect of input parameter uncertainty on predictions of numerical physics simulations},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymmetric CycleGANs for inverse design of photonic
metastructures. <em>AML</em>, <em>1</em>(4), 046105. (<a
href="https://doi.org/10.1063/5.0159264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using deep learning to develop nanophotonic structures has been an active field of research in recent years to reduce the time intensive iterative solutions found in finite-difference time-domain simulations. Existing work has primarily used a specific type of generative network: conditional deep convolutional generative adversarial networks. However, these networks have issues with producing clear optical structures in image files; for example, a large number of images show speckled noise, which often results in non-manufacturable structures. Here, we report the first use of cycle-consistent generative adversarial networks to design nanophotonic structures. This approach significantly reduces the amount of speckled noise present in generated geometric structures and allows shapes to have clear edges. We demonstrate that for a given input reflectance spectra, the system generates designs in the form of images, and a complementary network generates reflectance spectra if an image containing a shape is provided as an input. The results show a higher Frechet Inception Distance score than previous approaches, which indicates that the generated structures are of higher quality and are able to learn nonlinear relationships between both datasets. This method of designing nanophotonics provides alternative avenues for development that are more noise robust while still adhering to desired optical properties.},
  archive      = {J_AML},
  author       = {Panisilvam, Jeygopi and Hajizadeh, Elnaz and Weeratunge, Hansani and Bailey, James and Kim, Sejeong},
  doi          = {10.1063/5.0159264},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046105},
  shortjournal = {APL Mach. Learn.},
  title        = {Asymmetric CycleGANs for inverse design of photonic metastructures},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of brownian motion trajectories of non-spherical
nanoparticles using deep learning. <em>AML</em>, <em>1</em>(4), 046104.
(<a href="https://doi.org/10.1063/5.0160979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As nanoparticles are being put to practical use as useful materials in the medical, pharmaceutical, and industrial fields, the importance of technologies that can evaluate not only nanoparticle populations of homogeneous size and density but also those of rich diversity is increasing. Nano-tracking analysis (NTA) has been commercialized and widely used as a method to measure individual nanoparticles in liquids and evaluate their size distribution by analyzing Brownian motion. We have combined deep learning (DL) for NTA to extract more property information and explored a methodology to achieve an evaluation for individual particles to understand their diversity. Practical NTA always assumes spherical shape when quantifying particle size using the StokesâEinstein equation, but it is not possible to verify whether the measured particles are truly spherical. We developed a DL model that predicts the shape of nanoparticles using time series trajectory data of BM obtained from NTA measurements to address this problem. As a result, we were able to discriminate with â¼80\% accuracy between spherical and rod-shaped gold nanoparticles of different shapes, which are evaluated to have nearly equal particle size without any discrimination by conventional NTA. Furthermore, we demonstrated that the mixing ratio of spherical and rod-shaped nanoparticles can be quantitatively estimated from measured data of mixed samples of nanoparticles. This result suggests that it is possible to evaluate particle shape by applying DL analysis to NTA measurements, which was previously considered impossible, and opens the way to further value-added NTA.},
  archive      = {J_AML},
  author       = {Fukuda, Hiroaki and Kuramochi, Hiromi and Shibuta, Yasushi and Ichiki, Takanori},
  doi          = {10.1063/5.0160979},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046104},
  shortjournal = {APL Mach. Learn.},
  title        = {Analysis of brownian motion trajectories of non-spherical nanoparticles using deep learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization of a quantum cascade laser cavity for
single-spatial-mode operation via machine learning. <em>AML</em>,
<em>1</em>(4), 046103. (<a
href="https://doi.org/10.1063/5.0158204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks, trained with the ADAM algorithm followed by a globally convergent modification to Newtonâs method, are developed to predict the threshold gain of the fundamental and first higher-order modes as functions of the refractive-index profile in a quantum cascade laser cavity. The networks are used to optimize the design of a refractive-index profile that provides essentially single-spatial-mode performance in a nominally multi-moded cavity by maximizing the threshold-gain differential between the modes. The use of neural networks allows the optimization to be performed in seconds, instead of days or weeks which would be required if Maxwellâs equations were repeatedly solved to obtain the threshold gains.},
  archive      = {J_AML},
  author       = {Jacobs, S. A. and Kirch, J. D. and Hu, Y. and Suri, S. and Knipfer, B. and Yu, Z. and Botez, D. and Marsland, R. and Mawst, L. J.},
  doi          = {10.1063/5.0158204},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046103},
  shortjournal = {APL Mach. Learn.},
  title        = {Optimization of a quantum cascade laser cavity for single-spatial-mode operation via machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovery of structureâproperty relations for molecules via
hypothesis-driven active learning over the chemical space. <em>AML</em>,
<em>1</em>(4), 046102. (<a
href="https://doi.org/10.1063/5.0157644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of the molecular candidates for application in drug targets, biomolecular systems, catalysts, photovoltaics, organic electronics, and batteries necessitates the development of machine learning algorithms capable of rapid exploration of chemical spaces targeting the desired functionalities. Here, we introduce a novel approach for active learning over the chemical spaces based on hypothesis learning. We construct the hypotheses on the possible relationships between structures and functionalities of interest based on a small subset of data followed by introducing them as (probabilistic) mean functions for the Gaussian process. This approach combines the elements from the symbolic regression methods, such as SISSO and active learning, into a single framework. The primary focus of constructing this framework is to approximate physical laws in an active learning regime toward a more robust predictive performance, as traditional evaluation on hold-out sets in machine learning does not account for out-of-distribution effects which may lead to a complete failure on unseen chemical space. Here, we demonstrate it for the QM9 dataset, but it can be applied more broadly to datasets from both domains of molecular and solid-state materials sciences.},
  archive      = {J_AML},
  author       = {Ghosh, Ayana and Kalinin, Sergei V. and Ziatdinov, Maxim A.},
  doi          = {10.1063/5.0157644},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046102},
  shortjournal = {APL Mach. Learn.},
  title        = {Discovery of structureâproperty relations for molecules via hypothesis-driven active learning over the chemical space},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autoregressive transformers for data-driven spatiotemporal
learning of turbulent flows. <em>AML</em>, <em>1</em>(4), 046101. (<a
href="https://doi.org/10.1063/5.0152212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A convolutional encoderâdecoder-based transformer model is proposed for autoregressively training on spatiotemporal data of turbulent flows. The prediction of future fluid flow fields is based on the previously predicted fluid flow field to ensure long-term predictions without diverging. A combination of convolutional neural networks and transformer architecture is utilized to handle both the spatial and temporal dimensions of the data. To assess the performance of the model, a priori assessments are conducted, and significant agreements are found with the ground truth data. The a posteriori predictions, which are generated after a considerable number of simulation steps, exhibit predicted variances. The autoregressive training and prediction of a posteriori states are deemed crucial steps toward the development of more complex data-driven turbulence models and simulations. The highly nonlinear and chaotic dynamics of turbulent flows can be handled by the proposed model, and accurate predictions over long time horizons can be generated. Overall, the potential of using deep learning techniques to improve the accuracy and efficiency of turbulence modeling and simulation is demonstrated by this approach. The proposed model can be further optimized and extended to incorporate additional physics and boundary conditions, paving the way for more realistic simulations of complex fluid dynamics.},
  archive      = {J_AML},
  author       = {Patil, Aakash and Viquerat, Jonathan and Hachem, Elie},
  doi          = {10.1063/5.0152212},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {046101},
  shortjournal = {APL Mach. Learn.},
  title        = {Autoregressive transformers for data-driven spatiotemporal learning of turbulent flows},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scanning probe microscopy in the age of machine learning.
<em>AML</em>, <em>1</em>(4), 041501. (<a
href="https://doi.org/10.1063/5.0160568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scanning probe microscopy (SPM) has revolutionized our ability to explore the nanoscale world, enabling the imaging, manipulation, and characterization of materials at the atomic and molecular level. However, conventional SPM techniques suffer from limitations, such as slow data acquisition, low signal-to-noise ratio, and complex data analysis. In recent years, the field of machine learning (ML) has emerged as a powerful tool for analyzing complex datasets and extracting meaningful patterns and features in multiple fields. The combination of ML with SPM techniques has the potential to overcome many of the limitations of conventional SPM methods and unlock new opportunities for nanoscale research. In this review article, we will provide an overview of the recent developments in ML-based SPM, including its applications in topography imaging, surface characterization, and secondary imaging modes, such as electrical, spectroscopic, and mechanical datasets. We will also discuss the challenges and opportunities of integrating ML with SPM techniques and highlight the potential impact of this interdisciplinary field on various fields of science and engineering.},
  archive      = {J_AML},
  author       = {Rahman Laskar, Md Ashiqur and Celano, Umberto},
  doi          = {10.1063/5.0160568},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {041501},
  shortjournal = {APL Mach. Learn.},
  title        = {Scanning probe microscopy in the age of machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using the IBM analog in-memory hardware acceleration kit for
neural network training and inference. <em>AML</em>, <em>1</em>(4),
041102. (<a href="https://doi.org/10.1063/5.0168089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analog In-Memory Computing (AIMC) is a promising approach to reduce the latency and energy consumption of Deep Neural Network (DNN) inference and training. However, the noisy and non-linear device characteristics and the non-ideal peripheral circuitry in AIMC chips require adapting DNNs to be deployed on such hardware to achieve equivalent accuracy to digital computing. In this Tutorial, we provide a deep dive into how such adaptations can be achieved and evaluated using the recently released IBM Analog Hardware Acceleration Kit (AIHWKit), freely available at https://github.com/IBM/aihwkit . AIHWKit is a Python library that simulates inference and training of DNNs using AIMC. We present an in-depth description of the AIHWKit design, functionality, and best practices to properly perform inference and training. We also present an overview of the Analog AI Cloud Composer, a platform that provides the benefits of using the AIHWKit simulation in a fully managed cloud setting along with physical AIMC hardware access, freely available at https://aihw-composer.draco.res.ibm.com . Finally, we show examples of how users can expand and customize AIHWKit for their own needs. This Tutorial is accompanied by comprehensive Jupyter Notebook code examples that can be run using AIHWKit, which can be downloaded from https://github.com/IBM/aihwkit/tree/master/notebooks/tutorial .},
  archive      = {J_AML},
  author       = {Le Gallo, Manuel and Lammie, Corey and BÃ¼chel, Julian and Carta, Fabio and Fagbohungbe, Omobayode and Mackin, Charles and Tsai, Hsinyu and Narayanan, Vijay and Sebastian, Abu and El Maghraoui, Kaoutar and Rasch, Malte J.},
  doi          = {10.1063/5.0168089},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {041102},
  shortjournal = {APL Mach. Learn.},
  title        = {Using the IBM analog in-memory hardware acceleration kit for neural network training and inference},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tutorial on the bayesian statistical approach to inverse
problems. <em>AML</em>, <em>1</em>(4), 041101. (<a
href="https://doi.org/10.1063/5.0154773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse problems are ubiquitous in science and engineering. Two categories of inverse problems concerning a physical system are (1) estimate parameters in a model of the system from observed inputâoutput pairs and (2) given a model of the system, reconstruct the input to it that caused some observed output. Applied inverse problems are challenging because a solution may (i) not exist, (ii) not be unique, or (iii) be sensitive to measurement noise contaminating the data. Bayesian statistical inversion (BSI) is an approach to tackle ill-posed and/or ill-conditioned inverse problems. Advantageously, BSI provides a âsolutionâ that (i) quantifies uncertainty by assigning a probability to each possible value of the unknown parameter/input and (ii) incorporates prior information and beliefs about the parameter/input. Herein, we provide a tutorial of BSI for inverse problems by way of illustrative examples dealing with heat transfer from ambient air to a cold lime fruit. First, we use BSI to infer a parameter in a dynamic model of the lime temperature from measurements of the lime temperature over time. Second, we use BSI to reconstruct the initial condition of the lime from a measurement of its temperature later in time. We demonstrate the incorporation of prior information, visualize the posterior distributions of the parameter/initial condition, and show posterior samples of lime temperature trajectories from the model. Our Tutorial aims to reach a wide range of scientists and engineers.},
  archive      = {J_AML},
  author       = {Waqar, Faaiq G. and Patel, Swati and Simon, Cory M.},
  doi          = {10.1063/5.0154773},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {041101},
  shortjournal = {APL Mach. Learn.},
  title        = {A tutorial on the bayesian statistical approach to inverse problems},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bring memristive in-memory computing into general-purpose
machine learning: A perspective. <em>AML</em>, <em>1</em>(4), 040901.
(<a href="https://doi.org/10.1063/5.0167743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing (IMC) using emerging nonvolatile devices has received considerable attention due to its great potential for accelerating artificial neural networks and machine learning tasks. As the basic concept and operation modes of IMC are now well established, there is growing interest in employing its wide and general application. In this perspective, the path that leads memristive IMC to general-purpose machine learning is discussed in detail. First, we reviewed the development timeline of machine learning algorithms that employ memristive devices, such as resistive random-access memory and phase-change memory. Then we summarized two typical aspects of realizing IMC-based general-purpose machine learning. One involves a heterogeneous computing system for algorithmic completeness. The other is to obtain the configurable precision techniques for the compromise of the precision-efficiency dilemma. Finally, the major directions and challenges of memristive IMC-based general-purpose machine learning are proposed from a cross-level design perspective.},
  archive      = {J_AML},
  author       = {Zhou, Houji and Chen, Jia and Li, Jiancong and Yang, Ling and Li, Yi and Miao, Xiangshui},
  doi          = {10.1063/5.0167743},
  journal      = {APL Machine Learning},
  number       = {4},
  pages        = {040901},
  shortjournal = {APL Mach. Learn.},
  title        = {Bring memristive in-memory computing into general-purpose machine learning: A perspective},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental realization of a quantum classification: Bell
state measurement via machine learning. <em>AML</em>, <em>1</em>(3),
036111. (<a href="https://doi.org/10.1063/5.0149414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bell state is a crucial resource for the realization of quantum information tasks, and when combined with orbital angular momentum (OAM), it enables a high-dimensional Hilbert space, which is essential for high-capacity quantum communication. In this study, we demonstrate the recognition of OAM Bell states using interference patterns generated by a classical light source and a single-photon source from a Sagnac interferometer-based OAM Bell state evolution device. The interference patterns exhibit a one-to-one correspondence with the input Bell states, providing conclusive evidence for the full recognition of OAM Bell states. Furthermore, we introduce machine learning to the field of Bell state recognition by proposing a neural network model capable of accurately recognizing higher order single-photon OAM Bell states, even in the undersampling case. In particular, the modelâs training set includes interference patterns of OAM Bell states generated by classical light sources, yet it is able to recognize single-photon OAM Bell states with high accuracy, without relying on quantum resources during training. Our innovative application of neural networks to the recognition of single-photon OAM Bell states not only circumvents the resource consumption and experimental difficulties associated with quantum light sources but also facilitates the study of OAM-based quantum information.},
  archive      = {J_AML},
  author       = {Wu, Qing-Yuan and Meng, Zhe and Chen, Xiao-Xiao and Li, Jian and Yang, Jia-Zhi and Zhang, An-Ning},
  doi          = {10.1063/5.0149414},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036111},
  shortjournal = {APL Mach. Learn.},
  title        = {Experimental realization of a quantum classification: Bell state measurement via machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KoopmanLab: Machine learning for solving complex physics
equations. <em>AML</em>, <em>1</em>(3), 036110. (<a
href="https://doi.org/10.1063/5.0157763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous physics theories are rooted in partial differential equations (PDEs). However, the increasingly intricate physics equations, especially those that lack analytic solutions or closed forms, have impeded the further development of physics. Computationally solving PDEs by classic numerical approaches suffers from the trade-off between accuracy and efficiency and is not applicable to the empirical data generated by unknown latent PDEs. To overcome this challenge, we present KoopmanLab, an efficient module of the Koopman neural operator (KNO) family, for learning PDEs without analytic solutions or closed forms. Our module consists of multiple variants of the KNO, a kind of mesh-independent neural-network-based PDE solvers developed following the dynamic system theory. The compact variants of KNO can accurately solve PDEs with small model sizes, while the large variants of KNO are more competitive in predicting highly complicated dynamic systems govern by unknown, high-dimensional, and non-linear PDEs. All variants are validated by mesh-independent and long-term prediction experiments implemented on representative PDEs (e.g., the NavierâStokes equation and the BatemanâBurgers equation in fluid mechanics) and ERA5 (i.e., one of the largest high-resolution global-scale climate datasets in earth physics). These demonstrations suggest the potential of KoopmanLab to be a fundamental tool in diverse physics studies related to equations or dynamic systems.},
  archive      = {J_AML},
  author       = {Xiong, Wei and Ma, Muyuan and Huang, Xiaomeng and Zhang, Ziyang and Sun, Pei and Tian, Yang},
  doi          = {10.1063/5.0157763},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036110},
  shortjournal = {APL Mach. Learn.},
  title        = {KoopmanLab: Machine learning for solving complex physics equations},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of multi-frequency RF signals by extreme
learning, using magnetic tunnel junctions as neurons and synapses.
<em>AML</em>, <em>1</em>(3), 036109. (<a
href="https://doi.org/10.1063/5.0155447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting information from radio-frequency (RF) signals using artificial neural networks at low energy cost is a critical need for a wide range of applications from radars to health. These RF inputs are composed of multiple frequencies. Here, we show that magnetic tunnel junctions can process analog RF inputs with multiple frequencies in parallel and perform synaptic operations. Using a backpropagation-free method called extreme learning, we classify noisy images encoded by RF signals, using experimental data from magnetic tunnel junctions functioning as both synapses and neurons. We achieve the same accuracy as an equivalent software neural network. These results are a key step for embedded RF artificial intelligence.},
  archive      = {J_AML},
  author       = {Leroux, Nathan and MarkoviÄ, Danijela and Sanz-HernÃ¡ndez, DÃ©dalo and Trastoy, Juan and Bortolotti, Paolo and Schulman, Alejandro and Benetti, Luana and Jenkins, Alex and Ferreira, Ricardo and Grollier, Julie and Mizrahi, Frank Alice},
  doi          = {10.1063/5.0155447},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036109},
  shortjournal = {APL Mach. Learn.},
  title        = {Classification of multi-frequency RF signals by extreme learning, using magnetic tunnel junctions as neurons and synapses},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep ensemble inverse model for image-based estimation of
solar cell parameters. <em>AML</em>, <em>1</em>(3), 036108. (<a
href="https://doi.org/10.1063/5.0139707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical models can help improve solar cell efficiency during the design phase and for quality control after the fabrication process. We present a data-driven approach to inverse modeling that can predict the underlying parameters of a finite element method solar cell model based on an electroluminescence (EL) image of a solar cell with known cell geometry and laser scribed defects. For training the inverse model, 75â000 synthetic EL images were generated with randomized parameters of the physical cell model. We combine 17 deep convolutional neural networks based on a modified VGG19 architecture into a deep ensemble to add uncertainty estimates. Using the silicon solar cell model, we show that such a novel approach to data-driven statistical inverse modeling can help apply recent developments in deep learning to new engineering applications that require real-time parameterizations of physical models augmented by confidence intervals. The trained network was tested on four different physical solar cell samples, and the estimated parameters were used to create the corresponding model representations. Resimulations of the measurements yielded relative deviations of the calculated and the measured junction voltage values of 0.2\% on average with a maximum of 10\%, demonstrating the validity of the approach.},
  archive      = {J_AML},
  author       = {Battaglia, M. and Comi, E. and Stadelmann, T. and Hiestand, R. and Ruhstaller, B. and Knapp, E.},
  doi          = {10.1063/5.0139707},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036108},
  shortjournal = {APL Mach. Learn.},
  title        = {Deep ensemble inverse model for image-based estimation of solar cell parameters},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved prediction for failure time of multilayer ceramic
capacitors (MLCCs): A physics-based machine learning approach.
<em>AML</em>, <em>1</em>(3), 036107. (<a
href="https://doi.org/10.1063/5.0158360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilayer ceramic capacitors (MLCC) play a vital role in electronic systems, and their reliability is of critical importance. The ongoing advancement in MLCC manufacturing has improved capacitive volumetric density for both low and high voltage devices; however, concerns about long-term stability under higher fields and temperatures are always a concern, which impact their reliability and lifespan. Consequently, predicting the mean time to failure (MTTF) for MLCCs remains a challenge due to the limitations of existing models. In this study, we develop a physics-based machine learning approach using the eXtreme Gradient Boosting method to predict the MTTF of X7R MLCCs under various temperature and voltage conditions. We employ a transfer learning framework to improve prediction accuracy for test conditions with limited data and to provide predictions for test conditions where no experimental data exists. We compare our model with the conventional Eyring model (EM) and, more recently, the tipping point model (TPM) in terms of accuracy and performance. Our results show that the machine learning model consistently outperforms both the EM and TPM, demonstrating superior accuracy and stability across different conditions. Our model also exhibits a reliable performance for untested voltage and temperature conditions, making it a promising approach for predicting MTTF in MLCCs.},
  archive      = {J_AML},
  author       = {Yousefian, Pedram and Sepehrinezhad, Alireza and van Duin, Adri C. T. and Randall, Clive A.},
  doi          = {10.1063/5.0158360},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036107},
  shortjournal = {APL Mach. Learn.},
  title        = {Improved prediction for failure time of multilayer ceramic capacitors (MLCCs): A physics-based machine learning approach},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). 3D CNN and grad-CAM based visualization for predicting
generation of dislocation clusters in multicrystalline silicon.
<em>AML</em>, <em>1</em>(3), 036106. (<a
href="https://doi.org/10.1063/5.0156044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a machine learning-based technique to address the crystallographic characteristics responsible for the generation of crystal defects. A convolutional neural network was trained with pairs of optical images that display the characteristics of the crystal and photoluminescence images that show the distributions of crystal defects. The model was trained to predict the existence of crystal defects at the center pixel of the given image from its optical features. Prediction accuracy and separability were enhanced by feeding three-dimensional data and data augmentation. The prediction was successful with a high area under the curve of over 0.9 in a receiver operating characteristic curve. Likelihood maps showing the distributions of the predicted defects are in good resemblance with the correct distributions. Using the trained model, we visualized the most important regions to the predicted class by gradient-based class activation mapping. The extracted regions were found to contain mostly particular grains where the grain boundaries changed greatly due to crystal growth and clusters of small grains. This technique is beneficial in providing a rapid and statistical analysis of various crystal characteristics because the features of optical images are often complex and difficult to interpret. The interpretations can help us understand the physics of crystal growth and the effects of crystallographic characteristics on the generation of detrimental defects. We believe that this technique will contribute to the development of a better fabrication process for high-performance multicrystalline materials.},
  archive      = {J_AML},
  author       = {Hara, Kyoka and Kojima, Takuto and Kutsukake, Kentaro and Kudo, Hiroaki and Usami, Noritaka},
  doi          = {10.1063/5.0156044},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036106},
  shortjournal = {APL Mach. Learn.},
  title        = {3D CNN and grad-CAM based visualization for predicting generation of dislocation clusters in multicrystalline silicon},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable wavelength-multiplexing photonic reservoir
computing. <em>AML</em>, <em>1</em>(3), 036105. (<a
href="https://doi.org/10.1063/5.0158939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photonic reservoir computing (PRC) is a special hardware recurrent neural network, which is featured with fast training speed and low training cost. This work shows a wavelength-multiplexing PRC architecture, taking advantage of the numerous longitudinal modes in a FabryâPerot (FP) semiconductor laser. These modes construct connected physical neurons in parallel, while an optical feedback loop provides interactive virtual neurons in series. We experimentally demonstrate a four-channel wavelength-multiplexing PRC architecture with a total of 80 neurons. The clock rate of the multiplexing PRC reaches as high as 1.0Â GHz, which is four times higher than that of the single-channel case. In addition, it is proved that the multiplexing PRC exhibits a superior performance on the task of signal equalization in an optical fiber communication link. This improved performance is owing to the rich neuron interconnections both in parallel and in series. In particular, this scheme is highly scalable owing to the rich mode resources in FP lasers.},
  archive      = {J_AML},
  author       = {Li, Rui-Qian and Shen, Yi-Wei and Lin, Bao-De and Yu, Jingyi and He, Xuming and Wang, Cheng},
  doi          = {10.1063/5.0158939},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036105},
  shortjournal = {APL Mach. Learn.},
  title        = {Scalable wavelength-multiplexing photonic reservoir computing},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulation of the effect of material properties on yttrium
oxide memristor-based artificial neural networks. <em>AML</em>,
<em>1</em>(3), 036104. (<a
href="https://doi.org/10.1063/5.0143926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reports a simulation study concerning the effect of yttrium oxide stoichiometry on output features of a memristor-based single layer perceptron neural network. To carry out this investigation, a material-oriented behavioral compact model for bipolar-type memristive devices was developed and tested. The model is written for the SPICE (Simulation Program with Integrated Circuits Emphasis) simulator and considers as one of its inputs a measure of the oxygen flow used during the deposition of the switching layer. After a thorough statistical calibration of the model parameters using experimental currentâvoltage characteristics associated with different fabrication conditions, the corresponding curves were simulated and the results were compared with the original data. In this way, the average switching behavior of the structures (low and high current states, set and reset voltages, etc.) as a function of the oxygen content can be forecasted. In a subsequent phase, the collective response of the devices when used in a neural network was investigated in terms of the output features of the network (mainly power dissipation and power efficiency). The role played by parasitic elements, such as the line resistance and the read voltage influence on the inference accuracy, was also explored. Since a similar strategy can be applied to any other material-related fabrication parameter, the proposed approach opens up a new dimension for circuit designers, as the behavior of complex circuits employing devices with specific characteristics can be realistically assessed before fabrication.},
  archive      = {J_AML},
  author       = {Aguirre, F. and Piros, E. and Kaiser, N. and Vogel, T. and Petzold, S. and Gehrunger, J. and Oster, T. and Hofmann, K. and Hochberger, C. and SuÃ±Ã©, J. and Alff, L. and Miranda, E.},
  doi          = {10.1063/5.0143926},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036104},
  shortjournal = {APL Mach. Learn.},
  title        = {Simulation of the effect of material properties on yttrium oxide memristor-based artificial neural networks},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic graph representation algorithm for heterogeneous
catalysis. <em>AML</em>, <em>1</em>(3), 036103. (<a
href="https://doi.org/10.1063/5.0140487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most appealing aspects of machine learning for material design is its high throughput exploration of chemical spaces, but to reach the ceiling of machine learning-aided exploration, more than current model architectures and processing algorithms are required. New architectures such as graph neural networks have seen significant research investments recently. For heterogeneous catalysis, defining substrate intramolecular bonds and adsorbate/substrate intermolecular bonds is a time-consuming and challenging process. Before applying a model, dataset pre-processing, node/bond descriptor design, and specific model constraints have to be considered. In this work, a framework designed to solve these issues is presented in the form of an automatic graph representation algorithm (AGRA) tool to extract the local chemical environment of metallic surface adsorption sites. This tool is able to gather multiple adsorption geometry datasets composed of different systems and combine them into a single model. To show AGRAâs excellent transferability and reduced computational cost compared to other graph representation methods, it was applied to five different catalytic reaction datasets and benchmarked against the Open Catalyst Projects graph representation method. The two oxygen reduction reaction (ORR) datasets with O/OH adsorbates obtained 0.053Â eV root-mean-square deviation (RMSD) when combined together, whereas the three carbon dioxide reduction reaction datasets with CHO/CO/COOH obtained an average performance of 0.088Â eV RMSD. To further display the algorithmâs versatility and extrapolation ability, a model was trained on a subset combination of all five datasets with an RMSD of 0.105Â eV. This universal model was then used to predict a wide range of adsorption energies and an entirely new ORR catalyst system, which was then verified through density functional theory calculations.},
  archive      = {J_AML},
  author       = {Gariepy, Zachary and Chen, ZhiWen and Tamblyn, Isaac and Singh, Chandra Veer and Tetsassi Feugmo, Conrard Giresse},
  doi          = {10.1063/5.0140487},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036103},
  shortjournal = {APL Mach. Learn.},
  title        = {Automatic graph representation algorithm for heterogeneous catalysis},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning guided optimal composition selection of
niobium alloys for high temperature applications. <em>AML</em>,
<em>1</em>(3), 036102. (<a
href="https://doi.org/10.1063/5.0129528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nickel- and cobalt-based superalloys are commonly used as turbine materials for high-temperature applications. However, their maximum operating temperature is limited to about 1100âÂ°C. Therefore, to improve turbine efficiency, current research is focused on designing materials that can withstand higher temperatures. Niobium-based alloys can be considered as promising candidates because of their exceptional properties at elevated temperatures. The conventional approach to alloy design relies on phase diagrams and structureâproperty data of limited alloys and extrapolates this information into unexplored compositional space. In this work, we harness machine learning and provide an efficient design strategy for finding promising niobium-based alloy compositions with high yield and ultimate tensile strength. Unlike standard composition-based features, we use domain knowledge-based custom features and achieve higher prediction accuracy. We apply Bayesian optimization to screen out novel Nb-based quaternary and quinary alloy compositions and find these compositions have superior predicted strength over a range of temperatures. We develop a detailed design flow and include Python programming code, which could be helpful for accelerating alloy design in a limited alloy data regime.},
  archive      = {J_AML},
  author       = {Mohanty, Trupti and Chandran, K. S. Ravi and Sparks, Taylor D.},
  doi          = {10.1063/5.0129528},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036102},
  shortjournal = {APL Mach. Learn.},
  title        = {Machine learning guided optimal composition selection of niobium alloys for high temperature applications},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resistance transient dynamics in switchable perovskite
memristors. <em>AML</em>, <em>1</em>(3), 036101. (<a
href="https://doi.org/10.1063/5.0153289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memristor devices have been investigated for their properties of resistive modulation that can be used in data storage and brain-like computation elements as artificial synapses and neurons. Memristors are characterized by an onset of high current values under applied voltage that produces a transition to a low resistance state or successively to different stable states of increasing conductivity that implement synaptic weights. Here, we develop a nonlinear model to explain the variation with time of the voltage and the resistance and compare it to experimental results on ionicâelectronic halide perovskite memristors. We find separate experimental signatures of the capacitive discharge and inductive current increase. We show that the capacitor produces an increase step of the resistance due to the influence of the series resistance. In contrast, the inductor feature associated with inverted hysteresis causes a decrease of the resistance, as observed experimentally. The chemical inductor feature dominates the potentiation effect in which the conductivity increases with the voltage stimulus. Our results enable a quantitative characterization of highly nonlinear electronic devices using a combination of techniques such as time transient decays and impedance spectroscopy.},
  archive      = {J_AML},
  author       = {Bisquert, Juan and Bou, AgustÃ­n and Guerrero, Antonio and HernÃ¡ndez-Balaguera, Enrique},
  doi          = {10.1063/5.0153289},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {036101},
  shortjournal = {APL Mach. Learn.},
  title        = {Resistance transient dynamics in switchable perovskite memristors},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible optoelectronic synaptic transistors for
neuromorphic visual systems. <em>AML</em>, <em>1</em>(3), 031501. (<a
href="https://doi.org/10.1063/5.0163926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic visual systems that integrate the functionalities of sensing, memory, and processing are expected to overcome the shortcomings of conventional artificial visual systems, such as data redundancy, data access delay, and high-energy consumption. Neuromorphic visual systems based on emerging flexible optoelectronic synaptic devices have recently opened up innovative applications, such as robot visual perception, visual prosthetics, and artificial intelligence. Various flexible optoelectronic synaptic devices have been fabricated, which are either two-terminal memristors or three-terminal transistors. In flexible optoelectronic synaptic transistors (FOSTs), the synaptic weight can be modulated by the electricity and light synergistically, which endows the neuromorphic visual systems with versatile functionalities. In this Review, we present an overview of the working mechanisms, device structures, and active materials of FOSTs. Their applications in neuromorphic visual systems for color recognition, image recognition and memory, motion detection, and pain perception are presented. Perspectives on the development of FOSTs are finally outlined.},
  archive      = {J_AML},
  author       = {Liu, Xiao and Li, Dongke and Wang, Yue and Yang, Deren and Pi, Xiaodong},
  doi          = {10.1063/5.0163926},
  journal      = {APL Machine Learning},
  number       = {3},
  pages        = {031501},
  shortjournal = {APL Mach. Learn.},
  title        = {Flexible optoelectronic synaptic transistors for neuromorphic visual systems},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiplexed gradient descent: Fast online training of modern
datasets on hardware neural networks without backpropagation.
<em>AML</em>, <em>1</em>(2), 026118. (<a
href="https://doi.org/10.1063/5.0157645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present multiplexed gradient descent (MGD), a gradient descent framework designed to easily train analog or digital neural networks in hardware. MGD utilizes zero-order optimization techniques for online training of hardware neural networks. We demonstrate its ability to train neural networks on modern machine learning datasets, including CIFAR-10 and Fashion-MNIST, and compare its performance to backpropagation. Assuming realistic timescales and hardware parameters, our results indicate that these optimization techniques can train a network on emerging hardware platforms orders of magnitude faster than the wall-clock time of training via backpropagation on a standard GPU, even in the presence of imperfect weight updates or device-to-device variations in the hardware. We additionally describe how it can be applied to existing hardware as part of chip-in-the-loop training or integrated directly at the hardware level. Crucially, because the MGD framework is model-free it can be applied to nearly any hardware platform with tunable parameters, and its gradient descent process can be optimized to compensate for specific hardware limitations, such as slow parameter-update speeds or limited input bandwidth.},
  archive      = {J_AML},
  author       = {McCaughan, Adam N. and Oripov, Bakhrom G. and Ganesh, Natesh and Nam, Sae Woo and Dienstfrey, Andrew and Buckley, Sonia M.},
  doi          = {10.1063/5.0157645},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026118},
  shortjournal = {APL Mach. Learn.},
  title        = {Multiplexed gradient descent: Fast online training of modern datasets on hardware neural networks without backpropagation},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised machine learning discovery of structural units
and transformation pathways from imaging data. <em>AML</em>,
<em>1</em>(2), 026117. (<a
href="https://doi.org/10.1063/5.0147316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that unsupervised machine learning can be used to learn chemical transformation pathways from observational Scanning Transmission Electron Microscopy (STEM) data. To enable this analysis, we assumed the existence of atoms, a discreteness of atomic classes, and the presence of an explicit relationship between the observed STEM contrast and the presence of atomic units. With only these postulates, we developed a machine learning method leveraging a rotationally invariant variational autoencoder (VAE) that can identify the existing molecular fragments observed within a material. The approach encodes the information contained in STEM image sequences using a small number of latent variables, allowing the exploration of chemical transformation pathways by tracing the evolution of atoms in the latent space of the system. The results suggest that atomically resolved STEM data can be used to derive fundamental physical and chemical mechanisms involved, by providing encodings of the observed structures that act as bottom-up equivalents of structural order parameters. The approach also demonstrates the potential of variational (i.e., Bayesian) methods in the physical sciences and will stimulate the development of more sophisticated ways to encode physical constraints in the encoderâdecoder architectures and generative physical laws and causal relationships in the latent space of VAEs.},
  archive      = {J_AML},
  author       = {Kalinin, Sergei V. and Dyck, Ondrej and Ghosh, Ayana and Liu, Yongtao and Sumpter, Bobby G. and Ziatdinov, Maxim},
  doi          = {10.1063/5.0147316},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026117},
  shortjournal = {APL Mach. Learn.},
  title        = {Unsupervised machine learning discovery of structural units and transformation pathways from imaging data},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AnalogVNN: A fully modular framework for modeling and
optimizing photonic neural networks. <em>AML</em>, <em>1</em>(2),
026116. (<a href="https://doi.org/10.1063/5.0134156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present AnalogVNN, a simulation framework built on PyTorch that can simulate the effects of optoelectronic noise, limited precision, and signal normalization present in photonic neural network accelerators. We use this framework to train and optimize linear and convolutional neural networks with up to nine layers and â¼1.7 Ã 10 6 parameters, while gaining insights into how normalization, activation function, reduced precision, and noise influence accuracy in analog photonic neural networks. By following the same layer structure design present in PyTorch, the AnalogVNN framework allows users to convert most digital neural network models to their analog counterparts with just a few lines of code, taking full advantage of the open-source optimization, deep learning, and GPU acceleration libraries available through PyTorch.},
  archive      = {J_AML},
  author       = {Shah, Vivswan and Youngblood, Nathan},
  doi          = {10.1063/5.0134156},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026116},
  shortjournal = {APL Mach. Learn.},
  title        = {AnalogVNN: A fully modular framework for modeling and optimizing photonic neural networks},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pair distribution function analysis for oxide defect
identification through feature extraction and supervised learning.
<em>AML</em>, <em>1</em>(2), 026115. (<a
href="https://doi.org/10.1063/5.0130681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction and a neural network model are applied to predict defect types and concentrations in experimental anatase TiO 2 samples. A dataset of TiO 2 structures with vacancies and interstitials of oxygen and titanium is built, and the structures are relaxed using energy minimization. The features of the calculated pair distribution functions (PDFs) of these defected structures are extracted using linear methods (principal component analysis and non-negative matrix factorization) and non-linear methods (autoencoder and convolutional neural network). The extracted features are used as inputs to a neural network that maps feature weights to the concentration of each defect type. The performance of this machine learning pipeline is validated by predicting defect concentrations based on experimentally measured TiO 2 PDFs and comparing the results to brute-force predictions. A physics-based initialization of the autoencoder has the highest accuracy in predicting defect concentrations. This model incorporates physical interpretability and predictability of material structures, enabling a more efficient characterization process with scattering data.},
  archive      = {J_AML},
  author       = {Zhang, Shuyan and Gong, Jie and Chu, Sharon and Xiao, Daniel Z. and Reeja-Jayan, B. and McGaughey, Alan J. H.},
  doi          = {10.1063/5.0130681},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026115},
  shortjournal = {APL Mach. Learn.},
  title        = {Pair distribution function analysis for oxide defect identification through feature extraction and supervised learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing composition ratio of magnetic alloy multilayer for
transverse thermoelectric conversion by bayesian optimization.
<em>AML</em>, <em>1</em>(2), 026114. (<a
href="https://doi.org/10.1063/5.0140332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrated the effectiveness of the machine learning method combined with first-principles calculations for the enhancement of the anomalous Nernst effect (ANE) of multilayers. The composition ratio of CoNi homogeneous alloy superlattices was optimized by Bayesian optimization so as to maximize the transverse thermoelectric conductivity ( Î± xy ). The nonintuitive optimal composition with a large Î± xy of â¼10Â AÂ K â1 m â1 was identified through the two-step Bayesian optimization using rough and fine candidate pools. The Berry curvature and band dispersion analyses revealed that Î± xy is enhanced by the appearance of the flat band near the Fermi level due to the multilayer formation. The magnitude of the energy derivative of the anomalous Hall conductivity increases owing to the large Berry curvature near the flat band along the R-M high symmetry line, which emerges only in the optimized superlattice, leading to the Î± xy enhancement. The effective method verified here will broaden the choices of ANE materials to more complex systems and, therefore, lead to the development of transverse thermoelectric conversion technologies.},
  archive      = {J_AML},
  author       = {Chiba, Naoki and Masuda, Keisuke and Uchida, Ken-ichi and Miura, Yoshio},
  doi          = {10.1063/5.0140332},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026114},
  shortjournal = {APL Mach. Learn.},
  title        = {Designing composition ratio of magnetic alloy multilayer for transverse thermoelectric conversion by bayesian optimization},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A machine learning-based prediction of crystal orientations
for multicrystalline materials. <em>AML</em>, <em>1</em>(2), 026113. (<a
href="https://doi.org/10.1063/5.0138099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We established a rapid, low-cost, and accurate technique to measure crystallographic orientations in multicrystalline materials by optical images and machine learning. A long short-term memory neural network was trained with pairs of light reflection patterns and the correct orientations of each grain, successfully predicting orientation with an error median of 8.61Â°. The model was improved by diverse data taken from various incident light angles and by data augmentation. When trained on different incident angles, the model was capable of estimating different orientations. This is related to the geometrical configuration of the incident light angles and surface facets of the crystal. The failure in certain orientations is thought to be complemented by supplementary data taken from different incident angles. Combining data from multiple incident angles, we acquired an error median of 4.35Â°. Data augmentation was successfully performed, reducing error by an additional 35\%. This technique can provide the crystallographic orientations of a 15 Ã 15Â cm 2 sized wafer in less than 8Â min, while baseline techniques such as electron backscatter diffraction and Laue scanner may take more than 10Â h. The rapid and accurate measurement can accelerate data collection for full-sized ingots, helping us gain a comprehensive understanding of crystal growth. We believe that our technique will contribute to controlling crystalline structure for the fabrication of high-performance materials.},
  archive      = {J_AML},
  author       = {Hara, Kyoka and Kojima, Takuto and Kutsukake, Kentaro and Kudo, Hiroaki and Usami, Noritaka},
  doi          = {10.1063/5.0138099},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026113},
  shortjournal = {APL Mach. Learn.},
  title        = {A machine learning-based prediction of crystal orientations for multicrystalline materials},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent performance inference: A graph neural network
approach to modeling maximum achievable throughput in optical networks.
<em>AML</em>, <em>1</em>(2), 026112. (<a
href="https://doi.org/10.1063/5.0137426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key performance metrics for optical networks is the maximum achievable throughput for a given network. Determining it, however, is a nondeterministic polynomial time (NP) hard optimization problem, often solved via computationally expensive integer linear programming (ILP) formulations. These are infeasible to implement as objectives, even on very small node scales of a few tens of nodes. Alternatively, heuristics are used although these, too, require considerable computation time for a large number of networks. There is, thus, a need for an ultra-fast and accurate performance evaluation of optical networks. For the first time, we propose the use of a geometric deep learning model, message passing neural networks (MPNNs), to learn the relationship between node and edge features, the network structure, and the maximum achievable network throughput. We demonstrate that MPNNs can accurately predict the maximum achievable throughput while reducing the computational time by up to five-orders of magnitude compared to the ILP for small networks (10â15 nodes) and compared to a heuristic for large networks (25â100 nodes)âproving their suitability for the design and optimization of optical networks on different time- and distance-scales.},
  archive      = {J_AML},
  author       = {Matzner, Robin and Luo, Ruijie and Zervas, Georgios and Bayvel, Polina},
  doi          = {10.1063/5.0137426},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026112},
  shortjournal = {APL Mach. Learn.},
  title        = {Intelligent performance inference: A graph neural network approach to modeling maximum achievable throughput in optical networks},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Glass transition of amorphous polymeric materials informed
by machine learning. <em>AML</em>, <em>1</em>(2), 026111. (<a
href="https://doi.org/10.1063/5.0137357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The glass transition temperature ( T g ) is used to determine thermophysical properties of polymer materials and is often considered one of the most important descriptors. Methods for predicting various physical properties of materials based on machine learning algorithms and key molecular descriptors are efficient and accurate. However, it still needs improvements because an overly complex model is less practical and difficult to generalize. In addition, obtaining a large number of samples to achieve accurate predictions remains a challenge due to the complex and lengthy experimental process. In this work, based on T g of 100 polymers, we use a feature selection algorithm combining FeatureWiz and the least absolute shrinkage and selection operator to quickly select molecular descriptors that are minimally redundant and maximally relevant to T g . The processed dataset is interpolated from the original dataset using the nearest neighbor interpolation algorithm to solve the data deficiency problem. Finally, the synthetic minority oversampling technique algorithm is used to solve the data imbalance problem. The augmented dataset is used to construct the extreme gradient boosting prediction model to achieve good prediction accuracy. The experimental results demonstrate the robustness of the proposed model and the accuracy of its prediction results.},
  archive      = {J_AML},
  author       = {Hu, Anwen and Huang, Yongdi and Chen, Qionghai and Huang, Wanhui and Wu, Xiaohui and Cui, Lihong and Dong, Yining and Liu, Jun},
  doi          = {10.1063/5.0137357},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026111},
  shortjournal = {APL Mach. Learn.},
  title        = {Glass transition of amorphous polymeric materials informed by machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label free identification of different cancer cells using
deep learning-based image analysis. <em>AML</em>, <em>1</em>(2), 026110.
(<a href="https://doi.org/10.1063/5.0141730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer diagnostics is an important field of cancer recovery and survival with many expensive procedures needed to administer the correct treatment. Machine Learning (ML) approaches can help with the diagnostic prediction from circulating tumor cells in liquid biopsy or from a primary tumor in solid biopsy. After predicting the metastatic potential from a deep learning model, doctors in a clinical setting can administer a safe and correct treatment for a specific patient. This paper investigates the use of deep convolutional neural networks for predicting a specific cancer cell line as a tool for label free identification. Specifically, deep learning strategies for weight initialization and performance metrics are described, with transfer learning and the accuracy metric utilized in this work. The equipment used for prediction involves brightfield microscopy without the use of chemical labels, advanced instruments, or time-consuming biological techniques, giving an advantage over current diagnostic methods. In the procedure, three different binary datasets of well-known cancer cell lines were collected, each having a difference in metastatic potential. Two different classification models were adopted (EfficientNetV2 and ResNet-50) with the analysis given for each stage in the ML architecture. The training results for each model and dataset are provided and systematically compared. We found that the test set accuracy showed favorable performance for both ML models with EfficientNetV2 accuracy reaching up to 99\%. These test results allowed EfficientNetV2 to outperform ResNet-50 at an average percent increase of 3.5\% for each dataset. The high accuracy obtained from the predictions demonstrates that the system can be retrained on a large-scale clinical dataset.},
  archive      = {J_AML},
  author       = {Gardner, Karl and Joshi, Rutwik and Hasan Kashem, Md Nayeem and Pham, Thanh Quang and Lu, Qiugang and Li, Wei},
  doi          = {10.1063/5.0141730},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026110},
  shortjournal = {APL Mach. Learn.},
  title        = {Label free identification of different cancer cells using deep learning-based image analysis},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Physics-constrained 3D convolutional neural networks for
electrodynamics. <em>AML</em>, <em>1</em>(2), 026109. (<a
href="https://doi.org/10.1063/5.0132433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a physics-constrained neural network (PCNN) approach to solving Maxwellâs equations for the electromagnetic fields of intense relativistic charged particle beams. We create a 3D convolutional PCNN to map time-varying current and charge densities J(r, t) and Ï(r, t) to vector and scalar potentials A(r, t) and Ï(r, t) from which we generate electromagnetic fields according to Maxwellâs equations: B = â Ã A and E = ââÏ â âA/ât. Our PCNNs satisfy hard constraints, such as ââÂ·âB = 0, by construction. Soft constraints push A and Ï toward satisfying the Lorenz gauge.},
  archive      = {J_AML},
  author       = {Scheinker, Alexander and Pokharel, Reeju},
  doi          = {10.1063/5.0132433},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026109},
  shortjournal = {APL Mach. Learn.},
  title        = {Physics-constrained 3D convolutional neural networks for electrodynamics},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of VMM computation strategies to implement BNN
applications on RRAM arrays. <em>AML</em>, <em>1</em>(2), 026108. (<a
href="https://doi.org/10.1063/5.0139583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing interest in edge-AI solutions and advances in the field of quantized neural networks have led to hardware efficient binary neural networks (BNNs). Extreme BNNs utilize only binary weights and activations, making them more memory efficient. Such networks can be realized using exclusive-NOR (XNOR) gates and popcount circuits. The analog in-memory realization of BNNs utilizing emerging non-volatile memory devices has been widely explored recently. However, most realizations typically use 2T-2R synapses, resulting in sub-optimal area utilization. In this study, we investigate alternate computation mapping strategies to realize BNN using selectorless resistive random access memory arrays. A new differential computation scheme that shows a comparable performance with the well-established XNOR computation strategy is proposed. Through extensive experimental characterization, BNN implementation using a non-filamentary bipolar oxide-based random access memory device-based crossbar is demonstrated for two datasets: (i) experimental characterization was performed on a thermal-image based Rock-Paper-Scissors dataset to analyze the impact of sneak-paths with real-hardware experiments. (ii) Large-scale BNN simulations on the Fashion-MNIST dataset with multi-level cell characteristics of non-filamentary devices are performed to demonstrate the impact of device non-idealities.},
  archive      = {J_AML},
  author       = {Parmar, Vivek and Kingra, Sandeep Kaur and Negi, Shubham and Suri, Manan},
  doi          = {10.1063/5.0139583},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026108},
  shortjournal = {APL Mach. Learn.},
  title        = {Analysis of VMM computation strategies to implement BNN applications on RRAM arrays},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rotationally equivariant super-resolution of velocity fields
in two-dimensional flows using convolutional neural networks.
<em>AML</em>, <em>1</em>(2), 026107. (<a
href="https://doi.org/10.1063/5.0132326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the super-resolution of velocity fields in two-dimensional flows from the viewpoint of rotational equivariance. Super-resolution refers to techniques that enhance the resolution of an image from low to high resolution, and it has recently been applied in fluid mechanics. Rotational equivariance of super-resolution models is defined as the property by which the super-resolved velocity field is rotated according to a rotation of the input, leading to inferences that are covariant with the orientation of fluid systems. In physics, covariance is often related to symmetries. To better understand the connection with symmetries, the notion of rotational consistency of datasets is introduced within the framework of supervised learning, which is defined as the invariance of pairs of low- and high-resolution velocity fields with respect to rotation. This consistency is sufficient and necessary for super-resolution models to learn rotational equivariance from large datasets. Such a large dataset is not required when rotational equivariance is imposed on super-resolution models through the use of prior knowledge in the form of equivariant kernel patterns. Nonetheless, even if a fluid system has rotational symmetry, this symmetry may not carry over to a velocity dataset, which is not rotationally consistent. This inconsistency can arise when the rotation does not commute with the generation of low-resolution velocity fields. These theoretical assertions are supported by the results of numerical experiments, where two existing convolutional neural networks (CNNs) are converted into rotationally equivariant CNNs and the inferences of these CNNs are compared after the supervised training.},
  archive      = {J_AML},
  author       = {Yasuda, Yuki and Onishi, Ryo},
  doi          = {10.1063/5.0132326},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026107},
  shortjournal = {APL Mach. Learn.},
  title        = {Rotationally equivariant super-resolution of velocity fields in two-dimensional flows using convolutional neural networks},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning of nonlinear flame fronts development due to
darrieusâlandau instability. <em>AML</em>, <em>1</em>(2), 026106. (<a
href="https://doi.org/10.1063/5.0139857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The DarrieusâLandau instability is studied using a data-driven, deep neural network approach. The task is set up to learn a time-advancement operator mapping any given flame front to a future time. A recurrent application of such an operator rolls out a long sequence of predicted flame fronts, and a learned operator is required to not only make accurate short-term predictions but also reproduce characteristic nonlinear behavior, such as fractal front structures and detached flame pockets. Using two datasets of flame front solutions obtained from a heavy-duty direct numerical simulation and a light-duty modeling equation, we compare the performance of three state-of-art operator-regression network methods: convolutional neural networks, Fourier neural operator (FNO), and deep operator network. We show that, for learning complicated front evolution, FNO gives the best recurrent predictions in both the short and long term. A consistent extension allowing the operator-regression networks to handle complicated flame front shape is achieved by representing the latter as an implicit curve.},
  archive      = {J_AML},
  author       = {Yu, Rixin},
  doi          = {10.1063/5.0139857},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026106},
  shortjournal = {APL Mach. Learn.},
  title        = {Deep learning of nonlinear flame fronts development due to DarrieusâLandau instability},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DyFraNet: Forecasting and backcasting dynamic fracture
mechanics in space and time using a 2D-to-3D deep neural network.
<em>AML</em>, <em>1</em>(2), 026105. (<a
href="https://doi.org/10.1063/5.0135015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamics of material failure is a critical phenomenon relevant to a range of scientific and engineering fields, from healthcare to structural materials. We propose a specially designed deep neural network, DyFraNet, which can predict dynamic fracture behaviors by identifying a complete history of fracture propagationâfrom the onset of cracking, as a crack grows through the material, modeled as a series of frames evolving over time and dependent on each other. Furthermore, the model can not only forecast future fracture processes but also backcast to elucidate past fracture histories. In this scenario, once provided with the outcome of a fracture event, the model will reveal past events that led to this state and can also predict future evolutions of the failure process. By comparing the predicted results with atomistic-level simulations and theory, we show that DyFraNet can capture dynamic fracture mechanics by accurately predicting how cracks develop over time, including measures such as the crack speed, as well as when cracks become unstable. We use Gradient-weighted Class Activation Mapping, Grad-CAM, to interpret how DyFraNet perceives the relationship between geometric conditions and fracture dynamics, and we find that DyFraNet pays special attention to the areas around crack tips that have a critical influence in the early stage of fracture propagation. In later stages, the model pays increased attention to the existing or newly formed damaged regions in the material. The proposed approach offers the potential to accelerate the exploration of dynamical processes in material design against failure and can be adapted for all kinds of dynamical problems.},
  archive      = {J_AML},
  author       = {Hsu, Yu-Chuan and Buehler, Markus J.},
  doi          = {10.1063/5.0135015},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026105},
  shortjournal = {APL Mach. Learn.},
  title        = {DyFraNet: Forecasting and backcasting dynamic fracture mechanics in space and time using a 2D-to-3D deep neural network},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stoichiometric growth of SrTiO3 films via bayesian
optimization with adaptive prior mean. <em>AML</em>, <em>1</em>(2),
026104. (<a href="https://doi.org/10.1063/5.0132768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perovskite insulator SrTiO3 (STO) is expected to be applied to the next generation of electronic and photonic devices as high-k capacitors and photocatalysts. However, reproducible growth of highly insulating stoichiometric (STO) films remains challenging due to the difficulty of precise stoichiometry control in perovskite oxide films. Here, to grow stoichiometric (STO) thin films by fine-tuning multiple growth conditions, we developed a new Bayesian optimization (BO)-based machine learning method that encourages exploration of the search space by varying the prior mean to get out of suboptimal growth condition parameters. Using simulated data, we demonstrate the efficacy of the new BO method, which reproducibly reaches the global best conditions. With the BO method implemented in machine-learning-assisted molecular beam epitaxy (ML-MBE), a highly insulating stoichiometric (STO) film with no absorption in the bandgap was developed in only 44 MBE growth runs. The proposed algorithm provides an efficient experimental design platform that is not as dependent on the experience of individual researchers and will accelerate not only oxide electronics but also various material syntheses.},
  archive      = {J_AML},
  author       = {Wakabayashi, Yuki K. and Otsuka, Takuma and Krockenberger, Yoshiharu and Sawada, Hiroshi and Taniyasu, Yoshitaka and Yamamoto, Hideki},
  doi          = {10.1063/5.0132768},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026104},
  shortjournal = {APL Mach. Learn.},
  title        = {Stoichiometric growth of SrTiO3 films via bayesian optimization with adaptive prior mean},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pulse-stream impact on recognition accuracy of reservoir
computing from SiO2-based low power memory devices. <em>AML</em>,
<em>1</em>(2), 026103. (<a
href="https://doi.org/10.1063/5.0131524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reservoir computing (RC)-based neuromorphic applications exhibit extremely low power consumption, thus challenging the use of deep neural networks in terms of both consumption requirements and integration density. Under this perspective, this work focuses on the basic principles of RC systems. The ability of self-selective conductive-bridging random access memory devices to operate in two modes, namely, volatile and non-volatile, by regulating the applied voltage is first presented. We then investigate the relaxation time of these devices as a function of the applied amplitude and pulse duration, a critical step in determining the desired non-linearity by the reservoir. Moreover, we present an in-depth study of the impact of selecting the appropriate pulse-stream and its final effects on the total power consumption and recognition accuracy in a handwritten digit recognition application from the National Institute of Standards and Technology dataset. Finally, we conclude at the optimal pulse-stream of 3-bit, through the minimization of two cost criteria, with the total power remaining at 287 ÂµW and simultaneously achieving 82.58\% recognition accuracy upon the test set.},
  archive      = {J_AML},
  author       = {Tsioustas, C. and Bousoulas, P. and Kleitsiotis, G. and Tsoukalas, D.},
  doi          = {10.1063/5.0131524},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026103},
  shortjournal = {APL Mach. Learn.},
  title        = {Pulse-stream impact on recognition accuracy of reservoir computing from SiO2-based low power memory devices},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic identification of edge localized modes in the
DIII-d tokamak. <em>AML</em>, <em>1</em>(2), 026102. (<a
href="https://doi.org/10.1063/5.0134001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusion power production in tokamaks uses discharge configurations that risk producing strong type I edge localized modes. The largest of these modes will likely increase impurities in the plasma and potentially damage plasma facing components, such as the protective heat and particle divertor. Machine learning-based prediction and control may provide for the automatic detection and mitigation of these damaging modes before they grow too large to suppress. To that end, large labeled datasets are required for the supervised training of machine learning models. We present an algorithm that achieves 97.7\% precision when automatically labeling edge localized modes in the large DIII-D tokamak discharge database. The algorithm has no user controlled parameters and is largely robust to tokamak and plasma configuration changes. This automatically labeled database of events can subsequently feed future training of machine learning models aimed at autonomous edge localized mode control and suppression.},
  archive      = {J_AML},
  author       = {OâShea, Finn H. and Joung, Semin and Smith, David R. and Coffee, Ryan},
  doi          = {10.1063/5.0134001},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026102},
  shortjournal = {APL Mach. Learn.},
  title        = {Automatic identification of edge localized modes in the DIII-D tokamak},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long-term forecasts of residential energy profiles based on
Conv2D and LSTM models (electricity- and gas-based households).
<em>AML</em>, <em>1</em>(2), 026101. (<a
href="https://doi.org/10.1063/5.0137443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For power system operation and expansion of grid-import systems, an accurate forecast model plays an essential role in the better management of household electricity demands. With the aim of finding an accurate forecast model in the proper representation of various household energy profiles, our research objective is centered on the development of a reliable forecast system for a group of 24-household energy consumers. In this energy study, we proposed long-term forecasts of (1) residential energy profiles within the multi-classification framework and (2) energy costing of the household demands using the Keras two-dimensional convolutional neural network (Conv2D) model and long short-term memory (LSTM) models. These high-level Keras neural networks are built to extract multivariate features for household energy consumption modeling and forecasting. The proposed forecast systems utilized a similar model hyperparameter configuration, while the forecast skills are validated with spatialâtemporal variation datasets of ten remote locations. The actual costs of household demand and supply are estimated and compared with Conv2D predictions. The finding results (hourly and seasonal predictions and model evaluation) revealed that Conv2D and LSTM forecast systems are promising for household energy forecast solutions. Experimental results of the Conv2D predictive system achieved better forecast skills [correlation coefficient (0.727â0.994) and root mean square error (0.190â0.868)] than LSTM forecasts (0.308â0.987 and 0.278â1.212). However, experimental findings revealed that forecast skills of the predictive systems in residential energy demand predictions are highly influenced by the (1) quality of input datasets, (2) model hyperparameter tuning approach, and (3) learning rate of selected network optimizer(s).},
  archive      = {J_AML},
  author       = {Olaofe, Zaccheus O.},
  doi          = {10.1063/5.0137443},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {026101},
  shortjournal = {APL Mach. Learn.},
  title        = {Long-term forecasts of residential energy profiles based on Conv2D and LSTM models (electricity- and gas-based households)},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating the design and development of polymeric
materials via deep learning: Current status and future challenges.
<em>AML</em>, <em>1</em>(2), 021501. (<a
href="https://doi.org/10.1063/5.0131067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design and development of polymeric materials have been a hot domain for decades. However, traditional experiments and molecular simulations are time-consuming and labor-intensive, which no longer meet the requirements of new materials development. With the rapid advances of artificial intelligence and materials informatics, machine learning algorithms are increasingly applied in materials science, aiming to shorten the development period of new materials. With the evolution of polymeric materials, the structure of polymers has become more and more complex. Traditional machine learning algorithms often do not perform satisfactorily when dealing with complex data. Presently, deep learning algorithms, including deep neural networks, convolutional neural networks, generative adversarial networks, recurrent neural networks, and graph neural networks, show their uniquely excellent learning capabilities for large and complex data, which will be a powerful tool for the design and development of polymeric materials. This Review introduces principles of several currently popular deep learning algorithms and discusses their multiple applications in the materials field. Applications range from property prediction and molecular generation at the molecular level to structure identification and material synthesis in polymers. Finally, future challenges and opportunities for the application of deep learning in polymeric materials are discussed.},
  archive      = {J_AML},
  author       = {Li, Dazi and Ru, Yi and Chen, Zhudan and Dong, Caibo and Dong, Yining and Liu, Jun},
  doi          = {10.1063/5.0131067},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {021501},
  shortjournal = {APL Mach. Learn.},
  title        = {Accelerating the design and development of polymeric materials via deep learning: Current status and future challenges},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Materials cartography: A forward-looking perspective on
materials representation and devising better maps. <em>AML</em>,
<em>1</em>(2), 020901. (<a
href="https://doi.org/10.1063/5.0149804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is gaining popularity as a tool for materials scientists to accelerate computation, automate data analysis, and predict materials properties. The representation of input material features is critical to the accuracy, interpretability, and generalizability of data-driven models for scientific research. In this Perspective, we discuss a few central challenges faced by ML practitioners in developing meaningful representations, including handling the complexity of real-world industry-relevant materials, combining theory and experimental data sources, and describing scientific phenomena across timescales and length scales. We present several promising directions for future research: devising representations of varied experimental conditions and observations, the need to find ways to integrate machine learning into laboratory practices, and making multi-scale informatics toolkits to bridge the gaps between atoms, materials, and devices.},
  archive      = {J_AML},
  author       = {Torrisi, Steven B. and Bazant, Martin Z. and Cohen, Alexander E. and Cho, Min Gee and HummelshÃ¸j, Jens S. and Hung, Linda and Kamat, Gaurav and Khajeh, Arash and Kolluru, Adeesh and Lei, Xiangyun and Ling, Handong and Montoya, Joseph H. and Mueller, Tim and Palizhati, Aini and Paren, Benjamin A. and Phan, Brandon and Pietryga, Jacob and Sandraz, Elodie and Schweigert, Daniel and Shao-Horn, Yang and Trewartha, Amalie and Zhu, Ruijie and Zhuang, Debbie and Sun, Shijing},
  doi          = {10.1063/5.0149804},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {020901},
  shortjournal = {APL Mach. Learn.},
  title        = {Materials cartography: A forward-looking perspective on materials representation and devising better maps},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brains and bytes: Trends in neuromorphic technology.
<em>AML</em>, <em>1</em>(2), 020401. (<a
href="https://doi.org/10.1063/5.0162712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AML},
  author       = {Mehonic, Adnan and Eshraghian, Jason},
  doi          = {10.1063/5.0162712},
  journal      = {APL Machine Learning},
  number       = {2},
  pages        = {020401},
  shortjournal = {APL Mach. Learn.},
  title        = {Brains and bytes: Trends in neuromorphic technology},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A machine learning framework for elastic constants
predictions in multi-principal element alloys. <em>AML</em>,
<em>1</em>(1), 016109. (<a
href="https://doi.org/10.1063/5.0129928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On the one hand, multi-principal element alloys (MPEAs) have created a paradigm shift in alloy design due to large compositional space, whereas on the other, they have presented enormous computational challenges for theory-based materials design, especially density functional theory (DFT), which is inherently computationally expensive even for traditional dilute alloys. In this paper, we present a machine learning framework, namely PREDICT ( PR edict properties from E xisting D atabase I n C omplex alloys T erritory), that opens a pathway to predict elastic constants in large compositional space with little computational expense. The framework only relies on the DFT database of binary alloys and predicts VoigtâReussâHill Youngâs modulus, shear modulus, bulk modulus, elastic constants, and Poissonâs ratio in MPEAs. We show that the key descriptors of elastic constants are the AâB bond length and cohesive energy. The framework can predict elastic constants in hypothetical compositions as long as the constituent elements are present in the database, thereby enabling property exploration in multi-compositional systems. We illustrate predictions in a FCC Ni-Cu-Au-Pd-Pt system.},
  archive      = {J_AML},
  author       = {Linton, Nathan and Aidhy, Dilpuneet S.},
  doi          = {10.1063/5.0129928},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016109},
  shortjournal = {APL Mach. Learn.},
  title        = {A machine learning framework for elastic constants predictions in multi-principal element alloys},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiNLNet: Gauging required nonlinearity in deep neural
networks. <em>AML</em>, <em>1</em>(1), 016108. (<a
href="https://doi.org/10.1063/5.0134713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feedforward deep neural networks (DNNs) commonly involve layer-wise linear operations and subsequent nonlinear operations, which are repeated through all layers. The nonlinear operations by nonlinear activations in each layer remarkably enhance the expressiveness of DNNs, resulting in the great success in a variety of application domains. Although the necessity of layer-wise nonlinear operations is agreed, the optimal nonlinearity for each layer in a given DNN is not clear. In this regard, we propose an easy-to-use method to layer-wise measure the optimal nonlinearity for a given DNN using its replica termed a linear-nonlinear network (LiNLNet). The key to the LiNLNet is the use of linear-nonlinear units (LiNLUs) whose degree of nonlinearity is parameterized by a trainable parameter p . The parameter p is shared among all LiNLUs in a given layer, thus indicating the layer-wise optimal nonlinearity. This method allows layer-level pruning such that the layers that do not require nonlinearity are merged into the subsequent layers, reducing computational complexity. For proofs of concept, we applied the proposed method to a MLP, AlexNet, VGG16, and ResNet18 on CIFAR-10 and ImageNet. The results commonly indicate the last hidden layer as a linear layer that may be merged into the output layer, reducing memory usage by 27\% while maintaining the accuracy for LiNL-AlexNet on ImageNet.},
  archive      = {J_AML},
  author       = {Jin, SeongMin and Jeong, Doo Seok},
  doi          = {10.1063/5.0134713},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016108},
  shortjournal = {APL Mach. Learn.},
  title        = {LiNLNet: Gauging required nonlinearity in deep neural networks},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-speed CMOS-free purely spintronic asynchronous
recurrent neural network. <em>AML</em>, <em>1</em>(1), 016107. (<a
href="https://doi.org/10.1063/5.0129006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exceptional capabilities of the human brain provide inspiration for artificially intelligent hardware that mimics both the function and the structure of neurobiology. In particular, the recent development of nanodevices with biomimetic characteristics promises to enable the development of neuromorphic architectures with exceptional computational efficiency. In this work, we propose biomimetic neurons comprised of domain wall-magnetic tunnel junctions that can be integrated into the first trainable CMOS-free recurrent neural network with biomimetic components. This paper demonstrates the computational effectiveness of this system for benchmark tasks and its superior computational efficiency relative to alternative approaches for recurrent neural networks.},
  archive      = {J_AML},
  author       = {Mathews, Pranav O. and Duffee, Christian B. and Thayil, Abel and Stovall, Ty E. and Bennett, Christopher H. and Garcia-Sanchez, Felipe and Marinella, Matthew J. and Incorvia, Jean Anne C. and Hassan, Naimul and Hu, Xuan and Friedman, Joseph S.},
  doi          = {10.1063/5.0129006},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016107},
  shortjournal = {APL Mach. Learn.},
  title        = {High-speed CMOS-free purely spintronic asynchronous recurrent neural network},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust design of semi-automated clustering models for
4D-STEM datasets. <em>AML</em>, <em>1</em>(1), 016106. (<a
href="https://doi.org/10.1063/5.0130546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Materials discovery and design require characterizing material structures at the nanometer and sub-nanometer scale. Four-Dimensional Scanning Transmission Electron Microscopy (4D-STEM) resolves the crystal structure of materials, but many 4D-STEM data analysis pipelines are not suited for the identification of anomalous and unexpected structures. This work introduces improvements to the iterative Non-Negative Matrix Factorization (NMF) method by implementing consensus clustering for ensemble learning. We evaluate the performance of models during parameter tuning and find that consensus clustering improves performance in all cases and is able to recover specific grains missed by the best performing model in the ensemble. The methods introduced in this work can be applied broadly to materials characterization datasets to aid in the design of new materials.},
  archive      = {J_AML},
  author       = {Bruefach, Alexandra and Ophus, Colin and Scott, M. C.},
  doi          = {10.1063/5.0130546},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016106},
  shortjournal = {APL Mach. Learn.},
  title        = {Robust design of semi-automated clustering models for 4D-STEM datasets},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). An efficiency-driven, correlation-based feature elimination
strategy for small datasets. <em>AML</em>, <em>1</em>(1), 016105. (<a
href="https://doi.org/10.1063/5.0118207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With big datasets and highly efficient algorithms becoming increasingly available for many problem sets, rapid advancements and recent breakthroughs achieved in the field of machine learning encourage more and more scientific fields to make use of such a computational data analysis. Still, for many research problems, the amount of data available for training a machine learning (ML) model is very limited. An important strategy to combat the problems arising from data sparsity is feature eliminationâa method that aims at reducing the dimensionality of an input feature space. Most such strategies exclusively focus on analyzing pairwise correlations, or they eliminate features based on their relation to a selected output label or by optimizing performance measures of a certain ML model. However, those strategies do not necessarily remove redundant information from datasets and cannot be applied to certain situations, e.g., to unsupervised learning models. Neither of these limitations applies to the network-based, correlation-driven redundancy elimination (NETCORE) algorithm introduced here, where the size of a feature vector is reduced by considering both redundancy and elimination efficiency. The NETCORE algorithm is model-independent, does not require an output label, and is applicable to all kinds of correlation topographies within a dataset. Thus, this algorithm has the potential to be a highly beneficial preprocessing tool for various machine learning pipelines.},
  archive      = {J_AML},
  author       = {Rickert, Carolin A. and Henkel, Manuel and Lieleg, Oliver},
  doi          = {10.1063/5.0118207},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016105},
  shortjournal = {APL Mach. Learn.},
  title        = {An efficiency-driven, correlation-based feature elimination strategy for small datasets},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impact of analog memory device failure on in-memory
computing inference accuracy. <em>AML</em>, <em>1</em>(1), 016104. (<a
href="https://doi.org/10.1063/5.0131797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing using analog non-volatile memory (NVM) devices can improve the speed and reduce the latency of deep neural network (DNN) inference. It has been recently shown that neuromorphic crossbar arrays, where each weight is implemented using analog conductance values of phase-change memory devices, achieve competitive accuracy and high power efficiency. However, due to the large amount of NVMs needed and the challenge for making analog NVM devices, these chips typically include some failed devices from fabrication or developed over time. We study the impact of these failed devices on the analog in-memory computing accuracy for various networks. We show that larger networks with fewer reused layers are more tolerable to failed devices. Devices stuck at high resistance states are more tolerable than devices stuck at low resistance states. To improve the robustness of DNNs to defective devices, we develop training methods that add noise and corrupt devices in the weight matrices during network training and show that this can increase the network accuracy in the presence of the failed devices. We also provide estimated maximum defective device tolerance of some common networks.},
  archive      = {J_AML},
  author       = {Li, Ning and Tsai, Hsinyu and Narayanan, Vijay and Rasch, Malte},
  doi          = {10.1063/5.0131797},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016104},
  shortjournal = {APL Mach. Learn.},
  title        = {Impact of analog memory device failure on in-memory computing inference accuracy},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial neural network-based streamline tracing strategy
applied to hypersonic waverider design. <em>AML</em>, <em>1</em>(1),
016103. (<a href="https://doi.org/10.1063/5.0127034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streamline tracing in hypersonic flows is essential for designing a high-performance waverider and intake. Conventionally, the streamline equations are solved after obtaining the velocity field over a basic flow field from simplified flow differential equations or three-dimensional computational fluid dynamics. The hypersonic waverider shape is generated by repeatedly applying the streamline tracing approach along several planes. This approach is computationally expensive for iterative waverider optimization. We provide a novel strategy where an Artificial Neural Network (ANN) is trained to directly predict the streamlines without solving the differential equations. We consider the standard simple cone-derived waverider using TaylorâMaccoll equations for the conical flow field as a template for the study. First, the streamlines from the shock are solved for a wide range of cone angle and Mach number conditions resulting in an extensive database. The streamlines are parameterized by a third-order polynomial, and an ANN is trained to predict the coefficients of the polynomial for arbitrary inputs of Mach number, cone angle, and streamline originating locations. We apply this strategy to design a cone-derived waverider and compare the geometry obtained with the standard conical waverider design method and the simplified waverider design method. The ANN technique is highly accurate, with a difference of 0.68\% from the standard method in the coordinates of the waverider. The performance of the three waveriders is compared using Reynolds averaged NavierâStokes simulations. The ANN-derived waverider does not indicate severe flow spillage at the leading edge. The new ANN-based approach is 20 times faster than the standard method.},
  archive      = {J_AML},
  author       = {Rao, Anagha G. and Siddharth, Umesh and Rao, Srisha M. V.},
  doi          = {10.1063/5.0127034},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016103},
  shortjournal = {APL Mach. Learn.},
  title        = {Artificial neural network-based streamline tracing strategy applied to hypersonic waverider design},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning assisted interpretation of creep and
fatigue life in titanium alloys. <em>AML</em>, <em>1</em>(1), 016102.
(<a href="https://doi.org/10.1063/5.0129037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making reliable predictions of the mechanical behavior of alloys with a prolonged service life is beneficial for many structural applications. In this work, we propose an interpretable machine learning (ML) approach to predict fatigue life cycles ( N f ) and creep rupture life ( t r ) in titanium-based alloys. Chemical compositions, experimental parameters, and alloy processing conditions are employed as descriptors for the development of gradient boost regression models for log-scaled N f and t r . The models are trained on an extensive experimental dataset, predicting log-scaled N f and t r with a very small root mean squared error of 0.17 and 0.15, respectively. An intuitive interpretation of the ML models is carried out via SHapley Additive exPlanations (SHAP) to understand the complex interplay of various features with N f and t r . The SHAP interpretation of the ML models reveals close agreement with the general creep equation and WÃ¶hler curve of fatigue. The approach proposed in this study can accelerate the design of novel Ti-based alloys with desired properties.},
  archive      = {J_AML},
  author       = {Swetlana, Sucheta and Rout, Ashish and Singh, Abhishek Kumar},
  doi          = {10.1063/5.0129037},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016102},
  shortjournal = {APL Mach. Learn.},
  title        = {Machine learning assisted interpretation of creep and fatigue life in titanium alloys},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking energy consumption and latency for neuromorphic
computing in condensed matter and particle physics. <em>AML</em>,
<em>1</em>(1), 016101. (<a
href="https://doi.org/10.1063/5.0116699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive use of artificial neural networks (ANNs), increasingly popular in many areas of scientific computing, rapidly increases the energy consumption of modern high-performance computing systems. An appealing and possibly more sustainable alternative is provided by novel neuromorphic paradigms, which directly implement ANNs in hardware. However, little is known about the actual benefits of running ANNs on neuromorphic hardware for use cases in scientific computing. Here, we present a methodology for measuring the energy cost and compute time for inference tasks with ANNs on conventional hardware. In addition, we have designed an architecture for these tasks and estimate the same metrics based on a state-of-the-art analog in-memory computing (AIMC) platform, one of the key paradigms in neuromorphic computing. Both methodologies are compared for a use case in quantum many-body physics in two-dimensional condensed matter systems and for anomaly detection at 40Â MHz rates at the Large Hadron Collider in particle physics. We find that AIMC can achieve up to one order of magnitude shorter computation times than conventional hardware at an energy cost that is up to three orders of magnitude smaller. This suggests great potential for faster and more sustainable scientific computing with neuromorphic hardware.},
  archive      = {J_AML},
  author       = {KÃ¶sters, Dominique J. and Kortman, Bryan A. and Boybat, Irem and Ferro, Elena and Dolas, Sagar and Ruiz de Austri, Roberto and Kwisthout, Johan and Hilgenkamp, Hans and Rasing, Theo and Riel, Heike and Sebastian, Abu and Caron, Sascha and Mentink, Johan H.},
  doi          = {10.1063/5.0116699},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {016101},
  shortjournal = {APL Mach. Learn.},
  title        = {Benchmarking energy consumption and latency for neuromorphic computing in condensed matter and particle physics},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-memory computing with emerging memory devices: Status and
outlook. <em>AML</em>, <em>1</em>(1), 010902. (<a
href="https://doi.org/10.1063/5.0136403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing (IMC) has emerged as a new computing paradigm able to alleviate or suppress the memory bottleneck, which is the major concern for energy efficiency and latency in modern digital computing. While the IMC concept is simple and promising, the details of its implementation cover a broad range of problems and solutions, including various memory technologies, circuit topologies, and programming/processing algorithms. This Perspective aims at providing an orientation map across the wide topic of IMC. First, the memory technologies will be presented, including both conventional complementary metal-oxide-semiconductor-based and emerging resistive/memristive devices. Then, circuit architectures will be considered, describing their aim and application. Circuits include both popular crosspoint arrays and other more advanced structures, such as closed-loop memory arrays and ternary content-addressable memory. The same circuit might serve completely different applications, e.g., a crosspoint array can be used for accelerating matrix-vector multiplication for forward propagation in a neural network and outer product for backpropagation training. The different algorithms and memory properties to enable such diversification of circuit functions will be discussed. Finally, the main challenges and opportunities for IMC will be presented.},
  archive      = {J_AML},
  author       = {Mannocci, P. and Farronato, M. and Lepri, N. and Cattaneo, L. and Glukhov, A. and Sun, Z. and Ielmini, D.},
  doi          = {10.1063/5.0136403},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {010902},
  shortjournal = {APL Mach. Learn.},
  title        = {In-memory computing with emerging memory devices: Status and outlook},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep language models for interpretative and predictive
materials science. <em>AML</em>, <em>1</em>(1), 010901. (<a
href="https://doi.org/10.1063/5.0134317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) has emerged as an indispensable methodology to describe, discover, and predict complex physical phenomena that efficiently help us learn underlying functional rules, especially in cases when conventional modeling approaches cannot be applied. While conventional feedforward neural networks are typically limited to performing tasks related to static patterns in data, recursive models can both work iteratively based on a changing input and discover complex dynamical relationships in the data. Deep language models can model flexible modalities of data and are capable of learning rich dynamical behaviors as they operate on discrete or continuous symbols that define the states of a physical system, yielding great potential toward end-to-end predictions. Similar to how words form a sentence, materials can be considered as a self-assembly of physically interacted building blocks, where the emerging functions of materials are analogous to the meaning of sentences. While discovering the fundamental relationships between building blocks and function emergence can be challenging, language models, such as recurrent neural networks and long-short term memory networks, and, in particular, attention models, such as the transformer architecture, can solve many such complex problems. Application areas of such models include protein folding, molecular property prediction, prediction of material failure of complex nonlinear architected materials, and also generative strategies for materials discovery. We outline challenges and opportunities, especially focusing on extending the deep-rooted kinship of humans with symbolism toward generalizable artificial intelligence (AI) systems using neuro-symbolic AI, and outline how tools such as ChatGPT and DALLÂ·E can drive materials discovery.},
  archive      = {J_AML},
  author       = {Hu, Yiwen and Buehler, Markus J.},
  doi          = {10.1063/5.0134317},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {010901},
  shortjournal = {APL Mach. Learn.},
  title        = {Deep language models for interpretative and predictive materials science},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Editorial: Welcome to APL machine learning. <em>AML</em>,
<em>1</em>(1), 010401. (<a
href="https://doi.org/10.1063/5.0143646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AML},
  author       = {Mehonic, Adnan},
  doi          = {10.1063/5.0143646},
  journal      = {APL Machine Learning},
  number       = {1},
  pages        = {010401},
  shortjournal = {APL Mach. Learn.},
  title        = {Editorial: Welcome to APL machine learning},
  volume       = {1},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
