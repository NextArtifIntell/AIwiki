<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETCV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietcv---82">IETCV - 82</h2>
<ul>
<li><details>
<summary>
(2023). DASTSiam: Spatio-temporal fusion and discriminative
enhancement for siamese visual tracking. <em>IETCV</em>, <em>17</em>(8),
1017–1033. (<a href="https://doi.org/10.1049/cvi2.12213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of deep neural networks has revolutionised object tracking tasks, and Siamese trackers have emerged as a prominent technique for this purpose. Existing Siamese trackers use a fixed template or template updating technique, but it is prone to overfitting, lacks the capacity to exploit global temporal sequences, and cannot utilise multi-layer features. As a result, it is challenging to deal with dramatic appearance changes in complicated scenarios. Siamese trackers also struggle to learn background information, which impairs their discriminative ability. Hence, two transformer-based modules, the Spatio-Temporal Fusion (ST) module and the Discriminative Enhancement (DE) module, are proposed to improve the performance of Siamese trackers. The ST module leverages cross-attention to accumulate global temporal cues and generates an attention matrix with ST similarity to enhance the template&#39;s adaptability to changes in target appearance. The DE module associates semantically similar points from the template and search area, thereby generating a learnable discriminative mask to enhance the discriminative ability of the Siamese trackers. In addition, a Multi-Layer ST module (ST + ML) was constructed, which can be integrated into Siamese trackers based on multi-layer cross-correlation for further improvement. The authors evaluate the proposed modules on four public datasets and show comparative performance compared to existing Siamese trackers.},
  archive      = {J_IETCV},
  author       = {Yucheng Huang and Eksan Firkat and Jinlai Zhang and Lijuan Zhu and Bin Zhu and Jihong Zhu and Askar Hamdulla},
  doi          = {10.1049/cvi2.12213},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1017-1033},
  shortjournal = {IET Comput. Vis.},
  title        = {DASTSiam: Spatio-temporal fusion and discriminative enhancement for siamese visual tracking},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A monocular image depth estimation method based on weighted
fusion and point-wise convolution. <em>IETCV</em>, <em>17</em>(8),
1005–1016. (<a href="https://doi.org/10.1049/cvi2.12212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing monocular depth estimation methods based on deep learning have difficulty in estimating the depth near the edges of the objects in an image when the depth distance between these objects changes abruptly and decline in accuracy when an image has more noises. Furthermore, these methods consume more hardware resources because they have huge network parameters. To solve these problems, this paper proposes a depth estimation method based on weighted fusion and point-wise convolution. The authors design a maximum-average adaptive pooling weighted fusion module (MAWF) that fuses global features and local features and a continuous point-wise convolution module for processing the fused features derived from the (MAWF) module. The two modules work closely together for three times to perform weighted fusion and point-wise convolution of features of multi-scale from the encoder output, which can better decode the depth information of a scene. Experimental results show that our method achieves state-of-the-art performance on the KITTI dataset with δ 1 up to 0.996 and the root mean square error metric down to 8% and has demonstrated the strong generalisation and robustness.},
  archive      = {J_IETCV},
  author       = {Chen Lei and Liang Zhengyou and Sun Yu},
  doi          = {10.1049/cvi2.12212},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {1005-1016},
  shortjournal = {IET Comput. Vis.},
  title        = {A monocular image depth estimation method based on weighted fusion and point-wise convolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizable and efficient cross-domain person
re-identification model using deep metric learning. <em>IETCV</em>,
<em>17</em>(8), 993–1004. (<a
href="https://doi.org/10.1049/cvi2.12214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the successful person re-ID models conduct supervised training and need a large number of training data. These models fail to generalise well on unseen unlabelled testing sets. The authors aim to learn a generalisable person re-identification model. The model uses one labelled source dataset and one unlabelled target dataset during training and generalises well on the target testing set. To this end, after a feature extraction by the ResNext-50 network, the authors optimise the model by three loss functions. (a) One loss function is designed to learn the features of the target domain by tuning the distances between target images. Therefore, the trained model will be more robust to overcome the intra-domain variations in the target domain and generalises well on the target testing set. (b) One triplet loss is used which considers both source and target domains and makes the model learn the inter-domain variations between source and target domain as well as the variations in the target domain. (c) Also, one loss function is for supervised learning on the labelled source domain. Extensive experiments on Market1501 and DukeMTMC re-ID show that the model achieves a very competitive performance compared with state-of-the-art models and also it requires an acceptable amount of GPU RAM compared to other successful models.},
  archive      = {J_IETCV},
  author       = {Saba Sadat Faghih Imani and Kazim Fouladi-Ghaleh and Hossein Aghababa},
  doi          = {10.1049/cvi2.12214},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {993-1004},
  shortjournal = {IET Comput. Vis.},
  title        = {Generalizable and efficient cross-domain person re-identification model using deep metric learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attribute-guided transformer for robust person
re-identification. <em>IETCV</em>, <em>17</em>(8), 977–992. (<a
href="https://doi.org/10.1049/cvi2.12215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies reveal the crucial role of local features in learning robust and discriminative representations for person re-identification (Re-ID). Existing approaches typically rely on external tasks, for example, semantic segmentation, or pose estimation, to locate identifiable parts of given images. However, they heuristically utilise the predictions from off-the-shelf models, which may be sub-optimal in terms of both local partition and computational efficiency. They also ignore the mutual information with other inputs, which weakens the representation capabilities of local features. In this study, the authors put forward a novel Attribute-guided Transformer (AiT), which explicitly exploits pedestrian attributes as semantic priors for discriminative representation learning. Specifically, the authors first introduce an attribute learning process, which generates a set of attention maps highlighting the informative parts of pedestrian images. Then, the authors design a Feature Diffusion Module (FDM) to iteratively inject attribute information into global feature maps, aiming at suppressing unnecessary noise and inferring attribute-aware representations. Last, the authors propose a Feature Aggregation Module (FAM) to exploit mutual information for aggregating attribute characteristics from different images, enhancing the representation capabilities of feature embedding. Extensive experiments demonstrate the superiority of our AiT in learning robust and discriminative representations. As a result, the authors achieve competitive performance with state-of-the-art methods on several challenging benchmarks without any bells and whistles.},
  archive      = {J_IETCV},
  author       = {Zhe Wang and Jun Wang and Junliang Xing},
  doi          = {10.1049/cvi2.12215},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {977-992},
  shortjournal = {IET Comput. Vis.},
  title        = {Attribute-guided transformer for robust person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sketch face recognition based on light semantic transformer
network. <em>IETCV</em>, <em>17</em>(8), 962–976. (<a
href="https://doi.org/10.1049/cvi2.12209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch face recognition has a wide range of applications in criminal investigation, but it remains a challenging task due to the small-scale sample and the semantic deficiencies caused by cross-modality differences. The authors propose a light semantic Transformer network to extract and model the semantic information of cross-modality images. First, the authors employ a meta-learning training strategy to obtain task-related training samples to solve the small sample problem. Then to solve the contradiction between the high complexity of the Transformer and the small sample problem of sketch face recognition, the authors build the light semantic transformer network by proposing a hierarchical group linear transformation and introducing parameter sharing, which can extract highly discriminative semantic features on small–scale datasets. Finally, the authors propose a domain-adaptive focal loss to reduce the cross-modality differences between sketches and photos and improve the training effect of the light semantic Transformer network. Extensive experiments have shown that the features extracted by the proposed method have significant discriminative effects. The authors’ method improves the recognition rate by 7.6% on the UoM-SGFSv2 dataset, and the recognition rate reaches 92.59% on the CUFSF dataset.},
  archive      = {J_IETCV},
  author       = {Lin Cao and Jianqiang Yin and Yanan Guo and Kangning Du and Fan Zhang},
  doi          = {10.1049/cvi2.12209},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {962-976},
  shortjournal = {IET Comput. Vis.},
  title        = {Sketch face recognition based on light semantic transformer network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic deformable transformer for end-to-end face
alignment. <em>IETCV</em>, <em>17</em>(8), 948–961. (<a
href="https://doi.org/10.1049/cvi2.12208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heatmap-based regression (HBR) methods have dominated for a long time in the face alignment field while these methods need complex design and post-processing. In this study, the authors propose an end-to-end and simple enough coordinate-based regression (CBR) method called Dynamic Deformable Transformer (DDT) for face alignment. Unlike general pre-defined landmark queries, DDT uses Dynamic Landmark Queries (DLQs) to query landmarks&#39; classes and coordinates together. Besides, DDT adopts a deformable attention mechanism rather than a regular attention mechanism which has a faster convergence speed and lower computational complexity. Experiment results on three mainstream datasets 300W, WFLW, and COFW demonstrate DDT exceeds the state-of-the-art CBR methods by a large margin and is comparable to the current state-of-the-art HBR methods with much less computational complexity.},
  archive      = {J_IETCV},
  author       = {Liming Han and Chi Yang and Qing Li and Bin Yao and Zixian Jiao and Qianyang Xie},
  doi          = {10.1049/cvi2.12208},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {948-961},
  shortjournal = {IET Comput. Vis.},
  title        = {Dynamic deformable transformer for end-to-end face alignment},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Copy-paste with self-adaptation: A self-adaptive adjustment
method based on copy-paste augmentation. <em>IETCV</em>, <em>17</em>(8),
936–947. (<a href="https://doi.org/10.1049/cvi2.12207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation diversifies the information in the dataset. For class imbalance, the copy-paste augmentation generates new class information to alleviate the impact of this problem. However, these methods rely excessively on human intuition. Over-fitting or under-fitting can occur while adding the class information, which is inappropriate. The authors propose a self-adaptive data augmentation: the copy-paste with self-adaptation (CPA) algorithm, which improves the phenomenon of over-fitting and under-fitting. For the CPA, the evaluation results of a model are taken as an important adjustment basis. The evaluation results are combined with the information of class imbalance to generate a set of class weights. Different number of class information will be replenished according to class weights. Finally, the generated images will be inserted into the training dataset and the model will start formal training. The experimental results show that CPA can alleviate class imbalance. For TT100 K dataset, YOLOv3 is trained with the optimised dataset and its AP is increased by 2% for VOC2007 dataset, the mAP of RetinaNet on optimised dataset is 78.46, which is 1.2% higher than original dataset. For COCO2017 dataset, SSD300 is trained with the optimised dataset and its AP is increased by 1.3%.},
  archive      = {J_IETCV},
  author       = {Xiaoyu Yu and Fuchao Li and Pengfei Bai and Yan Liu and Yinglu Chen},
  doi          = {10.1049/cvi2.12207},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {936-947},
  shortjournal = {IET Comput. Vis.},
  title        = {Copy-paste with self-adaptation: A self-adaptive adjustment method based on copy-paste augmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SA-FlowNet: Event-based self-attention optical flow
estimation with spiking-analogue neural networks. <em>IETCV</em>,
<em>17</em>(8), 925–935. (<a
href="https://doi.org/10.1049/cvi2.12206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by biological vision mechanism, event-based cameras have been developed to capture continuous object motion and detect brightness changes independently and asynchronously, which overcome the limitations of traditional frame-based cameras. Complementarily, spiking neural networks (SNNs) offer asynchronous computations and exploit the inherent sparseness of spatio-temporal events. Notably, event-based pixel-wise optical flow estimations calculate the positions and relationships of objects in adjacent frames; however, as event camera outputs are sparse and uneven, dense scene information is difficult to generate and the local receptive fields of the neural network also lead to poor moving objects tracking. To address these issues, an improved event-based self-attention optical flow estimation network (SA-FlowNet) that independently uses criss-cross and temporal self-attention mechanisms, directly capturing long-range dependencies and efficiently extracting the temporal and spatial features from the event streams is proposed. In the former mechanism, a cross-domain attention scheme dynamically fusing the temporal-spatial features is introduced. The proposed network adopts a spiking-analogue neural network architecture using an end-to-end learning method and gains significant computational energy benefits especially for SNNs. The state-of-the-art results of the error rate for optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset compared with the current SNN-based approaches is demonstrated.},
  archive      = {J_IETCV},
  author       = {Fan Yang and Li Su and Jinxiu Zhao and Xuena Chen and Xiangyu Wang and Na Jiang and Quan Hu},
  doi          = {10.1049/cvi2.12206},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {925-935},
  shortjournal = {IET Comput. Vis.},
  title        = {SA-FlowNet: Event-based self-attention optical flow estimation with spiking-analogue neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel parameter decoupling approach of personalised
federated learning for image analysis. <em>IETCV</em>, <em>17</em>(8),
913–924. (<a href="https://doi.org/10.1049/cvi2.12204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the importance of privacy protection in databases and other institutions, federated learning (FL) is used to benefit training machine learning models based on these decentralised and private data so as to address the growing vision tasks. However, for federated learning, statistical heterogeneity continues to be a major problem. Recently, plenty of personalised federated learning methods have been explored to solve the problem of statistical heterogeneity. The usage of trained base layers and the effect of feature extraction in personalised layers, however, are hardly considered in those methods that employ the learning personalised models approach. To address the problem of the statistical heterogeneity in image analysis, PCCFED, a personalised federated learning method utilizing the strategy of parameter decoupling is proposed. It should be emphasised that the authors’ personalised federated learning method decouples the personalised (P) layers into a connecting (C) layer and classifier (C) layer in order to enhance the effectiveness of feature learning for personalised layers. Further, an approach is proposed to fully use the base layers to adapt a personalised model based on the newly admitted institution&#39;s dataset through meta-transfer. The performance of the proposed PCCFED on three datasets is evaluated under the practical non-independent and identically distributed (non-IID) setting. Extensive experiments demonstrate that compared with baseline methods, the proposed framework achieves the best performance in federated learning and fine-tuning. Through FL, the investigation reveals a method to reduce statistical heterogeneity while protecting the institutions&#39; privacy.},
  archive      = {J_IETCV},
  author       = {Ruizheng Su and Xiongwen Pang and Hui Wang},
  doi          = {10.1049/cvi2.12204},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {913-924},
  shortjournal = {IET Comput. Vis.},
  title        = {A novel parameter decoupling approach of personalised federated learning for image analysis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Triple critical feature capture network: A triple critical
feature capture network for weakly supervised object detection.
<em>IETCV</em>, <em>17</em>(8), 895–912. (<a
href="https://doi.org/10.1049/cvi2.12203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object detection (WSOD) is becoming increasingly important for computer vision tasks, as it alleviates the burden of manual annotation. Most WSOD techniques rely on multiple instance learning (MIL), which tends to localise the discriminative parts of salient objects instead of the whole object. In addition, network training is often supervised using simple image-level annotations, without including object quantities or location information. However, this can lead to ambiguous differentiation of object instances, both in terms of location and semantics. To address these issues, propose an end-to-end triple critical feature capture network (TCFCNet) for WSOD is proposed. Specifically, a multi-task branch, which can perform fully supervised classification and regression task, was integrated with a PCL in an end-to-end network for refining object locations in an online method. A cyclic parametric dropblock module (CPDM) was then designed to help the detector focus on the contextual information by using cyclic masking techniques to maximise the removal of the discriminative components of an object instance to alleviate the part domination problem. Finally, a feature decoupling module (FDM) is proposed to further reduce the ambiguous distinction of object instances by adaptively constructing robust critical features that adapt to multi-task branch for classification and regression tasks, which contains a feature enhancement module and task-specific polarisation functions. Comprehensive experiments are carried out on the challenging Pascal VOC 2007 and VOC 2012 datasets. The proposed method achieves a 54.6% mAP and a 44.3% mAP on the Pascal VOC 2007 and VOC 2012 datasets respectively, showed that our method outperformed existing mainstream techniques by a considerable margin.},
  archive      = {J_IETCV},
  author       = {Zhoufeng Liu and Kaihua Wang and Chunlei Li and Shunmin Ding and Jiangtao Xi},
  doi          = {10.1049/cvi2.12203},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {895-912},
  shortjournal = {IET Comput. Vis.},
  title        = {Triple critical feature capture network: A triple critical feature capture network for weakly supervised object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An encoder-decoder framework with dynamic convolution for
weakly supervised instance segmentation. <em>IETCV</em>, <em>17</em>(8),
883–894. (<a href="https://doi.org/10.1049/cvi2.12202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the systems of industrial robotics and autonomous vehicles, instance segmentation is widely employed. However, manually labelling an object outline is time-consuming. In order to reduce annotation costs, we present a weakly supervised instance segmentation method in this article. A deeply convolutional network is first used to construct multi-scale feature maps for each object in the input image. After that, the encoder-decoder framework with dynamic convolution is utilised to enhance model capacity and efficiency, while avoiding the issues of anchor design, proposal selection, and RoIAlign implementation. In particular, Dynamic Heads are used in the encoder to create dynamic convolution kernels, while Instance Heads are used in the decoder to provide the global feature map. With dynamic convolution, each instance can be segmented independently, reducing interference with other instances and improving segmentation accuracy. Under the supervision of projection loss and pixel point colour pairing loss, the contours of each object are finally outlined. On the PASCAL VOC and MS COCO datasets, the proposed method is competitive with more sophisticated approaches. In the VOC dataset, segmentation performance achieved 37.6% average precision with ResNet-101 and FPN networks. The extensively visualised results demonstrate the effectiveness of the proposed encoder-decoder framework with dynamic convolution.},
  archive      = {J_IETCV},
  author       = {Liangjun Zhu and Li Peng and Shuchen Ding and Zhongren Liu},
  doi          = {10.1049/cvi2.12202},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {883-894},
  shortjournal = {IET Comput. Vis.},
  title        = {An encoder-decoder framework with dynamic convolution for weakly supervised instance segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SiamCCF: Siamese visual tracking via cross-layer calibration
fusion. <em>IETCV</em>, <em>17</em>(8), 869–882. (<a
href="https://doi.org/10.1049/cvi2.12201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese networks have attracted wide attention in visual tracking due to their competitive accuracy and speed. However, the existing Siamese trackers usually leverage a fixed linear aggregation of feature maps, which does not effectively fuse the different layers of features with attention. Besides, most of Siamese trackers calculate the similarity between the template and the search region through a cross-correlation operation between the features of the last blocks from the two branches, which might introduce the redundant noise information. In order to solve these problems, this study proposes a novel Siamese visual tracking method via cross-layer calibration fusion, termed SiamCCF. An attention-based feature fusion module is employed using local attention and non-local attention to fuse the features from the deep and shallow layers, so as to capture both local details and high-level semantic information. Moreover, a cross-layer calibration module can use the fused features to calibrate the features of the last network blocks and build the cross-layer long-range spatial and inter-channel dependencies around each spatial location. Extensive experiments demonstrate that the proposed method has achieved competitive tracking performance compared with state-of-the-art trackers on challenging benchmarks, including OTB100, OTB2013, UAV123, UAV20L, and LaSOT.},
  archive      = {J_IETCV},
  author       = {Si Chen and Huang Huang and Shunzhi Zhu and Huarong Xu and Yifan He and Da-Han Wang},
  doi          = {10.1049/cvi2.12201},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {869-882},
  shortjournal = {IET Comput. Vis.},
  title        = {SiamCCF: Siamese visual tracking via cross-layer calibration fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D layout estimation of general rooms based on ordinal
semantic segmentation. <em>IETCV</em>, <em>17</em>(8), 855–868. (<a
href="https://doi.org/10.1049/cvi2.12149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Room layout estimation aims to predict the location and range of layout planes of interior spaces. Previous works treat each layout plane as an independent individual without considering the ordinal relation between walls, resulting the loss of the wall planes and the lack of integrity. This paper proposes a novel two-branch neural networks model to estimate 3D layouts of cuboid and non-cuboid room types. The model embeds the ordinal relation between layout planes into the layout segmentation branch through an proposed ordinal classification loss function, and outputs both pixel-level layout segmentation maps and layout plane parameter maps. Then, the instance-level plane parameters of each layout plane are determined by using an instance-aware pooling layer. Finally, the sharpness of layout edges of the 2D layout semantic segmentation map is optimized by using an improved depth map intersection algorithm. Furthermore, we annotate a large-scale 3D room layout estimation dataset, InteriorNet-Layout, to obtain a steady model. Experiments on synthesized real-world datasets show that the proposed method achieves faster calculation while maintaining high accuracy. Code is available at https://github.com/Hui-Yao/3D-ordinal-layout-estimation .},
  archive      = {J_IETCV},
  author       = {Hui Yao and Jun Miao and Guoxiang Zhang and Jun Chu},
  doi          = {10.1049/cvi2.12149},
  journal      = {IET Computer Vision},
  month        = {12},
  number       = {8},
  pages        = {855-868},
  shortjournal = {IET Comput. Vis.},
  title        = {3D layout estimation of general rooms based on ordinal semantic segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Position-aware spatio-temporal graph convolutional networks
for skeleton-based action recognition. <em>IETCV</em>, <em>17</em>(7),
844–854. (<a href="https://doi.org/10.1049/cvi2.12223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have been widely used in skeleton-based action recognition. Though significant performance has been achieved, it is still challenging to effectively model the complex dynamics of skeleton sequences. A novel position-aware spatio-temporal GCN for skeleton-based action recognition is proposed, where the positional encoding is investigated to enhance the capacity of typical baselines for comprehending the dynamic characteristics of action sequence. Specifically, the authors’ method systematically investigates the temporal position encoding and spatial position embedding, in favour of explicitly capturing the sequence ordering information and the identity information of nodes that are used in graphs. Additionally, to alleviate the redundancy and over-smoothing problems of typical GCNs, the authors’ method further investigates a subgraph mask, which gears to mine the prominent subgraph patterns over the underlying graph, letting the model be robust against the impaction of some irrelevant joints. Extensive experiments on three large-scale datasets demonstrate that our model can achieve competitive results comparing to the previous state-of-art methods.},
  archive      = {J_IETCV},
  author       = {Ping Yang and Qin Wang and Hao Chen and Zizhao Wu},
  doi          = {10.1049/cvi2.12223},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {844-854},
  shortjournal = {IET Comput. Vis.},
  title        = {Position-aware spatio-temporal graph convolutional networks for skeleton-based action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Texture and semantic convolutional auto-encoder for anomaly
detection and segmentation. <em>IETCV</em>, <em>17</em>(7), 829–843. (<a
href="https://doi.org/10.1049/cvi2.12200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a challenging task, especially detecting and segmenting tiny defect regions in images without anomaly priors. Although deep encoder-decoder-based convolutional neural networks have achieved good anomaly detection results, existing methods operate uniformly on all extracted image features without considering disentangling these features. To fully explore the texture and semantic information of images, A novel unsupervised anomaly detection method is proposed. Specifically, discriminative features are extracted from images by using a deep pre-trained network, where shallow and deep features are aggregated into texture and semantic modules, respectively. Then, a feature fusion module is developed to interactively enable feature information in two different modules. The texture and semantic segmentation results are obtained by comparing the texture features and semantic features before and after reconstruction, respectively. Finally, an anomaly segmentation module is designed to generate anomaly detection results by integrating the results of the texture and semantic modules by setting a threshold. Experimental results on benchmark datasets for anomaly detection demonstrate that our proposed method can efficiently and effectively detect anomalies, outperforming some state-of-the-art methods by 2.7% and 0.6% in classification and segmentation.},
  archive      = {J_IETCV},
  author       = {Jintao Luo and Can Gao and Da Wan and Linlin Shen},
  doi          = {10.1049/cvi2.12200},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {829-843},
  shortjournal = {IET Comput. Vis.},
  title        = {Texture and semantic convolutional auto-encoder for anomaly detection and segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain adaptive crowd counting via dynamic scale aggregation
network. <em>IETCV</em>, <em>17</em>(7), 814–828. (<a
href="https://doi.org/10.1049/cvi2.12198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is an important research topic in computer vision. Its goal is to estimate the people&#39;s number in an image. Researchers have dramatically improved counting accuracy in recent years by regressing density maps. However, because of the inherent domain shift, the model trained on an expensive manually labelled dataset (source domain) does not perform well on a dataset with scarce labels (target domain). For this issue, a novel dynamic scale aggregation network (DSANet) is proposed to reduce the gaps in style and cross-domain head scale variations. Specifically, a practical style transfer layer is introduced to reduce the appearance discrepancy between the source and target domains. Then, the translated source and target domain samples are encoded by a generator consisting of the VGG16 network and the dynamic scale aggregation modules (DSA Modules) and produce corresponding density maps. The DSA module can adaptively adjust parameters according to the input features and effectively fuse multi-scale information to overcome the cross-domain head scale variations. Next, a discriminator judges the input density map from the source or target domain. Last, domain distributions are aligned through adversarial between the generator and the discriminator. The experiments show that our network outperforms the current state-of-the-art methods and can improve the target domain&#39;s performance while maintaining the source domain&#39;s performance without significant degradation.},
  archive      = {J_IETCV},
  author       = {Zhanqiang Huo and Yanan Wang and Yingxu Qiao and Jing Wang and Fen Luo},
  doi          = {10.1049/cvi2.12198},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {814-828},
  shortjournal = {IET Comput. Vis.},
  title        = {Domain adaptive crowd counting via dynamic scale aggregation network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised detection of contrast enhanced highlight
landmarks. <em>IETCV</em>, <em>17</em>(7), 804–813. (<a
href="https://doi.org/10.1049/cvi2.12197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of landmark detection based on deep learning, most of the research utilise convolutional neural networks to represent landmarks, and rarely adopt Transformer to represent and encode landmarks. Meanwhile, many works focus on modifying the network structure to improve network performance, and there is little research on the distribution of landmarks. In this article,the authors propose an unsupervised model to extract landmarks of objects in images. First, Transformer structure is combined with the convolutional neural network structure to represent and encode the landmarks; next, positive and negative sample pairs between landmarks are constructed, so that the semantically consistent landmarks on the image are pulled closer in the feature space and the semantically inconsistent landmarks are pushed farther in the feature space; then the authors concentrate their attention on the most active points to distinguish the landmarks of an object from the background; finally, based on the new contrastive loss, the network reconstructs the image by the landmarks of the object that are continuously learnt during training. Experiments show that the proposed model achieves better performance than other unsupervised methods on the CelebA, Annotated Facial Landmarks in the Wild, 300W datasets.},
  archive      = {J_IETCV},
  author       = {Tao Wu and Wenzhuo Fan and Shuxian Li and Qingqing Li and Jianlin Zhang and Meihui Li},
  doi          = {10.1049/cvi2.12197},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {804-813},
  shortjournal = {IET Comput. Vis.},
  title        = {Unsupervised detection of contrast enhanced highlight landmarks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A latent topic-aware network for dense video captioning.
<em>IETCV</em>, <em>17</em>(7), 795–803. (<a
href="https://doi.org/10.1049/cvi2.12195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple events in a long untrimmed video possess the characteristics of similarity and continuity. These characteristics can be considered as a kind of topic semantic information, which probably behaves as same sports, similar scenes, same objects etc. Inspired by this, a novel latent topic-aware network (LTNet) is proposed in this article. The LTNet explores potential themes within videos and generates more continuous captions. Firstly, a global visual topic finder is employed to detect the similarity among events and obtain latent topic-level features. Secondly, a latent topic-oriented relation learner is designed to further enhance the topic-level representations by capturing the relationship between each event and the video themes. Benefiting from the finder and the learner, the caption generator is capable of predicting more accurate and coherent descriptions. The effectiveness of our proposed method is demonstrated on ActivityNet Captions and YouCook2 datasets, where LTNet shows a relative performance of over 3.03% and 0.50% in CIDEr score respectively.},
  archive      = {J_IETCV},
  author       = {Tao Xu and Yuanyuan Cui and Xinyu He and Caihua Liu},
  doi          = {10.1049/cvi2.12195},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {795-803},
  shortjournal = {IET Comput. Vis.},
  title        = {A latent topic-aware network for dense video captioning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image feature learning combined with attention-based
spectral representation for spatio-temporal photovoltaic power
prediction. <em>IETCV</em>, <em>17</em>(7), 777–794. (<a
href="https://doi.org/10.1049/cvi2.12199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clean energy is a major trend. The importance of photovoltaic power generation is also growing. Photovoltaic power generation is mainly affected by the weather. It is full of uncertainties. Previous work has relied chiefly on historical photovoltaics data for time series forecasts. However, unforeseen weather conditions can sometimes skew. Consequently, a spatial-temporal-meteorological-long short-term memory prediction model (STM-LSTM) is proposed to compensate for the shortage of photovoltaic prediction models for uncertainties. This model can simultaneously process satellite image data, historical meteorological data, and historical power generation data. In this way, historical patterns and meteorological change information are extracted to improve the accuracy of photovoltaic prediction. STM-LSTM processes raw satellite data to obtain cloud image data. It can extract cloud motion information using the dense optical flow method. First, the cloud images are processed to extract cloud position information. By adaptive attentive learning of images in different bands, a better representation for subsequent tasks can be obtained. Second, it is important to process historical meteorological data to learn meteorological change patterns. Last but not least, the historical photovoltaic power generation sequences are combined to obtain the final photovoltaic prediction results. After a series of experimental validation, the performance of the proposed STM-LSTM model has a good improvement compared with the baseline model.},
  archive      = {J_IETCV},
  author       = {Xingchen Guo and Jing Lai and Zhou Zheng and Chenxiang Lin and Yuxing Dai and Xuexin Xu and Haisheng San and Rong Jia and Zhihong Zhang},
  doi          = {10.1049/cvi2.12199},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {777-794},
  shortjournal = {IET Comput. Vis.},
  title        = {Image feature learning combined with attention-based spectral representation for spatio-temporal photovoltaic power prediction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiteCCLKNet: A lightweight criss-cross large kernel
convolutional neural network for hyperspectral image classification.
<em>IETCV</em>, <em>17</em>(7), 763–776. (<a
href="https://doi.org/10.1049/cvi2.12218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance convolutional neural networks (CNNs) stack many convolutional layers to obtain powerful feature extraction capability, which leads to huge storing and computational costs. The authors focus on lightweight models for hyperspectral image (HSI) classification, so a novel lightweight criss-cross large kernel convolutional neural network (LiteCCLKNet) is proposed. Specifically, a lightweight module containing two 1D convolutions with self-attention mechanisms in orthogonal directions is presented. By setting large kernels within the 1D convolutional layers, the proposed module can efficiently aggregate long-range contextual features. In addition, the authors effectively obtain a global receptive field by stacking only two of the proposed modules. Compared with traditional lightweight CNNs, LiteCCLKNet reduces the number of parameters for easy deployment to resource-limited platforms. Experimental results on three HSI datasets demonstrate that the proposed LiteCCLKNet outperforms the previous lightweight CNNs and has higher storage efficiency.},
  archive      = {J_IETCV},
  author       = {Chengcheng Zhong and Na Gong and Zitong Zhang and Yanan Jiang and Kai Zhang},
  doi          = {10.1049/cvi2.12218},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {763-776},
  shortjournal = {IET Comput. Vis.},
  title        = {LiteCCLKNet: A lightweight criss-cross large kernel convolutional neural network for hyperspectral image classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascading AB-YOLOv5 and PB-YOLOv5 for rib fracture detection
in frontal and oblique chest x-ray images. <em>IETCV</em>,
<em>17</em>(7), 750–762. (<a
href="https://doi.org/10.1049/cvi2.12239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional deep learning models have shown comparable performance to radiologists in detecting and classifying thoracic diseases. However, research on rib fractures remains limited compared to other thoracic abnormalities. Moreover, existing deep learning models primarily focus on using frontal chest X-ray (CXR) images. To address these gaps, the authors utilised the EDARib-CXR dataset, comprising 369 frontal and 829 oblique CXRs. These X-rays were annotated by experienced radiologists, specifically identifying the presence of rib fractures using bounding-box-level annotations. The authors introduce two detection models, AB-YOLOv5 and PB-YOLOv5, and train and evaluate them on the EDARib-CXR dataset. AB-YOLOv5 is a modified YOLOv5 network that incorporates an auxiliary branch to enhance the resolution of feature maps in the final convolutional network layer. On the other hand, PB-YOLOv5 maintains the same structure as the original YOLOv5 but employs image patches during training to preserve features of small objects in downsampled images. Furthermore, the authors propose a novel two-level cascaded architecture that integrates both AB-YOLOv5 and PB-YOLOv5 detection models. This structure demonstrates improved metrics on the test set, achieving an AP30 score of 0.785. Consequently, the study successfully develops deep learning-based detectors capable of identifying and localising fractured ribs in both frontal and oblique CXR images.},
  archive      = {J_IETCV},
  author       = {Hsin-Chun Tsai and Nan-Han Lu and Kuo-Ying Liu and Chuan-Han Lin and Jhing-Fa Wang},
  doi          = {10.1049/cvi2.12239},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {750-762},
  shortjournal = {IET Comput. Vis.},
  title        = {Cascading AB-YOLOv5 and PB-YOLOv5 for rib fracture detection in frontal and oblique chest X-ray images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-invariant attention network for transfer learning
between cross-scene hyperspectral images. <em>IETCV</em>,
<em>17</em>(7), 739–749. (<a
href="https://doi.org/10.1049/cvi2.12238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-sample-size problem is always a challenge for hyperspectral image (HSI) classification. Considering the co-occurrence of land-cover classes between similar scenes, transfer learning can be performed, and cross-scene classification is deemed a feasible approach proposed in recent years. In cross-scene classification, the source scene which possesses sufficient labelled samples is used for assisting the classification of the target scene that has a few labelled samples. In most situations, different HSI scenes are imaged by different sensors resulting in their various input feature dimensions (i.e. number of bands), hence heterogeneous transfer learning is desired. An end-to-end heterogeneous transfer learning algorithm namely domain-invariant attention network (DIAN) is proposed to solve the cross-scene classification problem. The DIAN mainly contains two modules. (1) A feature-alignment CNN (FACNN) is applied to extract features from source and target scenes, respectively, aiming at projecting the heterogeneous features from two scenes into a shared low-dimensional subspace. (2) A domain-invariant attention block is developed to gain cross-domain consistency with a specially designed class-specific domain-invariance loss, thus further eliminating the domain shift. The experiments on two different cross-scene HSI datasets show that the proposed DIAN achieves satisfying classification results.},
  archive      = {J_IETCV},
  author       = {Minchao Ye and Chenglong Wang and Zhihao Meng and Fengchao Xiong and Yuntao Qian},
  doi          = {10.1049/cvi2.12238},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {739-749},
  shortjournal = {IET Comput. Vis.},
  title        = {Domain-invariant attention network for transfer learning between cross-scene hyperspectral images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving multispectral pedestrian detection with
scale-aware permutation attention and adjacent feature aggregation.
<em>IETCV</em>, <em>17</em>(7), 726–738. (<a
href="https://doi.org/10.1049/cvi2.12159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High quality feature fusion module is one of the key components for multispectral pedestrian detection system in challenging situations, such as large-scale variance and occlusion. Although attention mechanism is one of the most effective ways for feature refining, the correlation between attention and scales in feature pyramid still remains unknown. Therefore, a scale-aware permutated attention module is proposed to enhance features of objects with different scales adaptively in the feature pyramid. Specifically, four different local and global attention sub-modules are investigated to refine feature maps with different permutations in the Feature Pyramid Networks, improving the quality of the feature fusion. Besides, to address the high miss-rate issue for small-sized pedestrians, an adjacent-branch feature aggregation module is proposed to aggregate features across different scales, taking both semantic context and spatial resolution into consideration. Both modules can benefit from each other with significant performance improvement in terms of efficiency and accuracy, when equipped with the dual-branch CenterNet detection framework. Experiments on the KAIST and FLIR datasets demonstrate its superior performance compared with other state-of-the-arts.},
  archive      = {J_IETCV},
  author       = {Xin Zuo and Zhi Wang and Jifeng Shen and Wankou Yang},
  doi          = {10.1049/cvi2.12159},
  journal      = {IET Computer Vision},
  month        = {10},
  number       = {7},
  pages        = {726-738},
  shortjournal = {IET Comput. Vis.},
  title        = {Improving multispectral pedestrian detection with scale-aware permutation attention and adjacent feature aggregation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Erratum: Integration graph attention network and
multi-centre constrained loss for cross-modality person
re-identification. <em>IETCV</em>, <em>17</em>(6), 722. (<a
href="https://doi.org/10.1049/cvi2.12210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETCV},
  doi          = {10.1049/cvi2.12210},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {722},
  shortjournal = {IET Comput. Vis.},
  title        = {Erratum: Integration graph attention network and multi-centre constrained loss for cross-modality person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recurrent unit augmented memory network for video
summarisation. <em>IETCV</em>, <em>17</em>(6), 710–721. (<a
href="https://doi.org/10.1049/cvi2.12194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarisation can relieve the pressure on video storage, transmission, archiving, and retrieval caused by the explosive growth of online videos in recent years. Most existing supervised video summarisation methods use convolutional neural network (CNN) or recurrent neural network (RNN) to model the temporal dependencies between video frames or video shots. CNN mainly focuses on local information, and RNN loses long-term information when the input sequence is long, both of which have limited ability to obtain long-range memory in the video. Therefore, a recurrent unit augmented memory network (RUAMN) for video summarisation is proposed, which effectively utilises the long-term memory extraction ability of the end-to-end memory network (MemN2N) and solves the problem that MemN2N is insensitive to temporal sequence information. At the same time, the proposed RUAMN enhances the process of memory update between multiple computational steps (hops), and finally generates a meaningful video summarisation result. Specifically, the proposed RUAMN is mainly composed of the input module, the global-and-local sampling, the memory module and the output module. The input module uses a bidirectional GRU to obtain the forward and backward information of each video frame. Then the global-and-local sampling performs global sampling and local sampling on the output sequence of the input module respectively to obtain several shorter sequences, so that the memory modules can capture the fine-grained relationship features between video frames more effectively. The memory module extracts the long-term memory information in the feature sequence, and finally the frame-level importance scores are predicted by the output module. Extensive experiments on benchmark datasets, that is, TVSum and SumMe, demonstrate the superiority of our method over several state-of-the-art supervised video summarisation methods.},
  archive      = {J_IETCV},
  author       = {Min Su and Ran Ma and Bing Zhang and Kai Li},
  doi          = {10.1049/cvi2.12194},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {710-721},
  shortjournal = {IET Comput. Vis.},
  title        = {Recurrent unit augmented memory network for video summarisation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-precision skeleton-based human repetitive action
counting. <em>IETCV</em>, <em>17</em>(6), 700–709. (<a
href="https://doi.org/10.1049/cvi2.12193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel counting model is presented by the authors to estimate the number of repetitive actions in temporal 3D skeleton data. As per the authors’ knowledge, this is the first work of this kind using skeleton data for high-precision repetitive action counting. Different from existing works on RGB video data, the authors’ model follows a bottom-up pipeline to clip the sub-action first followed by robust aggregation in inference. First, novel counting loss functions and robust inference with backtracking is proposed to pursue precise per-frame count as well as overall count with boundary frames. Second, an efficient synthetic approach is proposed to augment skeleton data in training and thus avoid time-consuming repetitive action data collection work. Finally, a challenging human repetitive action counting dataset named VSRep is collected with various types of action to evaluate the proposed model. Experiments demonstrate that the proposed counting model outperforms existing video-based methods by a large margin in terms of accuracy in real-time inference.},
  archive      = {J_IETCV},
  author       = {Chengxian Li and Ming Shao and Qirui Yang and Siyu Xia},
  doi          = {10.1049/cvi2.12193},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {700-709},
  shortjournal = {IET Comput. Vis.},
  title        = {High-precision skeleton-based human repetitive action counting},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-modal graph attention interaction network for person
re-identification. <em>IETCV</em>, <em>17</em>(6), 687–699. (<a
href="https://doi.org/10.1049/cvi2.12192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Re-identification (Re-ID) is a task of matching target pedestrians under cross-camera surveillance. Learning discriminative feature representations is the main issue for person Re-ID. A few recent methods introduce text descriptions as auxiliary information to enhance feature representations, as it offers richer semantic information and perspective consistency. However, these works usually process text and images separately, which leads to the absence of cross-modal interactions. In this article, a Dual-modal Graph Attention Interaction Network (Dual-GAIN) is proposed to integrate visual features and textual features into a heterogeneous graph to model the relationship between them, simultaneously. The proposed Dual-GAIN mainly consists of two components: a dual-stream feature extractor and a Graph Attention Interaction Network (GAIN). Specifically, the two-stream feature extractor is utilised to extract visual features and textual features respectively. Then, visual local features and textual features are treated as nodes to construct a multi-modal graph. Cosine similarity constrained attention weights are introduced in GAIN, which is designed for cross-modal interaction and feature fusion on this heterogeneous multi-modal graph. Experiments on public large-scale datasets, that is, Market-1501, CUHK03 labelled, and CUHK03 detected, demonstrate our method achieves the state-of-the-art performance.},
  archive      = {J_IETCV},
  author       = {Wen Wang and Gaoyun An and Qiuqi Ruan},
  doi          = {10.1049/cvi2.12192},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {687-699},
  shortjournal = {IET Comput. Vis.},
  title        = {A dual-modal graph attention interaction network for person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online multiple object tracking with enhanced
re-identification. <em>IETCV</em>, <em>17</em>(6), 676–686. (<a
href="https://doi.org/10.1049/cvi2.12191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In existing online multiple object tracking algorithms, schemes that combine object detection and re-identification (ReID) tasks in a single model for simultaneous learning have drawn great attention due to their balanced speed and accuracy. However, different tasks require to focus different features. Learning two different tasks in the same model extracted features can lead to competition between the two tasks, making it difficult to achieve optimal performance. To reduce this competition, a task-related attention network, which uses a self-attention mechanism to allow each branch to learn on feature maps related to its task is proposed. Besides, a smooth gradient-boosting loss function, which improves the quality of the extracted ReID features by gradually shifting the focus to the hard negative samples of each object during training is introduced. Extensive experiments on MOT16, MOT17, and MOT20 datasets demonstrate the effectiveness of the proposed method, which is also competitive in current mainstream algorithm.},
  archive      = {J_IETCV},
  author       = {Wenyu Yang and Yong Jiang and Shuai Wen and Yong Fan},
  doi          = {10.1049/cvi2.12191},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {676-686},
  shortjournal = {IET Comput. Vis.},
  title        = {Online multiple object tracking with enhanced re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online deep bingham network for probabilistic orientation
estimation. <em>IETCV</em>, <em>17</em>(6), 663–675. (<a
href="https://doi.org/10.1049/cvi2.12188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orientation estimation is one of the core problems in several computer vision tasks. Recently deep learning techniques combined with the Bingham distribution have attracted considerable interest towards this problem when considering ambiguities and rotational symmetries of objects. However, existing works suffer from two issues. First, the computational overhead for calculating the normalisation constant of the Bingham distribution is relatively high. Second, the choice of loss functions is uncertain. In light of these problems, we present an online deep Bingham network to estimate the orientation of objects. We sharply reduce the computational overhead of the normalisation constant by directly applying a numerical integration formula. Additionally, we are the first to give theorems on the convexity and Lipschitz continuity of the Bingham distribution&#39;s negative log-likelihood, which formally indicates that it is a proper choice of the loss function. We test our method on three public datasets, namely the UPNA, the T-LESS and Pascal3D+, showing that our method outperforms the state-of-the-art in terms of orientation accuracy and time efficiency, which can reduce the runtime by more than 6 h compared to the offline methods. The ablation experiments further demonstrate the effectiveness and robustness of our model.},
  archive      = {J_IETCV},
  author       = {Wenjie Li and Jia Liu and Wei Hao and Haisong Liu and Dayong Ren and Yanyan Wang and Lijun Chen},
  doi          = {10.1049/cvi2.12188},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {663-675},
  shortjournal = {IET Comput. Vis.},
  title        = {Online deep bingham network for probabilistic orientation estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refinement co-supervision network for real-time semantic
segmentation. <em>IETCV</em>, <em>17</em>(6), 652–662. (<a
href="https://doi.org/10.1049/cvi2.12187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a fundamental technology for autonomous driving. It has a high demand for inference speed and accuracy. However, a good trade-off between accuracy and latency is yet not present in existing semantic segmentation approaches. Due to the limitation of speed, the authors cannot increase the number of network layers without limit and cannot design modules like in the networks without real-time. It is a challenging problem how to design a model with good performance under limited resources. To alleviate these issues, in this study, the authors propose a refinement co-supervision network (RCNet), which is real-time on a high-resolution image (1024×2048). The authors first construct the context refinement module, which can provide low computation cost way for obtaining the large receptive field and context information. Furthermore, a boundary co-supervision mechanism is proposed. It strengthens the optimisation of easily neglected boundaries and small targets. Experimental results reveal that the proposed RCNet outperforms seven representative semantic segmentation methods.},
  archive      = {J_IETCV},
  author       = {Yongsheng Dong and Kaiyuan Zhao and Lintao Zheng and Haotian Yang and Qing Liu and Yuanhua Pei},
  doi          = {10.1049/cvi2.12187},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {652-662},
  shortjournal = {IET Comput. Vis.},
  title        = {Refinement co-supervision network for real-time semantic segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DSGEM: Dual scene graph enhancement module-based visual
question answering. <em>IETCV</em>, <em>17</em>(6), 638–651. (<a
href="https://doi.org/10.1049/cvi2.12186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) aims to appropriately answer a text question by understanding the image content. Attention-based VQA models mine the implicit relationships between objects according to the feature similarity, which neglects the explicit relationships between objects, for example, the relative position. Most Visual Scene Graph-based VQA models exploit the relative positions or visual relationships between objects to construct the visual scene graph, while they suffer from the semantic insufficiency of visual edge relations. Besides, the scene graph of text modality is often ignored in these works. In this article, a novel Dual Scene Graph Enhancement Module (DSGEM) is proposed that exploits the relevant external knowledge to simultaneously construct two interpretable scene graph structures of image and text modalities, which makes the reasoning process more logical and precise. Specifically, the authors respectively build the visual and textual scene graphs with the help of commonsense knowledge and syntactic structure, which explicitly endows the specific semantics to each edge relation. Then, two scene graph enhancement modules are proposed to propagate the involved external and structural knowledge to explicitly guide the feature interaction between objects (nodes). Finally, the authors embed such two scene graph enhancement modules to existing VQA models to introduce the explicit relation reasoning ability. Experimental results on both VQA V2 and OK-VQA datasets show that the proposed DSGEM is effective and compatible to various VQA architectures.},
  archive      = {J_IETCV},
  author       = {Boyue Wang and Yujian Ma and Xiaoyan Li and Heng Liu and Yongli Hu and Baocai Yin},
  doi          = {10.1049/cvi2.12186},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {638-651},
  shortjournal = {IET Comput. Vis.},
  title        = {DSGEM: Dual scene graph enhancement module-based visual question answering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple object tracking based on quadratic graph matching.
<em>IETCV</em>, <em>17</em>(6), 626–637. (<a
href="https://doi.org/10.1049/cvi2.12185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the development of deep-learning, the performance of multiple object tracking (MOT) algorithm based on deep neural networks has been greatly improved. However, it is still a difficult problem to successfully solve the tracking misalignment caused by occlusion and complex tracking scenes. Most of the work focusses on designing a sophisticated network, only little work focusses on data association. Actually, data association is very helpful in MOT. In this study, data association in tracking is taken as the main research task, and the authors introduce quadratic graph matching into MOT. Considering the objects in each frame as a graph, we can model the data association between two frames as a quadratic graph matching problem. And then it is transformed into a convex quadratic optimisation problem. Introducing high-order structure features into the matching function effectively solves some tracking problems. The data association is designed into the tracking model as an overall solution. Experiments show that the proposed data association algorithm performs favourably against several state-of-the-art data association methods and can be used in any tracking-by-detection method. Our code is publicly available from https://github.com/gjy0514/G_Model .},
  archive      = {J_IETCV},
  author       = {Jiayan Gao and Qi Zou and Hongwei Zhao},
  doi          = {10.1049/cvi2.12185},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {626-637},
  shortjournal = {IET Comput. Vis.},
  title        = {Multiple object tracking based on quadratic graph matching},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MFF: An effective method of solving the ill regions in
stereo matching. <em>IETCV</em>, <em>17</em>(6), 615–625. (<a
href="https://doi.org/10.1049/cvi2.12190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current stereo matching field, the accuracy of the derived disparity map is highly dependent on the processing capability of ill regions. Fortunately, we find that the use of local information will eliminate the negative effects associated with ill regions. As a result of the above discovery, we propose the Concatenated Dilated Convolution (CDC) block and the Multi-scale Feature Fusion module (MFF), which are capable of effectively extracting regional context by increasing the receptive field in parallel and channel-wise ways. The CDC block can expand the receptive field by applying multiple dilated convolutions at different dilation rates in parallel to enhance the smoothness of the feature map. By constructing parallel CDC blocks in a multiple dilated manner, the MFF module can improve the smoothness of the feature map. In addition, to control the number of parameters in the MFF network, a high-performance channel-distribution algorithm is proposed, capable of adjusting the weights of each module and convolution in an adaptive manner while reducing the number of parameters. Extensive experiments have demonstrated that MFF and CDC can effectively improve the performance of ill areas and networks with a minimal number of parameters.},
  archive      = {J_IETCV},
  author       = {Ren Qian and Renyan Feng and Wangduo Xie and Wenbang Yang and Yong Zhao},
  doi          = {10.1049/cvi2.12190},
  journal      = {IET Computer Vision},
  month        = {9},
  number       = {6},
  pages        = {615-625},
  shortjournal = {IET Comput. Vis.},
  title        = {MFF: An effective method of solving the ill regions in stereo matching},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The following article for this special issue was published
in a different issue. <em>IETCV</em>, <em>17</em>(5), 614. (<a
href="https://doi.org/10.1049/cvi2.12211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETCV},
  doi          = {10.1049/cvi2.12211},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {614},
  shortjournal = {IET Comput. Vis.},
  title        = {The following article for this special issue was published in a different issue},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot temporal event localisation: Label-free,
training-free, domain-free. <em>IETCV</em>, <em>17</em>(5), 599–613. (<a
href="https://doi.org/10.1049/cvi2.12224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal event localisation (TEL) has recently attracted increasing attention due to the rapid development of video platforms. Existing methods are based on either fully/weakly supervised or unsupervised learning, and thus they rely on expensive data annotation and time-consuming training. Moreover, these models, which are trained on specific domain data, limit the model generalisation to data distribution shifts. To cope with these difficulties, the authors propose a zero-shot TEL method that can operate without training data or annotations. Leveraging large-scale vision and language pre-trained models, for example, CLIP, we solve the two key problems: (1) how to find the relevant region where the event is likely to occur; (2) how to determine event duration after we find the relevant region. Query guided optimisation for local frame relevance relying on the query-to-frame relationship is proposed to find the most relevant frame region where the event is most likely to occur. Proposal generation method relying on the frame-to-frame relationship is proposed to determine the event duration. The authors also propose a greedy event sampling strategy to predict multiple durations with high reliability for the given event. The authors’ methodology is unique, offering a label-free, training-free, and domain-free approach. It enables the application of TEL purely at the testing stage. The practical results show it achieves competitive performance on the standard Charades-STA and ActivityCaptions datasets.},
  archive      = {J_IETCV},
  author       = {Li Sun and Ping Wang and Liuan Wang and Jun Sun and Takayuki Okatani},
  doi          = {10.1049/cvi2.12224},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {599-613},
  shortjournal = {IET Comput. Vis.},
  title        = {Zero-shot temporal event localisation: Label-free, training-free, domain-free},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot logo detection. <em>IETCV</em>, <em>17</em>(5),
586–598. (<a href="https://doi.org/10.1049/cvi2.12205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of deep learning has driven research into deep learning-based logo detection, which usually needs a large number of annotated data to train the model. However, due to the occasional appearance of new brands or the high cost of annotation, the number of training data is limited. Against this backdrop, the authors adapt the few-shot object detection into logo detection, and thus present a cutting-edge method called Double Classification Head (DCH) for Few-Shot Logo Detection (DCH-FSLogo), which aims at detecting the unseen logo classes using few annotated data. Unlike the traditional few-shot detection, some logo objects are similar to their backgrounds and have diverse shapes as well. For this reason, the authors adopt balanced feature pyramid and deformable Region of Interest pooling in DCH-FSLogo, this enhances the feature extraction capability and adapts to the different logo shapes. In addition, we introduce the DCH for few-shot logo detection to detect logo objects using few annotated data. Specifically, we use an extra classification head for the base classes to ease the influence from the novel classes. The experimental results on four datasets, namely: FlickrLogos-32, FoodLogoDet-1500-100, LogoDet-3K-100 and QMUL-OpenLogo-100, demonstrate that our method achieves better performance.},
  archive      = {J_IETCV},
  author       = {Sujuan Hou and Wenjie Liu and Awudu Karim and Zhixiang Jia and Weikuan Jia and Yuanjie Zheng},
  doi          = {10.1049/cvi2.12205},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {586-598},
  shortjournal = {IET Comput. Vis.},
  title        = {Few-shot logo detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Point completion by a stack-style folding network with
multi-scaled graphical features. <em>IETCV</em>, <em>17</em>(5),
576–585. (<a href="https://doi.org/10.1049/cvi2.12196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion is prevalent due to the insufficient results from current point cloud acquisition equipments, where a large number of point data failed to represent a relatively complete shape. Existing point cloud completion algorithms, mostly encoder-decoder structures with grids transform (also presented as folding operation), can hardly obtain a persuasive representation of input clouds due to the issue that their bottleneck-shape result cannot tell a precise relationship between the global and local structures. For this reason, this article proposes a novel point cloud completion model based on a Stack-Style Folding Network (SSFN). Firstly, to enhance the deep latent feature extraction, SSFN enhances the exploitation of shape feature extractor by integrating both low-level point feature and high-level graphical feature. Next, a precise presentation is obtained from a high dimensional semantic space to improve the reconstruction ability. Finally, a refining module is designed to make a more evenly distributed result. Experimental results shows that our SSFN produces the most promising results of multiple representative metrics with a smaller scale parameters than current models.},
  archive      = {J_IETCV},
  author       = {Yunbo Rao and Ping Xu and Shaoning Zeng and Jianping Gou},
  doi          = {10.1049/cvi2.12196},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {576-585},
  shortjournal = {IET Comput. Vis.},
  title        = {Point completion by a stack-style folding network with multi-scaled graphical features},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-balanced network for long-tail distribution object
detection. <em>IETCV</em>, <em>17</em>(5), 565–575. (<a
href="https://doi.org/10.1049/cvi2.12182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection on datasets with imbalanced distributions (i.e. long-tail distributions) dataset is a significantly challenging task. Some re-balancing solutions, such as re-weighting and re-sampling have two main disadvantages. First, re-balancing strategies only utilise a coarse-grained global threshold to suppress some of the most influential categories, while overlooking locally influential categories. Second, very few studies have specifically designed algorithms for object detection tasks under long-tail distribution. To address these two issues, a dual-balanced network for fine-grained re-balancing object detection is proposed. Our re-balancing strategies are both in proposal and classification logic, corresponding to two sub-networks, the Balance Region Proposal Network (BRPN) and the Balance Classification Network (BCN). The BRPN sub-network equalises the number of proposals in the background and foreground by reducing the sampling probability of simple backgrounds, and the BCN sub-network equalises the logic between head and tail categories by globally suppressing negative gradients and locally fixing the over-suppressed negative gradients. In addition, the authors advise a balance binary cross entropy loss to jointly re-balance the entire network. This design can be generalised to different two-stage object detection frameworks. The experimental mAP result of 26.40% on this LVIS-v0.5 dataset outperforms most SOTA methods.},
  archive      = {J_IETCV},
  author       = {Huiyun Gong and Yeguang Li and Jian Dong},
  doi          = {10.1049/cvi2.12182},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {565-575},
  shortjournal = {IET Comput. Vis.},
  title        = {A dual-balanced network for long-tail distribution object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global centralised and structured discriminative
non-negative matrix factorisation for hyperspectral unmixing.
<em>IETCV</em>, <em>17</em>(5), 549–564. (<a
href="https://doi.org/10.1049/cvi2.12168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances have been achieved in hyperspectral unmixing using the existing manifold Non-negative Matrix Factorisation methods, although most of these methods only exploit the preliminary structural information, that is, the nearest neighbour graph. Consequently, the performance of these methods would be degraded when considering only the geometrical structure due to the diverse distribution of the hyperspectral data, that is, the close pixels could belong to different categories or the distant points could be sampled from the same classes. In this context, the present study worked from the perspective of both global and local data relationships to develop and propose a novel approach—the Global centralised and Structured discriminative Non-negative Matrix Factorisation (GSNMF)—to achieve a further effective representation of hyperspectral unmixing. GSNMF involved maintaining the global centralised clustering and the local structured discriminative regularisation, based on which it could perfectly mine the structure information and drive a discriminative representation of the data. Experiments comparing the application of GSNMF and the state-of-the-art methods to synthetic data demonstrated the superiority of GSNMF. In addition, the consistency of the fractional abundances obtained using GSNMF with the real distributions of spectral data was evaluated on two real-world datasets.},
  archive      = {J_IETCV},
  author       = {Xue Li and Sifan Cao and Dan Huang and Ming Zhang and Yiwei Li},
  doi          = {10.1049/cvi2.12168},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {549-564},
  shortjournal = {IET Comput. Vis.},
  title        = {Global centralised and structured discriminative non-negative matrix factorisation for hyperspectral unmixing},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ACGAN: Age-compensated makeup transfer based on homologous
continuity generative adversarial network model. <em>IETCV</em>,
<em>17</em>(5), 537–548. (<a
href="https://doi.org/10.1049/cvi2.12138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The authors focus on the makeup transformation problem, which refers to the transfer of makeup from a reference face to a source face image while maintaining the source makeup-free face image. In recent years, makeup transformation has become a hot issue and a lot of research has been conducted on this basis, but there are some limitations in the existing methods, mainly due to the lack of consideration of age factor, which makes the final generated face makeup images appear not natural and lack appearance attractiveness. In order to further solve this problem, an age-compensated makeup transformation framework based on homology continuity is proposed. In order to achieve a stable and controllable age-compensation effect, the authors design a new coding module that can map the face makeup semantic vector into the higher feature space and achieve age compensation by adjusting the direction of the semantic vector. Finally, in order to comprehensively evaluate the effectiveness of the authors’ proposed method, a large number of qualitative and quantitative experiments have been conducted, and the experimental results show that the authors’ proposed framework outperforms existing methods.},
  archive      = {J_IETCV},
  author       = {Guoqiang Wu and Feng He and Yuan Zhou and Yimai Jing and Xin Ning and Chen Wang and Bo Jin},
  doi          = {10.1049/cvi2.12138},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {537-548},
  shortjournal = {IET Comput. Vis.},
  title        = {ACGAN: Age-compensated makeup transfer based on homologous continuity generative adversarial network model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Denseformer: A dense transformer framework for person
re-identification. <em>IETCV</em>, <em>17</em>(5), 527–536. (<a
href="https://doi.org/10.1049/cvi2.12118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer has shown its effectiveness and advantage in many computer vision tasks, for example, image classification and object re-identification (ReID). However, existing vision transformers are stacked layer by layer, lacking direct information exchange among every layer. Inspired by DenseNet, we propose a dense transformer framework (termed Denseformer) that connects each layer to every other layer through class tokens. We demonstrate that Denseformer can consistently achieve better performance on person ReID tasks across datasets (Market-1501, DukeMTMC, MSMT17, and Occluded-Duke), only at a negligible increase of computation. We show that Denseformer has several compelling advantages: it pays more attention to the main parts of human bodies and obtains discriminative global features.},
  archive      = {J_IETCV},
  author       = {Haoyan Ma and Xiang Li and Xia Yuan and Chunxia Zhao},
  doi          = {10.1049/cvi2.12118},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {527-536},
  shortjournal = {IET Comput. Vis.},
  title        = {Denseformer: A dense transformer framework for person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Makeup transfer: A review. <em>IETCV</em>, <em>17</em>(5),
513–526. (<a href="https://doi.org/10.1049/cvi2.12142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Makeup transfer (MT) aims to transfer the makeup style from a given reference makeup face image to a source image while preserving face identity and background information. In recent years, MT has attracted the attention of many scholars, and it has a wide range of application prospects and research value. Since then, many methods have been proposed to accomplish MT, most of which are based on Generative Adversarial Network methods. A taxonomy of existing algorithms in the field of MT is first proposed. Then, evaluation methods are proposed, existing methods are analysed, and existing datasets are introduced. This paper finally discusses the current problems in the field of MT and the trend of future research.},
  archive      = {J_IETCV},
  author       = {Feng He and Kai Bai and Yixin Zong and Yuan Zhou and Yimai Jing and Guoqiang Wu and Chen Wang},
  doi          = {10.1049/cvi2.12142},
  journal      = {IET Computer Vision},
  month        = {8},
  number       = {5},
  pages        = {513-526},
  shortjournal = {IET Comput. Vis.},
  title        = {Makeup transfer: A review},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient mixed attention module. <em>IETCV</em>,
<em>17</em>(4), 496–507. (<a
href="https://doi.org/10.1049/cvi2.12184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the application of attention mechanisms in convolutional neural networks (CNNs) has become a hot area in computer vision. Most existing methods focus on channel attention or spatial attention. Some mixed attention usually achieves better performance than channel attention or spatial attention with the help of a complex model structure, which increases the complexity of the model. This article proposes an efficient mixed attention that combines channel information with spatial information using learnable broadcast addition to reduce this complexity. In particular, this module can simplify learning and improve performance with fewer parameters. Furthermore, our method uses an excitation method based on the Tanh function to reduce computational resources while maintaining model performance, and it is a lightweight attention module that can be used in arbitrary CNNs to improve performance. Experiments on ImageNet and Cifar confirm the effectiveness of the proposed method. Besides, our method remains highly competitive for object detection tasks and image segmentation tasks.},
  archive      = {J_IETCV},
  author       = {Kuang Sheng and Pinghua Chen},
  doi          = {10.1049/cvi2.12184},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {496-507},
  shortjournal = {IET Comput. Vis.},
  title        = {An efficient mixed attention module},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selective feature fusion network for salient object
detection. <em>IETCV</em>, <em>17</em>(4), 483–495. (<a
href="https://doi.org/10.1049/cvi2.12183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully convolutional neural networks have achieved great success in salient object detection, in which the effective use of multi-layer features plays a critical role. Based on this advantage, many saliency detectors have emerged in recent years, and most of them designed a series of network structures to integrate the multi-level features generated by the backbone network. However, information in different layer play different roles in saliency object detection, how to integrate them effectively is still a great challenge. In this article, a selective feature fusion network which consists of a selective feature fusion module (SFM) and an attention-guide hierarchical feature emphasis module (AEM) is proposed. Most of the previous works mainly integrate multi-level feature by addition and concatenation, as a difference, SFM adaptively selects the important information from the input features in the fusion, which effectively avoids introducing too much redundant information. Besides, AEM combines spatial attention and channel attention to enhance features simply and effectively by hierarchical iteration, and further improve the accuracy of salient object detection. Experiments on five datasets show that the proposed selective feature fusion method achieve satisfactory results when comparing to other state-of-the-art salient object detection approaches.},
  archive      = {J_IETCV},
  author       = {Fengming Sun and Xia Yuan and Chunxia Zhao},
  doi          = {10.1049/cvi2.12183},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {483-495},
  shortjournal = {IET Comput. Vis.},
  title        = {Selective feature fusion network for salient object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loop and distillation: Attention weights fusion transformer
for fine-grained representation. <em>IETCV</em>, <em>17</em>(4),
473–482. (<a href="https://doi.org/10.1049/cvi2.12181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning subtle discriminative feature representation plays a significant role in Fine-Grained Visual Categorisation (FGVC). The vision transformer (ViT) achieves promising performance in the traditional image classification filed due to its multi-head self-attention mechanism. Unfortunately, ViT cannot effectively capture critical feature regions for FGVC due to only focusing on classification token and adopting the strategy of one-time image input. Besides, the advantage of attention weights fusion is not applied to ViT. To promote the performance of capturing vital regions for FGVC, the authors propose a novel model named RDTrans, which proposes discriminative region with top priority in a recurrent learning way. Specifically, proposed vital regions at each scale will be cropped and amplified as the next input parameters to finally locate the most discriminative region. Furthermore, a distillation learning method is employed to provide better supervision for elevating the generalisation ability. Concurrently, RDTrans can be easily trained end-to-end in a weakly-supervised learning way. Extensive experiments demonstrate that RDTrans yields state-of-the-art performance on four widely used fine-grained benchmarks, including CUB-200-2011, Stanford Cars, Stanford Dogs, and iNat2017.},
  archive      = {J_IETCV},
  author       = {Sun Fayou and Hea Choon Ngo and Zuqiang Meng and Yong Wee Sek},
  doi          = {10.1049/cvi2.12181},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {473-482},
  shortjournal = {IET Comput. Vis.},
  title        = {Loop and distillation: Attention weights fusion transformer for fine-grained representation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantics recalibration and detail enhancement network for
real-time semantic segmentation. <em>IETCV</em>, <em>17</em>(4),
461–472. (<a href="https://doi.org/10.1049/cvi2.12180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time semantic segmentation is a crucial technology in automatic driving scenarios, which needs to meet both high precision and real-time. The authors observe that learning complex correlations between object categories is vital in the real-time semantic segmentation task. Moreover, image spatial detail information plays an important role in small object segmentation and preserving edges and textures. A Semantics Recalibration and Detail Enhancement Network for real-time semantic segmentation based on BiSeNet V2 is proposed. On the one hand, a lightweight Semantics Recalibration module is designed to effectively extract global semantic contextual information, which combines pyramid segmentation and adaptive recalibration operations to learn the correlations between object categories. On the other hand, a Detail Enhancement module takes the feature maps of the shallow layers in the semantics branch as input and refines the feature maps to highlight the detail information. Finally, quantitative and qualitative analyses on Cityscapes and CamVid datasets demonstrate the effectiveness and generalisation of the proposed method.},
  archive      = {J_IETCV},
  author       = {Aizhong Mi and Mingming Gao and Zhanqiang Huo and Yingxu Qiao and Jian Chen and Haiyang Jia},
  doi          = {10.1049/cvi2.12180},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {461-472},
  shortjournal = {IET Comput. Vis.},
  title        = {Semantics recalibration and detail enhancement network for real-time semantic segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial expression recognition based on regional adaptive
correlation. <em>IETCV</em>, <em>17</em>(4), 445–460. (<a
href="https://doi.org/10.1049/cvi2.12179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the problem that the features extracted by CNN-based facial expression recognition (FER) do not consider structural information, a region adaptive correlation deep network (RACN) is proposed. The network consists of two branches. In one branch, the features obtained by applying CNN to facial sub-blocks are used as the input of the proposed second-order region correlation network (SRCN), which obtains structural features by adaptively learning the correlation of facial regions. Furthermore, they are fused with the parallel branch-extracted global features to obtain a comprehensive high-semantic feature representation. Finally, weights are assigned to the two features through the channel attention mechanism for more accurate expression classification. Experimental results show that our method can effectively extract expression features in an end-to-end manner, improve the accuracy of FER, and achieve competitive performance without relying on any a priori knowledge. And the region-adaptive correlation feature extraction branch RACN can be applied to other deep learning networks to extract discriminative structural-adaptive features. To the best of our knowledge, our work is the first to enrich the feature representation for end-to-end static FER by adaptively obtaining more discriminative regional adaptive correlation feature vectors via the autocorrelation matrix combined with CNN compared to the existing literature.},
  archive      = {J_IETCV},
  author       = {Meng Hao and Fei Yuan and Jing Li and Yuting Sun},
  doi          = {10.1049/cvi2.12179},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {445-460},
  shortjournal = {IET Comput. Vis.},
  title        = {Facial expression recognition based on regional adaptive correlation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-directional feature refinement network for real-time
semantic segmentation in urban street scenes. <em>IETCV</em>,
<em>17</em>(4), 431–444. (<a
href="https://doi.org/10.1049/cvi2.12178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient and accurate semantic segmentation is crucial for autonomous driving scene parsing. Capturing detailed information and semantic information efficiently through two-branch networks has been widely utilised in real-time semantic segmentation. This study proposes a network named MRFNet based on two-branch strategy to solve the problem of accuracy and speed of segmentation in urban scenes. Many real-time networks do not comprehensively consider contextual information from sub-regions in different directions and at different scales. To handle this problem, a Multi-directional Feature Refinement Module (MFRM) which has three sub-paths to capture information at different scales and directions is proposed. And MFRM reduces computation by using strip pooling and dilated convolution operations. In particular, the authors propose a Feature Cross-guide Aggregation Module to aggregate detailed information and contextual information through the mutual guidance of detailed information and semantic information. This module guides the extraction of feature maps in a more precise direction. Experiments on Cityscapes and CamVid datasets demonstrate the effectiveness of our method by achieving a balance between accuracy and inference speed. Specially, on single 1080Ti GPU, our method yields 78.9% mean intersection over union (mIoU) and 77.4% mIoU at speed of 144.5 frames per second (FPS) and 120.8 FPS on Cityscapes and CamVid datasets respectively.},
  archive      = {J_IETCV},
  author       = {Yan Zhou and Xihong Zheng and Yin Yang and Jianxun Li and Jinzhen Mu and Richard Irampaye},
  doi          = {10.1049/cvi2.12178},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {431-444},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-directional feature refinement network for real-time semantic segmentation in urban street scenes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TANet: Transformer-based asymmetric network for RGB-d
salient object detection. <em>IETCV</em>, <em>17</em>(4), 415–430. (<a
href="https://doi.org/10.1049/cvi2.12177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing RGB-D salient object detection methods mainly rely on a symmetric two-stream Convolutional Neural Network (CNN)-based network to extract RGB and depth channel features separately. However, there are two problems with the symmetric conventional network structure: first, the ability of CNN in learning global contexts is limited; second, the symmetric two-stream structure ignores the inherent differences between modalities. In this study, a Transformer-based asymmetric network is proposed to tackle the issues mentioned above. The authors employ the powerful feature extraction capability of Transformer to extract global semantic information from RGB data and design a lightweight CNN backbone to extract spatial structure information from depth data without pre-training. The asymmetric hybrid encoder effectively reduces the number of parameters in the model while increasing speed without sacrificing performance. Then, a cross-modal feature fusion module which enhances and fuses RGB and depth features with each other is designed. Finally, the authors add edge prediction as an auxiliary task and propose an edge enhancement module to generate sharper contours. Extensive experiments demonstrate that our method achieves superior performance over 14 state-of-the-art RGB-D methods on six public datasets. The code of the authors will be released at https://github.com/lc012463/TANet .},
  archive      = {J_IETCV},
  author       = {Chang Liu and Gang Yang and Shuo Wang and Hangxu Wang and Yunhua Zhang and Yutao Wang},
  doi          = {10.1049/cvi2.12177},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {415-430},
  shortjournal = {IET Comput. Vis.},
  title        = {TANet: Transformer-based asymmetric network for RGB-D salient object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised non-rigid structure from motion with
improved training of wasserstein GANs. <em>IETCV</em>, <em>17</em>(4),
404–414. (<a href="https://doi.org/10.1049/cvi2.12175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a self-supervised method to reconstruct 3D limbic structures from 2D landmarks extracted from a single view. The loss of self-consistency can be reduced by performing a random orthogonal projection of the reconstructed 3D structure. Thus, the training process can be self-supervised by using geometric self-consistency in the reconstruction–projection–reconstruction process. The self-supervised network mainly consists of graph convolution and Transformer encoders. This network is called the SS-Graphformer. By adding a discriminator, the SS-Graphformer is used as a generator to form a Wasserstein Generative Adversarial Network architecture with a Gradient Penalty to improve the accuracy of the reconstruction. It is experimentally demonstrated that the addition of the 2D structure discriminator can significantly improve the accuracy of the reconstruction.},
  archive      = {J_IETCV},
  author       = {Yaming Wang and Xiangyang Peng and Wenqing Huang and Xiaoping Ye and Mingfeng Jiang},
  doi          = {10.1049/cvi2.12175},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {404-414},
  shortjournal = {IET Comput. Vis.},
  title        = {Self-supervised non-rigid structure from motion with improved training of wasserstein GANs},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MCR: Multilayer cross-fusion with reconstructor for
multimodal abstractive summarisation. <em>IETCV</em>, <em>17</em>(4),
389–403. (<a href="https://doi.org/10.1049/cvi2.12173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal abstractive summarisation (MAS) aims to generate a textual summary from multimodal data collection, such as video-text pairs. Despite the success of recent work, the existing methods lack a thorough analysis for consistency across multimodal data. Besides, previous work relies on the fusion method to extract multimodal semantics, neglecting the constraints for complementary semantics of each modality. To address those issues, a multilayer cross-fusion model with the reconstructor for the MAS task is proposed. Their model could thoroughly conduct cross-fusion for each modality via layers of cross-modal transformer blocks, resulting in cross-modal fusion representations with consistency across modalities. Then the reconstructor is employed to reproduce source modalities based on cross-modal fusion representations. The reconstruction process constrains the fusion representations with the complementary semantics of each modality. Comprehensive comparison and ablation experiments on the open domain multimodal dataset How2 are proposed. The results empirically verify the effectiveness of the multilayer cross-fusion with the reconstructor structure on the proposed model.},
  archive      = {J_IETCV},
  author       = {Jingshu Yuan and Jing Yun and Bofei Zheng and Lei Jiao and Limin Liu},
  doi          = {10.1049/cvi2.12173},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {389-403},
  shortjournal = {IET Comput. Vis.},
  title        = {MCR: Multilayer cross-fusion with reconstructor for multimodal abstractive summarisation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video2mesh: 3D human pose and shape recovery by a temporal
convolutional transformer network. <em>IETCV</em>, <em>17</em>(4),
379–388. (<a href="https://doi.org/10.1049/cvi2.12172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From a 2D video of a person in action, human mesh recovery aims to infer the 3D human pose and shape frame by frame. Despite progress on video-based human pose and shape estimation, it is still challenging to guarantee high accuracy and smoothness simultaneously. To tackle this problem, we propose a Video2mesh, a temporal convolutional transformer (TConvTransformer) based temporal network which is able to recover accurate and smooth human mesh from 2D video. The temporal convolution block achieves the sequence-level smoothness by aggregating image features from adjacent frames. The subsequent multi-attention transformer improves the accuracy due to its multi-subspace for better middle-frame feature representation. Meanwhile, we add a TConvTransformer discriminator which is trained together with our 3D human mesh temporal encoder. This TConvTransformer discriminator further improves the accuracy and smoothness by restricting the pose and shape in a more reliable space based on the AMASS dataset. We conduct extensive experiments on three standard benchmark datasets and show that our proposed Video2mesh outperforms other state-of-the-art methods in both accuracy and smoothness.},
  archive      = {J_IETCV},
  author       = {Xianjin Chao and Zhipeng Ge and Howard Leung},
  doi          = {10.1049/cvi2.12172},
  journal      = {IET Computer Vision},
  month        = {6},
  number       = {4},
  pages        = {379-388},
  shortjournal = {IET Comput. Vis.},
  title        = {Video2mesh: 3D human pose and shape recovery by a temporal convolutional transformer network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MTSCANet: Multi temporal resolution temporal semantic
context aggregation network. <em>IETCV</em>, <em>17</em>(3), 366–378.
(<a href="https://doi.org/10.1049/cvi2.12163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localisation is a challenging task, and video context is crucial to localisation actions. Most existing cases that incorporate temporal and semantic contexts into video features suffer from single contextual representation and blurred temporal boundaries. In this study, a multi-temporal resolution pyramid structure model is proposed. Firstly, a temporal-semantic context aggregation module (TSCF) is designed to assign different attention weights to temporal contexts and combine them with multi-level semantics into video features. Secondly, for the problem of large differences in the time span between different actions in the video, a local-global attention module is designed to combine local and global temporal dependencies for each temporal point to obtain a more flexible and robust representation of contextual relations. The redundant representation of the convolution kernel is reduced by modifying the convolution and the arithmetic power is redeployed at a microscopic granularity. To verify the effectiveness of the model, extensive experiments on three challenging datasets are performed. On THUMOS14, the best performance is obtained in IoU@0.3–0.6 with an average mAP of 47.02%. On ActivityNet-1.3, an average mAP of 34.94% was obtained. On HACS, an average mAP of 28.46% was achieved.},
  archive      = {J_IETCV},
  author       = {Haiping Zhang and Conghao Ma and Dongjin Yu and Liming Guan and Dongjing Wang and Zepeng Hu and Xu Liu},
  doi          = {10.1049/cvi2.12163},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {366-378},
  shortjournal = {IET Comput. Vis.},
  title        = {MTSCANet: Multi temporal resolution temporal semantic context aggregation network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial attribute classification by deep mining
inter-attribute correlations. <em>IETCV</em>, <em>17</em>(3), 352–365.
(<a href="https://doi.org/10.1049/cvi2.12171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face attribute classification (FAC) has received considerable attention due to its excellent application value in bio-metric verification and face retrieval. Current FAC methods suffer two typical challenges: complex inter-attribute correlations and imbalanced learning. Aims at the challenges, presents an end-to-end FAC framework with integrated use of multiple strategies, which consists of a convolutional neural network (CNN) and a graph convolutional network (GCN). The GCN is used to model the semantic correlations among attributes and capture inter-dependency among them. The correlation information learnt via the GCN is used to guide the learning of the inter-dependent classification features of the FAC network. An adaptive thresholding strategy and a boosting scheme are adopted to alleviate the effect of the class-imbalance. To deal with the task imbalance problem, a new dynamic weighting scheme is proposed to update the weight of each attribute classification task in the training process. We apply four evaluation metrics to evaluate the proposed method. Experimental results show all the proposed strategies are effective, and our approach outperforms state-of-the-art FAC methods on two challenging datasets CelebA and LFWA.},
  archive      = {J_IETCV},
  author       = {Na Liu and Fan Zhang and Liang Chang and Fuqing Duan},
  doi          = {10.1049/cvi2.12171},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {352-365},
  shortjournal = {IET Comput. Vis.},
  title        = {Facial attribute classification by deep mining inter-attribute correlations},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-distillation object segmentation method via frequency
domain knowledge augmentation. <em>IETCV</em>, <em>17</em>(3), 341–351.
(<a href="https://doi.org/10.1049/cvi2.12170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most self-distillation methods need complex auxiliary teacher structures and require lots of training samples in object segmentation task. To solve this challenging, a self-distillation object segmentation method via frequency domain knowledge augmentation is proposed. Firstly, an object segmentation network which efficiently integrates multi-level features is constructed. Secondly, a pixel-wise virtual teacher generation model is proposed to drive the transferring of pixel-wise knowledge to the object segmentation network through self-distillation learning, so as to improve its generalisation ability. Finally, a frequency domain knowledge adaptive generation method is proposed to augment data, which utilise differentiable quantisation operator to adjust the learnable pixel-wise quantisation table dynamically. What&#39;s more, we reveal convolutional neural network is more inclined to learn low-frequency information during the train. Experiments on five object segmentation datasets show that the proposed method can enhance the performance of the object segmentation network effectively. The boosting performance of our method is better than recent self-distillation methods, and the average F β and mIoU are increased by about 1.5% and 3.6% compared with typical feature refinement self-distillation method.},
  archive      = {J_IETCV},
  author       = {Lei Chen and Tieyong Cao and Yunfei Zheng and Zheng Fang},
  doi          = {10.1049/cvi2.12170},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {341-351},
  shortjournal = {IET Comput. Vis.},
  title        = {A self-distillation object segmentation method via frequency domain knowledge augmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FPC-net: Learning to detect face forgery by adaptive feature
fusion of patch correlation with CG-loss. <em>IETCV</em>,
<em>17</em>(3), 330–340. (<a
href="https://doi.org/10.1049/cvi2.12169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of manipulation technologies, the generation of Deep Fake videos is more accessible than ever. As a result, face forgery detection becomes a challenging task, attracting a significant amount of attention from researchers worldwide. However, most previous work, consisting of convolutional neural networks (CNN), is not sufficiently discriminative and cannot fully utilise subtle clues and similar textures during the process of facial forgery detection. Moreover, these methods cannot simultaneously consider accuracy and time efficiency. To address such problems, we propose a novel framework named FPC-Net to extract some meaningful and unnatural expressions in local regions. This framework utilises CNN, long short-term memory (LSTM), channel groups loss (CG-Loss) and adaptive feature fusion to detect face forgery videos. First, the proposed method exploits spatial features by CNN, and a channel-wise attention mechanism is employed to separate channels. Specifically, with the help of channel groups loss, the channels are divided into two groups, each representing a specific class. Second, LSTM is applied to learn the correlation of spatial features. Finally, the correlation of features is mapped into other latent spaces. Through a lot of experiments, the results are that the detection speed of the proposed method reaches 420 FPS and the auc scores achieve best performance of 99.7%, 99.9%, 94.7%, and 82.0% on Raw Celeb-DF, Raw Face Forensics++, F2F and NT datasets respectively. The experimental results demonstrate that the proposed framework has great time efficiency performance while improving the detection performance compared with other frame-level methods in most cases.},
  archive      = {J_IETCV},
  author       = {Bin Wu and Lichao Su and Dan Chen and Yongli Cheng},
  doi          = {10.1049/cvi2.12169},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {330-340},
  shortjournal = {IET Comput. Vis.},
  title        = {FPC-net: Learning to detect face forgery by adaptive feature fusion of patch correlation with CG-loss},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CDF-net: A convolutional neural network fusing frequency
domain and spatial domain features. <em>IETCV</em>, <em>17</em>(3),
319–329. (<a href="https://doi.org/10.1049/cvi2.12167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN), as a classic deep learning algorithm, has been applied to various computer vision tasks. However, most classic CNN models focus on the extraction and utilisation of spatial domain features, while ignoring the potential ability of frequency domain feature extraction. In this study, the mechanism in the backbone design is explored. Firstly, the traditional DCT formula is converted into a convolution form through mathematical derivation. On the basis of a a new type of convolution, namely the DCT Convolution is designed. It is more applicable to deep learning network architectures. Secondly, based on the DCT Convolution, a new cross-domain fusion network named CDF-Net is designed. The frequency domain and spatial domain features of the input sample are extracted and fused by the network. CDF-Net is a general network framework which can be applied to most existing prevalent networks. Finally, various experiments are conducted. On image classification task, for Imagenet2012 dataset, the method proposed was applied to ResNet50, and the accuracy of Top1 was increased by 3.684%. On object detection task, for COCO2017 dataset, the method proposed in this study was applied to ResNet50 and ResNeXt50, mAP were improved by 0.5% and 1.2% respectively.},
  archive      = {J_IETCV},
  author       = {Aitao Yang and Min Li and Zhaoqing Wu and Yujie He and Xiaohua Qiu and Yu Song and Weidong Du and Yao Gou},
  doi          = {10.1049/cvi2.12167},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {319-329},
  shortjournal = {IET Comput. Vis.},
  title        = {CDF-net: A convolutional neural network fusing frequency domain and spatial domain features},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CAT: Learning to collaborate channel and spatial attention
from multi-information fusion. <em>IETCV</em>, <em>17</em>(3), 309–318.
(<a href="https://doi.org/10.1049/cvi2.12166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel and spatial attention mechanisms have proven to provide an evident performance boost of deep convolution neural networks. Most existing methods focus on one or run them parallel (series), neglecting the collaboration between the two attentions. In order to better establish the feature interaction between the two types of attentions, a plug-and-play attention module is proposed, which is termed as ‘CAT’—activating the Collaboration between spatial and channel Attentions based on learned Traits. Specifically, traits are represented as trainable coefficients (i.e. colla-factors) to adaptively combine contributions of different attention modules to fit different image hierarchies and tasks better. Moreover, the global entropy pooling is proposed apart from global average pooling and global maximum pooling (GMP) operators, which is an effective component in suppressing noise signals by measuring the information disorder of feature maps. A three-way pooling operation is introduced into attention modules and the adaptive mechanism is applied to fuse their outcomes. Extensive experiments on MS COCO, Pascal-VOC, Cifar-100, and ImageNet show that our CAT outperforms the existing state-of-the-art attention mechanisms in object detection, instance segmentation, and image classification. The model and code will be released soon.},
  archive      = {J_IETCV},
  author       = {Zizhang Wu and Man Wang and Weiwei Sun and Yuchen Li and Tianhao Xu and Fan Wang and Keke Huang},
  doi          = {10.1049/cvi2.12166},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {309-318},
  shortjournal = {IET Comput. Vis.},
  title        = {CAT: Learning to collaborate channel and spatial attention from multi-information fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural network acceleration methods via selective
activation. <em>IETCV</em>, <em>17</em>(3), 295–308. (<a
href="https://doi.org/10.1049/cvi2.12164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in neural network recognition accuracy is accompanied by a significant increase in the scales of networks and computations. To make deep learning frameworks widely used on mobile platforms, model acceleration has become extremely important in computer vision. In this study, a novel neural network acceleration method based on selective activation is proposed. First, as the algebraic basis for selective activation, mask general matrix multiplication is used to reduce matrix multiplication calculations. Second, to screen and remove activated neurons and reduce the number of calculations, we introduce an Activation Management Unit that includes two different strategies, Selective Activation with Primary Weights (SAPW) and Selective Activation with Primary Inputs (SAPI). SAPW greatly reduces the number of calculations of the fully connected layer and self-attention and better guarantees detection accuracy. SAPI has the best performance on convolutional architectures, which can significantly reduce the amount of convolutional computation while maintaining the image classification accuracy. We present result of extensive experiments on computational and accuracy tradeoffs and show strong performance for CIFAR-10 classification and Pascal VOC2012 detection. Compared with the dense method, the proposed selective activation method significantly reduces the number of neural network calculations with equal accuracy.},
  archive      = {J_IETCV},
  author       = {Siyu Wang and WeiPeng Li and Ruitao Lu and Xiaogang Yang and Jianxiang Xi and Jiuan Gao},
  doi          = {10.1049/cvi2.12164},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {295-308},
  shortjournal = {IET Comput. Vis.},
  title        = {Neural network acceleration methods via selective activation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Violence 4D: Violence detection in surveillance using 4D
convolutional neural networks. <em>IETCV</em>, <em>17</em>(3), 282–294.
(<a href="https://doi.org/10.1049/cvi2.12162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As violence has increased around the world, surveillance cameras are everywhere, and they are only going to get more ubiquitous. Due to the massive volume of video footage, automatic activity detection systems must be used to create an online warning in the event of aberrant activity. A deep learning architecture is presented in this study using four-dimensional video-level convolution neural networks. The proposed architecture includes residual blocks that are used with three-Dimensional Convolution Neural Networks 3D (CNNs) to learn long-term and short-term spatiotemporal representation from the video as well as record inter-clip interaction. ResNet50 is used as the backbone for three-dimensional convolution networks and dense optical flow for the region of interest. The proposed architecture is applied on four benchmarks for violence and non-violence videos, which are commonly used for violent detection. It obtained test accuracies of 94.67% on RWF2000, 97.29% on Crowd violence, 100% on Movie fight and 100% on the Hockey Fight dataset. These results outperform the previous methods used on RWF2000 datasets.},
  archive      = {J_IETCV},
  author       = {Mai Magdy and Mohamed Waleed Fakhr and Fahima A. Maghraby},
  doi          = {10.1049/cvi2.12162},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {282-294},
  shortjournal = {IET Comput. Vis.},
  title        = {Violence 4D: Violence detection in surveillance using 4D convolutional neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-scale feature representation and interaction network
for underwater object detection. <em>IETCV</em>, <em>17</em>(3),
265–281. (<a href="https://doi.org/10.1049/cvi2.12161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with natural images, underwater images are usually degraded with blur, scale variation, colour shift and texture distortion, which bring much challenge for computer vision tasks like object detection. In this case, generic object detection methods usually fail to achieve satisfactory performance. The main reason is considered that the current methods lack sufficient discriminativeness of feature representation for the degraded underwater images. A a novel multi-scale feature representation and interaction network for underwater object detection is proposed, in which two core modules are elaborately designed to enhance the discriminativeness of feature representation for underwater images. The first is the Context Integration Module, which extracts rich context information from high-level features and is integrated with the feature pyramid network to enhance the feature representation in a multi-scale way. The second is the Dual-refined Attention Interaction Module, which further enhances the feature representation by sufficient interactions between different levels of features both in channel and spatial domains based on attention mechanism. The proposed model is evaluated on four public underwater datasets. The experimental results compared with state-of-the-art object detection methods show that the proposed model has leading performance, which verifies that it is effective for underwater object detection. In addition, object detection experiments on a foggy dataset of Real-world Task-driven Testing Set (RTTS) and the natural image dataset of pattern analysis statistical modelling and computational learning, visual object classes (PASCAL VOC) are conducted. The results show that the proposed model can be applied on the degraded dataset of RTTS but fails on PASCAL VOC.},
  archive      = {J_IETCV},
  author       = {Jiaojiao Yuan and Yongli Hu and Yanfeng Sun and Baocai Yin},
  doi          = {10.1049/cvi2.12161},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {265-281},
  shortjournal = {IET Comput. Vis.},
  title        = {A multi-scale feature representation and interaction network for underwater object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Long short-distance topology modelling of 3D point cloud
segmentation with a graph convolution neural network. <em>IETCV</em>,
<em>17</em>(3), 251–264. (<a
href="https://doi.org/10.1049/cvi2.12160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud segmentation is a non-trivial problem due to its irregular, sparse, and unordered data structure. Existing methods only consider structural relationships of a 3D point and its spatial neighbours. However, the inner-point interactions and long-distance context of a 3D point cloud have been less investigated. In this study, we propose an effective plug-and-play module called the Long Short-Distance Topologically Modelled (LSDTM) Graph Convolutional Neural Network (GCNN) to learn the underlying structure of 3D point clouds. Specifically, we introduce the concept of subgraph to model the contextual-point relationships within a short distance. Then the proposed topology can be reconstructed by recursive aggregation of subgraphs, and importantly, to propagate the contextual scope to a long range. The proposed LSDTM can parse the point cloud data with maximisation of preserving the geometric structure and contextual structure, and the topological graph can be trained end-to-end through a seamlessly integrated GCNN. We provide a case study of triple-layer ternary topology and experimental results on ShapeNetPart, Stanford 3D Indoor Semantics and ScanNet datasets, indicating a significant improvement on the task of 3D point cloud segmentation and validating the effectiveness of our research.},
  archive      = {J_IETCV},
  author       = {Wen Jing Zhang and Song Zhi Su and Qing Qi Hong and Bei Zhan Wang and Li Sun},
  doi          = {10.1049/cvi2.12160},
  journal      = {IET Computer Vision},
  month        = {4},
  number       = {3},
  pages        = {251-264},
  shortjournal = {IET Comput. Vis.},
  title        = {Long short-distance topology modelling of 3D point cloud segmentation with a graph convolution neural network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LSUnetMix: Fuse channel feature information with long–short
term memory. <em>IETCV</em>, <em>17</em>(2), 241–249. (<a
href="https://doi.org/10.1049/cvi2.12158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation based on deep learning is becoming popular. To improve the segmentation accuracy of medical images such as cells and vessels, we propose the LSUnetMix model, which can effectively enhance the ability to extract channel information and capture image details better. It surpasses both traditional and latest models on medical datasets. This model is mainly based on Unet, and three modules have been added. The first is the Channel Information Transmission module, which uses long–short term memory to get the sequential features of layers and unidirectionally transfer the channel information of the upper layer to the lower layer, which combines multiple layers of information while avoiding information redundancy. The second is the Prospect Enhancement module, which activates the channel information of the image and enhances the ability to recognise the segmentation targets. The third is the Multiscale Dilated Convolution module, which uses multi-scale dilated convolution to strengthen the ability and to extract the image details. After experiments, their model performs best on GlaS and MoNuseg datasets, which can segment image details better and avoid redundant information, and the Dice coefficient reaches 91.92% on GlaS and 81.8% on MoNuSeg.},
  archive      = {J_IETCV},
  author       = {Chao Yuan and Yanbo Wang and Yunxuan Xiao},
  doi          = {10.1049/cvi2.12158},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {241-249},
  shortjournal = {IET Comput. Vis.},
  title        = {LSUnetMix: Fuse channel feature information with long–short term memory},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular 3D object detection via mask-revised network and
quality perception loss. <em>IETCV</em>, <em>17</em>(2), 231–240. (<a
href="https://doi.org/10.1049/cvi2.12157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of the monocular 3D detection tasks based on the Pseudo-LiDAR method is improved greatly. However, the depth map obtained by depth estimation contains a lot of noise, which limits the detection accuracy. To address this problem, an efficient monocular 3D target detection method combined with a Mask-Revised Network and Quality Perception Loss is proposed. The method adaptively encodes the image into the mask and adjusts the feature weight from the region of interest by the visual attention mechanism. Then, the 2D bounding box confidence and 3D bounding box quality are used to calculate each confidence of the 3D bounding box to realize the quality perception of prediction results. The proposed algorithm is evaluated on the KITTI test datasets, and the results show that the proposed method achieves state-of-the-art performance on the task of monocular 3D target detection and outperforms the existing methods by about 2.75% on AP 40 for Car categories.},
  archive      = {J_IETCV},
  author       = {Fengsui Wang and Yue Xu and Jingang Chen and Lei Xiong},
  doi          = {10.1049/cvi2.12157},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {231-240},
  shortjournal = {IET Comput. Vis.},
  title        = {Monocular 3D object detection via mask-revised network and quality perception loss},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting spatio-temporal knowledge for video action
recognition. <em>IETCV</em>, <em>17</em>(2), 222–230. (<a
href="https://doi.org/10.1049/cvi2.12154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition has been a popular area of computer vision research in recent years. The goal of this task is to recognise human actions in video frames. Most existing methods often depend on the visual features and their relationships inside the videos. The extracted features only represent the visual information of the current video itself and cannot represent the general knowledge of particular actions beyond the video. Thus, there are some deviations in these features, and the recognition performance still requires improvement. In this sudy, we present a novel spatio-temporal knowledge module (STKM) to endow the current methods with commonsense knowledge. To this end, we first collect hybrid external knowledge from universal fields, which contains both visual and semantic information. Then graph convolution networks (GCN) are used to represent and aggregate this knowledge. The GCNs involve (i) a spatial graph to capture spatial relations and (ii) a temporal graph to capture serial occurrence relations among actions. By integrating knowledge and visual features, we can get better recognition results. Experiments on AVA, UCF101-24 and JHMDB datasets show the robustness and generalisation ability of STKM. The results report a new state-of-the-art 32.0 mAP on AVA v2.1. On UCF101-24 and JHMDB datasets, our method also improves by 1.5 AP and 2.6 AP, respectively, over the baseline method.},
  archive      = {J_IETCV},
  author       = {Huigang Zhang and Liuan Wang and Jun Sun},
  doi          = {10.1049/cvi2.12154},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {222-230},
  shortjournal = {IET Comput. Vis.},
  title        = {Exploiting spatio-temporal knowledge for video action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-task few-shot learning with composed data augmentation
for image classification. <em>IETCV</em>, <em>17</em>(2), 211–221. (<a
href="https://doi.org/10.1049/cvi2.12150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning (FSL) attempts to learn and optimise the model from a few examples on image classification, which is still threatened by data scarcity. To generate more data as supplements, data augmentation is considered as a powerful and popular technique to enhance the robustness of few-shot models. However, there are still some weaknesses in applying augmentation methods. For example, all augmented samples have similar semantic information with respect to different augmented transformations, which makes these traditional augmentation methods incapable of learning the property being varied. To address this challenge, we introduce multi-task learning to learn a primary few-shot classification task and an auxiliary self-supervised task, simultaneously. The self-supervised task can learn transformation property as auxiliary self-supervision signals to improve the performance of the primary few-shot classification task. Additionally, we propose a simple, flexible, and effective mechanism for decision fusion to further improve the reliability of the classifier, named model-agnostic ensemble inference (MAEI). Specifically, the MAEI mechanism can eliminate the influence of outliers for FSL using non-maximum suppression. Extensive experiment results demonstrate that our method can outperform other state-of-the-art methods by large margins.},
  archive      = {J_IETCV},
  author       = {Rui Zhang and Yixin Yang and Yang Li and Jiabao Wang and Hang Li and Zhuang Miao},
  doi          = {10.1049/cvi2.12150},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {211-221},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-task few-shot learning with composed data augmentation for image classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dynamic adjust-head siamese network for object tracking.
<em>IETCV</em>, <em>17</em>(2), 203–210. (<a
href="https://doi.org/10.1049/cvi2.12148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Siamese network based trackers formulate tracking as a similarity matching problem between a target template and a search region. Virtually all popular Siamese trackers use cross-correlation to measure the similarity between the deep feature of template and search image. However, the emphasis for feature extraction in different parts of the image are the same. Besides, the global matching between the template and search region also seriously neglects the part-level information and the deformation of targets during tracking. In this study, to tackle the above issues, a simple but effective Dynamic Adjust-Head (SiamDAH) model is proposed to extract features from different parts of an object. In addition, an improved pixelwise cross-correlation model (PWCC) is designed to enhance the naive cross-correlation operation to produce multiple similarity maps associated with different parts of the target. Experiments on serval challenging benchmarks including OTB-100, GOT-10k, LaSOT, and TrackingNet demonstrate that the proposed SiamDAH outperforms many state-of-the-art trackers and achieves leading performance.},
  archive      = {J_IETCV},
  author       = {Shoumeng Qiu and Yuzhang Gu and Minghong Chen and Zeqiang Yuan and Zehao Yao and Xiaolin Zhang},
  doi          = {10.1049/cvi2.12148},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {203-210},
  shortjournal = {IET Comput. Vis.},
  title        = {A dynamic adjust-head siamese network for object tracking},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised image clustering from multiple incomplete
views via constrastive complementary generation. <em>IETCV</em>,
<em>17</em>(2), 189–202. (<a
href="https://doi.org/10.1049/cvi2.12147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete Multi-View Clustering aims to enhance clustering performance by using data from multiple modalities. Despite the fact that several approaches for studying this issue have been proposed, the following drawbacks still persist: (1) It is difficult to learn latent representations that account for complementarity yet consistency without using label information; (2) and thus fails to take full advantage of the hidden information in incomplete data results in suboptimal clustering performance when complete data is scarce. In this study, Contrastive Incomplete Multi-View Image Clustering with Generative Adversarial Networks (CIMIC-GAN), which uses Generative Adversarial Network (GAN) to fill in incomplete data and uses double contrastive learning to learn consistency on complete and incomplete data is proposed. More specifically, considering diversity and complementary information among multiple modalities, we incorporate autoencoding representation of complete and incomplete data into double contrastive learning to achieve learning consistency. Integrating GANs into the autoencoding process can not only take full advantage of new features of incomplete data, but also better generalise the model in the presence of high data missing rates. Experiments conducted on four extensively used data sets show that CIMIC-GAN outperforms state-of-the-art incomplete multi-View clustering methods.},
  archive      = {J_IETCV},
  author       = {Jiatai Wang and Zhiwei Xu and Xuewen Yang and Dongjin Guo and Limin Liu},
  doi          = {10.1049/cvi2.12147},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {189-202},
  shortjournal = {IET Comput. Vis.},
  title        = {Self-supervised image clustering from multiple incomplete views via constrastive complementary generation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine-grained classification of intracranial haemorrhage
subtypes in head CT scans. <em>IETCV</em>, <em>17</em>(2), 170–188. (<a
href="https://doi.org/10.1049/cvi2.12145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intracranial haemorrhage (ICH) is a haemorrhagic disease that occurs in the ventricle or brain tissue and has a high probability of mortality and disability. For ICH, it is important to obtain a correct diagnosis in the early stages. Currently, ICH classification mainly depends on professional radiologists for manual diagnosis. Therefore, it is necessary to develop a method that can efficiently and rapidly diagnose ICH. In the field of ICH subtype classification, most studies directly use the existing convolutional neural network (CNN) to extract CT slice features. However, these existing networks have the following shortcomings: (1) insufficient discrimination of CT slice features leads to an inability to achieve satisfactory classification performance. (2) Most CT slice data sets of ICH have the serious problem of sample imbalance. (3) There is a correlation between subtypes; however, in previous studies, this correlation has been ignored. To solve these problems, the authors propose a classification algorithm for ICH subtypes applied to CT images. The CNN–RNN architecture was adopted to classify ICH subtypes. In the CNN module, the problem is viewed from a fine-grained perspective, which solves the problem of insufficient feature discrimination in existing methods. A new loss function is also proposed to solve the problems of unbalanced data distribution and neglected dependencies among the labels. These parts are integrated into the proposed fine-grained network architecture. The image embeddings were obtained by the CNN module and then input to the RNN module. The authors’ method was evaluated on the Radiological Society of North America 2019 Brain CT Haemorrhage (RSNA-2019) benchmark. The experimental results demonstrated that the performance of the proposed method is state-of-the-art.},
  archive      = {J_IETCV},
  author       = {Pingping Liu and Gangjun Ning and Lida Shi and Qiuzhan Zhou and Xuan Chen},
  doi          = {10.1049/cvi2.12145},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {170-188},
  shortjournal = {IET Comput. Vis.},
  title        = {Fine-grained classification of intracranial haemorrhage subtypes in head CT scans},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PCformer: A parallel convolutional transformer network for
360° depth estimation. <em>IETCV</em>, <em>17</em>(2), 156–169. (<a
href="https://doi.org/10.1049/cvi2.12144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360° depth estimation has been extensively studied because 360° images provide a full field of view of the surrounding environment as well as a detailed description of the entire scene. However, most well-studied convolutional neural networks (CNNs) for 360° depth estimation can extract local features well, but fail to capture rich global features from the panorama due to a fixed receptive field in CNNs. PCformer, a parallel convolutional transformer network that combines the benefits of CNNs and transformers, is proposed for 360° depth estimation. The transformer has the nature to model long-range dependency and extract global features. With PCformer, both global dependency and local spatial features can be efficiently captured. To fully incorporate global and local features, a dual attention fusion module is designed. Besides, a distortion-weighted loss function is designed to reduce the distortion in panoramas. Extensive experiments demonstrate that the proposed method achieves competitive results against the state-of-the-art methods on three benchmark datasets. Additional experiments also demonstrate that the proposed model has benefits in terms of model complexity and generalisation capability.},
  archive      = {J_IETCV},
  author       = {Chao Xu and Huamin Yang and Cheng Han and Chao Zhang},
  doi          = {10.1049/cvi2.12144},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {156-169},
  shortjournal = {IET Comput. Vis.},
  title        = {PCformer: A parallel convolutional transformer network for 360° depth estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning body part-based pose lexicons for semantic action
recognition. <em>IETCV</em>, <em>17</em>(2), 135–155. (<a
href="https://doi.org/10.1049/cvi2.12143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic action recognition aims to classify actions based on the associated semantics, which can be applied in video captioning and human-machine interaction. In this paper the problem is addressed by jointly learning multiple pose lexicons based on multiple body parts. Specifically, multiple visual pose models are learnt, and one visual pose model is associated with one body part, which characterises the likelihood of an observed video frame being generated from hidden visual poses. Moreover, multiple pose lexicon models are simultaneously learnt along with visual pose models. One pose lexicon model is associated with one body part that establishes a probabilistic mapping between the hidden visual poses and semantic poses parsed from textual instructions. To capture the temporal relations among body parts, a transition model is also learnt to measure the probability of the alignment transitioned from one position to another position. The body part-based pose lexicon learning provides a novel method of cross-modality semantic correlation, which can be applied in other spatial and temporal data. Action classification is finally formulated as the problem of finding the maximum posterior probability that a given multiple sequences of visual frames follow multiple sequences of semantic poses, subject to the most likely visual pose sequences and alignment sequences. Experiments were conducted on five action datasets to validate the effectiveness of the proposed method.},
  archive      = {J_IETCV},
  author       = {Lijuan Zhou and Tao Jiang},
  doi          = {10.1049/cvi2.12143},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {135-155},
  shortjournal = {IET Comput. Vis.},
  title        = {Learning body part-based pose lexicons for semantic action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fine-to-coarse-to-fine weakly supervised framework for
volumetric SD-OCT image segmentation. <em>IETCV</em>, <em>17</em>(2),
123–134. (<a href="https://doi.org/10.1049/cvi2.12139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining accurate segmentation of central serous chorioretinopathy in spectral-domain optical coherence tomography (SD-OCT) is critical for the determination of the disease severity. Although existing methods achieve considerable segmentation results, they heavily depend on large-scale data with high-quality annotations. Also, the lesions bear a large shape variation across different patients, which are often difficult to encode. To address the above problems, we propose a fine-to-coarse-to-fine weakly supervised framework. Specifically, global alternate max-avg pooling (GTP) network can be employed to locate the lesion regions accurately by using only image-level annotations. A network module based on the GTP network and a semantic transfer module are proposed to iteratively guide the network to continuously discover and expand the target lesion regions. Then, we employ 3D grey distribution histogram to generate pseudo-volumetric labels. Finally, a novel 3D level set loss function is proposed to perform coarse-to-fine volumetric segmentation. Experiments on a challenging dataset demonstrate that the performance of our proposed method is closer to those of models trained with pixel-level supervision.},
  archive      = {J_IETCV},
  author       = {Sijie Niu and Ruiwen Xing and Xizhan Gao and Tingting Liu and Yuehui Chen},
  doi          = {10.1049/cvi2.12139},
  journal      = {IET Computer Vision},
  month        = {3},
  number       = {2},
  pages        = {123-134},
  shortjournal = {IET Comput. Vis.},
  title        = {A fine-to-coarse-to-fine weakly supervised framework for volumetric SD-OCT image segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CDFM: A cross-domain few-shot model for marine plankton
classification. <em>IETCV</em>, <em>17</em>(1), 111–121. (<a
href="https://doi.org/10.1049/cvi2.12137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marine plankton classification is important for monitoring marine biological populations and understanding marine ecosystems. However, it is difficult to obtain abundant annotated image data of marine plankton to train classification models of high quality. To classify marine plankton with a small number of image samples, a cross-domain few-shot learning model (CDFM) is proposed. First, CDFM learns knowledge from existing labelled images and then transfers the knowledge to new images. In this process, there is an issue of domain differences between the new images and the existing datasets due to differences in data acquisition time and locations. To address this issue, the authors pre-train a model in the source domain and then use fine-tuning to adapt it to the target domain. Second, the graph neural network is used as a meta-learning module to learn a feature distance metric for marine plankton classification with limited samples. Extensive experiments on four marine plankton image datasets are conducted, including Kaggle Plankton, miniPPlankton, ZooScan and WHOI, and CDFM outperforms existing methods for marine plankton classification.},
  archive      = {J_IETCV},
  author       = {Jin Guo and Wengen Li and Jihong Guan and Hang Gao and Baobo Liu and Lili Gong},
  doi          = {10.1049/cvi2.12137},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {111-121},
  shortjournal = {IET Comput. Vis.},
  title        = {CDFM: A cross-domain few-shot model for marine plankton classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D-FEGNet: A feature enhanced point cloud generation network
from a single image. <em>IETCV</em>, <em>17</em>(1), 98–110. (<a
href="https://doi.org/10.1049/cvi2.12136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based single view 3D reconstruction is a hot topic in computer vision. However, predicting a more realistic 3D point cloud from a single image is an ill-posed problem. In recent years, most of the 3D point cloud prediction methods based on single view are straight-through structure, which will cause the loss of part of feature information and the loss of part of detail information of the resulting point clouds, which will lead to the unsatisfactory visual effect of reconstructed point clouds. In this paper, a Feature-Enhanced 3D point clouds generation Network (3D-FENet) from a single image is proposed. In order to enhance the feature information of RGB image, edge extraction module is adopted. In the process of point cloud generation, a point cloud pyramid is designed, which combines low resolution point cloud with high resolution point cloud to enhance the local details of the generated point clouds. In the fine-tuning stage, the differential projection module is used to fine-tune the whole network by 2D projection of reconstructed point clouds. Experimental results show that the performance of the authors’ proposed method is better than the state-of-the-art studies.},
  archive      = {J_IETCV},
  author       = {Ende Wang and Hui Sun and Bing Wang and Zhiyu Cao and Zhiyuan Liu},
  doi          = {10.1049/cvi2.12136},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {98-110},
  shortjournal = {IET Comput. Vis.},
  title        = {3D-FEGNet: A feature enhanced point cloud generation network from a single image},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MFNet: Panoptic segmentation network based on multiscale
feature weighted fusion and frequency domain attention mechanism.
<em>IETCV</em>, <em>17</em>(1), 88–97. (<a
href="https://doi.org/10.1049/cvi2.12133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing panoptic segmentation networks based on multiscale methods do not distinguish shared features according to different scales, which leads to a loss of feature information and suboptimal results. To solve this problem, the authors improve on UPSNet (a unified panoptic segmentation network) by introducing a multiscale fusion module cascaded between layers into the semantic segmentation head of UPSNet. On the one hand, this enriches the feature information for differently scaled features by cascading the feature information of adjacent scales, and on the other hand, the feature information at each scale can be combined adaptively to achieve a better segmentation effect. Furthermore, a frequency domain attention mechanism is introduced into the backbone network of UPSNet to improve the feature extraction ability of the backbone network by learning more frequency domain feature information. The experimental results of the authors’ improved network on the Common Objects in Context (COCO) and Cityscapes data sets show that compared with UPSNet, the performance is significantly improved: the panoptic quality is improved by 0.9% and 1.6%, respectively.},
  archive      = {J_IETCV},
  author       = {Haiwei Lei and Fangyuan He and Bohui Jia and Qian Wu},
  doi          = {10.1049/cvi2.12133},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {88-97},
  shortjournal = {IET Comput. Vis.},
  title        = {MFNet: Panoptic segmentation network based on multiscale feature weighted fusion and frequency domain attention mechanism},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integration graph attention network and multi-centre
constrained loss for cross-modality person re-identification.
<em>IETCV</em>, <em>17</em>(1), 76–87. (<a
href="https://doi.org/10.1049/cvi2.12132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality person re-identification is a challenging task due to the large visual appearance difference between RGB and infrared images. Existing studies mainly focus on learning local features and ignore the correlation between local features. In this paper, the Integration Graph Attention Network is proposed to learn the completed correlation between local features via the graph structure. To this end, the authors learn the coarse-fine attention weights to aggregate the local features by considering local detail and global information. Furthermore, the Multi-Centre Constrained Loss is proposed to optimise the feature similarity by constraining the centres of modality and identity. It simultaneously utilises three kinds of centre constraints, that is intra-identity centre constraint, modality centre constraint, and inter-identity centre constraint, in order to reduce the influence of modality information explicitly. The proposed method is evaluated on two standard benchmark datasets, that is SYSU-MM01 and RegDB, and the results demonstrate that the authors’ method achieves better performance than the state-of-the-art methods, for example, surpassing NFS by 4.8% and 6.0% mAP on the single-shot setting in All-search and Indoor-search modes, respectively.},
  archive      = {J_IETCV},
  author       = {Di He and Jingrui Zhang and Zhong Zhang and Shuang Liu and Tariq S. Durrani},
  doi          = {10.1049/cvi2.12132},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {76-87},
  shortjournal = {IET Comput. Vis.},
  title        = {Integration graph attention network and multi-centre constrained loss for cross-modality person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot classification using gaussianisation prototypical
classifier. <em>IETCV</em>, <em>17</em>(1), 62–75. (<a
href="https://doi.org/10.1049/cvi2.12129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification (FSC) aims at classifying query samples into correct classes given only a few labelled samples. Prototypical Classifier (PC) can be chosen to be an ideal classifier for settling this problem, as it has good properties of low-capacity and parameter-free. However, the mean-based prototypes suffer from the issue of deviating from its ground-truth centre. In order to solve such problem of prototype bias, Gaussianisation Prototypical Classifier (GPC) is proposed, which is a kind of one-step prototype rectification method. Specifically, the authors first perform Gaussianisation operation over the feature extracted from the backbone network so that the features fit the particular Gaussian distribution. Second, the authors use prototype feature of the base class as prior information and employs Maximum a Posteriori estimation method to obtain the reliable prototype for each novel class. Finally, the query sample of novel class is classified to be its nearest prototype with non-parametric classifiers. Extensive experiments have been conducted on multiple FSC benchmarks. Comparative results also demonstrate that the authors’ method is superior to existing state-of-the-art FSC methods.},
  archive      = {J_IETCV},
  author       = {Fan Liu and Feifan Li and Sai Yang},
  doi          = {10.1049/cvi2.12129},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {62-75},
  shortjournal = {IET Comput. Vis.},
  title        = {Few-shot classification using gaussianisation prototypical classifier},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-template temporal information fusion for siamese
object tracking. <em>IETCV</em>, <em>17</em>(1), 51–61. (<a
href="https://doi.org/10.1049/cvi2.12128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The object tracking algorithm based on Siamese network often extracts the deep feature of the target to be tracked from the first frame of the video sequence as a template, and uses the template for the whole tracking process. Because the manually annotated target in the first frame of video sequence is more accurate, these algorithms often have stable performance. However, it is difficult to adapt to the changing target features only using the target template extracted from the first frame. Inspired by the feature fusion network based on a transformer, this paper proposes a template update module called multi-template temporary information fusion module (MTFM), which can be trained offline. By fusing multiple target template features on time series, the template can always adapt to the changes of target appearance in the tracking process. In order to train the MTFM, this paper proposes a training method using time series data and Mean Square Error (MSE) as the loss function. This paper uses the MTFM on SiamFC++ tracker, and obtains good experimental results in three challenging datasets, including VOT2016, OTB100 and GOT-10k. The running speed of the algorithm on graphics processing unit (GPU) is maintained at about 200fps, which exhibits good real-time performance.},
  archive      = {J_IETCV},
  author       = {Xiaofeng Lu and Zhengyang Wang and Xuan Wang and Xinhong Hei},
  doi          = {10.1049/cvi2.12128},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {51-61},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-template temporal information fusion for siamese object tracking},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic temporal alignment for few-shot action recognition.
<em>IETCV</em>, <em>17</em>(1), 39–50. (<a
href="https://doi.org/10.1049/cvi2.12127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot action recognition aims to learn a classification model with good generalisation ability when trained with only a few labelled videos. However, it is difficult to learn discriminative feature representations for videos in such a setting. The Elastic Temporal Alignment (ETA) for few-shot action recognition is proposed. First, a convolutional neural network is employed to extract feature representations of video frames sparsely sampled from videos. In order to obtain the similarity of two videos, a temporal alignment estimation function is utilised to estimate the matching score between each pair of frames from the two videos through an elastic alignment mechanism. The analysis shows that when we judge whether two frames from respective videos are matched, multiple adjacent frames in the videos should be considered, so as to embody the temporal information. Thus, before feeding per-frame feature vectors of videos into the temporal alignment estimation function, a temporal message passing function is leveraged to propagate the information of per-frame features in the temporal domain. The method has been evaluated on four action recognition datasets, including Kinetics, Something-Something V2, HMDB51, and UCF101. The experimental results verify the effectiveness of ETA and show its superiority over state-of-the-art methods.},
  archive      = {J_IETCV},
  author       = {Fei Pan and Chunlei Xu and Hongjie Zhang and Jie Guo and Yanwen Guo},
  doi          = {10.1049/cvi2.12127},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {39-50},
  shortjournal = {IET Comput. Vis.},
  title        = {Elastic temporal alignment for few-shot action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-specific feature recalibration and alignment for
multi-source unsupervised domain adaptation. <em>IETCV</em>,
<em>17</em>(1), 26–38. (<a
href="https://doi.org/10.1049/cvi2.12126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional unsupervised domain adaptation (UDA) usually assumes that the source domain has labels and the target domain has no labels. In a real environment, labelled source domain data usually comes from multiple different distributions. To handle this problem, multi-source unsupervised domain adaptation (MUDA) is proposed. Multi-source unsupervised domain adaptation aims to adapt the model trained on multi-labelled source domains to the unlabelled target domain. In this paper, a novel MUDA method by domain-specific feature recalibration and alignment (FRA) is proposed. Specifically, to achieve feature recalibration, the authors leverage channel attention to pick out significant channels and spatial attention to focus on important features in different channels. Such integration of channel and spatial attention can lead to effective domain-specific feature recalibration that may be of great importance to MUDA. In addition, to achieve better MUDA, the authors propose domain-specific feature alignment which consists of Maximum Mean Discrepancy and JS-divergence loss. Maximum Mean Discrepancy can reduce the difference between the source domain and target domain. Meanwhile, JS-divergence loss may ensure the prediction consistency of different classifiers in the source domains. Four experiments have proved that FRA can achieve significantly better results in popular benchmarks for MUDA.},
  archive      = {J_IETCV},
  author       = {Mengzhu Wang and Dingyao Chen and Fangzhou Tan and Tianyi Liang and Long Lan and Xiang Zhang and Zhigang Luo},
  doi          = {10.1049/cvi2.12126},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {26-38},
  shortjournal = {IET Comput. Vis.},
  title        = {Domain-specific feature recalibration and alignment for multi-source unsupervised domain adaptation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale pedestrian detection with global–local attention
and multi-scale receptive field context. <em>IETCV</em>, <em>17</em>(1),
13–25. (<a href="https://doi.org/10.1049/cvi2.12125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a basic component in the field of computer vision, the pedestrian detection plays an essential role in several real-world applications such as video surveillance. The promising performance has been achieved in pedestrian detection relying on deep learning, but large-scale variance and small-scale pedestrian detection remain inherently hard as before. In order to deal with the aforementioned problems, this paper proposes a multi-scale pedestrian detection method with global–local attention and multi-scale receptive field context (MRFC). To make the network focus on small-scale pedestrians, we add a high-resolution detection branch on the original detector. To better integrate the incongruous semantic feature, the global–local attention module is embedded to highlight the feature representation of pedestrians so as to implement the feature fusion effectively. In order to adapt the receptive field of the network to achieve scale-variance detection, the MRFC is applied. Based on integrating the above structures, the proposed method achieves competitive results on Caltech and CityPersons datasets. The source code is released in https://github.com/xiaopan999/yolov5-pedestrian_detection .},
  archive      = {J_IETCV},
  author       = {Pan Xue and Houjin Chen and Yanfeng Li and Jupeng Li},
  doi          = {10.1049/cvi2.12125},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {13-25},
  shortjournal = {IET Comput. Vis.},
  title        = {Multi-scale pedestrian detection with global–local attention and multi-scale receptive field context},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modality person re-identification using hybrid mutual
learning. <em>IETCV</em>, <em>17</em>(1), 1–12. (<a
href="https://doi.org/10.1049/cvi2.12123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality person re-identification (Re-ID) aims to retrieve a query identity from red, green, blue (RGB) images or infrared (IR) images. Many approaches have been proposed to reduce the distribution gap between RGB modality and IR modality. However, they ignore the valuable collaborative relationship between RGB modality and IR modality. Hybrid Mutual Learning (HML) for cross-modality person Re-ID is proposed, which builds the collaborative relationship by using mutual learning from the aspects of local features and triplet relation. Specifically, HML contains local-mean mutual learning and triplet mutual learning where they focus on transferring local representational knowledge and structural geometry knowledge so as to reduce the gap between RGB modality and IR modality. Furthermore, Hierarchical Attention Aggregation is proposed to fuse local feature maps and local feature vectors to enrich the information of the classifier input. Extensive experiments on two commonly used data sets, that is, SYSU-MM01 and RegDB verify the effectiveness of the proposed method.},
  archive      = {J_IETCV},
  author       = {Zhong Zhang and Qing Dong and Sen Wang and Shuang Liu and Baihua Xiao and Tariq S. Durrani},
  doi          = {10.1049/cvi2.12123},
  journal      = {IET Computer Vision},
  month        = {2},
  number       = {1},
  pages        = {1-12},
  shortjournal = {IET Comput. Vis.},
  title        = {Cross-modality person re-identification using hybrid mutual learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
