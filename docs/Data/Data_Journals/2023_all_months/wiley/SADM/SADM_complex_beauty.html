<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SADM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sadm---38">SADM - 38</h2>
<ul>
<li><details>
<summary>
(2023). Confidence bounds for threshold similarity graph in random
variable network. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>16</em>(6), 583–595. (<a
href="https://doi.org/10.1002/sam.11642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Problem of uncertainty of graph structure identification in random variable network is considered. An approach for the construction of upper and lower confidence bounds for graph structures is developed. This approach is applied for the construction of upper and lower confidence bounds for the threshold similarity graph. The stability of confidence bounds and gaps between upper and lower confidence bounds are investigated. Theoretical results are illustrated by numerical experiments.},
  archive  = {J},
  author   = {P. A. Koldanov and A. P. Koldanov and D. P. Semenov},
  doi      = {10.1002/sam.11642},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {583-595},
  title    = {Confidence bounds for threshold similarity graph in random variable network},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved D2GAN-based oversampling algorithm for
imbalanced data classification. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>16</em>(6), 569–582. (<a
href="https://doi.org/10.1002/sam.11640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To address the problems of pattern collapse, uncontrollable data generation and high overlap rate when generative adversarial network (GAN) oversamples imbalanced data, we propose an imbalanced data oversampling algorithm based on improved dual discriminator generative adversarial nets (D2GAN). First, we integrate the positive class attribute information into the generator and the discriminator to ensure that the generator only generates the samples for positive class samples, which overcomes the problem of uncontrollable data generation by the generator. Second, we introduce a classifier into D2GAN for discriminating the generated samples and the original data, which avoids the overlap among the generated samples and the negative class samples, and ensures the diversity of the generated samples, the problem of pattern collapse is solved. Finally, the performance of the proposed algorithm is evaluated on 9 datasets by using SVM and neural network classification algorithm for oversampling experiments, the results show that the proposed algorithm effectively improve the classification performance of imbalanced data.},
  archive  = {J},
  author   = {Xiaoqiang Zhao and Qinglei Yao},
  doi      = {10.1002/sam.11640},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {569-582},
  title    = {An improved D2GAN-based oversampling algorithm for imbalanced data classification},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A neutral zone classifier for three classes with an
application to text mining. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>16</em>(6), 560–568. (<a
href="https://doi.org/10.1002/sam.11639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A classifier may be limited by its conditional misclassification rates more than its overall misclassification rate. In the case that one or more of the conditional misclassification rates are high, a neutral zone may be introduced to decrease and possibly balance the misclassification rates. In this paper, a neutral zone is incorporated into a three-class classifier with its region determined by controlling conditional misclassification rates. The neutral zone classifier is illustrated with a text mining application that classifies written comments associated with student evaluations of teaching.},
  archive  = {J},
  author   = {Dylan C. Friel and Yunzhe Li and Benjamin Ellis and Daniel R. Jeske and Herbert K. H. Lee and Philip H. Kass},
  doi      = {10.1002/sam.11639},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {560-568},
  title    = {A neutral zone classifier for three classes with an application to text mining},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric clustering of RNA-sequencing data.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(6), 547–559. (<a
href="https://doi.org/10.1002/sam.11638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Identification of clusters of co-expressed genes in transcriptomic data is a difficult task. Most algorithms used for this purpose can be classified into two broad categories: distance-based or model-based approaches. Distance-based approaches typically utilize a distance function between pairs of data objects and group similar objects together into clusters. Model-based approaches are based on using the mixture-modeling framework. Compared to distance-based approaches, model-based approaches offer better interpretability because each cluster can be explicitly characterized in terms of the proposed model. However, these models present a particular difficulty in identifying a correct multivariate distribution that a mixture can be based upon. In this manuscript, we review some of the approaches used to select a distribution for the needed mixture model first. Then, we propose avoiding this problem altogether by using a nonparametric MSL (maximum smoothed likelihood) algorithm. This algorithm was proposed earlier in statistical literature but has not been, to the best of our knowledge, applied to transcriptomics data. The salient feature of this approach is that it avoids explicit specification of distributions of individual biological samples altogether, thus making the task of a practitioner easier. We performed both a simulation study and an application of the proposed algorithm to two different real datasets. When used on a real dataset, the algorithm produces a large number of biologically meaningful clusters and performs at least as well as several other mixture-based algorithms commonly used for RNA-seq data clustering. Our results also show that this algorithm is capable of uncovering clustering solutions that may go unnoticed by several other model-based clustering algorithms. Our code is publicly available on Github at https://github.com/Matematikoi/non_parametric_clustering},
  archive  = {J},
  author   = {Gabriel Lozano and Nadia Atallah and Michael Levine},
  doi      = {10.1002/sam.11638},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {547-559},
  title    = {Nonparametric clustering of RNA-sequencing data},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensemble learning for score likelihood ratios under the
common source problem. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>16</em>(6), 528–546. (<a
href="https://doi.org/10.1002/sam.11637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine learning-based score likelihood ratios (SLRs) have emerged as alternatives to traditional likelihood ratios and Bayes factors to quantify the value of evidence when contrasting two opposing propositions. When developing a conventional statistical model is infeasible, machine learning can be used to construct a (dis)similarity score for complex data and estimate the ratio of the conditional distributions of the scores. Under the common source problem, the opposing propositions address if two items come from the same source. To develop their SLRs, practitioners create datasets using pairwise comparisons from a background population sample. These comparisons result in a complex dependence structure that violates the independence assumption made by many popular methods. We propose a resampling step to remedy this lack of independence and an ensemble approach to enhance the performance of SLR systems. First, we introduce a source-aware resampling plan to construct datasets where the independence assumption is met. Using these newly created sets, we train multiple base SLRs and aggregate their outputs into a final value of evidence. Our experimental results show that this ensemble SLR can outperform a traditional SLR approach in terms of the rate of misleading evidence and discriminatory power and present more consistent results.},
  archive  = {J},
  author   = {Federico Veneri and Danica M. Ommen},
  doi      = {10.1002/sam.11637},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {528-546},
  title    = {Ensemble learning for score likelihood ratios under the common source problem},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A finely tuned deep transfer learning algorithm to compare
outsole images. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>16</em>(6), 511–527. (<a
href="https://doi.org/10.1002/sam.11636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In forensic practice, evaluating shoeprint evidence is challenging because the differences between images of two different outsoles can be subtle. In this paper, we propose a deep transfer learning-based matching algorithm called the Shoe-MS algorithm that quantifies the similarity between two outsole images. The Shoe-MS algorithm consists of a Siamese neural network for two input images followed by a transfer learning component to extract features from outsole impression images. The added layers are finely tuned using images of shoe soles. To test the performance of the method we propose, we use a study dataset that is both realistic and challenging. The pairs of images for which we know ground truth include (1) close non-matches and (2) mock-crime scene pairs. The Shoe-MS algorithm performed well in terms of prediction accuracy and was able to determine the source of pairs of outsole images, even when comparisons were challenging. When using a score-based likelihood ratio, the algorithm made the correct decision with high probability in a test of the hypothesis that images had a common source. An important advantage of the proposed approach is that pairs of images can be compared without alignment. In initial tests, Shoe-MS exhibited better-discriminating power than existing methods.},
  archive  = {J},
  author   = {Moonsoo Jang and Soyoung Park and Alicia Carriquiry},
  doi      = {10.1002/sam.11636},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {12},
  number   = {6},
  pages    = {511-527},
  title    = {A finely tuned deep transfer learning algorithm to compare outsole images},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traditional kriging versus modern gaussian processes for
large-scale mining data. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>16</em>(5), 488–506. (<a
href="https://doi.org/10.1002/sam.11635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The canonical technique for nonlinear modeling of spatial/point-referenced data is known as kriging in geostatistics, and as Gaussian Process (GP) regression for surrogate modeling and statistical learning. This article reviews many similarities shared between kriging and GPs, but also highlights some important differences. One is that GPs impose a process that can be used to automate kernel/variogram inference, thus removing the human from the loop. The GP framework also suggests a probabilistically valid means of scaling to handle a large corpus of training data, that is, an alternative to ordinary kriging. Finally, recent GP implementations are tailored to make the most of modern computing architectures, such as multi-core workstations and multi-node supercomputers. We argue that such distinctions are important even in classically geostatistical settings. To back that up, we present out-of-sample validation exercises using two, real, large-scale borehole data sets acquired in the mining of gold and other minerals. We compare classic kriging with several variations of modern GPs and conclude that the latter is more economical (fewer human and compute resources), more accurate and offers better uncertainty quantification. We go on to show how the fully generative modeling apparatus provided by GPs can gracefully accommodate left-censoring of small measurements, as commonly occurs in mining data and other borehole assays.},
  archive  = {J},
  author   = {Ryan B. Christianson and Ryan M. Pollyea and Robert B. Gramacy},
  doi      = {10.1002/sam.11635},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {488-506},
  title    = {Traditional kriging versus modern gaussian processes for large-scale mining data},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning factor analysis model based on
importance-weighted variational inference and normalizing flow priors:
Evaluation within a set of multidimensional performance assessments in
youth elite soccer players. <em>Statistical Analysis and Data Mining:
The ASA Data Science Journal</em>, <em>16</em>(5), 474–487. (<a
href="https://doi.org/10.1002/sam.11632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Exploratory factor analysis is a widely used framework in the social and behavioral sciences. Since measurement errors are always present in human behavior data, latent factors, generating the observed data, are important to identify. While most factor analysis methods rely on linear relationships in the data-generating process, deep learning models can provide more flexible modeling approaches. However, two problems need to be addressed. First, for interpretation, scaling assumptions are required, which can be (at least) cumbersome for deep generative models. Second, deep generative models are typically not identifiable, which is required in order to identify the underlying latent constructs. We developed a model that uses a variational autoencoder as an estimator for a complex factor analysis model based on importance-weighted variational inference. In order to receive interpretable results and an identified model, we use a linear factor model with identification constraints in the measurement model. To maintain the flexibility of the model, we use normalizing flow latent priors. Within the evaluation of performance measures in a talent development program in soccer, we found more clarity in the separation of the identified underlying latent dimensions with our models compared to traditional PCA analyses.},
  archive  = {J},
  author   = {Pascal Kilian and Daniel Leyhr and Christopher J. Urban and Oliver Höner and Augustin Kelava},
  doi      = {10.1002/sam.11632},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {474-487},
  title    = {A deep learning factor analysis model based on importance-weighted variational inference and normalizing flow priors: Evaluation within a set of multidimensional performance assessments in youth elite soccer players},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric detection of changepoints in location, scale,
and copula. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>16</em>(5), 456–473. (<a
href="https://doi.org/10.1002/sam.11622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes a new method to detect changepoints in the location and scale of univariate data sequences. The proposed method assumes that the data belong to the location-scale family of distributions and estimate the associated densities nonparametrically. Specifically, the approach does not require knowledge of the functional form of the distribution of the data sequence. As such, the approach can detect changepoints in many distributions. We also propose a new method to detect changes in the location of multivariate sequences, using the marginals and a copula to capture the dependence between variables without the influence of marginal distributions. The performance of the proposed semiparametric approach is contrasted against both other competing nonparametric and Gaussian methods, via simulation studies, as well as applications arising from health and finance.},
  archive  = {J},
  author   = {Gaurav Agarwal and Idris A. Eckley and Paul Fearnhead},
  doi      = {10.1002/sam.11622},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {456-473},
  title    = {Semiparametric detection of changepoints in location, scale, and copula},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new formulation of sparse multiple kernel k-means
clustering and its applications. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>16</em>(5), 436–455. (<a
href="https://doi.org/10.1002/sam.11621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multiple kernel k $$ k $$ -means (MKKM) clustering has been an important research topic in statistical machine learning and data mining over the last few decades. MKKM combines a group of prespecified base kernels to improve the clustering performance. Although many efforts have been made to improve the performance of MKKM further, the present works do not sufficiently consider the potential structure of the partition matrix. In this paper, we propose a novel sparse multiple kernel k $$ k $$ -means (SMKKM) clustering by introducing a ℓ 1 $$ {\ell}_1 $$ -norm to induce the sparsity of the partition matrix. We then design an efficient alternating algorithm with curve search technology. More importantly, the convergence and complexity analysis of the designed algorithm are established based on the optimality conditions of the SMKKM. Finally, extensive numerical experiments on synthetic and benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art methods in terms of clustering performance and robustness.},
  archive  = {J},
  author   = {Wentao Qu and Xianchao Xiu and Jun Sun and Lingchen Kong},
  doi      = {10.1002/sam.11621},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {436-455},
  title    = {A new formulation of sparse multiple kernel k-means clustering and its applications},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Association rules and decision rules. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>16</em>(5), 411–435. (<a
href="https://doi.org/10.1002/sam.11620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Determining association rules of significant interest is an essential task within data mining and statistical analysis. In this paper, we first precisely define the notion of association rule. For this, we introduce a general model, which includes the usual transaction model, and which allows many operations on the association rules. Then, we interpret association rules as statistical decision rules. This interpretation leads to four decisional measures, one of them being the usual confidence. Then, we give some strategies based on the use of these four decisional measures in order to select or to construct association rules with a given consequent. We finally present an experimental study to illustrate these strategies. This study is carried out in R language, with the R-package we specifically built for association rules mining.},
  archive  = {J},
  author   = {Abdelkader Mokkadem and Mariane Pelletier and Louis Raimbault},
  doi      = {10.1002/sam.11620},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {10},
  number   = {5},
  pages    = {411-435},
  title    = {Association rules and decision rules},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Categorical classifiers in multiclass classification with
imbalanced datasets. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>16</em>(4), 391–405. (<a
href="https://doi.org/10.1002/sam.11624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper discusses, in a multiclass classification setting, the issue of the choice of the so-called categorical classifier, which is the procedure or criterion that transforms the probabilities produced by a probabilistic classifier into a single category or class. The standard choice is the Bayes Classifier (BC), but it has some limits with rare classes. This paper studies the classification performance of the BC versus two alternatives, that are the Max Difference Classifier (MDC) and Max Ratio Classifier (MRC), through an extensive simulation and some case studies. The results show that both MDC and MRC are preferable to BC in a multiclass setting with imbalanced data.},
  archive  = {J},
  author   = {Maurizio Carpita and Silvia Golia},
  doi      = {10.1002/sam.11624},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {391-405},
  title    = {Categorical classifiers in multiclass classification with imbalanced datasets},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplicial depth and its median: Selected properties and
limitations. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>16</em>(4), 374–390. (<a
href="https://doi.org/10.1002/sam.11605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depth functions are important tools of nonparametric statistics that extend orderings, ranks, and quantiles to the setup of multivariate data. We revisit the classical definition of the simplicial depth and explore its theoretical properties when evaluated with respect to datasets or measures that do not necessarily possess a symmetric density. Recent advances from discrete geometry are used to refine the results about the robustness and continuity of the simplicial depth and its induced multivariate median. Further, we compute the exact simplicial depth in several scenarios and point out some undesirable behavior: (i) the simplicial depth does not have to be maximized at the center of symmetry of the distribution, (ii) it is not necessarily unimodal, and can possess local extremes, and (iii) the sets of the induced multivariate medians or other central regions do not have to be connected.},
  archive  = {J},
  author   = {Stanislav Nagy},
  doi      = {10.1002/sam.11605},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {374-390},
  title    = {Simplicial depth and its median: Selected properties and limitations},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplicial depth: Characterization and reconstruction.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(4), 358–373. (<a
href="https://doi.org/10.1002/sam.11618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Statistical depth functions have been designed with the intention of extending nonparametric inference toward multivariate setups. As such, the depths should serve as multivariate analogues of the quantile functions known from the analysis of real-valued data. The so-called characterization and reconstruction questions are among the fundamental open problems of the contemporary depth research. Roughly speaking, they ask: (a) Is it is possible that two different datasets, or more generally, two different probability distributions, correspond to identical depths, or does the depth function uniquely characterize the underlying distribution? (b) Knowing a depth function, can we reconstruct the corresponding distribution? For any given depth to constitute a fully-fledged alternative to the quantile function, the depth must characterize wide classes of probability measures, and these measures must be simple to recover from their depths. We investigate these characterization/reconstruction questions for the classical simplicial depth for multivariate data. We show that, under mild conditions, datasets (represented by measures putting equal mass 1 / n $$ 1/n $$ to each datum in a dataset of size n $$ n $$ ) and atomic measures are characterized by, and can be easily reconstructed from, their simplicial depth.},
  archive  = {J},
  author   = {Petra Laketa and Stanislav Nagy},
  doi      = {10.1002/sam.11618},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {358-373},
  title    = {Simplicial depth: Characterization and reconstruction},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Specifying composites in structural equation modeling: A
refinement of the henseler–ogasawara specification. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>16</em>(4), 348–357. (<a
href="https://doi.org/10.1002/sam.11608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Structural equation modeling (SEM) plays an important role in business and social science and so do composites, that is, linear combinations of variables. However, existing approaches to integrate composites into structural equation models still have limitations. A major leap forward has been the Henseler–Ogasawara (H–O) specification, which for the first time allows for seamlessly integrating composites into structural equation models. In doing so, it relies on emergent variables, that is, the composite of interest, and one or more orthogonal excrescent variables, that is, composites that have no surplus meaning but just span the remaining space of the emergent variable&#39;s components. Although the H–O specification enables researchers to flexibly model composites in SEM, it comes along with several practical problems: (i) The H–O specification is difficult to visualize graphically; (ii) its complexity could create difficulties for analysts, and (iii) at times SEM software packages seem to encounter convergence issues with it. In this paper, we present a refinement of the original H–O specification that addresses these three problems. In this new specification, only two components load on each excrescent variable, whereas the excrescent variables are allowed to covary among themselves. This results in a simpler graphical visualization. Additionally, researchers facing convergence issues of the original H–O specification are provided with an alternative specification. Finally, we illustrate the new specification&#39;s application by means of an empirical example and provide guidance on how (standardized) weights including their standard errors can be calculated in the R package lavaan. The corresponding Mplus model syntax is provided in the Supplementary Material.},
  archive  = {J},
  author   = {Xi Yu and Florian Schuberth and Jörg Henseler},
  doi      = {10.1002/sam.11608},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {348-357},
  title    = {Specifying composites in structural equation modeling: A refinement of the Henseler–Ogasawara specification},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Share density-based clustering of income data.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(4), 336–347. (<a
href="https://doi.org/10.1002/sam.11619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Lorenz curve is a fundamental tool for analyzing income and wealth distribution and inequality. Indeed, the Lorenz curve and its derivative, the so-called share density, provide valuable information regarding inequality. There is a widely recognized connection between the Lorenz curve and elements from information theory field. Starting from this evidence, the aim of this work is to compare the income inequality of different subgroups, by using a proper dissimilarity measure, borrowed from information theory, between parametric share densities. This measure is then considered for clustering purposes. To this end, a dynamic clustering algorithm is considered to group unconventional data, such as density functions. Finally, an application, regarding data from Survey on Households Income and Wealth (SHIW) by Bank of Italy, is shown.},
  archive  = {J},
  author   = {Francesca Condino},
  doi      = {10.1002/sam.11619},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {336-347},
  title    = {Share density-based clustering of income data},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new parametric approach to gender gap with application to
EUSILC data in poland and italy. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>16</em>(4), 319–335. (<a
href="https://doi.org/10.1002/sam.11623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Real income distribution comparisons are of interest to policy makers across European countries. Nowadays, a crucial component of income inequality remains the discrepancy between men and women, often called the gender gap. Since the gender gap is related to the whole distribution of incomes in a population, popular single metrics are not adequate, and previous studies applied the relative distribution method, a non-parametric approach to the comparison of distributions. Here, we propose a parametric approach for estimating the relative distribution. Then we extend it to assess the impact of selected covariates—related to the personal characteristics of the samples—on the existing gender gap in both countries. In more detail, models for income were fitted to empirical data from Poland and Italy, from the European Survey of Income and Living Conditions (wave 2018). Afterwards, their parameters were employed to obtain the estimates of relative distribution characteristics. The methods applied in the study turned out to be relevant to describe the gender gap over the entire income range. Finally, the results of the empirical analysis are discussed to reveal similarities and substantial differences between the countries.},
  archive  = {J},
  author   = {Francesca Greselin and Alina Jȩdrzejczak and Kamila Trzcińska},
  doi      = {10.1002/sam.11623},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {8},
  number   = {4},
  pages    = {319-335},
  title    = {A new parametric approach to gender gap with application to EUSILC data in poland and italy},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLADAG 2021 special issue: Selected papers on classification
and data analysis. <em>Statistical Analysis and Data Mining: The ASA
Data Science Journal</em>, <em>16</em>(4), 317–318. (<a
href="https://doi.org/10.1002/sam.11633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Chiara Bocci and Anna Gottard and Thomas Brendan Murphy and Giovanni C. Porzio},
  doi     = {10.1002/sam.11633},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month   = {8},
  number  = {4},
  pages   = {317-318},
  title   = {CLADAG 2021 special issue: Selected papers on classification and data analysis},
  volume  = {16},
  year    = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Buckley–james estimation of generalized additive accelerated
lifetime model with ultrahigh-dimensional data. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>16</em>(3),
305–312. (<a href="https://doi.org/10.1002/sam.11615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {High-dimensional covariates in lifetime data is a challenge in survival analysis, especially in gene expression profile. The objective of this paper is to propose an efficient algorithm to extend the generalized additive model to survival data with high-dimensional covariates. The algorithm is combined of generalized additive (GAM) model and Buckley–James estimation, which makes a nonparametric extension to the nonlinear model, where the GAM is exploited to illustrate the nonlinear effect of the covariates and the Buckley–James estimation is used to address the regression model with right-censored response. In addition, we use maximal-information-coefficient (MIC)-type variable screening and weighted p -value to reduce dimension in high-dimensional situations. The performance of the proposed algorithm is compared with the three benchmark models: Cox proportional hazards regression model, random survival forest, and BJ-AFT on a simulated dataset and two real survival datasets. The results, evaluated by concordance index (C-index) as well as modified mean squared error (mMSE), illustrated the superiority of the proposed algorithm.},
  archive  = {J},
  author   = {Zichang Li and Xuejing Zhao},
  doi      = {10.1002/sam.11615},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {305-312},
  title    = {Buckley–James estimation of generalized additive accelerated lifetime model with ultrahigh-dimensional data},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust deep neural network surrogate models with uncertainty
quantification via adversarial training. <em>Statistical Analysis and
Data Mining: The ASA Data Science Journal</em>, <em>16</em>(3), 295–304.
(<a href="https://doi.org/10.1002/sam.11610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Surrogate models have been used to emulate mathematical simulators of physical or biological processes for computational efficiency. High-speed simulation is crucial for conducting uncertainty quantification (UQ) when the simulation must repeat over many randomly sampled input points (aka the Monte Carlo method). A simulator can be so computationally intensive that UQ is only feasible with a surrogate model. Recently, deep neural network (DNN) surrogate models have gained popularity for their state-of-the-art emulation accuracy. However, it is well-known that DNN is prone to severe errors when input data are perturbed in particular ways, the very phenomenon which has inspired great interest in adversarial training. In the case of surrogate models, the concern is less about a deliberate attack exploiting the vulnerability of a DNN but more of the high sensitivity of its accuracy to input directions, an issue largely ignored by researchers using emulation models. In this paper, we show the severity of this issue through empirical studies and hypothesis testing. Furthermore, we adopt methods in adversarial training to enhance the robustness of DNN surrogate models. Experiments demonstrate that our approaches significantly improve the robustness of the surrogate models without compromising emulation accuracy.},
  archive  = {J},
  author   = {Lixiang Zhang and Jia Li},
  doi      = {10.1002/sam.11610},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {295-304},
  title    = {Robust deep neural network surrogate models with uncertainty quantification via adversarial training},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchy-assisted gene expression regulatory network
analysis. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(3), 272–294. (<a
href="https://doi.org/10.1002/sam.11609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Gene expressions have been extensively studied in biomedical research. With gene expression, network analysis, which takes a system perspective and examines the interconnections among genes, has been established as highly important and meaningful. In the construction of gene expression networks, a commonly adopted technique is high-dimensional regularized regression. Network construction can be unadjusted (which focuses on gene expressions only) and adjusted (which also incorporates regulators of gene expressions), and the two types of construction have different implications and can be equally important. In this article, we propose a variable selection hierarchy to connect the unadjusted regression-based network construction with the adjusted construction that incorporates two or more types of regulators. This hierarchy is sensible and amounts to additional information for both constructions, thus having the potential of improving variable selection and estimation. An effective computational algorithm is developed, and extensive simulation demonstrates the superiority of the proposed construction over multiple closely relevant alternatives. The analysis of TCGA data further demonstrates the practical utility of the proposed approach.},
  archive  = {J},
  author   = {Han Yan and Sanguo Zhang and Shuangge Ma},
  doi      = {10.1002/sam.11609},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {272-294},
  title    = {Hierarchy-assisted gene expression regulatory network analysis},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive boosting for ordinal target variables using neural
networks. <em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(3), 257–271. (<a
href="https://doi.org/10.1002/sam.11613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Boosting has proven its superiority by increasing the diversity of base classifiers, mainly in various classification problems. In reality, target variables in classification often are formed by numerical variables, in possession of ordinal information. However, existing boosting algorithms for classification are unable to reflect such ordinal target variables, resulting in non-optimal solutions. In this paper, we propose a novel algorithm of ordinal encoding adaptive boosting (AdaBoost) using a multi-dimensional encoding scheme for ordinal target variables. Extending an original binary-class AdaBoost, the proposed algorithm is equipped with a multi-class exponential loss function. We show that it achieves the Bayes classifier and establishes forward stagewise additive modeling. We demonstrate the performance of the proposed algorithm with a base learner as a neural network. Our experiments show that it outperforms existing boosting algorithms in various ordinal datasets.},
  archive  = {J},
  author   = {Insung Um and Geonseok Lee and Kichun Lee},
  doi      = {10.1002/sam.11613},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {257-271},
  title    = {Adaptive boosting for ordinal target variables using neural networks},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lq regularization for fair artificial intelligence robust to
covariate shift. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>16</em>(3), 237–256. (<a
href="https://doi.org/10.1002/sam.11616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is well recognized that historical biases exist in training data against a certain sensitive group (e.g., non-White, women) which are socially unacceptable, and these unfair biases are inherited in trained artificial intelligence (AI) models. Various learning algorithms have been proposed to remove or alleviate unfair biases in trained AI models. In this paper, we consider another type of bias in training data so-called covariate shift in view of fair AI. Here, covariate shift means that training data do not represent the population of interest well. Covariate shift occurs when special sampling designs (e.g., stratified sampling) are used when collecting training data, or the population where training data are collected is different from the population of interest. When covariate shift exists, fair AI models on training data may not be fair in test data. To ensure fairness on test data, we develop computationally efficient learning algorithms robust to covariate shifts. In particular, we propose a robust fairness constraint based on the L q norm which is a generic algorithm to be applied to various fairness AI problems without much hampering. By analyzing multiple benchmark datasets, we show that our proposed robust fairness AI algorithm improves existing fair AI algorithms much in terms of the fairness-accuracy tradeoff to covariate shift and has significant computational advantages compared to other robust fair AI algorithms.},
  archive  = {J},
  author   = {Seonghyeon Kim and Sara Kim and Kunwoong Kim and Yongdai Kim},
  doi      = {10.1002/sam.11616},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {237-256},
  title    = {Lq regularization for fair artificial intelligence robust to covariate shift},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly robust estimation for non-probability samples with
modified intertwined probabilistic factors decoupling. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>16</em>(3), 224–236. (<a
href="https://doi.org/10.1002/sam.11614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, non-probability samples, such as web survey samples, have become increasingly popular in many fields, but they may be subject to selection biases, which results in the difficulty for inference from them. Doubly robust (DR) estimation is one of the approaches to making inferences from non-probability samples. When many covariates are available, variable selection becomes important in DR estimation. In this paper, a new DR estimator for the finite population mean is constructed, where the intertwined probabilistic factors decoupling (IPAD) and modified IPAD are used to select important variables in the propensity score model and the outcome superpopulation model, respectively. Unlike the traditional variable selection approaches, such as adaptive least absolute shrinkage and selection operator and smoothly clipped absolute deviations, IPAD and the modified IPAD not only can select important variables and estimate parameters, but also can control the false discovery rate, which can produce more accurate population estimators. Asymptotic theories and variance estimation of the DR estimator with a modified IPAD are established. Results from simulation studies indicate that our proposed estimator performs well. We apply the proposed method to the analysis of the Pew Research Center data and the Behavioral Risk Factor Surveillance System data.},
  archive  = {J},
  author   = {Zhan Liu and Junbo Zheng and Yingli Pan},
  doi      = {10.1002/sam.11614},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {224-236},
  title    = {Doubly robust estimation for non-probability samples with modified intertwined probabilistic factors decoupling},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bilateral-weighted online adaptive isolation forest for
anomaly detection in streaming data. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>16</em>(3), 215–223. (<a
href="https://doi.org/10.1002/sam.11612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a method called Bilateral-Weighted Online Adaptive Isolation Forest (BWOAIF) for unsupervised anomaly detection based on Isolation Forest (IF), which is applicable to streaming data and able to cope with concept drift. Similar to IF, the proposed method has only few hyperparameters whose effect on the performance are easy to interpret by human intuition and therefore easy to tune. BWOAIF ingests data and classifies it as normal or anomalous, and simultaneously adapts its classifier by removing old trees as well as by creating new ones. We show that BWOAIF adapts gradually to slow concept drifts, and, at the same time, it is able to adapt fast to sudden changes of the data distribution. Numerical results show the efficacy of the proposed algorithm and its ability to learn different classes of concept drifts, such as slow/fast concept shift, concept split, concept appearance, and concept disappearance.},
  archive  = {J},
  author   = {Gábor Hannák and Gábor Horváth and Attila Kádár and Márk Dániel Szalai},
  doi      = {10.1002/sam.11612},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {6},
  number   = {3},
  pages    = {215-223},
  title    = {Bilateral-weighted online adaptive isolation forest for anomaly detection in streaming data},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised multi-label learning with missing labels by
exploiting feature-label correlations. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>16</em>(2), 187–209. (<a
href="https://doi.org/10.1002/sam.11607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The majority of multi-learning techniques now in use presuppose that there will be enough labeled instances. But in real-world applications, it is frequently the case that only partial labels are included for each training instance. This is either because getting a fully labeled training set takes a lot of time and effort or because doing so is expensive. Multi-label learning with missing labels, on the other hand, has greater practical value. In this paper, we propose a brand-new semi-supervised multi-label learning method (SMLMFC) that specifically addresses missing-label scenarios. After successfully filling in the missing labels for instances using two-stage label correlations, SMLMFC trains a semi-supervised multi-label classifier by imposing feature-label correlation restrictions directly on the output of labels. The complex relationships between features and labels can be learned and implicitly captured through feature-label correlations, in particular. The experimental results on a number of real-world multi-label datasets confirm that SMLMFC has strong competitiveness in comparison to other state-of-the-art methods.},
  archive  = {J},
  author   = {Runxin Li and Xuefeng Zhao and Zhenhong Shang and Lianyin Jia},
  doi      = {10.1002/sam.11607},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {187-209},
  title    = {Semi-supervised multi-label learning with missing labels by exploiting feature-label correlations},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model selection with bootstrap validation. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>16</em>(2), 162–186. (<a
href="https://doi.org/10.1002/sam.11606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Model selection is one of the most central tasks in supervised learning. Validation set methods are the standard way to accomplish this task: models are trained on training data, and the model with the smallest loss on the validation data is selected. However, it is generally not obvious how much validation data is required to make a reliable selection, which is essential when labeled data are scarce or expensive. We propose a bootstrap-based algorithm, bootstrap validation (BSV), that uses the bootstrap to adjust the validation set size and to find the best-performing model within a tolerance parameter specified by the user. We find that BSV works well in practice and can be used as a drop-in replacement for validation set methods or k -fold cross-validation. The main advantage of BSV is that less validation data is typically needed, so more data can be used to train the model, resulting in better approximations and efficient use of validation data.},
  archive  = {J},
  author   = {Rafael Savvides and Jarmo Mäkelä and Kai Puolamäki},
  doi      = {10.1002/sam.11606},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {162-186},
  title    = {Model selection with bootstrap validation},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Randomized algorithms for tensor response regression.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(2), 149–161. (<a
href="https://doi.org/10.1002/sam.11603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we consider the estimation algorithm of tensor response on vector covariate regression model. Based on projection theory of tensor and the idea of randomized algorithm for tensor decomposition, three new algorithms named SHOLRR, RHOLRR and RSHOLRR are proposed under the low-rank Tucker decomposition and some theoretical analyses for two randomized algorithms are also provided. To explore the nonlinear relationship between tensor response and vector covariate, we develop the KRSHOLRR algorithm based on kernel trick and RSHOLRR algorithm. Our proposed algorithms can not only guarantee high estimation accuracy but also have the advantage of fast computing speed, especially for higher-order tensor response. Through extensive synthesized data analyses and applications to two real datasets, we demonstrate the outperformance of our proposed algorithms over the stat-of-art.},
  archive  = {J},
  author   = {Zhe Cheng and Xiangjian Xu and Zihao Song and Weihua Zhao},
  doi      = {10.1002/sam.11603},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {149-161},
  title    = {Randomized algorithms for tensor response regression},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cluster analysis via random partition distributions.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(2), 135–148. (<a
href="https://doi.org/10.1002/sam.11602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hierarchical and k-medoids clustering are deterministic clustering algorithms defined on pairwise distances. We use these same pairwise distances in a novel stochastic clustering procedure based on a probability distribution. We call our proposed method CaviarPD, a portmanteau from cluster analysis via random partition distributions. CaviarPD first samples clusterings from a distribution on partitions and then finds the best cluster estimate based on these samples using algorithms to minimize an expected loss. Using eight case studies, we show that our approach produces results as close to the truth as hierarchical and k-medoids methods, and has the additional advantage of allowing for a probabilistic framework to assess clustering uncertainty. The method provides an intuitive graphical representation of clustering uncertainty through pairwise probabilities from partition samples. A software implementation of the method is available in the CaviarPD package for R.},
  archive  = {J},
  author   = {David B. Dahl and Jacob Andros and J. Brandon Carter},
  doi      = {10.1002/sam.11602},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {135-148},
  title    = {Cluster analysis via random partition distributions},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrative learning of structured high-dimensional data
from multiple datasets. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>16</em>(2), 120–134. (<a
href="https://doi.org/10.1002/sam.11601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Integrative learning of multiple datasets has the potential to mitigate the challenge of small n $$ n $$ and large p $$ p $$ that is often encountered in analysis of big biomedical data such as genomics data. Detection of weak yet important signals can be enhanced by jointly selecting features for all datasets. However, the set of important features may not always be the same across all datasets. Although some existing integrative learning methods allow heterogeneous sparsity structure where a subset of datasets can have zero coefficients for some selected features, they tend to yield reduced efficiency, reinstating the problem of losing weak important signals. We propose a new integrative learning approach which can not only aggregate important signals well in homogeneous sparsity structure, but also substantially alleviate the problem of losing weak important signals in heterogeneous sparsity structure. Our approach exploits a priori known graphical structure of features and encourages joint selection of features that are connected in the graph. Integrating such prior information over multiple datasets enhances the power, while also accounting for the heterogeneity across datasets. Theoretical properties of the proposed method are investigated. We also demonstrate the limitations of existing approaches and the superiority of our method using a simulation study and analysis of gene expression data from ADNI.},
  archive  = {J},
  author   = {Changgee Chang and Zongyu Dai and Jihwan Oh and Qi Long},
  doi      = {10.1002/sam.11601},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {120-134},
  title    = {Integrative learning of structured high-dimensional data from multiple datasets},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation and interpretation of driving risks: Automobile
claim frequency modeling with telematics data. <em>Statistical Analysis
and Data Mining: The ASA Data Science Journal</em>, <em>16</em>(2),
97–119. (<a href="https://doi.org/10.1002/sam.11599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the development of vehicle telematics and data mining technology, usage-based insurance (UBI) has aroused widespread interest from both academia and industry. The extensive driving behavior features make it possible to further understand the risks of insured vehicles, but pose challenges in the identification and interpretation of important ratemaking factors. This study, based on the telematics data of policyholders in China&#39;s mainland, analyzes insurance claim frequency of commercial trucks using both Poisson regression and several machine learning models, including regression tree, random forest, gradient boosting tree, XGBoost and neural network. After selecting the best model, we analyze feature importance, feature effects and the contribution of each feature to the prediction from an actuarial perspective. Our empirical study shows that XGBoost greatly outperforms the traditional models and detects some important risk factors, such as the average speed, the average mileage traveled per day, the fraction of night driving, the number of sudden brakes and the fraction of left/right turns at intersections. These features usually have a nonlinear effect on driving risk, and there are complex interactions between features. To further distinguish high−/low-risk drivers, we run supervised clustering for risk segmentation according to drivers&#39; driving habits. In summary, this study not only provide a more accurate prediction of driving risk, but also greatly satisfy the interpretability requirements of insurance regulators and risk management.},
  archive  = {J},
  author   = {Yaqian Gao and Yifan Huang and Shengwang Meng},
  doi      = {10.1002/sam.11599},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {4},
  number   = {2},
  pages    = {97-119},
  title    = {Evaluation and interpretation of driving risks: Automobile claim frequency modeling with telematics data},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>16</em>(1), 92. (<a
href="https://doi.org/10.1002/sam.11604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1002/sam.11604},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month   = {2},
  number  = {1},
  pages   = {92},
  title   = {Corrigendum},
  volume  = {16},
  year    = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature screening of ultrahigh dimensional longitudinal data
based on the c-statistic. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>16</em>(1), 80–91. (<a
href="https://doi.org/10.1002/sam.11597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper considers the feature screening method for the ultrahigh dimensional semiparametric linear models with longitudinal data. The C-statistic which measures the rank concordance between predictors and outcomes is generalized to the longitudinal data. On the basis of C-statistic and the score equation theory, we propose a feature screening method named LCSIS. Based on the smoothed technique and the score equations, the proposed estimating screening procedure is easy to compute and satisfies the feature screening consistency. Furthermore, Monte Carlo simulation studies and a real data application are conducted to examine the finite sample performance of the proposed procedure.},
  archive  = {J},
  author   = {Peng Lai and Qing Di and Zhezi Shen and Yanqiu Zhou},
  doi      = {10.1002/sam.11597},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {80-91},
  title    = {Feature screening of ultrahigh dimensional longitudinal data based on the C-statistic},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An auxiliary part-of-speech tagger for blog and microblog
cyber-slang. <em>Statistical Analysis and Data Mining: The ASA Data
Science Journal</em>, <em>16</em>(1), 65–79. (<a
href="https://doi.org/10.1002/sam.11596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The increasing impact of Web 2.0 involves a growing usage of slang, abbreviations, and emphasized words, which limit the performance of traditional natural language processing models. The state-of-the-art Part-of-Speech (POS) taggers are often unable to assign a meaningful POS tag to all the words in a Web 2.0 text. To solve this limitation, we are proposing an auxiliary POS tagger that assigns the POS tag to a given token based on the information deriving from a sequence of preceding and following POS tags. The main advantage of the proposed auxiliary POS tagger is its ability to overcome the need of tokens&#39; information since it only relies on the sequences of existing POS tags. This tagger is called auxiliary because it requires an initial POS tagging procedure that might be performed using online dictionaries (e.g., Wikidictionary) or other POS tagging algorithms. The auxiliary POS tagger relies on a Bayesian network that uses information about preceding and following POS tags. It was evaluated on the Brown Corpus, which is a general linguistics corpus, on the modern ARK dataset composed by Twitter messages, and on a corpus of manually labeled Web 2.0 data.},
  archive  = {J},
  author   = {Silvia Golia and Paola Zola},
  doi      = {10.1002/sam.11596},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {65-79},
  title    = {An auxiliary part-of-speech tagger for blog and microblog cyber-slang},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trees, forests, chickens, and eggs: When and why to prune
trees in a random forest. <em>Statistical Analysis and Data Mining: The
ASA Data Science Journal</em>, <em>16</em>(1), 45–64. (<a
href="https://doi.org/10.1002/sam.11594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Due to their long-standing reputation as excellent off-the-shelf predictors, random forests (RFs) continue to remain a go-to model of choice for applied statisticians and data scientists. Despite their widespread use, however, until recently, little was known about their inner workings and about which aspects of the procedure were driving their success. Very recently, two competing hypotheses have emerged–one based on interpolation and the other based on regularization. This work argues in favor of the latter by utilizing the regularization framework to reexamine the decades-old question of whether individual trees in an ensemble ought to be pruned. Despite the fact that default constructions of RFs use near full depth trees in most popular software packages, here we provide strong evidence that tree depth should be seen as a natural form of regularization across the entire procedure. In particular, our work suggests that RFs with shallow trees are advantageous when the signal-to-noise ratio in the data is low. In building up this argument, we also critique the newly popular notion of “double descent” in RFs by drawing parallels to U -statistics and arguing that the noticeable jumps in random forest accuracy are the result of simple averaging rather than interpolation.},
  archive  = {J},
  author   = {Siyu Zhou and Lucas Mentch},
  doi      = {10.1002/sam.11594},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {45-64},
  title    = {Trees, forests, chickens, and eggs: When and why to prune trees in a random forest},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online embedding and clustering of evolving data streams.
<em>Statistical Analysis and Data Mining: The ASA Data Science
Journal</em>, <em>16</em>(1), 29–44. (<a
href="https://doi.org/10.1002/sam.11590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Number of connected devices is steadily increasing and this trend is expected to continue in the near future. Connected devices continuously generate data streams and the data streams may often be high dimensional and contain concept drift. Clustering is one of the most suitable methods for real-time data stream processing, since clustering can be applied with less prior information about the data. Also, data embedding makes the visualization of high dimensional data possible and may simplify clustering process. There exist several data stream clustering algorithms in the literature; however, no data stream embedding method exists. Uniform Manifold Approximation and Projection (UMAP) is a data embedding algorithm that is suitable to be applied on stationary (stable) data streams, though it cannot adapt concept drift. In this study, we describe a novel method EmCStream, to apply UMAP on evolving (nonstationary) data streams, to detect and adapt concept drift and to cluster embedded data instances using a distance or partitioning-based clustering algorithm. We have evaluated EmCStream against the state-of-the-art stream clustering algorithms using both synthetic and real data streams containing concept drift. EmCStream outperforms DenStream and CluStream, in terms of clustering quality, on both synthetic and real evolving data streams. Datasets and code of this study are available online at https://gitlab.com/alaettinzubaroglu/emcstream .},
  archive  = {J},
  author   = {Alaettin Zubaroğlu and Volkan Atalay},
  doi      = {10.1002/sam.11590},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {29-44},
  title    = {Online embedding and clustering of evolving data streams},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of disease progression for ischemic heart disease
using latent markov with covariates. <em>Statistical Analysis and Data
Mining: The ASA Data Science Journal</em>, <em>16</em>(1), 16–28. (<a
href="https://doi.org/10.1002/sam.11589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Contemporaneous monitoring of disease progression, in addition to early diagnosis, is important for the treatment of patients with chronic conditions. Chronic disease-related factors are not easily tractable, and the existing data sets do not clearly reflect them, making diagnosis difficult. The primary issue is that databases maintained by health care, insurance, or governmental organizations typically do not contain clinical information and instead focus on patient appointments and demographic profiles. Due to the lack of thorough information on potential risk factors for a single patient, investigations on the nature of disease are imprecise. We suggest the use of a latent Markov model with variables in a latent process because it enables the panel analysis of many forms of data. The purpose of this study is to evaluate unobserved factors in ischemic heart disease (IHD) using longitudinal data from electronic health records. Based on the results we designate states as healthy, light, moderate, and severe to represent stages of disease progression. This study demonstrates that gender, patient age, and hospital visit frequency are all significant factors in the development of the disease. Females acquire IHD more rapidly than males, frequently developing from moderate and severe disease. In addition, it demonstrates that individuals under the age of 20 bypass the light state of IHD and proceed directly to the moderate state.},
  archive  = {J},
  author   = {Zarina Oflaz and Ceylan Yozgatligil and A. Sevtap Selcuk-Kestel},
  doi      = {10.1002/sam.11589},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {16-28},
  title    = {Estimation of disease progression for ischemic heart disease using latent markov with covariates},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A study of the impact of COVID-19 on the chinese stock
market based on a new textual multiple ARMA model. <em>Statistical
Analysis and Data Mining: The ASA Data Science Journal</em>,
<em>16</em>(1), 5–15. (<a
href="https://doi.org/10.1002/sam.11582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Coronavirus 2019 (COVID-19) has caused violent fluctuation in stock markets, and led to heated discussion in stock forums. The rise and fall of any specific stock is influenced by many other stocks and emotions expressed in forum discussions. Considering the transmission effect of emotions, we propose a new Textual Multiple Auto Regressive Moving Average (TM-ARMA) model to study the impact of COVID-19 on the Chinese stock market. The TM-ARMA model contains a new cross-textual term and a new cross-auto regressive (AR) term that measure the cross impacts of textual emotions and price fluctuations, respectively, and the adjacent matrix which measures the relationships among stocks is updated dynamically. We compute the textual sentiment scores by an emotion dictionary-based method, and estimate the parameter matrices by a maximum likelihood method. Our dataset includes the textual posts from the Eastmoney Stock Forum and the price data for the constituent stocks of the FTSE China A50 Index. We conduct a sliding-window online forecast approach to simulate the real-trading situations. The results show that TM-ARMA performs very well even after the attack of COVID-19.},
  archive  = {J},
  author   = {Weijun Xu and Zhineng Fu and Hongyi Li and Jinglong Huang and Weidong Xu and Yiyang Luo},
  doi      = {10.1002/sam.11582},
  journal  = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month    = {2},
  number   = {1},
  pages    = {5-15},
  title    = {A study of the impact of COVID-19 on the chinese stock market based on a new textual multiple ARMA model},
  volume   = {16},
  year     = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
