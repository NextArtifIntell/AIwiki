<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>EXSY_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="exsy---260">EXSY - 260</h2>
<ul>
<li><details>
<summary>
(2023). An optimized deep learning model for human activity
recognition using inertial measurement units. <em>EXSY</em>,
<em>40</em>(10), e13457. (<a
href="https://doi.org/10.1111/exsy.13457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) has recently gained popularity due to its applications in healthcare, surveillance, human-robot interaction, and various other fields. Deep learning (DL)-based models have been successfully applied to the raw data captured through inertial measurement unit (IMU) sensors to recognize multiple human activities. Despite the success of DL-based models in human activity recognition, feature extraction remains challenging due to class imbalance and noisy data. Additionally, selecting optimal hyperparameter values for DL models is essential since they affect model performance. The hyperparameter values of some of the existing DL-based HAR models are chosen randomly or through the trial-and-error method. The random selection of these significant hyperparameters may be suitable for some applications, but sometimes it may worsen the model&#39;s performance in others. Hence, to address the above-mentioned issues, this research aims to develop an optimized DL model capable of recognizing various human activities captured through IMU sensors. The proposed DL-based HAR model combines convolutional neural network (CNN) layers and bidirectional long short-term memory (Bi-LSTM) units to simultaneously extract spatial and temporal sequence features from raw sensor data. The Rao-3 metaheuristic optimization algorithm has been adopted to identify the ideal hyperparameter values for the proposed DL model in order to enhance its recognition performance. The proposed DL model&#39;s performance is validated on PAMAP2, UCI-HAR, and MHEALTH datasets and achieved 94.91%, 97.16%, and 99.25% accuracies, respectively. The results reveal that the proposed DL model performs better than the existing state-of-the-art (SoTA) models.},
  archive      = {J_EXSY},
  author       = {Sravan Kumar Challa and Akhilesh Kumar and Vijay Bhaskar Semwal and Nidhi Dua},
  doi          = {10.1111/exsy.13457},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13457},
  shortjournal = {Expert Syst.},
  title        = {An optimized deep learning model for human activity recognition using inertial measurement units},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Question answering over knowledge graphs using BERT based
relation mapping. <em>EXSY</em>, <em>40</em>(10), e13456. (<a
href="https://doi.org/10.1111/exsy.13456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A knowledge graph (KG) is a structured form of knowledge describing real-world entities, properties and relationships as a graph. Question answering over knowledge graphs (KGQA) allows people to ask questions in natural language and extract answers from KG accurately and more quickly. The main task of a KGQA is to convert a natural language query to the corresponding structured query form like SPARQL. However, generating the precise SPARQL query from a question is challenging and highly error-prone. Here we propose a question-answering framework that uses KG to answer simple questions without using SPARQL. Question classification, dependency parsing, entity linking, BERT-based relation finding and answer extraction constitute the main modules of the approach. We have used the DBpedia as the KG and tested the end-to-end system with a subset of QALD-4, LC-QuAD and SimpleQuestions datasets. Results show considerable improvement compared to other approaches in terms of F1-score.},
  archive      = {J_EXSY},
  author       = {Suneera C. M. and Jay Prakash and Pramod Kumar Singh},
  doi          = {10.1111/exsy.13456},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13456},
  shortjournal = {Expert Syst.},
  title        = {Question answering over knowledge graphs using BERT based relation mapping},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GBNN algorithm enhanced by movement planner for UV-c
disinfection. <em>EXSY</em>, <em>40</em>(10), e13455. (<a
href="https://doi.org/10.1111/exsy.13455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to maintain adequate levels of cleanliness and sanitation in public facilities, prevent the buildup of viruses and other harmful pathogens, and ensure health and safety, health and labor authorities have repeatedly warned of the need to adhere to proper disinfection protocols in the workplace. This is particularly important in public places where food is handled, where there are more vulnerable people, including hospitals and health care centers, or where there is a large concentration of people. One promising approach is the combination of ultraviolet-C (UV-C) light and mobile robots to automate disinfection processes. Being this technology effective for disinfection, an excessive dose of UV can damage the materials, limiting its applicability. Therefore, a major challenge for automatic disinfection is to find a route that covers the entire surface, ensures cleanliness, and provides the correct radiation dose while preventing environmental materials from being damaged. To achieve this, in this paper a novel intelligent control approach is proposed. A bio-inspired Glasius neural network with a motion planner, an UV estimation module, a speed regulator, and pure pursuit controller are combined into one intelligent system. The motion planner proposes a sequence of movements to go through the space in the most efficient way possible, avoiding obstacles of the environment. The speed controller adjusts the dose of UV-C radiation and the pure pursuit regulator ensures the following of the path. This approach has been tested in various simulation scenarios of increasing complexity and in four different areas of dosing requirements. In simulation, a 44% reduction of the maximum dose is achieved, 17% less distance travelled by the robot and, what is more important, 229% more locations with the appropriate dose.},
  archive      = {J_EXSY},
  author       = {D. V. Rodrigo and J. E. Sierra-Garc√≠a and M. Santos},
  doi          = {10.1111/exsy.13455},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13455},
  shortjournal = {Expert Syst.},
  title        = {GBNN algorithm enhanced by movement planner for UV-C disinfection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional neural networks applied to data organized as
OLAP cubes. <em>EXSY</em>, <em>40</em>(10), e13454. (<a
href="https://doi.org/10.1111/exsy.13454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Convolutional Neural Network (CNN) architecture named OlapNet , which incorporates implicit operations of OLAP cubes (or data cubes). OLAP cubes are produced from database tables or spreadsheets and they allow particular operations that support performing complex queries efficiently. OlapNet permits evaluating various combinations of these OLAP operations in its search space and thus, it enables, in part, to automate the data transformation step in the knowledge discovery process. A sample of data from an actual database containing anonymized data on the debt history of customers of a financial institution has been used to evaluate our proposal. A predictive classification problem to estimate the probability of any given customer contracting new credits in the next three months has been modelled from these data. Then, traditional methods of Machine Learning and CNN were applied. The results showed that CNN, using the OlapNet architecture, outperforms traditional methods in almost all cases, indicating that the proposed architecture is quite promising.},
  archive      = {J_EXSY},
  author       = {Rodrigo Ribeiro Caputo and Edimilson Batista dos Santos and Leonardo Chaves Dutra da Rocha},
  doi          = {10.1111/exsy.13454},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13454},
  shortjournal = {Expert Syst.},
  title        = {Convolutional neural networks applied to data organized as OLAP cubes},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting community and structural hole spanner for
influence maximization in social networks. <em>EXSY</em>,
<em>40</em>(10), e13451. (<a
href="https://doi.org/10.1111/exsy.13451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viral marketing is a frequently used social marketing strategy which aims to promote products on various social media. In order to devise an efficient viral marketing strategy, influence maximization problem is widely studied in social networks. Influence maximization problem tries to find a set of influential users who could influence social users maximally. However, existed influence maximization algorithms could not satisfy the needs of social marketers well. In social networks, community structure is an important feature where social users are closely connected together in groups, and structural hole spanners are those users who bridge different communities. In order to balance the effectiveness and efficiency of viral marketing in social networks, in this paper, we present to exploit the community structure and structural hole spanners in social networks for solving the influence maximization problem. Different to traditional algorithms, we devise a strategy to approximate social influences of social users to find seeds efficiently. In particular, for an internal user inside a community in social networks, we utilize his social influence to members inside this community to approximate his social influence in the whole social network. For a structural hole spanner who bridges multiple communities, we utilize his social influence to members in these communities to approximate his social influence in the whole network. Based on the approximate social influence, information diffusion model and greedy framework, we propose a Community and Structural Hole Spanner based Greedy (CSHS-G) algorithm which devises several influence lists to store social influences of users and fast computation scheme to find seeds. Besides, to further improve the efficiency, we propose a Community and Structural Hole Spanner based Heuristic (CSHS-H) algorithm which approximates social influences of structural hole spanners by utilizing their 2-hops influences. We conduct a comprehensive performance evaluation on the crawled real-world data set. Experimental results show that, compared to all of the baseline algorithms, our proposed algorithms not only have high efficiency, but also could guarantee larger influence spread.},
  archive      = {J_EXSY},
  author       = {Xiao Li and Ziang Chen},
  doi          = {10.1111/exsy.13451},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13451},
  shortjournal = {Expert Syst.},
  title        = {Exploiting community and structural hole spanner for influence maximization in social networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Study and development of hybrid and ensemble forecasting
models for air quality index forecasting. <em>EXSY</em>,
<em>40</em>(10), e13449. (<a
href="https://doi.org/10.1111/exsy.13449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a viable, robust, and highly accurate additive hybrid model employing autoregressive fractionally integrated moving average (ARFIMA) and support vector machine (SVM) with functionally expanded inputs (Additive-ARFIMA-SVM) is presented for forecasting the air quality index (AQI). Additionally, thirteen additive and multiplicative hybrid models are introduced. Several alternatives in feature engineering employing functional expansion of inputs are incorporated to boost the performance of hybrid models. Furthermore, a gradient whale optimization algorithm with group best leader strategy (GWOA-GBL) based meta-heuristic algorithm is proposed. The missing values are imputed and a variable weight ensemble forecasting model is developed using the proposed GWOA-GBL algorithm. To evaluate the effectiveness of the proposed Additive-ARFIMA-SVM forecasting model with functionally expanded inputs, comparisons are made with sixteen machine learning models, including long short-term memory (LSTM), five statistical models, seventeen hybrid models, and ten variable weight ensemble models. Extensive statistical analyses are carried out on the obtained results considering four accuracy measures that show the statistical supremacy of the proposed Additive-ARFIMA-SVM model and GWOA-GBL algorithm in predicting the AQI time series. The proposed Additive-ARFIMA-SVM model with functionally expanded inputs improves the AQI forecasting performance by 16.34% than autoregressive integrated moving average, 14.47% than ARFIMA, 33.96% than XGBoost, 43.47% than SVM, 49.39% than LSTM, 8.64% than Multiplicative-ARIMA-SVM model considering symmetric mean absolute percentage error. The proposed Additive-ARFIMA-SVM model is so efficient and reliable that it can be applied to forecast other time series like stock price, electricity load, crude oil price, sunspot number, stream flow, flood, drought etc.},
  archive      = {J_EXSY},
  author       = {Sushree Subhaprada Pradhan and Sibarama Panigrahi and Sourav Kumar Purohit and Jatindra Kumar Dash},
  doi          = {10.1111/exsy.13449},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13449},
  shortjournal = {Expert Syst.},
  title        = {Study and development of hybrid and ensemble forecasting models for air quality index forecasting},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Piece-wise constant cluster modelling of dynamics of
upwelling patterns. <em>EXSY</em>, <em>40</em>(10), e13446. (<a
href="https://doi.org/10.1111/exsy.13446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A comprehensive approach is presented to analyse season&#39;s coastal upwelling represented by weekly sea surface temperature (SST) image grids. Our three-stage data recovery clustering method assumes that the season&#39;s upwelling can be divided into shorter periods of stability, ranges, each to be represented by a constant core and variable shell parts. Corresponding clustering algorithms parameters are automatically derived by using the least-squares clustering criterion. The approach has been successfully applied to real-world SST data covering two distinct regions: Portuguese coast and Morocco coast, for 16‚Äâyears each.},
  archive      = {J_EXSY},
  author       = {Susana Nascimento and Alexandre Martins and Paulo Relvas and Joaquim F. Lu√≠s and Boris Mirkin},
  doi          = {10.1111/exsy.13446},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13446},
  shortjournal = {Expert Syst.},
  title        = {Piece-wise constant cluster modelling of dynamics of upwelling patterns},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MapIntel: A visual analytics platform for competitive
intelligence. <em>EXSY</em>, <em>40</em>(10), e13445. (<a
href="https://doi.org/10.1111/exsy.13445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competitive Intelligence allows an organization to keep up with market trends and foresee business opportunities. This practice is mainly performed by analysts scanning for any piece of valuable information in a myriad of dispersed and unstructured sources. Here we present MapIntel, a system for acquiring intelligence from vast collections of text data by representing each document as a multidimensional vector that captures its own semantics. The system is designed to handle complex Natural Language queries and visual exploration of the corpus, potentially aiding overburdened analysts in finding meaningful insights to help decision-making. The system searching module uses a retriever and re-ranker engine that first finds the closest neighbours to the query embedding and then sifts the results through a cross-encoder model that identifies the most relevant documents. The browsing or visualization module also leverages the embeddings by projecting them onto two dimensions while preserving the multidimensional landscape, resulting in a map where semantically related documents form topical clusters which we capture using topic modelling. This map aims at promoting a fast overview of the corpus while allowing a more detailed exploration and interactive information encountering process. We evaluate the system and its components on the 20 newsgroups data set, using the semantic document labels provided, and demonstrate the superiority of Transformer-based components. Finally, we present a prototype of the system in Python and show how some of its features can be used to acquire intelligence from a news article corpus we collected during a period of 8‚Äâmonths.},
  archive      = {J_EXSY},
  author       = {David Silva and Fernando Ba√ß√£o},
  doi          = {10.1111/exsy.13445},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13445},
  shortjournal = {Expert Syst.},
  title        = {MapIntel: A visual analytics platform for competitive intelligence},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A military reconnaissance network for small-scale open-scene
camouflaged people detection. <em>EXSY</em>, <em>40</em>(10), e13444.
(<a href="https://doi.org/10.1111/exsy.13444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the work of identifying animal and plant objects with highly similar patterns (e.g., texture, intensity, colour, etc.) to the background has recently attracted more research interest but rarely involves the military complex environment. Design an efficient camouflage small-scale object detection algorithm that is capable of quickly discriminating the objects in open scenes from a long distance, to pre-empt the enemy. In this work, we first recognize the fact that existing open-source training datasets are scarce, and we have created a specific disguised people benchmark covering multiple scenes and weather conditions. Second, because the severe corruption of camouflage capabilities and chaotic scenes in the open battlefield on detailed features and generalization intensifies the challenge of feature extraction from a long-range perspective, we propose a novel end-to-end Small-scale open scene Camouflage Object Detection Network, called SM-CODN. Inspired by the characteristics of biological brain partition, a multi-domain partition module (MPM) with domain-decoupling is proposed to enable specific knowledge learning for samples with obvious discrepancies in camouflage domain distribution. Concurrent with our work, we have designed a multi-scale fusion module (MFM) to strengthen the semantic features related to small-scale disguised objects. Moreover, due to the convergence direction of the detector in reasoning being inconsistent, a feature separation enhancement module (FSEM) is also proposed. Experimental results show that SM-CODN surpasses many classic object detection methods and shows strong competitiveness compared with state-of-the-art ones.},
  archive      = {J_EXSY},
  author       = {Maozhen Liu},
  doi          = {10.1111/exsy.13444},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13444},
  shortjournal = {Expert Syst.},
  title        = {A military reconnaissance network for small-scale open-scene camouflaged people detection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Overcoming traditional ETL systems architectural problems
using a service-oriented approach. <em>EXSY</em>, <em>40</em>(10),
e13442. (<a href="https://doi.org/10.1111/exsy.13442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing analytical systems imposes several challenges related not only to the amount and heterogeneity of the involved data but also to the constant need to readapt and evolve to overcome new business challenges. Data are a determinant factor in the success of analytical and decision-making applications, being its nature, availability, and quality, crucial aspects for planning and structuring populating analytical systems. Today&#39;s users are more demanding, requiring adaptable and flexible analytical applications, which impose serious challenges on extract-transform-load (ETL) systems design and development for ensuring flexible and robust data populating services, operating 24/7, and managing and processing large volumes of data. Thus, we should design and implement ETL processes using innovative and up-to-date approaches, having real application evidence. In this paper, we present a service-oriented implementation for ETL design and development. We mapped and implemented some of the most conventional ETL processes in a service-oriented architecture, to demonstrate the application and benefits that this kind of approach will provide to ETL systems project development.},
  archive      = {J_EXSY},
  author       = {Bruno Oliveira and √ìscar Oliveira and Orlando Belo},
  doi          = {10.1111/exsy.13442},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13442},
  shortjournal = {Expert Syst.},
  title        = {Overcoming traditional ETL systems architectural problems using a service-oriented approach},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An empirical study of learning-to-rank for spare parts
consumption in the repair process. <em>EXSY</em>, <em>40</em>(10),
e13441. (<a href="https://doi.org/10.1111/exsy.13441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The repair process of devices is an important part of the business of many original equipment manufacturers. The consumption of spare parts, during the repair process, is driven by the defects found during inspection of the devices, and these parts are a big part of the costs in the repair process. In previous work, we proposed a data-driven method for Supply Chain Control Tower solutions to provide support for the automatic check of spare parts consumption in the repair process. In this article, we continue our investigation of a multi-label classification problem and explore alternatives in the learning-to-rank approach, where we simulate the passage of time using more data while training and comparing hundreds of Machine Learning models to provide an automatic check in the consumption of spare parts. We investigate the effects of different train set sizes, retraining intervals, models and hyper-parameter search using Bayesian Optimization. We define a custom metric, the Ratio of Marked Parts, measuring how many spare parts are marked for revision at the end of the repair process. The results show that we were able to improve the trained models and achieve a higher mean NDCG@20 score of 86% when ranking the expected parts. While focusing on the most recent data, we achieved a NDCG@20 score of 90% and obtained a Ratio of Marked Parts of just 4% of the consumed parts for use in alert generation.},
  archive      = {J_EXSY},
  author       = {Edson Duarte and Daniel de Haro Moraes and Lucas Leonardo Padula},
  doi          = {10.1111/exsy.13441},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13441},
  shortjournal = {Expert Syst.},
  title        = {An empirical study of learning-to-rank for spare parts consumption in the repair process},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MCDIP-ADMM: Overcoming overfitting in DIP-based CT
reconstruction. <em>EXSY</em>, <em>40</em>(10), e13440. (<a
href="https://doi.org/10.1111/exsy.13440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the application of unsupervised learning methods for computed tomography reconstruction. To motivate our work, we review several existing priors, namely the truncated Gaussian prior, the l 1 prior, the total variation prior, and the deep image prior (DIP). We find that DIP outperforms the other three priors in terms of representational capability and visual performance. However, the performance of DIP deteriorates when the number of iterations exceeds a certain threshold due to overfitting. To address this issue, we propose a novel method (MCDIP-ADMM) based on multi-code deep image prior (MCDIP) and plug-and-play alternative direction method of multipliers (ADMM). Specifically, MCDIP utilizes multiple latent codes to generate a series of feature maps at an intermediate layer within a generator model. These maps are then composed with trainable weights, representing the complete image prior. Experimental results demonstrate the superior performance of the proposed MCDIP-ADMM compared to three existing competitors. In the case of parallel beam projection with Gaussian noise, MCDIP-ADMM achieves an average improvement of 4.3‚ÄâdB over DIP, 1.7‚ÄâdB over ADMM DIP-weighted total variation (WTV) and 1.2‚ÄâdB over PnP-DIP in terms of peak-signal-to-noise ratio (PSNR). Similarly, for fan-beam projection with Poisson noise, MCDIP-ADMM achieves an average improvement of 3.09‚ÄâdB over DIP, 1.86‚ÄâdB over ADMM DIP-WTV and 0.84‚ÄâdB over PnP-DIP in terms of PSNR.},
  archive      = {J_EXSY},
  author       = {Chen Cheng and Qingping Zhou},
  doi          = {10.1111/exsy.13440},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13440},
  shortjournal = {Expert Syst.},
  title        = {MCDIP-ADMM: Overcoming overfitting in DIP-based CT reconstruction},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto encoder with mode-based learning for keyframe
extraction in video summarization. <em>EXSY</em>, <em>40</em>(10),
e13437. (<a href="https://doi.org/10.1111/exsy.13437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential increase in video consumption has created new difficulties for browsing and navigating through video more effectively and efficiently. Researchers are interested in video summarization because it offers a brief but instructive video version that helps users and systems save time and effort when looking for and comprehending relevant content. Key frame extraction is a method of video summarization that only chooses the most important frames from a given video. In this article, a novel supervised learning method ‚ÄòTC-CLSTM Auto Encoder with Mode-based Learning‚Äô using temporal and spatial features is proposed for automatically choosing keyframes or important sub-shots from videos. The method was able to achieve an average F-score of 84.35 on TVSum dataset. Extensive tests on benchmark data sets show that the suggested methodology outperforms state-of-the-art methods.},
  archive      = {J_EXSY},
  author       = {Prashant Giridhar Shambharkar and Ruchi Goel},
  doi          = {10.1111/exsy.13437},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13437},
  shortjournal = {Expert Syst.},
  title        = {Auto encoder with mode-based learning for keyframe extraction in video summarization},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepCens: A deep learning-based system for real-time image
and video censorship. <em>EXSY</em>, <em>40</em>(10), e13436. (<a
href="https://doi.org/10.1111/exsy.13436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of social networks and video-on-demand platforms has increased the importance of image and video censorship. These platforms contain various content such as violence, explicit content, drug use and smoking that may be offensive or harmful to certain viewers. As a result, censorship is employed to filter or remove content that is unsuitable for particular audiences such as children and teenagers. However, policies for censoring harmful content in digital environments are either limited or nonexistent. This underscores the need for automated systems that can detect and censor harmful content in real time. To address these challenges, we developed the first of our knowledge systems using deep learning techniques to censor harmful content. We propose two novel, YOLO-based real-time censorship algorithms. Our approaches employ a pipeline-based architecture that parallelizes the operations with subprocesses. In our experiments, the proposed algorithms performed faster and with higher accuracies compared to traditional approaches. Specifically, its content-based accuracies were 98% for explicit content, 97% for alcohol, 98% for cigarettes and 97% for violence. Our research highlights the importance of developing effective and efficient solutions for censoring harmful content on digital media platforms. Our deep learning-based system represents a promising approach to this challenge and has the potential to enhance user safety and protect vulnerable groups from harmful and offensive content. Future research will continue to refine and improve such systems to better address the evolving landscape of digital media and the challenges posed by harmful content.},
  archive      = {J_EXSY},
  author       = {Asim Sinan Yuksel and Fatma Gulsah Tan},
  doi          = {10.1111/exsy.13436},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13436},
  shortjournal = {Expert Syst.},
  title        = {DeepCens: A deep learning-based system for real-time image and video censorship},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M2CE: Multi-convolutional neural network ensemble approach
for improved multiclass classification of skin lesion. <em>EXSY</em>,
<em>40</em>(10), e13435. (<a
href="https://doi.org/10.1111/exsy.13435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to inter-class homogeneity and intra-class variability, the classification of skin lesions in dermoscopy images has remained difficult. Although deep convolutional neural networks (DCNNs) have achieved satisfactory performance for binary skin cancer classification, multiclass skin lesion classification is still an open problem due to the limited training samples and class imbalance issues. To tackle these issues, in this article, we propose a multi-CNN ensemble approach dubbed for multiclass skin lesion classification. The includes three individual CNN models, each helping in extracting different high-level features from skin images and thereby yielding different prediction results. First, we design a lightweight CNN model to extract prominent features and train it from scratch, which primarily aims at avoiding the data scarcity problem. Then, we ensemble two different pre-trained CNN models with the lightweight model to improve the performance and generalization capability. The proposed ensemble approach can effectively fuse the predictions of each individual CNN model using the averaging method. The approach is validated using a benchmark data set, HAM10000, which contains skin lesion images of seven different classes. The results demonstrate that the outperforms base CNN models and state-of-the-art approaches without using any external data.},
  archive      = {J_EXSY},
  author       = {Himanshu K. Gajera and Deepak Ranjan Nayak and Mukesh A. Zaveri},
  doi          = {10.1111/exsy.13435},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13435},
  shortjournal = {Expert Syst.},
  title        = {M2CE: Multi-convolutional neural network ensemble approach for improved multiclass classification of skin lesion},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PooRaa-agri KG: An agricultural knowledge graph-based
simplified multilingual query system. <em>EXSY</em>, <em>40</em>(10),
e13434. (<a href="https://doi.org/10.1111/exsy.13434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current work proposes PooRaa-Agri KG, an agricultural knowledge graph-based simplified multilingual query system that works in real time to provide concise answers for agriculture-based queries. The proposed approach accommodates real-time and low-resource queries in English and Hindi with a novel multi-stage solution consisting of data pre-processing, sentence simplification, triplet extraction, knowledge graph generation, sentence reconstruction, query-to-reconstructed sentence matching, and machine translation as its sub-modules. In this work, a novel combination of rule-based sentence simplification and triplet extraction is carried out resulting in a triplet similarity score of 86.56% for the extracted triplets. This method is superior to the existing triplet extraction method whose triplet similarity score was found to be 60.65%. Further, the proposed work makes use of heuristic rules to reconstruct sentences which when evaluated by human evaluators for meaningfulness and grammar resulted in a score of 3.09/4 and 2.95/4 respectively. To complete end-to-end communication in the proposed system, a similarity-based query answer system is proposed in this work.},
  archive      = {J_EXSY},
  author       = {Nethraa Sivakumar and Pooja Srinivasan and Mrinalini Kannan and Vijayalakshmi P and Nagarajan T},
  doi          = {10.1111/exsy.13434},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13434},
  shortjournal = {Expert Syst.},
  title        = {PooRaa-agri KG: An agricultural knowledge graph-based simplified multilingual query system},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective overlapping community merging method oriented
to multidimensional attribute social networks. <em>EXSY</em>,
<em>40</em>(10), e13433. (<a
href="https://doi.org/10.1111/exsy.13433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multidimensional attribute social networks, most of the existing methods have limitations in dealing with community redundancy, namely, the influences of the network&#39;s multidimensional attributes and users&#39; interest preferences are ignored in the process of community merging. To this end, we propose an effective overlapping community merging method (EOCMM) oriented to multidimensional attribute social networks. In EOCMM, we focus on two core problems which are how to improve the method of improving overlapping community detection in multidimensional attribute social networks and how to merge the detected communities with high redundancy degree. To solve the first problem, based on the network topology characteristics and user interest preference, the Node Domain index is proposed to improve and optimize the selection of seed nodes. The traditional overlapping community detection method is improved by semantic fitness function and seed nodes. To solve the second problem, we merge the communities with high redundancy degree by Semantic Community Overlapping Degree which is fused by user similarity, community similarity and community tightness. Finally, we compared our method with other multiple mainstream methods by extensive experiments on three real social network datasets. The experimental comparison results show that the improvements on three evaluation metrics extension of modularity, partition density and semantic extension of modularity are 6.04%, 7.96% and 3.13%, respectively. And our method can make the results of community detection and merging more reasonable and effective.},
  archive      = {J_EXSY},
  author       = {Shulin Cheng and Shan Yang and Xiufang Cheng and Wanyan Wang},
  doi          = {10.1111/exsy.13433},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13433},
  shortjournal = {Expert Syst.},
  title        = {An effective overlapping community merging method oriented to multidimensional attribute social networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Membership-based aircraft parking stand allocation system
with time window constraints: An event-based time‚Äìspace separated
algorithm. <em>EXSY</em>, <em>40</em>(10), e13431. (<a
href="https://doi.org/10.1111/exsy.13431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outsourcing maintenance service providers are vital to guarantee safe operation in airline industry. To reduce the workload and avoid incompatible arrangement schemes in traditional manual arrangement, this article constructs an intelligent system with a novelly designed model and algorithm for membership-based aircraft parking stand allocation problem. This problem arises from outsourcing maintenance service providers. They need to first serve membership orders, while guaranteeing punctual delivery of other orders. In particular, mutual collision should be strictly avoided between aircrafts. To solve this problem, first, a mathematical model is constructed to optimize timetable and aircraft parking stand allocation scheme. Second, to quickly obtain feasible scheduling scheme, three kinds of mechanisms, including information guidance mechanism, boundary arrangement mechanisms and local optimal adjustment mechanism, are novelly proposed. Moreover, event-based time‚Äìspace separated heuristic algorithm is subtly designed based on time‚Äìspace separation characteristics. In addition, coding schemes are proposed through problem analysis. Finally, three cases with different scales are utilized and six comparison algorithms are selected to illustrate the superiority of our proposed algorithm.},
  archive      = {J_EXSY},
  author       = {Tianwei Zhou and Churong Zhang and Xizhang Yao and Ben Niu},
  doi          = {10.1111/exsy.13431},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13431},
  shortjournal = {Expert Syst.},
  title        = {Membership-based aircraft parking stand allocation system with time window constraints: An event-based time‚Äìspace separated algorithm},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of single-cell cervical pap smear images
using EfficientNet. <em>EXSY</em>, <em>40</em>(10), e13418. (<a
href="https://doi.org/10.1111/exsy.13418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer is the second leading cause of death in women in developing nations like India. The automated cervical cancer screening systems play a critical role in diagnosing the ailment at an early stage. They can also assist in routine cervical cancer screening and decision-making processes. The underlying binary classifier for detecting cancerous cells is necessary for building computerized screening systems. Thus, this work aims to develop a robust binary classifier capable of classifying single cervical cells as normal and cancerous. The current work aims to improvise the EfficientNet-B7 with empirical resolution parameters and skilful consolidation of the outcomes of the inner layers using the global pooling layer to improve the binary classification result. The proposed model is trained and evaluated on three independent pap smear datasets. The class activation heatmap visualizes the classification model for better interpretability. The performance of the autonomous classification tasks indicates that the deep learning-based binary classifier is robust in classifying cancerous cells as it achieves accuracy of 94% on Herlev Dataset which is 2% higher than the other state-of-the-art methods and comparable accuracy of nearly 99% on other cervical cell datasets using the proposed methodology.},
  archive      = {J_EXSY},
  author       = {Priyanka Rastogi and Kavita Khanna and Vijendra Singh},
  doi          = {10.1111/exsy.13418},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13418},
  shortjournal = {Expert Syst.},
  title        = {Classification of single-cell cervical pap smear images using EfficientNet},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-output regression for imbalanced data stream.
<em>EXSY</em>, <em>40</em>(10), e13417. (<a
href="https://doi.org/10.1111/exsy.13417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we describe an imbalanced regression method for making predictions over imbalanced data streams. We present MORSTS (Multiple Output Regression for Streaming Time Series), an online ensemble regressors devoted to non-stationary and imbalanced data streams. MORSTS relies on several multiple output regressor submodels, adopts a cost sensitive weighting technique for dealing with imbalanced datasets, and handles overfitting by means of the K-fold cross validation. For assessment purposes, experiments have been conducted on known real datasets and compared with known base regression techniques.},
  archive      = {J_EXSY},
  author       = {Tao Peng and Sana Sellami and Omar Boucelma and Richard Chbeir},
  doi          = {10.1111/exsy.13417},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13417},
  shortjournal = {Expert Syst.},
  title        = {Multi-output regression for imbalanced data stream},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comprehensive analysis of UK AADF traffic dataset set within
four geographical regions of england. <em>EXSY</em>, <em>40</em>(10),
e13415. (<a href="https://doi.org/10.1111/exsy.13415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic flow detection plays a significant part in freeway traffic surveillance systems. Currently, effective autonomous traffic analysis is a challenging task due to the complexity of traffic delays, despite the significant investment spent by authorities in monitoring and analysing traffic congestion. This study builds an intelligent analytic method based on machine-learning algorithms to investigate and predict road traffic flows in four locations in the United Kingdom (London, Yorkshire and the Humber, North East, and North West) with a range of relevant factors. While aiming to conduct the study, the dataset ‚Äòestimated annual average daily flows (AADFs) Data‚Äîmajor and minor roads‚Äô from the UK government was used. Machine-learning algorithms are used for this research and classification applied consists of Logistic Regression, Decision Trees, Random Forests, K-Nearest Neighbors, and Gradient Boosting. Each of these algorithms achieves an accuracy of over 93% and the F1 score of over 95%, with Random Forest outperforming the other algorithms. This analytical approach helps to focus attention on critical areas to reduce traffic flows on major and minor roads in the area. In summary, the findings on traffic analysis have been discussed in detail to demonstrate the practical insights of this study.},
  archive      = {J_EXSY},
  author       = {Victor Chang and Qianwen Ariel Xu and Karl Hall and Olojede Theophilus Oluwaseyi and Jiabin Luo},
  doi          = {10.1111/exsy.13415},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13415},
  shortjournal = {Expert Syst.},
  title        = {Comprehensive analysis of UK AADF traffic dataset set within four geographical regions of england},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of machine learning and econometric models for
pricing perpetual bitcoin futures and their application to algorithmic
trading. <em>EXSY</em>, <em>40</em>(10), e13414. (<a
href="https://doi.org/10.1111/exsy.13414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bitcoin (BTC) perpetual futures contracts are highly leveraged speculative trading instruments with daily market trading of $45 Billion. BTC perpetual futures are derivative contracts, which depend upon the underlying BTC SPOT (current) price. Pricing perpetual futures fairly is hard, using traditional arbitrage arguments, because of the volatile nature of the so called funding rate, which is used as the replacement of risk free rate in the Cryptocurrency market. This work presents a novel technique for pricing BTC futures contracts using conditional volatility and mean models. Intra-day high-frequency futures&#39; return volatility and mean are modelled using different ML and econometric techniques. A comparison is made using statistical measures to find the model that best captures the intra-day conditional mean and volatility. Exponential generalized autoregressive conditional heteroskedasticity is shown to be an almost unbiased predictor of intra-day volatility, while a constant autoregressive moving average (0, 0) model best captures the conditional mean of the returns. A market directional high frequency trading algorithm is developed using the volatility and mean models. The algorithm first prices the futures contract at some future point of time using the volatility and mean regression models. Next, the slope between the current futures price and the expected price are used to predict the market direction. A long or short position is taken depending upon the expected market direction movement. Extensive back-testing results show absolute returns of 1500%‚Äì8000% depending upon the transaction fees and leverage used. On average, the market direction is predicted correctly 85% of the time by the best model. Finally, the trading technique is market neutral, in that it gives large positive returns, with low SD, in both bull and bear markets.},
  archive      = {J_EXSY},
  author       = {Avinash Malik},
  doi          = {10.1111/exsy.13414},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13414},
  shortjournal = {Expert Syst.},
  title        = {A comparison of machine learning and econometric models for pricing perpetual bitcoin futures and their application to algorithmic trading},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning-based classification of multiple heart
disorders from PCG signals. <em>EXSY</em>, <em>40</em>(10), e13411. (<a
href="https://doi.org/10.1111/exsy.13411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Timely and accurate detection and diagnosis of heart disorders is a significant problem in the medical community since the mortality rate is increasing. Pulsing of cardiac structures and blood turbulence creates heart sounds recorded and detected through Phonococardiogram (PCG). As a non-invasive technique, PCG signals have a strong ability to be used for designing automatic classification of possible heart disorders. This paper presents an expert system design for the detection and classification of PCG signals for five classes, namely, healthy, aortic stenosis, mitral stenosis, mitral regurgitation, and mitral valve prolapse. In this work, a single-channel PCG signal is first decomposed using Empirical Mode Decomposition (EMD) into different modes known as intrinsic mode functions (IMFs). Manual signal analysis is applied to identify the relevant IMFs to construct a preprocessed signal. We proposed an automated energy-based signal reconstruction through IMFs. The proposed algorithms automatically identify the relevant IMFs and added them together to form a preprocessed signal. After preprocessing, the first nine features of Mel Frequency Cepstral Coefficients (MFCC) were computed and passed to several classification methods such as Fine Tree, Quadratic Discriminant, Kernel Naive Bayes, Support Vector Machines (SVM), Fine K-Nearest Neighbours (Fine-KNN), Ensemble Bagged Trees and Neural Network. The best performance of 99.3% accuracy was obtained via Fine-KNN using 10-fold cross-validation. The proposed method was evaluated on a publicly available dataset of heart sounds. The proposed method demonstrated improved performance as compared to the existing state-of-the-art methods.},
  archive      = {J_EXSY},
  author       = {Muhammad Talal and Sumair Aziz and Muhammad Umar Khan and Yazeed Ghadi and Syed Zohaib Hassan Naqvi and Muhammad Faraz},
  doi          = {10.1111/exsy.13411},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13411},
  shortjournal = {Expert Syst.},
  title        = {Machine learning-based classification of multiple heart disorders from PCG signals},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An enhanced bacterial colony optimization with dynamic
multi-leader co-evolution for multiobjective optimization problems.
<em>EXSY</em>, <em>40</em>(10), e13410. (<a
href="https://doi.org/10.1111/exsy.13410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information transfer mechanism within the population is an essential factor for population-based multiobjective optimization algorithms. An efficient leader selection strategy can effectively help the population to approach the true Pareto front. However, traditional population-based multiobjective optimization algorithms are restricted to a single global leader and cannot transfer information efficiently. To overcome those limitations, in this paper, a multiobjective bacterial colony optimization with dynamic multi-leader co-evolution (MBCO/DML) is proposed, and a novel information transfer mechanism is developed within the group for adaptive evolution. Specifically, to enhance convergence and diversity, a multi-leaders learning mechanism is designed based on a dynamically evolving elite archive via direction-based hierarchical clustering. Finally, adaptive bacterial elimination is proposed to enable bacteria to escape from the local Pareto front according to convergence status. The results of numerical experiments show the superiority of the proposed algorithm in comparison with related population-based multiobjective optimization algorithms on 24 frequently used benchmarks. This paper demonstrates the effectiveness of our dynamic leader selection in information transfer for improving both convergence and diversity to solve multiobjective optimization problems, which plays a significant role in information transfer of population evolution. Furthermore, we confirm the validity of the co-evolution framework to the bacterial-based optimization algorithm, greatly enhancing the searching capability for bacterial colony.},
  archive      = {J_EXSY},
  author       = {Hong Wang and Yixin Wang and Menglong Liu and Tianwei Zhou and Ben Niu},
  doi          = {10.1111/exsy.13410},
  journal      = {Expert Systems},
  month        = {12},
  number       = {10},
  pages        = {e13410},
  shortjournal = {Expert Syst.},
  title        = {An enhanced bacterial colony optimization with dynamic multi-leader co-evolution for multiobjective optimization problems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrated recovery system with bidding-based satisfaction:
An adaptive multi-objective approach. <em>EXSY</em>, <em>40</em>(9),
e13409. (<a href="https://doi.org/10.1111/exsy.13409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient management of aircraft and crew recovery system is crucial for cost savings and improving the satisfaction, which are related to the airline&#39;s reputation. However, most existing work considers only one objective of minimizing costs or maximizing satisfaction. In this study, we propose a new integrated multi-objective recovery system that takes both cost and satisfaction into account simultaneously. To better capture crew satisfaction in the event of airport closure, a bidding mechanism for early off-duty task is designed. To overcome the experience-dependent and labour-consuming problems associated with current manual or mathematical recoveries, we develop an intelligent optimizer based on multi-swarm and MOPSO frameworks, termed adaptive seeking and tracking multi-objective particle swarm optimization algorithm (ASTMOPSO). Specifically, during the evolutionary process, the sub-swarm size undergoes adaptive internal transfer while executing more efficient evolutionary strategies to approach the global Pareto front. Additionally, five ad-hoc repair procedures are designed to ensure feasibility for our aircraft and crew recovery system. The ASTMOPSO is applied to real-world instances from Shenzhen Airlines with different sizes. Experimental results demonstrate the statistical superiority of our method over other popular peer algorithms. And the infeasible solution repair procedures significantly improve the feasibility rate by at least 40%, particularly for large-scale instances.},
  archive      = {J_EXSY},
  author       = {Huifen Zhong and Zhaotong Lian and Tianwei Zhou and Ben Niu and Bowen Xue},
  doi          = {10.1111/exsy.13409},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13409},
  shortjournal = {Expert Syst.},
  title        = {Integrated recovery system with bidding-based satisfaction: An adaptive multi-objective approach},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting of taiwan‚Äôs weighted stock price index based on
machine learning. <em>EXSY</em>, <em>40</em>(9), e13408. (<a
href="https://doi.org/10.1111/exsy.13408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a stack framework of light gradient boosting machine (LGBM) for Taiwan stock market index prediction. Stock market predictions have been regarded as a challenging task, as the market is affected by several factors such as political events, general economic conditions, institutional investors&#39; choices, movement of the global market, psychology of investors. We construct a rich feature set to capture the impacts of global markets, institutional investors&#39; choices, and the psychology of investors. A feature selection algorithm is proposed to choose important feature subset and enhance the training performance. To further improve the prediction accuracy, we employ stacking strategy to combine multiple classifiers together. A 10-year period of the Taiwan stock exchange capitalization weighted stock index (TAIEX) is used to verify the performance of the proposed model. The experimental results suggest that our prediction model as well as the feature selection method can achieve good prediction performance.},
  archive      = {J_EXSY},
  author       = {I-Fang Su and Ping Lei Lin and Yu-Chi Chung and Chiang Lee},
  doi          = {10.1111/exsy.13408},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13408},
  shortjournal = {Expert Syst.},
  title        = {Forecasting of taiwan&#39;s weighted stock price index based on machine learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BERTuit: Understanding spanish language in twitter with
transformers. <em>EXSY</em>, <em>40</em>(9), e13404. (<a
href="https://doi.org/10.1111/exsy.13404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appearance of complex attention-based language models such as BERT, RoBERTa or GPT-3 has allowed to address highly complex tasks in a plethora of scenarios. However, when applied to specific domains, these models encounter considerable difficulties. This is the case of Social Networks such as Twitter, an ever-changing stream of information written with informal and complex language, where each message requires careful evaluation to be understood even by humans given the important role that context plays. Addressing tasks in this domain through Natural Language Processing involves severe challenges. When powerful state-of-the-art multilingual language models are applied to this scenario, language specific nuances get lost in translation. To face these challenges we present BERTuit, the largest transformer proposed so far for Spanish language, pre-trained on a massive dataset of 230‚ÄâM Spanish tweets using RoBERTa optimization. Our motivation is to provide a powerful resource to better understand Spanish Twitter and to be used on applications focused on this social network, with special emphasis on solutions devoted to tackle the spreading of misinformation in this platform. BERTuit is evaluated on several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very competitive multilingual transformers. The utility of our approach is shown with applications, in this case: an unsupervised methodology to visualize groups of hoaxes; and supervised profiling of authors spreading disinformation.},
  archive      = {J_EXSY},
  author       = {Javier Huertas-Tato and Alejandro Mart√≠n and David Camacho},
  doi          = {10.1111/exsy.13404},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13404},
  shortjournal = {Expert Syst.},
  title        = {BERTuit: Understanding spanish language in twitter with transformers},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concept drift detection and adaption framework using
optimized deep learning and adaptive sliding window approach.
<em>EXSY</em>, <em>40</em>(9), e13394. (<a
href="https://doi.org/10.1111/exsy.13394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift in online streaming data is a common issue due to dynamic smart systems, which results in system failure or performance degradation. Though there are several traditional approaches for handling the streaming data, they failed to handle the concept drift imposing the need for developing an adaptable approach to managing the dynamic IoT streaming data. Therefore, in this research, a new method is proposed for handling the concept drift issues in online data streaming. This research develops the dynamic streaming data analytic framework based on the optimized Deep CNN and optimized adaptive and sliding window (OASW) approach that effectively addresses both memory and time constraints. An optimized Deep CNN classifier is employed as a base classifier for offline learning, which is developed through hybridizing the proposed Desale&#39;s aggressive hunt optimization (AHO) algorithm with a Deep CNN classifier for tuning the optimal parameters of the classifier. An optimized adaptive and sliding window is utilized in this research to adapt the pattern changes in the data streams, which effectively handles the concept drift. The experimental analysis reveals that the proposed methods outperform the conventional methods considered for the analysis in terms of specificity, sensitivity, accuracy, F 1 score, and the precision score of 96.65%, 97.77%, 98.63%, 98.1487%, and 98.4469%, respectively.},
  archive      = {J_EXSY},
  author       = {Ketan Sanjay Desale and Swati V. Shinde},
  doi          = {10.1111/exsy.13394},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13394},
  shortjournal = {Expert Syst.},
  title        = {Concept drift detection and adaption framework using optimized deep learning and adaptive sliding window approach},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilingual bi-encoder models for biomedical entity
linking. <em>EXSY</em>, <em>40</em>(9), e13388. (<a
href="https://doi.org/10.1111/exsy.13388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) is a field of study that focuses on data analysis on texts with certain methods. NLP includes tasks such as sentiment analysis, spam detection, entity linking, and question answering, to name a few. Entity linking is an NLP task that is used to map mentions specified in the text to the entities of a Knowledge Base. In this study, we analysed the efficacy of bi-encoder entity linking models for multilingual biomedical texts. Using surface-based, approximate nearest neighbour search and embedding approaches during the candidate generation phase, accuracy, and recall values were measured on language representation models such as BERT, SapBERT, BioBERT, and RoBERTa according to language and domain. The proposed entity linking framework was analysed on the BC5CDR and Cantemist datasets for English and Spanish, respectively. The framework achieved 76.75% accuracy for the BC5CDR and 60.19% for the Cantemist. In addition, the proposed framework was compared with previous studies. The results highlight the challenges that come with domain-specific multilingual datasets.},
  archive      = {J_EXSY},
  author       = {Zekeriya Anil Guven and Andre Lamurias},
  doi          = {10.1111/exsy.13388},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13388},
  shortjournal = {Expert Syst.},
  title        = {Multilingual bi-encoder models for biomedical entity linking},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparison study of unsupervised paraphrase detection: Deep
learning‚Äîthe key for semantic similarity detection. <em>EXSY</em>,
<em>40</em>(9), e13386. (<a
href="https://doi.org/10.1111/exsy.13386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of concealed plagiarism in the form of paraphrases is a difficult task, and finding a successful unsupervised approach for paraphrase detection is necessary as a precondition to change that. This comparative study identified the most efficient methods for unsupervised paraphrased document detection using similarity measures alone or combined with Deep Learning (DL) models. It proved the hypothesis that some DL models are more successful than the best statistically-based methods in that task. Many experiments were carried out, and their results were compared. The text similarities between documents are obtained from 60 different methods using five paraphrase corpora, including the new one made by authors, as an important original contribution. Some DL models achieved significantly better results than those obtained by the best statistical methods, especially pre-trained transformer-based language models with average values of Accuracy and F1 of 85.8% and 88.3%, respectively, with top values of 99.9% and 98.4% for Accuracy and F1 on some corpora. These results are even better than those of supervised and combined approaches. Therefore, here presented results prove that detecting concealed plagiarism becomes an attainable goal. This study highlighted those language models with the best overall results for paraphrase detection as best suited for further research. The study also discussed the choice of similarity/distance measure paired with embeddings produced by DL models and some advantages of using cosine similarity as the fastest measure. For 60 different methods, complexity has been defined in O notation. Times needed for their implementation have also been presented. The article&#39;s results and conclusions are a firm base for future semantic similarity, paraphrasing, and plagiarism detection studies, clearly marking state-of-the-art tools and methods.},
  archive      = {J_EXSY},
  author       = {Tedo Vrbanec and Ana Me≈°troviƒá},
  doi          = {10.1111/exsy.13386},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13386},
  shortjournal = {Expert Syst.},
  title        = {Comparison study of unsupervised paraphrase detection: Deep learning‚ÄîThe key for semantic similarity detection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph attentive matrix factorization for social
recommendation. <em>EXSY</em>, <em>40</em>(9), e13385. (<a
href="https://doi.org/10.1111/exsy.13385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems commonly encounter with the problems of data sparsity and cold start. Recently, social recommendation has emerged with the rapid expansion of social platforms, offering an opportunity to alleviate such two obstacles. Nevertheless, there are still two key limitations in existing studies. From the perspective of model design, previous social recommenders only consider the influence of a user&#39;s direct friends or uniformly treat the influences from different friends. From the perspective of model learning, most of them apply a sampling-based optimization strategy, which requires high-quality positive and negative samples. In light of the aforementioned limitations, we propose a new probabilistic method, named Graph Attentive Matrix Factorization (GAMF). Our method not only explicitly captures high-order social relationships, but also adopts an attention mechanism to automatically pick up different influences between friends. Moreover, we develop an efficient optimization algorithm to learn model parameters in a non-sampling manner. Extensive experiments on four large-scale datasets show the superiority of GAMF over state-of-the-art recommenders, especially under the cold start scenario.},
  archive      = {J_EXSY},
  author       = {Xue Zhang and Bin Wu and Yangdong Ye},
  doi          = {10.1111/exsy.13385},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13385},
  shortjournal = {Expert Syst.},
  title        = {Graph attentive matrix factorization for social recommendation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive artificial electric field algorithm for
continuous optimization problems. <em>EXSY</em>, <em>40</em>(9), e13380.
(<a href="https://doi.org/10.1111/exsy.13380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The comprehensive learning strategy is a meticulous method for enhancing the optimization ability of population-based optimization algorithms. This article proposes an adaptive artificial electric field algorithm (iAEFA), which is developed by embedding a comprehensive learning strategy into AEFA. The proposed algorithm utilizes a novel adaptive approach for developing a better learning strategy in which an agent&#39;s velocity is updated using the comprehensive influence of the entire population. The developed scheme has shown a stronger potential to discover better candidate solutions in each iteration. The objective of the proposed method is to develop an efficient optimizer for continuous optimization problems. The performance of the proposed iAEFA is evaluated using a set of 13 classical benchmark test problems and the CEC 2019 (100-digit challenge) benchmark functions. The experimental results are compared to seven state-of-the-art optimization algorithms. Using the Wilcoxon signed-rank test, the statistical significance of the results is confirmed. This article also discusses the theoretical convergence of the proposed algorithm, along with other significant findings about the proposed scheme. The experimental results and the theoretical analysis shows that the proposed scheme can be an excellent choice for the function optimization task compared to other existing algorithms.},
  archive      = {J_EXSY},
  author       = {Dikshit Chauhan and Anupam Yadav},
  doi          = {10.1111/exsy.13380},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13380},
  shortjournal = {Expert Syst.},
  title        = {An adaptive artificial electric field algorithm for continuous optimization problems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EnLEFD-DM: Ensemble learning based ethereum fraud detection
using CRISP-DM framework. <em>EXSY</em>, <em>40</em>(9), e13379. (<a
href="https://doi.org/10.1111/exsy.13379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrencies continue to captivate businesses and investors despite market fluctuations. The number of crypto users has risen rapidly in the last few years, and alarmingly, many appear to be unaware of the risks involved. These risks aren&#39;t confined to market hazards but include very sophisticated cybercrimes related to cryptocurrencies. As cryptocurrencies have become a breeding ground for a variety of cybercrimes, resulting in enormous financial losses, it hinders user adoption limiting the utility of blockchain technology. It has become crucial to spot such scams and devise intelligent techniques to make this technology safer for investors. This paper proposes a classification model based on the Cross Industry Standard Process for Data Mining (CRISP-DM) framework to identify fraudulent transactions over the Ethereum blockchain. Its contribution is multi-faceted; first, the available imbalanced Ethereum dataset has been balanced to enhance the accuracy of the classification model. Second, the correlation-based feature selection technique has been applied to retain the best discriminating features. Thirdly, an effective machine learning-based ensemble classification model has been adopted for the identification of fraudulent transactions over the Ethereum network. A comparative analysis of 10 machine learning techniques has been presented consisting of both individual and ensemble classifiers. Evaluated outcomes show that ensemble classifiers appear to yield better performance measures over individual classifiers, and among all, the LightGbm classifier outperformed with 99.2% accuracy. Further, extensive experiments indicate that the proposed method outperforms the state-of-the-art method when applied to a similar dataset.},
  archive      = {J_EXSY},
  author       = {Lavina Pahuja and Ahmad Kamal},
  doi          = {10.1111/exsy.13379},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13379},
  shortjournal = {Expert Syst.},
  title        = {EnLEFD-DM: Ensemble learning based ethereum fraud detection using CRISP-DM framework},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A boosted co-training method for class-imbalanced learning.
<em>EXSY</em>, <em>40</em>(9), e13377. (<a
href="https://doi.org/10.1111/exsy.13377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance learning (CIL) has become one of the most challenging research topics. In this article, we propose a Boosted co-training method to modify the class distribution so that traditional classifiers can be readily adapted to imbalanced datasets. This article is among the first to utilize pseudo-labelled data of co-training to enlarge the training set of minority classes. Compared with existing oversampling methods which generate minority samples based on labelled data, the proposed method has the ability to learn from unlabelled data and then decrease the risk of overfitting. Furthermore, we propose a boosting-style technique which implicitly modifies the class distribution and combines it with co-training to alleviate the bias towards majority classes. Finally, we collect two series of classifiers generated during Boosted co-training to build an ensemble for the classification. It further improves the CIL performance by leveraging the strength of ensemble learning. By taking advantage of the diversity of co-training, we also contribute a new approach to generating base classifiers for ensemble learning. The proposed method is compared with eight state-of-the-art CIL methods on a variety of benchmark datasets. Measured by G-Mean, F-Measure, and AUC, Boosted co-training achieves the best performances and average ranks on 18 benchmark datasets. The experimental results demonstrate the significant superiority of Boosted co-training over other CIL methods.},
  archive      = {J_EXSY},
  author       = {Zhen Jiang and Lingyun Zhao and Yongzhao Zhan},
  doi          = {10.1111/exsy.13377},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13377},
  shortjournal = {Expert Syst.},
  title        = {A boosted co-training method for class-imbalanced learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hurst-based diffusion model using time series
characteristics for influence maximization in social networks.
<em>EXSY</em>, <em>40</em>(9), e13375. (<a
href="https://doi.org/10.1111/exsy.13375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social networks have grown exponentially in the recent years while finding applications in real life like marketing, recommendation systems, and social awareness campaigns. An important research area in this field is Influence Maximization, which pertains to finding methods for maximizing the spread of information (influence) across an OSN. Existing works in IM widely use a pre-defined edge propagation probability for node activation. Hurst exponent ( H ), which depicts the self-similarity in the time series depicting a user&#39;s past interaction behaviour, has also been used as activation criteria. In this work, we propose a Time Series Characteristic based Hurst-based Diffusion Model (TSC-HDM), which calculates H based on the stationary or non-stationary characteristic of the time series. TSC-HDM selects a handful of seed nodes and activates a seed node&#39;s inactive successor only if H &gt;‚Äâ0.5. The proposed model has been tested on four real-world OSN datasets. The results have been compared against four other IM models ‚Äì Independent Cascade, Weighted Cascade, Trivalency, and Hurst-based Influence Maximization. TSC-HDM is found to have achieved as much as 590% higher expected influence spread as compared to the other models. Moreover, TSC-HDM has attained 344% better average influence spread than other state-of-the-art models namely LIR, A-Greedy, LPIMA, Genetic Algorithm with Dynamic Probabilities, NeighborsRemove, DegreeDecrease, IGIM, IRR, and PHG.},
  archive      = {J_EXSY},
  author       = {Bhawna Saxena and Vikas Saxena and Nishit Anand and Vikas Hassija and Vinay Chamola and Amir Hussain},
  doi          = {10.1111/exsy.13375},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13375},
  shortjournal = {Expert Syst.},
  title        = {A hurst-based diffusion model using time series characteristics for influence maximization in social networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on software and hardware developments in automatic
epilepsy diagnosis using EEG datasets. <em>EXSY</em>, <em>40</em>(9),
e13374. (<a href="https://doi.org/10.1111/exsy.13374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a common non-communicable, group of neurological disorders affecting more than 50 million individuals worldwide. Different approaches of basic, clinical, and translational research of the human brain have been explored to diagnose, treat, and manage the growing no. of cases of epilepsy. Various hospital information from video, images, signals, forms, and so forth, are retrieved and analysed to develop a consensus for such patients. Electroencephalography (EEG) tests are routinely used to diagnose the type of epilepsy in a clinical setting. Artificial Intelligence algorithms are assisting in the early detection and prediction of epileptic patterns observed in EEG signals. This paper reviews recent and emerging state-of-the-art (SOTA) software and hardware approaches in data selection, signal processing, feature estimation, classification, detection methods, and evaluation metrics applied to open and private EEG datasets from 2014 to 2022. The work summarizes and compares reported works through subjective and objective parameters. Rise in hardware pipeline, start-ups, and companies, deep learning methods in software pipeline and release of free EEG datasets have been observed from 2014 to 2022. SOTA advancements have shown immense potential and paved a path to successful real-time detection, prediction, and management of epileptic seizures.},
  archive      = {J_EXSY},
  author       = {Palak Handa and Esha Gupta and Satya Muskan and Nidhi Goel},
  doi          = {10.1111/exsy.13374},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13374},
  shortjournal = {Expert Syst.},
  title        = {A review on software and hardware developments in automatic epilepsy diagnosis using EEG datasets},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning approach for the depression detection of
social media data with hybrid feature selection and attention mechanism.
<em>EXSY</em>, <em>40</em>(9), e13371. (<a
href="https://doi.org/10.1111/exsy.13371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a severe mental health issue. The user-generated content on social media (SM) is growing nowadays. Some computational approaches have been proposed for detecting depression based on users&#39; SM data. However, because of the use of formal language, short range of words and misspellings in the SM data, depression detection (DD) is a challenging task. This paper proposes a novel deep learning (DL) technique for performing DD of the SM data with the help of the hybrid feature selection (FS) mechanism. Initially, two publicly available datasets containing user tweets are collected for implementing the proposed research model. Then the collected datasets are preprocessed for further processing. The preprocessing phase includes critical processes that contribute to creating a ready-to-use dataset for training and testing. After preprocessing, the preprocessed data is divided into prime and non-prime words based on the dictionary approach. After that, the hybrid FS approach is implemented to select the most relevant features from the prime and non-prime words for higher classification accuracy (AC). In the hybrid model, firstly Term Frequency Inverse Document Frequency integrated Modified Information Gain (TFIDF-MIG) approach is proposed that assigns the score value of each prime and non-prime word in the dataset. Secondly, optimal features are selected from the weighted features using the Improved Elephant Herding Algorithm (IEHA). Finally, the decided features from the hybrid model are fed into the DL model, namely attention included improved ReLU-based Convolution Neural Network with Long Short-Term Memory (AIRCNN-LSTM) for DD. Experiments are performed on the collected datasets to assess the proposed model&#39;s performance efficiency. The results of the extensive experiments show that the presented work outperforms existing techniques regarding DD classification AC by locating the best solutions. At the same time, it reduces the number of features chosen.},
  archive      = {J_EXSY},
  author       = {M. Bhuvaneswari and V. Lakshmi Prabha},
  doi          = {10.1111/exsy.13371},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13371},
  shortjournal = {Expert Syst.},
  title        = {A deep learning approach for the depression detection of social media data with hybrid feature selection and attention mechanism},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable deep learning based text regression for
financial prediction. <em>EXSY</em>, <em>40</em>(9), e13368. (<a
href="https://doi.org/10.1111/exsy.13368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text regression is an important task in natural language processing (NLP), which aims to predict continuous numerical values associated with text. Previous work focused on linear text regression requiring manual feature selection for financial prediction. Recently, non-linear text regression through neural network models has become a trend. However, most models rely only on convolutional neural networks (CNN) and suffer from insufficient interpretability. In this paper, we propose a deep neural network model named EM-CBA for text regression and further interpret the model. The proposed model is powered by word EMbedding, CNN, Bidirectional long short-term memory (Bi-LSTM) and Attention mechanism. The proposed EM-CBA takes financial report texts as input and predicts a financial metric named return on assets (ROA). We conduct comprehensive experiments on a dataset about the reports of enterprises. Experimental results show that the proposed model provides more accurate predictions of enterprises&#39; metrics than previous convolutional neural network models and other classical models. The validity of each module of the model is also verified. Finally, we demonstrate a way of performing analysis in words change and results errors to intuitively interpret the effect of different text inputs on the model. The analysis demonstrates that the model is able to use information about sentiment words to analyse their associated contexts to revise the predictions.},
  archive      = {J_EXSY},
  author       = {Rufeng Liang and Weiwen Zhang and Haiming Ye},
  doi          = {10.1111/exsy.13368},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13368},
  shortjournal = {Expert Syst.},
  title        = {Interpretable deep learning based text regression for financial prediction},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the accuracy and speed of fast template-matching
algorithms by neural architecture search. <em>EXSY</em>, <em>40</em>(9),
e13358. (<a href="https://doi.org/10.1111/exsy.13358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search can be used to find convolutional neural architectures that are precise and robust while enjoying enough speed for industrial image processing applications. In this paper, our goal is to achieve optimal convolutional neural networks (CNNs) for multiple-templates matching for applications such as licence plates detection (LPD). We perform an iterative local neural architecture search for the models with minimum validation error as well as low computational cost from our search space of about 32‚Äâbillion models. We describe the findings of the experience and discuss the specifications of the final optimal architectures. About 20-times error reduction and 6-times computational complexity reduction is achieved over our engineered neural architecture after about 500 neural architecture evaluation (in about 10‚Äâh). The typical speed of our final model is comparable to classic template matching algorithms while performing more robust and multiple-template matching with different scales.},
  archive      = {J_EXSY},
  author       = {Seyed Mahdi Shariatzadeh and Mahmood Fathy and Reza Berangi},
  doi          = {10.1111/exsy.13358},
  journal      = {Expert Systems},
  month        = {11},
  number       = {9},
  pages        = {e13358},
  shortjournal = {Expert Syst.},
  title        = {Improving the accuracy and speed of fast template-matching algorithms by neural architecture search},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new method of multi-attribute group decision making based
on hesitant fuzzy soft expert information. <em>EXSY</em>,
<em>40</em>(8), e13357. (<a
href="https://doi.org/10.1111/exsy.13357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interesting insights into uncertain multi-attribute decision-making systems have come from the theory of hesitant fuzzy sets. Outputs become more reliable when a decision-maker is not forced to produce one single assessment in the presence of hesitation. Hesitancy in fuzzy environments grants more flexibility to the decision-maker, therefore enhancing the validity of all subsequent results. Due to these facts, hesitation has become a landmark in the enhancement of fuzzy-soft-inspired theories, and techniques using models such as hesitant fuzzy soft sets or hesitant (fuzzy) -soft sets have thrived. However, the literature lacks a comparable improvement for fuzzy soft expert information. The main goal of this paper is the integration of both hesitation and fuzziness with the characteristics of soft expert sets. The resulting hybrid model is therefore called hesitant fuzzy soft expert set, and its utilization for multiple attribute group decision-making (MAGDM) will be investigated too. To that purpose, we first study the required algebraic operations and properties (subsethood and equality, complement, union, intersection, plus AND and OR operation). Respective numerical examples illustrate their utilization. We also establish some important laws for hesitant fuzzy soft expert sets, including commutativity, associativity, distributivity, and De Morgan&#39;s laws. Moreover, we define four novel types of level soft expert sets for fuzzy soft expert sets, and three types of fuzzy soft expert sets for hesitant fuzzy soft expert sets, which are: -level soft expert sets, level soft expert sets regarding fuzzy threshold, mid-level soft expert sets and top-level soft expert sets. Armed with these tools, we present a natural algorithm for MAGDM under hesitant fuzzy soft expert information that is accompanied by an illustrative application that concerns a selection problem for the samples of electric vehicles manufactured by different companies. This is followed by a comparison with existing mathematical tools such as hesitant fuzzy soft sets and fuzzy soft expert sets, in order to show the advantages of our work over them. In the end, we provide some future research directions.},
  archive      = {J_EXSY},
  author       = {Muhammad Akram and Ghous Ali and Jos√© Carlos R. Alcantud},
  doi          = {10.1111/exsy.13357},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13357},
  shortjournal = {Expert Syst.},
  title        = {A new method of multi-attribute group decision making based on hesitant fuzzy soft expert information},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ASYv3: Attention-enabled pooling embedded swin
transformer-based YOLOv3 for obscenity detection. <em>EXSY</em>,
<em>40</em>(8), e13337. (<a
href="https://doi.org/10.1111/exsy.13337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rampant spread of explicit content across social media can leave a damaging mark on our society. Hence, the need to be vigilant in detecting and curtailing sexually explicit content cannot be overstated. As such, it becomes paramount to discern and manage sexually explicit material to curb its dissemination and safeguard our digital communities from its harmful effects. In this article, we propose a unique technique entitled attention-enabled pooling (ABP) embedded Swin transformer-based YOLOv3 (ASYv3) for the detection of obscene areas present in the images with a bounding box around the offensive regions. ASYv3 employs a unique two-step approach for enhanced performance in obscene detection. In the first step, a scalable and efficient Swin transformer block is integrated, utilizing self-attention and model parallelism to train massive models effectively. In the second phase, the embedding layer of the Swin transformer is replaced with ABP, mitigating disruption of feature context. ABP allows for the projection of raw-valued features into linear form with proper attention to feature context information at specified locations, resulting in optimized feature extraction. The proposed ABP embedded Swin transformer-based YOLOv3 (ASYv3) was trained with annotated obscene images (AOI) dataset. The proposed ASYv3 model surpassed the state-of-the-art methods by achieving 97% testing accuracy, 96.62% precision, 97.40% sensitivity, 3.48% FPR rate, 97.37% NPV values, and 95.59% mAP values, respectively.},
  archive      = {J_EXSY},
  author       = {Sonali Samal and Yu-Dong Zhang and Thippa Reddy Gadekallu and Bunil Kumar Balabantaray},
  doi          = {10.1111/exsy.13337},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13337},
  shortjournal = {Expert Syst.},
  title        = {ASYv3: Attention-enabled pooling embedded swin transformer-based YOLOv3 for obscenity detection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel multimodal online news popularity prediction model
based on ensemble learning. <em>EXSY</em>, <em>40</em>(8), e13336. (<a
href="https://doi.org/10.1111/exsy.13336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of news popularity is having substantial importance for the digital advertisement community in terms of selecting and engaging users. Traditional approaches are based on empirical data collected through surveys and applied statistical measures to prove a hypothesis. However, predicting news popularity based on statistical measures applied to past data is highly questionable. Therefore, in this article, we predict news popularity using machine learning classification models and deep residual neural network models. Articles are usually made up of textual content and in many cases, images are also used. Although it is evident that the appropriate amount of textual data is required to extract features and create models, image data is also helpful in gaining useful information. In this article, we present a novel multimodal online news popularity prediction model based on ensemble learning. This research work acts as a guide for extensive feature engineering, feature extraction, feature selection and effective modelling to create a robust news popularity Prediction Model. Three kinds of features‚Äîmeta-features, text features and image features are used to design an influential and robust model. The relative error performance measure Root Mean Squared logarithmic error (RMSLE) is used to quantify the popularity prediction error. Further, the RMSLE outcome shows 0.351 which is the lowest error value given by the proposed model. Further, the most important features are also sought out to show the dependence of the best-fit model on text and image features.},
  archive      = {J_EXSY},
  author       = {Anuja Arora and Vikas Hassija and Shivam Bansal and Siddharth Yadav and Vinay Chamola and Amir Hussain},
  doi          = {10.1111/exsy.13336},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13336},
  shortjournal = {Expert Syst.},
  title        = {A novel multimodal online news popularity prediction model based on ensemble learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-disease classification model using deep neural network
and strassen‚Äôs rectilinear fine-tune bouncing training algorithm.
<em>EXSY</em>, <em>40</em>(8), e13335. (<a
href="https://doi.org/10.1111/exsy.13335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep neural network (DNN) is being used in the healthcare industry to improve care delivery at a lower cost and in less time. A DNN is well-known for its diagnostic applications. However, it is also increasingly being utilized to guide healthcare management decisions. DNNs have been applied to real-life disease classification problems. The performances of DNN are improved by reducing the training time, performing fast classification, and improving the parameters such as accuracy and error rate, and so forth. We have proposed a hybrid DNN named Strassen&#39;s rectilinear fine-tune bouncing training (SRFBT) Algorithm by combining Strassen&#39;s theorem and bouncing training algorithm. The simulation result shows that the SRFBT algorithm outperforms both the DNN, support vector machine, radial basis function network and deep belief network algorithms in terms of network training, testing time and accuracy on various UCI machine learning healthcare datasets. The SRFBT has improved performance with a minimum average training time of 43.6249 and a maximum accuracy of 96.86.},
  archive      = {J_EXSY},
  author       = {Velliangiri Sarveshwaran and P. Karthikeyan and J. Premalatha},
  doi          = {10.1111/exsy.13335},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13335},
  shortjournal = {Expert Syst.},
  title        = {Multi-disease classification model using deep neural network and strassen&#39;s rectilinear fine-tune bouncing training algorithm},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge-featured graph attention network with dependency
features for causality detection of events. <em>EXSY</em>,
<em>40</em>(8), e13332. (<a
href="https://doi.org/10.1111/exsy.13332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causality detection, as a more fine-grained task than causality extraction, which aims to detect the components that represent the cause and effect in sentence-level texts with causality, is a significant task in the field of Natural Language Processing (NLP). Previous research on causality detection has concentrated on text token features whilst ignoring the dependency attributes between tokens. In this paper, we propose a model that uses the Edge-featured Graph Attention Network based on dependency-directed graphs for the causality detection task. To begin, we convert the texts with causality into the representation of dependency-directed graphs (DDG), which regard the dependency attributes between tokens as edge features. Then we use Edge-featured Graph Attention Network to aggregate the node and edge features of DDG. Finally, we put the graph embedding into Bi-directional Long Short-Term Memory (BiLSTM) layer to learn the dependencies between forward and backward long-distance nodes in DDG. Experiments on three datasets prove that this method achieves better performance in precision, recall, and other evaluation metrics compared with other methods.},
  archive      = {J_EXSY},
  author       = {Jianxiang Wei and Yuhang Chen and Pu Han and Yunxia Zhu and Weidong Huang},
  doi          = {10.1111/exsy.13332},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13332},
  shortjournal = {Expert Syst.},
  title        = {Edge-featured graph attention network with dependency features for causality detection of events},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective sunflower optimization: A new hypercubic
meta-heuristic for constrained engineering problems. <em>EXSY</em>,
<em>40</em>(8), e13331. (<a
href="https://doi.org/10.1111/exsy.13331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve challenging engineering problems, the state-of-the-art in multi-objective optimization shows a trend toward using meta-heuristics and a posteriori decision-making methods. This encourages the search for algorithms better able to find Pareto fronts with more convergence, coverage, and lower computational cost. This work shows the creation and validation of the Multi-objective Sunflower Optimization (MOSFO), a hypercubic and constrained multi-objective meta-heuristic inspired by the phototropic life cycle of sunflowers around the sun. Having a much simpler programming model than most evolutionary algorithms, MOSFO was validated using the most difficult set of test functions in the literature (CEC 2009) and applied to ten constrained multi-objective optimization problems (CEC 2021). The proposed algorithm was compared with ten other powerful algorithms: MOGWO, MOPSO, NSGA-II, MOEA/D, NSGA-III, CCMO, ARMOEA, ToP, TiGE 2, and AnD. Inverted General Distance, Spacing, Maximum Spread, and Hyper volume were used as comparison metrics to evaluate the convergence and coverage capabilities of the algorithms. MOSFO had the best average IGD value in 8 of the 10 test functions when compared with the other algorithms. In terms of MS, MOSFO had the highest average value of MS for 7 of the test functions. In summary, MOSFO showed substantial convergence and coverage capabilities and proved to be very competitive among the algorithms used, which were carefully selected to be popular and recent. The method is even more promising for problems with three or more objectives.},
  archive      = {J_EXSY},
  author       = {Jo√£o Luiz Junho Pereira and Guilherme Ferreira Gomes},
  doi          = {10.1111/exsy.13331},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13331},
  shortjournal = {Expert Syst.},
  title        = {Multi-objective sunflower optimization: A new hypercubic meta-heuristic for constrained engineering problems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison between chaos theory and l√©vy flights in
sunflower optimization for feature selection. <em>EXSY</em>,
<em>40</em>(8), e13330. (<a
href="https://doi.org/10.1111/exsy.13330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a knowledge discovery tool to understand the problem by analysing features. In particular, the application of feature selection in data mining can not only improve the quality of extracted patterns and knowledge but also decrease computational costs. Various techniques have been applied to this complex optimization problem, in which metaheuristics have been validated to be superior. This study introduces a new metaheuristic known for having lean and fast programming, inspired by the sunflower&#39;s motions for feature selection for the first time. It is equipped with a v -shaped transfer function and associated with the KNN classifier to become the binary sunflower optimization (BSFO). A total of 12 variants of BSFO are designed based on the chaos theory and L√©vy flights, called improved binary sunflower optimization (IBSFO). A discussion between these improvement theories for feature selection has also not been made yet, and it is performed in this paper using 15 benchmark datasets from the UCI repository. The experimental results show that all variants can advance the fitness value of BSFO, and nine of them considerably decrease the computational costs. Furthermore, the chaotic BSFO with the Chebyshev function, taking replacement to normal rand, has the lowest fitness value (‚àí11.37%) and execution time (‚àí9.31%) than the original BSFO. Further, IBSFO is compared with another eight metaheuristics and outperforms these competitors on average fitness value and execution time. Overall, IBSFO proved to find subsets with reduced dimension and high accuracy with meagre computational cost due to its robust explorative and exploitative capacities.},
  archive      = {J_EXSY},
  author       = {Jo√£o Luiz Junho Pereira and Benedict Jun Ma and Matheus Brendon Francisco and Ronny Francis Ribeiro Junior and Guilherme Ferreira Gomes},
  doi          = {10.1111/exsy.13330},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13330},
  shortjournal = {Expert Syst.},
  title        = {A comparison between chaos theory and l√©vy flights in sunflower optimization for feature selection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing arabic-text feature extraction utilizing
label-semantic augmentation in few/zero-shot learning. <em>EXSY</em>,
<em>40</em>(8), e13329. (<a
href="https://doi.org/10.1111/exsy.13329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing amount of research use pre-trained language models to address few/zero-shot text classification problems. Most of these studies neglect the semantic information hidden implicitly beneath the natural language names of class labels and develop a meta learner from the input texts solely. In this work, we demonstrate how label information can be utilized to extract enhanced feature representation of the input text from a Transformer-based pre-trained language model such as AraBERT. In addition, how this approach can improve performance when the data resources are scarce like in the Arabic language and the input text is short with little semantic information as is the case using tweets. The work also applies zero-shot text classification to predict new classes with no training examples across different domains including sarcasm detection and sentiment analysis using the information in the last layer of a trained classifier in a transfer learning setting. Experiments show that our approach has a better performance for the few-shot sentiment classification compared to baseline models and models trained without augmenting label information. Moreover, the zero-shot implementation achieved an accuracy up to 0.874 in Arabic sarcasm detection from a model trained on a sentiment analysis task.},
  archive      = {J_EXSY},
  author       = {Seham Basabain and Erik Cambria and Khalid Alomar and Amir Hussain},
  doi          = {10.1111/exsy.13329},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13329},
  shortjournal = {Expert Syst.},
  title        = {Enhancing arabic-text feature extraction utilizing label-semantic augmentation in few/zero-shot learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Review of audio deepfake detection techniques: Issues and
prospects. <em>EXSY</em>, <em>40</em>(8), e13322. (<a
href="https://doi.org/10.1111/exsy.13322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past years, multimedia content has improved in realism and plausibility owing to the development of deep learning techniques, particularly the generative adversarial networks and variational auto-encoders. Though digital content, especially digital movies shot from a certain viewpoint gives a true representation of reality, yet the ubiquitous usage of content manipulation techniques casts doubt on its veracity. Deepfaking an AI based tampering technique, is able to map facial and acoustic features of a source person onto the target with an intention to make target say or enact the things that has not happened in real. Numerous approaches have been proposed in the literature for detection of image and video deepfakes. With technological advancement, researchers have also started to examine audio deepfakes and ways to detect them. As there is currently no comprehensive overview of audio deepfake generation and detection techniques, this paper aims to provide a survey of the relevant literature in this area. This survey paper intends to help research fraternity about the available audio generation and detection approaches for design of reliable detection models in future to classify fake and real audios.},
  archive      = {J_EXSY},
  author       = {Abhishek Dixit and Nirmal Kaur and Staffy Kingra},
  doi          = {10.1111/exsy.13322},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13322},
  shortjournal = {Expert Syst.},
  title        = {Review of audio deepfake detection techniques: Issues and prospects},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of feature selection methods utilizing random
forest and logistic regression for lung tissue categorization using HRCT
images. <em>EXSY</em>, <em>40</em>(8), e13320. (<a
href="https://doi.org/10.1111/exsy.13320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Categorization of lung tissue patterns with interstitial lung diseases (ILD) utilize high-resolution computed tomography (HRCT) lung images of the TALISMAN dataset which is challenging due to high intra-class variation and inter-class ambiguity. To tackle this, major contributions are made in three aspects. First, a novel shape-based feature is proposed to quantify the amount of fibrotic and nodular components in a lung tissue pattern which helps to minimize intra-class variation and inter-class ambiguity. Second, we address the curse of dimensionality which often arises due to huge feature space. Third, to prevent an overfitting issue, the Grid Search optimization algorithm is utilized by tuning the Random Forest hyper-parameters. In this manuscript, a framework is proposed to categorize lung tissue patterns by integrating four types of feature domains (a) intensity-based, (b) texture-based, (c) wavelet-based, and (d) shape-based along with the novel shape-based feature. As a result, we encounter a large feature space (i.e., ), which leads to high dimensionality. To address this issue, we reduce the feature space using filter-based f-statistic, reliefF, minimum Redundancy Maximum Relevance (mRMR), and embedded-based decision trees, regularization models. We found that the regularization model shrinks the feature space by 2.5 times in just 90‚Äâs whereas mRMR methods reduce the feature space by 10 times in 13‚Äâmin. Using the proposed feature set, we employ Random Forest and Logistic Regression as potential classifiers to classify lung tissue patterns. Experiential results reveal that the proposed framework categorizes lung tissue patterns more effectively than state-of-the-art hand-crafted and deep learning-based approaches.},
  archive      = {J_EXSY},
  author       = {Rashmi Vishraj and Savita Gupta and Sukhwinder Singh},
  doi          = {10.1111/exsy.13320},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13320},
  shortjournal = {Expert Syst.},
  title        = {Evaluation of feature selection methods utilizing random forest and logistic regression for lung tissue categorization using HRCT images},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable fault prediction using learning fuzzy cognitive
maps. <em>EXSY</em>, <em>40</em>(8), e13316. (<a
href="https://doi.org/10.1111/exsy.13316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT sensors capture different aspects of the environment and generate high throughput data streams. Besides capturing these data streams and reporting the monitoring information, there is significant potential for adopting deep learning to identify valuable insights for predictive preventive maintenance. One specific class of applications involves using Long Short-Term Memory Networks (LSTMs) to predict faults happening in the near future. However, despite their remarkable performance, LSTMs can be very opaque. This paper deals with this issue by applying Learning Fuzzy Cognitive Maps (LFCMs) for developing simplified auxiliary models that can provide greater transparency. An LSTM model for predicting faults of industrial bearings based on readings from vibration sensors is developed to evaluate the idea. An LFCM is then used to imitate the performance of the baseline LSTM model. Through static and dynamic analyses, we demonstrate that LFCM can highlight (i) which members in a sequence of readings contribute to the prediction result and (ii) which values could be controlled to prevent possible faults. Moreover, we compare LFCM with state-of-the-art methods reported in the literature, including decision trees and SHAP values. The experiments show that LFCM offers some advantages over these methods. Moreover, LFCM, by conducting a what-if analysis, could provide more information about the black-box model. To the best of our knowledge, this is the first time LFCMs have been used to simplify a deep learning model to offer greater explainability.},
  archive      = {J_EXSY},
  author       = {Taha Mansouri and Sunil Vadera},
  doi          = {10.1111/exsy.13316},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13316},
  shortjournal = {Expert Syst.},
  title        = {Explainable fault prediction using learning fuzzy cognitive maps},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning based spatio-temporal hand gesture recognition
system in complex environment. <em>EXSY</em>, <em>40</em>(8), e13313.
(<a href="https://doi.org/10.1111/exsy.13313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition nowadays has grabbed the attention of researchers as they represent human behaviour in multiple practical ways. Amongst a variety of gestures available, hand gestures play an essential role in the field of human-computer interaction when recognised efficiently in complex and dynamic environments. In this paper, we propose a dynamic hand gesture recognition system to recognise hand gestures appearing in different indoor and outdoor environments. Hand detection and tracking uses a two-level system resulting in the formation of gesture trajectory in challenging conditions in which existing detection and tracking algorithms could not do so. A set of 45 features is provided as input to the various classification techniques. The redundancy problem has been reduced by selecting a set of optimum features using the analysis of variance method, which ranks the list of features. An incremental feature selection technique calculates recognition accuracy by selecting features according to rankings. This system provides an accuracy of 96.32% when used with machine learning and 97.5% when used with deep learning techniques. Recognition accuracy is calculated for various environments, including an extra hand, multiple persons in the video frame, and outdoor environment. All machine-learning classifiers are combined using classifier combination to calculate the accuracy according to the majority-voting rule. Based on the experimental results, it has been observed that deep learning provides better results compared to machine learning.},
  archive      = {J_EXSY},
  author       = {Shweta Saboo and Joyeeta Singha and Rabul Hussain Laskar},
  doi          = {10.1111/exsy.13313},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13313},
  shortjournal = {Expert Syst.},
  title        = {Deep learning based spatio-temporal hand gesture recognition system in complex environment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PrismPatNet: Novel prism pattern network for accurate fault
classification using engine sound signals. <em>EXSY</em>,
<em>40</em>(8), e13312. (<a
href="https://doi.org/10.1111/exsy.13312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engines are prone to various types of faults, and it is crucial to detect and indeed classify them accurately. However, manual fault type detection is time-consuming and error-prone. Automated fault type detection promises to reduce inter- and intra-observer variability while ensuring time invariant attention during the observation duration. We have proposed an automated fault-type detection model based on sound signals to realize these advantageous properties. We have named the detection model prism pattern network (PrismPatNet) to reflect the fact that our design incorporates a novel feature extraction algorithm that was inspired by a 3D prism shape. Our prism pattern model achieves high accuracy with low-computational complexity. It consists of three main phases: (i) prism pattern inspired multilevel feature generation and maximum pooling operator, (ii) feature ranking and feature selection using neighbourhood component analysis (NCA), and (iii) support vector machine (SVM) based classification. The maximum pooling operator decomposes the sound signal into six levels. The proposed prism pattern algorithm extracts parameter values from both the signal itself and its decompositions. The generated parameter values are merged and fed to the NCA algorithm, which extracts 512 features from that input. The resulting feature vectors are passed on to the SVM classifier, which labels the input as belonging to 1 of 27 classes. We have validated our model with a newly collected dataset containing the sound of (1) a normal engine and (2) 26 different types of engine faults. Our model reached an accuracy of 99.19% and 98.75% using 80:20 hold-out validation and 10-fold cross-validation, respectively. Compared with previous studies, our model achieved the highest overall classification accuracy even though our model was tasked with identifying significantly more fault classes. This performance indicates that our PrismPatNet model is ready to be installed in real-world applications.},
  archive      = {J_EXSY},
  author       = {Sakir Engin Sahin and Gokhan Gulhan and Prabal Datta Barua and Turker Tuncer and Sengul Dogan and Oliver Faust and U. Rajendra Acharya},
  doi          = {10.1111/exsy.13312},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13312},
  shortjournal = {Expert Syst.},
  title        = {PrismPatNet: Novel prism pattern network for accurate fault classification using engine sound signals},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperspectral imaging for early diagnosis of diseases: A
review. <em>EXSY</em>, <em>40</em>(8), e13311. (<a
href="https://doi.org/10.1111/exsy.13311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral Imaging (HSI) has grown to be one of the most crucial optical imaging modalities with applications in numerous industries. The non-invasive nature of HSI has led to widening its horizon to the medical domain, especially in areas like early diagnosis of various diseases. HSI combines both imaging and spectroscopy properties, thereby exploiting spectral and spatial dimensions of images captured, providing quick and accurate interpretation of data. The current study aims to give an exhaustive overview of HSI&#39;s applications pertaining to the medical industry for fast detection of diseases and aiding in surgical procedures. The survey focuses on hyperspectral imaging combined with various approaches - machine learning, deep learning, genetic algorithms, and anomaly detection for the treatment of disorders. In addition, the survey highlights accompanying pre-processing approaches, performance metrics, inferences, and future prospects of HSI in the medical domain. The current study can gauge computer vision specialists, researchers in machine and deep learning domain, doctors, and scientists by giving them a platform for improving existing treatment methods for the betterment of society.},
  archive      = {J_EXSY},
  author       = {Harshita Mangotra and Sahima Srivastava and Garima Jaiswal and Ritu Rani and Arun Sharma},
  doi          = {10.1111/exsy.13311},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13311},
  shortjournal = {Expert Syst.},
  title        = {Hyperspectral imaging for early diagnosis of diseases: A review},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An interpretable diagnostic approach for lung cancer:
Combining maximal clique and improved BERT. <em>EXSY</em>,
<em>40</em>(8), e13310. (<a
href="https://doi.org/10.1111/exsy.13310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lung cancer incidence and mortality in China have always been high. Moreover, due to the limited level of professional technology, misdiagnosis and missed diagnosis of lung cancer often occur. To improve the accuracy of diagnosis, this paper proposes an interpretable diagnostic method for lung cancer based on Chinese electronic medical records (EMRs). First, to overcome the difficulty in word segmentation of clinical texts in Chinese EMRs, a dictionary construction method is proposed based on the idea of maximal clique, and 730 medical professional terms related to lung diseases are identified. Then, the ProbSparse self-attention mechanism and self-attention distilling operation in Informer are used to improve the Bidirectional Encoder Representations from Transformer (BERT) to realize the representation of long clinical texts with lower time complexity and memory consumption. Finally, the convolutional neural network with an attention mechanism is employed to process the representation results to realize the interpretable prediction of lung cancer. This method is applied to the lung cancer diagnosis of inpatients in a tertiary hospital in Hunan Province, obtaining excellent results of about 0.9 for area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). In addition, the results of the comparative analysis with existing dictionaries, word embedding methods and diagnostic methods further confirm the superiority of the proposed method. Specifically, the proposed method improves the precision by at least 6%, the recall by at least 2.6%, the F1 score by at least 5.2%, AUROC by at least 7.3% and AUPRC by at least 7.7% compared with all these state-of-the-art methods.},
  archive      = {J_EXSY},
  author       = {Zi-yu Chen and Fei Xiao and Xiao-kang Wang and Wen-hui Hou and Rui-lu Huang and Jian-qiang Wang},
  doi          = {10.1111/exsy.13310},
  journal      = {Expert Systems},
  month        = {9},
  number       = {8},
  pages        = {e13310},
  shortjournal = {Expert Syst.},
  title        = {An interpretable diagnostic approach for lung cancer: Combining maximal clique and improved BERT},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid computer aided diagnostic system designs for screen
film mammograms using DL-based feature extraction and ML-based
classifiers. <em>EXSY</em>, <em>40</em>(7), e13309. (<a
href="https://doi.org/10.1111/exsy.13309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography is most popular imaging method used often in predicting breast cancer within women above age of 38‚Äâyears. Various computer-assisted algorithms have been employed for classifying breast masses as normal or malignant using screen film mammographic (SFM) images. In present research work, exhaustive experimentations have been carried out with nine deep learning-based convolutional neural networks (CNNs) belonging to the three different categories of CNN architectures including (a) simple convolution-based series models, that is VGG16 and VGG19 (b) simple convolution-based directed acyclic graph (DAG) model, that is GoogleNet and (c) dilated convolution-based DAG models, that is ResNet18, ResNet50, Inceptionv3, XceptionNet, ShuffleNet and MobileNet-V2, for binary classification of the mammographic masses with SFM images. The experimental work has been carried out using 518 mammographic images taken from DDSM dataset with 208 images œµ benign class and 310 images œµ malignant class, respectively. The encoder-decoder based semantic segmentation network model, that is ResNet50 has been used for the segmentation of mammographic masses from SFM images. The segmented masses images obtained from the ResNet50 model are subjected to classification experiments. To design a robust hybrid classifier design, that is, deep learning (DL)-based feature extraction and machine learning (ML)-based classification, the first step is to obtain an optimal DL-based feature extractor for classification task. The optimal feature set obtained by the best performing CNN Model, that is, VGG 19 has been subjected to correlation-based feature extraction and ML-based classifiers including (i) adaptive neuro fuzzy classifier-linguistic hedges (ii) principal component analysis- support vector machine classifier and (iii) GA-SVM classifier to yield an optimal hybrid computer aided diagnostic (CAD) system design. It is found that hybrid CAD system using VGG19 as feature extractor and ANFC-LH as classifier yields 96% the highest classification accuracy. The other performance parameters yield values, that is 96% sensitivity, 96% specificity, 96% F-score, 96% precision and 92% MCC, which indicates a best prediction of binary classification. The test images which were misclassified by these hybrid CAD system designs were analysed subjectively by experienced participating radiologist. The results obtained by present work suggests that proposed hybrid CAD system with VGG19 Network model acting as feature extractor and ANFC-LH classifier can be employed for the differential diagnosis of benign as well as malignant mammographic masses using SFM images in a routine clinical setting.},
  archive      = {J_EXSY},
  author       = {Jyoti Rani and Jaswinder Singh and Jitendra Virmani},
  doi          = {10.1111/exsy.13309},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13309},
  shortjournal = {Expert Syst.},
  title        = {Hybrid computer aided diagnostic system designs for screen film mammograms using DL-based feature extraction and ML-based classifiers},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Opposition-based ideal gas molecular movement algorithm with
cauchy mutation, velocity clamping, and mirror operator. <em>EXSY</em>,
<em>40</em>(7), e13306. (<a
href="https://doi.org/10.1111/exsy.13306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ideal gas molecular movement (IGMM) as a metaheuristic optimization method is a prominent option for solving optimization problems. However, in some complex cases, IGMM may possess premature convergence or get trapped in local optima. Therefore, to tackle these issues, this paper indicates a new modified IGMM algorithm named opposition-based IGMM, which has been incorporated with opposition based learning (OBL), Cauchy mutation (CM), velocity clamping (VC), and mirror operator (MO) to enhance its performance. OBL, VC, and MO improve the convergence of IGMM, whereas CM assists IGMM to escape local optima. The effect of each strategy, OBL, CM, VC, and MO, on IGMM, is confirmed through 30 low and high-dimensional benchmarks, including 23 well-known mathematical problems and CEC2017 as complex test functions and three engineering problems. Analysis results represent that integration IGMM with OBL, CM, VC, and MO has the best performance among other IGMM variants and eventually improved IGMM in exploration, exploitation, accelerating convergence, and local optima avoidance.},
  archive      = {J_EXSY},
  author       = {Mahsa Safari and Hesam Varaee},
  doi          = {10.1111/exsy.13306},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13306},
  shortjournal = {Expert Syst.},
  title        = {Opposition-based ideal gas molecular movement algorithm with cauchy mutation, velocity clamping, and mirror operator},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SE_SPnet: Rice leaf disease prediction using stacked
parallel convolutional neural network with squeeze-and-excitation.
<em>EXSY</em>, <em>40</em>(7), e13304. (<a
href="https://doi.org/10.1111/exsy.13304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice is one of the significant crops, and the early identification and prevention of its diseases are essential to ensure adequate and healthy availability to the world&#39;s growing population. The use of image processing is an encouraging method for automatic rice leaf disease identification and detection. In particular, the recent advancements indicate the effectiveness of convolutional neural network (CNN) based deep learning approaches. In this direction, the present work proposes a novel stacked parallel convolution layers-based network (SPnet) with the squeeze-and-excitation (SE) architecture, named (SE_SPnet), for classifying diseased rice leaf images. The stacked parallel network block comprises four parallel convolution layers with different kernel sizes for abstractions of the global and local features. The SE block extracts feature information automatically while removing invalid ones. We compare the SE_SPnet model with state-of-the-art CNN models such as VGG16, DenseNet121, and InceptionV3 based on computational effort, accuracy, sensitivity, specificity, precision, recall, and F1-score. The experimental results show that the SE_SPnet outperforms standard CNN models for the considered rice leaf disease image datasets. In particular, the SE_SPnet achieves the highest accuracy (99.2%), sensitivity (98.2%), specificity (98.5%), precision (98.4%), recall (98.2%), and F1-score (98.5%) while using stochastic gradient descent (with momentum) optimizer with a 0.01 learning rate. Furthermore, the SE_SPnet also exhibits to outperform when compared with some of the most recent and relevant existing works.},
  archive      = {J_EXSY},
  author       = {Parag Bhuyan and Pranav Kumar Singh and Sujit Kumar Das and Anshuman Kalla},
  doi          = {10.1111/exsy.13304},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13304},
  shortjournal = {Expert Syst.},
  title        = {SE_SPnet: Rice leaf disease prediction using stacked parallel convolutional neural network with squeeze-and-excitation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comprehensive analysis of supervised algorithms for coronary
artery heart disease detection. <em>EXSY</em>, <em>40</em>(7), e13300.
(<a href="https://doi.org/10.1111/exsy.13300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mortality rate of cardiovascular disease is increasing at an alarming rate and affects more than 31% of the world&#39;s population. The increment in obesity, strain, high blood pressure, concern, high body mass index, depression, high glucose level, and high cholesterol are the key factors that increase the risk of cardiovascular disease. Coronary heart and cerebrovascular diseases cause 80% of cardiovascular diseases with a higher mortality rate, out of which Coronary Artery Heart Disease (CAHD) causes 25% of the fatality. CAHD fatality can be minimized by early diagnosis. However, due to common symptoms of the non-fatal disease, CAHD is often ignored by the patients and the health care team. In this paper, the authors comprehensively analysed and compared the two supervised Machine Learning models, i.e., Logistic Regression (LR) and XGBoost, for the early diagnosis of CAHD. The models are applied to a benchmark dataset (Statlog heart disease dataset). The model&#39;s parameters are also optimized by performing the Random SearchCV hyperparameter tuning. The comprehensive study is performed on both non-optimized and optimized models as well as compared to the other well-known existing models. After parametric optimization, both the models exhibit higher accuracy, i.e., LR 87.78% and XGBoost 91.85%, compared to the non-optimized techniques, i.e., LR‚Äâ=‚Äâ83.33% and XGBoost‚Äâ=‚Äâ77.77%.},
  archive      = {J_EXSY},
  author       = {Sanjay Dhanka and Vibhor Kumar Bhardwaj and Surita Maini},
  doi          = {10.1111/exsy.13300},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13300},
  shortjournal = {Expert Syst.},
  title        = {Comprehensive analysis of supervised algorithms for coronary artery heart disease detection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IoT based arrhythmia classification using the enhanced hunt
optimization-based deep learning. <em>EXSY</em>, <em>40</em>(7), e13298.
(<a href="https://doi.org/10.1111/exsy.13298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of information technology, the Internet of Things (IoT), and several miniaturize equipment&#39;s enhances the healthcare field that provides real-time patient monitoring, which helps to provide medication anywhere and anytime. However, accurate detection is still a challenging task for which an effective classification model is introduced in this research. The proposed method is the Enhanced Hunt optimization based Deep convolutional neural network (Enhanced Hunt based-Deep CNN), in which the Enhanced Hunt optimization algorithm (EHOA) is developed by fusing the hunting habit of the predator and the herding characteristics of herding dog for enhancing the global optimal convergence. Here, the ECG signal from the individuals is collected using the IoT network and stored in the Hospital server, which is accessed by the doctor when requested, the classification is performed using the Enhanced Hunt based-Deep CNN and the performance revealed the effectiveness with the accuracy, sensitivity, and specificity of 95.33%, 94.92%, and 97.57%.},
  archive      = {J_EXSY},
  author       = {Abhishek Kumar and Swarn Avinash Kumar and Vishal Dutt and S. Shitharth and Esha Tripathi},
  doi          = {10.1111/exsy.13298},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13298},
  shortjournal = {Expert Syst.},
  title        = {IoT based arrhythmia classification using the enhanced hunt optimization-based deep learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). H7N9 avian influenza diagnosis based on a multilayer belief
rule-based inference methodology. <em>EXSY</em>, <em>40</em>(7), e13296.
(<a href="https://doi.org/10.1111/exsy.13296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {H7N9 avian influenza is a novel virus with high morbidity and mortality that threatens human health and life. Therefore, it is necessary to diagnose H7N9 avian influenza in a timely and rapid manner to prevent further transmission of the virus and greatly reduce the infection and mortality rates. This paper proposes an H7N9 avian influenza diagnostic model that is based on a multilayer belief rule-based (BRB) inference methodology by considering five typical characteristics of influenza: epidemiology, clinical manifestations, complications, characteristics of imaging tests and positive pathogen test results. Specifically, the severity of H7N9 avian influenza is gradually identified by a multilayer BRB model, and then the diagnostic model is optimized by a genetic algorithm (GA) to improve the diagnostic accuracy. Finally, the feasibility of the model is verified by fivefold cross-validation with a real clinical dataset. The performance of the proposed diagnostic model is compared with those of the BP neural network (BPNN) model and support vector machine (SVM) model, and the results show that the multilayer BRB model can achieve rapid and satisfactory diagnostic results for H7N9 avian influenza. The experiment shows that the accuracy of the BRB model for H7N9 avian influenza hierarchical diagnosis provided in this paper is 0.903, which is higher than 0.818 of the BP neural network (BPNN) modules and 0.844 of the support vector machine (SVM) models. Especially when diagnosing the suspected and confirmed degree of H7N9 disease, it is more realized satisfactory diagnostic accuracy.},
  archive      = {J_EXSY},
  author       = {Xiaojian Xu and Yucai Gao and Xiaobin Xu and Libo Dai and Shelan Liu and Shuo Zhang and Xu Weng},
  doi          = {10.1111/exsy.13296},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13296},
  shortjournal = {Expert Syst.},
  title        = {H7N9 avian influenza diagnosis based on a multilayer belief rule-based inference methodology},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Microcell-net: A deep neural network for multi-class
classification of microscopic blood cell images. <em>EXSY</em>,
<em>40</em>(7), e13295. (<a
href="https://doi.org/10.1111/exsy.13295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood-related diseases are one of the major concerns in the biomedical domain and most of the disease symptoms are reflected through the analysis of blood cells. The diagnosis by experts in laboratories is very costly and time-consuming, thus artificial intelligence-based systems can help in the automatic diagnosis and monitoring of an individual&#39;s health. In this study, a CNN-based architecture Microcell-Net is proposed which is trained on a microscopic image dataset of peripheral blood cells in eight different classes. The images have several inter-class and intra-class diversity with different magnification levels and the noise present in the images makes the classification task significantly challenging. Experimental results indicated that the proposed model can efficiently classify various types of microscopic blood cells with good accuracy. The experimental findings accomplished 98.76% validation accuracy and 97.65% test accuracy in complex background conditions. The performance of the model is compared with other state-of-the-art models and the proposed deep neural network performs significantly better than others. The proposed model can be utilized in a real-time diagnosis system because it is fast, automatic and efficient, which can assist in taking clinical decisions and early diagnosis of haematological disorders.},
  archive      = {J_EXSY},
  author       = {Karnika Dwivedi and Malay Kishore Dutta},
  doi          = {10.1111/exsy.13295},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13295},
  shortjournal = {Expert Syst.},
  title        = {Microcell-net: A deep neural network for multi-class classification of microscopic blood cell images},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of brain tumour based on texture and deep
features of magnetic resonance images. <em>EXSY</em>, <em>40</em>(7),
e13294. (<a href="https://doi.org/10.1111/exsy.13294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the world health organization report, brain cancer has the highest death rate. magnetic resonance imaging (MRI) for detecting brain tumours is adopted these days due to several advantages over other detection techniques. This paper presents a novel methodology to classify MR images based on texture and deep features, z -score normalization, and, Comprehensive learning elephant herding optimization (CLEHO) based feature optimization and classification. Deep features of brain MR images have been extracted through DenseNet121 convolutional neural network and texture features have been extracted by using the Gabor 2D filter, Haralick texture feature, edge continuity texture feature, first order statistical texture feature, local binary pattern feature, difference theoretic texture feature, and spectral texture feature techniques. Normalization has been done using three normalization techniques that is, z score, mean median absolute deviation (MMAD), and Tanh-based after aggregating the features extracted from the previous step. z -score normalization has been suggested for feature normalization after comparing the results attained from the three techniques. Lastly, binary CLEHO has been proposed for selecting an optimal feature set and also optimizing the ‚Äò k ‚Äô value of the k -NN classifier. The outcome of this proposed work is compared with other state-of-the-art methods for a publicly available magnetic resonance image Fighshare dataset of 3064 slices from 233 patients. The proposed work has a brain tumour average classification accuracy of 98.97%, which is better than the other state-of-the-art methods. The proposed work can be used to assist the radiologist in the screening of multi-class brain tumours.},
  archive      = {J_EXSY},
  author       = {Hare Krishna Mishra and Manpreet Kaur},
  doi          = {10.1111/exsy.13294},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13294},
  shortjournal = {Expert Syst.},
  title        = {Classification of brain tumour based on texture and deep features of magnetic resonance images},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Time series forecasting based on a novel ensemble-based
network and variational mode decomposition. <em>EXSY</em>,
<em>40</em>(7), e13291. (<a
href="https://doi.org/10.1111/exsy.13291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical and computational intelligence methods contain weaknesses in handling nonlinearity, non-stationarity and noise. This research develops a novel decomposition ensemble-based network named VMD-DENetwork for time series forecasting over different horizons. A robust decomposition technique called variational mode decomposition (VMD) is applied to decompose the input sequence into several intrinsic modes in a non-recursive manner. The optimal number of intrinsic modes is selected based on a comprehensive analysis to ensure the stability of the framework. The proposed DENetwork is developed based on stacking architecture and constitutes heterogeneous learners to model the nonlinear and complex relationships. It combines a convolutional neural network, long short-term memory and an extreme learning machine. A firefly optimization algorithm is adopted for utilizing hyperparameters of the proposed model to enhance the efficiency of VMD-DENetwork. The forecasting performance is verified by using six real-world data sets from the New York Mercantile and International Petroleum Exchange. The final obtained results are compared with several peer-advanced algorithms using the root mean squared error (RMSE), mean absolute error (MAE), Theil inequality coefficient (TIC) and correlation coefficient (R) metrics. The experimental results confirm that the proposed model demonstrates outstanding prediction performance. The employed optimization algorithm is compared with three frequently used bio-inspired optimization algorithms, and their performance is tested using standard CEC benchmarks.},
  archive      = {J_EXSY},
  author       = {Fatemeh Nazarieh and Mohammad Naderi Dehkordi},
  doi          = {10.1111/exsy.13291},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13291},
  shortjournal = {Expert Syst.},
  title        = {Time series forecasting based on a novel ensemble-based network and variational mode decomposition},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting impersonation episode using teaching
learning-based optimization and support vector machine techniques.
<em>EXSY</em>, <em>40</em>(7), e13290. (<a
href="https://doi.org/10.1111/exsy.13290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last few decades, computer and internet security has become a vital area because of eye-opening numbers of data breaches, intruders on perilous infrastructure and malware attacks that are increasing day by day. In order to monitor abnormal activities and identify unusual attacks, several security solutions have been proposed in the recent past years. To protect computer system, intrusion detection system (IDS) opens up great opportunities to determine vulnerabilities and detect anomalies in security system. For detecting new attacks or building security solutions, in this study we present a new wrapper technique for security specialists that can help to detect more complicated attacks by combining teaching learning-based optimization with opposition learning scheme and simulated annealing method. In order to address advance attack, firstly opposition learning strategy is used to update population generation of teaching learning-based optimization that affect the robustness of the model after that simulated annealing is integrated into the teaching learning-based optimization. In proposed method to choose the relevant features, we have used support vector machine as a fitness function that can be helped to recognize attacks precisely. The proposed algorithm is evaluated on three popular datasets namely NSL-KDD, ISCX 2012 and UNSW-NB15. Experimental results show that the proposed method is superior to other existing wrapper algorithms in terms of detection rate, accuracy and false alarm rates.},
  archive      = {J_EXSY},
  author       = {Alok Kumar Shukla},
  doi          = {10.1111/exsy.13290},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13290},
  shortjournal = {Expert Syst.},
  title        = {Detecting impersonation episode using teaching learning-based optimization and support vector machine techniques},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hierarchical automatic phoneme recognition model for
hindi-devanagari consonants using machine learning technique.
<em>EXSY</em>, <em>40</em>(7), e13288. (<a
href="https://doi.org/10.1111/exsy.13288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A phoneme is perceptually the smallest distinct sound unit distinguished among words in a particular language. Every language has its own set of phonemes, and all the words are ordered sequences of phonemes. Therefore, phoneme recognition is essential to automatic speech recognition (ASR) systems. Phonemes of a language can be classified together using a single machine learning (ML) model through the direct classification (also known as the baseline or flat classification) approach. However, it is observed that the performance of such phoneme recognition degrades with the increase in the number of phoneme classes. The challenge is pronounced in languages with a larger number of phoneme classes, like Hindi, which has 48 phonemes. In this paper, we propose a speaker-independent hierarchical classification approach for 33 Hindi-Devanagari consonants/phonemes using cepstral features with ML techniques like support vector machine (SVM), random forest (RF) and fully connected deep neural network (DNN). In this hierarchical approach, a given phoneme is classified into successive subgroups until the particular phoneme class is identified. To perform the classification task, a binary or multi-class classifier is invoked for each internal (non-leaf) node in the hierarchy tree. Our model identified pairs of Optimal Feature Sets (based on mutual information ) and the best suitable ML classifier for each internal decision node in the hierarchy using 10-fold cross-validation to help in efficient classification. Our proposed hierarchical model leads to better accuracy and 57% improved performance for phoneme recognition compared with the non-hierarchical, that is, the direct classification approach.},
  archive      = {J_EXSY},
  author       = {Mousumi Malakar and Ravindra B. Keskar and Ajit Zadgaonkar},
  doi          = {10.1111/exsy.13288},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13288},
  shortjournal = {Expert Syst.},
  title        = {A hierarchical automatic phoneme recognition model for hindi-devanagari consonants using machine learning technique},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulation research on knowledge flow in a collaborative
innovation network. <em>EXSY</em>, <em>40</em>(7), e13280. (<a
href="https://doi.org/10.1111/exsy.13280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study takes diffusion capacity, absorptive capacity, and relationship strength as the main influencing factors, constructs models of knowledge flow in small-world networks and scale-free networks, and uses numerical simulation to observe the flow characteristics of explicit knowledge and tacit knowledge in different networks. The knowledge of explicit flow in different networks exhibits the phenomenon of knowledge emergence, but this phenomenon is more obvious in a small-world network. The flow of tacit knowledge in a small-world network has a better effect. In a scale-free network, the quantity and frequency of knowledge flow are significantly higher than those in a small-world network. The reason for this phenomenon is the differences in the response, connection and structure of different networks. The quantity and frequency of the flow of explicit knowledge in the same network are significantly higher than those of tacit knowledge. The reason for this phenomenon is the different types of knowledge flow in different modes, with different levels of flow difficulty and flow sustainability. First, this study visually compares the differences in flow between the two types of knowledge. Second, flow models of the two types of knowledge are constructed, and the flow characteristics of the two types of knowledge in different networks are simulated. Finally, the reasons for the differences in flow between the two types of knowledge are explained by using loosely coupled theory.},
  archive      = {J_EXSY},
  author       = {Yi Su and Xuesong Jiang},
  doi          = {10.1111/exsy.13280},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13280},
  shortjournal = {Expert Syst.},
  title        = {Simulation research on knowledge flow in a collaborative innovation network},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Age invariant face recognition using discriminative tensor
subspace learning with fuzzy based classification. <em>EXSY</em>,
<em>40</em>(7), e13278. (<a
href="https://doi.org/10.1111/exsy.13278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aging is a complex process that affects both the shape and texture of a face. In the aging domain, various methods have been proposed for age estimation or simulation of the face based on an image at a given age. Face recognition methods that are designed for automatic face recognition are still a challenging issue. We present a novel age invariant face recognition method using tensor subspace learning with fuzzy synthetic classification. Local Binary Pattern (LBP) processed face images are used to learn the tensor sub-space, which converts high dimensional feature space to age-invariants while retaining key elements of the local geometrical structure of the face. Tensor sub-space is better in terms of sub-space learning and feature extraction than Principal Component Analysis (PCA)/Linear Discriminant Analysis (LDA) that been used in many techniques in the literature. Tensor Normalized Face images are generated which exhibit maximum inter-class distances and minimum intra-class distances. Further local patches are used for synthesizing global Fuzzy membership scores to classify the test face images. Experiments performed on standard face-aging datasets, namely FG-NET, AGEDB, and MORPH-Album-II, and received accuracy of 99.15%, 99.20%, and 99.8%, respectively. Experimental results outperform the current state-of-the-art techniques, and results show the promise of the proposed system for personal identification despite the aging process. It also proved that the local descriptor gives better performance over the global descriptor like PCA for the aging process. The method also demonstrated improved performance as compared with compute-intensive methods that required training on deep networks.},
  archive      = {J_EXSY},
  author       = {Sonia Mittal and Mukesh Patel},
  doi          = {10.1111/exsy.13278},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13278},
  shortjournal = {Expert Syst.},
  title        = {Age invariant face recognition using discriminative tensor subspace learning with fuzzy based classification},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Atrial fibrillation detection using poincare geometry and
heart beat intervals. <em>EXSY</em>, <em>40</em>(7), e13277. (<a
href="https://doi.org/10.1111/exsy.13277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection of atrial fibrillation (AF) remains one of the major concerns in the field of medical research. AF is one of the main cause for stroke. AF is characterized by irregular heartbeats and absence of P waves in electrocardiogram (ECG)¬†signal. In this article, we propose a method, combining Poincare plot derived and RR interval-based features to classify given ECG signal into normal, AF and other rhythms. Classification and regression tree, K-nearest neighbor, support vector machine, artificial neural network, ResNet18, convolutional neural network (CNN)-long short term memory (LSTM) are implemented for classification of ECG signal. The class specific accuracies for the three rhythms are computed. Physionet challenge 2017 database is used for evaluation and testing of the developed algorithm. The database has 5154 normal, 771 AF, 2557 other rhythm and 46 noisy signals. Three Poincare derived features viz: SD1, SD2, and ratio of SD1 to SD2, three RR interval features viz: Mean stepping increment of RR interval, approximate entropy and sample entropy are computed and are given to classifiers. During fivefold cross-validation, CNN-LSTM classifier showed best result with class specific accuracy for normal of 96.65%, AF of 97.55%, other rhythms of 94.87%, overall accuracy of 96.17% and F 1 score of 0.9589. The developed technique can bring change in conventional practice in AF diagnosis and can aid the physician as an assisted tool.},
  archive      = {J_EXSY},
  author       = {S. K. Shrikanth Rao and Maheshkumar H. Kolekar and Roshan Joy Martis},
  doi          = {10.1111/exsy.13277},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13277},
  shortjournal = {Expert Syst.},
  title        = {Atrial fibrillation detection using poincare geometry and heart beat intervals},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-attribute group decision-making based on linguistic
pythagorean fuzzy copula extended power average operator. <em>EXSY</em>,
<em>40</em>(7), e13272. (<a
href="https://doi.org/10.1111/exsy.13272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims to propose a novel multi-attribute group decision-making (MAGDM) method based on linguistic Pythagorean fuzzy copula extended power average operator. Existing researches under linguistic Pythagorean fuzzy environment lack of the ability to handle with extreme values and the flexible operational rules. To fill these two gaps, this paper first provides the definition of Archimedean copula and co-copula operational rules under linguistic Pythagorean fuzzy environment, which can reflect the connection among arguments and provide more choices for experts to express their preferences. Then, we gather the extended power average (EPA) operator to present some new aggregation operators, which can reduce the negative influence of extreme evaluation values. To show the application of the proposed method to MAGDM problems, we apply it to handle a case of takeout O2O platform assessment problem. The numerical case and comparative analysis with other existing methods illustrate that our proposed method is more scientific and flexible.},
  archive      = {J_EXSY},
  author       = {Yaqing Kou and Jun Wang and Wuhuan Xu and Yuan Xu},
  doi          = {10.1111/exsy.13272},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13272},
  shortjournal = {Expert Syst.},
  title        = {Multi-attribute group decision-making based on linguistic pythagorean fuzzy copula extended power average operator},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Schweizer-sklar operations based hybrid aggregation operator
to dual hesitant q-rung orthopair fuzzy set and its application on
MCGDM. <em>EXSY</em>, <em>40</em>(7), e13257. (<a
href="https://doi.org/10.1111/exsy.13257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dual hesitant q -rung orthopair fuzzy (DH q -ROF) set appears as a powerful tool in compare to other variants of fuzzy sets to deal with uncertainties associated with available information in various real-life decision-making cases. In order to make DH q -ROF aggregation information process flexible, at first some operations viz., addition, multiplication, scalar multiplication, exponential laws based on Schweizer-Sklar class of t -conorms and t -norms are defined. Subsequently, using these operations, weighted average and geometric operators and ordered weighted average and geometric operators are introduced. But weighted average or geometric operators and ordered weighted average or geometric operators consider only the weight of the opinions and the weight of the ordered position of each given opinion respectively. To resolve weights of the arguments, hybrid aggregation operators viz., DH q -ROF Schweizer-Sklar hybrid averaging, DH q -ROF Schweizer-Sklar hybrid geometric operators are developed and their properties are discussed. Afterwards, a new method to deal with multicriteria group decision making problems under DH q -ROF environment is framed. To illustrate the proposed method a decision making problem related to investment company selection is considered and solved. To show the advantages of the proposed study, a comparative analysis among the developed and existing studies is discussed.},
  archive      = {J_EXSY},
  author       = {Souvik Gayen and Arun Sarkar and Animesh Biswas},
  doi          = {10.1111/exsy.13257},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13257},
  shortjournal = {Expert Syst.},
  title        = {Schweizer-sklar operations based hybrid aggregation operator to dual hesitant q-rung orthopair fuzzy set and its application on MCGDM},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human activity recognition based on multi-instance learning.
<em>EXSY</em>, <em>40</em>(7), e13256. (<a
href="https://doi.org/10.1111/exsy.13256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) is the process of classifying a person&#39;s actions, and it is an essential task for many human-centered applications. Multi-instance learning (MIL) is a special case of machine learning where the training examples are bags containing many instances, and a single class label is assigned for an entire bag of instances. In this study, we integrated these two concepts by introducing a novel approach: ‚Äúhuman activity recognition based on multi-instance learning‚Äù, called HAR-MIL. Unlike previous studies, the proposed HAR-MIL method represents human activities differently: as a bag of various wearable sensors (gyroscope, magnetometer, accelerometer, and linear acceleration). HAR-MIL presents an applicable and flexible model by providing multi-instance representation and eliminating the restrictions of traditional single-instance representation. Therefore, the adverse effects of missing data, defective sensors, and biased measurement on activity classification performance were minimized. This study is the first to investigate the performance of two MIL algorithms (SimpleMI and MIWrapper) on HAR. In this study, we explored the effect of four main factors (sensor positions, sensor types, base learners, and single or multiple participants) on the multi-instance representation of relevant human daily activities. The effectiveness of the proposed HAR-MIL method was demonstrated on 50 participant-based and sensor-position-based activity recognition datasets. The experimental results showed that HAR-MIL is effective for wearable sensor-based HAR with high classification accuracy (99.32%). Furthermore, the results showed that the proposed method outperformed the state-of-the-art methods by 10% on average on the same dataset.},
  archive      = {J_EXSY},
  author       = {Duygu Bagci Das and Derya Birant},
  doi          = {10.1111/exsy.13256},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13256},
  shortjournal = {Expert Syst.},
  title        = {Human activity recognition based on multi-instance learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach to detect fraud in ethereum transactions
using stacking. <em>EXSY</em>, <em>40</em>(7), e13255. (<a
href="https://doi.org/10.1111/exsy.13255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ever since Ether is launched as a digital currency, its rise has been rapid. It is currently the second most valuable digital currency in the world. There are more than 1 million transactions happening on the Ethereum network every day, and this number is expected to continue to increase. Due to the increasing number of transactions, fraudulent transactions have also increased, which has resulted in a large amount of money being lost and has also destroyed the livelihoods of many individuals. Due to their similarity to valid transactions, it is extremely difficult to distinguish between them. Additionally, Ethereum&#39;s pseudo-anonymity adds to the difficulty of identifying the parties involved. Since there are millions of transactions every day, it would be difficult to manually verify each one. Therefore, a mechanism for validating these transactions is needed. In this context, this paper proposes a novel approach to detecting fraudulent accounts associated with these transactions by implementing machine learning algorithms among the given set of transactions. We propose a framework for creating a stacking classifier by combining several standalone classification algorithms and creating a meta-learner based on the output of each base algorithm. The algorithms include Logistic Regression, Naive Bayes, Decision Trees, Random Forests, AdaBoosts, KNNs, SVMs, and Gradient Boosts. As a result of combining these algorithms, a powerful classifier with the ability to detect fraudulent transactions. A variety of machine learning models were trained and evaluated on the test set using various metrics. Based on the results of the individual algorithm the Random Forest algorithm achieved the highest accuracy of 95.47%, followed by Gradient Boosting at 94.61% which is an ensemble algorithm using the boosting technique. The Stacking classifier that combines Multinomial Naive Bayes and Random Forest as the base learners and logistic regression as the Meta learner achieved the highest accuracy of 97.18% with an F1 score of 97.02%. Based on the results of all the stacking models developed, it is concluded that algorithms tend to perform better when combined properly. When compared to the other approaches, the proposed approach has outperformed the others, making it feasible in the real world to detect fraudulent transactions.},
  archive      = {J_EXSY},
  author       = {Abdul Quadir Md and S. M. Satya Sree Narayanan and H. Sabireen and Arun Kumar Sivaraman and Kong Fah Tee},
  doi          = {10.1111/exsy.13255},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13255},
  shortjournal = {Expert Syst.},
  title        = {A novel approach to detect fraud in ethereum transactions using stacking},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EDL-NSGA-II: Ensemble deep learning framework with NSGA-II
feature selection for heart disease prediction. <em>EXSY</em>,
<em>40</em>(7), e13254. (<a
href="https://doi.org/10.1111/exsy.13254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the onset of pandemics and accrescent stress among people, heart disease has become one of the leading contributors to premature deaths worldwide. Early prognosis of this disorder is the most viable strategy for increasing the prospects of individuals&#39; survival. Numerous methods exploiting machine learning algorithms for heart disease prediction have been reported in the literature, but they all suffer from overfitting problems. Conspicuously, to improve the prediction accuracy, in this research work, an efficient meta-heuristic-based feature selection technique, namely NSGA-II, is employed. The proposed solution aims to reduce the feature set and thus improve the prediction accuracy supported by intelligent machine learning models. The presented classifier is trained and validated using a comprehensive heart disease dataset by utilizing NSGA-II selected features with 10-fold cross-validation. For performance validation, the results of the proposed model are compared with the state-of-the-art machine learning algorithms, such as a k-nearest neighbour, support vector machine, Bayesian belief networks, random forest, and naive bayes. The simulation results highlight the improved performance of the proposed model with NSGA-II and achieve a high prediction accuracy of 97.32%. Furthermore, results of sensitivity (92.84%), specificity (92.60%), precision (91.25%), and F -measure (92.17%) prove the utility of the proposed approach for heart disease prediction over all other variants.},
  archive      = {J_EXSY},
  author       = {Aditya Gupta and Amritpal Singh},
  doi          = {10.1111/exsy.13254},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13254},
  shortjournal = {Expert Syst.},
  title        = {EDL-NSGA-II: Ensemble deep learning framework with NSGA-II feature selection for heart disease prediction},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on soft computing approaches for predicting
maintainability of software: State-of-the-art, technical challenges, and
future directions. <em>EXSY</em>, <em>40</em>(7), e13250. (<a
href="https://doi.org/10.1111/exsy.13250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The software is changing rapidly with the invention of advanced technologies and methodologies. The ability to rapidly and successfully upgrade software in response to changing business requirements is more vital than ever. For the long-term management of software products, measuring software maintainability is crucial. The use of soft computing techniques for software maintainability prediction has shown immense promise in software maintenance process by providing accurate prediction of software maintainability. To better understand the role of soft computing techniques for software maintainability prediction, we aim to provide a systematic literature review of soft computing techniques for predicting software maintainability. Firstly, we provide a detailed overview of software maintainability. Following this, we explore the fundamentals of software maintainability and the reasons for adopting soft computing methodologies for predicting software maintainability. Later, we examine the soft computing approaches employed in the process of software maintainability prediction. Furthermore, we discuss the difficulties and potential solutions associated with the use of soft computing techniques in predicting maintainability of software. Finally, we conclude the review with some promising future directions to drive further research innovations and developments in this promising area. This systematic literature review provides a comprehensive overview of the soft computing strategies utilized for software maintainability for future researchers.},
  archive      = {J_EXSY},
  author       = {Gokul Yenduri and Thippa Reddy Gadekallu},
  doi          = {10.1111/exsy.13250},
  journal      = {Expert Systems},
  month        = {8},
  number       = {7},
  pages        = {e13250},
  shortjournal = {Expert Syst.},
  title        = {A review on soft computing approaches for predicting maintainability of software: State-of-the-art, technical challenges, and future directions},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How interesting and coherent are the stories generated by a
large-scale neural language model? Comparing human and automatic
evaluations of machine-generated text. <em>EXSY</em>, <em>40</em>(6),
e13292. (<a href="https://doi.org/10.1111/exsy.13292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluation of the narrative text generated by machines has traditionally been a challenge, particularly when attempting to evaluate subjective elements such as interest or believability. Recent improvements in narrative machine text generation have been largely driven by the emergence of transformer-based language models, trained on massive quantities of data, resulting in higher quality text generation. In this study, a corpus of stories is generated using the pre-trained GPT-Neo transformer model, with human-written prompts as inputs upon which to base the narrative text. The stories generated through this process are subsequently evaluated through both human evaluation and two automated metrics: BERTScore and BERT Next Sentence Prediction, with the aim of determining whether there is a correlation between the automatic scores and the human judgements. The results show variation in human evaluation results in comparison to modern automated metrics, suggesting further work is required to train automated metrics to identify text that is defined as interesting by humans.},
  archive      = {J_EXSY},
  author       = {Dominic Callan and Jennifer Foster},
  doi          = {10.1111/exsy.13292},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13292},
  shortjournal = {Expert Syst.},
  title        = {How interesting and coherent are the stories generated by a large-scale neural language model? comparing human and automatic evaluations of machine-generated text},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A modified lightweight DNA-based cryptography method for
internet of things devices. <em>EXSY</em>, <em>40</em>(6), e13270. (<a
href="https://doi.org/10.1111/exsy.13270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data generated by the applications of the internet of things (IoT) that transmit sensitive and secret information via wireless sensor networks (WSN) must be safeguarded as major components of any cyber physical system (CPS). Interconnected devices, such as IoT devices or objects are viewed as constrained because they lack adequate computational resources. These resources consist of energy, memory, and processing power, however, when attempting to encrypt transferred data using traditional encryption methods, these obstacles offered by IoT devices are viewed as a challenge because their limited resources may dramatically limit the potential and performance of CPS. Therefore, we introduced the modified DNA-based lightweight encryption method, which relies on the DNA sequence, as a way to get around the fact that IoT devices have insufficient computing processing and storage resources. The randomized structure of the DNA sequence was leveraged in the modified lightweight encryption method to obtain an encryption secret key. The secret key is employed to encrypt and decrypt data generated by IoT devices in a manner that is consistent with the computing capabilities of IoT devices during encryption operations involving substitution, transposition, and circular shifting to increase the confusion level of encrypted data. We also studied the effects of changing the size of the data block along with the key space of the modified DNA-based lightweight encryption method. Performance and experimental results in terms of key lengths, the amount of time it takes to encrypt, the degree of distortion, and the security level of the modified DNA-based lightweight encryption method surpass those of currently employed encryption algorithms.},
  archive      = {J_EXSY},
  author       = {Bassam AL-Shargabi and Ahmad Dar Assi},
  doi          = {10.1111/exsy.13270},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13270},
  shortjournal = {Expert Syst.},
  title        = {A modified lightweight DNA-based cryptography method for internet of things devices},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An automated epileptic seizure detection using optimized
neural network from EEG signals. <em>EXSY</em>, <em>40</em>(6), e13260.
(<a href="https://doi.org/10.1111/exsy.13260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the central nervous system (neurological) disorders, epilepsy is considered to be a dangerous and chronic disorder that causes recurring seizures, showing unusual behaviour for some period of time, shaking of hands and legs, loss of sensation and awareness. It occurs when electrical activities in the brain become abnormal. The test performed in order to detect epileptic seizures is known as electroencephalograph (EEG) signal test which is conducted by analysing the electrical impulses in the brain. The manual identification process of EEG brain signals is time consuming and a laborious task. So the neurologists may sometimes give a varying result which may affect the performance of detecting epileptic seizures. The combination of manual identification of the signal and machine learning technique will be able to provide a correct result in a confusion situation reducing analytic and therapeutic errors that are unavoidable in human clinical practice. The existing approaches of detection of epileptic seizures cannot correctly train their network if the size of the dataset is very large thus obtaining lower classification accuracy. An optimized neural network technique is proposed in this work for automatic detection of Epileptic seizure using EEG signals of the brain. The EEG signals are used for feature extraction by applying Discrete Wavelet Transform (DWT). Using the extracted features, input samples are created and then fed to a hybrid model which is a combination of self-organizing neural network (SONN) and multilayer perceptron (MLP) trained with a genetic algorithm (GA). This model acts as a classifier for detecting Epileptic seizures by classifying the samples into two classes namely Non-Epileptic and Epileptic class. Clustering is performed first for the large dataset samples using SONN and for each cluster, an MLP-GA network is used to train the samples belonging to the same cluster. Performing clustering helps to correctly train the MLP networks for obtaining high classification accuracy. GA has been specifically chosen as a learning algorithm instead of Backpropagation (BP) algorithm for MLP because GA can find the global minima thus solving the local minima problem faced by BP algorithm. The performance measures are evaluated thereby achieving precision value of 98%, recall value of 100%, F 1 score value of 98.99% and an overall accuracy of 99.2%. The proposed hybrid method using SONN and MLP-GA has more potential to classify the EEG signals.},
  archive      = {J_EXSY},
  author       = {Maibam Mangalleibi Chanu and Ngangbam Herojit Singh and Khelchandra Thongam},
  doi          = {10.1111/exsy.13260},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13260},
  shortjournal = {Expert Syst.},
  title        = {An automated epileptic seizure detection using optimized neural network from EEG signals},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subnetwork ensembling and data augmentation: Effects on
calibration. <em>EXSY</em>, <em>40</em>(6), e13252. (<a
href="https://doi.org/10.1111/exsy.13252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning models based on convolutional neural networks are known to be uncalibrated, that is, they are either overconfident or underconfident in their predictions. Safety-critical applications of neural networks, however, require models to be well-calibrated, and there are various methods in the literature to increase model performance and calibration. Subnetwork ensembling is based on the over-parametrization of modern neural networks by fitting several subnetworks into a single network to take advantage of ensembling them without additional computational costs. Data augmentation methods have also been shown to enhance model performance in terms of accuracy and calibration. However, ensembling and data augmentation seem orthogonal to each other, and the total effect of combining these two methods is not well-known; the literature in fact is inconsistent. Through an extensive set of empirical experiments, we show that combining subnetwork ensemble methods with data augmentation methods does not degrade model calibration.},
  archive      = {J_EXSY},
  author       = {A. √áaƒürƒ± Demir and Simon Caton and Pierpaolo Dondio},
  doi          = {10.1111/exsy.13252},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13252},
  shortjournal = {Expert Syst.},
  title        = {Subnetwork ensembling and data augmentation: Effects on calibration},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised short text classification with heterogeneous
graph neural networks. <em>EXSY</em>, <em>40</em>(6), e13249. (<a
href="https://doi.org/10.1111/exsy.13249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text classification has been a fundamental task in natural language processing, which benefits various applications, such as sentiment analysis, news tagging, and intent recommendation. However, classifying short texts is challenging due to the information sparsity in the text corpus. Besides, the performance of existing machine learning classification models largely relies on sufficient training data, yet labels can be scarce and expensive to obtain in real-world text classification scenarios. In this article, we propose a novel self-supervised short text classification method. Specifically, we first model the short text corpus as a heterogeneous graph to address the information sparsity problem. Then, we introduce a self-attention-based heterogeneous graph neural network model to learn short text embeddings. In addition, we adopt a self-supervised learning framework to exploit internal and external similarities among short texts. Experiments on five real-world short text benchmarks validate the effectiveness of our proposed method compared with the state-of-the-art methods.},
  archive      = {J_EXSY},
  author       = {Meng Cao and Jinliang Yuan and Hualei Yu and Baoming Zhang and Chongjun Wang},
  doi          = {10.1111/exsy.13249},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13249},
  shortjournal = {Expert Syst.},
  title        = {Self-supervised short text classification with heterogeneous graph neural networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of clustering large probabilistic graphs:
Techniques, evaluations, and applications. <em>EXSY</em>,
<em>40</em>(6), e13248. (<a
href="https://doi.org/10.1111/exsy.13248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the growth of uncertainty in the real world, analysing probabilistic graphs is crucial. Clustering is one of the most fundamental methods of mining probabilistic graphs to discover the hidden patterns in them. This survey examines an extensive and organized analysis of the clustering techniques of large probabilistic graphs proposed in the literature. First, the definition of probabilistic graphs and modelling them are introduced. Second, the clustering of such graphs and their challenges, such as uncertainty of edges, high dimensions, and the impossibility of applying certain graph clustering techniques directly, are expressed. Then, a taxonomy of clustering approaches is discussed in two main categories: threshold-based and possible worlds-based methods. The techniques presented in each category are explained and examined. Here, these methods are evaluated on real datasets, and their performance is compared with each other. Finally, the survey is summarized by describing some of the applications of probabilistic graph clustering and future research directions.},
  archive      = {J_EXSY},
  author       = {Malihe Danesh and Morteza Dorrigiv and Farzin Yaghmaee},
  doi          = {10.1111/exsy.13248},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13248},
  shortjournal = {Expert Syst.},
  title        = {A survey of clustering large probabilistic graphs: Techniques, evaluations, and applications},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classifying the heart sound signals using textural-based
features for an efficient decision support system. <em>EXSY</em>,
<em>40</em>(6), e13246. (<a
href="https://doi.org/10.1111/exsy.13246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases have surpassed cancer as the leading cause of death on the planet today. Numerous decision-making systems with computer-assisted support have been developed to assist cardiologists to detect heart disease, and thus, lowering the mortality rate. The purpose of this research is to classify audio signals received from the heart as normal or abnormal. The PhysioNet Computing in Cardiology (CinC) 2016 benchmark dataset, popularly known as PhysioNet 2016, has been used to validate the proposed methodology presented here. PhysioNet 2016 contains a total of 3200 phonocardiogram (PCG) recordings divided into sub-datasets A-F. The state-of-the-art studies conducted till date have not considered the harmonic details of the beat that can be extracted from its equivalent chromagram image. In this work, textural features such as linear binary pattern (LBP), adaptive-LBP, and ring-LBP have been extracted from the existing spectrogram and combined with the features extracted from the chromagram. It has been observed that the combination of features extracted from both the image variants has resulted in a greater accuracy as compared to the scenario where researchers were using only the spectrogram. The experiment yielded the mean accuracy, precision, and F1-score as 94.87, 93.11, and 95.273, respectively. The heart sound classification models employ spectrogram, scalogram, and mel-spectrogram images to view and analyse the acoustic properties of a PCG signal. Although these visual tools provide useful information about the signal, yet they are unable to distinguish between pitch and resonance in heart sound generation. However, this paper proposes an alternative approach of heart sound signal representation that allows for a more precise measure of pitch-related changes in the heart sound. Its results highlight the significance of extracting textural features from time-chroma representation (i.e., chromagram) of PCG signals that have not been explored yet in the domain related to classification of heart sound signals as normal and abnormal.},
  archive      = {J_EXSY},
  author       = {Kriti Taneja and Vinay Arora and Karun Verma},
  doi          = {10.1111/exsy.13246},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13246},
  shortjournal = {Expert Syst.},
  title        = {Classifying the heart sound signals using textural-based features for an efficient decision support system},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel preference-based artificial-bee-colony algorithm
approach to the land reallocation optimization problem in a land
consolidation case study: DOT village in turkey. <em>EXSY</em>,
<em>40</em>(6), e13245. (<a
href="https://doi.org/10.1111/exsy.13245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land consolidation (LC) is a widely used spatial land management tool. The most essential stage of the LC process is land reallocation, which comprises the exchange of property rights. For this reason, land reallocation may be a potential source of dissatisfaction among farmers, and thus, optimization techniques are continually being developed to minimize these dissatisfactions. The goal of this study was to develop a novel preference-based artificial-bee-colony (ABC) algorithm for use in the land reallocation project for DOT village. In the study, the farmer preferences were prioritized. Specifically, 70% of the lands were preference-based, with the remainder being random. Furthermore, the ABC algorithm&#39;s performance was assessed using three limit parameter (LP) values and two probability functions. The best success was attained when the LP was set to 50 and the roulette-wheel probability function was employed. The success of land reallocation was defined by the occupancy rate in the blocks and the reallocation rates based on the preferences of the farmer&#39;s lands. The occupancy rate in all the blocks using the ABC algorithm was 95.54%, the full occupancy rate of blocks according to farmer preferences was 38%, and the land reallocation rate based on the farmers&#39; preferences was 73%. The remaining lands were distributed to the block areas at random using the ABC method. Moreover, the success of the preference-based population surpassed that of randomly formed populations.},
  archive      = {J_EXSY},
  author       = {Abdurrahman √ñzbeyaz and Ya≈üar ƒ∞nceyol},
  doi          = {10.1111/exsy.13245},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13245},
  shortjournal = {Expert Syst.},
  title        = {A novel preference-based artificial-bee-colony algorithm approach to the land reallocation optimization problem in a land consolidation case study: DOT village in turkey},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial transformer network on skeleton-based gait
recognition. <em>EXSY</em>, <em>40</em>(6), e13244. (<a
href="https://doi.org/10.1111/exsy.13244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based gait recognition models suffer from the robustness problem, as the rank-1 accuracy varies from 90% in normal walking cases to 70% in walking with coats cases. In this work, we propose a state-of-the-art robust skeleton-based gait recognition model called Gait-TR, which is based on the combination of spatial transformer frameworks and temporal convolutional networks. Gait-TR achieves substantial improvements over other skeleton-based gait models with higher accuracy and better robustness on the well-known gait dataset CASIA-B. Particularly in walking with coats cases, Gait-TR gets a ‚àº90% accuracy rate. This result is higher than the best result of silhouette-based models, which usually have higher accuracy than the skeleton-based gait recognition models. Moreover, our experiment on CASIA-B shows that the spatial transformer network can extract gait features from the human skeleton better than the widely used graph convolutional network.},
  archive      = {J_EXSY},
  author       = {Cun Zhang and Xing-Peng Chen and Guo-Qiang Han and Xiang-Jie Liu},
  doi          = {10.1111/exsy.13244},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13244},
  shortjournal = {Expert Syst.},
  title        = {Spatial transformer network on skeleton-based gait recognition},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight convolutional neural network architecture design
for music genre classification using evolutionary stochastic
hyperparameter selection. <em>EXSY</em>, <em>40</em>(6), e13241. (<a
href="https://doi.org/10.1111/exsy.13241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have succeeded in various domains, including music information retrieval (MIR). Music genre classification (MGC) is one such task in the MIR that has gained attention over the years because of the massive increase in online music content. Accurate indexing and automatic classification of these large volumes of music content require high computational resources, which pose a significant challenge to building a lightweight system. CNNs are a popular deep learning-based choice for building systems for MGC. However, finding an optimal CNN architecture for MGC requires domain knowledge both in CNN architecture design and music. We present MGA-CNN, a genetic algorithm-based approach with a novel stochastic hyperparameter selection for finding an optimal lightweight CNN-based architecture for the MGC task. The proposed approach is unique in automating the CNN architecture design for the MGC task. MGA-CNN is evaluated on three widely used music datasets and compared with seven peer rivals, which include three automatic CNN architecture design approaches and four manually designed popular CNN architectures. The experimental results show that MGA-CNN surpasses the peer approaches in terms of classification accuracy, parameter numbers, and execution time. The optimal architectures generated by MGA-CNN also achieve classification accuracy comparable to the manually designed CNN architectures while spending fewer computing resources.},
  archive      = {J_EXSY},
  author       = {Yeshwant Singh and Anupam Biswas},
  doi          = {10.1111/exsy.13241},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13241},
  shortjournal = {Expert Syst.},
  title        = {Lightweight convolutional neural network architecture design for music genre classification using evolutionary stochastic hyperparameter selection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Some novel q-rung orthopair fuzzy similarity measures and
entropy measures with their applications. <em>EXSY</em>, <em>40</em>(6),
e13240. (<a href="https://doi.org/10.1111/exsy.13240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have introduced some new similarity measures for q -rung orthopair fuzzy sets and discussed their various features. These similarity measures have been constructed with the help of t -conorms. The suggested similarity measures are more effective than the available ones in computing the similarity between distinct q -rung orthopair fuzzy sets. With the help of these suggested similarity measures, we have also developed some novel entropy measures for -rung orthopair fuzzy sets. The newly proposed entropy measures are more effective than the existing ones in handling linguistic hedges. The applicability of the suggested similarity measures has been demonstrated in pattern analysis and the findings have been found to be better than due to existing ones. The traditional decision-making method namely, the technique for order preference by similarity to the ideal solution gives us a compromise solution that is either closest to the positive ideal solution and farthest from the negative ideal solution but not both. So, to overcome this major drawback, we suggest a novel multi-attribute decision-making method in the -rung orthopair fuzzy environment and apply it to a decision-making problem.},
  archive      = {J_EXSY},
  author       = {Abdul Haseeb Ganie and Surender Singh},
  doi          = {10.1111/exsy.13240},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13240},
  shortjournal = {Expert Syst.},
  title        = {Some novel q-rung orthopair fuzzy similarity measures and entropy measures with their applications},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stress recognition with multi-modal sensing using
bootstrapped ensemble deep learning model. <em>EXSY</em>,
<em>40</em>(6), e13239. (<a
href="https://doi.org/10.1111/exsy.13239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The factors that influence a person&#39;s mental health are numerous, interconnected, and multi-dimensional. Recognition of stress is one of the facets in developing the Mental Healthcare (MHC) system framework. With the advent of technology, smart wearable devices have paved a way to collect data in real-time to provide the cutting-edge reports about the individual. Due to the physiological sensors present in the smart wearable devices, it is now possible to have a robust system to recognize the stress of the smart wearable devices user thus consecutively leading to recognition of factors in leading to stress. However, the current MHC system for recognition and identification of stress have several drawbacks. First, stress recognition is mostly designed for a particular group of individuals like occupational stress, perinatal maternal stress, or health worker stress and fails to propose a framework that would not be targeted for a particular group of individual. Second, most of the previous work done on stress recognition focuses on the extraction of handcrafted features thus requiring human intervention and expertise. To address these issues, this study proposes, a hybrid deep learning based ensemble approach for automated extraction of features and classification into various state of stress for MHC system. The proposed framework takes input from wearable physiological sensors and is provided to deep learning classifier of convolutional neural network (CNN) and CNN-long short term memory based ensemble model. The proposed framework has been experimented on the wearable stress and affect detection dataset and reports an accuracy of 91.52% that is 7.20% higher than earlier reported accuracies from other machine learning and deep learning models.},
  archive      = {J_EXSY},
  author       = {Ghanapriya Singh and Orchid Chetia Phukan and Ravinder Kumar},
  doi          = {10.1111/exsy.13239},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13239},
  shortjournal = {Expert Syst.},
  title        = {Stress recognition with multi-modal sensing using bootstrapped ensemble deep learning model},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Label informed hierarchical transformers for sequential
sentence classification in scientific abstracts. <em>EXSY</em>,
<em>40</em>(6), e13238. (<a
href="https://doi.org/10.1111/exsy.13238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting scientific abstracts into discourse categories like background, objective, method, result, and conclusion is useful in many downstream tasks like search, recommendation and summarization. This task of classifying each sentence in the abstract into one of a given set of discourse categories is called sequential sentence classification. Existing machine learning-based approaches to this problem consider the content of only the abstract to obtain the neural representation of each sentence, which is then labelled with a discourse category. But this ignores the semantic information offered by the discourse labels themselves. In this paper, we propose LIHT, Label Informed Hierarchical Transformers ‚Äì a method for sequential sentence classification that explicitly and hierarchically exploits the semantic information in the labels to learn label-aware neural sentence representations. The hierarchical model helps to capture not only the fine-grained interactions between the discourse labels and the words in the abstract at the sentence level but also the potential dependencies that may exist in the label sequence. Thus, LIHT generates label-aware contextual sentence representations that are then labelled with a conditional random field. We evaluate LIHT on three publicly available datasets, namely, PUBMED-RCT, NICTA-PIBOSO and CSAbstract. The incremental gain in F1-score in all the three cases over the respective state-of-the-art approaches is around . Though the gains are modest, LIHT establishes a new performance benchmark for this task and is a novel technique of independent interest. We also perform an ablation study to identify the contribution of each component of LIHT in the observed performance, and a case study to visualize the roles of the different components of our model.},
  archive      = {J_EXSY},
  author       = {Yaswanth Sri Sai Santosh Tokala and Sai Saketh Aluru and Anoop Vallabhajosyula and Debarshi Kumar Sanyal and Partha Pratim Das},
  doi          = {10.1111/exsy.13238},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13238},
  shortjournal = {Expert Syst.},
  title        = {Label informed hierarchical transformers for sequential sentence classification in scientific abstracts},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting COVID-19 cases using dynamic time warping and
incremental machine learning methods. <em>EXSY</em>, <em>40</em>(6),
e13237. (<a href="https://doi.org/10.1111/exsy.13237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The investment of time and resources for developing better strategies is key to dealing with future pandemics. In this work, we recreated the situation of COVID-19 across the year 2020, when the pandemic started spreading worldwide. We conducted experiments to predict the coronavirus cases for the 50 countries with the most cases during 2020. We compared the performance of state-of-the-art machine learning algorithms, such as long-short-term memory networks, against that of online incremental machine learning algorithms. To find the best strategy, we performed experiments to test three different approaches. In the first approach (single-country), we trained each model using data only from the country we were predicting. In the second one (multiple-country), we trained a model using the data from the 50 countries, and we used that model to predict each of the 50 countries. In the third experiment, we first applied clustering to calculate the nine most similar countries to the country that we were predicting. We consider two countries to be similar if the differences between the curve that represents the COVID-19 time series are small. To do so, we used time series similarity measures (TSSM) such as Euclidean Distance (ED) and Dynamic Time Warping (DTW). TSSM return a real value that represents the distance between the points in two time series which can be interpreted as how similar they are. Then, we trained the models with the data from the nine more similar countries to the one that was predicted and the predicted one. We used the model ARIMA as a baseline for our results. Results show that the idea of using TSSM is a very effective approach. By using it with the ED, the obtained RMSE in the single-country and multiple-country approaches was reduced by 74.21% and 74.70%, respectively. And by using the DTW, the RMSE was reduced by 74.89% and 75.36%. The main advantage of our methodology is that it is very simple and fast to apply since it is only based on time series data, as opposed to more complex methodologies that require a deep and thorough study to consider the number of parameters involved in the spread of the virus and their corresponding values. We made our code public to allow other researchers to explore our proposed methodology.},
  archive      = {J_EXSY},
  author       = {Luis Miralles-Pechu√°n and Ankit Kumar and Andr√©s L. Su√°rez-Cetrulo},
  doi          = {10.1111/exsy.13237},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13237},
  shortjournal = {Expert Syst.},
  title        = {Forecasting COVID-19 cases using dynamic time warping and incremental machine learning methods},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of court decision from arabic documents using
deep learning. <em>EXSY</em>, <em>40</em>(6), e13236. (<a
href="https://doi.org/10.1111/exsy.13236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing amount of electronic legal documents represents a great opportunity for the development of intelligent computational systems for legal texts processing and classification. Most of these systems use classical machine learning and large datasets in English. This paper proposes an approach to automatically predict legal case outcome from written description of the events in Arabic using deep learning. An in-house corpus from the decisions of the Moroccan Court of Cassation is built and used to train a deep learning model that predicts judgement. As the created corpus is of limited size, a new data augmentation method is proposed to boost the prediction performance. Two settings for text representation are tested, namely FastText and GloVe embeddings, and multiple deep learning models architectures are tested. The proposed approach succeeds in predicting judicial decisions of the Moroccan Court of Cassation with an accuracy of 80.51% on six classes. Even with a small dataset, the proposed data augmentation method was helpful in improving the overall models&#39; performance. Despite the advancement in the area of legal judgement prediction over the years, this work is the first attempt to predict legal outcome using the documents of the Moroccan Cassation court. The corpus created in the context of this work will be made publicly available to the community.},
  archive      = {J_EXSY},
  author       = {Jihad Zahir},
  doi          = {10.1111/exsy.13236},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13236},
  shortjournal = {Expert Syst.},
  title        = {Prediction of court decision from arabic documents using deep learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An attention-based representation learning model for
multiple relational knowledge graph. <em>EXSY</em>, <em>40</em>(6),
e13234. (<a href="https://doi.org/10.1111/exsy.13234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph embedding models are used to learn low-dimensional representations of entities and relations in knowledge graphs. In this paper, we propose Multi-RAttE, an attention-based learning method for multiple relational knowledge graph embedding representation, which divides the information transfer in the knowledge graph into cross-relational information transfer and relation-specific information transfer, and divides the embedding of knowledge graph entities into structural embedding and multi-relational embedding for joint learning. To objectively analyse the performance of the Multi-RAttE model, we select two typical datasets and different representative baseline models for experimental evaluation on several tasks such as link prediction, multi-relation prediction and node classification. The experimental results show that the Multi-RAttE model improves 8% over the state-of-the-art model Composition-based Multi-Relational Graph Convolutional Networks (CompGCN) in terms of Hits@1 metric on the link prediction task on FB15k-237 dataset; on the multi-relation prediction task, the accuracy improves by 1.8% and 3.7% in the auc metric and F1 metric, respectively. The experimental results have proved that the Multi-RAttE model can effectively perform the representation of multiple relations.},
  archive      = {J_EXSY},
  author       = {Zhongming Han and Fuyu Chen and Hui Zhang and Zhiyu Yang and Wenwen Liu and Zequan Shen and Haitao Xiong},
  doi          = {10.1111/exsy.13234},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13234},
  shortjournal = {Expert Syst.},
  title        = {An attention-based representation learning model for multiple relational knowledge graph},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BWOA based metaheuristic approach for uncertain nonlinear
milling CNC machine system. <em>EXSY</em>, <em>40</em>(6), e13233. (<a
href="https://doi.org/10.1111/exsy.13233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scientific advances in the machining process inculcate intelligence in computer numerical control (CNC), to enhance reliability and quality production. The study made in this article has been directed towards the metaheuristic-based intelligent control of a nonlinear uncertain CNC machine for milling purposes. The presence of cutting force and frictional type constraints, along with parametric uncertainties, degrade the production quality in milling operations. Moreover, enhanced performance of milling operation can be achieved via intelligent control of position and velocity of machine table and tool. In this work, the black widow optimization algorithm (BWOA) driven proportional‚Äìintegral‚Äìderivative (PID) and PI controllers are employed for the position and the velocity of machine table and tool, respectively. Servo and regulatory are the two important control problems that are considered, analysed, and addressed in this work. The most challenging task of simultaneous tuning of PI and PID controllers is also addressed in this brief. Finally, a vivid comparative analysis of five state-of-the-art optimization techniques is also performed, and the obtained results demonstrate the superiority of the proposed control scheme. While dealing with the servo performance of x -axis, the proposed controller gives a 100% improvement in overshoot and 61.54% improvement in settling time, and for regulatory performance, it gives a 98.45% improvement in undershoot. Moreover, for z -axis control, the proposed controller gives a 100% improvement in overshoot and 61.54% improvement in settling time and for regulatory performance gives a 100% improvement in undershoot compared to published literature.},
  archive      = {J_EXSY},
  author       = {Pawan Kumar Pathak and Anil Kumar Yadav and Anshuman Shastri},
  doi          = {10.1111/exsy.13233},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13233},
  shortjournal = {Expert Syst.},
  title        = {BWOA based metaheuristic approach for uncertain nonlinear milling CNC machine system},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep feature fusion and selection-based retinal eye
disease detection from OCT images. <em>EXSY</em>, <em>40</em>(6),
e13232. (<a href="https://doi.org/10.1111/exsy.13232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coherence tomography (OCT) is one of the principal imaging modalities for retinal eye disease detection and classification. Different retinal eye diseases are the leading cause of blindness that can be overcome by early detection. However, ophthalmologists are currently carrying out retinal eye disease detection manually with the help of OCT images that may be erroneous and subjective. Different methods have been presented to automate the manual retinal eye disease detection process that needs further improvement in detection accuracy. This research proposed an automatic method for retinal eye disease detection and classification from OCT images using fusion and selection techniques. First, the modified-Alexnet and ResNet-50 are utilized for deep feature vector extraction. In the next step, these vectors are fused serially and rectified by the proposed feature selection framework and passed as input to different machine learning classifiers for retinal disease diagnosis. For this purpose, a publicly available dataset of retinal eye diseases with four classes is utilized. The proposed retinal eye disease detection method achieved an overall average accuracy index of greater than 99.95%, higher than the top one in the literature, that is, 99.39%. Experimental results authenticated that the proposed retinal eye disease detection methodology can reliably be used for automatic eye disease detection from OCT images. Furthermore, the proposed deep feature and selection-based retinal eye disease detection methodology achieved state-of-the-art performance.},
  archive      = {J_EXSY},
  author       = {Muhammad Junaid Umer and Muhammad Sharif and Mudassar Raza and Seifedine Kadry},
  doi          = {10.1111/exsy.13232},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13232},
  shortjournal = {Expert Syst.},
  title        = {A deep feature fusion and selection-based retinal eye disease detection from OCT images},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ranked soft sets. <em>EXSY</em>, <em>40</em>(6), e13231. (<a
href="https://doi.org/10.1111/exsy.13231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article defines ranked soft sets and establishes their fundamental theory. This new model of uncertain knowledge is a non-numerical yet powerful improvement of soft sets. The model relies on a qualitative improvement of the basic parameterized description posed by soft sets. We define relations between ranked soft sets and some existing models ( N -soft sets, fuzzy soft sets, probabilistic soft sets) that enhance the soft set spirit with the help of additional quantities. Primary contributions to their development include set-theoretic operations and representation theorems, at a theoretical level; and scores and aggregation operators, at a practical level. Finally we design a multi-person decision-making strategy for data in the form of ranked soft sets that takes advantage of these elements.},
  archive      = {J_EXSY},
  author       = {Gustavo Santos-Garc√≠a and Jos√© Carlos R. Alcantud},
  doi          = {10.1111/exsy.13231},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13231},
  shortjournal = {Expert Syst.},
  title        = {Ranked soft sets},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SBMYv3: Improved MobYOLOv3 a BAM attention-based approach
for obscene image and video detection. <em>EXSY</em>, <em>40</em>(6),
e13230. (<a href="https://doi.org/10.1111/exsy.13230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Countless cybercrime instances have shown the need for detecting and blocking obscene material from social media sites. Deep learning methods (DLMs) outperformed in recognizing obscene content flooded on many online platforms. However, these contemporary DLMs primarily treat the recognition of obscene content as a simple task of binary classification, rather than focusing on the labelling of obscene areas. Hence, many of these methods could not pay attention to the fact that misclassification samples are so diverse. Therefore, this paper focuses on two aspects (i) developing a deep learning model that could classify and label the obscene portion, and (ii) generating a labelled obscene image dataset with a wide variety of obscene samples to minimize the risks of inaccurate recognition. We have proposed a method named S3Pooling based bottleneck attention module (BAM) embedded MobileNetV2-YOLOv3 (SBMYv3) for automatic detection of obscene content using an attention mechanism and a suitable pooling strategy. The key contributions of our article are: (i) generation of a well-labelled obscene image dataset with a variety of augmentation strategies using Pix-2-Pix GAN (ii) modifications to the backend architecture of YOLOv3 using MobileNetV2 and BAM to ensure focused and accurate feature extraction, and (iii) selection of an optimal pooling strategy, that is, S3Pooling strategy, while taking the design of the feature extractor into account. The proposed SBMYv3 model outperformed other state-of-the-art models with 99.26% testing accuracy, 99.39% recall, 99.13% precision, and 99.13% IoU values respectively.},
  archive      = {J_EXSY},
  author       = {Sonali Samal and Yu-Dong Zhang and Thippa Reddy Gadekallu and Rajashree Nayak and Bunil Kumar Balabantaray},
  doi          = {10.1111/exsy.13230},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13230},
  shortjournal = {Expert Syst.},
  title        = {SBMYv3: Improved MobYOLOv3 a BAM attention-based approach for obscene image and video detection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating the structural graph-based model from a program
source-code using chaotic forrest optimization algorithm. <em>EXSY</em>,
<em>40</em>(6), e13228. (<a
href="https://doi.org/10.1111/exsy.13228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important and costly stages in software development is maintenance. Understanding the structure of software will make it easier to maintain it more efficiently. Clustering software modules is thought to be an effective reverse engineering technique for deriving structural models of software from source code. In software module clustering, the most essential objectives are to minimize connections between produced clusters, maximize internal connections within created clusters, and maximize clustering quality. Finding the appropriate software system clustering model is considered an NP-complete task. The previously proposed approaches&#39; key limitations are their low success rate, low stability, and poor modularization quality. In this paper, for optimal clustering of software modules, Chaotic based heuristic method using a forest optimization algorithm is proposed. The impact of chaos theory on the performance of the other SFLA-GA and PSO-GA has also been investigated. The results show that using the logistic chaos approach improves the performance of these methods in the software-module clustering problem. The performance of chaotic based FOA, SFLA-GA and PSO-GA is superior to the other heuristic methods in terms of modularization quality and stability of the results.},
  archive      = {J_EXSY},
  author       = {Bahman Arasteh and Reza Ghanbarzadeh and Farhad Soleimanian Gharehchopogh and Ali Hosseinalipour},
  doi          = {10.1111/exsy.13228},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13228},
  shortjournal = {Expert Syst.},
  title        = {Generating the structural graph-based model from a program source-code using chaotic forrest optimization algorithm},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smart contract-based resource allocation service mechanism
for internet of vehicles. <em>EXSY</em>, <em>40</em>(6), e13227. (<a
href="https://doi.org/10.1111/exsy.13227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential part of the intelligent transportation system, vehicular ad hoc network plays a vital role in the application of the Internet of Vehicles and security services. However, as the scale of the network continues to increase, trust-based access and data sharing between vehicles have also become essential security issues. Given how to allocate vehicle resources safely, efficiently and reliably, this paper proposes a blockchain-based key data sharing mechanism for the Internet of Vehicles. We first utilize certificateless public key cryptography to alleviate redundancy overhead and key escrow issues. Second, with the help of the security, decentralization, and non-tampering characteristics of the blockchain, two smart contracts are designed and used to realize vehicle identity management and data access control. The identity chain smart contract solves the security problem of vehicle identity management, and the data chain smart contract ensures reliable transaction verification and efficient resource management. At the same time, an auction method is proposed to facilitate vehicle resource transactions. The experimental results show that our solution can realize secure data sharing of the Internet of Vehicles and ensure the operating efficiency of the system. Regarding system overhead, the proposed scheme has 30.6% and 33% lower private key generation overhead and 3.3% and 20.1% lower resource transaction time than AFSN and BSM under the same conditions. In addition, using our bidding method in data chain smart contracts can have a regulatory effect on market data transactions. In the aspect of resource transaction efficiency, for the change of the number of buyers, the transaction completion rate and transaction time of the proposed scheme are better than those of the contrast scheme. When the number of buyers is 100, the transaction completion rate was 9.2% higher than that of AFSN and 17.3% higher than that of BAM. The transaction time of the scheme is 32.2% and 48.1% lower than that of AFSN and BSM, respectively.},
  archive      = {J_EXSY},
  author       = {Ming Mao and Peng Yi and Lilong Hou and Guanying Zhang},
  doi          = {10.1111/exsy.13227},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13227},
  shortjournal = {Expert Syst.},
  title        = {Smart contract-based resource allocation service mechanism for internet of vehicles},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep residual architectures and ensemble learning for
efficient brain tumour classification. <em>EXSY</em>, <em>40</em>(6),
e13226. (<a href="https://doi.org/10.1111/exsy.13226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prompt and accurate detection of brain tumours is essential for disease management and life-saving. This paper introduces an efficient and robust completely automated system for classifying the three prominent types of brain tumour. The aim is to contribute for enhanced classification accuracy with minimum pre-processing and less inference time. The power of deep networks is thoroughly investigated, with and without transfer learning. Fine-tuned deep Residual Networks (ResNets) with depth up to 101 are introduced to manage the complex nature of brain images, and to capture their microstructural information. The proposed residual architectures with their in-depth representations are evaluated and compared to other fine-tuned networks (AlexNet, GoogLeNet and VGG16). A novel Convolutional Network (ConvNet) built and trained from scratch is also proposed for tumour type classification. Proven models are integrated by combining their decisions using majority voting to obtain the final classification accuracy. Results show that the residual architectures can be optimized efficiently, and a noticeable accuracy can be gained with them. Although ResNet models are deeper than VGG16, they show lower complexity. Results also indicate that building ensemble of models is a successful strategy to enhance the system performance. Each model in the ensemble learns specific patterns with certain filters. This stochastic nature boosts the classification accuracy. The accuracies obtained from ResNet18, ResNet101, and the proposed ConvNet are 98.91%, 97.39% and 95.43%, respectively. The accuracy based on decision fusion for the three networks is 99.57%, which is better than those of all state-of-the-art techniques. The accuracy obtained with ResNet50 is 98.26%, and its fusion with ResNet18 and the designed network yields a 99.35% accuracy, which is also better than those of previous methods, meanwhile achieving minimum detection time requirements. Finally, visual representation of the learned features is provided to understand what the models have learned.},
  archive      = {J_EXSY},
  author       = {Hanaa S. Ali and Asmaa I. Ismail and El-Sayed M. El-Rabaie and Fathi E. Abd El-Samie},
  doi          = {10.1111/exsy.13226},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13226},
  shortjournal = {Expert Syst.},
  title        = {Deep residual architectures and ensemble learning for efficient brain tumour classification},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blockchain-based multi-layered federated extreme learning
networks in connected vehicles. <em>EXSY</em>, <em>40</em>(6), e13222.
(<a href="https://doi.org/10.1111/exsy.13222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent and networked vehicles help build an efficient vehicular network&#39;s infrastructure. The widespread use of electronic software exposes these networks to cyber-attacks. Intrusion detection systems (IDS) are useful for preventing vehicle network assaults. IDS have been customized using machine and deep learning networks for greater real-time performance. Current learning-based intrusion detection systems demand substantial processing capabilities to train and update intricate training models in vehicular devices, resulting in decreased efficiency and ability to defend against assaults. This study presents Blockchain-based Multi-Layer Federated Extreme Learning Machines (MLFEM) enabled IDS (BEF-IDS) for safe data transfers. The proposed IDS leverages federated learning to generate Multi-Layered Extreme Learning Machines, which are offloaded to dispersed vehicular edge devices such as Road-Side Units (RSU) and connected vehicles. This federated strategy decreases resource use without sacrificing security. Blockchain technology records and shares training models, assuring network security. Using real-time data sets, the suggested algorithm&#39;s performance under different attack scenarios were extensively tested. The suggested method obtained 98% accuracy and Recall, 97.9% Precision, and 97.9% F1 Score performance, which suggests it&#39;s incredibly secure and costs very little to transmit.},
  archive      = {J_EXSY},
  author       = {Durga Rajan and Poovammal Eswaran and Gautam Srivastava and Kadiyala Ramana and Celestine Iwendi},
  doi          = {10.1111/exsy.13222},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13222},
  shortjournal = {Expert Syst.},
  title        = {Blockchain-based multi-layered federated extreme learning networks in connected vehicles},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving and fine-grained data sharing for
resource-constrained healthcare CPS devices. <em>EXSY</em>,
<em>40</em>(6), e13220. (<a
href="https://doi.org/10.1111/exsy.13220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical cyber-physical systems (CPS) provide the possibility for real-time health monitoring of patients and flexible diagnostic services based on expert systems by collaboratively integrating and connecting various physical devices including sensors, terminals, and cloud infrastructure. However, the ubiquitous security threats in cyberspace have raised concerns about data security and user privacy. Although related works propose to protect data security and user privacy with cryptographic protocols, their heavy computational and storage overheads incur performance and battery life challenges for resource-constrained devices in the healthcare CPS. This article proposes an energy-saving and privacy-preserving data sharing (ESPPDS) scheme to address the challenge. ESPPDS inherits the anonymous fine-grained access control from attribute-based encryption (ABE) while protecting data integrity and supporting efficient user revocation. We also eliminate the repetitive computations of ciphertext components by utilizing the online/ offline encryption technology, and design a subtle and secure trick to delegate the decryption operations to the edge device, thereby reducing the computational overheads of the resource-constrained devices. We then show the security proof, and discuss the construction in the untrusted/ compromised server setting. The comparison and experiment indicate that ESPPDS is practical and more efficient than related schemes.},
  archive      = {J_EXSY},
  author       = {Yangyang Bao and Weidong Qiu and Xiaochun Cheng},
  doi          = {10.1111/exsy.13220},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13220},
  shortjournal = {Expert Syst.},
  title        = {Privacy-preserving and fine-grained data sharing for resource-constrained healthcare CPS devices},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of user‚Äôs car parking behaviour through twitter
hashtags. <em>EXSY</em>, <em>40</em>(6), e13218. (<a
href="https://doi.org/10.1111/exsy.13218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its ease and popularity, social media has recently become an essential source of data for researchers and various stakeholder groups that seek a reliable assessment of their policies based on a comprehensive understanding of users&#39; feedback and inputs, which are reflected in their posts and discussions. In this study, we investigate the issue of users&#39; car parking behaviour through a comprehensive analysis of related hashtags collected from the Twitter social media platform. For this purpose, we adopted a two-step strategy where in the first stage, a surface-level analysis of the identified hashtags involving inductive reasoning, sentiment analysis, and user interaction in terms of engagement and diversity scores is performed. In the second phase, a tweet content analysis is performed using sentiment analysis and Empath categorization with respect to the most frequent wordings (assimilating to separate topics), in the same spirit as aspect-sentiment analysis, to gain further insights regarding the occurrence of negative and positive posts. A quantitative evaluation of the coherence of the Empath categorization indicates that achieved 0.83% coherence score and outperformed both LDA and LSA that had a score of 0.77% and 0.76% respectively. Furthermore, the common word technique assimilated to aspect sentiment compared to the state-of-the-art model for aspect sentiment-based deberta-v3-base model. Besides, the influence of bots or spammers is evaluated using engagement/diversity measures and Botometer API. The results provide valuable insights in terms of discriminating between positive and negative posts and the correlation of surface-level analysis with content-based analysis, as well as the impact of various categorizations. The results expect to enable urban planners and policymakers to advance evidence-based policing in the future design of intelligent parking systems.},
  archive      = {J_EXSY},
  author       = {Nabil Arhab and Mourad Oussalah and Yazid Bounab},
  doi          = {10.1111/exsy.13218},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13218},
  shortjournal = {Expert Syst.},
  title        = {Analysis of user&#39;s car parking behaviour through twitter hashtags},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instance selection using one-versus-all and one-versus-one
decomposition approaches in multiclass classification datasets.
<em>EXSY</em>, <em>40</em>(6), e13217. (<a
href="https://doi.org/10.1111/exsy.13217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance is important in data analysis and mining; it filters out unrepresentative, redundant, or noisy data from a given training set to obtain effective model learning. Various instance selection algorithms are proposed in the literature, and their potential and applicability in data cleaning and preprocessing steps are demonstrated. For multiclass classification datasets, the existing instance selection algorithms must deal with all the instances across the different classes simultaneously to produce a reduced training set. Generally, every multiclass classification dataset can be regarded as a complex domain problem, which can be effectively solved using the divide-and-conquer principle. In this study, the one-versus-all (OVA) and one-versus-one (OVO) decomposition approaches were used to decompose a multiclass dataset into multiple binary class datasets. These approaches have been widely employed when constructing the classifier but have never been considered in instance selection. The results of instance selection performance obtained with the OVA, OVO, and baseline approaches were assessed and compared for 20 different domain multiclass datasets as the first study and five medical domain datasets as the validation study. Furthermore, three instance selection algorithms were compared, including IB3, DROP3, and GA. The results demonstrate that using the OVO approach to perform instance selection can make the support vector machine (SVM) and k-nearest neighbour (k-NN) classifiers perform significantly better than the OVA and baseline approaches in terms of the area under the ROC curve (AUC) rate, regardless of the instance selection algorithm used. Moreover, the OVO approach can provide reasonably good data reduction rates and processing times, which are all better than those of the OVA approach.},
  archive      = {J_EXSY},
  author       = {Ching-Lin Fang and Ming-Chang Wang and Chih-Fong Tsai and Wei-Chao Lin and Pei-Qi Liao},
  doi          = {10.1111/exsy.13217},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13217},
  shortjournal = {Expert Syst.},
  title        = {Instance selection using one-versus-all and one-versus-one decomposition approaches in multiclass classification datasets},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heuristics-based sequence labelling model for finding
educational domain acronym expansions. <em>EXSY</em>, <em>40</em>(6),
e13216. (<a href="https://doi.org/10.1111/exsy.13216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An acronym is a short form of the text used in many applications like Web search, Research and scientific documents, information retrieval, biomedical documents, etc. The main objective of the work is to develop a heuristics-based sequence labelling model to identify acronym expansion pairs in educational documents. The proposed system is implemented in four phases: (i) Data Collection (ii) Acronym Sequence Identification (iii) Expansion Determination and (iv) Acronym Expansion Validation. In this work, the acronym sequences identification is done by heuristic rules. Determining expansions is accomplished by a heuristics-based sequence labelling model. Again, the validation of acronym expansion sequences is also done by using heuristics rules. The result section shows that our proposed heuristics-based sequence labelling model achieved 98.4% accuracy in finding acronym expansion pairs from education research documents. The results are also compared with the acronym finder online acronym expansion repository and proved that more new acronym expansion sequences are identified by using our model.},
  archive      = {J_EXSY},
  author       = {Ramakrishnan Menaha and Veiravan Jayanthi},
  doi          = {10.1111/exsy.13216},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13216},
  shortjournal = {Expert Syst.},
  title        = {Heuristics-based sequence labelling model for finding educational domain acronym expansions},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curiously exploring affordance spaces of a pouring task.
<em>EXSY</em>, <em>40</em>(6), e13213. (<a
href="https://doi.org/10.1111/exsy.13213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human beings and other biological agents appear driven by curiosity to explore the affordances of their environments. Such exploration is its own reward ‚Äì children have fun when playing ‚Äì but it probably also serves the practical purpose of learning theories with which to predict outcomes of actions. Cognitive robots however have yet to match the performance of human beings at learning and reusing manipulation skills. In this paper, we implement a method that emulates the curiosity drive and uses it as a heuristic to guide (simulated) exploration of a particular task ‚Äì pouring liquids. The result of this exploration is a collection of symbolic rules linking qualitative descriptions of object arrangements and the pouring action with qualitative descriptions of likely outcomes. The manner in which qualitative descriptions of object arrangements and actions are converted to numerical descriptions for the purpose of simulation parametrization is via probability distributions, which themselves are adjusted in the process of simulated exploration. This allows the grounding of the symbolic descriptions to attempt to adapt itself to the task. The resulting symbolic rules form a theory that, together with the probability distributions that ground it in numerical parametrizations, is intended to be used to predict qualitative outcomes or select manners of pouring towards achieving a goal.},
  archive      = {J_EXSY},
  author       = {Mihai Pomarlan and Maria M. Hedblom and Robert Porzel},
  doi          = {10.1111/exsy.13213},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13213},
  shortjournal = {Expert Syst.},
  title        = {Curiously exploring affordance spaces of a pouring task},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dexterous robotic manipulation using deep reinforcement
learning and knowledge transfer for complex sparse reward-based tasks.
<em>EXSY</em>, <em>40</em>(6), e13205. (<a
href="https://doi.org/10.1111/exsy.13205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a deep reinforcement learning (DRL) approach that won Phase 1 of the Real Robot Challenge (RRC) 2021, and then extends this method to a more difficult manipulation task. The RRC consisted of using a TriFinger robot to manipulate a cube along a specified positional trajectory, but with no requirement for the cube to have any specific orientation. We used a relatively simple reward function, a combination of a goal-based sparse reward and a distance reward, in conjunction with Hindsight Experience Replay (HER) to guide the learning of the DRL agent (Deep Deterministic Policy Gradient [DDPG]). Our approach allowed our agents to acquire dexterous robotic manipulation strategies in simulation. These strategies were then deployed on the real robot and outperformed all other competition submissions, including those using more traditional robotic control techniques, in the final evaluation stage of the RRC. Here we extend this method, by modifying the task of Phase 1 of the RRC to require the robot to maintain the cube in a particular orientation, while the cube is moved along the required positional trajectory. The requirement to also orient the cube makes the agent less able to learn the task through blind exploration due to increased problem complexity. To circumvent this issue, we make novel use of a Knowledge Transfer (KT) technique that allows the strategies learned by the agent in the original task (which was agnostic to cube orientation) to be transferred to this task (where orientation matters). KT allowed the agent to learn and perform the extended task in the simulator, which improved the average positional deviation from 0.134 to 0.02‚Äâm, and average orientation deviation from 142¬∞ to 76¬∞ during evaluation. This KT concept shows good generalization properties and could be applied to any actor-critic learning algorithm.},
  archive      = {J_EXSY},
  author       = {Qiang Wang and Francisco Roldan Sanchez and Robert McCarthy and David Cordova Bulens and Kevin McGuinness and Noel O&#39;Connor and Manuel W√ºthrich and Felix Widmaier and Stefan Bauer and Stephen J. Redmond},
  doi          = {10.1111/exsy.13205},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13205},
  shortjournal = {Expert Syst.},
  title        = {Dexterous robotic manipulation using deep reinforcement learning and knowledge transfer for complex sparse reward-based tasks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial emotions recognition using local monotonic pattern
and grey level co-occurrence matrices images aided development.
<em>EXSY</em>, <em>40</em>(6), e13189. (<a
href="https://doi.org/10.1111/exsy.13189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, local monotonic pattern (LMP) paired with grey level co-occurrence matrix (GLCM) methods are suggested to identify facial emotions with a high identification rate even when the face pictures are rotated. The proposed method extracts image features using the properties of the LMP algorithm and features extracted from the GLCM, which are then fed into the Support Vector Machine (SVM) process that reduces the dimensionality of the features vector and classifies the output into different facial expressions or emotions. The SVM performance rate is then compared to the K-nearest neighbour approach (KNN) to see which method produces the best facial emotion identification and categorization. The study identified facial emotions in the images using advanced algorithms of GLCM and LMP models to be compared. As a result, the accuracy of SVM and KNN was utilized to determine the method&#39;s usefulness in classification using the application of MATLAB. A result of more than 93% was achieved using the SVM method compared with 89.4% using the KNN for the recognition process. The study also demonstrated that this approach would lead to more classification outcomes if the LMP and GLCM are combined with an edge-based technique yielding a new method that is more efficient and more effective.},
  archive      = {J_EXSY},
  author       = {Firas H. Almukhtar},
  doi          = {10.1111/exsy.13189},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13189},
  shortjournal = {Expert Syst.},
  title        = {Facial emotions recognition using local monotonic pattern and grey level co-occurrence matrices images aided development},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint one-sided synthetic unpaired image translation and
segmentation for colorectal cancer prevention. <em>EXSY</em>,
<em>40</em>(6), e13137. (<a
href="https://doi.org/10.1111/exsy.13137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has shown excellent performance in analysing medical images. However, datasets are difficult to obtain due privacy issues, standardization problems, and lack of annotations. We address these problems by producing realistic synthetic images using a combination of 3D technologies and generative adversarial networks. We propose CUT-seg, a joint training where a segmentation model and a generative model are jointly trained to produce realistic images while learning to segment polyps. We take advantage of recent one-sided translation models because they use significantly less memory, allowing us to add a segmentation model in the training loop. CUT-seg performs better, is computationally less expensive, and requires less real images than other memory-intensive image translation approaches that require two stage training. Promising results are achieved on five real polyp segmentation datasets using only one real image and zero real annotations. As a part of this study we release Synth-Colon, an entirely synthetic dataset that includes 20,000 realistic colon images and additional details about depth and 3D geometry: https://enric1994.github.io/synth-colon},
  archive      = {J_EXSY},
  author       = {Enric Moreu and Eric Arazo and Kevin McGuinness and Noel E. O&#39;Connor},
  doi          = {10.1111/exsy.13137},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13137},
  shortjournal = {Expert Syst.},
  title        = {Joint one-sided synthetic unpaired image translation and segmentation for colorectal cancer prevention},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implementation of sliding mode backstepping controller for
boost converter in real-time for LED application. <em>EXSY</em>,
<em>40</em>(6), e13095. (<a
href="https://doi.org/10.1111/exsy.13095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes the real-time implementation of a sliding mode backstepping controller for boost converters for LED lighting applications. The sliding surface is used to calculate the first error value in the backstepping control procedure, which ensures the system&#39;s robustness over a wide range of disturbances by achieving an asymptotically stable system based on the Lyapunov function. Due to the non-minimum phase nature of boost converter, the system will become unstable. Hence the output voltage is indirectly controlled by controlling the sliding surface in the proposed sliding mode backstepping controller in order to achieve the asymptotically stable system. The detailed simulation is carried out on the MATLAB/Simulink platform, and the results are compared to that of a conventional proportional-integral-derivative (PID) controller and a traditional sliding mode controller, which ensures system robustness across a wide range of load resistance and input voltage changes, resulting in enhanced transient and, steady-state responses. In the laboratory, a prototype of proposed controller is designed to ensure its accuracy in real-time. By comparing the proposed control technique to the conventional sliding mode controller and PID controller, the flexibility of the proposed control method is validated for any variation at the input and output sides. From the experimental results, it is apparent that the proposed controller responds accurately and rapidly regardless of the disturbances caused to the system.},
  archive      = {J_EXSY},
  author       = {Rajesh Narayan Deo and Ashish Shrivastava and Kalyan Chatterjee},
  doi          = {10.1111/exsy.13095},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13095},
  shortjournal = {Expert Syst.},
  title        = {Implementation of sliding mode backstepping controller for boost converter in real-time for LED application},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Threshold multiparty multi-randomness secure partially
homomorphic encryption for data security in cloud. <em>EXSY</em>,
<em>40</em>(6), e13043. (<a
href="https://doi.org/10.1111/exsy.13043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threshold cryptography is used to secure a cryptographic secret by dividing it into N shares and storing each claim on a diverse server. Any subgroup of t servers can use the secret without re-making it. However, an adversary cannot recover the secret by compromising t ‚àí¬†1 servers. Homomorphic encryption operates directly on the ciphertext. In this study, a Threshold multiparty multi-randomness secure partially homomorphic encryption (TM-MR-SPHE) algorithm is proposed to secure the outsourced data and perform multiplication and division operations on the ciphertext. The model is implemented in Eclipse IDE and AWS Toolkit for Eclipse and deployed in Amazon Elastic Beanstalk (EB) environment. This model is mainly used to secure the patient e-health details and perform computation on outsourced data. The patient details are encrypted by the algorithm MR-SPHE and uploaded in AWS (Amazon Web Service) S3 bucket. AWS identity and access management (IAM) creates the users&#39; service and secret shares among multiple users in the EB environment. The proposed model performance is studied by comparing with other partially homomorphic Elgamal, Pailler, and Benaloh. This model achieves data integrity and confidentiality using the threshold multiparty with multi-randomness secure partially homomorphic encryption.},
  archive      = {J_EXSY},
  author       = {M D Boomija and S V Kasmir Raja},
  doi          = {10.1111/exsy.13043},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e13043},
  shortjournal = {Expert Syst.},
  title        = {Threshold multiparty multi-randomness secure partially homomorphic encryption for data security in cloud},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Architecture of an effective convolutional deep neural
network for segmentation of skin lesion in dermoscopic images.
<em>EXSY</em>, <em>40</em>(6), e12689. (<a
href="https://doi.org/10.1111/exsy.12689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of dermoscopic-based skin lesion images is considered to be challenging owing to various factors. Some of the most tangible reasons include poor contrast near the affected skin lesion, the fuzzy and unpredictable lesion limits, the presence of variations in noise, and capturing images under different conditions. This paper aims to develop an efficient segmentation model for dermoscopic images of different skin lesions based on deep learning. This paper proposes the 11-layer convolutional deep neural network with two segmentation models trained from start to finish and do not depend on any previous information about the data. The viability, efficiency, and speculation ability of the models are evaluated on the ISIC2018 database. The proposed model achieves 0.903 accuracy and 0.820 Jaccard index in the segmentation of skin lesions. The model shows better performance compared to other image segmentation techniques from the leaderboards of ISIC2018 using deep learning.},
  archive      = {J_EXSY},
  author       = {Ginni Arora and Ashwani Kumar Dubey and Zainul Abdin Jaffery and Alvaro Rocha},
  doi          = {10.1111/exsy.12689},
  journal      = {Expert Systems},
  month        = {7},
  number       = {6},
  pages        = {e12689},
  shortjournal = {Expert Syst.},
  title        = {Architecture of an effective convolutional deep neural network for segmentation of skin lesion in dermoscopic images},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An experimental study measuring the generalization of
fine-tuned language representation models across commonsense reasoning
benchmarks. <em>EXSY</em>, <em>40</em>(5), e13243. (<a
href="https://doi.org/10.1111/exsy.13243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last 5‚Äâyears, language representation models, such as BERT and GPT-3, based on transformer neural networks, have led to enormous progress in natural language processing (NLP). One such NLP task is commonsense reasoning, where performance is usually evaluated through multiple-choice question answering benchmarks. Till date, many such benchmarks have been proposed, and ‚Äòleaderboards‚Äô tracking state-of-the-art performance on those benchmarks suggest that transformer-based models are approaching human-like performance. Because these are commonsense benchmarks, however, such a model should be expected to generalize, that is, at least in aggregate, should not exhibit excessive performance loss across independent commonsense benchmarks regardless of the specific benchmark on (the training set of) which it has been fine-tuned. In this article, we evaluate this expectation by proposing a methodology and experimental study to measure the generalization ability of language representation models using a rigorous and intuitive metric. Using five established commonsense reasoning benchmarks, our experimental study shows that the models do not generalize well, and may be (potentially) susceptible to issues such as dataset bias. The results therefore suggest that current performance on benchmarks may be an over-estimate, especially if we want to use such models on novel commonsense problems for which a ‚Äòtraining‚Äô dataset may not be available, for the language representation model, to fine-tune on.},
  archive      = {J_EXSY},
  author       = {Ke Shen and Mayank Kejriwal},
  doi          = {10.1111/exsy.13243},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13243},
  shortjournal = {Expert Syst.},
  title        = {An experimental study measuring the generalization of fine-tuned language representation models across commonsense reasoning benchmarks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing CNN-LSTM hybrid classifier using HCA for
biomedical image classification. <em>EXSY</em>, <em>40</em>(5), e13235.
(<a href="https://doi.org/10.1111/exsy.13235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical science, imaging is the most effective diagnostic and therapeutic tool. Almost all modalities have transitioned to direct digital capture devices, which have emerged as a major future healthcare option. Three diseases such as Alzheimer&#39;s (AD), Haemorrhage (HD), and COVID-19 have been used in this manuscript for binary classification purposes. Three datasets (AD, HD, and COVID-19) were used in this research out of which the first two, that is, AD and HD belong to brain Magnetic Resonance Imaging (MRI) and the last one, that is, COVID-19 belongs to Chest X-Ray (CXR) All of the diseases listed above cannot be eliminated, but they can be slowed down with early detection and effective medical treatment. This paper proposes an intelligent method for classifying brain (MRI) and CXR images into normal and abnormal classes for the early detection of AD, HD, and COVID-19 based on an ensemble deep neural network (DNN). In the proposed method, the convolutional neural network (CNN) is used for automatic feature extraction from images and long-short term memory (LSTM) is used for final classification. Moreover, the Hill-Climbing Algorithm (HCA) is implemented for finding the best possible value for hyper parameters of CNN and LSTM, such as the filter size of CNN and the number of units of LSTM while fixing the other parameters. The data-set is pre-processed (resized, cropped, and noise removed) before feeding the train images to the proposed models for accurate and fast learning. Forty-five MR images of AD, Sixty MR images of HD, and 600 CXR images of COVID-19 were used for testing the proposed model ‚ÄòCNN-LSTM-HCA‚Äô. The performance of the proposed model is evaluated using six types of statistical assessment metrics such as; Accuracy, Sensitivity, Specificity, F-measure, ROC, and AUC. The proposed model compared with the other three types of hybrid models such as CNN-LSTM-PSO, CNN-LSTM-Jaya, and CNN-LSTM-GWO and also with state-of-art techniques. The overall accuracy of the proposed model received was 98.87%, 85.75%, and 99.1% for COVID-19, Haemorrhage, and Alzheimer&#39;s data sets, respectively.},
  archive      = {J_EXSY},
  author       = {Ashwini Kumar Pradhan and Kaberi Das and Debahuti Mishra and Premkumar Chithaluru},
  doi          = {10.1111/exsy.13235},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13235},
  shortjournal = {Expert Syst.},
  title        = {Optimizing CNN-LSTM hybrid classifier using HCA for biomedical image classification},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robot facilitated rehabilitation of children with autism
spectrum disorder: A 10 year scoping review. <em>EXSY</em>,
<em>40</em>(5), e13204. (<a
href="https://doi.org/10.1111/exsy.13204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Manu Kohli and Arpan Kumar Kar and Shuchi Sinha},
  doi          = {10.1111/exsy.13204},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13204},
  shortjournal = {Expert Syst.},
  title        = {Robot facilitated rehabilitation of children with autism spectrum disorder: A 10 year scoping review},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An evolving feature weighting framework for radial basis
function neural network models. <em>EXSY</em>, <em>40</em>(5), e13201.
(<a href="https://doi.org/10.1111/exsy.13201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Via Granular Computing (GrC), one can create effective computational frameworks for obtaining information from data, motivated by the human perception of combining similar objects. Combining knowledge gained via GrC with a Fuzzy inference engine (Neural-Fuzzy) enable us to develop a transparent system. While weighting variables based on their importance during the iterative data granulation process has been proposed before (W-GrC), there is no work in the literature to demonstrate effectiveness and impact on Type-2 Fuzzy Logic systems (T2-FLS). The main contribution of this paper is to extend W-GrC, for the first time, to both Type-1 and Type-2 models known as Radial Basis Function Neural Network (RBFNN) and General Type-2 Radial Basis Function Neural Network (GT2-RBFNN). The proposed framework is validated using popular datasets: Iris, Wine, Breast Cancer, Heart and Cardiotocography. Results show that with the appropriate selection of feature weight parameter, the new computational framework achieves better classification accuracy outcomes. In addition, we also introduce in this research work an investigation on the modelling structure&#39;s interpretability (via Nauck&#39;s index) where it is shown that a good balance of interpretability and accuracy can be maintained.},
  archive      = {J_EXSY},
  author       = {Muhammad Zaiyad Muda and Adrian R. Solis and George Panoutsos},
  doi          = {10.1111/exsy.13201},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13201},
  shortjournal = {Expert Syst.},
  title        = {An evolving feature weighting framework for radial basis function neural network models},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social network analytics and visualization: Dynamic
topic-based influence analysis in evolving micro-blogs. <em>EXSY</em>,
<em>40</em>(5), e13195. (<a
href="https://doi.org/10.1111/exsy.13195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence Analysis is one of the well-known areas of Social Network Analysis. However, discovering influencers from micro-blog networks based on topics has gained recent popularity due to its specificity. Besides, these data networks are massive, continuous and evolving. Therefore, to address the above challenges we propose a dynamic framework for topic modelling and identifying influencers in the same process. It incorporates dynamic sampling, community detection and network statistics over graph data stream from a social media activity management application. Further, we compare the graph measures against each other empirically and observe that there is no evidence of correlation between the sets of users having large number of friends and the users whose posts achieve high acceptance (i.e., highly liked, commented and shared posts). Therefore, we propose a novel approach that incorporates a user&#39;s reachability and also acceptability by other users. Consequently, we improve on graph metrics by including a dynamic acceptance score (integrating content quality with network structure) for ranking influencers in micro-blogs. Additionally, we analysed the topic clusters&#39; structure and quality with empirical experiments and visualization.},
  archive      = {J_EXSY},
  author       = {Shazia Tabassum and Jo√£o Gama and Paulo J. Azevedo and Mario Cordeiro and Carlos Martins and Andre Martins},
  doi          = {10.1111/exsy.13195},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13195},
  shortjournal = {Expert Syst.},
  title        = {Social network analytics and visualization: Dynamic topic-based influence analysis in evolving micro-blogs},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous driving planning and decision making based on
game theory and reinforcement learning. <em>EXSY</em>, <em>40</em>(5),
e13191. (<a href="https://doi.org/10.1111/exsy.13191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving technology is one of the important methods that avoid the hidden dangers of traffic safety. Although the existing autonomous driving technology can meet some needs of real traffic scenarios, the ability of multi-agents to effectively generate autonomous driving strategy in complex traffic environments remains to be improved. Aiming at this problem, the automatic drive model based on game theory and reinforcement learning is proposed by combining these two technologies and applying them in multi-agent cooperative driving, which enables multi-agents to carry out strategic reasoning with negotiation in traffic scenarios by extending the game description language, and puts forward the constrained multi-agents deep deterministic policies gradient algorithm. Finally, the related experiments are conducted through the autonomous driving simulation platform, and the experimental results show that the proposed model can effectively generate driving strategies for multi-agents in complex traffic environments, which verifies the validity and feasibility of the proposed model, and provides the general research basis for multi-agents autonomous driving.},
  archive      = {J_EXSY},
  author       = {Weiping Duan and Zhongyi Tang and Wei Liu and Hongbiao Zhou},
  doi          = {10.1111/exsy.13191},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13191},
  shortjournal = {Expert Syst.},
  title        = {Autonomous driving planning and decision making based on game theory and reinforcement learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal positional lexicon expansion for federated learning
based on hyperpatism detection. <em>EXSY</em>, <em>40</em>(5), e13183.
(<a href="https://doi.org/10.1111/exsy.13183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet-based information exchange has resulted in the propagation of false and misleading information, which is highly detrimental to individuals and humankind. Due to the speed and volume of social media news production, supervised artificial intelligence algorithms require many annotated data, which is difficult, costly, and time-consuming. To address this issue, we offer a novel federated semi-supervised framework based on self-ensembling that utilizes the linguistic and stylometric information of annotated news articles and searches for hidden patterns in unlabeled data to denoise labels. Self-ensembling predicts the labels of unlabeled data by using the outcomes of network-in-training from earlier epochs. These cumulative predictions should be a stronger predictor for unknown labels than the output of the most recent training epoch; hence, they may be utilized as a substitute for the labels of unlabeled data. The approach is distinctive in collecting all of the outputs from the neural network&#39;s past training periods. It utilizes them as an unsupervised target against which to assess the current output prediction of unlabeled articles. We intend to create a dataset centred on denoising to forward the study. The dataset is mapped using (1) the shifting focus time from published news articles and (2) the semi-supervised method based on coincidence contexts for a neural contrast embedding model for learning low-dimensional continuous vectors that generate a focus time-based query in sequential news articles for temporal comprehension. The model achieved 0.83% F-measure with lexicon expansion semi-supervised learning.},
  archive      = {J_EXSY},
  author       = {Usman Ahmed and Jerry Chun-Wei Lin and Gautam Srivastava},
  doi          = {10.1111/exsy.13183},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13183},
  shortjournal = {Expert Syst.},
  title        = {Temporal positional lexicon expansion for federated learning based on hyperpatism detection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biomedical knowledge graph embeddings for personalized
medicine: Predicting disease-gene associations. <em>EXSY</em>,
<em>40</em>(5), e13181. (<a
href="https://doi.org/10.1111/exsy.13181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized medicine is a concept that has been subject of increasing interest in medical research and practice in the last few years. However, significant challenges stand in the way of practical implementations, namely in regard to extracting clinically valuable insights from the vast amount of biomedical knowledge generated in the last few years. Here, we describe an approach that uses Knowledge Graph Embedding (KGE) methods on a biomedical Knowledge Graph (KG) as a path to reasoning over the wealth of information stored in publicly accessible databases. We built a Knowledge Graph using data from DisGeNET and GO, containing relationships between genes, diseases and other biological entities. The KG contains 93,657 nodes of 5 types and 1,705,585 relationships of 59 types. We applied KGE methods to this KG, obtaining an excellent performance in predicting gene-disease associations (MR 0.13, MRR 0.96, HITS@1 0.93, HITS@3 0.99, and HITS@10 0.99). The optimal hyperparameter set was used to predict all possible novel gene-disease associations. An in-depth analysis of novel gene-disease predictions for disease terms related to Autism Spectrum Disorder (ASD) shows that this approach produces predictions consistent with known candidate genes and biological pathways and yields relevant insights into the biology of this paradigmatic complex disorder.},
  archive      = {J_EXSY},
  author       = {Joana Vilela and Muhammad Asif and Ana Rita Marques and Jo√£o Xavier Santos and C√©lia Rasga and Astrid Vicente and Hugo Martiniano},
  doi          = {10.1111/exsy.13181},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13181},
  shortjournal = {Expert Syst.},
  title        = {Biomedical knowledge graph embeddings for personalized medicine: Predicting disease-gene associations},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel YOLOv4-modified approach for efficient object
detection in satellite imagery. <em>EXSY</em>, <em>40</em>(5), e13180.
(<a href="https://doi.org/10.1111/exsy.13180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpreting high-resolution satellite imagery could be an expensive and time-consuming task for human eyes. Computer Vision and Deep Learning techniques can help to solve this major problem by applying detection algorithms, which can ease the task of analysing such images for the benefit of humans. It can help in changing the way we comprehend and anticipate the economic activity around the world. Such techniques help us to observe the urban development in high security areas such as national and international borders. Constant progressions in improving and making satellites deployment, a cost-effective process to strengthen the networks of satellite orbiting the earth is one of the reasons such tasks can be easily solved with the help of high-resolution images. Current computer vision research works have achieved significant milestones in accuracy and speed but, there are still room for improvements. In this paper, we addressed some of these methods to bring them to a combined pipeline and proposed a set of improvements to further improve the speed and the accuracy of the detections. We proposed a unified framework, which combines several object detection algorithms and the state-of-art architecture of YoloV4 along with the TensorFlow object detection API. This framework can detect small and well as large objects with improved speed and accuracy by using two detectors for different scales. Evaluation ran on these high-resolution images yield mAP of 85.6% F1-score of 0.84.},
  archive      = {J_EXSY},
  author       = {Rishabh Tiwari and Ashwani Kumar Dubey and Alvaro Rocha},
  doi          = {10.1111/exsy.13180},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13180},
  shortjournal = {Expert Syst.},
  title        = {A novel YOLOv4-modified approach for efficient object detection in satellite imagery},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of china‚Äôs regional innovation capability based
on simulated annealing projection pursuit model and nested fuzzy
evaluation model. <em>EXSY</em>, <em>40</em>(5), e13179. (<a
href="https://doi.org/10.1111/exsy.13179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regional innovation capability is an important indicator of both regional innovative and long-term development. The purpose of this study is to build an evaluation index system for regional innovation capability in order to identify regional differences and support innovation more effectively. After establishing a reasonable evaluation value for regional innovation capability, a combination of simulated annealing optimized projection pursuit (SA‚ÄìPP) and N-layer nested fuzzy comprehensive evaluation models is used to assess China&#39;s regional innovation capabilities. The results show that the SA‚ÄìPP model effectively mitigates the risk of erroneous evaluation results caused by index weight uncertainty, resulting in a more reasonable, robust, and intelligent assessment of regional innovation capability. Furthermore, the nested fuzzy comprehensive evaluation model is capable of easily resolving the evaluation factor set&#39;s heterogeneity and multilayer problems. The most significant influences on China&#39;s regional innovation capability are knowledge acquisition and enterprise innovation. The comprehensive score of the proposed combinational evaluation model manifests that provinces with strong regional innovation capabilities are mainly concentrated in the southeast coastal regions. The research results allow for precise weight determination and object ranking.},
  archive      = {J_EXSY},
  author       = {Mina Ge and Hualiang Lin},
  doi          = {10.1111/exsy.13179},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13179},
  shortjournal = {Expert Syst.},
  title        = {Evaluation of china&#39;s regional innovation capability based on simulated annealing projection pursuit model and nested fuzzy evaluation model},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel approach for quantification for severity estimation of
blight diseases on leaves of tomato plant. <em>EXSY</em>,
<em>40</em>(5), e13174. (<a
href="https://doi.org/10.1111/exsy.13174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study uses digital image processing and machine learning to quantify the infection patterns on tomato leaves due to blight diseases. Quantification, also known as severity measurement, is a technique to determine how much a leaf is diseased by calculating a numeric value. This value could be a fraction representing how much the diseased region is present on the leaf compared to the entire leaf region, or it can be a percentage value too. There are two main approaches to measuring disease severity; the first technique involves visual estimation using references like standard area diagrams. The second approach involves taking a digital image of the leaf, separating the diseased regions from the healthy regions, and then calculating the area of those two regions. The approach we took is similar. We first took the digital image and segmented the diseased and healthy regions. For quantification, we calculated the ratio of total pixels representing the diseased region to the total number of pixels representing the leaf. While finding ways to improve the accuracy of the segmentation algorithm, we also discovered our segmentation technique which automatically segments the diseased regions of the leaves from the healthy areas using k-means clustering. The clustering-segmentation algorithm did give good results for the sample images to which it was applied. The main thing about the clustering-segmentation algorithm is that it tends to be automatic compared to some of the semi-automatic segmentation approaches that have been discovered till now. We could reproduce the validated quantification results as other authors achieved in the recent work, which also validated our methodology.},
  archive      = {J_EXSY},
  author       = {Aahan Singh Charak and Aditya Sinha and Tarun Jain},
  doi          = {10.1111/exsy.13174},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13174},
  shortjournal = {Expert Syst.},
  title        = {Novel approach for quantification for severity estimation of blight diseases on leaves of tomato plant},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated learning based covid-19 detection. <em>EXSY</em>,
<em>40</em>(5), e13173. (<a
href="https://doi.org/10.1111/exsy.13173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world is affected by COVID-19, an infectious disease caused by the SARS-CoV-2 virus. Tests are necessary for everyone as the number of COVID-19 affected individual&#39;s increases. So, the authors developed a basic sequential CNN model based on deep and federated learning that focuses on user data security while simultaneously enhancing test accuracy. The proposed model helps users detect COVID-19 in a few seconds by uploading a single chest X-ray image. A deep learning-aided architecture that can handle client and server sides efficiently has been proposed in this work. The front-end part has been developed using StreamLit, and the back-end uses a Flower framework. The proposed model has achieved a global accuracy of 99.59% after being trained for three federated communication rounds. The detailed analysis of this paper provides the robustness of this work. In addition, the Internet of Medical Things (IoMT) will improve the ease of access to the aforementioned health services. IoMT tools and services are rapidly changing healthcare operations for the better. Hopefully, it will continue to do so in this difficult time of the COVID-19 pandemic and will help to push the envelope of this work to a different extent.},
  archive      = {J_EXSY},
  author       = {Deepraj Chowdhury and Soham Banerjee and Madhushree Sannigrahi and Arka Chakraborty and Anik Das and Ajoy Dey and Ashutosh Dhar Dwivedi},
  doi          = {10.1111/exsy.13173},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13173},
  shortjournal = {Expert Syst.},
  title        = {Federated learning based covid-19 detection},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive ankle impedance control for bipedal robotic upright
balance. <em>EXSY</em>, <em>40</em>(5), e13168. (<a
href="https://doi.org/10.1111/exsy.13168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upright balance control is a fundamental skill of bipedal robots for various tasks that are usually performed by human beings. Conventional robotic control is often realized by developing accurate dynamic models using a series of fixed torque-ankle states, but their success is subject to accurate physical and kinematic models. This can be particularly challenging when external disturbing forces present, but this is common in unstructured robotic working environments, leading to ineffective robotic control. To address such limitation, this paper presents an adaptive ankle impedance control method with the support of the advances of adaptive fuzzy inference systems, by which the desired ankle torques are generated in real time to adaptively meet the dynamic control requirement. In particular, the control method is initialised with specific external disturbing forces first representing a general situation, which then evolves whilst performing in a real-world working environment by acting on the feedback from the control system. This is implemented by initialising a rule base for a typical situation, and then allowing the rule base to evolve to specific robotic working environments. This closed loop feedback and action mechanism timely and effectively configures the control system to meet the dynamic control requirements. The proposed control method was applied to a bipedal robot on a moving vehicle for system validation and evaluation, with robotic loads ranging from 0 to 1.65‚Äâkg and external disturbances in terms of vehicle acceleration ranging from 0.5 to 1.5 , leading to robotic swing angles up to and anti-disturbance timespans up to 8.5‚Äâs. These experimental results demonstrate the power of the proposed upright balance control method in improving the robustness, and thus applicability, of bipedal robots.},
  archive      = {J_EXSY},
  author       = {Kaiyang Yin and Yifei Wang and Pengfei Li and Kejie Dai and Yaxu Xue and Longzhi Yang},
  doi          = {10.1111/exsy.13168},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13168},
  shortjournal = {Expert Syst.},
  title        = {Adaptive ankle impedance control for bipedal robotic upright balance},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expert system and entropy-ordered weighted average method in
cold supply chain risk evaluation. <em>EXSY</em>, <em>40</em>(5),
e13163. (<a href="https://doi.org/10.1111/exsy.13163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If not well identified and controlled, risks in systematically engineered cold supply chains can lead directly to food safety incidents. In current approaches to cold supply chain risk evaluation, there is a lack of systematic classification and quantitative analysis of the influences of risk factors in the chain. To accurately evaluate unknown risks that can exert a fluctuating influence and result in great losses, this study builds a knowledge base of expert systems based on expertise and extensive experience and modifies the expert weights in conjunction with entropy weights to reduce subjective error. An ordered weighted average (OWA) operator is used to evaluate risk in a cold supply chain effectively. First, in combination with a case study of a typical fresh food e-commerce enterprise, a typical fresh food e-commerce enterprise, this paper identifies the risks in a food refrigeration supply chain. Second, relying on an expert group with professional knowledge and rich experience, an expert system knowledge base is constructed, risks are assigned, and the expert weight is modified in combination with the entropy weight method. Finally, based on the OWA operator, the modified risk assignment is effectively evaluated, and the risk value is obtained and sorted. Specific risk control measures are put forward according to the results of the calculation. The results show that an evaluation method combining an expert system knowledge base and the entropy-OWA method can effectively depict and process the evaluation information needed for risk rankings and solve the problems of supply chain risks more rapidly and effectively.},
  archive      = {J_EXSY},
  author       = {Yuyan Shen and Yan Qian},
  doi          = {10.1111/exsy.13163},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13163},
  shortjournal = {Expert Syst.},
  title        = {Expert system and entropy-ordered weighted average method in cold supply chain risk evaluation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clean-label poisoning attacks on federated learning for IoT.
<em>EXSY</em>, <em>40</em>(5), e13161. (<a
href="https://doi.org/10.1111/exsy.13161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is suitable for the application scenarios of distributed edge collaboration of the Internet of Things (IoT). It can provide data security and privacy, which is why it is widely used in the IoT applications such as Industrial IoT (IIoT). Latest research shows that the federated learning framework is vulnerable to poisoning attacks in the case of an active attack by the adversary. However, the existing backdoor attack methods are easy to be detected by the defence methods. To address this challenge, we focus on edge-cloud synergistic FL clean-label attacks. Unlike common backdoor attack, to ensure the attack&#39;s concealment, we add a small perturbation to realize the clean label attack by judging the cosine similarity between the gradient of the adversarial loss and the gradient of the normal training loss. In order to improve the attack success rate and robustness, the attack is implemented when the global model is about to converge. The experimental results verified that 1% of poisoned data could make an attack successful with a high probability. Our method maintains stealth while performing model poisoning attacks, and the average Peak Signal-to-Noise Ratio (PSNR) of poisoning images reaches over 30‚ÄâdB, and the average Structural SIMilarity (SSIM) is close to 0.93. Most importantly, our attack method can bypass the Byzantine aggregation defence.},
  archive      = {J_EXSY},
  author       = {Jie Yang and Jun Zheng and Thar Baker and Shuai Tang and Yu-an Tan and Quanxin Zhang},
  doi          = {10.1111/exsy.13161},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13161},
  shortjournal = {Expert Syst.},
  title        = {Clean-label poisoning attacks on federated learning for IoT},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Metaheuristics with federated learning enabled intrusion
detection system in internet of things environment. <em>EXSY</em>,
<em>40</em>(5), e13138. (<a
href="https://doi.org/10.1111/exsy.13138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of increased applications of Internet of Things (IoT) environment in real-time environment, confidential data gathered by the IoT devices are being communicated to the cloud environment to train the machine learning (ML) models in understanding the patterns that exist in the data. At the same time, the sensitive nature of the IoT data attracts malicious users into hacking efforts. An intrusion detection system (IDS) can be applied to ensure security in the IoT environment. In order to improve security, the ML models can be executed at the data source instead of centralized cloud server. Federated learning (FL) is a recent progression of ML model which enables the ML models to move into the data source rather than moving the data to centralized cloud and thereby resolves cybersecurity problems in the IoT environment. In this view, this study introduces an FL based IDS using bird swarm algorithm based feature selection with classification (FLIDS-BSAFSC) model in IoT environment. The presented FLIDS-BSAFSC model undergoes training on multiple aspects of IoT dataset in a decentralized format to classify, detect, and defend against attacks. The proposed FLIDS-BSAFSC model initially applies min‚Äìmax normalization technique to pre-process the IoT data. Besides, BSA based feature selection (BSA-FS) technique is designed to elect feature subsets. Finally, social group optimization algorithm with kernel extreme learning machine model is employed for identifying various kinds of classes. In the view of FL where the IoT dataset is not distributed to the server carries out profile aggregation competently with the advantage of peer learning. The experimental validation of the FLIDS-BSAFSC model is tested using benchmark datasets and the results are inspected under several aspects. The experimental values highlighted the better performance of the FLIDS-BSAFSC model over recent approaches.},
  archive      = {J_EXSY},
  author       = {Thavavel Vaiyapuri and Shabbab Algamdi and Rajan John and Zohra Sbai and Munira Al-Helal and Ahmed alkhayyat and Deepak Gupta},
  doi          = {10.1111/exsy.13138},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13138},
  shortjournal = {Expert Syst.},
  title        = {Metaheuristics with federated learning enabled intrusion detection system in internet of things environment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Plant disease detection using machine learning approaches.
<em>EXSY</em>, <em>40</em>(5), e13136. (<a
href="https://doi.org/10.1111/exsy.13136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant health care is the science of anticipating and diagnosing the advent of life-threatening diseases in plants. The fatality rate of plants can be reduced by diagnosing them for any signs early on. The early detection of such diseases is one possibility for lowering plant mortality rates. Machine learning (ML), a type of artificial intelligence technology that allows researchers to enhance and develop without being explicitly programmed, is used in this study to build early prediction models for plant disease diagnosis. Due to the similarities of crops throughout the early phonological phases, crop classification has proved problematic. ML can be applied to a variety of tasks recognize different types of crops at low altitude platforms with the help of drones that provide high-resolution optical imagery. The drones are employed to photograph phonological stages, and these greyscale photographs are then utilized to develop grey level co-occurrence matrices-based characteristics. In this article, the proposed plant disease detection models are developed using ML approaches such as random forest-nearest neighbours, linear regression, Naive Bayes, neural networks, and support vector machine. The performance of the generated plants disease risk evaluation model is calculated using unbiased metrics such as true positive rate, true negative rate, precision, recall, and F 1-score method are all factors to consider. The results revealed that the ensemble plants disease model outperforms the other proposed and developed plant disease detection models. The proposed and developed plant disease prediction models aimed to predict disease detection in the early stages, allowing for early preventive actions and predictive maintenance.},
  archive      = {J_EXSY},
  author       = {Imtiaz Ahmed and Pramod Kumar Yadav},
  doi          = {10.1111/exsy.13136},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13136},
  shortjournal = {Expert Syst.},
  title        = {Plant disease detection using machine learning approaches},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An analysis and prediction model based on complex network
time series. <em>EXSY</em>, <em>40</em>(5), e13132. (<a
href="https://doi.org/10.1111/exsy.13132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, mainstream research on complex network evolution involves measuring the developing characteristics of network structure at the macro level. Few studies have focused on the internal mechanism of network structure change and the prediction of network structure evolution at the micro-level. However, for some special complex network time series, the traditional analysis and research methods at the macro level are invalid, and the micro-level analysis method needs to be adopted. Based on the evolution characteristics of nodes and edges in complex networks, this paper constructs a time series network analysis and prediction algorithm to explore the structural evolution trends of complex network time series and make a prediction. Based on the enterprise supply‚Äìdemand relationship data, this paper constructs the enterprise supply‚Äìdemand network time series. Then, we use the method proposed in this paper to analyse and predict the complex network time series and verify the rationality and feasibility of the proposed method.},
  archive      = {J_EXSY},
  author       = {Wenting Zhang},
  doi          = {10.1111/exsy.13132},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13132},
  shortjournal = {Expert Syst.},
  title        = {An analysis and prediction model based on complex network time series},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ShareChain: Blockchain-enabled model for sharing patient
data using federated learning and differential privacy. <em>EXSY</em>,
<em>40</em>(5), e13131. (<a
href="https://doi.org/10.1111/exsy.13131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Every individual in our technologically evolved world needs proper data security. The procedure of exchanging medical information is increasingly concerned with data privacy. Many techniques have been offered for preserving data security. These techniques use approaches such as k -anonymity, l -diversity, and others. However, such solutions are vulnerable to attribute disclosure, homogeneity, and background knowledge risks due to their syntactic nature. In this work, we describe a safe and secure architecture and semantic approach for data sharing that is based on blockchain, local differential privacy (LDP), and federated learning (FL). The proposed framework generates an atmosphere devoid of trust in which data owners are no longer required to have trust in the controllers. The FL models enable the whole network to decentralize its data-driven learning. Interplanetary file system (IPFS) is used to provide data security in a distributed environment because each file in IPFS has a digital fingerprint that is computed using a cryptographic hash function on the file&#39;s whole contents. Due to the rigorous privacy guarantee, data owners no longer need to be worried about the security of their data. The proposed model&#39;s assessment parameters include latency, throughput, privacy, and accuracy. The data privacy of the proposed model is protected via LDP and FL, and its latency and throughput communication transactions on permissioned blockchain are calculated and compared with those of the benchmark model. The findings indicate that the proposed model delivers 85% more accurate privacy than the benchmark model.},
  archive      = {J_EXSY},
  author       = {Laraib Javed and Adeel Anjum and Bello Musa Yakubu and Majid Iqbal and Syed Atif Moqurrab and Gautam Srivastava},
  doi          = {10.1111/exsy.13131},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13131},
  shortjournal = {Expert Syst.},
  title        = {ShareChain: Blockchain-enabled model for sharing patient data using federated learning and differential privacy},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards personalized environment-aware outdoor gait analysis
using a smartphone. <em>EXSY</em>, <em>40</em>(5), e13130. (<a
href="https://doi.org/10.1111/exsy.13130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic gait analysis in free-living environments using inertial sensors requires individualized approach as local acceleration and velocity profiles vary with the walker and the topological properties of the environment (e.g., walking in the forest vs. walking on sand). Here, we propose a smartphone-based gait assessment architecture which consists of two data processing modules. The first module employs a set of personalized classifiers for automatic recognition of the walking environment. The second module provides accurate step time estimates by selecting the optimal filtering frequency tailored to the predicted environment. The performance of the architecture was evaluated using experimental data collected from 10 participants walking in 10 different conditions typically encountered during daily living. Compared with ground truth data, the architecture successfully recognized the walking environments; the percentage of correctly classified instances was above 92%. It also estimated step time with high accuracy; the mean absolute error was less than 10¬†ms, outperforming or at the very least matching the performance levels achieved in controlled laboratory trials (indoor flat surface walking). Compared with using one filtering frequency for all environments, using optimal frequency tailored to each environment reduced step time estimation error by more than 39%. To the best of our knowledge, this is the first study which successfully demonstrates that parameter tuning can improve gait characterization in outdoor environments. However, further research using a larger data set (including more participants with varying demographics and degree of impairment) is needed to confirm this result. Our findings highlight the importance of environment-aware gait analysis, and lay the groundwork for a smartphone-based technology that can be used in the community.},
  archive      = {J_EXSY},
  author       = {Arshad Sher and Megan Taylor Bunker and Otar Akanyeti},
  doi          = {10.1111/exsy.13130},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13130},
  shortjournal = {Expert Syst.},
  title        = {Towards personalized environment-aware outdoor gait analysis using a smartphone},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent sustainable efficient transmission internet
protocol to switch between user datagram protocol and transmission
control protocol in IoT computing. <em>EXSY</em>, <em>40</em>(5),
e13129. (<a href="https://doi.org/10.1111/exsy.13129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, Internet of things (IoT), Cloud and Fog networks have spread out around the world. The more these networks grow, the more their energy consumption comes to attention. Many efforts have been made during recent years to decrease this energy consumption, mainly focused on utilizing low-power devices. Green algorithms are recently proposed to reduce energy consumption by modifying the structure of many algorithms employed in the network and its protocols. This paper proposes a new green reliability algorithm for Transmission Control Protocol/Internet Protocol (TCP/IP protocol) in Fog computing. The proposed algorithm does not require extensive TCP/IP protocol changes or relevant hardware. It is based on transferring less number of packets in the network by using the advantage of differences between TCP and User Datagram Protocol (UDP). TCP and User Datagram Protocol (UDP) are different in nature as the number of total packets in UDP is half that of TCP. As a result, the number of complete packets in UDP is half that of TCP. The proposed method is built around the loss of some packets in applications, such as voice and online video, does not severely degrade the end results. Therefore, the UDP protocol can substitute TCP in such situations. The criterion to switch between the two is the minimum acceptable Quality of Service (QoS) of the overall network. In other words, the UDP protocol will be used as long as QoS requirements are met. The switching process between UDP and TCP is dynamic, optimized by estimating network noise in the period. Additionally, we evaluated the proposed method based on several QoS functions, including delay, throughput, and energy usage.},
  archive      = {J_EXSY},
  author       = {Shadi Mahmoodi Khaniabadi and Amir Javadpour and Mehdi Gheisari and Weizhe Zhang and Yang Liu and Arun Kumar Sangaiah},
  doi          = {10.1111/exsy.13129},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13129},
  shortjournal = {Expert Syst.},
  title        = {An intelligent sustainable efficient transmission internet protocol to switch between user datagram protocol and transmission control protocol in IoT computing},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuro-weighted multi-functional nearest-neighbour
classification. <em>EXSY</em>, <em>40</em>(5), e13125. (<a
href="https://doi.org/10.1111/exsy.13125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Guanli Yue and Yanpeng Qu and Ansheng Deng and Qianyi Zhang},
  doi          = {10.1111/exsy.13125},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13125},
  shortjournal = {Expert Syst.},
  title        = {Neuro-weighted multi-functional nearest-neighbour classification},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A performance efficient joint clustering and routing
approach for heterogeneous wireless sensor networks. <em>EXSY</em>,
<em>40</em>(5), e13121. (<a
href="https://doi.org/10.1111/exsy.13121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensor networks have proven to be a promising paradigm due to their wide applicability in the real-world scenarios. Because of the resource constraint, type-variant and battery-powered features of the sensor nodes, the key challenge is how to improve the energy usage and network life in the heterogeneous sensor networks. In this paper, the Mahalanobis distance based K-Means algorithm is integrated with a novel evolutionary approach for clustering called calf search optimization algorithm (K-CSOA) that creates energy efficient clusters. The routing in the network is performed using a power efficient ant colony optimization algorithm (ACO). The extensive and multiple simulations are performed to validate the effectiveness of the proposed method. It is observed that the running time of the proposed scheme is lowered due to the implementation of hybrid clustering. The results are validated for various performance metrics wherein the proposed method shows 41.6% and 4.3%‚Äì12% reduction in energy expenditure and end-to-end delay respectively and around 60% enhancement in the throughput of the network when compared with other similar state-of-the-art methods. In addition to this, statistical analysis is also carried out to identify the performance of the proposed algorithm.},
  archive      = {J_EXSY},
  author       = {Pallavi Joshi and Sarvesh Kumar and Ajay Singh Raghuvanshi},
  doi          = {10.1111/exsy.13121},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13121},
  shortjournal = {Expert Syst.},
  title        = {A performance efficient joint clustering and routing approach for heterogeneous wireless sensor networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A supervised machine learning model for determining
lubricant oil operating conditions. <em>EXSY</em>, <em>40</em>(5),
e13116. (<a href="https://doi.org/10.1111/exsy.13116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning tools for analysing lubricating oil data have enabled better information in the condition based maintenance (CBM) approach to the process of diagnosing and predicting failures in diesel-powered vehicle fleets. With the increase in the number of sensors inserted in vehicles, it is possible for companies to stockpile large quantities of information in real time. As such, the development of data-driven tools will enable accurate identification of the level of wear of a system, evolving CBM into a more reliable and dynamic approach. Following this type of data-based analysis focusing on determining the wear of systems and equipment, this paper presents an intelligent system for assessing the condition of lubricating oil in automotive diesel engines. To this end, we analyse the use of raw data obtained from the sensors installed in the car and evaluate in conjunction with the insertion of engineered features designed the best way to determine the operating state of the oils. The results presented in this analysis show that to explain 90% of the variation in the original data only the variables kinematic viscosity, dynamic viscosity, engine oil temperature and OSF_v3 are needed. After evaluating the quality of the variables, we conducted an experimental study to analyse the performance of various machine learning algorithms, taking into account the number of features as input data. The results show that the proposed system has the ability to identify the operating conditions of lubricating oil using seven variables as input to a model based on gradient boosting, obtaining a recall result of 93%, precision of 96% and F1-score of 94%. We conducted a set of additional studies to understand how different subsets of variables affected the performance of the models, and the results show that the best combination includes information regarding the engine speed, coolant and oil temperature, oil pressure, the oil stress factor (OSF_v3), kinematic viscosity, and the dynamic viscosity.},
  archive      = {J_EXSY},
  author       = {Roney Malaguti and Nuno Louren√ßo and Cristov√£o Silva},
  doi          = {10.1111/exsy.13116},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13116},
  shortjournal = {Expert Syst.},
  title        = {A supervised machine learning model for determining lubricant oil operating conditions},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Realization of natural language processing and machine
learning approaches for text-based sentiment analysis. <em>EXSY</em>,
<em>40</em>(5), e13114. (<a
href="https://doi.org/10.1111/exsy.13114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The leading intention of the current paper is to review the research work accomplished by various researchers to achieve sentiment analysis on the text and to elaborate on natural language processing (NLP) and various machine learning algorithms used to evaluate textual sentiments. In this study, primitive cases are considered that used crucial algorithms, and knowledge that can be opted for sentiment analysis. A survey of the work that has been done till now is conducted observing the results and outcomes concerning varying parameters of various researchers who worked on previously existing as well as novel and hybrid algorithms opting legion methodologies. The fundamental algorithms like Support Vector Machine (SVM), Bayesian Networks (BN), Maximum Entropy (MaxEnt), Conditional Random Fields (CRF) and Artificial Neural Networks (ANN) are also discussed to achieve practice percentage and accuracy score in the field of NLP, sentiment analysis and text analytics. Various other novel approaches and algorithms like CNN, LSTM, KNN, K*, K-means, K-means++, SOM and ENORA, along with their limitations and the performance metrics providing accuracies for major open data sets are also analyzed.},
  archive      = {J_EXSY},
  author       = {Kanchan Naithani and Yadav Prasad Raiwani},
  doi          = {10.1111/exsy.13114},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13114},
  shortjournal = {Expert Syst.},
  title        = {Realization of natural language processing and machine learning approaches for text-based sentiment analysis},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Telco customer top-ups: Stream-based multi-target
regression. <em>EXSY</em>, <em>40</em>(5), e13111. (<a
href="https://doi.org/10.1111/exsy.13111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telecommunication operators compete not only for new clients, but, above all, to maintain current ones. The modelling and prediction of the top-up behaviour of prepaid mobile subscribers allows operators to anticipate customer intentions and implement measures to strengthen customer relationship. This research explores a data set from a Portuguese operator, comprising 30‚Äâmonths of top-up events, to predict the top-up monthly frequency and average value of prepaid subscribers using offline and online multi-target regression algorithms. The offline techniques adopt a monthly sliding window, whereas the online techniques use an event sliding window. Experiments were performed to determine the most promising set of features, analyse the accuracy of the offline and online regressors and the impact of sliding window dimension. The results show that online regression outperforms the offline counterparts. The best accuracy was achieved with adaptive model rules and a sliding window of 500,000 events (approximately 5¬†months). Finally, the predicted top-up monthly frequency and average value of each subscriber were converted to individual date and value intervals, which can be used by the operator to identify early signs of subscriber disengagement and immediately take pre-emptive measures.},
  archive      = {J_EXSY},
  author       = {Pedro Miguel Alves and Ricardo √Çngelo Filipe and Benedita Malheiro},
  doi          = {10.1111/exsy.13111},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13111},
  shortjournal = {Expert Syst.},
  title        = {Telco customer top-ups: Stream-based multi-target regression},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of intelligent and integrated technology for
pattern recognition in EMG signals for robotic prosthesis command.
<em>EXSY</em>, <em>40</em>(5), e13109. (<a
href="https://doi.org/10.1111/exsy.13109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prostheses play an important role in the rehabilitation of people who have suffered some type of amputation. However, due to its high-cost and high complexity in performing movements of everyday tasks, users of these prostheses may encounter many difficulties. Therefore, this work proposes the development of a future artificial intelligence technology based on a low-cost functional prosthesis prototype (manufactured in a 3D printer). In the present work, we describe an intelligent system that uses an artificial neural network to recognize patterns in muscle biopotential signals in order to control a prosthesis prototype in real time. Such a system is divided into three parts: the first that performs a human‚Äìmachine integration through a graphical user interface; the second that performs the signal acquisition; the third that performs the training and generalization steps of the artificial neural network. The developed interface runs on a web application that has a database hosted in the cloud and in it the system user can: Acquisition of electromyography signals; Training phase of the artificial neural network; Sends the matrix of weights of the trained network to the microcontroller; Activates in the microcontroller, the state of action of the commands from the identified gestures. To compose the results of the present work, a search was initially carried out for the ideal parameters of the artificial neural network through signals obtained from 20 volunteers. In this step, it was possible to identify the topology that best classifies the signals of each gesture, as well as the investigation of the number of neurons in the hidden layer that causes a low generalization power due to overfitting. At the end of the project, it was possible to validate the use of the system with 15 new volunteers, and it was observed that in most cases, the performance of the commands in the prosthesis prototype were performed correctly. In addition, a project cost analysis was carried out, and it was possible to verify that the prototype developed is viable and has an affordable cost in relation to the Brazilian cost of living standards. In this way, the objective of the present work is in the development of a low cost artificial intelligence technology. Such a system is equipped with an algorithm based on neural networks that can deal with different muscle biopotential signals, in order to command a robotic prosthesis.},
  archive      = {J_EXSY},
  author       = {Yongzhao Xu and Paulo C. S. Barbosa and Joel S. da Cunha Neto and Lijuan Zhang and Vimal Shanmuganathan and Victor Hugo C. de Albuquerque and Subbulakshmi Pasupathi},
  doi          = {10.1111/exsy.13109},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13109},
  shortjournal = {Expert Syst.},
  title        = {Development of intelligent and integrated technology for pattern recognition in EMG signals for robotic prosthesis command},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial neural networks for prediction of COVID-19 in
india by using backpropagation. <em>EXSY</em>, <em>40</em>(5), e13105.
(<a href="https://doi.org/10.1111/exsy.13105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has affected thousands of people around the world. In this study, we used artificial neural network (ANN) models to forecast the COVID-19 outbreak for policymakers based on 1st January to 31st October 2021 of positive cases in India. In the confirmed cases of COVID-19 in India, it&#39;s critical to use an estimating model with a high degree of accuracy to get a clear understanding of the situation. Two explicit mathematical prediction models were used in this work to anticipate the COVID-19 epidemic in India. A Boltzmann Function-based model and Beesham&#39;s prediction model are among these methods and also estimated using the advanced ANN-BP models. The COVID-19 information was partitioned into two sections: training and testing. The former was utilized for training the ANN-BP models, and the latter was used to test them. The information examination uncovers critical day-by-day affirmed case changes, yet additionally unmistakable scopes of absolute affirmed cases revealed across the time span considered. The ANN-BP model that takes into consideration the preceding 14-days outperforms the others based on the archived results. In forecasting the COVID-19 pandemic, this comparison provides the maximum incubation period, in India. Mean square error, and mean absolute percent error have been treated as the forecast model performs more accurately and gets good results. In view of the findings, the ANN-BP model that considers the past 14-days for the forecast is proposed to predict everyday affirmed cases, especially in India that have encountered the main pinnacle of the COVID-19 outbreak. This work has not just demonstrated the relevance of the ANN-BP techniques for the expectation of the COVID-19 outbreak yet additionally showed that considering the incubation time of COVID-19 in forecast models might produce more accurate assessments.},
  archive      = {J_EXSY},
  author       = {Balakrishnama Manohar and Raja Das},
  doi          = {10.1111/exsy.13105},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13105},
  shortjournal = {Expert Syst.},
  title        = {Artificial neural networks for prediction of COVID-19 in india by using backpropagation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving federated learning cyber-threat detection
for intelligent transport systems with blockchain-based security.
<em>EXSY</em>, <em>40</em>(5), e13103. (<a
href="https://doi.org/10.1111/exsy.13103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) techniques implemented at a large scale in intelligent transport systems (ITS), have considerably enhanced the vehicles&#39; autonomous behaviour in making independent decisions about cyber threats, attacks, and faults. While, AI techniques are based on data sharing among the vehicles, it is important to note that sensitive data cannot be shared. Thus, federated learning (FL) has been implemented to protect privacy in vehicles. On the other hand, the integrity of data and the safety of aggregation are ensured by using blockchain technology. This paper applied classification approaches to VANET and ITS cyber-threats detection at the vehicle. Subsequently, by using blockchain and by applying an aggregation strategy to different models, models from the previous step were uploaded in a smart contract. Lastly, we returned the updated models to the vehicles. Furthermore, we conducted an experimental study to measure the effectiveness of the proposed prototype. In this paper, the VeReMi data set was distributed in a balanced manner into five parts in the experimental study. Thus, classification techniques were executed by each vehicle separately, and models were generated. Upon the aggregation of the models in blockchain, they were returned to the vehicles. Lastly, the vehicles updated their decision functions and accessed the precision and accuracy of cyber-threat detection. The results indicated that the precision and accuracy decreased by 7.1% on average with comparable F 1-score and recall. Our solution ensures the privacy preservation of vehicles whereas blockchain guarantees the safety of aggregation technique and low gas consumption.},
  archive      = {J_EXSY},
  author       = {Tarek Moulahi and Rateb Jabbar and Abdulatif Alabdulatif and Sidra Abbas and Salim El Khediri and Salah Zidi and Muhammad Rizwan},
  doi          = {10.1111/exsy.13103},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13103},
  shortjournal = {Expert Syst.},
  title        = {Privacy-preserving federated learning cyber-threat detection for intelligent transport systems with blockchain-based security},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Avoiding excess computation in asynchronous evolutionary
algorithms. <em>EXSY</em>, <em>40</em>(5), e13100. (<a
href="https://doi.org/10.1111/exsy.13100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous evolutionary algorithms are becoming increasingly popular as a means of making full use of many processors while solving computationally expensive search and optimization problems. These algorithms excel at keeping large clusters fully utilized, but may sometimes inefficiently sample an excess of fast-evaluating solutions at the expense of higher-quality, slow-evaluating ones. We have previously introduced a steady-state parent selection strategy, SWEET (‚ÄúSelection whilE EvaluaTing‚Äù), that sometimes selects individuals that are still being evaluated and allows them to reproduce early. We perform a takeover-time analysis that confirms that this strategy gives slow-evaluating individuals that have higher fitnesses an increased ability to multiply in the population. We also find that SWEET appears effective at improving optimization performance on problems in which solution quality is positively correlated with evaluation time. We evaluate our approach on six simulated real-valued optimization problems and three real-world applications: an autonomous vehicle controller problem that involves tuning a spiking neural network and two adversarial EA problems. We further evaluate SWEET versus a basic asynchronous process in a simulated setting. We present evidence that SWEET outperforms basic asynchronous processes in a use-case in which performance is positively correlated with evaluation time, and performs comparably (and often better) than basic asynchronous processes in several use-cases where performance is negatively correlated with evaluation time. That said, in the cases where performance and evaluation time are negatively correlated the variance of outcomes for SWEET is notably high.},
  archive      = {J_EXSY},
  author       = {Eric O. Scott and Mark Coletti and Catherine D. Schuman and Bill Kay and Shruti R. Kulkarni and Maryam Parsa and Chathika Gunaratne and Kenneth A. De Jong},
  doi          = {10.1111/exsy.13100},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13100},
  shortjournal = {Expert Syst.},
  title        = {Avoiding excess computation in asynchronous evolutionary algorithms},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards utilization of rule base structure to support fuzzy
rule interpolation. <em>EXSY</em>, <em>40</em>(5), e13097. (<a
href="https://doi.org/10.1111/exsy.13097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy rule interpolation (FRI) offers a reliable approach for providing an interpretable approximate decision with a sparse rule base, when a new observation does not match any existing rules. As the mainstream application of a fuzzy rule base is to extract valuable approximate information from each individual rules, existing FRI methods typically work by postulating that the more rules used to implement the interpolation the better the reasoning outcomes. Yet, empirical results have shown that using a large number of rules in an FRI process may adversely lead to worsening the accuracy of the inference outcomes, not just degrading efficiency. The objective of this work is to set a firm theoretical foundation for the eventual establishment of a novel FRI approach. It achieves this goal by mapping the structural patterns within a given fuzzy rule base onto a mathematically isomorphic data space, such that the essential information embedded in the original rule base can be effectively captured, represented and analysed. The resulting mathematically mapped patterns enable the production of a theorem that determines the upper limit of the number of rules required to effectively and efficiently perform FRI. The experimental investigations reported herein demonstrate that the number of required rules to perform FRI obeys the theorem discovered in this work.},
  archive      = {J_EXSY},
  author       = {Changhong Jiang and Shangzhu Jin and Changjing Shang and Qiang Shen},
  doi          = {10.1111/exsy.13097},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13097},
  shortjournal = {Expert Syst.},
  title        = {Towards utilization of rule base structure to support fuzzy rule interpolation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear pricing game based power control with resource
allocation and interference management in device-to-device communication
for IoT applications. <em>EXSY</em>, <em>40</em>(5), e13094. (<a
href="https://doi.org/10.1111/exsy.13094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-to-Device (D2D) communication is one of the leading technologies which works under the aegis of fifth generation (5G) communication for the Internet of Things in Healthcare. In D2D communication, power control with resource allocation is the most challenging area for the research to make them suitable for IoT devices. In the present work, power control with resource allocation is done by using the Linear pricing game power allocation method. First, a mathematical min-function based problem is proposed whilst taking the interference, data rate and minimum power requirement as a constraint to ensure the resource and power allocation. In the next phase, a linear pricing game (LPG) method is utilized to propose the LPG power control with resource allocation (L-PaRAG). Mathematical analysis and theorem ensure the minimum cost function with global stability for an optimal solution of the proposed problem. Numerical analysis and results show that the proposed algorithm gives the best results for power control and resource management whilst managing the interference to maintain the quality of services (QoS) in D2D communication. Optimized power can be utilized for the communication between the base station and D2D users, especially, during disaster management.},
  archive      = {J_EXSY},
  author       = {Krishna Pandey and Rajeev Arya},
  doi          = {10.1111/exsy.13094},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13094},
  shortjournal = {Expert Syst.},
  title        = {Linear pricing game based power control with resource allocation and interference management in device-to-device communication for IoT applications},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covid-19 cases prediction using SARIMAX model by tuning
hyperparameter through grid search cross-validation approach.
<em>EXSY</em>, <em>40</em>(5), e13086. (<a
href="https://doi.org/10.1111/exsy.13086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SARS-Coronavirus was first detected in December 2019, later named COVID-19, and declared a pandemic by the World Health Organization (WHO). As prediction models assist policymakers in making decisions based on expected outcomes. Existing models were only used to anticipate a smaller range of data resulting in irrelevant predictions. Our research focuses on predicting COVID-19 confirmed, recovered, and deceased Indian cases for 20‚Äâdays ahead. Tuning of hyperparameters is performed with a grid search cross-validation approach. The dataset is collected from the Kaggle. Our forecast indicates that the count of confirmed and deceased cases is higher whereas, recovered cases prediction shows a decreasing trend. The R 2 Score achieved is 0.5112 and root-mean-square error (RMSE) is 1251 using optimized SARIMAX. Finally, Monte Carlo simulation has also been performed to justify the prediction accuracy as compared to other models such as linear, polynomial, prophet, and SARIMAX without grid search cross validation.},
  archive      = {J_EXSY},
  author       = {Sweeti Sah and Balasubramanian Surendiran and Ramasamy Dhanalakshmi and Mohammed Yamin},
  doi          = {10.1111/exsy.13086},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13086},
  shortjournal = {Expert Syst.},
  title        = {Covid-19 cases prediction using SARIMAX model by tuning hyperparameter through grid search cross-validation approach},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cyber security for federated learning environment using AI
technique. <em>EXSY</em>, <em>40</em>(5), e13080. (<a
href="https://doi.org/10.1111/exsy.13080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cyber-security threat in the Android environment is seen as malicious code. Due to the rapid development of Android malware deployments, manually detecting malicious apps inside the Android environment is almost impossible. In the end of an outcome, machine learning has emerged as a promising method for detecting malware. Because the increasing accessibility and appropriate attributes have a significant impact on machine learning performance, selecting features becomes more critical in malware detection using the machine learning. In the existing system, a filter-based feature selection method was used. The filter-based feature subset selection strategies are mathematically efficient for computation. But they refuse to address concerns including such multicollinearity, which impacts the filter methods&#39; accuracy. To overcome this issue, this research develops a hybrid method that combines the findings of both static and dynamic malware examination. This method tackles the issue of more effectively analysing, identifying, and classifying Android malware. Many feature selection approaches are compared and contrasted based on a variety of parameters, such as the composition of important extracted features. By using extracted features, whilst completing static and dynamic malware examinations, several machine learning methods are used to identify and characterize malware. The experimental result shows the evaluation of the feature selection method which is varied from other algorithms. It enhances the accuracy and minimizes the run time of the learning models.},
  archive      = {J_EXSY},
  author       = {Hasan J. Alyamani},
  doi          = {10.1111/exsy.13080},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13080},
  shortjournal = {Expert Syst.},
  title        = {Cyber security for federated learning environment using AI technique},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SVM-based generative adverserial networks for federated
learning and edge computing attack model and outpoising. <em>EXSY</em>,
<em>40</em>(5), e13072. (<a
href="https://doi.org/10.1111/exsy.13072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning are vulnerable to the threats. The Intruders can utilize the malicious nature of the nodes to attack the training dataset to worsen the process and manipulate the learning and make the over all system with less efficiency and performance. The optimized poison attack procedures are already introduced to estimate the overall bad scenario, design the intrusion as bi-level optimization and it is considered computational complexity is high and demanding, in contrary the applicability is limited such models deep neural networks. In this research papers, we have proposed, novel proposed system, poisoning attacks against the Machine learning training dataset, including the genuine data points that reduce the accuracy of the classifier in the process of training. The proposed system have 3 components of Generative Adverserial networks (GAN) generator, discriminator, and the target classifier. The proposed system allows to detect the vulnerability easy and it can be found as similar as realistic attacks to detect the area where the underlying data distribution have more possibility of poising attack which cause vulnerability to the network. Our experimentation, proves the claim our that the proposed model is effective on compromising the classifiers uses the machine learning algorithms and also deep learning networks.},
  archive      = {J_EXSY},
  author       = {Poongodi Manoharan and Ranjan Walia and Celestine Iwendi and Tariq Ahamed Ahanger and S. T. Suganthi and M. M. Kamruzzaman and Sami Bourouis and Wajdi Alhakami and Mounir Hamdi},
  doi          = {10.1111/exsy.13072},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13072},
  shortjournal = {Expert Syst.},
  title        = {SVM-based generative adverserial networks for federated learning and edge computing attack model and outpoising},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards a model for determining patent revenue odds: An
empirical study of technology transfer offices. <em>EXSY</em>,
<em>40</em>(5), e13037. (<a
href="https://doi.org/10.1111/exsy.13037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technology transfer offices (TTO) were created with the mission of executing innovation policy and its technology transfer to industry. Most studies regarding TTO focus on the context of developed countries, so there is a lack of research on the subject in emerging economies, such as Brazil. In addition, the issue of how diverse the skills of the team of a given TTO should be for their best revenue performance is still little addressed. In this sense, the present study aims to identify which characteristics of the TTO, related to their human resources, influence obtaining revenue from the patents that constitute their portfolios. To achieve this objective, 272 TTO in Brazil were analysed with the help of the Logistic Regression technique, based on Maximum Likelihood Estimation. A remarkable conclusion that emerges from our results is that the universities to increase their revenue must invest in full-time employees (e.g., rather than in scholarship students, as tends to be the norm) and foster the inventions&#39; communications, as well as to attract and retain employees with skills directly related to knowledge fields such as Law, Engineering, and Communication (quite surprisingly, Management and Economics graduates are not included). The combination of these factors can increase the probabilities or odds of a given TTO obtaining revenue. Thus, our results contribute to TTO human resource practices, especially those in structuring stages, such as those in Brazil and Latin American countries.},
  archive      = {J_EXSY},
  author       = {Rafael Angelo Santos Leite and Cicero Eduardo Walter and Igor Bezerra Reis and Pedro Elias Figueredo de Sousa and Iracema Machado de Arag√£o and Manuel Au-Yong-Oliveira},
  doi          = {10.1111/exsy.13037},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13037},
  shortjournal = {Expert Syst.},
  title        = {Towards a model for determining patent revenue odds: An empirical study of technology transfer offices},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An image watermark removal method for secure internet of
things applications based on federated learning. <em>EXSY</em>,
<em>40</em>(5), e13036. (<a
href="https://doi.org/10.1111/exsy.13036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Watermark adding is one of the important means for image security and privacy protection in Internet of things (IOT) applications based on federated learning. It is often inseparable from adversarial training with watermark removal algorithms. The effect of watermark removal algorithms will directly affect the final result of watermark addition. However, the existing watermark removal algorithms have drawbacks such as incomplete image watermark removal, poor image quality after watermark removal, large demand for training data, and incorrect filling, which seriously affects the development of image information security and privacy protection in IOT applications based on federated learning. To solve the above problems, this paper proposes an improved image watermark removal convolutional network model based on deep image prior. First, we improve the U-Net network model, using six downsamping layers and six deconvolution layers combined with deep image prior method to reduce the loss of details and perceive high-level features, thereby improving the ability of the network to extract high-level features of the image. In addition, we design a new type of loss function which is called stair loss, and add L1 loss and perception loss to establish new constraints. In order to verify the effectiveness of our method, a comprehensive experimental comparison was conducted on the public dataset PASCAL VOC 2012 in the same experimental environment with CGAN and the deep prior method. The experimental results show that the improved model combined with the deep image prior method can extract the high-level feature information and can directly remove the watermark from the picture without pretraining the network, the L1 loss and perceptual loss can better retain the image structure information and speed up the watermark removal of the model, the stair loss corrects the final output more accurately by correcting the output of each layer; our method improves the learning ability of the model, and under the condition of the same training time, the image quality after watermark removal is higher, and the final watermark removal result is better, which is more suitable for distributed structure of IoT application based on federated learning.},
  archive      = {J_EXSY},
  author       = {Hongan Li and Guanyi Wang and Qiaozhi Hua and Zheng Wen and Zhanli Li and Ting Lei},
  doi          = {10.1111/exsy.13036},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13036},
  shortjournal = {Expert Syst.},
  title        = {An image watermark removal method for secure internet of things applications based on federated learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Building a pervasive social gaming experience using
SocialPG. <em>EXSY</em>, <em>40</em>(5), e13023. (<a
href="https://doi.org/10.1111/exsy.13023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pervasive computing has become a key element to build applications that use fun as a motivating component because it allows exploring new interaction schemes by making the concept of space and time ambiguous and confusing. The present research describes a pervasive social gaming experience, using as a reference SocialPG, which is a model that describes social expansion as a strategy to improve gaming experiences supported by pervasive computing. In this article a description of the model is offered, software architecture is proposed to support it and a case study related to the process of error resolution and detection of improvement opportunities in software products is developed, finally, a general idea about the final software product that will support the game experience is offered together with an evaluation performed by a set of users, where some important findings are highlighted, such as the importance of the missions as a unit of cooperative work and the spectator&#39;s participation.},
  archive      = {J_EXSY},
  author       = {Ram√≥n Valera-Aranguren and Patricia Paderewski Rodriguez and Francisco Luis Gutierrez Vela and Jeferson Arango-L√≥pez and Fernando Moreira},
  doi          = {10.1111/exsy.13023},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e13023},
  shortjournal = {Expert Syst.},
  title        = {Building a pervasive social gaming experience using SocialPG},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ‚ÄúWant to come play with me?‚Äù Outlier subgroup discovery on
spatio-temporal interactions. <em>EXSY</em>, <em>40</em>(5), e12686. (<a
href="https://doi.org/10.1111/exsy.12686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our lives are made of social interactions which can be recorded through personal gadgets as well as sensors capturing ubiquitous and social data. This type of data, such as spatio-temporal data from the real-time location of people, for example, can then be used for inferring interactions which can be translated into behavioural patterns. In this paper, we consider the automatic discovery of exceptional social behaviour from spatio-temporal interaction data, focusing on two areas: exceptional subgroups and spatio-temporal outliers ‚Äì both in the form of descriptive patterns. For that, we propose a method for exceptional social behaviour discovery , combining subgroup discovery and network science methods for identifying behaviour that deviates from the norm. We also propose the use of two outlier detection metrics for identifying outliers, namely the Local Outlier Factor (LOF) and the Voronoi area. We applied the proposed method on synthetic data as well as two real datasets containing location data from children playing in the school playground. Our results indicate that this is a valid approach which is able to obtain meaningful knowledge from the data.},
  archive      = {J_EXSY},
  author       = {Carolina Centeio Jorge and Martin Atzmueller and Behzad M. Heravi and Jenny L. Gibson and Rosaldo J. F. Rossetti and Cl√°udio Rebelo de S√°},
  doi          = {10.1111/exsy.12686},
  journal      = {Expert Systems},
  month        = {6},
  number       = {5},
  pages        = {e12686},
  shortjournal = {Expert Syst.},
  title        = {‚ÄúWant to come play with me?‚Äù outlier subgroup discovery on spatio-temporal interactions},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consumer sentiment analysis with aspect fusion and GAN-BERT
aided adversarial learning. <em>EXSY</em>, <em>40</em>(4), e13247. (<a
href="https://doi.org/10.1111/exsy.13247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sentiment analysis (SA) from the user-generated reviews is the latest research topic in natural language processing. Nowadays, the extraction of consumer sentiment from the content of the consumer reviews is getting much attention because of its importance in understanding consumers&#39; experiences regarding services or products. Consumer sentiment may be helpful for both consumers and organizations; a consumer can refer to previous consumers&#39; feedback while making their purchase decisions, and organizations can use it in service improvements. For the consumer SA, this article proposed the BERT-GAN model with review aspect fusion, which improves the fine-tuning performance of the BERT model by introducing semi-supervised adversarial learning. For our objective, we extracted various service aspects from consumer reviews and fused them with the word sequences before feeding them into the model. That helps in incorporating aspect representation as well as position information in context with the sentences. Our results analysis and their demonstration show the contribution of the presented model in terms of accuracy compared with the existing models found in the previous work.},
  archive      = {J_EXSY},
  author       = {Praphula Kumar Jain and Waris Quamer and Rajendra Pamula},
  doi          = {10.1111/exsy.13247},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13247},
  shortjournal = {Expert Syst.},
  title        = {Consumer sentiment analysis with aspect fusion and GAN-BERT aided adversarial learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel double keys adapted elliptic curve cryptography and
log normalized gaussian sigmoid adaptive neuro-fuzzy interference system
based secure resource allocation system in decentralized cloud storage.
<em>EXSY</em>, <em>40</em>(4), e13206. (<a
href="https://doi.org/10.1111/exsy.13206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has emerged as a promising platform that grants users direct yet shared access to computing resources and services without worrying about the internal complex infrastructure. However, secure data storage and data retrieval is the basic characteristics. Therefore, this paper proposes a novel double keys adapted elliptic curve cryptography (DKECC) and log normalized Gaussian sigmoid adaptive neuro-fuzzy interference system (LGS-ANFIS) based secure resource allocation system in decentralized cloud storage. Initially, the input data undergoes data fragmentation. After that, encryption is carried out using DKECC algorithm. Next, hash code generation using Pearson hash function is done. Then, resource availability is estimated via logistic sine chaotic mapping indulged rock hyraxes swarm optimization technique. Afterwards, data is checked for deduplication. Finally, resource allocation through LGS-ANFIS takes place. The experimental outcomes demonstrated better results compared to baseline techniques. The modelling process showed that the suggested safe resource allocation model achieves the total security level of 96.27% while also having shorter reaction times (3018‚Äâms), greater throughputs (1258), reduced load balancing (0.355874), and reduced latency (4372‚Äâms).},
  archive      = {J_EXSY},
  author       = {Lakshmanan Karuppasamy and Venkatraman Vasudevan},
  doi          = {10.1111/exsy.13206},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13206},
  shortjournal = {Expert Syst.},
  title        = {A novel double keys adapted elliptic curve cryptography and log normalized gaussian sigmoid adaptive neuro-fuzzy interference system based secure resource allocation system in decentralized cloud storage},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based smishing message identification using
regular expression feature generation. <em>EXSY</em>, <em>40</em>(4),
e13153. (<a href="https://doi.org/10.1111/exsy.13153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in the number of undesired SMS termed smishing message and the data imbalance problem has generated a great demand for the development of more reliable anti-spam filters. State of the art machine learning approaches are being employed to recognize and separate spam messages. Most recent studies target message classification by using numerous properties and features of the words but fail to consider the circumstantial features like long-range dependencies between the words that are extremely important in identifying smishing messages. The idea is to develop an intelligent model that will distinguish between smishing messages and ham messages, by adopting a combined approach of regular expression (Regex), machine learning (ML) and deep learning (DL) models. Regex rules are generated using the dataset&#39;s spam messages for the purpose of refining the dataset. Support vector machine (SVM), Multinomial Naive Bayes and Random Forest are included under machine learning models and long short-term memory (LSTM), bidirectional long short-term memory (Bi-LSTM), stacked LSTM and stacked Bi-LSTM are included under deep learning models. The comparison between machine learning models and deep learning models is also carried out based on the performance evaluation parameters namely accuracy, precision, recall and F 1 score of the models. It is observed that deep learning models perform better than machine learning models and the introduction of a regular expression to the dataset increases the efficiency of both the deep learning models and machine learning models.},
  archive      = {J_EXSY},
  author       = {Aakanksha Sharaff and Vrihas Pathak and Siddhartha Shankar Paul},
  doi          = {10.1111/exsy.13153},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13153},
  shortjournal = {Expert Syst.},
  title        = {Deep learning-based smishing message identification using regular expression feature generation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CA-MLBS: Content-aware machine learning based load balancing
scheduler in the cloud environment. <em>EXSY</em>, <em>40</em>(4),
e13150. (<a href="https://doi.org/10.1111/exsy.13150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is the on-demand provision of computing resources over the Internet, such as cloud storage, computing power, network, and so on. Cloud computing has several advantages, including high speed, cost reduction, data security, and scalability. The main challenge in cloud environment is to balance the workloads and network traffic among the available resources to achieve maximum performance. Several methods have been proposed in the literature for effective load balancing, including heuristic, meta-heuristic, and hybrid algorithms. The performance of these techniques has been improved by combining machine learning based Artificial Intelligence (AI) techniques and meta-heuristic algorithms. Most of the existing load balancing techniques are not aware of the content type of user tasks. However, from the literature, the content type of the tasks can be very effective to design a balanced workload distribution system in the cloud. In this work, a novel AI-assisted hybrid approach called Content-aware Machine Learning based Load Balancing Scheduler (CA-MLBS) is proposed. The scheduling system CA-MLBS combines machine learning and meta-heuristic algorithms to perform classification based on file type. To achieve this, a Support Vector Machine (SVM) based classifier is used to classify user tasks into different content types such as video, audio, image, and text. A metaheuristic algorithm based on Particle Swarm Optimization (PSO) is used to map users&#39; tasks in the cloud. The proposed approach was implemented and evaluated using a renowned Cloudsim simulation kit and compared with Ant Colony Optimization File Type Format (ACOFTF) and Data Files Type Formatting (DFTF) heuristics. The results of the proposed study show that the proposed CA-MLBS technique achieved improvements of up to 29%, 29%, and 44% in terms of makespan, response time, and throughput, respectively.},
  archive      = {J_EXSY},
  author       = {Muhammad Adil and Said Nabi and Muhammad Aleem and Vicente Garcia Diaz and Jerry Chun-Wei Lin},
  doi          = {10.1111/exsy.13150},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13150},
  shortjournal = {Expert Syst.},
  title        = {CA-MLBS: Content-aware machine learning based load balancing scheduler in the cloud environment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Internet of agents system for age and gender classification
using grasshopper optimization with deep convolution neural networks.
<em>EXSY</em>, <em>40</em>(4), e13115. (<a
href="https://doi.org/10.1111/exsy.13115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The internet of agents (IoA) is an emergent model, which mainly intends to resolve the limitations of the internet of things (IoT) devices with respect to reasoning and social competencies for improving proactivity, intelligence, and interoperability. This article presents a novel grasshopper optimization with deep learning enabled multi-agent system for age and gender classification (GOADL-MASAGC) model for IoA. The proposed GOADL-MASAGC technique intends to categorize age as well as gender. The proposed GOADL-MASAGC technique applies a multi-agent system, which incorporates distinct processes for age and gender classification like pre-processing, feature extraction, and classification. Besides, the GOADL-MASAGC technique enables to performance concurrent process of classification and regression to identify age and gender respectively. In addition, the GOA with Capsule Network (CapsNet) model was executed for deriving a suitable group of feature vectors and the GOA is employed as a hyperparameter optimizer. Finally, wavelet kernel extreme learning machine (WKELM) was employed as a classifier for gender classification and deep belief network (DBN) is used as a regression approach for age recognition. For demonstrating the improved performance of the GOADL-MASAGC model, a series of simulations were executed and the outcomes are examined in various aspects. The extensive comparative analysis reported the enhanced outcomes of the GOADL-MASAGC approach over the existing methods.},
  archive      = {J_EXSY},
  author       = {Ashit Kumar Dutta and Basit Qureshi and Yasser Albagory and Majed Alsanea and Deepak Gupta and Ashish Khanna},
  doi          = {10.1111/exsy.13115},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13115},
  shortjournal = {Expert Syst.},
  title        = {Internet of agents system for age and gender classification using grasshopper optimization with deep convolution neural networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Challenges and vulnerability evaluation of smart cities in
IoT device based on cybersecurity mechanism. <em>EXSY</em>,
<em>40</em>(4), e13113. (<a
href="https://doi.org/10.1111/exsy.13113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, Cyber thieves discover new software vulnerabilities and attack vectors to take advantage of them. Cyber Security (CS) specialists are having a constant battle to stay on top of the latest threats. Knowledge sharing, sometimes called intelligence sharing, can enhance collective resilience and responsiveness in the face of dangers if peers are willing to share information proactively. Internet of Things (IoT) is the buzz word which integrates almost all the technology concerning computation and communication. Before the IoT goal can be fully realized, some barriers must be addressed, with security and privacy being two of the most essential. When IoT devices connect to an IoT gateway or other edge device, the sensor data they gather is either transmitted to the cloud for analysis or processed locally. Based on the received information, they act and it can expose a device as well as its data. Insufficient or no device authentication and authorization are common problems, as is poor or no encryption. Due to restricted energy supplies and low processing power, IoT security and privacy are complex, especially for resource-constrained devices. In this article, IoT-CS security problems and associated threats are discussed. The overarching goals were identifying assets and exploring possible threats, assaults, and vulnerabilities encountered by the IoT in the sectors like healthcare, transportation, utilities, safety, and environmental health that might be significantly improved with IoT-CS deployment in smart cities. The predominant challenges in the deployment of the IoT related technologies had been studied in the aspects of cyber security and mitigation strategies had been discussed.},
  archive      = {J_EXSY},
  author       = {Alaa Omran Almagrabi},
  doi          = {10.1111/exsy.13113},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13113},
  shortjournal = {Expert Syst.},
  title        = {Challenges and vulnerability evaluation of smart cities in IoT device based on cybersecurity mechanism},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A rendezvous block-based authentication framework for
service-level security in decentralized cloud resources. <em>EXSY</em>,
<em>40</em>(4), e13106. (<a
href="https://doi.org/10.1111/exsy.13106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud services are distributed from manifold sources with autonomous security procedures. The conventional authentication methods result in asynchronous security towards multi-resource access and sharing. This provides volatile authentication for sequential service sessions. For alleviating this issue, a Rendezvous Block-based Authentication Framework (RBAF) is proposed. This framework is backboned with a blockchain paradigm that differentiates authentication based on rendezvous and asynchronous attributes. The modifications in initial and final attributes are observed, intended for training, and supplanted with service-dependent authentications. In the different sessions, attribute-based authentication and agreed end-to-end security are administered using agreed keys that are valid within the sessions. This key generation is modified using the new session and user attributes based on learning recommendations. The ledger paradigm records the session and its associated attributes for different training instances, based on which service flexibility is ensured. The proposed framework&#39;s performance is verified using the metrics service distribution ratio, false rate, session uptime, and service delay.},
  archive      = {J_EXSY},
  author       = {Pajany Murugaiyan and Zayaraz Godandapani},
  doi          = {10.1111/exsy.13106},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13106},
  shortjournal = {Expert Syst.},
  title        = {A rendezvous block-based authentication framework for service-level security in decentralized cloud resources},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent edge enabled 6G-flying ad-hoc network
ecosystem for precision agriculture. <em>EXSY</em>, <em>40</em>(4),
e13090. (<a href="https://doi.org/10.1111/exsy.13090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle based precision agriculture is a predominant research area. The modern flying ad-hoc network leverages the advanced low latency vehicular communication and intelligent computing paradigms that help the ecosystem to grow up to the next level. In this work, we propose an ecosystem for precision agriculture that leverages the use of the opportunistic MQTT protocol in an edge-enabled intelligent drone network for sensing and performing crop prediction using an intelligent ensemble machine learning model. The proposed approach leverages the edge computing system that requires low energy devices and also exploits the ultra-low latency opportunistic message transfer methodology. The experimental results show the maximum of 0.9 message delivery ratio and a minimum of 600‚Äâms latency is achieved by opportunistic MQTT protocol in an ultra-low latency sparse network scenario. A weighted ensemble model is deployed onto the edge enabled devices or the drones. An accuracy of 96.5% is achieved in predicting the type of crops that can be grown in the soil about the selected area of interest.},
  archive      = {J_EXSY},
  author       = {Amartya Mukherjee and Ayan Kumar Panja and Nilanjan Dey and Rub√©n Gonz√°lez Crespo},
  doi          = {10.1111/exsy.13090},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13090},
  shortjournal = {Expert Syst.},
  title        = {An intelligent edge enabled 6G-flying ad-hoc network ecosystem for precision agriculture},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective memetic approach for the optimal web
services composition. <em>EXSY</em>, <em>40</em>(4), e13084. (<a
href="https://doi.org/10.1111/exsy.13084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service composition is the process of combining a set of elementary or atomic services. The aim is to produce a new composite service to satisfy the user&#39;s request that cannot be satisfied by the atomic services. Combining multiple services is a complex problem that has been the subject of several research studies. The meta-heuristic approaches are good techniques that have been used to solve several complex problems in various domains. These techniques are able to discover promising search regions and locate good quality solutions in reasonable time without exploring the whole solution space. In this paper, we deal with the problem of optimal web service composition by using meta-heuristic approaches. Given a set of services and a set of tasks to be completed, the problem is to find the best set of services composition to complete all tasks where each service must be assigned to a given task. This problem can be modelled as a combinatorial optimization problem with a set of objective functions that need to be optimized. We search for a composite service that allows us to execute the considered tasks and offers the best quality of services (QoS). More precisely, we search for an execution plan that indicates for each task the assigned service. First, we propose a multi-objective local search based meta-heuristic (MO-LS) and a multi-objective genetic algorithm (MO-GA) to handle our problem. Then we propose a multi-objective memetic algorithm (MO-MA) that combines the two methods LS and GA. The role of GA is to detect promising regions to be explored. The role of LS is to exploit efficiently the potential regions created by GA. Four objective functions are used to compute the Pareto optimal set of solutions. The main objective is to minimize cost and time and to maximize availability and reputation and produce a good composite service. The three proposed approaches namely MO-LS, MO-GA, and MO-MA are evaluated on some datasets generated randomly and on the well-known QWS dataset to select the best fit services in terms of maximum or minimum aggregated end-to-end QoS parameters. The numerical results are encouraging and demonstrate the effectiveness of the proposed MO-MA for the web service composition.},
  archive      = {J_EXSY},
  author       = {Yacine Azouz and Dalila Boughaci},
  doi          = {10.1111/exsy.13084},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13084},
  shortjournal = {Expert Syst.},
  title        = {Multi-objective memetic approach for the optimal web services composition},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing time delay in cloud using enhanced multi-hold
inherited maximization algorithm to reduce cost and improve bandwidth.
<em>EXSY</em>, <em>40</em>(4), e13073. (<a
href="https://doi.org/10.1111/exsy.13073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the time delay in the cloud is one of the essential parameters to be considered for providing an efficient environment for the user to work. Virtual machine (VM) allocation is performed on various heuristic and genetic algorithms (GAs) to schedule the VM to the users in lesser time. However, the scheduling mechanisms do not provide the best serving VM among different data centres. This paper proposes an Enhanced multi-hold inherited maximization (Enhanced MHIM) algorithm to provide an optimal allocation of VMs across multiple geographically distributed data centres. Based on load information shared between the various data centres, the algorithm computes the best performing machine among the data centres and shares the template for allocating the new device to the user. As a result, the workload is completed quicker by assigning to each user optimally. Heuristic scheduling algorithms, including Min-Min, Ant Colony Optimization (ACO), Particle Swarm Optimization (PSO), and GA, are compared with the proposed algorithm. Experimental results indicate that the proposed algorithm is efficient by maximum reduction of 12.63% of the makespan time, 13.06% of execution time, 3.89% of computational cost, and producing the 3√ó times best serving VM.},
  archive      = {J_EXSY},
  author       = {Dharmalingam Sundaram Manoj Kumar and Premkumar Sriramya},
  doi          = {10.1111/exsy.13073},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13073},
  shortjournal = {Expert Syst.},
  title        = {Optimizing time delay in cloud using enhanced multi-hold inherited maximization algorithm to reduce cost and improve bandwidth},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 6G wireless communication assisted security management using
cloud edge computing. <em>EXSY</em>, <em>40</em>(4), e13061. (<a
href="https://doi.org/10.1111/exsy.13061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security management is the process of identifying a company&#39;s assets (such as people, buildings, equipment, systems, and information assets) and then developing, documenting, and implementing policies and procedures to secure those assets. Meanwhile, artificial intelligence (AI) applications are flourishing thanks to advances in deep learning and numerous hardware architecture improvements based on cloud edge computing (CEC) issues are associated with the Internet of Things (IoT), including inadequate security measures, user ignorance, and the dreaded active monitoring. Therefore, a 6G wireless communication-assisted security management using artificial intelligence (WC-SM-AI) has been introduced to enhance security. The energy-efficient 6G real-time communication framework and the enhanced deep neural network security module are key components of this architecture. The first module optimizes network lifespan and spectral efficiency while reducing energy consumption and latency. Another module offers a more secure network connection while increasing privacy, data integrity, and access. For this reason, this article discusses how AI can strengthen the security of 6G networks while promoting strategy problems and solutions.},
  archive      = {J_EXSY},
  author       = {M. M. Kamruzzaman},
  doi          = {10.1111/exsy.13061},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13061},
  shortjournal = {Expert Syst.},
  title        = {6G wireless communication assisted security management using cloud edge computing},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smart decision for device selection in D2D-assisted
multi-path video transmission network. <em>EXSY</em>, <em>40</em>(4),
e13049. (<a href="https://doi.org/10.1111/exsy.13049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Watching the live video on a bus/train/tram has become an important pattern for people to enjoy their travel time. Due to the complex environment changes and obstacles caused by the rapid movement, the network state of the user device is often unstable. A large number of interruptions and resolution reductions seriously affect the user&#39;s watching experience. Through direct transmission between devices, Device-to-Device (D2D) can effectively improve video quality. Accurate decision for helper device selection is the key to optimize video streaming services based on D2D technology. However, the existing methods through D2D rarely consider the states and the intention of the helper devices when selecting cooperative devices, such as the watching status, the state of charge, the state of cache, and the connection quality. This is highly probable to reduce the efficiency of D2D transmission and the cooperation enthusiasm of helper devices. In addition, the existing methods seldom consider the multiple network interfaces in the device. In view of these challenges, we propose a dynamic smart decision method for d evice s election in D 2D- a ssisted m ulti- p ath video transmission network (named DS-DAMP ). Specifically, this method takes the current states and the resource constraints into consideration to select the appropriate helper devices for video cooperative transmission, so as to improve the cooperation satisfaction of helper devices and optimize the video quality of the requesting device. Besides, we make full use of multiple network interfaces to realize multi-path parallel transmission between devices. A large number of experiments have proved the good performance of our method in terms of video quality and network utility.},
  archive      = {J_EXSY},
  author       = {Xuan Zhao and Bowen Liu and Xutong Jiang and Wenda Tang and Wanchun Dou},
  doi          = {10.1111/exsy.13049},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13049},
  shortjournal = {Expert Syst.},
  title        = {Smart decision for device selection in D2D-assisted multi-path video transmission network},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Node localization and data aggregation scheme using cuckoo
search and neural network. <em>EXSY</em>, <em>40</em>(4), e13033. (<a
href="https://doi.org/10.1111/exsy.13033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among multi-hop technology, wireless sensor network (WSN) has been extensively investigated owing to its potential application in vivid fields. However, a key issue probing WSN is node location that is also the major area of interest in the present paper. The paper takes advantage of cuckoo search (CS) as the swarm intelligence technique used to address the issues of identification of malicious or unknown nodes within the network. The distance vector (DV)-hop is used to determine the distance between the anchor sensor node and the unknown or the node with compromised nature. Then, artificial neural network architecture is used to distinguish the nodes based on the characteristics. This is followed by the evaluation of the proposed scheme to offer reliable data transmission using CS optimized data aggregation scheme. The simulation analysis over 1000 deployed nodes shows that CS significantly decreases the localization error to 0.494 and localization time to 0.058‚Äâs along with 15%‚Äì20% improvement in the throughput and packet delivery ratio. This shows that the proposed CS optimized architecture is successful in identifying the position of unknown nodes as well as compromised nodes that significantly improved the reliability of the data transmission.},
  archive      = {J_EXSY},
  author       = {Simarjeet Kaur and Navdeep Kaur and Kamaljit Singh Bhatia and Mohd Abdul Rahim Khan and Manoj Gupta and Naveen Kumar Sharma and Sunil Kumar Sharma},
  doi          = {10.1111/exsy.13033},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13033},
  shortjournal = {Expert Syst.},
  title        = {Node localization and data aggregation scheme using cuckoo search and neural network},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A convex problem optimization solution for information
management in edge computing platforms. <em>EXSY</em>, <em>40</em>(4),
e13017. (<a href="https://doi.org/10.1111/exsy.13017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent computing and social optimization techniques have been used to manage mobile edge (ME) environments for pervasive services. However, the service demand is the prime cause for requiring flawless information management. This article proposes a coherent information management (CIM) process for resolving the convex optimization problems by considering the flaws in raw information management. The information management problem for pervasiveness is defined as a nonlinear problem for user-to-information availability. According to the CIM, pervasive convergence is identified for the available information. Furthermore, convolutional neural learning identifies unavailable or utilized information between the service provider and the end-user layers. This identification improves the training rate for convergence improvement and information coherency. Therefore, the nonlinear pervasiveness is distributed between different service providers, improving efficiency. Therefore, the proposed optimization method achieves a 7.4% high admittance ratio, 8.28% high utilization, 25.25% less allocation time, and 24.4% less waiting time for different availability ratios for different information availability ratios.},
  archive      = {J_EXSY},
  author       = {Aldosary Saad and Zafer Almakhadmeh},
  doi          = {10.1111/exsy.13017},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13017},
  shortjournal = {Expert Syst.},
  title        = {A convex problem optimization solution for information management in edge computing platforms},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wireless electroencephalogram monitoring system for
deciphering neurological disorders using brain connectivity patterns.
<em>EXSY</em>, <em>40</em>(4), e13016. (<a
href="https://doi.org/10.1111/exsy.13016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last two decades, there has been a tremendous increase in research activity on brain connectivity studies and its application in different neurological disorders. Studies have been focused on different connectivity patterns generated and potential biomarkers that could be derived to find the etymology of the disorder. In this review, the focus is on the utilization of wireless electroencephalogram monitoring system for functional connectivity analysis and its capacity for deciphering neurological disorders. The paper reviews different methods adopted to estimate connectivity and the possible convergence of connectivity patterns in four neurological disorders: epilepsy, autism spectrum disorder, Alzheimer and Parkinson&#39;s disease. The paper reviews the current status of connectivity research in the aforementioned neurological disorders and its potential in developing a smart e-health service.},
  archive      = {J_EXSY},
  author       = {Surya Das and Subha D. Puthankattil},
  doi          = {10.1111/exsy.13016},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13016},
  shortjournal = {Expert Syst.},
  title        = {Wireless electroencephalogram monitoring system for deciphering neurological disorders using brain connectivity patterns},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing the dependency of having prior domain knowledge for
effective online information retrieval. <em>EXSY</em>, <em>40</em>(4),
e13014. (<a href="https://doi.org/10.1111/exsy.13014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sometimes Internet users struggle to find what they are looking for on the Internet due to information overload. Search engines intend to identify documents related to a given keyphrase on the Internet and provide suggestions. Having some background knowledge about a topic or a domain will help in building effective search keyphrases that will lead to accurate results in information retrieval. This is further pronounced among students that rely on the internet to learn about a new topic. Students might not have the required background knowledge to build effective keyphrases and find what they are looking for. In this research, we are addressing this problem, and aim to help students find relevant information online. This research furthers existing literature by enhancing information retrieval frameworks through keyphrase assignment, aiming to expose students to new terminologies, therefore reducing the dependency of having background knowledge about the domain under study. We evaluated this framework and identified how it can be enhanced to suggest more effective search keyphrases. Our proposed suggestion is to introduce a keyphrase Ranking Mechanism that will improve the keyphrase assignment part of the framework by taking into consideration the part-of-speech of the generated keyphrases. To evaluate the proposed approach, various data sets were downloaded and processed. The results obtained showed that our proposed approach produces more effective keyphrases than the existing framework.},
  archive      = {J_EXSY},
  author       = {Omar Zammit and Serengul Smith and David Windridge and Clifford De Raffaele},
  doi          = {10.1111/exsy.13014},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13014},
  shortjournal = {Expert Syst.},
  title        = {Reducing the dependency of having prior domain knowledge for effective online information retrieval},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A 64-channel scheme for autism detection via scaled
conjugate gradient-based neural network classification of
electroencephalogram ripples‚Äô complexity. <em>EXSY</em>, <em>40</em>(4),
e13000. (<a href="https://doi.org/10.1111/exsy.13000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The description of autism spectrum disorder (ASD) includes typically the alterations in social skills and attitude as well as problems of communication. At present, the most promising approaches suggested for ASD automatic diagnosis are the machine learning (ML)-based procedures that begin by the identification and extraction of the most discriminant features. One of the proposed discriminant features of ASD is electroencephalogram (EEG) entropy that reflects the level of complexity or randomness of brain dynamics. The two main techniques currently used to describe the entropy in ASD are multi-scale entropy (MSE) and spectral entropy (SE). However, MSE is unable to track the exact frequency content on the different time scales that suffer‚Äîthemselves‚Äîfrom mismatching. In addition, the outcomes of SE approaches in literature are contradictory and confusing about the altered band, direction of change as well as about the affected brain region. The common point in those SE approaches is the examination of one or more of the well-known frequency bands: Delta, Theta, Alpha, Beta and low Gamma. Hence, the range of high Gamma ripples (&gt;80‚ÄâHz) is barely examined in ASD literature due to the needed high carefulness in its technical acquisition, detection and processing as well as to the common acceptance of the idea that it is rather related to pathological seizure activity; which is a misconception recently refuted. Further, the study of automatic ASD diagnosis based on ripples is almost inexistent in literature. The present work suggests an accurate automatic technique to classify ASD and neuro-typical EEG entropy based on the whole range of ripples (75‚Äì250‚ÄâHz): (1) EEG signals are collected (120 ASD and neuro-typical children) by an efficient 64-channel system highly suitable in noisy environment with movement artefacts. (2) All signal disturbances are automatically removed. (3) The power spectra of all EEG channels are computed. (4) Shannon entropy values are calculated for the range of ripples without filtering. (5) Scaled Conjugate Gradient-based Neural Network (NN) classification, k -fold cross validation and T-test are applied to entropy values. (6) Statistical assessment is conducted. The best testing and the overall accuracies were found to be 98.9% and 95.0% respectively.},
  archive      = {J_EXSY},
  author       = {Enas Abdulhay and Maha Alafeef and Hikmat Hadoush and N. Arunkumar},
  doi          = {10.1111/exsy.13000},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e13000},
  shortjournal = {Expert Syst.},
  title        = {A 64-channel scheme for autism detection via scaled conjugate gradient-based neural network classification of electroencephalogram ripples&#39; complexity},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective energy usage and data compression approach using
data mining algorithms for IoT data. <em>EXSY</em>, <em>40</em>(4),
e12997. (<a href="https://doi.org/10.1111/exsy.12997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of technology and communication system paved way for the development of the internet of things (IoT). The IoT system generates diversified data and the quantity of the data is also huge. The IoT systems are developed to adhere to the situation and to make intelligent decisions in a specified time. Hence, the IoT system necessitates high processing and storage environment, which makes the effective response in a short-duration. The data transmission across the mobile nodes and cloud service has made huge utilization of energy. The storage and energy consumption are considered as major issues in the IoT system whereas these issues will reflect in the performance of the IoT system. Initiation of edge computing into the IoT system permits the workload to be offloaded from the cloud providers, which is attained from the closer location of the source of data. This improves privacy, minimizes the saving time, and traffic. In this article, an Effective Energy usage and Data Compression Approach using Data Mining proposed for IoT data. The proposed approach is investigated by considering the driving behaviour and it achieves effective compression without influencing the quality of the data. The stress level of the driver is also identified with high accuracy.},
  archive      = {J_EXSY},
  author       = {Sathishkumar Karupusamy and J. Refonaa and Sakthidasan Sankaran and Priyanka Dahiya and Mohd Anul Haq and Anil Kumar},
  doi          = {10.1111/exsy.12997},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12997},
  shortjournal = {Expert Syst.},
  title        = {Effective energy usage and data compression approach using data mining algorithms for IoT data},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid multi agent optimization for optimal battery storage
using micro grid. <em>EXSY</em>, <em>40</em>(4), e12995. (<a
href="https://doi.org/10.1111/exsy.12995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to rising of power demands and distributed renewable power saturation, determining optimal capability of the battery energy storage system (BESS) and demand response (DR) inside the microgrid (MG) is critical. To overcome these issues, research proposed in this manuscript employs a hybrid swarm intelligence approach that incorporates game theory. Both BESS and DR concepts are used in the hybrid method operation. The proposed method employs multi-agent guiding particle swarm optimization (MAPS) and Halton sequence and social motivation strategy based on the grey wolf optimizer (HSGW). The HSGW method helps in minimizing power utilization and energy loss. The results show that, when compared with the traditional methodology, the proposed MAPS-HSGW method provides the cost-effective selection between the agents and also decreases the operational cost by about 7.5%. Moreover, it was discovered that the proposed MAPS-HSGW method outperforms other techniques in terms of results‚Äô quality and computational time metrics.},
  archive      = {J_EXSY},
  author       = {Nebojsa Bacanin},
  doi          = {10.1111/exsy.12995},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12995},
  shortjournal = {Expert Syst.},
  title        = {Hybrid multi agent optimization for optimal battery storage using micro grid},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A supervised learning-based approach for focused web
crawling for IoMT using global co-occurrence matrix. <em>EXSY</em>,
<em>40</em>(4), e12993. (<a
href="https://doi.org/10.1111/exsy.12993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irrelevant search results for a given topic end up wasting search engine users&#39; time. A learning focused web crawler downloads relevant URLs for a given topic using machine-learning algorithms. The dynamic nature of the web is a challenge in related computation for focused web crawlers. Studies have shown that the learning focused crawler utilizes term frequency-inverse document frequency (TF-IDF) to compute the relevance between a web page and a given topic. The TF-IDF detects similarity of the given topic to its co-occurrence on the web page. The necessity of efficient mechanism to compute the relevance of URLs syntactically and semantically has led to the proposal of this paper with a word embedding approach to compute the relevance of the web page. The global vector representation cosine similarity is calculated between a topic and the web page contents. The calculated cosine similarity is provided as input to the trained random forest classifier to predict the relevancy of the web page. The evaluation results proved that the proposed crawler produced an average of 0.41 and of 0.59, which outperformed other learning-focused crawlers on support vector machines, Naive Bayes and artificial neural networks.},
  archive      = {J_EXSY},
  author       = {S Rajiv and C Navaneethan},
  doi          = {10.1111/exsy.12993},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12993},
  shortjournal = {Expert Syst.},
  title        = {A supervised learning-based approach for focused web crawling for IoMT using global co-occurrence matrix},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fed_ADBN: An efficient intrusion detection framework based
on client selection in AMI network. <em>EXSY</em>, <em>40</em>(4),
e12983. (<a href="https://doi.org/10.1111/exsy.12983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data transmission between smart meters and data center is facing network security threats in advanced metering infrastructure of smart grid. The traditional solution is to move the data to the data center to build a centralized attack detection model, or divide the collected data into several independent and identically distributed datasets to build a distributed attack detection model. However, the long-distance transmission and the centralized storage of data not only increase the communication overhead and time overhead, but also increase the risk of being attacked, causing privacy disclosure during the process of building the model. In this paper, we propose an efficient intrusion detection framework Fed_ADBN based on federated attention deep belief network and client selection. Clients cooperate with the data center to jointly build a horizontal federated learning framework. Under the premise of protecting data security by keeping data on the clients, we design a client selection algorithm based on client computing power, communication quality and security risks, which can improve the operating efficiency of federated learning. We also deploy a deep belief neural network with attention mechanism in each client to accurately detect possible network attacks in AMI network in real time. Experimental results show that compared with state-of-the-art methods, the proposed framework can not only maintain good detection accuracy but also protect privacy.},
  archive      = {J_EXSY},
  author       = {Zhuoqun Xia and Yaling Chen and Bo Yin and Haolan Liang and Hongmei Zhou and Ke Gu and Fei Yu},
  doi          = {10.1111/exsy.12983},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12983},
  shortjournal = {Expert Syst.},
  title        = {Fed_ADBN: An efficient intrusion detection framework based on client selection in AMI network},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory clustering and query processing analysis
framework for knowledge discovery in cloud environment. <em>EXSY</em>,
<em>40</em>(4), e12968. (<a
href="https://doi.org/10.1111/exsy.12968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a cloud environment, resources should be acquired quickly and automatically released at runtime. Traditional trajectory data partitions, indexing and query processing techniques are extended, so that they can take advantage of the cloud of large clusters of highly parallel processing capabilities. There are ways with trajectory data in the cloud database query processing. The advanced sensing techniques available today caused the existence of many types of trajectory datasets whose study can form a strong basis for decision making regarding a data context. Trajectory data generally includes trajectory classification, trajectory clustering, and trajectory associations and so on. All these tools need trajectory similarity measures to get comparisons in trajectory data. Some of the popular existing methods of trajectory similarity include Euclidean distance, semantic distance and there near variants. Each variant of these techniques exhibits notable differences in measurement of similarity and computational difficulty. This paper targeted to deal with this situation with the goal of lower computational efforts in trajectory similarity computations. A two-phase trajectory similarity measure is defined. The first phase does point level analysis of trajectory points representing all trajectories and the following phase use the analysis to find the similarities. Using a hybrid similarity framework, new means for trajectory clustering and trajectory query processing are developed. From trajectory analysis followed by trajectory clustering, evaluation of cluster quality and processing of trajectory query are undertaken in this paper in different ways from the existing methods. All the proposed developments are tested with trajectory datasets with multiple attributes and the results are more favour to proposed framework than the existing ones.},
  archive      = {J_EXSY},
  author       = {Selvin Shabu Lilly Pushpam Jany Shabu and Kusum Yadav and Elham Kariri and Kamal Kumar Gola and Mohd AnulHaq and Anil Kumar},
  doi          = {10.1111/exsy.12968},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12968},
  shortjournal = {Expert Syst.},
  title        = {Trajectory clustering and query processing analysis framework for knowledge discovery in cloud environment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Numerical investigation of hemodynamic pattern in carotid
artery dynamic aneurysm on bifurcation region for early clinical
decision making. <em>EXSY</em>, <em>40</em>(4), e12951. (<a
href="https://doi.org/10.1111/exsy.12951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a numerical estimation of wall shear stress (WSS) and study on hemodynamic pattern in carotid artery (CA) near the bifurcation region with dynamic growth of aneurysm using computational fluid dynamics (CFD) for making early clinical decision is presented. Aneurysm in the carotid artery affects the blood supply to brain and if it is untreated at early stage may lead to sudden death. Computed tomography images of four different cases of stroke subjects scanned of 600 slices with 1‚Äâmm resolution with neighbouring layers from neck to head are considered for this study. Numerically a CA with bifurcation region is developed from these images and aneurysm is and allowed to grow dynamically from 10 to 20‚Äâmm. WSS and hemodynamic pattern is estimated numerically using Ansys platform at various region of interest in both rigid and compliant wall conditions. The arterial wall thinning was analytically estimated using thick cylinder theory to estimate the increase in aneurysm under various stress conditions (rest and exercise conditions). The findings show that WSS is found to reduce at the aneurysm region with a corresponding strong vortex pattern. Thus, the vortex pattern could be the cause of tissue damage and thinning rather than WSS. The corresponding increase in velocity gradient at the bifurcation region is also captured. This high gradient is the cause for higher WSS at the bifurcation region which is a possible cause for the formation of plaque and arteriosclerosis. It is interesting to note that the WSS did not change drastically for 10 and 15‚Äâmm aneurysm but a large change was seen from normal to 10 and 15‚Äì20‚Äâmm respectively. This study provides a better clinical insight on the effect of aneurysm in CA bifurcation region using a systematic approach to numerical modelling compared to traditional imaging modalities. It can be used as an adjunct tool for physicians and surgeons for planning necessary clinical interventions.},
  archive      = {J_EXSY},
  author       = {Mohamed Yacin Sikkandar and Ahmed Ayman A. Almedhun},
  doi          = {10.1111/exsy.12951},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12951},
  shortjournal = {Expert Syst.},
  title        = {Numerical investigation of hemodynamic pattern in carotid artery dynamic aneurysm on bifurcation region for early clinical decision making},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model innovation of students‚Äô mental health education from
the perspective of big data. <em>EXSY</em>, <em>40</em>(4), e12948. (<a
href="https://doi.org/10.1111/exsy.12948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the effect of college students&#39; mental health education, and to meet the needs of mental health education data processing, this article combines big data technology and gene regulation ideas to propose a mental health education data processing algorithm based on big data technology and immunotherapy gene regulation network. Moreover, this article uses improved algorithms to extract features from mental health education network data, controls the problems in mental health education in real-time, and proposes targeted improvement strategies. In addition, this article combines the improved algorithm to construct a mental health education system based on big data technology and immunotherapy gene regulation network are combined for an improved algorithm to process mental health education data. Finally, this article verifies the effectiveness of the system through experimental research.},
  archive      = {J_EXSY},
  author       = {Bin Yang},
  doi          = {10.1111/exsy.12948},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12948},
  shortjournal = {Expert Syst.},
  title        = {Model innovation of students&#39; mental health education from the perspective of big data},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using electroencephalogram classification in a convolutional
neural network, infer privacy on healthcare internet of things 5.0.
<em>EXSY</em>, <em>40</em>(4), e12942. (<a
href="https://doi.org/10.1111/exsy.12942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When enabled by the internet of health things (IoHT), brain neuroscience may conduct online analysis of brain information through multi-variate electroencephalogram (EEG) classification, which would be a requirement for the recent surge in biofeedback technologies and medical supervision. With the ever-increasing privacy issues and vulnerabilities of conventional methods, a universal and reliable-based authentication framework for smart IoHT application with 5G technology (healthcare 5.0) is needed. Research teams have come to trust the EEG features because of their reliability, durability and universality. Fortunately, the testing paradigm&#39;s restricted functionality and poor classification accuracy have kept an EEG-based identity authentication scheme from being widely seen in IoHT scenarios. However, due to unsatisfactory categories and the failure of a reliable identity authentication scheme, it remains important in research challenges. This research presents the design of an EEG identity authentication system supported via convolutional neural network classification includes cloud support storage methodology in the healthcare 5.0 environment, resulting in extremely high reliability, consistency and protection for the next generation of smart systems. The experimental results indicate that the accuracy and efficacy of the user authentication expect a higher legal probability of success and a lower unauthorized likelihood of success from a safety perspective. As compared to other frameworks, traditional EEG-based authorization approaches test results to reveal that the proposed methodology yields the desired classification accuracy of 97.6%. The experiment performance on an authentication scenario is structured to prove that the proposed method is efficient, reliable and accurate.},
  archive      = {J_EXSY},
  author       = {U. S. B. K. Mahalaxmi and Kishan Bhushan Sahay and R. Sabitha and Sulaima Lebbe Abdul Haleem and Prabjot Kaur and P. Vijayakumar},
  doi          = {10.1111/exsy.12942},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12942},
  shortjournal = {Expert Syst.},
  title        = {Using electroencephalogram classification in a convolutional neural network, infer privacy on healthcare internet of things 5.0},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generative adversarial learning for optimizing ontology
alignment. <em>EXSY</em>, <em>40</em>(4), e12936. (<a
href="https://doi.org/10.1111/exsy.12936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge computing in knowledge-defined network (KDN) is a kind of distributed computing architecture, and the format of edge resources stored in different edge computing nodes are different, which yields the data heterogeneity problem and hampers the interaction between edge nodes. Ontology is considered as the solution of data heterogeneity on Semantic Web, and matching ontologies is a high-efficiency method of addressing the data heterogeneity problem. Ontology meta-matching investigates how to determine the optimal weights to aggregate multiple similarity measures to achieve high-quality ontology alignment, which is a challenge about nonlinear mathematical problem in ontology matching domain. To face this challenge, unsupervised learning method such as generative adversarial network (GAN) becomes an effective methodology. GAN consists of two models of different targets that are opposed to each other in training to produce the final best result. To improve the GAN&#39;s efficiency, this work further proposes a GAN with simulated annealing algorithm (SA-GAN), where the stagnation counter is introduced to accelerate GAN&#39;s the convergence speed. The experiment uses the famous benchmark in the ontology domain, and the comparisons with the advanced ontology matching systems shows that SA-GAN is able to find high-quality alignments to help build bridges between edge nodes on edge computing.},
  archive      = {J_EXSY},
  author       = {Xingsi Xue and Qihan Huang},
  doi          = {10.1111/exsy.12936},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12936},
  shortjournal = {Expert Syst.},
  title        = {Generative adversarial learning for optimizing ontology alignment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diagnosis of depression level using multimodal approaches
using deep learning techniques with multiple selective features.
<em>EXSY</em>, <em>40</em>(4), e12933. (<a
href="https://doi.org/10.1111/exsy.12933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a serious mental health condition that may lead to poor mental and emotional functioning at work, at school and in the family causing the mental imbalance. In worst scenarios, depression may lead to severe anxiety or suicide. Hence, it is necessary to diagnose depression at early stages. This paper elaborates the development of a novel approach for a convolutional neural network model that can examine facial images from the recorded interview sessions to discover facial patterns that could indicate depression level. The user-generated data helps to distinguish between different depressive groups with depression symptoms that can manifest people with various mental illnesses in different ways. In particular, we want to automatically predict the depression scale and differentiate depression from other mental disorders using the patient&#39;s psychiatric illness history and dynamic textual descriptions extracted from the user inputs. We apply the k-nearest neighbour algorithm on the dynamic textual descriptors to make a linguistic analysis for classifying mental illness into different classes. We apply dimensionality reduction and regression using the Random Forest algorithm to predict the depression scale. The proposed framework is an extension to pre-existing frameworks, replacing the handcrafted feature extraction technique with the deep feature extraction. The model performs 2.7% better than existing frameworks in facial detection and feature extraction.},
  archive      = {J_EXSY},
  author       = {Pratiksha Meshram and Radha Krishna Rambola},
  doi          = {10.1111/exsy.12933},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12933},
  shortjournal = {Expert Syst.},
  title        = {Diagnosis of depression level using multimodal approaches using deep learning techniques with multiple selective features},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preschool education optimization based on mobile edge
computing under COVID-19. <em>EXSY</em>, <em>40</em>(4), e12922. (<a
href="https://doi.org/10.1111/exsy.12922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has brought profound changes in people&#39;s live and work. It has also accelerated the development of education from traditional model to online model, which is particularly important in preschool education. Preschoolers communicate with teachers through online video, so how to provide high quality and low latency online teaching has become a new challenge. In cloud computing, users offload computing tasks to the cloud to meet the high computing demands of their devices, but cloud-based solutions have led to huge bandwidth usage and unpredictable latency. In order to solve this problem, mobile edge computing (MEC) deploys the server at the edge of the network to provide the service with close range and low latency. In task scheduling, edge computing (EC) devices have rational thinking, and they are unwilling to collaborate with MEC server to perform tasks due to their selfishness. Therefore, it is necessary to design an effective incentive mechanism to encourage the collaboration of EC devices. Through analysis of MEC server and EC devices, we propose a distributed task scheduling algorithm‚ÄîStackelberg game approach based on alternating direction method of multipliers, which selects the appropriate incentive mechanism to encourage the collaboration of EC devices. The experimental results demonstrate that the proposed approach can rapidly converge to a certain accuracy within 40 iterations, and in incentive mechanism comparison and quality of experience, the proposed approach also has a good performance in anti-jitter and low latency.},
  archive      = {J_EXSY},
  author       = {Hongzhi Wei and Yuqian Yang and Zhijian Liu},
  doi          = {10.1111/exsy.12922},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12922},
  shortjournal = {Expert Syst.},
  title        = {Preschool education optimization based on mobile edge computing under COVID-19},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algorithm improvement of neural network in endoscopic image
recognition of upper digestive tract system. <em>EXSY</em>,
<em>40</em>(4), e12912. (<a
href="https://doi.org/10.1111/exsy.12912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of effective gastrointestinal diseases computer-aided diagnosis tools and automatic image quality assessment algorithms is very important to improve the effectiveness of diagnosis and treatment. In order to further study the application of neural network algorithm in the endoscopic image of upper digestive tract, improve the efficiency of neural network algorithm in the field of endoscopic image. In this study, neural network algorithms were used to identify endoscopic images of the upper digestive tract. 1335 cases with upper gastrointestinal endoscopic images were collected. After the data was enlarged, it was randomly divided into training set and test set according to the proportion, and the obtained training set was input. After convolutional neural network training, an algorithm model was established in the institute. 1653 test set data samples were input into the neural network to verify the accuracy. Finally, the accuracy of the neural root network model constructed in this study reached 0.0942. Through horizontal comparison, it can be concluded that the neural network model proposed in this study not only has a higher accuracy rate, but also is better than the current existing related neural network algorithms. Based on the above experimental verification, it can be concluded that the upper gastrointestinal endoscopic image recognition algorithm based on neural network proposed in this study can more accurately and effectively identify the lesions in the upper gastrointestinal endoscopic images.},
  archive      = {J_EXSY},
  author       = {Bin Lu},
  doi          = {10.1111/exsy.12912},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12912},
  shortjournal = {Expert Syst.},
  title        = {Algorithm improvement of neural network in endoscopic image recognition of upper digestive tract system},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent algorithm for detection of dengue using
mobilenetv2-based deep features with lymphocyte nucleus. <em>EXSY</em>,
<em>40</em>(4), e12904. (<a
href="https://doi.org/10.1111/exsy.12904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dengue is a vector-borne disease that is highly endemic in countries located in tropical regions. It can cause severe complications and can even lead to death in the case of delayed diagnosis. Detection of dengue is done by manually examining the platelets and lymphocytes in Leishman&#39;s stained peripheral blood smear (PBS) images. PBS examination is considered the gold standard for diagnosing various haematological disorders. However, manual analysis of the PBS is labour-intensive, tedious, and time-consuming, requiring a skilled and experienced haematologist. Today, soft computing methods and artificial intelligence have made their way into every science and technology branch. One such area which has adopted this approach is digital pathology, for automatically identifying and diagnosing various diseases. The main objective of this work was to design an intelligent algorithm to classify normal and dengue patients with the help of digital microscopic blood smear images. A total of 94 normal and dengue-infected PBSs were acquired at a magnification of 100√ó. Grey-level segmentation based on Otsu&#39;s thresholding was used for the segmentation of the nucleus of lymphocytes. Distinct features from the nucleus that differentiated infected cells from normal were extracted using a pre-trained MobileNetV2 network and local binary pattern. Significant features were selected using the ReliefF algorithm. Subsequently, these features were fed to the support vector machine (SVM) classifier. Our proposed system gave an accuracy, sensitivity, and specificity of 95.74%, 98.14%, and 92.50%, respectively. Hence, the developed intelligent model with deep and hand-crafted features can be valuable for dengue diagnosis.},
  archive      = {J_EXSY},
  author       = {Hilda Mayrose and Niranjana Sampathila and G. Muralidhar Bairy and Sushma Belurkar and Kavitha Saravu and Akash Basu and Saman Khan},
  doi          = {10.1111/exsy.12904},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12904},
  shortjournal = {Expert Syst.},
  title        = {Intelligent algorithm for detection of dengue using mobilenetv2-based deep features with lymphocyte nucleus},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expert system for detection of congestive heart failure
using optimal wavelet and heart rate variability signals for wireless
cloud-based environment. <em>EXSY</em>, <em>40</em>(4), e12903. (<a
href="https://doi.org/10.1111/exsy.12903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Congestive heart failure (CHF) is a cardiac disorder caused due to inefficient pumping of the heart, which leads to insufficient blood flow to the various parts of the body. The electrocardiogram (ECG) is widely used for the detection of heart diseases. However, it is prone to noise resulting in the detection of P, Q, R, S, and T waves ambiguous and erroneous. The heart rate variability (HRV) is considered to be a good indicator of various cardiac abnormalities. Hence, HRV is preferred. HRV can depict the magnitude of pumping of the heart in the RR interval signals accurately. This work proposes a method to automatically identify CHF using two-band stopband energy (SBE) optimized orthogonal wavelet filter bank with HRV signals. In the proposed method, we have segmented the HRV data into lengths of 500 and 2000 samples. The HRV signals are decomposed into six sub-bands, and the wavelet coefficients obtained are used for the extraction of fuzzy entropy (FE) and log energy (LE) features. The extracted features are utilized to classify HRV signals into control and CHF-affected patients using support vector machine (SVM), bagged tree, complex tree, k-nearest neighbour (KNN), and linear discriminant classifiers. The SVM performed better than other classifiers yielding the classification accuracy and maximum classification accuracy of 99.30% with (2000 samples) using cubic SVM (CSVM). The 10-fold cross-validation method is employed during classification to reduce the over-fitting phenomenon (Sharma, Dhiman, &amp; Acharya, 2021). It appears that the proposed optimal wavelet-based automated system can identify CHF accurately using HRV signals. Hence, the model may be applied in clinical usage during an emergency employing a cloud-based wireless system after testing the developed model with more data.},
  archive      = {J_EXSY},
  author       = {Manish Sharma and Sohamkumar Patel and U. Rajendra Acharya},
  doi          = {10.1111/exsy.12903},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12903},
  shortjournal = {Expert Syst.},
  title        = {Expert system for detection of congestive heart failure using optimal wavelet and heart rate variability signals for wireless cloud-based environment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An IoT and blockchain-based approach for the smart water
management system in agriculture. <em>EXSY</em>, <em>40</em>(4), e12892.
(<a href="https://doi.org/10.1111/exsy.12892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture in rural areas facing critical issues such as irrigation with the increase in water crises followed by some other issues line seed quality, poor fertilizers and many others. The recent advances suggest that IoT and Blockchain Technology along with artificial intelligence will be most dominant technologies in near future. In this article, the integration of Internet of Things (IoT) with Blockchain technology is implemented for monitoring agricultural fields efficiently. An efficient seed quality monitoring and smart water management system is design using IoT and Blockchain Technology for managing and coordinating the use of good quality seeds and water resources among communities. The Blockchain network is implemented for securing the information and supporting trust among the members of community. The Blockchain network is also implemented for sporting trust among commercial resource constrained systems, which are communicating with the Blockchain network consisting of a hardware platform. The design of a prototype and its performance evaluation based on implementation is also presented.},
  archive      = {J_EXSY},
  author       = {Hui Zeng and Gaurav Dhiman and Ashutosh Sharma and Amit Sharma and Alexey Tselykh},
  doi          = {10.1111/exsy.12892},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12892},
  shortjournal = {Expert Syst.},
  title        = {An IoT and blockchain-based approach for the smart water management system in agriculture},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scaled feature fusion enabled convolutional neural
network for predicting fibrous dysplasia bone disorder. <em>EXSY</em>,
<em>40</em>(4), e12882. (<a
href="https://doi.org/10.1111/exsy.12882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Saranya Arirangan and Kottilingam Kottursamy},
  doi          = {10.1111/exsy.12882},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12882},
  shortjournal = {Expert Syst.},
  title        = {Multi-scaled feature fusion enabled convolutional neural network for predicting fibrous dysplasia bone disorder},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of permutation entropy and factorization network
in the classification of congenital heart disease. <em>EXSY</em>,
<em>40</em>(4), e12877. (<a
href="https://doi.org/10.1111/exsy.12877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the nonstationary feature of heart sound signal which is often disturbed by noise, a method based on permutation entropy and factorization network in the classification of heart sound of congenital heart disease is proposed. Firstly, the Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN) of normal and abnormal heart sound signals is performed to obtain several intrinsic mode function (IMF) components. Secondly, the optimal parameters in the permutation entropy algorithm are determined by C_C algorithm, and then according to the algorithm, the entropy values of the IMF components of each order are calculated to form high-dimensional eigenvectors. Finally, the recommended models of factorization machines (FMs) are used to classify the heart sounds and the generalisation performance of the FM network model are evaluated from accuracy and Area Under ROC Curve (AUC). The results show that the FM accuracy is 0.865, the AUC value is 0.887; the SVM accuracy is 0.812, and the AUC value is 0.805.},
  archive      = {J_EXSY},
  author       = {Zhe Lin and Zhiqiang Zhang and Miao Xiao and Zhijian Xiao and Qichu Wang and Qian Jiang},
  doi          = {10.1111/exsy.12877},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12877},
  shortjournal = {Expert Syst.},
  title        = {Application of permutation entropy and factorization network in the classification of congenital heart disease},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using tsetlin machine to discover interpretable rules in
natural language processing applications. <em>EXSY</em>, <em>40</em>(4),
e12873. (<a href="https://doi.org/10.1111/exsy.12873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tsetlin Machines (TM) use finite state machines for learning and propositional logic to represent patterns. The resulting pattern recognition approach captures information in the form of conjunctive clauses, thus facilitating human interpretation. In this work, we propose a TM-based approach to three common natural language processing (NLP) tasks, namely, sentiment analysis, semantic relation categorization and identifying entities in multi-turn dialogues. By performing frequent itemset mining on the TM-produced patterns, we show that we can obtain a global and a local interpretation of the learning, one that mimics existing rule-sets or lexicons. Further, we also establish that our TM based approach does not compromise on accuracy in the quest for interpretability, via comparison with some widely used machine learning techniques. Finally, we introduce the idea of a relational TM, which uses a logic-based framework to further extend the interpretability.},
  archive      = {J_EXSY},
  author       = {Rupsa Saha and Ole-Christoffer Granmo and Morten Goodwin},
  doi          = {10.1111/exsy.12873},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12873},
  shortjournal = {Expert Syst.},
  title        = {Using tsetlin machine to discover interpretable rules in natural language processing applications},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Internet of things and deep learning enabled healthcare
disease diagnosis using biomedical electrocardiogram signals.
<em>EXSY</em>, <em>40</em>(4), e12864. (<a
href="https://doi.org/10.1111/exsy.12864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent advancements in the internet of things (IoT), wearables, and sensing technologies, the quality of healthcare services gets improved and it caused a shift from conventional clinical-based healthcare to real-time monitoring. The sensors are commonly integrated into several medical gadgets to save the bio-signals produced by the physiological activities of the human body. At the same time, a biomedical electrocardiogram (ECG) signal is employed as a familiar way to examine and diagnose cardiovascular diseases (CVDs), which is rapid and non-invasive. Since the increasing number of patients degrades the classification performance due to high differences in the ECG signal patterns among several patients, computer-assisted automated diagnostic tools are essential for ECG signal classification. With this motivation, this paper introduces a new IoT and deep learning (DL) enabled healthcare disease diagnosis (IoTDL-HDD) model using biomedical ECG signals. The proposed IoTDL-HDD model aims to detect the presence of CVDs by the use of DL models in biomedical ECG signals. In addition, the proposed IoTDL-HDD model utilizes a BiLSTM feature extraction technique to extract useful feature vectors from the ECG signals. For improving the efficiency of the BiLSTM technique, the artificial flora optimization (AFO) algorithm is employed as a hyperparameter optimizer. Besides, a fuzzy deep neural network (FDNN) classifier is employed for assigning proper class labels to the ECG signals. The performance of the IoTDL-HDD model is examined on biomedical ECG signals and the outcomes are inspected in distinct features. The resultant experimental outcomes pointed out the supremacy of the IoTDL-HDD model with the maximum accuracy of 93.452%.},
  archive      = {J_EXSY},
  author       = {Ashish Khanna and Pandiaraj Selvaraj and Deepak Gupta and Tariq Hussain Sheikh and Piyush Kumar Pareek and Vishnu Shankar},
  doi          = {10.1111/exsy.12864},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12864},
  shortjournal = {Expert Syst.},
  title        = {Internet of things and deep learning enabled healthcare disease diagnosis using biomedical electrocardiogram signals},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diagnostic evaluation of low-rank matrix denoising
algorithm-based magnetic resonance imaging on tibial plateau fracture
complicated with meniscus injury. <em>EXSY</em>, <em>40</em>(4), e12858.
(<a href="https://doi.org/10.1111/exsy.12858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It was to analyse the denoising ability of low-rank matrix-based denoising algorithms for magnetic resonance imaging (MRI) images, and explore the diagnostic value of MRI for tibial plateau fracture (TPF) with meniscus injury. The Gaussian mixture algorithm was introduced based on low-rank matrix algorithm, and a new algorithm Gaussian mixture model-based low-rank matrix factorization (GLRMF) was established. Then, the root-mean-squared error (RMSE) of the GLRMF algorithm under different noise intensities was calculated, and the structural similarity (SSIM) of the GLRMF algorithm, anisotropic diffusion filter (ADF), and nonlocal means (NLM) algorithm was compared. Sixty cases of TPF patients diagnosed in the Department of Orthopaedics and Traumatology of X Hospital from March 2018 to October 2019 were recruited as the research objects. The results of Schatzker classification of patients were counted, and the results of intraoperative exploration or arthroscopy were used as the standard to analyse the accuracy of MRI in diagnosing TPFs with meniscus injuries. It was found that with the increase of noise intensity, the RMSE of the GLRMF algorithm showed an increasing trend. When the K value was 100, the denoising effect was relatively good. Under the same noise intensity, the SSIM of the GLRMF algorithm was higher than the other two algorithms. Moreover, type VI patients accounted for 55% at most. MRI diagnosed 56 cases (93.33%) of TPF patients with meniscus injury, and 52 cases (86.67%) of meniscus injury were found in intraoperative exploration. The sensitivity of MRI in diagnosing meniscus injury was 94.76%, the accuracy was 93.33%, and the specificity was 89%. It showed that the new GLRMF algorithm based on the low-rank matrix algorithm had a good noise reduction effect and significantly improved the image quality. The accuracy of knee MRI scanning was significantly improved, it was safe, and it was worthy of clinical application.},
  archive      = {J_EXSY},
  author       = {Ji Ke and Shufa Wang and Zhao Qiu and Quan Liu},
  doi          = {10.1111/exsy.12858},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12858},
  shortjournal = {Expert Syst.},
  title        = {Diagnostic evaluation of low-rank matrix denoising algorithm-based magnetic resonance imaging on tibial plateau fracture complicated with meniscus injury},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified particle swarm optimization based magnetic
resonance imaging analysis for identification of risk factors tissue
damage incidence. <em>EXSY</em>, <em>40</em>(4), e12856. (<a
href="https://doi.org/10.1111/exsy.12856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study was to analyse the risk factors of unstable patella and the incidence of tissue damage (TD) with magnetic resonance imaging (MRI) image under the modified particle swarm optimization (MPSO). In this study, 45 patients with patella instability who visited hospital from 1 November 2018 to 28 September 2020 were selected as the experimental group (group E), and 45 volunteers with healthy knee joints were recruited as the control group (group C). MRI images under MPSO were to evaluate the four groups of related indicators (trochlear groove angle, trochlear groove depth, trochlear surface ratios, and Insall-Salvati index) of unstable patella in the two groups and count the incidence of TD caused by unstable patella. Comparison of four indicators revealed that there was an obvious difference in the data of the two groups. The abnormal rate of patients with unstable patella in the group E was much higher than that of the group C, and the difference was statistical ( p &lt;‚Äâ0.05). The incidences of trochlea dysplasia and high patella accounted for 88.9% and 57.8% in group E and accounted for 4.4% and 3.2% in the group C, respectively, showing statistically obvious differences ( p &lt;‚Äâ0.05). The incidence of patella cartilage TD, thigh cartilage TD, lateral meniscus TD, medial meniscus TD, patella medial retinaculum TD, patella lateral retinaculum TD, patellar ligament damage, and joint effusions were 14.5%, 11.45%, 50.38%, 49.62%, 10.69%, 16.03%, 9.16%, and 79.39%, respectively in the patients with unstable patella in the group E. In short, trochlear dysplasia and high patella were risk factors for unstable patella, and unstable patella was one of the risk factors for multiple TDs of the knee joint. This study provided a new research basis for the early treatment of unstable patella.},
  archive      = {J_EXSY},
  author       = {Ji Ke and Shufa Wang and Jiangchun Li and Quan Liu},
  doi          = {10.1111/exsy.12856},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12856},
  shortjournal = {Expert Syst.},
  title        = {Modified particle swarm optimization based magnetic resonance imaging analysis for identification of risk factors tissue damage incidence},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ant colony based fish crowding degree optimization algorithm
for magnetic resonance imaging segmentation in sports knee joint injury
assessment. <em>EXSY</em>, <em>40</em>(4), e12849. (<a
href="https://doi.org/10.1111/exsy.12849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim was to analyse the application value of magnetic resonance imaging (MRI) in the injury evaluation of posterolateral corner (PLC) caused by sports, and the ant colony-based crowding degree of fish swarm optimization algorithm for image segmentation optimization of MRI images. The crowding degree of fish swarm was introduced to construct the ant colony optimization algorithm (ACOA), and the maximum and minimum ant system (MMAS) and ant colony algorithm based on variation features (VF-based ACA) were introduced to compare with ACOA. Besides, ACOA was applied to the MRI diagnosis of 98 patients with PLC injuries. Then the number of Degree I, Degree II, and Degree III in PLC patients were compared. The results showed that the iteration times and running time of the optimal solution were obtained by ACOA when the number of cities was 10, which were compared with those obtained by MMAS and VF-based ACA with no great differences ( P &gt;‚Äâ0.05), and the iteration times and running time of ACOA were greater than those of MMAS and VF-based ACA when the number of cities was 50 ( P &lt;‚Äâ0.05). There were 73 patients with PLC injuries involved 2 or more structures, 11 patients with PLC injuries only involved the lateral collateral ligament (LCL), and 7 patients with PLC injuries only involved the popliteal tendon (PLT). The injury rates of LCL, medial collateral ligament, anterior cruciate ligament (ACL), posterior cruciate ligament (PCL), and PLT were 87.02%, 51.33%, 72.45%, 41.75%, and 84.06%, respectively. The number of patients with PLC injuries combined with Grade I of LCL, PCL, ACL, PCL, biceps femoris tendon, and PLT was higher than that of patients with Grade II and III ( P &lt;‚Äâ0.05). In conclusion, ACOA was much better than MMAS and VF-based ACA in dealing with complex work, which effectively improved the ergodicity of search and reduced the running time. There were more common in PLC injuries combined with LCL, PCL, ACL, PCL, and PLT, MRI images based on ACOA could clearly show the degree of ligament and tendon injuries.},
  archive      = {J_EXSY},
  author       = {Ziyu Song and Bowen Yang},
  doi          = {10.1111/exsy.12849},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12849},
  shortjournal = {Expert Syst.},
  title        = {Ant colony based fish crowding degree optimization algorithm for magnetic resonance imaging segmentation in sports knee joint injury assessment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computed tomography image and mechanism of spiral neuron
t-type calcification channel of elderly patient with senile sudden
deafness under embedded system combined with artificial intelligence
algorithm. <em>EXSY</em>, <em>40</em>(4), e12848. (<a
href="https://doi.org/10.1111/exsy.12848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To analyse the computed tomography (CT) images of spiral neuron T-type calcification channels and the mechanism of action in patients with sudden deafness in the elderly, the artificial intelligence (edge algorithm) algorithm was applied to the embedded system to analyse the CT images of the sudden deafness of the elderly, and then the mechanism of the spiral neuron T-calcification channel was studied. The results showed that among the 48 patients with labyrinthitis, 26 were acute labyrinthitis, 14 were chronic labyrinthitis, and 8 were sclerosing labyrinthitis. Œ±1G, Œ±1H, and Œ±1I were expressed on the cochlea and spiral neurons in the 66‚Äì68-year-old population, but the expression levels were significantly different. The expression of the three receptors on spiral neurons was slightly higher than that of the cochlea. Œ±1H was most expressed on spiral neurons compared with the other two calcium channel receptors ( p &lt;‚Äâ0.05). In addition, the minimum intensity projection method, multi-layer reconstruction method, and surface reconstruction method can fully display the bony labyrinth, cochlea, cochlear duct (2.5‚Äâweeks), and shape and edge damage. In summary, the adoption of artificial intelligence algorithms based on embedded systems to analyse CT images can comprehensively and accurately reflect the three-dimensional structure of the inner surface of the inner ear labyrinth. It can also fully display the scope, shape, density, and edge of the lesion. T-type calcium channel receptors were expressed on the cochlea and spiral neurons of the C57BL/6J elderly, and decreased with age.},
  archive      = {J_EXSY},
  author       = {Qiong Dai and Wei Ai and Kun Yuan and Min Jia and Jingwu Sun},
  doi          = {10.1111/exsy.12848},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12848},
  shortjournal = {Expert Syst.},
  title        = {Computed tomography image and mechanism of spiral neuron T-type calcification channel of elderly patient with senile sudden deafness under embedded system combined with artificial intelligence algorithm},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic segmentation algorithm for magnetic resonance
imaging in prediction of breast tumour histological grading.
<em>EXSY</em>, <em>40</em>(4), e12846. (<a
href="https://doi.org/10.1111/exsy.12846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To analyse the application value of dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) based on computer semi-automatic segmentation (CSS) algorithm in tumour histological grading of breast cancer patients, the CSS algorithm of DCE-MRI breast image was established based on Canny edge detection operator and dynamic binarization (DB) algorithm to compare with the fuzzy-c-means (FCM) algorithm based on FCM clustering and wavelet transform (WT) algorithm. Besides, CSS was applied to DCE-MRI image diagnosis in 121 breast cancer patients, who were then classified as grade I, II, and III according to the 2019 edition of the World Health Organization Histological Grading Criteria for Breast Tumors. The results showed that the false positive rate (FPR) of CSS was sharply lower than that of FCM and WT, while the true positive rate (TPR) of CSS increased greatly in contrast to FCM and WT, indicating that there were statistically significant ( p &lt;‚Äâ0.05). The area under the curve (AUC) predicted by FCM, WT, and CSS for histological classification were 0.818, 0.801, and 0.924, respectively. The fractional anisotropy (FA) value of patients with grade III was less than that of patients with grade I and II, while the apparent diffusion coefficient (ADC) value of patients with grade III was greater than that of patients with grade I and II ( p &lt;‚Äâ0.05). The tumour diameter, number of lesions, and number of lymph node metastasis of patients with grade III increased markedly compared to patients with grade I and II ( p &lt;‚Äâ0.05). To sum up, DCE-MRI image segmentation results based on CSS were superior to FCM and WT in the evaluation of breast tumour histological grading. What&#39;s more, there were substantial differences in the obtained tumour diameter, FA value, ADC value, and number of lymph node metastasis based on DCE-MRI of patients with grade I, II, and III, which could be applied to predict the histological grading of patients with breast cancer.},
  archive      = {J_EXSY},
  author       = {Qingyu He and Ruigen Pan},
  doi          = {10.1111/exsy.12846},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12846},
  shortjournal = {Expert Syst.},
  title        = {Automatic segmentation algorithm for magnetic resonance imaging in prediction of breast tumour histological grading},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BOSS: A new QoS aware blockchain assisted framework for
secure and smart healthcare as a service. <em>EXSY</em>, <em>40</em>(4),
e12838. (<a href="https://doi.org/10.1111/exsy.12838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest epidemic of COVID-19 has significantly impacted both human capital and the global economy, contributing to pandemics and severe global crises. Research into the creation and propagation of the disease is desperately needed. The Internet of Things, cloud computing, and artificial intelligence offer modern technology for real-time processing for multiple applications such as healthcare applications, transport, traffic control, and so on blockchain is an evolving technology that will dramatically boost transaction protection in finance, supply chain, and other transaction networks. A stable and latency-sensitive Quality of Service framework for COVID-19 is the need of an hour. The purpose of this paper is to combine Fog computing and Artificial Intelligence with smart health to establish a reliable platform for early-stage detection of COVID-19 infection. A new ensemble-based classifier is proposed to detect COVID-19 patients. This research offers a blockchain platform to analyse how the unrelated cases of the COVID-19 virus can be tracked and identified using peer-to-peer, time stamping, and the shared storage advantages of blockchain. In addition to growing patient loyalty, this would effectively enhance the consistency, flexibility, productivity, performance, and effectiveness of healthcare services. The idea of blockchain is used to establish security for the whole framework. Different implementations measure the efficiency of the suggested system. The performance of the proposed framework is evaluated in terms of delay, network usages, RAM usages, and energy consumption. On the other hand, the classifier is evaluated in terms of classifier accuracy, recall, precision, kappa static, and root mean square error. The result shows the performance of the proposed framework and classifier is always better than the traditional frameworks and classifiers.},
  archive      = {J_EXSY},
  author       = {Prabh Deep Singh and Rajbir Kaur and Gaurav Dhiman and Giridhar Reddy Bojja},
  doi          = {10.1111/exsy.12838},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12838},
  shortjournal = {Expert Syst.},
  title        = {BOSS: A new QoS aware blockchain assisted framework for secure and smart healthcare as a service},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-step finite-state automaton for arbitrarily
deterministic tsetlin machine learning. <em>EXSY</em>, <em>40</em>(4),
e12836. (<a href="https://doi.org/10.1111/exsy.12836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high arithmetic complexity and scalability challenges of deep learning, there is a critical need to shift research focus towards energy efficiency. Tsetlin Machines (TMs) are a recent approach to machine learning (ML) that has demonstrated significantly reduced energy compared to neural networks alike, while providing comparable accuracy on several benchmarks. However, TMs rely heavily on energy-costly random number generation to stochastically guide a team of Tsetlin Automata (TA) in TM learning. In this paper, we propose a novel finite-state learning automaton that can replace the TA in the TM, for increased determinism. The new automaton uses multi-step deterministic state jumps to reinforce sub-patterns, without resorting to randomization. A determinism parameter finely controls trading off the energy consumption of random number generation, against randomization for increased accuracy. Randomization is controlled by flipping a coin before every state jump, ignoring the state jump on tails. For example, makes every update random and makes the automaton completely deterministic. Both theoretically and empirically, we establish that the proposed automaton converges to the optimal action almost surely. Further, used together with the TM, only substantial degrees of determinism reduce accuracy. Energy-wise, random number generation constitutes switching energy consumption of the TM, saving up to 11‚ÄâmW power for larger datasets with high values. Our new learning automaton approach thus facilitates low-energy ML.},
  archive      = {J_EXSY},
  author       = {Kuruge Darshana Abeyrathna and Ole-Christoffer Granmo and Rishad Shafik and Lei Jiao and Adrian Wheeldon and Alex Yakovlev and Jie Lei and Morten Goodwin},
  doi          = {10.1111/exsy.12836},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12836},
  shortjournal = {Expert Syst.},
  title        = {A multi-step finite-state automaton for arbitrarily deterministic tsetlin machine learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedded intelligent fetal heart rate monitoring system
under empirical mode decomposition algorithm and its usage in pregnant
women with pregnancy bacterial infection complications and nursing.
<em>EXSY</em>, <em>40</em>(4), e12825. (<a
href="https://doi.org/10.1111/exsy.12825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {this study was to discuss the application value of the embedded intelligent fetal heart rate monitoring system (EI system) based on empirical mode decomposition (EMD) algorithm in the monitoring of pregnant women with pregnancy bacterial infection complications, aiming to provide a reference basis for timely nursing of pregnancy bacterial infection complications. In this study, the blind source separation method was introduced to denoise the screened intrinsic modal function (IMF) components based on the EI system and the EMD algorithm. The signal to noise ratio (SNR) based on the EMD optimization algorithm was compared with that of other algorithms. Two hundred and sixty-eight pregnant women in the obstetric clinic were selected as the research objects and divided into an observation group (abnormal fetal heart rate) and a control group (normal fetal heart rate). The pregnant women in both groups accepted the conventional nursing methods, and the EI system was adopted to monitor the fetal heart rate. The probability of abnormal fetal heart monitoring of pregnant women in observation group and control group was analysed. At the same time, the probability of bacterial infection complications was analysed. It was found that the EI system based on EMD algorithm showed obviously higher SNR in contrast to other algorithms. The waveforms of fetal heart monitoring in the observation group and the control group were significantly different ( œá 2 =¬†11.215, p &lt;‚Äâ0.05); microscopic examination of pregnant women secretion in the bacterial infection group revealed the presence of a large number of pathogenic bacteria; it was found that the infection rate of vaginosis was significantly different between the control group and the observation group ( p &lt;‚Äâ0.01); there was a significant difference between the observation group and the control group in the complication of premature rupture of fetal membrane caused by bacteria ( œá 2 =¬†11.2, p &lt;‚Äâ0.01). In conclusion, the EI system based on the EMD algorithm could be undertaken as a tool for the preliminary diagnosis of pregnancy bacterial infection complications in pregnant women, which provided relevant basis for later nursing.},
  archive      = {J_EXSY},
  author       = {Faying Lian and Hui Liu and Huan Shao and Yanyan Han},
  doi          = {10.1111/exsy.12825},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12825},
  shortjournal = {Expert Syst.},
  title        = {Embedded intelligent fetal heart rate monitoring system under empirical mode decomposition algorithm and its usage in pregnant women with pregnancy bacterial infection complications and nursing},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven decision-making model based on artificial
intelligence in higher education system of colleges and universities.
<em>EXSY</em>, <em>40</em>(4), e12820. (<a
href="https://doi.org/10.1111/exsy.12820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of management decisions is one of the main issues facing higher education institutions today. Strategic decisions taken by the higher education institutions affect policies, schemes, and actions that the institutions are considering. Machine learning is an emergent artificial intelligence field that utilizes different algorithms, analyses data, and delivers a better understanding of the data contained in a specific context. Hence, in this paper, data-driven decision-making model has been proposed based on artificial intelligence in colleges and universities. Student data, graduation rate and curriculum design have been analysed for administrative decision-making in college or university based on the machine learning method. With the availability of huge quantities and high-quality input training data, machine-learning progressions can attain precise outcomes and enable informed decision-making. The experimental findings show that the suggested model improves the outcome ratio of 90.72%, the performance ratio of 97.62%, prediction ratio of 96.35%, decision-making level of 95.51%, accuracy ratio of 95.61%, an efficiency ratio of 98.14%, graduation rate of 85.86%, data security rate (95.61%) and error rate 33.21% compared to other methods.},
  archive      = {J_EXSY},
  author       = {Yusi Teng and Jie Zhang and Ting Sun},
  doi          = {10.1111/exsy.12820},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12820},
  shortjournal = {Expert Syst.},
  title        = {Data-driven decision-making model based on artificial intelligence in higher education system of colleges and universities},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extracting low dimensional representations from large size
whole slide images using deep convolutional autoencoders. <em>EXSY</em>,
<em>40</em>(4), e12819. (<a
href="https://doi.org/10.1111/exsy.12819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Yusuf √áelik and Murat Karabatak},
  doi          = {10.1111/exsy.12819},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12819},
  shortjournal = {Expert Syst.},
  title        = {Extracting low dimensional representations from large size whole slide images using deep convolutional autoencoders},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of chemical components of urinary stones through
colour doppler ultrasound twinkling artefacts under optimized space
composite algorithm. <em>EXSY</em>, <em>40</em>(4), e12818. (<a
href="https://doi.org/10.1111/exsy.12818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study focused on the application value of the space compounding algorithm-based colour Doppler ultrasound twinkling artefacts in predicting chemical components of urinary tract stones. Specifically, the minimum variance method was used to optimize the space-compounding algorithm to improve the quality of colour Doppler ultrasound artefacts. Then, the urinary system stone model was built to quantitatively analyse the influence of stone components on colour Doppler twinkling artefacts. The results showed that no notable differences were noted in the twinkling artefact intensity (TAI), the width of twinkling artefact (TAW), and the length of twinkling artefact (TAL) between the oxalic acid monohydrate, hydroxyapatite, uric acid, and cystine stone models ( p‚Äâ&lt; 0.01). The area under the curve (AUC) of the TAI for the diagnosis of calcium-free calculus models was 0.978, that of TAW was 0.9462, and that of TAL was 0.9315. The diagnostic sensitivity and specificity of TAI were 89% and 96%, respectively, the diagnostic sensitivity and specificity of TAW were 86% and 80%, respectively, and the diagnostic sensitivity and specificity of TAL were 78% and 97%, respectively. The ability of TAL (AUC of 0.953) to distinguish between hydroxyapatite stone model and cystine stone was better than TAI (AUC of 0.702) and TAW (AUC of 0.657) ( p‚Äâ&lt; 0.01); and TAL and TAW were positively correlated with TAI (R2 was 0.79 and 0.68, respectively, p‚Äâ&lt; 0.01). In conclusion, the space compounding algorithm based on minimum variance method can improve the quality of colour Doppler ultrasound twinkling artefacts, and colour Doppler ultrasound twinkling artefacts can predict the chemical components of urinary stones, which can be used as a criterion for clinical diagnosis.},
  archive      = {J_EXSY},
  author       = {Aiwei Zhang and Junwei Wang and Cunming Zhang and Lingzhu Gu},
  doi          = {10.1111/exsy.12818},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12818},
  shortjournal = {Expert Syst.},
  title        = {Prediction of chemical components of urinary stones through colour doppler ultrasound twinkling artefacts under optimized space composite algorithm},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COVID-19 patient diagnosis and treatment data mining
algorithm based on association rules. <em>EXSY</em>, <em>40</em>(4),
e12814. (<a href="https://doi.org/10.1111/exsy.12814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Association rules are used in different data mining applications, including Web mining, intrusion detection, and bioinformatics. This study mainly discusses the COVID-19 patient diagnosis and treatment data mining algorithm based on association rules. General data The key time interval during the main diagnosis and treatment process (including onset to dyspnea, first diagnosis, admission, mechanical ventilation, death, and the time from first diagnosis to admission, etc.), the cause of death by laboratory examination, and so forth. The frequency of drug use was counted and association rule algorithm was used to analyse and study the effect of drug treatment. The results could provide reference for rational drug use in COVID-19 patients. In this study, in order to improve the efficiency of data mining in data processing, it is necessary to pre-process these data. Secondly, in the application of this data mining, the main objective is to extract association rules of COVID-19 complications. So its properties for mining should be various diseases. Therefore, it is necessary to classify individual disease types. During the construction of association rules database, the data in the data warehouse is analysed online and the association rules data mining is analysed. The results are stored in the knowledge base for decision support. For example, the prediction results of the decision tree can be displayed at this level. After the construction of the mining model, the display interface can be mined, and the decision-maker can input the corresponding attribute value and then predict it. 0.76% of people had both COVID-19, CHD and hypertension, while 46.5% of people with COVID-19 and CHD were likely to have hypertension. This study is helpful to analyse the imaging factors of COVID-19 disease.},
  archive      = {J_EXSY},
  author       = {Zicheng Shan and Wei Miao},
  doi          = {10.1111/exsy.12814},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12814},
  shortjournal = {Expert Syst.},
  title        = {COVID-19 patient diagnosis and treatment data mining algorithm based on association rules},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information collection system for fall detection of stroke
patients under cascade algorithm in the context of multi-modal
information fusion and e-health. <em>EXSY</em>, <em>40</em>(4), e12809.
(<a href="https://doi.org/10.1111/exsy.12809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively detect stroke patients after falling under e-health, the fall situation of patients is graded, and warnings are given to improve the survival probability of stroke patients after falling. First, the fall model of stroke patients is analysed. According to the model, multi-modal information fusion fall detection technology is proposed, including data fusion algorithm and feature recognition technology. Also, various sensor data are adjusted. The fall detection cascade algorithm is proposed to classify data with different features in order, thereby completing target detection. Finally, combined with heart rate sensor, height sensor and microcontroller unit software and patients&#39; e-health information, the research and development of an information collection system for fall detection are realized. Six young volunteers are selected to test the system performance. The results show that for the six testers, the heart rate detected by the ordinary device and the device of this investigation is the same when it is resting in different states of resting, walking, as well as walking and falling. The heart rate difference between walking and falling detection is not large (within the allowable error range of the device). But the best detection effect is to measure after the patient falls, which not only reduces power consumption but also keeps the detection error to a minimum. The height sensor is in the static state, increased by 75‚Äâm in the vertical direction and decreased by 75‚Äâcm from the static position in the vertical direction. The height difference of the data information exported from these three cases has some errors compared with the actual 75‚Äâcm. The tester&#39;s three situations of resting, walking and falling, standing up after sitting still and falling are observed. The waveform when resting is stable, and the acceleration information also fluctuates significantly when walking and after standing up. The accuracy of the developed system is above 80%, which achieves the expected effect and assisted medical treatment of fall detection based on patients&#39; e-health information. The results show that the proposed information collection system for fall detection of stroke patients under cascade algorithm in the context of multi-modal information fusion has good performance and high practical value.},
  archive      = {J_EXSY},
  author       = {Xiao Lv and Dingying Ma and Jinsheng Zhou and Lili Zhang and Xinke Li},
  doi          = {10.1111/exsy.12809},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12809},
  shortjournal = {Expert Syst.},
  title        = {Information collection system for fall detection of stroke patients under cascade algorithm in the context of multi-modal information fusion and e-health},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterization of heart rate variability and oxygen
saturation in sepsis patients. <em>EXSY</em>, <em>40</em>(4), e12806.
(<a href="https://doi.org/10.1111/exsy.12806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most major and common health crises which occur across all the hospitals, worldwide, is seen to be sepsis that occurs in patients. However, despite its wide prevalence no novel tool has been devised for predicting its occurrence. An accurate and early prediction of sepsis in the patients could significantly help the physicians administer proper treatment and decrease the uncertain diagnosis. Some machine-learning-based models or schemes can help in identifying the potential clinical variables and display a better performance compared to the prevailing conventional low-performance models. In this study, a machine learning-based scheme for fast and accurate sepsis identification was proposed. This scheme employed the power spectrum and mean estimation for data record intervals, which were then classified for reaching the final decision. For a 72-h interval, the obtained detection accuracy was 94.2% that shows very good sign to use it as a fast and robust sepsis identification.},
  archive      = {J_EXSY},
  author       = {Bilal Yaseen Al-Mualemi and Lu Lu},
  doi          = {10.1111/exsy.12806},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12806},
  shortjournal = {Expert Syst.},
  title        = {Characterization of heart rate variability and oxygen saturation in sepsis patients},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated detection and screening of depression using
continuous wavelet transform with electroencephalogram signals.
<em>EXSY</em>, <em>40</em>(4), e12803. (<a
href="https://doi.org/10.1111/exsy.12803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nearly 264 million people around the globe currently suffer from clinical depression, according to the World Health Organization. Although there are diagnostic techniques and treatments presently used by professionals, they are not always helpful. Herein, we suggest the use of advanced technological methods to diagnose depressed patients correctly. A machine learning approach is presented, which uses the electroencephalogram for diagnostics. The model extracts multiple features by applying a continuous wavelet transform (CWT) for each recording. These recordings are employed to train and test the model, with data gathered from 15 depressed and 15 normal patients. After the features are extracted from these recordings, it is organized into matrix form. The features are dimensionally reduced using kernel-principal component analysis and principal component analysis techniques, ranked using Student&#39;s t -test, and then labelled as normal or depressed with various classifiers. Accuracies of 99.33% and 99.13% were achieved for the right and left hemispheres of the brain, respectively, and 99.26% for the combined hemispheres of the brain. As compared to the discrete and empirical wavelet transform feature extraction methods, the CWT attained the best results. A depression severity index was also developed, using two features for discriminating the classes: normal versus depressed.},
  archive      = {J_EXSY},
  author       = {U. Raghavendra and Anjan Gudigar and Yashas Chakole and Praneet Kasula and D. P. Subha and Nahrizul Adib Kadri and Edward J. Ciaccio and U. Rajendra Acharya},
  doi          = {10.1111/exsy.12803},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12803},
  shortjournal = {Expert Syst.},
  title        = {Automated detection and screening of depression using continuous wavelet transform with electroencephalogram signals},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of big data information system in early
diagnosis, treatment, and nursing of cervical cancer infected by human
papillomavirus. <em>EXSY</em>, <em>40</em>(4), e12791. (<a
href="https://doi.org/10.1111/exsy.12791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to study the application value of big data information monitoring system in early diagnosis, treatment, and nursing of human papillomavirus (HPV) infected cervical cancer, firstly, the traditional ID3-based decision tree model was optimized through the minimum sample number of different leaf nodes. According to the optimization model, the diagnosis model of HPV-infected cervical cancer was constructed and applied to the monitoring system of big data of cervical cancer. Diagnosis model and information monitoring system (DMIMS) was compared with Decision tree based on decision support degree (DTBDS). Then, 876 HPV-infected cervical cancer patients diagnosed in our hospital from January 30, 2018 to January 30, 2019 were defined as the experimental group, and 670 HPV-infected cervical cancer patients diagnosed in our hospital from January 30, 2017 to January 30, 2018 were defined as the control group. Only the experimental group used cervical cancer data and information diagnosis system. Finally, the biopsy rate after examination, the coincidence rate with pathology diagnosis, the detection rate of precancerous lesion of HPV infection, and the detection rate of HPV infection of cervical cancer were compared between the two groups. The results showed that the classification accuracy of DMIMS (95%) was higher than that of DTBDS (67%) ( p‚Äâ&lt; 0.05). The biopsy rate of the experimental group was significantly lower than that of the control group ( p‚Äâ&lt; 0.05); the coincidence rate between biopsy and pathological diagnosis, detection rate of HPV precancerous lesion, and detection rate of HPV infected cervical cancer were significantly higher in the experimental group than in the control group ( p‚Äâ&lt; 0.05), which showed that the big data monitoring system of cervical cancer played an important role in the detection of HPV infection cervical cancer, which can make the patients with precancerous lesions and early cervical cancer get timely diagnosis, treatment, and nursing.},
  archive      = {J_EXSY},
  author       = {Guo Chen and Wenjian Zhang},
  doi          = {10.1111/exsy.12791},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12791},
  shortjournal = {Expert Syst.},
  title        = {Application of big data information system in early diagnosis, treatment, and nursing of cervical cancer infected by human papillomavirus},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent IoMT enabled feature extraction method for
early detection of knee arthritis. <em>EXSY</em>, <em>40</em>(4),
e12784. (<a href="https://doi.org/10.1111/exsy.12784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoarthritis and rheumatoid are most common form of arthritis disorder, affecting millions of people worldwide. This article presents a computer aided detection system (CAD) for early knee osteoarthritis and rheumatoid detection using X-ray images and machine learning classifiers. This work also proposed a novel feature extractor from X-ray images of knee to assist in detection and classification, called explainable Renyi entropic segmentation with Internet of Things (IoT) framework. The proposed method later utilizes model agnostic algorithm using post hoc explainability for extracting relevant information from prediction of knee joint segmentation. CAD system is integrated with an IoT framework and can be used remotely to assist medical practitioners in treatments of knee arthritis. The presented results show commendable improvement over different existing feature extractors in combination with different classifiers. The best result of proposed extractor method was obtained when combined with random forest classifier having Euclidean hyperparameter that gave an accuracy of 95.23%, among all the evaluators. The obtained results show the effectiveness of proposed feature extractor model to determine relevant features from knee and describe the suitable knee disorders.},
  archive      = {J_EXSY},
  author       = {Aditya Khamparia and Babita Pandey and Fadi Al-Turjman and Prajoy Podder},
  doi          = {10.1111/exsy.12784},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12784},
  shortjournal = {Expert Syst.},
  title        = {An intelligent IoMT enabled feature extraction method for early detection of knee arthritis},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid intelligent technology for plant health using the
fusion of evolutionary optimization and deep neural networks.
<em>EXSY</em>, <em>40</em>(4), e12756. (<a
href="https://doi.org/10.1111/exsy.12756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, plant leaf disease identification has been an efficient research subject. In connection with this interest, deep learning architectures show a remarkable era in various fields of image processing and computer vision, including image classification, function detection, and image pattern recognition. In this study, we examine many aspects of convolutional neural networks for image pattern recognition. We examine the early and late fusion of multiple pattern recognition classifiers using various plant leaves. Commonly, it considers disease discovery with the diagnostic technologies available. In standard cases, planters usually do not discover the disease. Therefore, plant leaf disease detection is a significant research problem, and one of their goals is to uncover an effective way to identify leaf image disease. The article has made a potential effort to find a process that should be able to expose plant leaf disease using early and late fusion of two classifiers: modified Optimized Deep Neural Network (ODNN) with different parameters of evolutionary optimization of Grasshopper algorithm (GOA), Speeded Up Robust Features (SURF) and Convolutional Neural Network (CNN) that could support the system to achieve excellent performance. Classification quality parameters are determined, and research to explain the validation of the model has been carried out.},
  archive      = {J_EXSY},
  author       = {Jalal Sadoon Hameed Al-bayati and Burak Berk √úst√ºndaƒü},
  doi          = {10.1111/exsy.12756},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12756},
  shortjournal = {Expert Syst.},
  title        = {Hybrid intelligent technology for plant health using the fusion of evolutionary optimization and deep neural networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A chaotic grey wolf optimizer for constrained optimization
problems. <em>EXSY</em>, <em>40</em>(4), e12719. (<a
href="https://doi.org/10.1111/exsy.12719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bio-inspired algorithms have become popular due to their capability of finding good solutions for complex optimization problems in an acceptable computational time. The Grey Wolf Optimizer is a nature-inspired, population-based metaheuristic that simulates the social hierarchy and the hunting strategy observed in a grey wolf pack. Although the Grey Wolf Optimizer has been successfully applied to solve different optimization problems, it may suffer from premature convergence and get stuck in local optima. In order to overcome these drawbacks, this paper proposes a chaotic version of the Grey Wolf Optimizer that differs from the original algorithm and previously published modified versions because it does not add a chaotic variable in the parameters that control the execution of the algorithm. Instead, the proposed model uses a chaotic variable to define the wolves in the pack that will be used to guide the hunting process in each iteration of the algorithm. Numerical experiments using 20 benchmark functions are carried out. The performance of the proposed model is compared with the performance of the original Grey Wolf Optimizer and other well-known algorithms, namely the Particle Swarm Optimization, the Genetic Algorithm, the Symbiotic Organisms Search, and the Teaching-Learning Based Optimization. Nine chaotic maps reported in the literature are tested. The results show that the proposed algorithm has a very competitive performance, and the Chebyshev map presented the best performance among the chaotic maps simulated. The proposed algorithm can be integrated into other modified versions of the Grey Wolf Optimizer in a straightforward way.},
  archive      = {J_EXSY},
  author       = {Leonardo Ramos Rodrigues},
  doi          = {10.1111/exsy.12719},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12719},
  shortjournal = {Expert Syst.},
  title        = {A chaotic grey wolf optimizer for constrained optimization problems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid evolutionary network architecture search (HyENAS) for
convolution class of deep neural networks with applications.
<em>EXSY</em>, <em>40</em>(4), e12690. (<a
href="https://doi.org/10.1111/exsy.12690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) and its variants are increasingly used across wide domain of applications achieving high performance measures. For high performance, application specific CNN architecture is required, hence the need for network architecture search (NAS) becomes essential. This paper proposes a hybrid evolutionary approach for network architecture search (HyENAS), and targets convolution class of neural networks. One of the significant contribution of this technique is to completely evolve the high performance network by simultaneously finding network structures and their corresponding parameters. An elegant string representation has been proposed which efficiently represents the network. The concept of sparse block evolving requisite layer wise features for dense network is deployed. This permits the network to dynamically evolve for a specific application. In comparison to the other state-of-art methods, the high performance of the proposed HyENAS approach is demonstrated across various benchmark data sets belonging to the domain of malariology, oncology, neurology, ophthalmology, and genomics. Further, to deploy the proposed model on lower hardware specification devices, another salient feature of the HyENAS technique is to seamlessly sift out the simpler network architecture with comparable accuracy.},
  archive      = {J_EXSY},
  author       = {Soniya and Lotika Singh and Sandeep Paul},
  doi          = {10.1111/exsy.12690},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12690},
  shortjournal = {Expert Syst.},
  title        = {Hybrid evolutionary network architecture search (HyENAS) for convolution class of deep neural networks with applications},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel meta-heuristic approach for influence maximization
in social networks. <em>EXSY</em>, <em>40</em>(4), e12676. (<a
href="https://doi.org/10.1111/exsy.12676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization in a social network focuses on the task of extracting a small set of nodes from a network which can maximize the propagation in a cascade model. Though greedy methods produce good solutions to the aforementioned problem, their high computational complexity is a major drawback. Centrality-based heuristic methods often fail to overcome local optima, thereby producing sub-optimal results. To this end, in this article, a framework has been presented which involves community detection in a social network and the utilization of the Shuffled Frog Leaping algorithm, in maximizing the two-hop spread of influence under the independent cascade model. Local search strategies like the Late acceptance based hill climbing have been employed to improve the solution further. Experiments performed on three real-world datasets have shown that our method performs markedly well with respect to the comparing algorithms.},
  archive      = {J_EXSY},
  author       = {Bitanu Chatterjee and Trinav Bhattacharyya and Kushal Kanti Ghosh and Agneet Chatterjee and Ram Sarkar},
  doi          = {10.1111/exsy.12676},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12676},
  shortjournal = {Expert Syst.},
  title        = {A novel meta-heuristic approach for influence maximization in social networks},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An experimental comparison of metaheuristic frameworks for
multi-objective optimization. <em>EXSY</em>, <em>40</em>(4), e12672. (<a
href="https://doi.org/10.1111/exsy.12672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective optimization problems frequently appear in many diverse research areas and application domains. Metaheuristics, as efficient techniques to solve them, need to be easily accessible to users with different expertise and programming skills. In this context, metaheuristic optimization frameworks are helpful, as they provide popular algorithms, customizable components and additional facilities to conduct experiments. Due to the broad range of available tools, this paper presents a systematic evaluation and experimental comparison of 10 frameworks, covering from multi-purpose, consolidated tools to recent libraries specifically designed for multi-objective optimization. The evaluation is organized around seven characteristics: search components and techniques, configuration, execution, utilities, external support and community, software implementation and performance. An analysis of code metrics and a series of experiments serves to assess the last two features. Lesson learned and open issues are also discussed as part of the comparative study. The outcomes of the evaluation process reveal a contrasted support to recent advances in multi-objective optimization, with a lack of novel algorithms and variety of metaheuristics other than evolutionary algorithms. The experimental comparison also reports significant differences in terms of both execution time and memory usage under demanding configurations.},
  archive      = {J_EXSY},
  author       = {Aurora Ram√≠rez and Rafael Barbudo and Jos√© Ra√∫l Romero},
  doi          = {10.1111/exsy.12672},
  journal      = {Expert Systems},
  month        = {5},
  number       = {4},
  pages        = {e12672},
  shortjournal = {Expert Syst.},
  title        = {An experimental comparison of metaheuristic frameworks for multi-objective optimization},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Some novel operational laws and spherical fuzzy
choquet-frank operators and their application to MCDM. <em>EXSY</em>,
<em>40</em>(3), e13210. (<a
href="https://doi.org/10.1111/exsy.13210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some decision-making problems are required to be analysed using insufficient and vague information under an extended space of fuzziness, considering flexibility in the decision-making process and the interdependency and interaction between the criteria. To tackle these challenges, a spherical fuzzy set (SFS), which is an extension of the ordinary picture fuzzy set (PFS), can be used to fulfil the requirement of an extended space of fuzziness by quantifying uncertainty. Similarly, the Frank operations can provide flexibility to the decision-making process due to the involvement of a parameter, whereas the Choquet integral considers all possible interactive characteristics between different criteria. Therefore, this paper integrates these three important concepts to make more sound and realistic decisions by extending the existing arithmetic operation for spherical fuzzy numbers (SFNs) using Frank t -norm and t -conorm. In addition, the paper introduces some novel Choquet-Frank aggregation operators, such as spherical fuzzy Choquet-Frank averaging (SFCFA) and spherical fuzzy Choquet-Frank geometric (SFCFG). To demonstrate the effectiveness of the proposed operators, a multi-criteria decision making (MCDM) problem related to the selection of an enterprise resource planning (ERP) system is analysed for the available five options , and . Sensitivity analysis is performed to investigate the impact of taking variation in parameter values ( ) on aggregated values. Results confirmed that the best alternatives for SFCFA and SFCFG operators are and , respectively, while is the worst alternative for both these operators. To show the effectiveness of the proposed operators, a comparative analysis has been performed to compare the results obtained from other existing aggregation operators. Results concluded that the findings resemble most of the existing operators which confirms the feasibility of the proposed operators.},
  archive      = {J_EXSY},
  author       = {Tanuja Punetha and Komal},
  doi          = {10.1111/exsy.13210},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13210},
  shortjournal = {Expert Syst.},
  title        = {Some novel operational laws and spherical fuzzy choquet-frank operators and their application to MCDM},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Photovoltaic power combination prediction system based on
improved multi-objective optimization algorithm and nonlinear weighting
strategy. <em>EXSY</em>, <em>40</em>(3), e13209. (<a
href="https://doi.org/10.1111/exsy.13209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large-scale connection of photovoltaic (PV) power generation to the power grid introduces considerable challenges to the grid&#39;s stable operation. To ensure grid stability, the key is to improve the accuracy of PV power prediction. Currently, PV power prediction is primarily dependent on single-model construction, which ignores the role of data processing technologies, thus resulting in inadequate prediction performance. Accordingly, in this study, a combined prediction system is developed by integrating advanced deep learning algorithms and data processing techniques to improve the accuracy of PV power prediction. In addition, a nonlinear weighting strategy based on an improved multi-objective dragonfly optimization algorithm (IMODA) is proposed to determine the final prediction result. In the IMODA, cloud model generator, tent mapping, and Pareto solution selection strategy based on knee points are introduced to resolve the defects of the original algorithm. The performance of the proposed combined system was scientifically evaluated and analysed by considering the PV power datasets of four seasons in Belgium as an example. The mean absolute percentage errors of the proposed system on the four datasets are 4.0198, 4.7943, 4.3587, and 5.9286. These values indicate an improvement range of 5%‚Äì30% compared with the errors of other benchmark models.},
  archive      = {J_EXSY},
  author       = {Jianzhou Wang and Honggang Guo and Aiyi Song},
  doi          = {10.1111/exsy.13209},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13209},
  shortjournal = {Expert Syst.},
  title        = {Photovoltaic power combination prediction system based on improved multi-objective optimization algorithm and nonlinear weighting strategy},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibration and frequency estimation in sensors for
electrical parameter measurement using regression and metaheuristic
based models. <em>EXSY</em>, <em>40</em>(3), e13208. (<a
href="https://doi.org/10.1111/exsy.13208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration is the backbone of any sensor and measurement philosophy. The conventional calibration techniques for electrical parameter measurement using nonlinear sensors are affected by the repeating analogue signals and lead to errors in measurement. This research paper investigates different regression-based mathematical models to calibrate the Hall sensor for measuring RMS and the fundamental frequency. The novelty of this research work lies in the feature-based input modelling to measure RMS current and frequency with an error of 3.37e-12% and 7.61e-9%, respectively. The conventional Fourier transform method is compared with six different bio-inspired metaheuristic algorithms to estimate the frequency components of the analogue signal received from the measurement setup. Apart from the conventional sine waveforms, this paper investigates sawtooth and square waveforms as the periodic signals for determining the frequency components. The results from the comparison study show that the Whale Optimization Algorithm exhibits 1.25% lesser error whilst predicting the features in measured frequency components. Apart from this, the paper identifies a unique combination of features that effectively measures the instantaneous electrical parameter with RMSE, NRMSE, and value of 158.8, 0.51, and 0.44, respectively. Experimentally it is found that the decision tree and random forest regression models calibrate the Hall sensor with 93% less error than their linear and polynomial counterparts.},
  archive      = {J_EXSY},
  author       = {Soumyaranjan Ranasingh and Tapan Pradhan and Koteswara Raju Dhenuvakonda},
  doi          = {10.1111/exsy.13208},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13208},
  shortjournal = {Expert Syst.},
  title        = {Calibration and frequency estimation in sensors for electrical parameter measurement using regression and metaheuristic based models},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of optimum thresholds for binary classification
using genetic algorithm: An application to solve a credit scoring
problem. <em>EXSY</em>, <em>40</em>(3), e13203. (<a
href="https://doi.org/10.1111/exsy.13203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main issue in a classification problem is classifying observations into various disjoint classes. Different classification techniques generate a continuous number between a and b, usually between 0 and 1; thus, the optimal cut-off value(s) must be carefully selected to discriminate classes precisely. The decision is about setting a threshold value and transforming the continuous score into a binary output. Therefore, in addition to using the so-called sophisticated classification methods to have a more accurate classification, there is a need to identify and choose the optimal threshold value(s). However, the latter has not been thoroughly investigated. Hence, this study proposes an approach based on a Genetic Algorithm (GA) and Neural Networks (NNs) to automatically find customized cut-off values, considering different performance criteria and given datasets. Since credit scoring is a binary classification problem, two popular credit scoring datasets, namely ‚ÄúAustralian‚Äù and ‚ÄúGerman‚Äù credit datasets, are used to test the proposed approach. Our numerical results revealed that the proposed GA-NN model could successfully find customized acceptance thresholds, considering predetermined performance criteria, including Accuracy, Estimated Misclassification Cost (EMC), and Area under ROC Curve (AUC) for the tested datasets. Furthermore, the best-obtained results and the paired-samples t -test results show that utilizing the customized cut-off points leads to a more accurate classification than the commonly-used threshold value of 0.5.},
  archive      = {J_EXSY},
  author       = {Hamid Reza Kazemi and Kaveh Khalili-Damghani and Soheil Sadi-Nezhad},
  doi          = {10.1111/exsy.13203},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13203},
  shortjournal = {Expert Syst.},
  title        = {Estimation of optimum thresholds for binary classification using genetic algorithm: An application to solve a credit scoring problem},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fault diagnosis for high-speed train braking system based on
disentangled causal representation learning. <em>EXSY</em>,
<em>40</em>(3), e13197. (<a
href="https://doi.org/10.1111/exsy.13197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven methods have shown a great potential in diagnosing ongoing faults in high-speed trains (HSTs). However, lacking enough interpretability, data-driven methods have not been widely considered in practical operation of HST. In recent years, the rapid development of the causal discovery technology provides an effective way to improve the model interpretability. In this work, based on disentangled causal representation learning (DCRL), an effective and interpretable fault diagnosis framework is proposed for HST braking system. Independent potential factors of the high-dimensional monitoring data are extracted by the DCRL based on factor analysis. A stable and clear causal network in the factor space is obtained based on causal discovery, and the information irrelevant to fault diagnosis can be eliminated by feature selection. With logistic regression as the fault diagnosis model, the risk importance ranking of the monitoring features can be obtained. Compared with most commonly used methods, the method proposed in this paper has high interpretability and application value, which is more conducive to the subsequent fault location and troubleshooting. Based on real monitoring data of a HST braking system, it is justified that the effectiveness of the fault diagnosis model can be significantly improved by DCRL. Moreover, the applicability of the proposed method is also discussed.},
  archive      = {J_EXSY},
  author       = {Chong Wang and Jie Liu},
  doi          = {10.1111/exsy.13197},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13197},
  shortjournal = {Expert Syst.},
  title        = {Fault diagnosis for high-speed train braking system based on disentangled causal representation learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group decision-making analysis based on distance measures
under rough environment. <em>EXSY</em>, <em>40</em>(3), e13196. (<a
href="https://doi.org/10.1111/exsy.13196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rough set as a superset of a crisp set is a mathematical tool to cope with uncertainty using initial data without additional assumptions and pre-defined parameters. Using the technique of upper and lower approximations, rough models provide a complete description of the problem. This paper aims to introduce the notion of distance function, which is a metric, in rough graphs. We establish formulae of distance function, degree and radius of certain products of rough graphs in terms of initial given rough graphs. We discuss the concepts of weak isomorphism and isomorphism in rough graphs, and describe the properties of isomorphic rough graphs. We demonstrate the significance and importance of the distance function with an application to a decision making problem concerning organ trafficking networks. An application of rough matrices in steam valve systems is discussed, and we show how they can help to identify the potential failures and risk components during the power generation process. The results obtained under the rough model are compared with existing extensions of graphs and fuzzy based approaches.},
  archive      = {J_EXSY},
  author       = {Saba Fatima and Musavarah Sarwar and Fariha Zafar and Muhammad Akram},
  doi          = {10.1111/exsy.13196},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13196},
  shortjournal = {Expert Syst.},
  title        = {Group decision-making analysis based on distance measures under rough environment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing new digital tools to augment human creative
thinking at work: An application in elite sports coaching.
<em>EXSY</em>, <em>40</em>(3), e13194. (<a
href="https://doi.org/10.1111/exsy.13194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creative thinking is desirable in many professions. This article reports new research that followed a design science approach to develop and investigate a co-creative tool called Sport Sparks in one profession ‚Äì the coaching of professional football players. In response to a coach entering a text description of a coaching challenge (e.g., struggling to maintain the fitness of an athlete ) into the tool, the tool automatically generated potentially novel ideas (e.g., reducing game time and changing their nutrition ) that the coach could select and/or adapt and evolve into a simple action plan (e.g., which links nutrition to increased game time ). This Sport Sparks tool was designed to be an example of human-centred artificial intelligence that aspires to empower humans, to deliver high levels of human control as well as automation, and empower people rather than emulate their expertise. It was engineered with rule-based reasoning to automate the generation of potential new ideas that coaches could select and refine during interactions which provide high user control over this automation. The potential of such a co-creative tool, and value of the guidelines, were demonstrated during the tool&#39;s evaluation by coaching practitioners at a Premier League football club. The practitioners used the tool to generate new ideas to coaching challenges, and reported evidence of different forms of creative thinking, although some also reported the need for more support for creative collaborations and solution planning. The paper ends by discussing future directions for both the Sport Sparks tool and other co-creative AI tools.},
  archive      = {J_EXSY},
  author       = {Neil Maiden and James Lockerbie and Konstantinos Zachos and Alex Wolf and Amanda Brown},
  doi          = {10.1111/exsy.13194},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13194},
  shortjournal = {Expert Syst.},
  title        = {Designing new digital tools to augment human creative thinking at work: An application in elite sports coaching},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple-attribute decision-making spherical vague normal
operators and their applications for the selection of farmers.
<em>EXSY</em>, <em>40</em>(3), e13188. (<a
href="https://doi.org/10.1111/exsy.13188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we discuss a few fresh approaches to the spherical vague normal set (SVNS) approach to multiple attribute decision-making (MADM) problems. A new generalization of the vague set (VS) and the spherical interval valued fuzzy set (SIVFS) is the spherical vague set (SVS). The spherical vague number (SVN) concepts consolidate normal fuzzy number (NFN) and we defined the spherical vague normal number (SVNN) and some of its intriguing fundamental operations. The purpose of this article is to discuss a novel idea of spherical vague normal weighted averaging (SVNWA), spherical vague normal weighted geometric (SVNWG), generalized spherical vague normal weighted averaging (GSVNWA) and generalized spherical vague normal weighted geometric (GSVNWG) operators. We talked about a flowchart with an algorithm that uses this operators and the MADM approach. With the help of a numerical example, we interact the extended euclidean and hamming distance measures. In this communication, it is also important to elaborate on some key SVN approach characteristics based on various algebraic operations, such as idempotency, boundedness, commutativity, and monotonicity. They are quicker to find the best option, more straightforward and practical. Think about five farmers. The four factors that are taken into account for each of the five farmers are climate, water, soil, disease, and flood, and their corresponding weights are displayed. We want to select the best option from a large number of choices by comparing expert assessments with the criteria. As a result, the conclusions of the defined models are more precise and closely related to . We contrast some of the current models with the ones that have been proposed in order to demonstrate the dependability and utility of the models under investigation. The study&#39;s findings are also intriguing and fascinating.},
  archive      = {J_EXSY},
  author       = {Murugan Palanikumar and Krishnan Arulmozhi and Chiranjibe Jana and Madhumangal Pal},
  doi          = {10.1111/exsy.13188},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13188},
  shortjournal = {Expert Syst.},
  title        = {Multiple-attribute decision-making spherical vague normal operators and their applications for the selection of farmers},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid convolutional neural network-support vector machine
architecture for classification of super-resolution enhanced chromosome
images. <em>EXSY</em>, <em>40</em>(3), e13186. (<a
href="https://doi.org/10.1111/exsy.13186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article offers a framework of a hybrid convolution neural network (CNN) ‚Äì support vector machine (SVM) model for the classification of human metaphase chromosome images. In this model, the features extracted by CNN were fed to the SVM classifier to label the chromosomes into 24 classes. Classification accuracy is generally affected by low-resolution chromosome images. Hence, in the proposed work, a Laplacian pyramidal super-resolution network (LaPSRN) was deployed to improve input chromosome images resolution before feeding them to the classifier network. The experimentation showed that due to LaPSRN, resolution-enhanced images are obtained, which has improved the classification accuracy by 3% compared to the architecture without a super-resolution block. In addition, instead of using the usual ReLU function in CNN architecture, a Swish activation function was utilized, which enhanced the validation accuracy by 0.8%. After experimenting with various hyper-parameters for the CNN SVM architecture, the ADAM optimizer with step decay outperformed the other standard optimizers. The novel part of this work is the combination of Laplacian pyramidal super-resolution with the hybrid CNN-SVM model, which has not been employed yet for chromosome classification tasks as per our knowledge. The proposed hybrid CNN SVM architecture with the Swish activation function and super-resolution techniques was tested on the metaphase chromosome images from the BioImLab dataset, and an improved classification accuracy of 94.6% was obtained. Moreover, the standard metrics for classification like F1 score, precision, support, and the confusion matrix values of the proposed CNN model with SVM classifier are superior to the prevailing CNN architectures for chromosome images.},
  archive      = {J_EXSY},
  author       = {Dinesh Menaka and Subramanian Ganesh Vaidyanathan},
  doi          = {10.1111/exsy.13186},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13186},
  shortjournal = {Expert Syst.},
  title        = {A hybrid convolutional neural network-support vector machine architecture for classification of super-resolution enhanced chromosome images},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COVID-19 detection based on pre-trained deep networks and
LSTM model using x-ray images enhanced contrast with artificial bee
colony algorithm. <em>EXSY</em>, <em>40</em>(3), e13185. (<a
href="https://doi.org/10.1111/exsy.13185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus (COVID-19) is an infectious disease that has spread across the world within a short period of time and is causing rapid casualties. The main symptoms of this virus are shortness of breath, fever, cough, and a sore throat. The virus is detected through samples, such as throat swabs and sputum, taken from people who meet the possible case definition and the results are usually obtained within a few hours or a day. The development of test kits to detect the COVID-19 virus is still an open research topic, and automated and faster diagnostic tools are needed. Recent studies have shown that biomedical images can be used for COVID-19 testing. This study proposes the hybrid use of pre-trained deep networks and the long short-term memory (LSTM) for the classification of COVID-19 from contrast-enhanced chest X-rays. In the proposed system, a transformation function is applied to X-ray images first. Then, the artificial bee colony (ABC) algorithm is used to optimize the parameters obtained from the transformation function. The pre-trained deep network models and LSTM are preferred to extract features from the contrast-enhanced chest X-rays. At the final stage, COVID-19, normal (healthy), and pneumonia chest X-ray are classified using softmax. To evaluate the performance of the proposed method, the ‚ÄúCOVID-19 radiography‚Äù dataset, which is widely used in the literature, is preferred. From the proposed model, 98.97% accuracy, 98.80% precision, and 98.70% sensitivity rates are obtained. Experimental results reveal that the proposed model provides efficient results compared to other methods. Thanks to the application of ABC-based image enhancement, increased classification of 2.5% has been achieved against other state-of-the-art models.},
  archive      = {J_EXSY},
  author       = {Mehmet Bilal Er},
  doi          = {10.1111/exsy.13185},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13185},
  shortjournal = {Expert Syst.},
  title        = {COVID-19 detection based on pre-trained deep networks and LSTM model using X-ray images enhanced contrast with artificial bee colony algorithm},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extended PROMETHEE approach with 2-tuple linguistic m-polar
fuzzy sets for selection of elliptical cardio machine. <em>EXSY</em>,
<em>40</em>(3), e13178. (<a
href="https://doi.org/10.1111/exsy.13178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Preference Ranking Organization Method for Enrichment of Evaluations (PROMETHEE) is an outranking series of multi-attribute decision making used to assess a finite collection of alternatives based on conflicting criteria. One of its significant benefits is its adjustability in response to a set of acceptable preference functions that may quantify the differences between alternatives conditional on the kind and structure of the criteria. This article offers the novel approach of the PROMETHEE method, named the 2TL m F PROMETHEE method, which combines the multi-polarity with linguistic information of decision problems. The customary procedure of the offered technique begins with the normalized weights of the attributes assigned by decision-makers. After that, Gaussian and the usual preference functions are utilized to determine the preferences of the alternatives. The ultimate choice is based on the alternative&#39;s positive and negative outranking flow. The positive outranking demonstrates how one alternative outranks all other options, while the negative preference flow implies an alternative is superior to all other options. There are several modifications to the PROMETHEE approach, two of which are discussed in this article. The first is PROMETHEE I, which uses positive and negative preference flows to generate a partial ranking. Secondly, PROMETHEE II gives a comprehensive rating list based on the net flow, representing the balance of positive and negative preference flows. Further, a comprehensive flowchart is used to show the entire picture of the proposed methodology. As an application, a real-life situation related to selecting an elliptical machine for a home gym is chosen to illustrate the transparency and reliability of the presented work. Finally, a comparative study is carried out to illustrate the strength and applicability of the proposed method.},
  archive      = {J_EXSY},
  author       = {Muhammad Akram and Uzma Noreen and Dragan Pamucar},
  doi          = {10.1111/exsy.13178},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13178},
  shortjournal = {Expert Syst.},
  title        = {Extended PROMETHEE approach with 2-tuple linguistic m-polar fuzzy sets for selection of elliptical cardio machine},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of choice models in tourism recommender systems.
<em>EXSY</em>, <em>40</em>(3), e13177. (<a
href="https://doi.org/10.1111/exsy.13177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choice models (CM) are proposed in the field of tourism recommender systems (TRS) with the aim of providing algorithms with both a theoretical understanding of tourist&#39;s motivations and a certain degree of transparency. The goal of this work is to overcome some of the limitations of current state-of-art algorithms used in TRSs by providing: (1) accurate preferences, which are learnt from user choices rather than from ratings, and (2) interpretable coefficients, which are achieved by means of the set of estimated parameters of CM. The study was carried out with a gastronomic data set generated in an ecological experiment in the tourism domain. The performance of CM has been compared with a set of baseline algorithms (rating-based and ensembles) by using two evaluation metrics: precision and DCG. The CM outperformed the baseline algorithms when the size of the choice set was limited. The findings suggest that CM may provide an optimal trade-off between theoretical soundness, interpretability and performance in the field of TRS.},
  archive      = {J_EXSY},
  author       = {Ameed Almomani and Paula Saavedra and Pablo Barreiro and Roi Dur√°n and Rosa Crujeiras and Mar√≠a Loureiro and Eduardo S√°nchez},
  doi          = {10.1111/exsy.13177},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13177},
  shortjournal = {Expert Syst.},
  title        = {Application of choice models in tourism recommender systems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancement of clustering techniques by coupling clustering
tree and neural network: Application to brain tumour segmentation.
<em>EXSY</em>, <em>40</em>(3), e13176. (<a
href="https://doi.org/10.1111/exsy.13176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, no classical clustering algorithm is efficient on its own. The predefined number of clusters required for their operation does not consistently produce satisfactory segmentation results. They exhibit cluster instability, are vulnerable to the local optimum trap, and are sensitive to noise and imaging artefacts. Most contributions designed to overcome these drawbacks incorporate prior knowledge such as cluster label information and statistic measures that demand minimal labelled training data. Although these approaches improve the segmentation accuracy, they tend to diminish the advantages of clustering algorithms over the supervised learning methods. This study proposes a shift from the use of a predefined number of clusters to a clustering tree-based method for performance enhancement of classical clustering algorithms. The proposed method is a three-stage algorithm. It begins with the extraction of low-level features from a clustering tree. Clustering trees are sets of labelled clusters of an image at multiple clustering resolutions. The second stage extracts high-level features by coupling the clustering tree to a single-layer feedforward neural network. The third stage is the classification stage, where the basic model of a neural network extracts the tumour from a high-level feature map. Because neither of the neural networks requires training, the proposed method is both fully unsupervised and fully automated and retains all its advantages over supervised methods. A performance evaluation using FLAIR MRI images of brain tumour patients from the BRATS2015 and BRATS2020 databases demonstrates significant performance enhancement over four classical clustering algorithms and two of the four proposed techniques were comparable to deep learning methods.},
  archive      = {J_EXSY},
  author       = {Michael Osadebey and Marius Pedersen and Meeta Kalra and Dag Waaler and Nizar Bouguila},
  doi          = {10.1111/exsy.13176},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13176},
  shortjournal = {Expert Syst.},
  title        = {Enhancement of clustering techniques by coupling clustering tree and neural network: Application to brain tumour segmentation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge-aware representation learning for diagnosis
prediction. <em>EXSY</em>, <em>40</em>(3), e13175. (<a
href="https://doi.org/10.1111/exsy.13175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosis prediction exploits electronic health records (EHRs) to predict the future diagnoses of patients, further supporting clinical decision making and personalized treatments. However, a patient&#39;s EHR is an irregular sequence of visits that contains a large number of medical concepts. The disease progression patterns are closely related to the visits, as well as the contextual knowledge of each visit. The existing diagnosis prediction methods ignore the complex relationships between the visits and the contextual knowledge, and thus cannot achieve satisfactory performance. Therefore, we develop a knowledge-aware representation learning method to comprehensively model these complex relationships. Specifically, we first construct a medical knowledge graph to model the correlations between medical concepts in EHRs, and project the contextual knowledge into the pre-learned vectors. We then devise an enhanced gated recurrent unit (GRU) neural network to extract the long-term intra-relationships between visits, and design a novel knowledge attention module to capture the complex inter-relationships between the visits and the contextual knowledge. Armed with these, we provide a powerful and flexible framework to capture the long-term discriminative disease progression patterns for diagnosis prediction. Intensive experiments are conducted on two real-world EHR datasets. And the model achieves a competitive performance with Code-level Accuracy@20 of 0.7465 and Visit-level Precision@20 of 0.7547 on MIMIC-III, and Code-level Accuracy@20 of 0.9337 and Visit-level Precision@20 of 0.9427 on MIMIC-IV.},
  archive      = {J_EXSY},
  author       = {Weihua Li and Hang Li and Bei Yang and Lihua Zhou and Xianming Yang and Miao Zhang and Bingyi Wang},
  doi          = {10.1111/exsy.13175},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13175},
  shortjournal = {Expert Syst.},
  title        = {Knowledge-aware representation learning for diagnosis prediction},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical learning from brazilian fake news.
<em>EXSY</em>, <em>40</em>(3), e13171. (<a
href="https://doi.org/10.1111/exsy.13171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fake news is information that does not represent reality but is commonly shared on the internet as if it were true, mainly because of its dramatic, appealing, and controversial content. Therefore, a relevant issue is to find characteristics that can assist in identifying Fake News, mainly nowadays, where an increasing number of fake news is spread all over the internet every day. This work aims to extract knowledge from Brazilian fake news data based on statistical learning. Initially, an exploratory data analysis is performed for the available variables to extract insights from the differences between fake and true news. Then, the prediction and modelling are carried out. The learning phase aims to build a model and measure the features that best explain the behaviour of misleading texts, which leads to a parsimonious model. Finally, the test phase estimates the fitted model accuracy based on 10-fold cross-validation in the Monte Carlo framework. The results show that four variables are significant to explain fake news. Moreover, our model achieved comparable results with state-of-the-art, 0.941 F -measure, for a single classifier while having the advantage of being a parsimonious model. This work&#39;s details and code can be found at https://github.com/limagbz/fake-news-ptBR .},
  archive      = {J_EXSY},
  author       = {Gabriel B. Lima and Thiago de M. Chaves and Wanessa W. L. Freitas and Renata M. C. R. de Souza},
  doi          = {10.1111/exsy.13171},
  journal      = {Expert Systems},
  month        = {3},
  number       = {3},
  pages        = {e13171},
  shortjournal = {Expert Syst.},
  title        = {Statistical learning from brazilian fake news},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-adaptive bald eagle search optimization algorithm
with dynamic opposition-based learning for global optimization problems.
<em>EXSY</em>, <em>40</em>(2), e13170. (<a
href="https://doi.org/10.1111/exsy.13170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bald Eagle Search optimization (BES) is introduced recently, which mimics the bald eagles&#39; hunting and food searching behaviour. The capability of a BES algorithm is enhanced in this paper by avoiding local optima stagnation and premature convergence problems. The BES algorithm is modified to enhance the performance of the algorithm and the modified algorithm is called the Self Adaptive Bald Eagle Search (SABES) algorithm. Dynamic-opposite Learning (DOL) method is invoked in the initialization phase to increase the population diversity and convergence speed. To find better global solutions, the exploitation capability of the SBES algorithm is enhanced by considering the dynamic-opposite solutions. In addition, the algorithmic parameter values of the BES algorithm have been determined using the linear and non-linear time-varying adaption strategy to create a balance between the search abilities which promotes the overall performance of the algorithm. The performance of the SABES algorithm is validated by comparing the results of 50 benchmark functions and CEC2017 functions with different erstwhile algorithms. The proposed algorithm achieves the best results in 80% of the benchmark functions, whereas the BES only gets the best results in 56% of functions. For the CEC2017, the SABES algorithm achieves optimal results for 20 functions which are highest in comparison to state-of-the-art algorithms. The feasibility and effectiveness of the proposed algorithm are checked using the 15 CEC2020 competition real-world single-objective constrained optimization problems. The SABES achieves a 100% success rate and 100% feasibility rate in comparison to other well-regarded algorithms. The statistical significance of the algorithm has been proved using Friedman&#39;s mean rank and Wilcoxon sign rank test.},
  archive      = {J_EXSY},
  author       = {Suvita Rani Sharma and Manpreet Kaur and Birmohan Singh},
  doi          = {10.1111/exsy.13170},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13170},
  shortjournal = {Expert Syst.},
  title        = {A self-adaptive bald eagle search optimization algorithm with dynamic opposition-based learning for global optimization problems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A decision-making support system module for customer
segmentation and ranking. <em>EXSY</em>, <em>40</em>(2), e13169. (<a
href="https://doi.org/10.1111/exsy.13169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many businesses have their own method of segmentation and of customer evaluation to help them give the appropriate managerial attention to each segment and each customer. This paper proposes a more refined and objective decision-making support system module that allows segmentation and full ranking of customers. It offers several improvements and advantages over the state-of-the-art methods. The module is based on customer criteria with quantitative values that can be extracted from the organizational information system. Customer scores are calculated objectively on the basis of measurable criteria without the need for human evaluation. The module determines the relative location of each customer within that customer&#39;s segment (for example, Platinum, Gold, Silver, and Bronze), tracks changes that occur over time, and enables a full and precise ranking of the customers according to company-defined criteria. The module can be computerized and results can be generated quickly at any time, using up-to-date data. The module&#39;s design was based on feedback from a survey conducted among 39 managers and its applicability was successfully demonstrated in a real-world case study. The decision-makers in the organization where the case study was conducted stated that they would apply the proposed method quarterly (four times a year) instead of once a year. It was also found that the proposed method saves about 90% of the time and resources required to prepare a customer portfolio management and customer ranking compared to the subjective method that was used.},
  archive      = {J_EXSY},
  author       = {Yossi Hadad and Baruch Keren},
  doi          = {10.1111/exsy.13169},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13169},
  shortjournal = {Expert Syst.},
  title        = {A decision-making support system module for customer segmentation and ranking},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward masked face recognition: An effective facial feature
extraction and refinement model in multiple scenes. <em>EXSY</em>,
<em>40</em>(2), e13166. (<a
href="https://doi.org/10.1111/exsy.13166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the impact of the COVID-19 epidemic, the demand for masked face recognition technology has increased. In the process of masked face recognition, some problems such as less feature information and poor robustness to the environment are obvious. The current masked face recognition model is not quantified enough for feature extraction, there are large errors for faces with high similarity, and the categories cannot be clustered during the detection process, resulting in poor classification of masks, which cannot be well adapted to changes in multiple environments. To solve current problems, this paper designs a new masked face recognition model, taking improved Single Shot Multibox Detector (SSD) model as a face detector, and replaces the input layer VGG16 of SSD with Deep Residual Network (ResNet) to increase the receptive field. In order to better adapt to the network, we adjust the convolution kernel size of ResNet. In addition, we fine-tune the Xception network by designing a new fully connected layer, and reduce the training cycle. The weights of the three input samples including anchor, positive and negative are shared and clustered together with triplet network to improve recognition accuracy. Meanwhile, this paper adjusts alpha parameter in triplet loss. A higher value of alpha can improve the accuracy of model recognition. We further adopt a small trick to classify and predict face feature vectors using multi-layer perceptron (MLP), and a total of 60 neural nodes are set in the three neural layers of MLP to get higher classification accuracy. Moreover, three datasets of MFDD, RMFRD and SMFRD are fused to obtain high-quality images in different scenes, and we also add data augmentation and face alignment methods for processing, effectively reducing the interference of the external environment in the process of model recognition. According to the experimental results, the accuracy of masked face recognition reaches 98.3%, it achieves better results compared with other mainstream models. In addition, the hyper-parameters tuning experiment is carried out to improve the utilization of computing resources, which shows better results than the indicators of different networks.},
  archive      = {J_EXSY},
  author       = {Hongxing Peng and Zheng Xing and Xiaotang Liu and Zongmei Gao and Huijun He},
  doi          = {10.1111/exsy.13166},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13166},
  shortjournal = {Expert Syst.},
  title        = {Toward masked face recognition: An effective facial feature extraction and refinement model in multiple scenes},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A text matching model based on dynamic multi-mask and
augmented adversarial. <em>EXSY</em>, <em>40</em>(2), e13165. (<a
href="https://doi.org/10.1111/exsy.13165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The text matching is a basic task of NLP and is important for tasks such as text retrieval, question answering, and so forth. The development of pre-trained language models has promoted the progress of text matching tasks. However, due to the natural particularity of Chinese characters and expressions, the Chinese text matching tasks still have problems such as word segmentation difficulty, serious semantic loss, and model instability. In this paper, we propose the DAINet model, which includes DMM, AA and IO modules. We use the Dynamic Multi-Mask module (DMM) to enhance the completeness of word segmentation. Then we use the Augmented Adversarial module (AA) to further extraction of semantic information. Finally, we use the Integrated Output module (IO) for a more stable output. We conducted experiments on LCQMC, BQ and Xiaobu datasets and compared the results with seven strong baseline models. The results showed that DAINet model made great improvement, including improving ACC value of BQ dataset to , AUC value to , ACC value of LCQMC dataset to and AUC value to . The ACC value of Xiaobu dataset was improved to and the AUC value was improved to . Further ablation experiment results show that the proposed DMM, AA and IO modules have good adaptability and improvement over existing models.},
  archive      = {J_EXSY},
  author       = {Lin Zhong and Jun Zeng and Yang Yu and Hongjin Tao and Wenying Jiang and Luxi Cheng},
  doi          = {10.1111/exsy.13165},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13165},
  shortjournal = {Expert Syst.},
  title        = {A text matching model based on dynamic multi-mask and augmented adversarial},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Machine learning techniques for the ab initio bravais
lattice determination. <em>EXSY</em>, <em>40</em>(2), e13160. (<a
href="https://doi.org/10.1111/exsy.13160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning-based algorithms have been widely applied recently in different areas due to its ability to solve problems in all fields. In this research, machine learning techniques classifying the Bravais lattices from a conventional X-ray diffraction diagram have been applied. Indexing algorithms are an essential tool of the preliminary protocol for the structural determination problem in crystallography. The task of reverting the obtained information in reciprocal lattice to direct space is a complex issue. As an alternative way to afford this problem, different machine learning algorithms have been applied and a comparison between them has been conducted. The obtained accuracy was 95.9% using 10-fold cross-validation (while the best result obtained so far has been 84%). A model based on Bragg positions was our unique predictor, allowing us to obtain the set of the interplanar lattice distances. Our model was successfully checked with a complex example. In addition, our procedure incorporates the following advantages: robustness versus imprecision in data acquisition and reduction of the amount of necessary input data. This is the first time so far that such classification has been carried out in true ab initio condition.},
  archive      = {J_EXSY},
  author       = {Esther-Lydia Silva-Ram√≠rez and Inmaculada Cumbrera-Conde and Rafael Cano-Crespo and Francisco-Luis Cumbrera},
  doi          = {10.1111/exsy.13160},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13160},
  shortjournal = {Expert Syst.},
  title        = {Machine learning techniques for the ab initio bravais lattice determination},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Association rule mining using new discrete elephant swarm
approaches. <em>EXSY</em>, <em>40</em>(2), e13159. (<a
href="https://doi.org/10.1111/exsy.13159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we are proposing two novel swarm intelligence approaches for solving discrete problems. Two different continuous evolutionary algorithms inspired by the behaviour of elephants, namely elephant herding optimization and elephant swarm water search algorithm, are studied and analysed in order to propose discrete versions of the latter called discrete elephant herding optimization and discrete elephant swarm water search algorithm. As an illustration of how our proposals can work on discrete problems, a case study on association rule mining is carried out where the proposed discrete algorithms are modelled and applied on the problem in order to extract interesting association rules from large-scale databases. Extensive experiments on eight different real-life and relevant datasets with various sizes showed that both our proposals yield good results that compete and sometimes outperform state of the art algorithms.},
  archive      = {J_EXSY},
  author       = {Hadjer Moulai and Habiba Drias},
  doi          = {10.1111/exsy.13159},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13159},
  shortjournal = {Expert Syst.},
  title        = {Association rule mining using new discrete elephant swarm approaches},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Four-gene signature based on machine learning filtration
could predict prognosis of patients with breast cancer. <em>EXSY</em>,
<em>40</em>(2), e13157. (<a
href="https://doi.org/10.1111/exsy.13157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Bo Liu and Huina Wang and Xin Wang and Junqi Long and Xujie Zhuang and Xinchan Ji and Nian Zhu and Jinmeng Li and Ting Gao and Xuehui Zhang and Jiangyong Yu and Shuangtao Zhao},
  doi          = {10.1111/exsy.13157},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13157},
  shortjournal = {Expert Syst.},
  title        = {Four-gene signature based on machine learning filtration could predict prognosis of patients with breast cancer},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classify nodes based on their degree distribution: A more
scalable method for influence maximization. <em>EXSY</em>,
<em>40</em>(2), e13156. (<a
href="https://doi.org/10.1111/exsy.13156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main problems in viral marketing is influence maximization (IM). With a social network and a predefined propagation model, the aim is to seek a subset of nodes that spread the influence widely into the network. Most scalable methods with provable approximation guarantees are presented for this problem based on the reverse influence sampling (RIS) framework. The RIS framework has two phases: sampling and node selection. The sampling phase encountered two challenges in the sampling phase: the number of required samples and the sampling method. Most methods have focused on the first challenge, that is, sample size, and have tried to provide a rigid sample size. In this paper, we focus on the second challenge: how to improve the precision of sampling. We propose to use stratified sampling rather than simple random sampling. Since the degree of each node is one of the affecting factors in the diffusion process. This issue leads us to use stratified sampling based on a degree distribution. The results show that with the application of the proposed method, the solution can estimate with fewer samples, which is faster than the state-of-the-art methods.},
  archive      = {J_EXSY},
  author       = {Rouhollah Javadpour Boroujeni and Seyfollah Soleimani},
  doi          = {10.1111/exsy.13156},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13156},
  shortjournal = {Expert Syst.},
  title        = {Classify nodes based on their degree distribution: A more scalable method for influence maximization},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An unsupervised policy relevance scoring method: Taking
chinese social security policies as the application case. <em>EXSY</em>,
<em>40</em>(2), e13149. (<a
href="https://doi.org/10.1111/exsy.13149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizing and managing policy documents (PDs) issued to the public in a good way can improve the efficiency of government employees and make it easier for the public to find the needed policy information. However, existing PDs are organized only by dates and manually defined categories; besides, PDs issued by different government branches are isolated from each other. These problems make it challenging and time-consuming for the public to find the needed policy information. We argue that implicit links should be established between PDs based on their relevance, thus helping the public find the needed policy information efficiently. To this end, we propose an unsupervised relevance scoring method for PDs consist six modules, taking Chinese social security policies as the application case. The method combines the TextRank algorithm, TF-IDF representation, mutual information and left‚Äìright information entropy algorithm, and BERT. The method can decrease the interference of noisy words in PDs to relevance scoring. In addition, the method can consider multiple features of PDs simultaneously so that the measure of relevance can be more comprehensive. The method is not driven by domain-specific labelled data, hence can be easily generalized to PDs in various domains. We construct a dataset containing 5000 Chinese social security policies and then conduct experiments on it to evaluate our method. Experimental results show that our method is feasible and can bring convenience to government agencies and the public to a certain extent. Furthermore, our method achieves more than a 3% improvement in evaluation results on test tasks than the methods with a similar purpose in the legal AI community.},
  archive      = {J_EXSY},
  author       = {Jingyun Sun and Shaobin Huang and Rongsheng Li},
  doi          = {10.1111/exsy.13149},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13149},
  shortjournal = {Expert Syst.},
  title        = {An unsupervised policy relevance scoring method: Taking chinese social security policies as the application case},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation and ranking of failures in manufacturing process
by combining best-worst method and VIKOR under type-2 fuzzy environment.
<em>EXSY</em>, <em>40</em>(2), e13148. (<a
href="https://doi.org/10.1111/exsy.13148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the important and persistent problems that engineers face in the automotive industry is the reliability of production equipment. This research promotes a new fuzzy multicriteria model for determining the priority of failures in an exact manner. In this way, the decision makers can determine the management activities whose application should result in enhanced manufacturing process reliability, promptly. The analysis of failures is based on Failure mode and effects analysis that is extended with added risk factors, which represents the incremental improvement compared to the current literature sources. The relative importance of risk factors and their values are presented by pre-defined linguistic expressions modelled by the interval type-2 fuzzy numbers. The assessment of risk factors&#39; relative importance is set as a fuzzy group decision-making problem. The weights vector is determined by using the extended Best-Worst Method. The rank of failures is obtained by employing the modified VIseKriterijumska Optimizacija i kompromisno Resenje (Eng. Multi-criteria optimization and compromise solution) which reflects the scientific contribution of the research, and simultaneously, the second incremental improvement of the proposed model compared to the existing state of the art. These incremental improvements are: (i) The fuzzy algebra rules have been used for determining the group utility value and (ii) and individual regret value is determined by comparing the two interval type-2 fuzzy numbers. The model testing and verification are performed on real data in an automotive supply chain.},
  archive      = {J_EXSY},
  author       = {Aleksandar Aleksiƒá and Dragan D. Milanoviƒá and Nikola Komatina and Danijela Tadiƒá},
  doi          = {10.1111/exsy.13148},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13148},
  shortjournal = {Expert Syst.},
  title        = {Evaluation and ranking of failures in manufacturing process by combining best-worst method and VIKOR under type-2 fuzzy environment},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid data-level sampling approach in learning from
skewed user-click data for click fraud detection in online advertising.
<em>EXSY</em>, <em>40</em>(2), e13147. (<a
href="https://doi.org/10.1111/exsy.13147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenging issues in user-click data of online advertising is the uneven class distribution which biases classification models. Resampling the data is a popular choice for obtaining class balance. However, oversampling results in overfitting, whilst under-sampling results in information loss. Moreover, enhancing separability between samples, where the classes overlap closer to the decision boundary, is another challenge, which requires a careful pruning of instances towards increasing the separability in data space. Therefore, in this work, a new hybrid data sampling algorithm SMOTEOSS is designed and evaluated, concatenating the synthetic minority oversampling technique (SMOTE) followed by one-sided selection (OSS) to balance the class distribution. The working of SMOTEOSS is twofold- first, it oversamples the under-represented class distribution using the SMOTE by generating synthetic instances. However, the generation of synthetic instances closer to the decision boundary directly influences the learning model&#39;s decision-making. Utilising OSS, the proposed method then identifies TOMEKLINKS and eliminates the noisy majority instances whilst eliminating the redundant instances. The proposed method&#39;s effectiveness is validated on the FDMA 2012 dataset against 10 state-of-the-art sampling methods utilising the gradient tree boosting learning model. To authenticate SMOTEOSS, a fair comparison is made by conducting experiments on other 10 benchmark imbalanced datasets using 10-fold cross-validation. Performance is measured using average precision, recall, F1-score, G-mean, the area under curve (AUC) and reduction rate. Results showed that the designed hybrid methodology is an efficient alternative to existing sampling methods. The Wilcoxon signed-rank test is employed to demonstrate significant differences amidst the proposed and conventional sampling algorithms.},
  archive      = {J_EXSY},
  author       = {Deepti Sisodia and Dilip Singh Sisodia},
  doi          = {10.1111/exsy.13147},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13147},
  shortjournal = {Expert Syst.},
  title        = {A hybrid data-level sampling approach in learning from skewed user-click data for click fraud detection in online advertising},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chaotic spotted hyena optimizer for numerical problems.
<em>EXSY</em>, <em>40</em>(2), e13146. (<a
href="https://doi.org/10.1111/exsy.13146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spotted hyena optimizer (SHO) is a new metaheuristic algorithm that replicates spotted hyenas&#39; hunting and social behaviour. This article proposes novel SHO algorithm that utilizes chaotic maps for fine-tuning of control parameters. The chaotic maps help SHO to enhance the searching behaviour and preclude the solution to get trapped in local optima. The authors suggest 10 novel chaotic versions of SHO. The algorithms&#39; performance is evaluated using 29 standardized test functions. The finding reveal that some of the presented algorithms outperform the standard SHO in terms of search capability and solution quality. In addition, five competitive approaches are compared with the suggested algorithms. It is observed from the results that chaos-based spotted hyena optimizer (CSHO) achieved approximately 3% improvement over SHO in terms of fitness value. CSHO is also tested using five engineering design problems. CSHO achieved a 3%‚Äì5% improvement over the existing metaheuristic algorithms in terms of optimal design cost. Experimental results reveal that CSHO outperforms the existing metaheuristic algorithms.},
  archive      = {J_EXSY},
  author       = {Vijay Kumar and Dilbag Singh},
  doi          = {10.1111/exsy.13146},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13146},
  shortjournal = {Expert Syst.},
  title        = {Chaotic spotted hyena optimizer for numerical problems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective points of interest recommendation approach
based on embedded meta-path of spatiotemporal data. <em>EXSY</em>,
<em>40</em>(2), e13145. (<a
href="https://doi.org/10.1111/exsy.13145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of mobile networks and the rapid prevalence of location-based social networks (LBSN), a massive volume of spatiotemporal data has been generated, which is valuable for points of interest (POI) recommendation. However, current studies have not unleashed the full power of such spatiotemporal data, which either explore only a single dimension of the data or consider multiple factors in an asynchronous fashion. In this article, we propose a novel spatiotemporal network-based recommender framework (STNBR) to effectively recommend POIs for users. Specifically, we first establish a comprehensive conceptual model of spatiotemporal data, involving various essential factors for POIs recommendation. On top of the conceptual model, we design a series of meaningful meta-paths that simultaneously consider the time and location factors to precisely capture the semantics of user behaviours. By profiling users based on their embedded meta-paths, our approach can yield meaningful POIs recommendations. We have evaluated our proposal using a realistic dataset obtained from Foursquare and Gowalla, the results of which show that our STNBR model outperforms existing approaches.},
  archive      = {J_EXSY},
  author       = {Rui Song and Tong Li and Xin Dong and Zhiming Ding},
  doi          = {10.1111/exsy.13145},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13145},
  shortjournal = {Expert Syst.},
  title        = {An effective points of interest recommendation approach based on embedded meta-path of spatiotemporal data},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ASDFL: An adaptive super-pixel discriminative
feature-selective learning for vehicle matching. <em>EXSY</em>,
<em>40</em>(2), e13144. (<a
href="https://doi.org/10.1111/exsy.13144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a large number of cameras in modern transportation system that capture numerous vehicle images continuously. Therefore, automatic analysis of these vehicle images is helpful for traffic flow management, criminal investigations and vehicle inspections. Vehicle matching, which aims to determine whether two input images depict an identical vehicle, is one of the core tasks in vehicle analysis. Recent relevant studies have focused on local feature extraction instead of global extraction, since local details can provide crucial cues to distinguish between cars. However, these methods do not select local features; that is, they do not assign weights to local features. Therefore, in this research, we systematically study the vehicle matching task, and present a novel annotation-free local-based deep learning method called Adaptive super-pixel discriminative feature-selective learning (ASDFL) to address this issue. In ASDFL, vehicle images are segmented into clusters of super-pixels of similar size by considering the location and colour similarities of pixels without using any component-level annotation. These super-pixels are deemed to be the virtual components of vehicles. Moreover, a convolutional neural network is used to extract the deep features of these virtual components. Thereafter, an instance-specific mask generation module driven by the extracted global features is enhanced to produce a mask to select the most distinctive virtual components of each vehicle image pair in the feature space. Finally, the vehicle matching task is accomplished by classifying the selected virtual component features of each imaged vehicle pair. Extensive experiments on two popular vehicle identification benchmarks demonstrate that our method is 1.57% and 0.8% more accurate than the previous baselines in a vehicle matching task on the VeRi and VehicleID datasets, respectively, which demonstrates the effectiveness of our method.},
  archive      = {J_EXSY},
  author       = {Rong Qin and Huanhuan Lv and Yi Zhang and Luwen Huangfu and Sheng Huang},
  doi          = {10.1111/exsy.13144},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13144},
  shortjournal = {Expert Syst.},
  title        = {ASDFL: An adaptive super-pixel discriminative feature-selective learning for vehicle matching},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid recommender system based on description/dialetheic
logic and linked data. <em>EXSY</em>, <em>40</em>(2), e13143. (<a
href="https://doi.org/10.1111/exsy.13143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we developed a hybrid recommender system based on description/dialetheic logic, which provides an innovative architecture for the recommendation based on ambiguous reasoning. In addition, our approach allows enriching the knowledge during the reasoning using the linked data paradigm. Thus, the architecture allows the integration of linked data, in order to be exploited by the recommender. On the other hand, the reasoning mechanisms of dialetheic logic allow handling situations of contradiction or inconsistency, to determine the information to offer to users. According to the reviewed literature, our proposal is the first that implements a dialetheic engine in a Recommender System. In general, there are a lot of works about the utilization of linked open data (LOD) to improve the Recommender Systems, for example, to enrich the information to recommend or to solve the cold start problem. However, there are no proposals to deal with the problems of ambiguity, incoherence or incompleteness of the information in these environments extended with LOD. In this way, this paper proposes a hybrid reasoning engine that uses the linked data paradigm for the knowledge extraction, and dialetheic logic for the management of inconsistency/ambiguity information, in order to obtain recommendations. Particularly, the information extracted with Linked Data is processed with the Dialetheic Logic reasoner to solve ambiguous cases. Thus, each recommendation is enriched with related content extracted from the linked data sources, which has been disambiguated. The results are very promising since our hybrid reasoning mechanism allows obtaining more precise recommendations, considering the different classical states of ambiguity in dialetheic logic (contingent statements about the future, failure of a presupposition, vagueness, counterfactual reasoning), according to various metrics of quality used to evaluate the recommendations achieved.},
  archive      = {J_EXSY},
  author       = {Ricardo Dos Santos and Jose Aguilar},
  doi          = {10.1111/exsy.13143},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e13143},
  shortjournal = {Expert Syst.},
  title        = {A hybrid recommender system based on description/dialetheic logic and linked data},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Four-dimensional collision detection and behaviour based on
the physics-based calculation. <em>EXSY</em>, <em>40</em>(2), e12668.
(<a href="https://doi.org/10.1111/exsy.12668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of 4-D space visualization, the information of 4-D space is obtained by projecting 4-D data onto 3-D space. Most of the previous research has been aimed at the recognition of 4-D space, whereas the target of the recognition has been limited to the geometrical information of 4-D objects in 4-D space or static spatial information without dynamics. Our research aims to develop a visualization system for providing the human experience of the physics-based environment in 4-D space. In this research, we mainly focus on collision detection and the behaviour of 4-D objects in 4-D space in order to construct the physics-based environment of 4-D space. Our contribution in this paper is the development of a collision detection algorithm for 4-D objects and a calculation method for physics based behaviour of 4-D objects. Our proposed collision detection algorithm is based on the intersection test of tetrahedrons in 4-D space, so that 4-D objects in our system is represented as tetrahedral meshes. The tetrahedron-based collision detection algorithm is performed by a combination of half-space tests with the use of 5-D homogeneous processing to enhance the calculation accuracy of the collision detection. Our proposed method calculates the behaviour of the 4-D objects after the collision by solving the motion equation based on the principle of physics. Consequently, the visualization system with the proposed algorithm allows us to observe the physics-based environment in 4-D space.},
  archive      = {J_EXSY},
  author       = {Yuki Nakai and Takanobu Miwa and Hiroki Shigemune and Hideyuki Sawada},
  doi          = {10.1111/exsy.12668},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12668},
  shortjournal = {Expert Syst.},
  title        = {Four-dimensional collision detection and behaviour based on the physics-based calculation},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topic identification of text-based expert stock comments
using multi-level information fusion. <em>EXSY</em>, <em>40</em>(2),
e12641. (<a href="https://doi.org/10.1111/exsy.12641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock investment is an important mode of asset allocation and a crucial means of financial management. How to grasp the movement of stock price and predict its trend have been the focus of investors and investment companies. Since expert stock comments contain abundant essential information for investment decisions, how to identify the topic of expert stock comments with high precision and efficiency is an important research topic. However, the existing methods usually employ single feature selection strategies for topic identification of stock comments, which may lead to low accuracy. Thus, to deal with this limitation, we propose a multi-level information fusion method to construct a topic identification system of stock comments. Specifically, we firstly fuse various complementary feature selection methods via a multi-view learning framework which can comprehensively represent text-based topics. In addition , regarding the decision process, we propose a fusion strategy based on belief value which can further improve the classification performance. The experimental results indicate that the proposed multi-level information fusion method is not only superior to other methods in terms of classification, it is also able to accurately capture topics of expert stock comments.},
  archive      = {J_EXSY},
  author       = {Feng Zhao and Jiahui Zhang and Zhiyuan Chen and Xiaofeng Zhang and Qingsong Xie},
  doi          = {10.1111/exsy.12641},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12641},
  shortjournal = {Expert Syst.},
  title        = {Topic identification of text-based expert stock comments using multi-level information fusion},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning approach for specular highlight removal from
transmissive materials. <em>EXSY</em>, <em>40</em>(2), e12598. (<a
href="https://doi.org/10.1111/exsy.12598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appearance of specular highlights in images is one main factor affecting accurate material or object recognition tasks. Such an appearance has a misleading effect on the true gradient information found in transmissive material images. Certain methods use specular highlights as an intrinsic feature of transparency to detect transparent objects. However, this process reduces the robustness of methods in applications with opaque and shiny materials and in the classification of tasks among related features, such as transparency and translucency. Thus, correcting this artefact can enhance texture- or gradient-based image and video analyses. However, the correction of small or large regions with specular highlights from transmissive materials, such as glass, plastic and water, is complex and ambiguous. These materials are sensitive to specular highlights and exhibit high degrees of reflection. In this study, we propose a deep learning framework to address the problem. A partial convolution-based inpainting method is integrated with automatic semantic mask generation by using a simple adaptive binarization to detect highlight spots during training and inference. The proposed framework improves the learning process by capturing the semantic nature of specular highlights. Moreover, the framework eliminates the use of image-mask pairs during inference and avoids predefined irregular random mask training. We qualitatively and quantitatively evaluate the proposed framework by using new and existing publicly available datasets that contain specular images. Experimental results show that our framework registers competitive performance and considerably reduces computational time.},
  archive      = {J_EXSY},
  author       = {Amanuel Hirpa Madessa and Junyu Dong and Yanhai Gan and Feng Gao},
  doi          = {10.1111/exsy.12598},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12598},
  shortjournal = {Expert Syst.},
  title        = {A deep learning approach for specular highlight removal from transmissive materials},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retyping of triple-negative breast cancer based on
clustering method. <em>EXSY</em>, <em>40</em>(2), e12583. (<a
href="https://doi.org/10.1111/exsy.12583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triple-negative breast cancer is the worst prognosis in breast cancer, accounting for 10.0‚Äì20.8% of all breast cancers. Considering that triple-negative breast cancer has great heterogeneity and very poor prognosis, clinical medication guidance is in urgent need of a more detailed classification of breast cancer itself. Although many researchers have been dedicated to the clustering of triple-negative breast cancer and have found possible targets based on typing, their results are not closely related to the prognosis. This paper utilizes three clustering methods to retype the patient data with triple-negative breast cancer, and the results show that the triple-negative breast cancer data could be classified into two categories. Eight important genes and three important clinical factors related to the prognosis of two types of triple-negative breast cancer have been obtained. These genes have the following three characteristics: co-expression, differential expression and interaction. In terms of breast cancer control, the prognosis can be controlled as much as possible by regulating gene levels, which provides new directions and ideas for related research on breast cancer prognosis.},
  archive      = {J_EXSY},
  author       = {Bo Liu and Xingrui Li and Huina Wang and Shuangtao Zhao and Jianqiang Li and Guangzhi Qu and Fei Wang},
  doi          = {10.1111/exsy.12583},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12583},
  shortjournal = {Expert Syst.},
  title        = {Retyping of triple-negative breast cancer based on clustering method},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved model for sentiment analysis on luxury hotel
review. <em>EXSY</em>, <em>40</em>(2), e12580. (<a
href="https://doi.org/10.1111/exsy.12580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a heuristic model for sentiment analysis on luxury hotel reviews to analyse and explore marketing insights from attitudes and emotions expressed in reviews. We make several significant contributions to visual and multimedia analytics. This research will develop the practical application of visual and multimedia analytics as the research foundation is based on information analytics, geospatial analytics, statistical analytics and data management. Large amounts of data are generated by hotel customers on the Internet, which provides a good opportunity for managers and analysts to explore the hidden information. The analysis of luxury hotels involves different types of data, including real-world scale data, high-dimensional data and geospatial data. The diversity of data increases the difficulty of processing computational visual analytics. It leads to that some classical classification methods, which cost too much time and have high requirements for hardware, are excluded. The goal is to achieve a compromise between performance and cost. An experiment of this model is operated using data extracted from Booking.com. The entire framework of this experiment includes data collection, data preprocessing, feature engineering consisting of term frequency-inverse document frequency and Doc2Vec based feature generation and feature selection, Random Forest classification, data analysis and data visualization. The whole process combines statistical analysis, review sentiment analysis and visual analysis to make full use of this dataset and gain more decision-making information to improve luxury hotels&#39; service quality. Compared with simple sentiment analysis, this integrated analytics in social media is expected to be used in practice to gain more insights. The result shows that luxury hotels should focus on staff training, cleanness of rooms and location choice to improve customer satisfaction. The sentiment distribution shows that scores are consistent with the emotion they show in reviews. Hotels in Spain have a much better average score than hotels in the other five countries. In the experiment, the sentiment analysis model is evaluated by receiver operating characteristic and precision-recall curve. It is proved that this model performs well. Twenty most essential features have been listed for future adjustments to the model.},
  archive      = {J_EXSY},
  author       = {Victor Chang and Lian Liu and Qianwen Xu and Taiyu Li and Ching-Hsien Hsu},
  doi          = {10.1111/exsy.12580},
  journal      = {Expert Systems},
  month        = {2},
  number       = {2},
  pages        = {e12580},
  shortjournal = {Expert Syst.},
  title        = {An improved model for sentiment analysis on luxury hotel review},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial neural network and dataset optimization for
implementation of linear system models in resource-constrained embedded
systems. <em>EXSY</em>, <em>40</em>(1), e13142. (<a
href="https://doi.org/10.1111/exsy.13142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-board training of artificial neural network (ANN) is important in instances where real time data are required for model training. Provision of on-board intelligence enables the developed systems to self-recalibrate and enhances their efficiencies. In this work, investigations have been performed to determine optimized parameters of ANN model for linear systems. The performance parameters that is, model parameters, memory requirements, accuracy and processing time are chosen by considering the model to be installed on commercially available microcontrollers that have very limited on-board memory. Minimum data requirements for training ANN models of linear systems are also explored for better performance. All dataset ranges are normalized in order to exclude the effects of range differences. It is shown that for linear systems, 1‚Äì3‚Äì1 architecture produces best results against ‚â§100 data points when Bayesian Regularization (BR) training function is used along with Log Sigmoid Activation function. Simulations for 1‚Äì3‚Äì1 architecture are then performed for datasets having 10, 25, 50 and 100 data points. The results show that training with 25 data points produces over-all better performance than other datasets. A large dataset utilizes more training time and memory whereas a smaller dataset produces relatively lesser accuracy. The effects of clustered data and uniformly distributed data are also explored. It is found that total epochs in case of clustered data are significantly higher than uniformly distributed data. The combination of these optimized parameters that is, 1‚Äì3‚Äì1 architecture, with BR and Log function, for ‚â§100 data points can be used for the development and implementation of linear components or systems in resource-constrained embedded systems.},
  archive      = {J_EXSY},
  author       = {Abdul Sami and Ali Asif and Muhammad Imran and Farah Aziz and Muhammad Yasir Noor},
  doi          = {10.1111/exsy.13142},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13142},
  shortjournal = {Expert Syst.},
  title        = {Artificial neural network and dataset optimization for implementation of linear system models in resource-constrained embedded systems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of COVID-19 and its pulmonary stage using bayesian
hyperparameter optimization and deep feature selection methods.
<em>EXSY</em>, <em>40</em>(1), e13141. (<a
href="https://doi.org/10.1111/exsy.13141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the first case of COVID-19 was reported in December 2019, many studies have been carried out on artificial intelligence for the rapid diagnosis of the disease to support health services. Therefore, in this study, we present a powerful approach to detect COVID-19 and COVID-19 findings from computed tomography images using pre-trained models using two different datasets. COVID-19, influenza A (H1N1) pneumonia, bacterial pneumonia and healthy lung image classes were used in the first dataset. Consolidation, crazy-paving pattern, ground-glass opacity, ground-glass opacity and consolidation, ground-glass opacity and nodule classes were used in the second dataset. The study consists of four steps. In the first two steps, distinctive features were extracted from the final layers of the pre-trained ShuffleNet, GoogLeNet and MobileNetV2 models trained with the datasets. In the next steps, the most relevant features were selected from the models using the Sine‚ÄìCosine optimization algorithm. Then, the hyperparameters of the Support Vector Machines were optimized with the Bayesian optimization algorithm and used to reclassify the feature subset that achieved the highest accuracy in the third step. The overall accuracy obtained for the first and second datasets is 99.46% and 99.82%, respectively. Finally, the performance of the results visualized with Occlusion Sensitivity Maps was compared with Gradient-weighted class activation mapping. The approach proposed in this paper outperformed other methods in detecting COVID-19 from multiclass viral pneumonia. Moreover, detecting the stages of COVID-19 in the lungs was an innovative and successful approach.},
  archive      = {J_EXSY},
  author       = {Nedim Muzoƒülu and Ahmet Mesrur Halefoƒülu and Muhammed Onur Avci and Melike Kaya Karaaslan and Bekir Sƒ±ddƒ±k Binboƒüa Yarman},
  doi          = {10.1111/exsy.13141},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13141},
  shortjournal = {Expert Syst.},
  title        = {Detection of COVID-19 and its pulmonary stage using bayesian hyperparameter optimization and deep feature selection methods},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of decision model for sensitive crop irrigation
system. <em>EXSY</em>, <em>40</em>(1), e13119. (<a
href="https://doi.org/10.1111/exsy.13119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture Industry is highly dependent on environmental and weather conditions. Many times, crops are spoiled because of sudden changes in weather. Therefore, we need a decision model to take care the water requirement of sensitive crops of agriculture industry. The proposed work presents a novel and proficient hybrid model for sensitive crop irrigation system (SCIS). For implementation of the model, brassica crop is taken. The duration and amount of water to be supplied is based upon the weather prediction and soil condition information. The decision model is developed using adaptive neuro-fuzzy inference system (ANFIS) and artificial neural network (ANN) for brassica crops. In this model, if the input data values are available in range, then ANFIS model would be preferred and if the data sets are available for training, testing and validation then ANN model would be the best choice. The soil moisture, soil status in terms of temperature and leaf wetness are the input and flow control of sprinklers is the out for SCIS. The predicted outputs are analysed to assert the suitability of the proposed approach in the brassica crops. The proposed SCIS achieved an accuracy of 91% and 99% for ANFIS and ANN models respectively.},
  archive      = {J_EXSY},
  author       = {Anita Thakur and Prakriti Aggarwal and Ashwani Kumar Dubey and Ahmed Abdelgawad and Alvaro Rocha},
  doi          = {10.1111/exsy.13119},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13119},
  shortjournal = {Expert Syst.},
  title        = {Design of decision model for sensitive crop irrigation system},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subgroup mining for performance analysis of regression
models. <em>EXSY</em>, <em>40</em>(1), e13118. (<a
href="https://doi.org/10.1111/exsy.13118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms have shown several advantages compared to humans, namely in terms of the scale of data that can be analysed, delivering high speed and precision. However, it is not always possible to understand how algorithms work. As a result of the complexity of some algorithms, users started to feel the need to ask for explanations, boosting the relevance of Explainable Artificial Intelligence. This field aims to explain and interpret models with the use of specific analytical methods that usually analyse how their predicted values and/or errors behave. While prediction analysis is widely studied, performance analysis has limitations for regression models. This paper proposes a rule-based approach, Error Distribution Rules (EDRs), to uncover atypical error regions, while considering multivariate feature interactions without size restrictions. Extracting EDRs is a form of subgroup mining. EDRs are model agnostic and a drill-down technique to evaluate regression models, which consider multivariate interactions between predictors. EDRs uncover regions of the input space with deviating performance providing an interpretable description of these regions. They can be regarded as a complementary tool to the standard reporting of the expected average predictive performance. Moreover, by providing interpretable descriptions of these specific regions, EDRs allow end users to understand the dangers of using regression tools for some specific cases that fall on these regions, tha·πØ is, they improve the accountability of models. The performance of several models from different problems was studied, showing that our proposal allows the analysis of many situations and direct model comparison. In order to facilitate the examination of rules, two visualization tools based on boxplots and density plots were implemented. A network visualization tool is also provided to rapidly check interactions of every feature condition. An additional tool is provided by using a grid of boxplots, where comparison between quartiles of every distribution with a reference is performed. Based on this comparison, an extrapolation of counterfactual examples to regression was also implemented. A set of examples is described, including a setting where regression models performance is compared in detail using EDRs. Specifically, the error difference between two models in a dataset is studied by deriving rules highlighting regions of the input space where model performance difference is unexpected. The application of visual tools is illustrated using EDRs examples derived from public available datasets. Also, case studies illustrating the specialization of subgroups, identification of counter factual subgroups and detecting unanticipated complex models are presented. This paper extends the state of the art by providing a method to derive explanations for model performance instead of explanations for model predictions.},
  archive      = {J_EXSY},
  author       = {Jo√£o Pimentel and Paulo J. Azevedo and Lu√≠s Torgo},
  doi          = {10.1111/exsy.13118},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13118},
  shortjournal = {Expert Syst.},
  title        = {Subgroup mining for performance analysis of regression models},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel distance and entropy definitions for linear
diophantine fuzzy sets and an extension of TOPSIS (LDF-TOPSIS).
<em>EXSY</em>, <em>40</em>(1), e13104. (<a
href="https://doi.org/10.1111/exsy.13104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature of multiple attribute decision making (MADM) is fruitful since there are various and successful applications of different fuzzy set extensions such as intuitionistic, Pythagorean and q-Rung orthopair fuzzy sets (IFS, PFS and q -ROFS). Besides their powerful aspects, some definitional limitations are known. In order to eradicate these boundaries regarding the definitions of membership and non-membership degrees, linear Diophantine fuzzy set (LDFS) concept has been recently emerged. By considering two parameters, LDFS extends the representation area of the previous fuzzy set definitions and provides more extensive human judgement coverage field. In this study, the first distance and entropy measures in the literature have been developed for LDFSs. Their axiomatic definitions are given, and the proofs are shown. Also, thanks to our extensive literature review, we became aware that there is no MADM extension dedicatedly proposed for LDFS. So, the first MADM method extension for LDFS environment has also been developed in this study. A very well-known MADM approach, TOPSIS, has been extended into LDFS environment for the first time in the literature. The applicability is shown in a healthcare management decision problem and the validity is checked and approved by comparing the alternative rankings LDF-TOPSIS and the aggregation operators that were obtained from the literature produced.},
  archive      = {J_EXSY},
  author       = {Sait G√ºl and Ali Aydoƒüdu},
  doi          = {10.1111/exsy.13104},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13104},
  shortjournal = {Expert Syst.},
  title        = {Novel distance and entropy definitions for linear diophantine fuzzy sets and an extension of TOPSIS (LDF-TOPSIS)},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tampering detection and localization base on sample guidance
and individual camera device convolutional neural network features.
<em>EXSY</em>, <em>40</em>(1), e13102. (<a
href="https://doi.org/10.1111/exsy.13102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an image tampering detection algorithm based on sample guidance and individual camera device&#39;s convolutional neural network (CNN) features (SGICD-CF) to address the challenges in the authenticity and integrity of images. Due to the development of the digital image processing technology, which makes image editing and processing, image tampering and forgery easy and lot simplified, thus solving the problem of image tamper detection, to maintain information security. The principle of SGICD-CF assumes that pixels of the pristine image come from a single camera device, but on the contrary, if an image to be tested is spliced by multiple images from different cameras, then the pixels from the multiple camera devices will be detected. SGICD-CF divides the image to be tested into 64‚Äâ√ó‚Äâ64 pixel image patches, extracts the camera-related features and some camera model-related information of image patches by source camera identification network (SCI-Net) which is proposed by us, and obtains the classification confidence degree of the image patch. Furthermore, it determines whether the image patch contains foreign pixels according to the obtained confidence degree and finally determines whether the image was tampered according to the classification results of all the image patches, thus locating the tampered area. However, the experimental results show that SGICD-CF can detect and locate the tampered area of an image accurately and our methods have a better performance than other existing methods. Our algorithm can achieve an average correct rate of 0.855 on the synthetic data set based on Dresden, which is higher than other existing detection methods.},
  archive      = {J_EXSY},
  author       = {Changhui You and Hong Zheng and Zhongyuan Guo and Tianyu Wang and Xiongbin Wu},
  doi          = {10.1111/exsy.13102},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13102},
  shortjournal = {Expert Syst.},
  title        = {Tampering detection and localization base on sample guidance and individual camera device convolutional neural network features},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A regularization-based deep unsupervised model for affine
multimodal co-registration. <em>EXSY</em>, <em>40</em>(1), e13101. (<a
href="https://doi.org/10.1111/exsy.13101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive network receptive field is key for unsupervised affine registration because instead of deformable registration that takes care of local subtleties, the affine registration is global so that the last layers need to see big patches of the organ-in-interest. To extend the network&#39;s receptive field, we need to go for deeper networks, which causes producing complex models. On the other hand, affine transformation is restricted by its low degree-of-freedom (DoF) where larger models increasingly develop the hazard of overfitting. To worsen the situation, the regularizer module cannot be applied to the affine transformation with such a restricted DoF. In this paper, we propose a differentiable computational layer to convert the affine transformation outputted by the network to its corresponding dense displacement field. Such an affine-to-field layer enables us to apply different regularization terms on the outputted transformation in order to avoid the overfitting phenomenon while deepening the network. The proposed approach was evaluated on an annotated hard multimodal dataset containing 1109 pairs of CT/MR images of the brain with different heterogeneity for example, variety in scanners, setups and resolutions. Based on the results, the proposed customized layer is fully successful to handle the overfitting for deeper networks that are able to produce richer transformations than the shallower networks from different evaluation metrics for example, in target registration error the proposed network with seven layers has a 13.3% (or 9.1‚Äâmm) improvement in performance. The implementation of the proposed customized affine-to-field layer in the Python, Keras package with the Tensorflow backend can be publically accessed via https://github.com/boveiri/Deep-coReg .},
  archive      = {J_EXSY},
  author       = {Hamid Reza Boveiri and Raouf Khayami and Reza Javidan and Alireza Mehdizadeh and Samaneh Abbasi},
  doi          = {10.1111/exsy.13101},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13101},
  shortjournal = {Expert Syst.},
  title        = {A regularization-based deep unsupervised model for affine multimodal co-registration},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of COVID-19 from chest x-ray images: Boosting the
performance with convolutional neural network and transfer learning.
<em>EXSY</em>, <em>40</em>(1), e13099. (<a
href="https://doi.org/10.1111/exsy.13099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus disease (COVID-19) is a pandemic that has caused thousands of casualties and impacts all over the world. Most countries are facing a shortage of COVID-19 test kits in hospitals due to the daily increase in the number of cases. Early detection of COVID-19 can protect people from severe infection. Unfortunately, COVID-19 can be misdiagnosed as pneumonia or other illness and can lead to patient death. Therefore, in order to avoid the spread of COVID-19 among the population, it is necessary to implement an automated early diagnostic system as a rapid alternative diagnostic system. Several researchers have done very well in detecting COVID-19; however, most of them have lower accuracy and overfitting issues that make early screening of COVID-19 difficult. Transfer learning is the most successful technique to solve this problem with higher accuracy. In this paper, we studied the feasibility of applying transfer learning and added our own classifier to automatically classify COVID-19 because transfer learning is very suitable for medical imaging due to the limited availability of data. In this work, we proposed a CNN model based on deep transfer learning technique using six different pre-trained architectures, including VGG16, DenseNet201, MobileNetV2, ResNet50, Xception, and EfficientNetB0. A total of 3886 chest X-rays (1200 cases of COVID-19, 1341 healthy and 1345 cases of viral pneumonia) were used to study the effectiveness of the proposed CNN model. A comparative analysis of the proposed CNN models using three classes of chest X-ray datasets was carried out in order to find the most suitable model. Experimental results show that the proposed CNN model based on VGG16 was able to accurately diagnose COVID-19 patients with 97.84% accuracy, 97.90% precision, 97.89% sensitivity, and 97.89% of F 1-score. Evaluation of the test data shows that the proposed model produces the highest accuracy among CNNs and seems to be the most suitable choice for COVID-19 classification. We believe that in this pandemic situation, this model will support healthcare professionals in improving patient screening.},
  archive      = {J_EXSY},
  author       = {Sohaib Asif and Yi Wenhui and Kamran Amjad and Hou Jin and Yi Tao and Si Jinhai},
  doi          = {10.1111/exsy.13099},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13099},
  shortjournal = {Expert Syst.},
  title        = {Detection of COVID-19 from chest X-ray images: Boosting the performance with convolutional neural network and transfer learning},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A compendium and evaluation of taxonomy quality attributes.
<em>EXSY</em>, <em>40</em>(1), e13098. (<a
href="https://doi.org/10.1111/exsy.13098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_EXSY},
  author       = {Michael Unterkalmsteiner and Waleed Adbeen},
  doi          = {10.1111/exsy.13098},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13098},
  shortjournal = {Expert Syst.},
  title        = {A compendium and evaluation of taxonomy quality attributes},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective deep learning based multimodal sentiment analysis
from unstructured big data. <em>EXSY</em>, <em>40</em>(1), e13096. (<a
href="https://doi.org/10.1111/exsy.13096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {More recently, as images, memes and graphics interchange formats have dominated social feeds, typographic/infographic visual content has emerged as an important social media component. This multimodal text combines text and image, defining a novel visual language that must be analysed because it has the potential to modify, confirm or grade the sentiment&#39;s polarity. The problem is how to effectively use information from the visual and textual content in image-text posts. This article presents a new deep learning-based multimodal sentiment analysis (MSA) model using multimodal data such as images, text and multimodal text (image with embedded text). The text analytic unit, the discretization control unit, the picture analytic component and the decision-making component are all included in this system. The discretization unit separates the text from the picture using the variant and channel augmented maximally stable extremal regions (VCA-MSERs) technique, which are then analysed as discrete elements and fed into the appropriate image and text analytics units. The text analytics system utilizes a stacked recurrent neural network with multilevel attention and feedback module (SRNN-MAFM) to detect the sentiment of the text. A deep convolutional neural network (CNN) structure with parallel-dilated convolution and self-attention module (PDC-SAM) is developed to forecast the emotional response to visual content. Finally, the decision component employs a Boolean framework including an OR function to evaluate and classify the output into three fine-grained sentiment classes: positive, neutral and negative. The proposed work is simulated in the python platform using the STS-Gold, Flickr 8k and B-T4SA datasets for sentiment analysis of text and visual and multimodal text. Simulation outcomes proved that the suggested method achieved better accuracy of 97.8%, 97.7% and 90% for text, visual and MSA individually compared to other methods.},
  archive      = {J_EXSY},
  author       = {Swasthika Jain Thandaga Jwalanaiah and Israel Jeena Jacob and Ajay Kumar Mandava},
  doi          = {10.1111/exsy.13096},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13096},
  shortjournal = {Expert Syst.},
  title        = {Effective deep learning based multimodal sentiment analysis from unstructured big data},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical analysis based reactive power optimization using
improved differential evolutionary algorithm. <em>EXSY</em>,
<em>40</em>(1), e13091. (<a
href="https://doi.org/10.1111/exsy.13091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel improved differential evolutionary (IDE) algorithm for optimizing reactive power management (RPM) problems. The effectiveness of IDE algorithm is tested on different unimodal and multimodal benchmark functions. The objective function of the RPM is considered as the minimization of active power losses. Initially, the power flow analysis approach is employed to detect the optimal position of flexible AC transmission system (FACTS) devices. The proposed method is used to determine the optimal value of control variables such as generator&#39;s reactive power generation, transformer tap settings, and reactive power sources. Furthermore, the efficacy of the IDE approach is compared with other promising optimization methods such as variants of differential evolution algorithm, moth flame optimization (MFO), brainstorm-based optimization algorithm (BSOA), and particle swarm optimization (PSO) on various IEEE standard test bus (i.e., IEEE-30, -57, -118, and -300) systems with active and reactive loading incorporating FACTS devices. A Static VAR compensator (SVC) for shunt compensation and a thyristor-controlled series compensator (TCSC) for series compensation were used as FACTS devices. The proposed IDE method significantly reduces the active power loss, that is, 55.65% in IEEE 30, 39.68% in IEEE 57, 16.32% in IEEE 118, and 8.56% in IEEE 300 bus system at nominal loading. Finally, the statistical analysis such as Wilcoxon signed-rank test (WSRT) and ANOVA test were thoroughly analysed to demonstrate the firmness and accuracy of the proposed technique.},
  archive      = {J_EXSY},
  author       = {Lalit Kumar and Manoj Kumar Kar and Sanjay Kumar},
  doi          = {10.1111/exsy.13091},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13091},
  shortjournal = {Expert Syst.},
  title        = {Statistical analysis based reactive power optimization using improved differential evolutionary algorithm},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Radial eco-efficiency in the presence of weakly disposable
undesirable outputs: Evaluating agricultural sectors. <em>EXSY</em>,
<em>40</em>(1), e13089. (<a
href="https://doi.org/10.1111/exsy.13089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data envelopment analysis (DEA) is a popular technique for evaluating relative efficiency. In many real-world applications, undesirable outputs must be considered, and evaluating eco-efficiency instead of operational efficiency is demanded. Unfortunately, eco-efficiency models in DEA are problematic, especially in variable returns-to-scale (VRS) and weak disposability conditions. In this paper, the shortcomings of the available models are discussed and a new method is proposed to provide a valid eco-efficiency measurement. The proposed model is capable of assessing radial efficiency which is not straightforward in the presence of undesirable outputs. Weak disposability condition plays an important role in modelling undesirable outputs. The proposed model also offers a new formulation for weak disposability in VRS technology. Compared to conventional DEA models with a large number of decision making units evaluated as efficient, the performance scores in the proposed model are much more dispersed, leading to a deeper assessment. None of the operational and environmental efficiencies is overlooked by the new method. The proposed model is used to evaluate the eco-efficiency of the agriculture sector in 62 countries. The results confirm that poverty and extreme weather have negative impacts on environmental efficiency.},
  archive      = {J_EXSY},
  author       = {Mohammad Afzalinejad},
  doi          = {10.1111/exsy.13089},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13089},
  shortjournal = {Expert Syst.},
  title        = {Radial eco-efficiency in the presence of weakly disposable undesirable outputs: Evaluating agricultural sectors},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modified spider monkey optimization algorithm based feature
selection and probabilistic neural network classifier in face
recognition. <em>EXSY</em>, <em>40</em>(1), e13088. (<a
href="https://doi.org/10.1111/exsy.13088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel and robust predictive method using modified spider monkey optimization (MSMO) and probabilistic neural network (PNN) for face recognition. The limitation of the traditional spider monkey optimization (SMO) approach to obtaining an optimal solution for classification problems is overcome by enhancing the performance of SMO by modifying the perturbation rate with a non-linear function, thereby improving the convergence of SMO. The framework comprises image preprocessing, feature extraction using dual tree complex wavelet transform (DT-CWT), feature selection using the modified spider monkey optimization algorithm (MSMO), and classification using PNN. The proposed method is tested on the Yale and AR Face datasets. Experimental outcomes reveal that the proposed framework attain an accuracy of 99.4% with appreciable sensitivity, specificity, and G-mean. To examine the efficacy of MSMO, parametric studies are conducted, which showed that MSMO converges faster with high fitness when compared to similar evolutionary algorithms like Genetic Algorithm (GA), Grey Wolf Optimization Algorithm (GWO), Particle Swarm Algorithm (PSO), and Cuckoo Search (CS) in selecting the optimal feature set. The MSMO-PNN method outperforms similar state-of-the-art methods, which reveals that the method proposed is competitive. The proposed model is robust to Gaussian and salt‚Äìpepper noise, obtaining the highest accuracy of 97.89% for varied noise density and variance.},
  archive      = {J_EXSY},
  author       = {Kishore Balasubramanian and Ananthamoorthy Nalligoundenpalayam Periyasamy and Ramya Kishore},
  doi          = {10.1111/exsy.13088},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13088},
  shortjournal = {Expert Syst.},
  title        = {Modified spider monkey optimization algorithm based feature selection and probabilistic neural network classifier in face recognition},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expectile regression forest: A new nonparametric expectile
regression model. <em>EXSY</em>, <em>40</em>(1), e13087. (<a
href="https://doi.org/10.1111/exsy.13087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical nonlinear expectile regression has two shortcomings. It is difficult to choose a nonlinear function, and it does not consider the interaction effects among explanatory variables. Therefore, we combine the random forest model with the expectile regression method to propose a new nonparametric expectile regression model: expectile regression forest (ERF). The major novelty of the ERF model is using the bagging method to build multiple decision trees, calculating the conditional expectile of each leaf node in each decision tree, and deriving final results through aggregating these decision tree results via simple average approach. At the same time, in order to compensate for the black box problem in the model interpretation of the ERF model, the measurement of the importance of explanatory variable and the partial dependence is defined to evaluate the magnitude and direction of the influence of each explanatory variable on the response variable. The advantage of ERF model is illustrated by Monte Carlo simulation studies. The numerical simulation results show that the estimation and prediction ability of the ERF model is significantly better than alternative approaches. We also apply the ERF model to analyse the real data. From the nonparametric expectile regression analysis of these data sets, we have several conclusions that are consistent with the results of numerical simulation.},
  archive      = {J_EXSY},
  author       = {Chao Cai and Haotian Dong and Xinyi Wang},
  doi          = {10.1111/exsy.13087},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e13087},
  shortjournal = {Expert Syst.},
  title        = {Expectile regression forest: A new nonparametric expectile regression model},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring knowledge benchmarking using time-series
directional distance functions and bibliometrics. <em>EXSY</em>,
<em>40</em>(1), e12967. (<a
href="https://doi.org/10.1111/exsy.12967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For strategic reasons, benchmarking best practices from efficient competitors is not usual in many data envelopment analysis (DEA) applications. Even for industries composed of multiple branches, providing information about efficient practices for their peers can jeopardize results for those branches if they compete for market, resources or recognition by the central administration. In this work, a time-series adaptation for the DEA directional model is proposed as an alternative for coping with this problem. The methodological approach has three stages for this benchmarking to occur: Data, Information and Knowledge Extraction. In the first stage, we compare the same unit in different moments to identify efficient periods instead of efficient competitors. As a result, successful performance strategies are investigated using the bibliometric coupling of employees&#39; relevant statements in the second and third stages. The application in a branch of the Brazilian Federal Savings Bank allowed an internal benchmarking of efficient periods when specific performance incentives, innovative processes, competitive strategies, and human resource changes were adopted for improving the unit&#39;s performance.},
  archive      = {J_EXSY},
  author       = {Thyago Celso C. Nepomuceno and Victor Diogho H. de Carvalho and K√©ssia Thais C. Nepomuceno and Ana Paula C. S. Costa},
  doi          = {10.1111/exsy.12967},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12967},
  shortjournal = {Expert Syst.},
  title        = {Exploring knowledge benchmarking using time-series directional distance functions and bibliometrics},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel orca cultural algorithm and applications.
<em>EXSY</em>, <em>40</em>(1), e12928. (<a
href="https://doi.org/10.1111/exsy.12928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the paradigm of machine culture as an extension to machine intelligence is introduced. This new concept is modelled based on animal intelligence and culture. The example of orca intelligence and culture is considered as orcas possess in addition to skills allowing them to reach preys, the ability to transmit their culture from generation to generation. The orca intelligence is studied and then simulated to design an algorithm called Orca Algorithm (OA). OA consists in modelling the orca lifestyle and in particular the orcas social organization, echolocation behaviour and hunting techniques. In order to integrate the cultural dimension, OA was hybridized with the Cultural Algorithm (CA) to get an algorithm called Orca Cultural Algorithm (OCA). OCA was tested on 22 benchmark problems of the literature to evaluate its performance. Extensive experiments were first performed to set the algorithm parameters prior to measure its effectiveness and efficiency. In a second stage, OCA was adapted to discrete problems and applied to the maze game with four level of complexity. Additional experiments were held to compare the designed algorithm with recent state-of-the-art evolutionary algorithms. The overall obtained results are very promising.},
  archive      = {J_EXSY},
  author       = {Habiba Drias and Yassine Drias and Ilyes Khennak},
  doi          = {10.1111/exsy.12928},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12928},
  shortjournal = {Expert Syst.},
  title        = {A novel orca cultural algorithm and applications},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using meta-learning to predict performance metrics in
machine learning problems. <em>EXSY</em>, <em>40</em>(1), e12900. (<a
href="https://doi.org/10.1111/exsy.12900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has been facing significant challenges over the last years, much of which stem from the new characteristics of machine learning problems, such as learning from streaming data or incorporating human feedback into existing datasets and models. In these dynamic scenarios, data change over time and models must adapt. However, new data do not necessarily mean new patterns. The main goal of this paper is to devise a method to predict a model&#39;s performance metrics before it is trained, in order to decide whether it is worth it to train it or not. That is, will the model hold significantly better results than the current one? To address this issue, we propose the use of meta-learning. Specifically, we evaluate two different meta-models, one built for a specific machine learning problem, and another built based on many different problems, meant to be a generic meta-model, applicable to virtually any problem. In this paper, we focus only on the prediction of the root mean square error (RMSE). Results show that it is possible to accurately predict the RMSE of future models, event in streaming scenarios. Moreover, results also show that it is possible to reduce the need for re-training models between 60% and 98%, depending on the problem and on the threshold used.},
  archive      = {J_EXSY},
  author       = {Davide Carneiro and Miguel Guimar√£es and Mariana Carvalho and Paulo Novais},
  doi          = {10.1111/exsy.12900},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12900},
  shortjournal = {Expert Syst.},
  title        = {Using meta-learning to predict performance metrics in machine learning problems},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proof-of-concept of an information visualization
classification approach based on their fine-grained features.
<em>EXSY</em>, <em>40</em>(1), e12872. (<a
href="https://doi.org/10.1111/exsy.12872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The misinformation problem affects the development of the society. Misleading content and unreliable information overwhelm social networks and media. In this context, the use of data visualizations to support news and stories is increasing. The use of misleading visualizations both intentionally or accidentally influence in the audience perceptions, which usually are not visualization and domain experts. Several factors influence o accurately tag a visualization as confusing or misleading. In this paper, we present a machine learning approach to detect if an information visualization can be potentially confusing and misunderstood based on the analytic task it tries to support. This approach is supported by fine-grained features identified through domain engineering and meta modelling on the information visualization and dashboards domain. We automatically generated visualizations from a tri-variate dataset through the software product line paradigm and manually labelled them to obtain a training dataset. The results support the viability of the proposal as a tool to support journalists, audience and society in general, not only to detect confusing visualizations, but also to select the visualization that supports a previous defined task according to the data domain.},
  archive      = {J_EXSY},
  author       = {Andrea V√°zquez-Ingelmo and Alicia Garc√≠a-Holgado and Francisco Jos√© Garc√≠a-Pe√±alvo and Roberto Ther√≥n},
  doi          = {10.1111/exsy.12872},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12872},
  shortjournal = {Expert Syst.},
  title        = {Proof-of-concept of an information visualization classification approach based on their fine-grained features},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big data analytics on patents for innovation public
policies. <em>EXSY</em>, <em>40</em>(1), e12673. (<a
href="https://doi.org/10.1111/exsy.12673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study seeks to answer the following research question: ‚ÄúWhat factors can explain the number of patent filing requests made by residents in Brazil at patent offices in Brazil, the United States, Europe, and triadic patent families?‚Äù. The methods used in this research are quantitative, using big data from private and public investments in Science and Technology, and about patent deposit numbers in Brazil from 2000 to 2017. A model of linear regression was performed and explains how these investments in Science and Technology influence patent deposit numbers. The results of this research study point towards the importance of universities, up and beyond the traditional training and education aspect of university activity. The importance of public and private innovation investments is also shown to be important. This study shows that the patent registrations in the different regions under analysis are affected by different factors. There is thus no single formula towards the creation of innovation output and governments would do well to continue to invest in higher education while also investing in public research and development activities. Additionally, and not least important, private entities should be continually encouraged to make innovation investments and favourable government policies need to thus exist for this to happen. Finally, the low numbers regarding patent filings in Brazil may be linked to institutional deficiencies in the country. Patent breaches may be difficult to punish, and the judicial system may be slow and untrustworthy, compared to the United States and to Europe‚Äîleading to diminished patent registrations in Brazil. A set of implications and recommendations for policy derived from this study and will be strategic for policymakers.},
  archive      = {J_EXSY},
  author       = {Maria Jos√© Sousa and George Jamil and Cicero Eduardo Walter and Manuel Au-Yong-Oliveira and Fernando Moreira},
  doi          = {10.1111/exsy.12673},
  journal      = {Expert Systems},
  month        = {1},
  number       = {1},
  pages        = {e12673},
  shortjournal = {Expert Syst.},
  title        = {Big data analytics on patents for innovation public policies},
  volume       = {40},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
