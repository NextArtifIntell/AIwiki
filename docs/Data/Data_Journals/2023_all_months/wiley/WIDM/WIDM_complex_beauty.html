<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WIDM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="widm---30">WIDM - 30</h2>
<ul>
<li><details>
<summary>
(2023). A review on client selection models in federated learning.
<em>WIDM</em>, <em>13</em>(6), e1514. (<a
href="https://doi.org/10.1002/widm.1514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a decentralized machine learning (ML) technique that enables multiple clients to collaboratively train a common ML model without them having to share their raw data with each other. A typical FL process involves (1) FL client(s) selection, (2) global model distribution, (3) local training, and (4) aggregation. As such FL clients are heterogeneous edge devices (i.e., mobile phones) that differ in terms of computational resources, training data quality, and distribution. Therefore, FL client(s) selection has a significant influence on the execution of the remaining steps of an FL process. There have been a variety of FL client(s) selection models proposed in the literature, however, their critical review and/or comparative analysis is much less discussed. This paper brings the scattered FL client(s) selection models onto a single platform by first categorizing them into five categories, followed by providing a detailed analysis of the benefits/shortcomings and the applicability of these models for different FL scenarios. Such understanding can help researchers in academia and industry to develop improved FL client(s) selection models to address the requirement challenges and shortcomings of the current models. Finally, future research directions in the area of FL client(s) selection are also discussed. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Monalisa Panigrahi and Sourabh Bharti and Arun Sharma},
  doi          = {10.1002/widm.1514},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1514},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review on client selection models in federated learning},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comprehensive survey of personal knowledge graphs.
<em>WIDM</em>, <em>13</em>(6), e1513. (<a
href="https://doi.org/10.1002/widm.1513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information that can encapsulate a person&#39;s daily life and its different aspects provides insightful knowledge. This knowledge can prove to be more useful than general knowledge for improving personalized tasks. When it comes to storing such knowledge, personal knowledge graphs (PKGs) come in as handy saviors. PKGs are knowledge graphs which store details that are pertinent to a user but not, in general, useful to the rest of humanity. Conversational agents can access these PKGs to answer queries related to the user&#39;s day-to-day life, whereas recommender systems can harness the knowledge stored in PKGs to make personalized suggestions. Despite the immense applicability of PKGs, there has not been significant research in this area. We present an extensive review of PKGs. We categorize them according to the domains in which they are most relevant; in particular, we highlight the use of PKGs in medicine, finance, and education and research. We also categorize the different ways of constructing a PKG based on the source of data required for such constructions. Furthermore, we discuss the limitations of PKGs and suggest directions for future work. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Prantika Chakraborty and Debarshi Kumar Sanyal},
  doi          = {10.1002/widm.1513},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1513},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A comprehensive survey of personal knowledge graphs},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Filter bubbles in recommender systems: Fact or fallacy—a
systematic review. <em>WIDM</em>, <em>13</em>(6), e1512. (<a
href="https://doi.org/10.1002/widm.1512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A filter bubble refers to the phenomenon where Internet customization effectively isolates individuals from diverse opinions or materials, resulting in their exposure to only a select set of content. This can lead to the reinforcement of existing attitudes, beliefs, or conditions. In this study, our primary focus is to investigate the impact of filter bubbles in recommender systems (RSs). This pioneering research aims to uncover the reasons behind this problem, explore potential solutions, and propose an integrated tool to help users avoid filter bubbles in RSs. To achieve this objective, we conduct a systematic literature review on the topic of filter bubbles in RSs. The reviewed articles are carefully analyzed and classified, providing valuable insights that inform the development of an integrated approach. Notably, our review reveals evidence of filter bubbles in RSs, highlighting several biases that contribute to their existence. Moreover, we propose mechanisms to mitigate the impact of filter bubbles and demonstrate that incorporating diversity into recommendations can potentially help alleviate this issue. The findings of this timely review will serve as a benchmark for researchers working in interdisciplinary fields such as privacy, artificial intelligence ethics, and RSs. Furthermore, it will open new avenues for future research in related domains, prompting further exploration and advancement in this critical area. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Qazi Mohammad Areeb and Mohammad Nadeem and Shahab Saquib Sohail and Raza Imam and Faiyaz Doctor and Yassine Himeur and Amir Hussain and Abbes Amira},
  doi          = {10.1002/widm.1512},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1512},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Filter bubbles in recommender systems: Fact or fallacy—A systematic review},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A white paper on good research practices in benchmarking:
The case of cluster analysis. <em>WIDM</em>, <em>13</em>(6), e1511. (<a
href="https://doi.org/10.1002/widm.1511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve scientific progress in terms of building a cumulative body of knowledge, careful attention to benchmarking is of the utmost importance, requiring that proposals of new methods are extensively and carefully compared with their best predecessors, and existing methods subjected to neutral comparison studies. Answers to benchmarking questions should be evidence-based, with the relevant evidence being collected through well-thought-out procedures, in reproducible and replicable ways. In the present paper, we review good research practices in benchmarking from the perspective of the area of cluster analysis. Discussion is given to the theoretical, conceptual underpinnings of benchmarking based on simulated and empirical data in this context. Subsequently, the practicalities of how to address benchmarking questions in clustering are dealt with, and foundational recommendations are made based on existing literature. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Iven Van Mechelen and Anne-Laure Boulesteix and Rainer Dangl and Nema Dean and Christian Hennig and Friedrich Leisch and Douglas Steinley and Matthijs J. Warrens},
  doi          = {10.1002/widm.1511},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1511},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A white paper on good research practices in benchmarking: The case of cluster analysis},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on artificial intelligence in pulmonary imaging.
<em>WIDM</em>, <em>13</em>(6), e1510. (<a
href="https://doi.org/10.1002/widm.1510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, deep learning (DL) has contributed to a paradigm shift in computer vision and image recognition creating widespread opportunities of using artificial intelligence in research as well as industrial applications. DL has been extensively studied in medical imaging applications, including those related to pulmonary diseases. Chronic obstructive pulmonary disease, asthma, lung cancer, pneumonia, and, more recently, COVID-19 are common lung diseases affecting nearly 7.4% of world population. Pulmonary imaging has been widely investigated toward improving our understanding of disease etiologies and early diagnosis and assessment of disease progression and clinical outcomes. DL has been broadly applied to solve various pulmonary image processing challenges including classification, recognition, registration, and segmentation. This article presents a survey of pulmonary diseases, roles of imaging in translational and clinical pulmonary research, and applications of different DL architectures and methods in pulmonary imaging with emphasis on DL-based segmentation of major pulmonary anatomies such as lung volumes, lung lobes, pulmonary vessels, and airways as well as thoracic musculoskeletal anatomies related to pulmonary diseases. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Punam K. Saha and Syed Ahmed Nadeem and Alejandro P. Comellas},
  doi          = {10.1002/widm.1510},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {11},
  number       = {6},
  pages        = {e1510},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey on artificial intelligence in pulmonary imaging},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sentiment analysis using fuzzy logic: A comprehensive
literature review. <em>WIDM</em>, <em>13</em>(5), e1509. (<a
href="https://doi.org/10.1002/widm.1509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and comprehending humans&#39; views, beliefs, attitudes, or opinions toward a particular entity is sentiment analysis (SA). Advancements in e-commerce platforms has led to an abundance of the real-time and free forms of opinions floating on social media platforms. This real-world data are imprecise and vague hence fuzzy logic is required to deal with such subjective data. Since opinions can be fuzzy in nature and definitions of opinion words can be elucidated differently; fuzzy logic has witnessed itself as an effective method to capture the expression of opinions. The study presents an elaborate review of the around 170 published research works for SA using fuzzy logic. The primary emphasis is focused on text-based SA, audio-based SA, and fusion of text-audio features-based SA. This article discusses the various novel ways of classifying fuzzy logic-based SA research articles, which have not been accomplished by any other review article till date. The article puts forward the importance of SA tasks and identifies how fuzzy logic adds to this importance. Finally, the article outlines a taxonomy for sentiment classification based on the technique-supervised and unsupervised in the SA models and comprehensively reviews the SA approaches specific to their task. Prominently, this study highlights the suitability of fuzzy-based SA approaches into five different classes vis-a-vis (a) Sentiment Cognition from Words using fuzzy logic, (b) Sentiment Cognition from Phrases using fuzzy logic, (c) Fuzzy-rule based SA, (d) Neuro-fuzzy network-based SA, and (e) Fuzzy Emotion Recognition. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Srishti Vashishtha and Vedika Gupta and Mamta Mittal},
  doi          = {10.1002/widm.1509},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1509},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Sentiment analysis using fuzzy logic: A comprehensive literature review},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on mining software repositories to facilitate
refactoring. <em>WIDM</em>, <em>13</em>(5), e1508. (<a
href="https://doi.org/10.1002/widm.1508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software refactoring focuses on improving software quality by applying changes to the internal structure that do not alter the observable behavior. Determining which refactorings should be applied and presented to developers the most relevant and optimal refactorings is often challenging. Existing literature suggests that one of the potential sources to identify and recommend required refactorings is the past software development and evolution histories which are often archived in software repositories. In this article, we review a selection of existing literature that has attempted to propose approaches that facilitate refactoring by exploiting information mined from software repositories. Based on the reviewed papers, existing works leverage software history mining to support analysis of code smells, refactoring, and guiding software changes. First, past history information is used to detect design flaws in source code commonly referred to as code smells. Moreover, other studies analyze the evolution of code smells to establish how and when they are introduced into the code base and get resolved. Second, software repositories mining provides useful insights that can be used in predicting the need for refactoring and what specific refactoring operations are required. In addition, past history can be used in detecting and analyzing previously applied refactorings to establish software change facts, for instance, how developers refactor code and the motivation behind it. Finally, change patterns are used to predict further changes that might be required and recommend a set of files for change during a given modification task. The paper further suggests other exciting possibilities that can be pursued in the future in this research direction. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ally S. Nyamawe},
  doi          = {10.1002/widm.1508},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1508},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Research on mining software repositories to facilitate refactoring},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Use of artificial intelligence algorithms to predict
systemic diseases from retinal images. <em>WIDM</em>, <em>13</em>(5),
e1506. (<a href="https://doi.org/10.1002/widm.1506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of non-invasive, rapid, and widely accessible quantitative high-resolution imaging methods, such as modern retinal photography and optical coherence tomography (OCT), has significantly impacted ophthalmology. These techniques offer remarkable accuracy and resolution in assessing ocular diseases and are increasingly recognized for their potential in identifying ocular biomarkers of systemic diseases. The application of artificial intelligence (AI) has been demonstrated to have promising results in identifying age, gender, systolic blood pressure, smoking status, and assessing cardiovascular disorders from the fundus and OCT images. Although our understanding of eye–body relationships has advanced from decades of conventional statistical modeling in large population-based studies incorporating ophthalmic assessments, the application of AI to this field is still in its early stages. In this review article, we concentrate on the areas where AI-based investigations could expand on existing conventional analyses to produce fresh findings using retinal biomarkers of systemic diseases. Five databases—Medline, Scopus, PubMed, Google Scholar, and Web of Science were searched using terms related to ocular imaging, systemic diseases, and artificial intelligence characteristics. Our review found that AI has been employed in a wide range of clinical tests and research applications, primarily for disease prediction, finding biomarkers and risk factor identification. We envisage artificial intelligence-based models to have significant clinical and research impacts in the future through screening for high-risk individuals, particularly in less developed areas, and identifying new retinal biomarkers, even though technical and socioeconomic challenges remain. Further research is needed to validate these models in real-world setting. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Rehana Khan and Janani Surya and Maitreyee Roy and M. N. Swathi Priya and Sashwanthi Mohan and Sundaresan Raman and Akshay Raman and Abhishek Vyas and Rajiv Raman},
  doi          = {10.1002/widm.1506},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1506},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Use of artificial intelligence algorithms to predict systemic diseases from retinal images},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sports analytics review: Artificial intelligence
applications, emerging technologies, and algorithmic perspective.
<em>WIDM</em>, <em>13</em>(5), e1496. (<a
href="https://doi.org/10.1002/widm.1496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid and impromptu interest in the coupling of machine learning (ML) algorithms with wearable and contactless sensors aimed at tackling real-world problems warrants a pedagogical study to understand all the aspects of this research direction. Considering this aspect, this survey aims to review the state-of-the-art literature on ML algorithms, methodologies, and hypotheses adopted to solve the research problems and challenges in the domain of sports. First, we categorize this study into three main research fields: sensors , computer vision , and wireless and mobile-based applications . Then, for each of these fields, we thoroughly analyze the systems that are deployable for real-time sports analytics. Next, we meticulously discuss the learning algorithms (e.g., statistical learning, deep learning, reinforcement learning) that power those deployable systems while also comparing and contrasting the benefits of those learning methodologies. Finally, we highlight the possible future open-research opportunities and emerging technologies that could contribute to the domain of sports analytics. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Indrajeet Ghosh and Sreenivasan Ramasamy Ramamurthy and Avijoy Chakma and Nirmalya Roy},
  doi          = {10.1002/widm.1496},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {9},
  number       = {5},
  pages        = {e1496},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Sports analytics review: Artificial intelligence applications, emerging technologies, and algorithmic perspective},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic review of green AI. <em>WIDM</em>,
<em>13</em>(4), e1507. (<a
href="https://doi.org/10.1002/widm.1507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ever-growing adoption of artificial intelligence (AI)-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this article, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm-agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115%, with savings over 50% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Roberto Verdecchia and June Sallou and Luís Cruz},
  doi          = {10.1002/widm.1507},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1507},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A systematic review of green AI},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The benefits and dangers of using machine learning to
support making legal predictions. <em>WIDM</em>, <em>13</em>(4), e1505.
(<a href="https://doi.org/10.1002/widm.1505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rule-based systems have been used in the legal domain since the 1970s. Save for rare exceptions, machine learning has only recently been used. But why this delay? We investigate the appropriate use of machine learning to support and make legal predictions. To do so, we need to examine the appropriate use of data in global legal domains—including in common law, civil law, and hybrid jurisdictions. The use of various forms of Artificial Intelligence, including rule-based reasoning, case-based reasoning and machine learning in law requires an understanding of jurisprudential theories. We will see that the use of machine learning is particularly appropriate for non-professionals: in particular self-represented litigants or those relying upon legal aid services. The primary use of machine learning to support decision-making in legal domains has been in criminal detection, financial domains, and sentencing. The use in these areas has led to concerns that the inappropriate use of Artificial Intelligence leads to biased decision making. This requires us to examine concerns about governance and ethics. Ethical concerns can be minimized by providing enhanced explanation, choosing appropriate data to be used, appropriately cleaning that data, and having human reviews of any decisions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {John Zeleznikow},
  doi          = {10.1002/widm.1505},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1505},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The benefits and dangers of using machine learning to support making legal predictions},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bias in human data: A feedback from social sciences.
<em>WIDM</em>, <em>13</em>(4), e1498. (<a
href="https://doi.org/10.1002/widm.1498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fairness of human-related software has become critical with its widespread use in our daily lives, where life-changing decisions are made. However, with the use of these systems, many erroneous results emerged. Technologies have started to be developed to tackle unexpected results. As for the solution to the issue, companies generally focus on algorithm-oriented errors. The utilized solutions usually only work in some algorithms. Because the cause of the problem is not just the algorithm; it is also the data itself. For instance, deep learning cannot establish the cause–effect relationship quickly. In addition, the boundaries between statistical or heuristic algorithms are unclear. The algorithm&#39;s fairness may vary depending on the data related to context. From this point of view, our article focuses on how the data should be, which is not a matter of statistics. In this direction, the picture in question has been revealed through a scenario specific to “vulnerable and disadvantaged” groups, which is one of the most fundamental problems today. With the joint contribution of computer science and social sciences, it aims to predict the possible social dangers that may arise from artificial intelligence algorithms using the clues obtained in this study. To highlight the potential social and mass problems caused by data, Gerbner&#39;s “cultivation theory” is reinterpreted. To this end, we conduct an experimental evaluation on popular algorithms and their data sets, such as Word2Vec, GloVe, and ELMO. The article stresses the importance of a holistic approach combining the algorithm, data, and an interdisciplinary assessment. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Savaş Takan and Duygu Ergün and Sinem Getir Yaman and Onur Kılınççeker},
  doi          = {10.1002/widm.1498},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1498},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Bias in human data: A feedback from social sciences},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepFixCX: Explainable privacy-preserving image compression
for medical image analysis. <em>WIDM</em>, <em>13</em>(4), e1495. (<a
href="https://doi.org/10.1002/widm.1495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanations of a model&#39;s biases or predictions are essential to medical image analysis. Yet, explainable machine learning approaches for medical image analysis are challenged by needs to preserve privacy of patient data, and by current trends in deep learning to use unsustainably large models and large datasets. We propose DeepFixCX for explainable and privacy-preserving medical image compression that is nimble and performant. We contribute a review of the field and a conceptual framework for simultaneous privacy and explainability via tools of compression. DeepFixCX compresses images without learning by removing or obscuring spatial and edge information. DeepFixCX is ante-hoc explainable and gives privatized post hoc explanations of spatial and edge bias without accessing the original image. DeepFixCX privatizes images to prevent image reconstruction and mitigate patient re-identification. DeepFixCX is nimble. Compression can occur on a laptop CPU or GPU to compress and privatize 1700 images per second of size 320 × 320. DeepFixCX enables use of low memory MLP classifiers for vision data; permitting small performance loss gives end-to-end MLP performance over 70× faster and batch size over 100× larger. DeepFixCX consistently improves predictive classification performance of a Deep Neural Network (DNN) by 0.02 AUC ROC on Glaucoma and Cervix Type detection datasets, and can improve multi-label chest x-ray classification performance in seven of 10 tested settings. In all three datasets, compression to less than 5% of original number of pixels gives matching or improved performance. Our main novelty is to define an explainability versus privacy problem and address it with lossy compression. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Alex Gaudio and Asim Smailagic and Christos Faloutsos and Shreshta Mohan and Elvin Johnson and Yuhao Liu and Pedro Costa and Aurélio Campilho},
  doi          = {10.1002/widm.1495},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1495},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {DeepFixCX: Explainable privacy-preserving image compression for medical image analysis},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised EHR-based phenotyping via matrix and tensor
decompositions. <em>WIDM</em>, <em>13</em>(4), e1494. (<a
href="https://doi.org/10.1002/widm.1494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational phenotyping allows for unsupervised discovery of subgroups of patients as well as corresponding co-occurring medical conditions from electronic health records (EHR). Typically, EHR data contains demographic information, diagnoses and laboratory results. Discovering (novel) phenotypes has the potential to be of prognostic and therapeutic value. Providing medical practitioners with transparent and interpretable results is an important requirement and an essential part for advancing precision medicine. Low-rank data approximation methods such as matrix (e.g., nonnegative matrix factorization) and tensor decompositions (e.g., CANDECOMP/PARAFAC) have demonstrated that they can provide such transparent and interpretable insights. Recent developments have adapted low-rank data approximation methods by incorporating different constraints and regularizations that facilitate interpretability further. In addition, they offer solutions for common challenges within EHR data such as high dimensionality, data sparsity and incompleteness. Especially extracting temporal phenotypes from longitudinal EHR has received much attention in recent years. In this paper, we provide a comprehensive review of low-rank approximation-based approaches for computational phenotyping. The existing literature is categorized into temporal versus static phenotyping approaches based on matrix versus tensor decompositions. Furthermore, we outline different approaches for the validation of phenotypes, that is, the assessment of clinical significance. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Florian Becker and Age K. Smilde and Evrim Acar},
  doi          = {10.1002/widm.1494},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {7},
  number       = {4},
  pages        = {e1494},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Unsupervised EHR-based phenotyping via matrix and tensor decompositions},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A feature selection for video quality of experience
modeling: A systematic literature review. <em>WIDM</em>, <em>13</em>(3),
e1497. (<a href="https://doi.org/10.1002/widm.1497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality of Experience (QoE) multidimensional concept is the key for successful delivery of multimedia services. Higher user requirements for new experiences such as augmented reality, virtual reality, and future 6G services set higher requirements for QoE. A more complex QoE space requires the use of data mining methods in order to process the data for better QoE prediction. The increased dimensionality of the QoE space becomes a limiting factor for achieving the desired QoE prediction accuracy. Existing studies considering the QoE multidimensional concept with approaches that overcome the challenge of increased QoE space dimensionality are of great importance for future research. Accordingly, this article aims to review the applications of Feature Selection (FS) methods in video QoE modeling. It provides a comprehensive overview of the existing studies with the categorization and review of applied FS methods with reference to the data collection and data modeling steps. The analysis included 71 studies which provides overview of the FS methods applications in video QoE modeling depending on the input Influence Factor (IF) dimension sizes, type of IFs, QoE prediction methods used and QoE evaluation type. Our review revealed the advantages of using FS methods in video QoE modeling, frequency of application of FS methods with potential of applying more FS methods in a series or a parallel, gives an overview of the achieved dimensionality reduction degree for different methods, and provides insights in opportunities for researchers for applying FS methods on complex multidimensional QoE space. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Fatima Skaka - Čekić and Jasmina Baraković Husić},
  doi          = {10.1002/widm.1497},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1497},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A feature selection for video quality of experience modeling: A systematic literature review},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable and explainable machine learning: A
methods-centric overview with concrete examples. <em>WIDM</em>,
<em>13</em>(3), e1493. (<a
href="https://doi.org/10.1002/widm.1493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretability and explainability are crucial for machine learning (ML) and statistical applications in medicine, economics, law, and natural sciences and form an essential principle for ML model design and development. Although interpretability and explainability have escaped a precise and universal definition, many models and techniques motivated by these properties have been developed over the last 30 years, with the focus currently shifting toward deep learning. We will consider concrete examples of state-of-the-art, including specially tailored rule-based, sparse, and additive classification models, interpretable representation learning, and methods for explaining black-box models post hoc. The discussion will emphasize the need for and relevance of interpretability and explainability, the divide between them, and the inductive biases behind the presented “zoo” of interpretable models and explanation methods. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Ričards Marcinkevičs and Julia E. Vogt},
  doi          = {10.1002/widm.1493},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1493},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Interpretable and explainable machine learning: A methods-centric overview with concrete examples},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ethical issues when using digital biomarkers and artificial
intelligence for the early detection of dementia. <em>WIDM</em>,
<em>13</em>(3), e1492. (<a
href="https://doi.org/10.1002/widm.1492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dementia poses a growing challenge for health services but remains stigmatized and under-recognized. Digital technologies to aid the earlier detection of dementia are approaching market. These include traditional cognitive screening tools presented on mobile devices, smartphone native applications, passive data collection from wearable, in-home and in-car sensors, as well as machine learning techniques applied to clinic and imaging data. It has been suggested that earlier detection and diagnosis may help patients plan for their future, achieve a better quality of life, and access clinical trials and possible future disease modifying treatments. In this review, we explore whether digital tools for the early detection of dementia can or should be deployed, by assessing them against the principles of ethical screening programs. We conclude that while the importance of dementia as a health problem is unquestionable, significant challenges remain. There is no available treatment which improves the prognosis of diagnosed disease. Progression from early-stage disease to dementia is neither given nor currently predictable. Available technologies are generally not both minimally invasive and highly accurate. Digital deployment risks exacerbating health inequalities due to biased training data and inequity in digital access. Finally, the acceptability of early dementia detection is not established, and resources would be needed to ensure follow-up and support for those flagged by any new system. We conclude that early dementia detection deployed at scale via digital technologies does not meet standards for a screening program and we offer recommendations for moving toward an ethical mode of implementation. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Elizabeth Ford and Richard Milne and Keegan Curlewis},
  doi          = {10.1002/widm.1492},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1492},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Ethical issues when using digital biomarkers and artificial intelligence for the early detection of dementia},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A geometric framework for outlier detection in
high-dimensional data. <em>WIDM</em>, <em>13</em>(3), e1491. (<a
href="https://doi.org/10.1002/widm.1491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework which exploits the metric structure of a data set. Our approach rests on the manifold assumption , that is, that the observed, nominally high-dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel, mathematically precise and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high-dimensional data, but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high-dimensional and non-tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Moritz Herrmann and Florian Pfisterer and Fabian Scheipl},
  doi          = {10.1002/widm.1491},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1491},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A geometric framework for outlier detection in high-dimensional data},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning based image steganography: A review.
<em>WIDM</em>, <em>13</em>(3), e1481. (<a
href="https://doi.org/10.1002/widm.1481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A review of the deep learning based image steganography techniques is presented in this paper. For completeness, the recent traditional steganography techniques are also discussed briefly. The three key parameters (security, embedding capacity, and invisibility) for measuring the quality of an image steganographic technique are described. Various steganography techniques, with emphasis on the above three key parameters, are reviewed. The steganography techniques are classified here into three main categories: Traditional, Hybrid, and fully Deep Learning. The hybrid techniques are further divided into three sub-categories: Cover Generation, Distortion Learning, and Adversarial Embedding. The fully Deep Learning techniques, based on the nature of the input, are further divided into three sub-categories: GAN Embedding, Embedding Less, and Category Label. The main ideas of the important deep learning based steganography techniques are described. The strong and weak features of these techniques are outlined. The results reported by researchers on benchmark data sets CelebA, Bossbase, PASCAL-VOC12, CIFAR-100, ImageNet, and USC-SIPI are used to evaluate the performance of various steganography techniques. Analysis of the results shows that there is scope for new suitable deep learning architectures that can improve the capacity and invisibility of image steganography. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Mohd Arif Wani and Bisma Sultan},
  doi          = {10.1002/widm.1481},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {5},
  number       = {3},
  pages        = {e1481},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Deep learning based image steganography: A review},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving data mining and machine learning in
healthcare: Applications, challenges, and solutions. <em>WIDM</em>,
<em>13</em>(2), e1490. (<a
href="https://doi.org/10.1002/widm.1490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data mining (DM) and machine learning (ML) applications in medical diagnostic systems are budding. Data privacy is essential in these systems as healthcare data are highly sensitive. The proposed work first discusses various privacy and security challenges in these systems. To address these next, we discuss different privacy-preserving (PP) computation techniques in the context of DM and ML for secure data evaluation and processing. The state-of-the-art applications of these systems in healthcare are analyzed at various stages such as data collection, data publication, data distribution, and output phases regarding PPDM and input, model, training, and output phases in the context of PPML. Furthermore, PP federated learning is also discussed. Finally, we present open challenges in these systems and future research directions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Vankamamidi S. Naresh and Muthusamy Thamarai},
  doi          = {10.1002/widm.1490},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1490},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Privacy-preserving data mining and machine learning in healthcare: Applications, challenges, and solutions},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey of online video advertising. <em>WIDM</em>,
<em>13</em>(2), e1489. (<a
href="https://doi.org/10.1002/widm.1489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of social media and the ubiquity of the Internet, recent years have witnessed the rapid development of online video advertising among publishers and advertisers. Video advertising, as a new type of advertisement, has gained significant research attention from both academia and industry, coinciding with the ever-growing volume of online videos. In this research, we provide a comprehensive survey of online video advertising in the fields of social science and computer science. We investigate state-of-the-art articles from 1990 to the present and provide a new taxonomy of extant research topics based on these articles. We also highlight the factors that cause advertising to affect people and the most popular video advertising techniques used in computer science. Finally, on the basis of the analytics of the surveyed papers, future challenges are identified and potential solutions to these are discussed. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Haijun Zhang and Xiangyu Mu and Han Yan and Lang Ren and Jianghong Ma},
  doi          = {10.1002/widm.1489},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1489},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A survey of online video advertising},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A review on multimodal zero-shot learning. <em>WIDM</em>,
<em>13</em>(2), e1488. (<a
href="https://doi.org/10.1002/widm.1488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning provides a path to fully utilize all types of information related to the modeling target to provide the model with a global vision. Zero-shot learning (ZSL) is a general solution for incorporating prior knowledge into data-driven models and achieving accurate class identification. The combination of the two, known as multimodal ZSL (MZSL), can fully exploit the advantages of both technologies and is expected to produce models with greater generalization ability. However, the MZSL algorithms and applications have not yet been thoroughly investigated and summarized. This study fills this gap by providing an objective overview of MZSL&#39;s definition, typical algorithms, representative applications, and critical issues. This article will not only provide researchers in this field with a comprehensive perspective, but it will also highlight several promising research directions. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Weipeng Cao and Yuhao Wu and Yixuan Sun and Haigang Zhang and Jin Ren and Dujuan Gu and Xingkai Wang},
  doi          = {10.1002/widm.1488},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1488},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {A review on multimodal zero-shot learning},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Review of artificial intelligence-based question-answering
systems in healthcare. <em>WIDM</em>, <em>13</em>(2), e1487. (<a
href="https://doi.org/10.1002/widm.1487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Use of conversational agents, like chatbots, avatars, and robots is increasing worldwide. Yet, their effectiveness in health care is largely unknown. The aim of this advanced review was to assess the use and effectiveness of conversational agents in various fields of health care. A literature search, analysis, and synthesis were conducted in February 2022 in PubMed and CINAHL. The included evidence was analyzed narratively by employing the principles of thematic analysis. We reviewed articles on artificial intelligence-based question-answering systems in health care. Most of the identified articles report its effectiveness; less is known about its use. We outlined study findings and explored directions of future research, to provide evidence-based knowledge about artificial intelligence-based question-answering systems. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Leona Cilar Budler and Lucija Gosak and Gregor Stiglic},
  doi          = {10.1002/widm.1487},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1487},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Review of artificial intelligence-based question-answering systems in healthcare},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards federated learning: An overview of methods and
applications. <em>WIDM</em>, <em>13</em>(2), e1486. (<a
href="https://doi.org/10.1002/widm.1486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a collaborative, decentralized privacy-preserving method to attach the challenges of storing data and data privacy. Artificial intelligence, machine learning, smart devices, and deep learning have strongly marked the last years. Two challenges arose in data science as a result. First, the regulation protected the data by creating the General Data Protection Regulation, in which organizations are not allowed to keep or transfer data without the owner&#39;s authorization. Another challenge is the large volume of data generated in the era of big data, and keeping that data in one only server becomes increasingly tricky. Therefore, the data is allocated into different locations or generated by devices, creating the need to build models or perform calculations without transferring data to a single location. The new term FL emerged as a sub-area of machine learning that aims to solve the challenge of making distributed models with privacy considerations. This survey starts by describing relevant concepts, definitions, and methods, followed by an in-depth investigation of federated model evaluation. Finally, we discuss three promising applications for further research: anomaly detection, distributed data streams, and graph representation. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Paula Raissa Silva and João Vinagre and João Gama},
  doi          = {10.1002/widm.1486},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1486},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Towards federated learning: An overview of methods and applications},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Remote patient monitoring using artificial intelligence:
Current state, applications, and challenges. <em>WIDM</em>,
<em>13</em>(2), e1485. (<a
href="https://doi.org/10.1002/widm.1485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of artificial intelligence (AI) in healthcare is growing rapidly. Remote patient monitoring (RPM) is one of the common healthcare applications that assist doctors to monitor patients with chronic or acute illness at remote locations, elderly people in-home care, and even hospitalized patients. The reliability of manual patient monitoring systems depends on staff time management which is dependent on their workload. Conventional patient monitoring involves invasive approaches which require skin contact to monitor health status. This study aims to do a comprehensive review of RPM systems including adopted advanced technologies, AI impact on RPM, challenges and trends in AI-enabled RPM. This review explores the benefits and challenges of patient-centric RPM architectures enabled with Internet of Things wearable devices and sensors using the cloud, fog, edge, and blockchain technologies. The role of AI in RPM ranges from physical activity classification to chronic disease monitoring and vital signs monitoring in emergency settings. This review results show that AI-enabled RPM architectures have transformed healthcare monitoring applications because of their ability to detect early deterioration in patients&#39; health, personalize individual patient health parameter monitoring using federated learning, and learn human behavior patterns using techniques such as reinforcement learning. This review discusses the challenges and trends to adopt AI to RPM systems and implementation issues. The future directions of AI in RPM applications are analyzed based on the challenges and trends. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Thanveer Shaik and Xiaohui Tao and Niall Higgins and Lin Li and Raj Gururajan and Xujuan Zhou and U. Rajendra Acharya},
  doi          = {10.1002/widm.1485},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1485},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Remote patient monitoring using artificial intelligence: Current state, applications, and challenges},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hyperparameter optimization: Foundations, algorithms, best
practices, and open challenges. <em>WIDM</em>, <em>13</em>(2), e1484.
(<a href="https://doi.org/10.1002/widm.1484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most machine learning algorithms are configured by a set of hyperparameters whose values must be carefully chosen and which often considerably impact performance. To avoid a time-consuming and irreproducible manual process of trial-and-error to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods—for example, based on resampling error estimation for supervised machine learning—can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods, from simple techniques such as grid or random search to more advanced methods like evolution strategies, Bayesian optimization, Hyperband, and racing. This work gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with machine learning pipelines, runtime improvements, and parallelization. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Bernd Bischl and Martin Binder and Michel Lang and Tobias Pielok and Jakob Richter and Stefan Coors and Janek Thomas and Theresa Ullmann and Marc Becker and Anne-Laure Boulesteix and Difan Deng and Marius Lindauer},
  doi          = {10.1002/widm.1484},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1484},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ExplainFix: Explainable spatially fixed deep networks.
<em>WIDM</em>, <em>13</em>(2), e1483. (<a
href="https://doi.org/10.1002/widm.1483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Is there an initialization for deep networks that requires no learning? ExplainFix adopts two design principles: the “fixed filters” principle that all spatial filter weights of convolutional neural networks can be fixed at initialization and never learned, and the “nimbleness” principle that only few network parameters suffice. We contribute (a) visual model-based explanations , (b) speed and accuracy gains , and (c) novel tools for deep convolutional neural networks. ExplainFix gives key insights that spatially fixed networks should have a steered initialization, that spatial convolution layers tend to prioritize low frequencies, and that most network parameters are not necessary in spatially fixed models. ExplainFix models have up to × 100 fewer spatial filter kernels than fully learned models and matching or improved accuracy. Our extensive empirical analysis confirms that ExplainFix guarantees nimbler models (train up to 17 % faster with channel pruning), matching or improved predictive performance (spanning 13 distinct baseline models, four architectures and two medical image datasets), improved robustness to larger learning rate, and robustness to varying model size. We are first to demonstrate that all spatial filters in state-of-the-art convolutional deep networks can be fixed at initialization, not learned. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Alex Gaudio and Christos Faloutsos and Asim Smailagic and Pedro Costa and Aurélio Campilho},
  doi          = {10.1002/widm.1483},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {3},
  number       = {2},
  pages        = {e1483},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {ExplainFix: Explainable spatially fixed deep networks},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Table understanding: Problem overview. <em>WIDM</em>,
<em>13</em>(1), e1482. (<a
href="https://doi.org/10.1002/widm.1482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tables are probably the most natural way to represent relational data in various media and formats. They store a large number of valuable facts that could be utilized for question answering, knowledge base population, natural language generation, and other applications. However, many tables are not accompanied by semantics for the automatic interpretation of the information they present. Table Understanding (TU) aims at recovering the missing semantics that enables the extraction of facts from tables. This problem covers a range of issues from table detection in document images to semantic table interpretation with the help of external knowledge bases. To date, the TU research has been ongoing on for 30 years. Nevertheless, there is no common point of view on the scope of TU; the terminology still needs agreement and unification. In recent years, science and technology have shown a rapidly increasing interest in TU. Nowadays, it is especially important to check the meaning of this research problem once again. This article gives a comprehensive characterization of the TU problem, including a description of its subproblems, tasks, subtasks, and applications. It also discusses the common limitations used in the existing problem statements and proposes some directions for further research that would help overcome the corresponding limitations. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Alexey Shigarov},
  doi          = {10.1002/widm.1482},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1482},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Table understanding: Problem overview},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of AI for developing digital twins in healthcare:
The case of cancer care. <em>WIDM</em>, <em>13</em>(1), e1480. (<a
href="https://doi.org/10.1002/widm.1480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twins, succinctly described as the digital representation of a physical object, is a concept that has emerged relatively recently with increasing application in the manufacturing industry. This article proposes the application of this concept to the healthcare domain to provide enhanced clinical decision support and enable more patient-centric, and simultaneously more precise and individualized care to ensue. Digital twins combined with advances in Artificial Intelligence (AI) have the potential to facilitate the integration and processing of vast amounts of heterogeneous data stemming from diversified sources. Hence, in healthcare this can provide enhanced diagnosis and treatment decision support. In applying digital twins in combination with AI to complex healthcare contexts to assist clinical decision making, it is also likely that a key current challenge in healthcare; namely, providing better quality care which is of high value and can lead to better clinical outcomes and a higher level of patient satisfaction, can ensue. In this focus article, we address this proposition by focusing on the case study of cancer care and present our conceptualization of a digital twin model combined with AI to address key, current limitations in endometrial cancer treatment. We highlight the role of AI techniques in developing digital twins for cancer care and simultaneously identify key barriers and facilitators of this process from both a healthcare and technology perspective. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Rohit Kaul and Chinedu Ossai and Abdur Rahim Mohammad Forkan and Prem Prakash Jayaraman and John Zelcer and Stephen Vaughan and Nilmini Wickramasinghe},
  doi          = {10.1002/widm.1480},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1480},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {The role of AI for developing digital twins in healthcare: The case of cancer care},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributional regression modeling via generalized additive
models for location, scale, and shape: An overview through a data set
from learning analytics. <em>WIDM</em>, <em>13</em>(1), e1479. (<a
href="https://doi.org/10.1002/widm.1479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of technological developments is allowing to gather large amounts of data in several research fields. Learning analytics (LA)/educational data mining has access to big observational unstructured data captured from educational settings and relies mostly on unsupervised machine learning (ML) algorithms to make sense of such type of data. Generalized additive models for location, scale, and shape (GAMLSS) are a supervised statistical learning framework that allows modeling all the parameters of the distribution of the response variable with respect to the explanatory variables. This article overviews the power and flexibility of GAMLSS in relation to some ML techniques. Also, GAMLSS&#39; capability to be tailored toward causality via causal regularization is briefly commented. This overview is illustrated via a data set from the field of LA. This article is categorized under:},
  archive      = {J_WIDM},
  author       = {Fernando Marmolejo-Ramos and Mauricio Tejo and Marek Brabec and Jakub Kuzilek and Srecko Joksimovic and Vitomir Kovanovic and Jorge González and Thomas Kneib and Peter Bühlmann and Lucas Kook and Guillermo Briseño-Sánchez and Raydonal Ospina},
  doi          = {10.1002/widm.1479},
  journal      = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  month        = {1},
  number       = {1},
  pages        = {e1479},
  shortjournal = {WIREs Data Mining Knowl. Discov.},
  title        = {Distributional regression modeling via generalized additive models for location, scale, and shape: An overview through a data set from learning analytics},
  volume       = {13},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
