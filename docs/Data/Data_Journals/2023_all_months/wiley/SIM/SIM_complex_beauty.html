<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sim---310">SIM - 310</h2>
<ul>
<li><details>
<summary>
(2023). Win-loss parameters for right-censored event data, with
application to recurrent events. <em>SIM</em>, <em>42</em>(30),
5723–5735. (<a href="https://doi.org/10.1002/sim.9937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The win ratio has become a popular method for comparing multiple event data between two groups in clinical cohort studies. The win ratio compares the event data in prioritized order, where the first prioritized event is death and a typical example for the second prioritized event is hospitalization. Literature is sparse on inference for win and loss parameters, including the win ratio, for censored event data. Inference for two prioritized censored event times has been developed for independent right-censoring. Many clinical studies include recurrent event data such as hospitalizations. In this article, we suggest inference for win-loss parameters for death and a recurrent event outcome under independent right-censoring. The small sample properties of the proposed method are studied in a simulation study showing that the variance formula is accurate even for small samples. The method is applied on a data set from a randomized clinical trial.},
  archive      = {J_SIM},
  author       = {Erik T. Parner and Morten Overgaard},
  doi          = {10.1002/sim.9937},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5723-5735},
  shortjournal = {Stat. Med.},
  title        = {Win-loss parameters for right-censored event data, with application to recurrent events},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BEATS: Bayesian hybrid design with flexible sample size
adaptation for time-to-event endpoints. <em>SIM</em>, <em>42</em>(30),
5708–5722. (<a href="https://doi.org/10.1002/sim.9936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the roles of historical trials and real-world evidence in drug development have substantially increased, several approaches have been proposed to leverage external data and improve the design of clinical trials. While most of these approaches focus on methodology development for borrowing information during the analysis stage, there is a risk of inadequate or absent enrollment of concurrent control due to misspecification of heterogeneity from external data, which can result in unreliable estimates of treatment effect. In this study, we introduce a Bayesian hybrid design with flexible sample size adaptation (BEATS) that allows for adaptive borrowing of external data based on the level of heterogeneity to augment the control arm during both the design and interim analysis stages. Moreover, BEATS extends the Bayesian semiparametric meta-analytic predictive prior (BaSe-MAP) to incorporate time-to-event endpoints, enabling optimal borrowing performance. Initially, BEATS calibrates the expected sample size and initial randomization ratio based on heterogeneity among the external data. During the interim analysis, flexible sample size adaptation is performed to address conflicts between the concurrent and historical control, while also conducting futility analysis. At the final analysis, estimation is provided by incorporating the calibrated amount of external data. Therefore, our proposed design allows for an approximation of an ideal randomized controlled trial with an equal randomization ratio while controlling the size of the concurrent control to benefit patients and accelerate drug development. BEATS also offers optimal power and robust estimation through flexible sample size adaptation when conflicts arise between the concurrent control and external data.},
  archive      = {J_SIM},
  author       = {Dehua Bi and Meizi Liu and Jianchang Lin and Rachael Liu},
  doi          = {10.1002/sim.9936},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5708-5722},
  shortjournal = {Stat. Med.},
  title        = {BEATS: Bayesian hybrid design with flexible sample size adaptation for time-to-event endpoints},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing a phase-III time-to-event clinical trial using a
modified sample size formula and poisson-gamma model for subject accrual
that accounts for the lag in site initiation using the PERT
distribution. <em>SIM</em>, <em>42</em>(30), 5694–5707. (<a
href="https://doi.org/10.1002/sim.9935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A priori estimation of sample size and subject accrual in multi-site, time-to-event clinical trials is often challenging. Such trials are powered based on the number of events needed to detect a clinically significant difference. Sample size based on number of events relates to the expected duration of observation time for each subject. Temporal patterns in site initiation and subject enrollment ultimately affect when subjects can be accrued into the study. Lag times are common as the site start-up process optimizes, resulting in delays that may curtail observational follow-up and therefore undermine power. The proposed method introduces a Program Evaluation and Review Technique (PERT) model into the sample size estimation which accounts for the lag in site start-up. Additionally, a PERT model is introduced into a Poisson-Gamma subject accrual model to predict the quantity of study sites needed. The introduction of the PERT model provides greater flexibility in both a priori power assessment and planning the number of sites, as it specifically allows for the inclusion of anticipated delays in site start-up time. This model results in minimal power loss even when PERT distribution inputs are misspecified compared to the traditional assumption of simultaneous start-up for all sites. Together these updated formulations for sample size and subject accrual models offer an improved method for designing a multi-site time-to-event clinical trial that accounts for a flexible site start-up process.},
  archive      = {J_SIM},
  author       = {Virginia B. Shipes and Caitlyn Meinzer and Bethany J. Wolf and Hong Li and Mathew J. Carpenter and Hooman Kamel and Renee H. Martin},
  doi          = {10.1002/sim.9935},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5694-5707},
  shortjournal = {Stat. Med.},
  title        = {Designing a phase-III time-to-event clinical trial using a modified sample size formula and poisson-gamma model for subject accrual that accounts for the lag in site initiation using the PERT distribution},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rank selection for non-negative matrix factorization.
<em>SIM</em>, <em>42</em>(30), 5676–5693. (<a
href="https://doi.org/10.1002/sim.9934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Negative Matrix Factorization (NMF) is a widely used dimension reduction method that factorizes a non-negative data matrix into two lower dimensional non-negative matrices: one is the basis or feature matrix which consists of the variables and the other is the coefficients matrix which is the projections of data points to the new basis. The features can be interpreted as sub-structures of the data. The number of sub-structures in the feature matrix is also called the rank. This parameter controls the model complexity and is the only tuning parameter for the NMF model. An appropriate rank will extract the key latent features while minimizing the noise from the original data. However due to the large amount of optimization error always present in the NMF computation, the rank selection has been a difficult problem. We develop a novel rank selection method based on hypothesis testing, using a deconvolved bootstrap distribution to assess the significance level accurately. Through simulations, we compare our method with a rank selection method based on hypothesis testing using bootstrap distribution without deconvolution and a method based on cross-validation; we demonstrate that our method is not only accurate at estimating the true ranks for NMF, especially when the features are hard to distinguish, but also efficient at computation. When applied to real microbiome data (eg, OTU data and functional metagenomic data), our method also shows the ability to extract interpretable subcommunities in the data.},
  archive      = {J_SIM},
  author       = {Yun Cai and Hong Gu and Toby Kenney},
  doi          = {10.1002/sim.9934},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5676-5693},
  shortjournal = {Stat. Med.},
  title        = {Rank selection for non-negative matrix factorization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size and predictive performance of machine learning
methods with survival data: A simulation study. <em>SIM</em>,
<em>42</em>(30), 5657–5675. (<a
href="https://doi.org/10.1002/sim.9931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction models are increasingly developed and used in diagnostic and prognostic studies, where the use of machine learning (ML) methods is becoming more and more popular over traditional regression techniques. For survival outcomes the Cox proportional hazards model is generally used and it has been proven to achieve good prediction performances with few strong covariates. The possibility to improve the model performance by including nonlinearities, covariate interactions and time-varying effects while controlling for overfitting must be carefully considered during the model building phase. On the other hand, ML techniques are able to learn complexities from data at the cost of hyper-parameter tuning and interpretability. One aspect of special interest is the sample size needed for developing a survival prediction model. While there is guidance when using traditional statistical models, the same does not apply when using ML techniques. This work develops a time-to-event simulation framework to evaluate performances of Cox regression compared, among others, to tuned random survival forest, gradient boosting, and neural networks at varying sample sizes. Simulations were based on replications of subjects from publicly available databases, where event times were simulated according to a Cox model with nonlinearities on continuous variables and time-varying effects and on the SEER registry data.},
  archive      = {J_SIM},
  author       = {Gabriele Infante and Rosalba Miceli and Federico Ambrogi},
  doi          = {10.1002/sim.9931},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5657-5675},
  shortjournal = {Stat. Med.},
  title        = {Sample size and predictive performance of machine learning methods with survival data: A simulation study},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of non-monotonic transition rates in a
semi-markov process with covariates adjustments and application to
caregivers’ stress data. <em>SIM</em>, <em>42</em>(30), 5646–5656. (<a
href="https://doi.org/10.1002/sim.9930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the large ongoing number of aged people and Alzheimer&#39;s disease (AD) patients worldwide, unpaid caregivers have become the primary sources of their daily caregiving. Alzheimer&#39;s family caregivers often suffer from physical and mental morbidities owing to various reasons. The aims of this paper were to develop alternate methods to understand the transition properties, the dynamic change, and the long-run behavior of AD caregivers&#39; stress levels, by assuming their transition to the next level only depends on the duration of the current stress level. In this paper, we modeled the transition rates in the semi-Markov Process with log-logistic hazard functions. We assumed the transition rates were non-monotonic over time and the scale of transition rates depended on covariates. We also extended the uniform accelerated expansion to calculate the long-run probability distribution of stress levels while adjusting for multiple covariates. The proposed methods were evaluated through an empirical study. The application results showed that all the transition rates of caregivers&#39; stress levels were right skewed. Care recipients&#39; baseline age was significantly associated with the transitions. The long-run probability of severe state was slightly higher, implying a prolonged recovery time for severe stress patients.},
  archive      = {J_SIM},
  author       = {Esther Ngan and Wenyaw Chan and Luis Leon-Novelo and Valory Pavlik},
  doi          = {10.1002/sim.9930},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5646-5656},
  shortjournal = {Stat. Med.},
  title        = {Estimation of non-monotonic transition rates in a semi-markov process with covariates adjustments and application to caregivers&#39; stress data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous selection and incorporation of consistent
external aggregate information. <em>SIM</em>, <em>42</em>(30),
5630–5645. (<a href="https://doi.org/10.1002/sim.9929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest has grown in synthesizing participant level data of a study with relevant external aggregate information. Several efficient and flexible procedures have been developed under the assumption that the internal study and the external sources concern the same population. This homogeneity condition, albeit commonly being imposed, is hard to check due to limitedly available external information in aggregate data forms. Bias may be introduced when the assumption is violated. In this article, we propose a penalized likelihood approach that avoids undesirable bias by simultaneously selecting and synthesizing consistent external aggregate information. The proposed approach provides a general framework which incorporate consistent external information from heterogeneous study populations as long as the conditional distribution of the dependent variable under investigation is same and differences in the independent variable distributions are properly accounted for via a semi-parametric density ratio model. The proposed approach also properly accounts for the sampling errors in the external information. A two-step estimator and an optimization algorithm are proposed for computation. We establish the selection and estimation consistency and the asymptotic normality of the two-step estimator. The proposed approach is illustrated with an analysis of gestational weight gain management studies.},
  archive      = {J_SIM},
  author       = {Yunxiang Huang and Chiung-Yu Huang and Mi-Ok Kim},
  doi          = {10.1002/sim.9929},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5630-5645},
  shortjournal = {Stat. Med.},
  title        = {Simultaneous selection and incorporation of consistent external aggregate information},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Genome-wide search algorithms for identifying dynamic gene
co-expression via bayesian variable selection. <em>SIM</em>,
<em>42</em>(30), 5616–5629. (<a
href="https://doi.org/10.1002/sim.9928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wealth of gene expression data generated by high-throughput techniques provides exciting opportunities for studying gene-gene interactions systematically. Gene-gene interactions in a biological system are tightly regulated and are often highly dynamic. The interactions can change flexibly under various internal cellular signals or external stimuli. Previous studies have developed statistical methods to examine these dynamic changes in gene-gene interactions. However, due to the massive number of possible gene combinations that need to be considered in a typical genomic dataset, intensive computation is a common challenge for exploring gene-gene interactions. On the other hand, oftentimes only a small proportion of gene combinations exhibit dynamic co-expression changes. To solve this problem, we propose Bayesian variable selection approaches based on spike-and-slab priors. The proposed algorithms reduce the computational intensity by focusing on identifying subsets of promising gene combinations in the search space. We also adopt a Bayesian multiple hypothesis testing procedure to identify strong dynamic gene co-expression changes. Simulation studies are performed to compare the proposed approaches with existing exhaustive search heuristics. We demonstrate the implementation of our proposed approach to study the association between gene co-expression patterns and overall survival using the RNA-sequencing dataset from The Cancer Genome Atlas breast cancer BRCA-US project.},
  archive      = {J_SIM},
  author       = {Wenda Zhang and Zichen Ma and Lianming Wang and Daping Fan and Yen-Yi Ho},
  doi          = {10.1002/sim.9928},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5616-5629},
  shortjournal = {Stat. Med.},
  title        = {Genome-wide search algorithms for identifying dynamic gene co-expression via bayesian variable selection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric estimation and testing for panel count data
with informative interval-censored failure event. <em>SIM</em>,
<em>42</em>(30), 5596–5615. (<a
href="https://doi.org/10.1002/sim.9927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panel count data and interval-censored data are two types of incomplete data that often occur in event history studies. Almost all existing statistical methods are developed for their separate analysis. In this paper, we investigate a more general situation where a recurrent event process and an interval-censored failure event occur together. To intuitively and clearly explain the relationship between the recurrent current process and failure event, we propose a failure time-dependent mean model through a completely unspecified link function. To overcome the challenges arising from the blending of nonparametric components and parametric regression coefficients, we develop a two-stage conditional expected likelihood-based estimation procedure. We establish the consistency, the convergence rate and the asymptotic normality of the proposed two-stage estimator. Furthermore, we construct a class of two-sample tests for comparison of mean functions from different groups. The proposed methods are evaluated by extensive simulation studies and are illustrated with the skin cancer data that motivated this study.},
  archive      = {J_SIM},
  author       = {Li Liu and Wen Su and Xingqiu Zhao},
  doi          = {10.1002/sim.9927},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5596-5615},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric estimation and testing for panel count data with informative interval-censored failure event},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On semiparametric accelerated failure time models with
time-varying covariates: A maximum penalised likelihood estimation.
<em>SIM</em>, <em>42</em>(30), 5577–5595. (<a
href="https://doi.org/10.1002/sim.9926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated failure time (AFT) model offers an important and useful alternative to the conventional Cox proportional hazards model, particularly when the proportional hazards assumption for a Cox model is violated. Since an AFT model is basically a log-linear model, meaningful interpretations of covariate effects on failure times can be made directly. However, estimation of a semiparametric AFT model imposes computational challenges even when it only has time-fixed covariates, and the situation becomes much more complicated when time-varying covariates are included. In this paper, we propose a penalised likelihood approach to estimate the semiparametric AFT model with right-censored failure time, where both time-fixed and time-varying covariates are permitted. We adopt the Gaussian basis functions to construct a smooth approximation to the nonparametric baseline hazard. This model fitting method requires a constrained optimisation approach. A comprehensive simulation study is conducted to demonstrate the performance of the proposed method. An application of our method to a motor neuron disease data set is provided.},
  archive      = {J_SIM},
  author       = {Ding Ma and Jun Ma and Petra L. Graham},
  doi          = {10.1002/sim.9926},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5577-5595},
  shortjournal = {Stat. Med.},
  title        = {On semiparametric accelerated failure time models with time-varying covariates: A maximum penalised likelihood estimation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatio-temporal dirichlet process mixture model for
coronavirus disease-19. <em>SIM</em>, <em>42</em>(30), 5555–5576. (<a
href="https://doi.org/10.1002/sim.9925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the spatio-temporal patterns of the coronavirus disease 2019 (COVID-19) is essential to construct public health interventions. Spatially referenced data can provide richer opportunities to understand the mechanism of the disease spread compared to the more often encountered aggregated count data. We propose a spatio-temporal Dirichlet process mixture model to analyze confirmed cases of COVID-19 in an urban environment. Our method can detect unobserved cluster centers of the epidemics, and estimate the space-time range of the clusters that are useful to construct a warning system. Furthermore, our model can measure the impact of different types of landmarks in the city, which provides an intuitive explanation of disease spreading sources from different time points. To efficiently capture the temporal dynamics of the disease patterns, we employ a sequential approach that uses the posterior distribution of the parameters for the previous time step as the prior information for the current time step. This approach enables us to incorporate time dependence into our model in a computationally efficient manner without complicating the model structure. We also develop a model assessment by comparing the data with theoretical densities, and outline the goodness-of-fit of our fitted model.},
  archive      = {J_SIM},
  author       = {Jaewoo Park and Seorim Yi and Won Chang and Jorge Mateu},
  doi          = {10.1002/sim.9925},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5555-5576},
  shortjournal = {Stat. Med.},
  title        = {A spatio-temporal dirichlet process mixture model for coronavirus disease-19},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning for medical data: A review of
probabilistic factorization methods. <em>SIM</em>, <em>42</em>(30),
5541–5554. (<a href="https://doi.org/10.1002/sim.9924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We review popular unsupervised learning methods for the analysis of high-dimensional data encountered in, for example, genomics, medical imaging, cohort studies, and biobanks. We show that four commonly used methods, principal component analysis, K-means clustering, nonnegative matrix factorization, and latent Dirichlet allocation, can be written as probabilistic models underpinned by a low-rank matrix factorization. In addition to highlighting their similarities, this formulation clarifies the various assumptions and restrictions of each approach, which eases identifying the appropriate method for specific applications for applied medical researchers. We also touch upon the most important aspects of inference and model selection for the application of these methods to health data.},
  archive      = {J_SIM},
  author       = {Dorien Neijzen and Gerton Lunter},
  doi          = {10.1002/sim.9924},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {30},
  pages        = {5541-5554},
  shortjournal = {Stat. Med.},
  title        = {Unsupervised learning for medical data: A review of probabilistic factorization methods},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint clustering multiple longitudinal features: A
comparison of methods and software packages with practical guidance.
<em>SIM</em>, <em>42</em>(29), 5513–5540. (<a
href="https://doi.org/10.1002/sim.9917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering longitudinal features is a common goal in medical studies to identify distinct disease developmental trajectories. Compared to clustering a single longitudinal feature, integrating multiple longitudinal features allows additional information to be incorporated into the clustering process, which may reveal co-existing longitudinal patterns and generate deeper biological insight. Despite its increasing importance and popularity, there is limited practical guidance for implementing cluster analysis approaches for multiple longitudinal features and evaluating their comparative performance in medical datasets. In this paper, we provide an overview of several commonly used approaches to clustering multiple longitudinal features, with an emphasis on application and implementation through R software. These methods can be broadly categorized into two categories, namely model-based (including frequentist and Bayesian) approaches and algorithm-based approaches. To evaluate their performance, we compare these approaches using real-life and simulated datasets. These results provide practical guidance to applied researchers who are interested in applying these approaches for clustering multiple longitudinal features. Recommendations for applied researchers and suggestions for future research in this area are also discussed.},
  archive      = {J_SIM},
  author       = {Zihang Lu and Mojtaba Ahmadiankalati and Zhiwen Tan},
  doi          = {10.1002/sim.9917},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5513-5540},
  shortjournal = {Stat. Med.},
  title        = {Joint clustering multiple longitudinal features: A comparison of methods and software packages with practical guidance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric normal transformation joint model of
multivariate longitudinal and bivariate time-to-event data.
<em>SIM</em>, <em>42</em>(29), 5491–5512. (<a
href="https://doi.org/10.1002/sim.9923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models for longitudinal and survival data (JMLSs) are widely used to investigate the relationship between longitudinal and survival data in clinical trials in recent years. But, the existing studies mainly focus on independent survival data. In many clinical trials, survival data may be bivariately correlated. To this end, this paper proposes a novel JMLS accommodating multivariate longitudinal and bivariate correlated time-to-event data. Nonparametric marginal survival hazard functions are transformed to bivariate normal random variables. Bayesian penalized splines are employed to approximate unknown baseline hazard functions. Incorporating the Metropolis-Hastings algorithm into the Gibbs sampler, we develop a Bayesian adaptive Lasso method to simultaneously estimate parameters and baseline hazard functions, and select important predictors in the considered JMLS. Simulation studies and an example taken from the International Breast Cancer Study Group are used to illustrate the proposed methodologies.},
  archive      = {J_SIM},
  author       = {An-Ming Tang and Cheng Peng and Niansheng Tang},
  doi          = {10.1002/sim.9923},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5491-5512},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric normal transformation joint model of multivariate longitudinal and bivariate time-to-event data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The survival-incorporated median vs the median in the
survivors or in the always-survivors: What are we measuring? And why?
<em>SIM</em>, <em>42</em>(29), 5479–5490. (<a
href="https://doi.org/10.1002/sim.9922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical studies evaluate the benefit of a treatment based on both survival and other continuous/ordinal clinical outcomes, such as quality of life scores. In these studies, when subjects die before the follow-up assessment, the clinical outcomes become undefined and are truncated by death. Treating outcomes as “missing” or “censored” due to death can be misleading for treatment effect evaluation. We show that if we use the median in the survivors or in the always-survivors as estimands to summarize clinical outcomes, we may conclude that a trade-off exists between the probability of survival and good clinical outcomes, even in settings where both the probability of survival and the probability of any good clinical outcome are better for one treatment. Therefore, we advocate not always treating death as a mechanism through which clinical outcomes are missing, but rather as part of the outcome measure. To account for the survival status, we describe the survival-incorporated median as an alternative summary measure for outcomes in the presence of death. The survival-incorporated median is the threshold such that 50% of the population is alive with an outcome above that threshold. Through conceptual examples and an application to a prostate cancer treatment study, we show that the survival-incorporated median provides a simple and useful summary measure to inform clinical practice.},
  archive      = {J_SIM},
  author       = {Qingyan Xiang and Ronald J. Bosch and Judith J. Lok},
  doi          = {10.1002/sim.9922},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5479-5490},
  shortjournal = {Stat. Med.},
  title        = {The survival-incorporated median vs the median in the survivors or in the always-survivors: What are we measuring? and why?},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibrating machine learning approaches for probability
estimation: A comprehensive comparison. <em>SIM</em>, <em>42</em>(29),
5451–5478. (<a href="https://doi.org/10.1002/sim.9921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical prediction models have gained popularity in applied research. One challenge is the transfer of the prediction model to a different population which may be structurally different from the model for which it has been developed. An adaptation to the new population can be achieved by calibrating the model to the characteristics of the target population, for which numerous calibration techniques exist. In view of this diversity, we performed a systematic evaluation of various popular calibration approaches used by the statistical and the machine learning communities for estimating two-class probabilities. In this work, we first provide a review of the literature and, second, present the results of a comprehensive simulation study. The calibration approaches are compared with respect to their empirical properties and relationships, their ability to generalize precise probability estimates to external populations and their availability in terms of easy-to-use software implementations. Third, we provide code from real data analysis allowing its application by researchers. Logistic calibration and beta calibration, which estimate an intercept plus one and two slope parameters, respectively, consistently showed the best results in the simulation studies. Calibration on logit transformed probability estimates generally outperformed calibration methods on nontransformed estimates. In case of structural differences between training and validation data, re-estimation of the entire prediction model should be outweighted against sample size of the validation data. We recommend regression-based calibration approaches using transformed probability estimates, where at least one slope is estimated in addition to an intercept for updating probability estimates in validation studies.},
  archive      = {J_SIM},
  author       = {Francisco M. Ojeda and Max L. Jansen and Alexandre Thiéry and Stefan Blankenberg and Christian Weimar and Matthias Schmid and Andreas Ziegler},
  doi          = {10.1002/sim.9921},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5451-5478},
  shortjournal = {Stat. Med.},
  title        = {Calibrating machine learning approaches for probability estimation: A comprehensive comparison},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assessable and interpretable sensitivity analysis in the
pattern graph framework for nonignorable missingness mechanisms.
<em>SIM</em>, <em>42</em>(29), 5419–5450. (<a
href="https://doi.org/10.1002/sim.9920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pattern graph framework solves a wide range of missing data problems with nonignorable mechanisms. However, it faces two challenges of assessability and interpretability, particularly important in safety-critical problems such as clinical diagnosis: (i) How can one assess the validity of the framework&#39;s a priori assumption and make necessary adjustments to accommodate known information about the problem? (ii) How can one interpret the process of exponential tilting used for sensitivity analysis in the pattern graph framework and choose the tilt perturbations based on meaningful real-world quantities? In this paper, we introduce Informed Sensitivity Analysis, an extension of the pattern graph framework that enables us to incorporate substantive knowledge about the missingness mechanism into the pattern graph framework. Our extension allows us to examine the validity of assumptions underlying pattern graphs and interpret sensitivity analysis results in terms of realistic problem characteristics. We apply our method to a prevalent nonignorable missing data scenario in clinical research. We validate and compare our method&#39;s results of our method with a number of widely-used missing data methods, including Unweighted CCA, KNN Imputer, MICE, and MissForest. The validation is done using both boot-strapped simulated experiments as well as real-world clinical observations in the MIMIC-III public dataset.},
  archive      = {J_SIM},
  author       = {Alireza Zamanian and Narges Ahmidi and Mathias Drton},
  doi          = {10.1002/sim.9920},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5419-5450},
  shortjournal = {Stat. Med.},
  title        = {Assessable and interpretable sensitivity analysis in the pattern graph framework for nonignorable missingness mechanisms},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A longitudinal transition imputation model for categorical
data applied to a large registry dataset. <em>SIM</em>, <em>42</em>(29),
5405–5418. (<a href="https://doi.org/10.1002/sim.9919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imputation of longitudinal categorical covariates with several waves and many predictors is cumbersome in terms of implausible transitions, colinearity, and overfitting. We designed a simulation study with data obtained from a general practitioners&#39; morbidity registry in Belgium for three waves, with smoking as the longitudinal covariate of interest. We set varying proportions of data on smoking to missing completely at random and missing not at random with proportions of missingness equal to 10%, 30%, 50%, and 70%. This study proposed a 3-stage approach that allows flexibility when imputing time-dependent categorical covariates. First, multiple imputation using fully conditional specification or multiple imputation for the predictor variables was deployed using the wide format such that previous and future information of the same patient was utilized. Second, a joint Markov transition model for initial, forward, backward, and intermittent probabilities was developed for each imputed dataset. Finally, this transition model was used for imputation. We compared the performance of this methodology with an analyses of the complete data and with listwise deletion in terms of bias and root mean square error. Next, we applied this methodology in a clinical case for years 2017 to 2021, where we estimated the effect of several covariates on the pneumococcal vaccination. This methodological framework ensures that the plausibility of transitions is preserved, overfitting and colinearity issues are resolved, and confounders can be utilized. Finally, a companion R package was developed to enable the replication and easy application of this methodology.},
  archive      = {J_SIM},
  author       = {Pavlos Mamouris and Vahid Nassiri and Geert Verbeke and Arne Janssens and Bert Vaes and Geert Molenberghs},
  doi          = {10.1002/sim.9919},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5405-5418},
  shortjournal = {Stat. Med.},
  title        = {A longitudinal transition imputation model for categorical data applied to a large registry dataset},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric estimation of restricted mean survival time
as a function of restriction time. <em>SIM</em>, <em>42</em>(29),
5389–5404. (<a href="https://doi.org/10.1002/sim.9918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The restricted mean survival time (RMST) is an appealing measurement in clinical or epidemiological studies with censored survival outcome and receives a lot of attention in the past decades. It provides a useful alternative to the Cox model for evaluating the covariate effect on survival time. The covariate effect on RMST usually varies with the restriction time. However, existing methods cannot address this problem properly. In this article, we propose a semiparametric framework that directly models RMST as a function of the restriction time. Our proposed model adopts a widely-used proportional form, enabling the estimation of RMST predictions across an interval using a unified model. Furthermore, the covariate effect for multiple restriction time points can be derived simultaneously. We develop estimators based on estimating equations theories and establish the asymptotic properties of the proposed estimators. The finite sample properties of the estimators are evaluated through extensive simulation studies. We further illustrate the application of our proposed method through the analysis of two real data examples. Supplementary Material are available online.},
  archive      = {J_SIM},
  author       = {Fangfang Bai and Xiaoran Yang},
  doi          = {10.1002/sim.9918},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5389-5404},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric estimation of restricted mean survival time as a function of restriction time},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network and covariate adjusted response-adaptive design for
binary response. <em>SIM</em>, <em>42</em>(29), 5369–5388. (<a
href="https://doi.org/10.1002/sim.9915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomization is a distinguishing feature of clinical trials for unbiased assessment of treatment efficacy. With a growing demand for more flexible and efficient randomization schemes and motivated by the idea of adaptive design, in this article we propose the network and covariate adjusted response-adaptive (NCARA) design that can concurrently manage three challenges: (1) maximizing benefits of a trial by assigning more patients to the superior treatment group randomly; (2) balancing social network ties across treatment arms to eliminate potential network interference; and (3) ensuring balance of important covariates, such as age, gender, and other potential confounders. We conduct simulation with different network structures and a variety of parameter settings. It is observed that the NCARA design outperforms four alternative randomization designs in solving the above-mentioned problems and has comparable power and type I error for detecting true difference between treatment groups. In addition, we conduct real data analysis to implement the new design in two clinical trials. Compared to equal randomization (the original design utilized in the trials), the NCARA design slightly increases power, largely increases the percentage of patients assigned to the better-performing group, and significantly improves network and covariate balances. It is also noted that the advantages of the NCARA design are augmented when the sample size is small and the level of network interference is high. In summary, the proposed NCARA design assists researchers in conducting clinical trials with high-quality and high-efficiency.},
  archive      = {J_SIM},
  author       = {Hao Mei and Jiaxin Xie and Yichen Qin and Yang Li},
  doi          = {10.1002/sim.9915},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5369-5388},
  shortjournal = {Stat. Med.},
  title        = {Network and covariate adjusted response-adaptive design for binary response},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ball divergence for the equality test of crossing survival
curves. <em>SIM</em>, <em>42</em>(29), 5353–5368. (<a
href="https://doi.org/10.1002/sim.9914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a very common problem to test survival equality using the right-censored time-to-event data in clinical research. Although the log-rank test is popularly used in various studies, it may become insensitive when the proportional hazards assumption is violated. As follows, there have a variety of statistical methods being proposed to identify the discrepancy between crossing survival curves or hazard functions. The omnibus tests against general alternatives are usually preferred due to their wide applicability to complicated scenarios in real applications. In this paper, we propose two novel statistics to estimate the ball divergence using the right-censored survival data, and then implement them in the equality test on survival time in two independent groups. The simulation analysis demonstrates their efficiency in identifying the survival discrepancy. Compared to the existing methods, our proposed methods present higher power in situations with complex distributions, especially when there is a scale shift between groups. Real examples illustrate its advantage in practical applications.},
  archive      = {J_SIM},
  author       = {Na You and Xueyi He and Hongsheng Dai and Xueqin Wang},
  doi          = {10.1002/sim.9914},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5353-5368},
  shortjournal = {Stat. Med.},
  title        = {Ball divergence for the equality test of crossing survival curves},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian adaptive design for covariate-adaptive historical
control information borrowing. <em>SIM</em>, <em>42</em>(29), 5338–5352.
(<a href="https://doi.org/10.1002/sim.9913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interest in incorporating historical data in the clinical trial has increased with the rising cost of conducting clinical trials. The intervention arm for the current trial often requires prospective data to assess a novel treatment, and thus borrowing historical control data commensurate in distribution to current control data is motivated in order to increase the allocation ratio to the current intervention arm. Existing historical control borrowing adaptive designs adjust allocation ratios based on the commensurability assessed through study-level summary statistics of the response agnostic of the distributions of the trial subject characteristics in the current and historical trials. This can lead to distributional imbalance of the current trial subject characteristics across the treatment arms as well as between current control data and borrowed historical control data. Such covariate imbalance may threaten the internal validity of the current trial by introducing confounding factors that affect study endpoints. In this article, we propose a Bayesian design which borrows and updates the treatment allocation ratios both covariate-adaptively and commensurate to covariate dependently assessed similarity between the current and historical control data. We employ covariate-dependent discrepancy parameters which are allowed to grow with the sample size and propose a regularized local regression procedure for the estimation of the parameters. The proposed design also permits the current and the historical controls to be similar to varying degree, depending on the subject level characteristics. We evaluate the proposed design extensively under the settings derived from two placebo-controlled randomized trials on vertebral fracture risk in post-menopausal women.},
  archive      = {J_SIM},
  author       = {Huaqing Jin and Mi-Ok Kim and Aaron Scheffler and Fei Jiang},
  doi          = {10.1002/sim.9913},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5338-5352},
  shortjournal = {Stat. Med.},
  title        = {Bayesian adaptive design for covariate-adaptive historical control information borrowing},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The diagnostic likelihood ratio function and modified test
for trend: Identifying, evaluating, and validating nontraditional
biomarkers in case-control studies. <em>SIM</em>, <em>42</em>(29),
5313–5337. (<a href="https://doi.org/10.1002/sim.9912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ROC curve and its associated summary statistic, the AUC, are used to identify informative diagnostic biomarkers under the assumption that risk of disease is a monotone function of the biomarker. We refer to biomarkers that meet this assumption as traditional, and those that do not as nontraditional. Nontraditional biomarkers most often arise when both low and high biomarker values are associated with an outcome of interest, such as blood pressure with medical complications or leukocyte count with ICU prognosis. Since nontraditional biomarkers do not meet the assumptions for ROC-based analyses, we propose using the discrete diagnostic likelihood ratio (DLR) function to evaluate a wider class of informative biomarkers. We obtain the DLR function using the multinomial logistic regression (MLR) model to improve upon existing estimation techniques, and implement a likelihood ratio test to identify candidate informative traditional and nontraditional biomarkers. We propose a modification of the Cochran-Armitage test for trend that separates biomarkers deemed informative into traditional and nontraditional categories. The statistical properties of the likelihood ratio test and modified test for trend are explored under simulation. Together, these methods achieve the identification, evaluation, and validation of biomarkers from early discovery research. Finally, we show that incorporating covariates into the MLR model results in a covariate-adjusted DLR function that is useful for integrating multiple sources of information in clinical decision making. The methods are applied to gene expression data from subjects with high grade serous ovarian cancer, where stage, early stage vs late stage, is the outcome of interest.},
  archive      = {J_SIM},
  author       = {Hanna Lindner and Phyllis A. Gimotty and Warren B. Bilker},
  doi          = {10.1002/sim.9912},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {29},
  pages        = {5313-5337},
  shortjournal = {Stat. Med.},
  title        = {The diagnostic likelihood ratio function and modified test for trend: Identifying, evaluating, and validating nontraditional biomarkers in case-control studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimization in randomized clinical trials. <em>SIM</em>,
<em>42</em>(28), 5285–5311. (<a
href="https://doi.org/10.1002/sim.9916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized trials, comparability of the treatment groups is ensured through allocation of treatments using a mechanism that involves some random element, thus controlling for confounding of the treatment effect. Completely random allocation ensures comparability between the treatment groups for all known and unknown prognostic factors. For a specific trial, however, imbalances in prognostic factors among the treatment groups may occur. Although accidental bias can be avoided in the presence of such imbalances by stratifying the analysis, most trialists, regulatory agencies, and other stakeholders prefer a balanced distribution of prognostic factors across the treatment groups. Some procedures attempt to achieve balance in baseline covariates, by stratifying the allocation for these covariates, or by dynamically adapting the allocation using covariate information during the trial (covariate-adaptive procedures). In this Tutorial, the performance of minimization, a popular covariate-adaptive procedure, is compared with two other commonly used procedures, completely random allocation and stratified blocked designs. Using individual patient data of 2 clinical trials (in advanced ovarian cancer and age-related macular degeneration), the procedures are compared in terms of operating characteristics (using asymptotic and randomization tests), predictability of treatment allocation, and achieved balance. Fifty actual trials of various sizes that applied minimization for treatment allocation are used to investigate the achieved balance. Implementation issues of minimization are described. Minimization procedures are useful in all trials but especially when (1) many major prognostic factors are known, (2) many centers of different sizes accrue patients, or (3) the trial sample size is moderate.},
  archive      = {J_SIM},
  author       = {Elisabeth Coart and Perrine Bamps and Emmanuel Quinaux and Geneviève Sturbois and Everardo D. Saad and Tomasz Burzykowski and Marc Buyse},
  doi          = {10.1002/sim.9916},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5285-5311},
  shortjournal = {Stat. Med.},
  title        = {Minimization in randomized clinical trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian framework for pathway-guided identification of
cancer subgroups by integrating multiple types of genomic data.
<em>SIM</em>, <em>42</em>(28), 5266–5284. (<a
href="https://doi.org/10.1002/sim.9911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, comprehensive cancer genomics platforms, such as The Cancer Genome Atlas (TCGA), provide access to an enormous amount of high throughput genomic datasets for each patient, including gene expression, DNA copy number alterations, DNA methylation, and somatic mutation. While the integration of these multi-omics datasets has the potential to provide novel insights that can lead to personalized medicine, most existing approaches only focus on gene-level analysis and lack the ability to facilitate biological findings at the pathway-level. In this article, we propose Bayes-InGRiD (Bayesian Integrative Genomics Robust iDentification of cancer subgroups), a novel pathway-guided Bayesian sparse latent factor model for the simultaneous identification of cancer patient subgroups (clustering) and key molecular features (variable selection) within a unified framework, based on the joint analysis of continuous, binary, and count data. By utilizing pathway (gene set) information, Bayes-InGRiD does not only enhance the accuracy and robustness of cancer patient subgroup and key molecular feature identification, but also promotes biological understanding and interpretation. Finally, to facilitate an efficient posterior sampling, an alternative Gibbs sampler for logistic and negative binomial models is proposed using Pólya-Gamma mixtures of normal to represent latent variables for binary and count data, which yields a conditionally Gaussian representation of the posterior. The R package “INGRID” implementing the proposed approach is currently available in our research group GitHub webpage ( https://dongjunchung.github.io/INGRID/ ).},
  archive      = {J_SIM},
  author       = {Zequn Sun and Dongjun Chung and Brian Neelon and Andrew Millar-Wilson and Stephen P. Ethier and Feifei Xiao and Yinan Zheng and Kristin Wallace and Gary Hardiman},
  doi          = {10.1002/sim.9911},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5266-5284},
  shortjournal = {Stat. Med.},
  title        = {A bayesian framework for pathway-guided identification of cancer subgroups by integrating multiple types of genomic data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint semiparametric kernel network regression.
<em>SIM</em>, <em>42</em>(28), 5247–5265. (<a
href="https://doi.org/10.1002/sim.9910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection and graphical modeling play essential roles in highly correlated and high-dimensional (HCHD) data analysis. Variable selection methods have been developed under both parametric and nonparametric model settings. However, variable selection for nonadditive, nonparametric regression with high-dimensional variables is challenging due to complications in modeling unknown dependence structures among HCHD variables. Gaussian graphical models are a popular and useful tool for investigating the conditional dependence between variables via estimating sparse precision matrices. For a given class of interest, the estimated precision matrices can be mapped onto networks for visualization. However, the limitation of Gaussian graphical models is that they are only applicable to discretized response variables and for the case when , where is the number of variables and is the sample size. They are necessary to develop a joint method for variable selection and graphical modeling. To the best of our knowledge, the methods for simultaneously selecting variable selection and estimating networks among variables in the semiparametric regression settings are quite limited. Hence, in this paper, we develop a joint semiparametric kernel network regression method to solve this limitation and to provide a connection between them. Our approach is a unified and integrated method that can simultaneously identify important variables and build a network among those variables. We developed our approach under a semiparametric kernel machine regression framework, which can allow for nonlinear or nonadditive associations and complicated interactions among the variables. The advantages of our approach are that it can (1) simultaneously select variables and build a network among HCHD variables under a regression setting; (2) model unknown and complicated interactions among the variables and estimate the network among these variables; (3) allow for any form of semiparametric model, including non-additive, nonparametric model; and (4) provide an interpretable network that considers important variables and a response variable. We demonstrate our approach using a simulation study and real application on genetic pathway-based analysis.},
  archive      = {J_SIM},
  author       = {Byung-Jun Kim and Inyoung Kim},
  doi          = {10.1002/sim.9910},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5247-5265},
  shortjournal = {Stat. Med.},
  title        = {Joint semiparametric kernel network regression},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size optimization for clinical trials using graphical
approaches for multiplicity adjustment. <em>SIM</em>, <em>42</em>(28),
5229–5246. (<a href="https://doi.org/10.1002/sim.9909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical approach provides a useful framework for multiplicity adjustment in clinical trials with multiple endpoints. When designing a graphical approach, initial weight and transition probability for the endpoints are often assigned based on clinical importance. For example, practitioners may prefer putting more weights on some primary endpoints. The clinical preference can be formulated as a constrain in the sample size optimization problem. However, there has been a lack of theoretical guidance on how to specify initial weight and transition probability in a graphical approach to meet the clinical preference but at the same time to minimize the sample size needed for a power requirement. To fill this gap, we propose statistical methods to optimize sample size over initial weight and transition probability in a graphical approach under a common setting, which is to use marginal power for each endpoint in a trial design. Importantly, we prove that some of the commonly used graphical approaches such as putting all initial weights on one endpoint are suboptimal. Our methods are flexible, which can be used for both single-arm trials and randomized controlled trials with either continuous or binary or mixed types of endpoints. Additionally, we prove the existence of optimal solution where all marginal powers are placed exactly at the prespecified values, assuming continuity. Two hypothetical clinical trial designs are presented to illustrate the application of our methods under different scenarios. Results are first presented for a design with two endpoints and are further generalized to three or more endpoints. Our findings are helpful to guide the design of a graphical approach and the sample size calculation in clinical trials.},
  archive      = {J_SIM},
  author       = {Fengqing Zhang and Jiangtao Gou},
  doi          = {10.1002/sim.9909},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5229-5246},
  shortjournal = {Stat. Med.},
  title        = {Sample size optimization for clinical trials using graphical approaches for multiplicity adjustment},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new accuracy metric under three classes when subclasses
are involved and its confidence interval estimation. <em>SIM</em>,
<em>42</em>(28), 5207–5228. (<a
href="https://doi.org/10.1002/sim.9908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {“Compound multi-class classification” refers to the setting where three or more main classes are involved and at least one of the main classes have multiple subclasses. A common practice in evaluating biomarker performance under “compound multi-class classification” is “subclasses pooling.” In this article, we first explore the downsides of accuracy metrics based on pooled data. Then we propose a new accuracy measure proper for “compound multi-class classification” with three ordinal main classes, namely “volume under compound R ⁢ O ⁢ C $$ ROC $$ surface ( V ⁢ U ⁢ S C $$ VU{S}_C $$ ).” The proposed V ⁢ U ⁢ S C $$ VU{S}_C $$ evaluates the accuracy of a biomarker appropriately by identifying main classes without requiring specification of an ordering for marker values of subclasses within each main class. For confidence interval estimation of , both parametric and nonparametric methods are studied, and simulation studies are carried out to assess coverage probabilities. A subset of Alzheimer&#39;s Disease Neuroimaging Initiative study dataset is analyzed.},
  archive      = {J_SIM},
  author       = {Nan Nan and Lili Tian},
  doi          = {10.1002/sim.9908},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5207-5228},
  shortjournal = {Stat. Med.},
  title        = {A new accuracy metric under three classes when subclasses are involved and its confidence interval estimation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fitting a stochastic model of intensive care occupancy to
noisy hospitalization time series during the COVID-19 pandemic.
<em>SIM</em>, <em>42</em>(28), 5189–5206. (<a
href="https://doi.org/10.1002/sim.9907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensive care occupancy is an important indicator of health care stress that has been used to guide policy decisions during the COVID-19 pandemic. Toward reliable decision-making as a pandemic progresses, estimating the rates at which patients are admitted to and discharged from hospitals and intensive care units (ICUs) is crucial. Since individual-level hospital data are rarely available to modelers in each geographic locality of interest, it is important to develop tools for inferring these rates from publicly available daily numbers of hospital and ICU beds occupied. We develop such an estimation approach based on an immigration-death process that models fluctuations of ICU occupancy. Our flexible framework allows for immigration and death rates to depend on covariates, such as hospital bed occupancy and daily SARS-CoV-2 test positivity rate, which may drive changes in hospital ICU operations. We demonstrate via simulation studies that the proposed method performs well on noisy time series data and apply our statistical framework to hospitalization data from the University of California, Irvine (UCI) Health and Orange County, California. By introducing a likelihood-based framework where immigration and death rates can vary with covariates, we find, through rigorous model selection, that hospitalization and positivity rates are crucial covariates for modeling ICU stay dynamics and validate our per-patient ICU stay estimates using anonymized patient-level UCI hospital data.},
  archive      = {J_SIM},
  author       = {Achal Awasthi and Volodymyr M. Minin and Jenny Huang and Daniel Chow and Jason Xu},
  doi          = {10.1002/sim.9907},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5189-5206},
  shortjournal = {Stat. Med.},
  title        = {Fitting a stochastic model of intensive care occupancy to noisy hospitalization time series during the COVID-19 pandemic},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating seroconversion rates accounting for repeated
infections by approximate bayesian computation. <em>SIM</em>,
<em>42</em>(28), 5160–5188. (<a
href="https://doi.org/10.1002/sim.9906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel approach for inferring the incidence of infections by employing a quantitative model of the serum antibody response. Current methodologies often overlook the cumulative effect of an individual&#39;s infection history, making it challenging to obtain a marginal distribution for antibody concentrations. Our proposed approach leverages approximate Bayesian computation to simulate cross-sectional antibody responses and compare these to observed data, factoring in the impact of repeated infections. We then assess the empirical distribution functions of the simulated and observed antibody data utilizing Kolmogorov deviance, thereby incorporating a goodness-of-fit check. This new method not only matches the computational efficiency of preceding likelihood-based analyses but also facilitates the joint estimation of antibody noise parameters. The results affirm that the predictions generated by our within-host model closely align with the observed distributions from cross-sectional samples of a well-characterized population. Our findings mirror those of likelihood-based methodologies in scenarios of low infection pressure, such as the transmission of pertussis in Europe. However, our simulations reveal that in settings of higher infection pressure, likelihood-based approaches tend to underestimate the force of infection. Thus, our novel methodology presents significant advancements in estimating infection incidence, thereby enhancing our understanding of disease dynamics in the field of epidemiology.},
  archive      = {J_SIM},
  author       = {Peter F. M. Teunis and Yuke Wang and Kristen Aiemjoy and Mirjam Kretzschmar and Marc Aerts},
  doi          = {10.1002/sim.9906},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5160-5188},
  shortjournal = {Stat. Med.},
  title        = {Estimating seroconversion rates accounting for repeated infections by approximate bayesian computation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Post-test diagnostic accuracy measures under tree ordering
of disease classes. <em>SIM</em>, <em>42</em>(28), 5135–5159. (<a
href="https://doi.org/10.1002/sim.9905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The medical field commonly employs post-test measures such as predictive values and likelihood ratios to assess diagnostic accuracy. Predictive values, including positive and negative values ( PPV and NPV ), indicate the probability that individuals have a target health condition based on test results. On the other hand, likelihood ratios, including positive and negative ratios ( LR + and LR − respectively), compare the probability of a particular test result between the diseased and non-diseased groups. While predictive values are useful in evaluating diagnostic test accuracy in populations with varying disease prevalence, likelihood ratios provide a direct link between pre-test and post-test probabilities in specific patients. In this study, we introduce and analyze a new approach called generalized predictive values and likelihood ratios, using a tree ordering of disease classes. We evaluate the effectiveness of these methods through simulation studies and illustrate their use with real data on lung cancer.},
  archive      = {J_SIM},
  author       = {Hani Samawi and Marwan Alsharman and Mario Keko and Jing Kersey},
  doi          = {10.1002/sim.9905},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5135-5159},
  shortjournal = {Stat. Med.},
  title        = {Post-test diagnostic accuracy measures under tree ordering of disease classes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the estimation of interval censored destructive negative
binomial cure model. <em>SIM</em>, <em>42</em>(28), 5113–5134. (<a
href="https://doi.org/10.1002/sim.9904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a competitive risk survival model is considered in which the initial number of risks, assumed to follow a negative binomial distribution, is subject to a destructive mechanism. Assuming the population of interest to have a cure component, the form of the data as interval-censored, and considering both the number of initial risks and risks remaining active after destruction to be missing data, we develop two distinct estimation algorithms for this model. Making use of the conditional distributions of the missing data, we develop an expectation maximization (EM) algorithm, in which the conditional expected complete log-likelihood function is decomposed into simpler functions which are then maximized independently. A variation of the EM algorithm, called the stochastic EM (SEM) algorithm, is also developed with the goal of avoiding the calculation of complicated expectations and improving performance at parameter recovery. A Monte Carlo simulation study is carried out to evaluate the performance of both estimation methods through calculated bias, root mean square error, and coverage probability of the asymptotic confidence interval. We demonstrate the proposed SEM algorithm as the preferred estimation method through simulation and further illustrate the advantage of the SEM algorithm, as well as the use of a destructive model, with data from a children&#39;s mortality study.},
  archive      = {J_SIM},
  author       = {Jodi Treszoks and Suvra Pal},
  doi          = {10.1002/sim.9904},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5113-5134},
  shortjournal = {Stat. Med.},
  title        = {On the estimation of interval censored destructive negative binomial cure model},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint modeling the frequency and duration of
accelerometer-measured physical activity from a lifestyle intervention
trial. <em>SIM</em>, <em>42</em>(28), 5100–5112. (<a
href="https://doi.org/10.1002/sim.9903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical activity (PA) guidelines recommend that PA be accumulated in bouts of 10 minutes or more in duration. Recently, researchers have sought to better understand how participants in PA interventions increase their activity. Participants can increase their daily PA by increasing the number of PA bouts per day while keeping the duration of the bouts constant; they can keep the number of bouts constant but increase the duration of each bout; or participants can increase both the number of bouts and their duration. We propose a novel joint modeling framework for modeling PA bouts and their duration over time. Our joint model is comprised of two sub-models: a mixed-effects Poisson hurdle sub-model for the number of bouts per day and a mixed-effects location scale gamma regression sub-model to characterize the duration of the bouts and their variance. The model allows us to estimate how daily PA bouts and their duration vary together over the course of an intervention and by treatment condition and is specifically designed to capture the unique distributional features of bouted PA as measured by accelerometer: frequent measurements, zero-inflated bouts, and skewed bout durations. We apply our methods to the Make Better Choices study, a longitudinal lifestyle intervention trial to increase PA. We perform a simulation study to evaluate how well our model is able to estimate relationships between outcomes.},
  archive      = {J_SIM},
  author       = {Juned Siddique and Michael J. Daniels and Gül Inan and Samuel Battalio and Bonnie Spring and Donald Hedeker},
  doi          = {10.1002/sim.9903},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5100-5112},
  shortjournal = {Stat. Med.},
  title        = {Joint modeling the frequency and duration of accelerometer-measured physical activity from a lifestyle intervention trial},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian hierarchical models incorporating study-level
covariates for multivariate meta-analysis of diagnostic tests without a
gold standard with application to COVID-19. <em>SIM</em>,
<em>42</em>(28), 5085–5099. (<a
href="https://doi.org/10.1002/sim.9902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When evaluating a diagnostic test, it is common that a gold standard may not be available. One example is the diagnosis of SARS-CoV-2 infection using saliva sampling or nasopharyngeal swabs. Without a gold standard, a pragmatic approach is to postulate a “reference standard,” defined as positive if either test is positive, or negative if both are negative. However, this pragmatic approach may overestimate sensitivities because subjects infected with SARS-CoV-2 may still have double-negative test results even when both tests exhibit perfect specificity. To address this limitation, we propose a Bayesian hierarchical model for simultaneously estimating sensitivity, specificity, and disease prevalence in the absence of a gold standard. The proposed model allows adjusting for study-level covariates. We evaluate the model performance using an example based on a recently published meta-analysis on the diagnosis of SARS-CoV-2 infection and extensive simulations. Compared with the pragmatic reference standard approach, we demonstrate that the proposed Bayesian method provides a more accurate evaluation of prevalence, specificity, and sensitivity in a meta-analytic framework.},
  archive      = {J_SIM},
  author       = {Zheng Wang and Thomas A Murray and Mengli Xiao and Lifeng Lin and Demissie Alemayehu and Haitao Chu},
  doi          = {10.1002/sim.9902},
  journal      = {Statistics in Medicine},
  month        = {12},
  number       = {28},
  pages        = {5085-5099},
  shortjournal = {Stat. Med.},
  title        = {Bayesian hierarchical models incorporating study-level covariates for multivariate meta-analysis of diagnostic tests without a gold standard with application to COVID-19},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to “summarizing empirical information on
between-study heterogeneity for bayesian random-effects meta-analysis.”
<em>SIM</em>, <em>42</em>(27), 5084. (<a
href="https://doi.org/10.1002/sim.9933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.9933},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5084},
  shortjournal = {Stat. Med.},
  title        = {Correction to “Summarizing empirical information on between-study heterogeneity for bayesian random-effects meta-analysis”},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size requirements for testing treatment effect
heterogeneity in cluster randomized trials with binary outcomes.
<em>SIM</em>, <em>42</em>(27), 5054–5083. (<a
href="https://doi.org/10.1002/sim.9901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) refer to a popular class of experiments in which randomization is carried out at the group level. While methods have been developed for planning CRTs to study the average treatment effect, and more recently, to study the heterogeneous treatment effect, the development for the latter objective has currently been limited to a continuous outcome. Despite the prevalence of binary outcomes in CRTs, determining the necessary sample size and statistical power for detecting differential treatment effects in CRTs with a binary outcome remain unclear. To address this methodological gap, we develop sample size procedures for testing treatment effect heterogeneity in two-level CRTs under a generalized linear mixed model. Closed-form sample size expressions are derived for a binary effect modifier, and in addition, a computationally efficient Monte Carlo approach is developed for a continuous effect modifier. Extensions to multiple effect modifiers are also discussed. We conduct simulations to examine the accuracy of the proposed sample size methods. We present several numerical illustrations to elucidate features of the proposed formulas and to compare our method to the approximate sample size calculation under a linear mixed model. Finally, we use data from the Strategies and Opportunities to Stop Colon Cancer in Priority Populations (STOP CRC) CRT to illustrate the proposed sample size procedure for testing treatment effect heterogeneity.},
  archive      = {J_SIM},
  author       = {Lara Maleyeff and Rui Wang and Sebastien Haneuse and Fan Li},
  doi          = {10.1002/sim.9901},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5054-5083},
  shortjournal = {Stat. Med.},
  title        = {Sample size requirements for testing treatment effect heterogeneity in cluster randomized trials with binary outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference on tree-structured subgroups with subgroup size
and subgroup effect relationship in clinical trials. <em>SIM</em>,
<em>42</em>(27), 5039–5053. (<a
href="https://doi.org/10.1002/sim.9900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When multiple candidate subgroups are considered in clinical trials, we often need to make statistical inference on the subgroups simultaneously. Classical multiple testing procedures might not lead to an interpretable and efficient inference on the subgroups as they often fail to take subgroup size and subgroup effect relationship into account. In this paper, built on the selective traversed accumulation rules (STAR), we propose a data-adaptive and interactive multiple testing procedure for subgroups which can take subgroup size and subgroup effect relationship into account under prespecified tree structure. The proposed method is easy-to-implement and can lead to a more interpretable and efficient inference on prespecified tree-structured subgroups. Possible accommodations to post hoc identified tree-structure subgroups are also discussed in the paper. We demonstrate the merit of our proposed method by re-analyzing the panitumumab trial with the proposed method.},
  archive      = {J_SIM},
  author       = {Yuanhui Luo and Xinzhou Guo},
  doi          = {10.1002/sim.9900},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5039-5053},
  shortjournal = {Stat. Med.},
  title        = {Inference on tree-structured subgroups with subgroup size and subgroup effect relationship in clinical trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing missing data in the estimation of time-varying
treatments in comparative effectiveness research. <em>SIM</em>,
<em>42</em>(27), 5025–5038. (<a
href="https://doi.org/10.1002/sim.9899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative effectiveness research is often concerned with evaluating treatment strategies sustained over time, that is, time-varying treatments. Inverse probability weighting (IPW) is often used to address the time-varying confounding by re-weighting the sample according to the probability of treatment receipt at each time point. IPW can also be used to address any missing data by re-weighting individuals according to the probability of observing the data. The combination of these two distinct sets of weights may lead to inefficient estimates of treatment effects due to potentially highly variable total weights. Alternatively, multiple imputation (MI) can be used to address the missing data by replacing each missing observation with a set of plausible values drawn from the posterior predictive distribution of the missing data given the observed data. Recent studies have compared IPW and MI for addressing the missing data in the evaluation of time-varying treatments, but they focused on missing confounders and monotone missing data patterns. This article assesses the relative advantages of MI and IPW to address missing data in both outcomes and confounders measured over time, and across monotone and non-monotone missing data settings. Through a comprehensive simulation study, we find that MI consistently provided low bias and more precise estimates compared to IPW across a wide range of scenarios. We illustrate the implications of method choice in an evaluation of biologic drugs for patients with severe rheumatoid arthritis, using the US National Databank for Rheumatic Diseases, in which 25% of participants had missing health outcomes or time-varying confounders.},
  archive      = {J_SIM},
  author       = {Juan Segura-Buisan and Clemence Leyrat and Manuel Gomes},
  doi          = {10.1002/sim.9899},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5025-5038},
  shortjournal = {Stat. Med.},
  title        = {Addressing missing data in the estimation of time-varying treatments in comparative effectiveness research},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using temporal recalibration to improve the calibration of
risk prediction models in competing risk settings when there are trends
in survival over time. <em>SIM</em>, <em>42</em>(27), 5007–5024. (<a
href="https://doi.org/10.1002/sim.9898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have previously proposed temporal recalibration to account for trends in survival over time to improve the calibration of predictions from prognostic models for new patients. This involves first estimating the predictor effects using data from all individuals (full dataset) and then re-estimating the baseline using a subset of the most recent data whilst constraining the predictor effects to remain the same. In this article, we demonstrate how temporal recalibration can be applied in competing risk settings by recalibrating each cause-specific (or subdistribution) hazard model separately. We illustrate this using an example of colon cancer survival with data from the Surveillance Epidemiology and End Results (SEER) program. Data from patients diagnosed in 1995–2004 were used to fit two models for deaths due to colon cancer and other causes respectively. We discuss considerations that need to be made in order to apply temporal recalibration such as the choice of data used in the recalibration step. We also demonstrate how to assess the calibration of these models in new data for patients diagnosed subsequently in 2005. Comparison was made to a standard analysis (when improvements over time are not taken into account) and a period analysis which is similar to temporal recalibration but differs in the data used to estimate the predictor effects. The 10-year calibration plots demonstrated that using the standard approach over-estimated the risk of death due to colon cancer and the total risk of death and that calibration was improved using temporal recalibration or period analysis.},
  archive      = {J_SIM},
  author       = {Sarah Booth and Sarwar I. Mozumder and Lucinda Archer and Joie Ensor and Richard D. Riley and Paul C. Lambert and Mark J. Rutherford},
  doi          = {10.1002/sim.9898},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {5007-5024},
  shortjournal = {Stat. Med.},
  title        = {Using temporal recalibration to improve the calibration of risk prediction models in competing risk settings when there are trends in survival over time},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian predictive model averaging approach to joint
longitudinal-survival modeling: Application to an immuno-oncology
clinical trial. <em>SIM</em>, <em>42</em>(27), 4990–5006. (<a
href="https://doi.org/10.1002/sim.9897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In immuno-oncology clinical trials, multiple immunological biomarkers are usually examined over time to comprehensively and appropriately evaluate the efficacy of treatments. Because predicting patients&#39; future survival statuses on the basis of such recorded longitudinal information might be of great interest, joint modeling of longitudinal and time-to-event data has been intensively discussed as a toolkit to implement such a prediction. To achieve a desirable predictive performance, averaging over multiple candidate predictive models to account for the model uncertainty might be a more suitable statistical approach than selecting the single best model. Although Bayesian model averaging can be one of the approaches, several problems related to model weights with marginal likelihoods have been discussed. To address these problems, we here propose a Bayesian predictive model averaging (BPMA) method that uses Bayesian leave-one-out cross-validation predictive densities to account for the subject-specific and time-dependent nature of the prediction. We examine the operating characteristics of the proposed BPMA method in terms of the predictive accuracy (ie, the calibration and discrimination abilities) in extensive simulation studies. In addition, we discuss the strengths and limitations of the proposed method by applying it to an immuno-oncology clinical trial in patients with advanced ovarian cancer.},
  archive      = {J_SIM},
  author       = {Zixuan Yao and Satoshi Morita and Sumiyuki Nishida and Haruo Sugiyama},
  doi          = {10.1002/sim.9897},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4990-5006},
  shortjournal = {Stat. Med.},
  title        = {Bayesian predictive model averaging approach to joint longitudinal-survival modeling: Application to an immuno-oncology clinical trial},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric multivariate joint model for
skewed-longitudinal and survival data: A bayesian approach.
<em>SIM</em>, <em>42</em>(27), 4972–4989. (<a
href="https://doi.org/10.1002/sim.9896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint models and statistical inference for longitudinal and survival data have been an active area of statistical research and have mostly coupled a longitudinal biomarker-based mixed-effects model with normal distribution and an event time-based survival model. In practice, however, the following issues may standout: (i) Normality of model error in longitudinal models is a routine assumption, but it may be unrealistically violating data features of subject variations. (ii) Data collected are often featured by the mixed types of multiple longitudinal outcomes which are significantly correlated, ignoring their correlation may lead to biased estimation. Additionally, a parametric model specification may be inflexible to capture the complicated patterns of longitudinal data. (iii) Missing observations in the longitudinal data are often encountered; the missing measures are likely to be informative (nonignorable) and ignoring this phenomenon may result in inaccurate inference. Multilevel item response theory (MLIRT) models have been increasingly used to analyze the multiple longitudinal data of mixed types (ie, continuous and categorical) in clinical studies. In this article, we develop an MLIRT-based semiparametric joint model with skew-t distribution that consists of an extended MLIRT model for the mixed types of multiple longitudinal data and a Cox proportional hazards model, linked through random-effects. A Bayesian approach is employed for joint modeling. Simulation studies are conducted to assess performance of the proposed models and method. A real example from primary biliary cirrhosis clinical study is analyzed to estimate parameters in the joint model and also evaluate sensitivity of parameter estimates for various plausible nonignorable missing data mechanisms.},
  archive      = {J_SIM},
  author       = {Jiaqing Chen and Yangxin Huang and Qing Wang},
  doi          = {10.1002/sim.9896},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4972-4989},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric multivariate joint model for skewed-longitudinal and survival data: A bayesian approach},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification of longitudinal profiles using
semi-parametric nonlinear mixed models with p-splines and the SAEM
algorithm. <em>SIM</em>, <em>42</em>(27), 4952–4971. (<a
href="https://doi.org/10.1002/sim.9895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose an extension of a semiparametric nonlinear mixed-effects model for longitudinal data that incorporates more flexibility with penalized splines (P-splines) as smooth terms. The novelty of the proposed approach consists of the formulation of the model within the stochastic approximation version of the EM algorithm for maximum likelihood, the so-called SAEM algorithm. The proposed approach takes advantage of the formulation of a P-spline as a mixed-effects model and the use of the computational advantages of the existing software for the SAEM algorithm for the estimation of the random effects and the variance components. Additionally, we developed a supervised classification method for these non-linear mixed models using an adaptive importance sampling scheme. To illustrate our proposal, we consider two studies on pregnant women where two biomarkers are used as indicators of changes during pregnancy. In both studies, information about the women&#39;s pregnancy outcomes is known. Our proposal provides a unified framework for the classification of longitudinal profiles that may have important implications for the early detection and monitoring of pregnancy-related changes and contribute to improved maternal and fetal health outcomes. We show that the proposed models improve the analysis of this type of data compared to previous studies. These improvements are reflected both in the fit of the models and in the classification of the groups.},
  archive      = {J_SIM},
  author       = {Maritza Márquez and Cristian Meza and Dae-Jin Lee and Rolando De la Cruz},
  doi          = {10.1002/sim.9895},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4952-4971},
  shortjournal = {Stat. Med.},
  title        = {Classification of longitudinal profiles using semi-parametric nonlinear mixed models with P-splines and the SAEM algorithm},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian record linkage with variables in one file.
<em>SIM</em>, <em>42</em>(27), 4931–4951. (<a
href="https://doi.org/10.1002/sim.9894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many healthcare and social science applications, information about units is dispersed across multiple data files. Linking records across files is necessary to estimate the associations of interest. Common record linkage algorithms only rely on similarities between linking variables that appear in all the files. Moreover, analysis of linked files often ignores errors that may arise from incorrect or missed links. Bayesian record linking methods allow for natural propagation of linkage error, by jointly sampling the linkage structure and the model parameters. We extend an existing Bayesian record linkage method to integrate associations between variables exclusive to each file being linked. We show analytically, and using simulations, that the proposed method can improve the linking process, and can result in accurate inferences. We apply the method to link Meals on Wheels recipients to Medicare enrollment records.},
  archive      = {J_SIM},
  author       = {Gauri Kamat and Mingyang Shan and Roee Gutman},
  doi          = {10.1002/sim.9894},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4931-4951},
  shortjournal = {Stat. Med.},
  title        = {Bayesian record linkage with variables in one file},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new approach to evaluating loop inconsistency in network
meta-analysis. <em>SIM</em>, <em>42</em>(27), 4917–4930. (<a
href="https://doi.org/10.1002/sim.9872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In network meta-analysis, studies evaluating multiple treatment comparisons are modeled simultaneously, and estimation is informed by a combination of direct and indirect evidence. Network meta-analysis relies on an assumption of consistency, meaning that direct and indirect evidence should agree for each treatment comparison. Here we propose new local and global tests for inconsistency and demonstrate their application to three example networks. Because inconsistency is a property of a loop of treatments in the network meta-analysis, we locate the local test in a loop. We define a model with one inconsistency parameter that can be interpreted as loop inconsistency. The model builds on the existing ideas of node-splitting and side-splitting in network meta-analysis. To provide a global test for inconsistency, we extend the model across multiple independent loops with one degree of freedom per loop. We develop a new algorithm for identifying independent loops within a network meta-analysis. Our proposed models handle treatments symmetrically, locate inconsistency in loops rather than in nodes or treatment comparisons, and are invariant to choice of reference treatment, making the results less dependent on model parameterization. For testing global inconsistency in network meta-analysis, our global model uses fewer degrees of freedom than the existing design-by-treatment interaction approach and has the potential to increase power. To illustrate our methods, we fit the models to three network meta-analyses varying in size and complexity. Local and global tests for inconsistency are performed and we demonstrate that the global model is invariant to choice of independent loops.},
  archive      = {J_SIM},
  author       = {Rebecca M. Turner and Tim Band and Tim P. Morris and David J. Fisher and Julian P. T. Higgins and James R. Carpenter and Ian R. White},
  doi          = {10.1002/sim.9872},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4917-4930},
  shortjournal = {Stat. Med.},
  title        = {A new approach to evaluating loop inconsistency in network meta-analysis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-step log-linear procedure for graphical representation
and inference of associations in cross-classified data for disease
diagnosis. <em>SIM</em>, <em>42</em>(27), 4897–4916. (<a
href="https://doi.org/10.1002/sim.9854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrical sciences and disease diagnosis in particular, are often concerned with the analysis of associations for cross-classified data, for which distance association models give us a graphical interpretation for non-sparse matrices with a low number of categories. In this framework, usually binary exploratory and response variables are present, with analysis based on individual profiles being of great interest. For saturated models, we show the usual linear relationship for log-linear models is preserved in full dimension for the distance association parameterization. This enables a two-step procedure to facilitate the analysis and the interpretation of associations in terms of unfolding after the overall and main effects are removed. The proposed procedure can deal with cross-classified data for profiles by binary variables, and it is easy to implement using traditional statistical software. For disease diagnosis, the problems of a degenerate solution in the unfolding representation, and that of determining significant differences between the profile locations are addressed. A hypothesis test of independence based on odds ratio is considered. Furthermore, a procedure is proposed to determine the causes of the significance of the test, avoiding the problem of error propagation. The equivalence between a test for equality of odds ratio pairs and the test for equality of location for two profiles in the unfolding representation in the disease diagnosis is shown. The results have been applied to a real example on the diagnosis of coronary disease, relating the odds ratios with performance parameters of the diagnostic test.},
  archive      = {J_SIM},
  author       = {J. Fernando Vera and José A. Roldán-Nofuentes},
  doi          = {10.1002/sim.9854},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {27},
  pages        = {4897-4916},
  shortjournal = {Stat. Med.},
  title        = {A two-step log-linear procedure for graphical representation and inference of associations in cross-classified data for disease diagnosis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximum approximate likelihood estimation in accelerated
failure time model for interval-censored data. <em>SIM</em>,
<em>42</em>(26), 4886–4896. (<a
href="https://doi.org/10.1002/sim.9893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approximate Bernstein polynomial model, a mixture of beta distributions, is applied to obtain maximum likelihood estimates of the regression coefficients, the baseline density and the survival functions in an accelerated failure time model based on interval censored data including current status data. The estimators of the regression coefficients and the underlying baseline density function are shown to be consistent with almost parametric rates of convergence under some conditions for uncensored and/or interval censored data. Simulation shows that the proposed method is better than its competitors. The proposed method is illustrated by fitting the Breast Cosmetic and the HIV infection time data using the accelerated failure time model.},
  archive      = {J_SIM},
  author       = {Zhong Guan},
  doi          = {10.1002/sim.9893},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4886-4896},
  shortjournal = {Stat. Med.},
  title        = {Maximum approximate likelihood estimation in accelerated failure time model for interval-censored data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian method for estimating gene-level polygenicity
under the framework of transcriptome-wide association study.
<em>SIM</em>, <em>42</em>(26), 4867–4885. (<a
href="https://doi.org/10.1002/sim.9892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polygenicity refers to the phenomenon that multiple genetic variants have a nonzero effect on a complex trait. It is defined as the proportion of genetic variants with a nonzero effect on the trait. Evaluation of polygenicity can provide valuable insights into the genetic architecture of the trait. Several recent works have attempted to estimate polygenicity at the single nucleotide polymorphism level. However, evaluating polygenicity at the gene level can be biologically more meaningful. We propose the notion of gene-level polygenicity, defined as the proportion of genes having a nonzero effect on the trait under the framework of a transcriptome-wide association study. We introduce a Bayesian approach genepoly to estimate this quantity for a trait. The method is based on spike and slab prior and simultaneously estimates the subset of non-null genes. Our simulation study shows that genepoly efficiently estimates gene-level polygenicity. The method produces a downward bias for small choices of trait heritability due to a non-null gene, which diminishes rapidly with an increase in the genome-wide association study (GWAS) sample size. While identifying the subset of non-null genes, genepoly offers a high level of specificity and an overall good level of sensitivity—the sensitivity increases as the sample size of the reference panel expression and GWAS data increase. We applied the method to seven phenotypes in the UK Biobank, integrating expression data. We find height to be the most polygenic and asthma to be the least polygenic.},
  archive      = {J_SIM},
  author       = {Arunabha Majumdar and Bogdan Pasaniuc},
  doi          = {10.1002/sim.9892},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4867-4885},
  shortjournal = {Stat. Med.},
  title        = {A bayesian method for estimating gene-level polygenicity under the framework of transcriptome-wide association study},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inconsistency identification in network meta-analysis via
stochastic search variable selection. <em>SIM</em>, <em>42</em>(26),
4850–4866. (<a href="https://doi.org/10.1002/sim.9891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliability of the results of network meta-analysis (NMA) lies in the plausibility of the key assumption of transitivity. This assumption implies that the effect modifiers&#39; distribution is similar across treatment comparisons. Transitivity is statistically manifested through the consistency assumption which suggests that direct and indirect evidence are in agreement. Several methods have been suggested to evaluate consistency. A popular approach suggests adding inconsistency factors to the NMA model. We follow a different direction by describing each inconsistency factor with a candidate covariate whose choice relies on variable selection techniques. Our proposed method, stochastic search inconsistency factor selection (SSIFS), evaluates the consistency assumption both locally and globally, by applying the stochastic search variable selection method to determine whether the inconsistency factors should be included in the model. The posterior inclusion probability of each inconsistency factor quantifies how likely is a specific comparison to be inconsistent. We use posterior model odds or the median probability model to decide on the importance of inconsistency factors. Differences between direct and indirect evidence can be incorporated into the inconsistency detection process. A key point of our proposed approach is the construction of a reasonable “informative” prior concerning network consistency. The prior is based on the elicitation of information derived historical data from 201 published network meta-analyses. The performance of our proposed method is evaluated in two published network meta-analyses. The proposed methodology is publicly available in an R package called ssifs , published on CRAN and developed and maintained by the authors of this work.},
  archive      = {J_SIM},
  author       = {Georgios Seitidis and Stavros Nikolakopoulos and Ioannis Ntzoufras and Dimitris Mavridis},
  doi          = {10.1002/sim.9891},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4850-4866},
  shortjournal = {Stat. Med.},
  title        = {Inconsistency identification in network meta-analysis via stochastic search variable selection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frequentist analysis of basket trials with one-sample
mantel-haenszel procedures. <em>SIM</em>, <em>42</em>(26), 4824–4849.
(<a href="https://doi.org/10.1002/sim.9890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent substantial advances of molecular targeted oncology drug development is requiring new paradigms for early-phase clinical trial methodologies to enable us to evaluate efficacy of several subtypes simultaneously and efficiently. The concept of the basket trial is getting of much attention to realize this requirement borrowing information across subtypes, which are called baskets. Bayesian approach is a natural approach to this end and indeed the majority of the existing proposals relies on it. On the other hand, it required complicated modeling and may not necessarily control the type 1 error probabilities at the nominal level. In this article, we develop a purely frequentist approach for basket trials based on one-sample Mantel-Haenszel procedure relying on a very simple idea for borrowing information under the common treatment effect assumption over baskets. We show that the proposed Mantel-Haenszel estimator for the treatment effect is consistent under two limiting models of the large strata and sparse data limiting models (dually consistent) and propose dually consistent variance estimators. The proposed estimators are interpretable even if the common treatment effect assumptions are violated. Then, we can design basket trials in a confirmatory matter. We also propose an information criterion approach to identify effective subclasses of baskets.},
  archive      = {J_SIM},
  author       = {Satoshi Hattori and Satoshi Morita},
  doi          = {10.1002/sim.9890},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4824-4849},
  shortjournal = {Stat. Med.},
  title        = {Frequentist analysis of basket trials with one-sample mantel-haenszel procedures},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian multistage spatio-temporally dependent model for
spatial clustering and variable selection. <em>SIM</em>,
<em>42</em>(26), 4794–4823. (<a
href="https://doi.org/10.1002/sim.9889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In spatio-temporal epidemiological analysis, it is of critical importance to identify the significant covariates and estimate the associated time-varying effects on the health outcome. Due to the heterogeneity of spatio-temporal data, the subsets of important covariates may vary across space and the temporal trends of covariate effects could be locally different. However, many spatial models neglected the potential local variation patterns, leading to inappropriate inference. Thus, this article proposes a flexible Bayesian hierarchical model to simultaneously identify spatial clusters of regression coefficients with common temporal trends, select significant covariates for each spatial group by introducing binary entry parameters and estimate spatio-temporally varying disease risks. A multistage strategy is employed to reduce the confounding bias caused by spatially structured random components. A simulation study demonstrates the outperformance of the proposed method, compared with several alternatives based on different assessment criteria. The methodology is motivated by two important case studies. The first concerns the low birth weight incidence data in 159 counties of Georgia, USA, for the years 2007 to 2018 and investigates the time-varying effects of potential contributing covariates in different cluster regions. The second concerns the circulatory disease risks across 323 local authorities in England over 10 years and explores the underlying spatial clusters and associated important risk factors.},
  archive      = {J_SIM},
  author       = {Shaopei Ma and Keming Yu and Man-lai Tang and Jianxin Pan and Wolfgang Karl Härdle and Maozai Tian},
  doi          = {10.1002/sim.9889},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4794-4823},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multistage spatio-temporally dependent model for spatial clustering and variable selection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A latent functional approach for modeling the effects of
multidimensional exposures on disease risk. <em>SIM</em>,
<em>42</em>(26), 4776–4793. (<a
href="https://doi.org/10.1002/sim.9888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the relationships between exposure and disease incidence is an important problem in environmental epidemiology. Typically, a large number of these exposures are measured, and it is found either that a few exposures transmit risk or that each exposure transmits a small amount of risk, but, taken together, these may pose a substantial disease risk. Further, these exposure effects can be nonlinear. We develop a latent functional approach, which assumes that the individual effect of each exposure can be characterized as one of a series of unobserved functions, where the number of latent functions is less than or equal to the number of exposures. We propose Bayesian methodology to fit models with a large number of exposures and show that existing Bayesian group LASSO approaches are a special case of the proposed model. An efficient Markov chain Monte Carlo sampling algorithm is developed for carrying out Bayesian inference. The deviance information criterion is used to choose an appropriate number of nonlinear latent functions. We demonstrate the good properties of the approach using simulation studies. Further, we show that complex exposure relationships can be represented with only a few latent functional curves. The proposed methodology is illustrated with an analysis of the effect of cumulative pesticide exposure on cancer risk in a large cohort of farmers.},
  archive      = {J_SIM},
  author       = {Sungduk Kim and Laura E. Beane Freeman and Paul S. Albert},
  doi          = {10.1002/sim.9888},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4776-4793},
  shortjournal = {Stat. Med.},
  title        = {A latent functional approach for modeling the effects of multidimensional exposures on disease risk},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of secondary failure time responses in studies with
response-dependent sampling schemes. <em>SIM</em>, <em>42</em>(26),
4763–4775. (<a href="https://doi.org/10.1002/sim.9887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Response-dependent sampling is routinely used as an enrichment strategy in the design of family studies investigating the heritable nature of disease. In addition to the response of primary interest, investigators often wish to investigate the association between biomarkers and secondary responses related to possible comorbidities. Statistical analysis regarding genetic biomarkers and their association with the secondary outcome must address the biased sampling scheme involving the primary response. In this article, we develop composite likelihoods and two-stage estimation procedures for such secondary analyses in which the within-family dependence structure for the primary and secondary outcomes is modeled via a Gaussian copula. The dependence among responses within family members is modeled based on kinship coefficients. Auxiliary data from independent individuals are exploited by augmenting the composite likelihoods to increase precision of marginal parameter estimates and enhance the efficiency of estimators of the dependence parameters. Simulation studies are carried out to evaluate the finite sample performance of the proposed method, and an application to a motivating family study in psoriatic arthritis is given for illustration.},
  archive      = {J_SIM},
  author       = {Yujie Zhong and Richard J. Cook and Aiai Yu},
  doi          = {10.1002/sim.9887},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4763-4775},
  shortjournal = {Stat. Med.},
  title        = {Analysis of secondary failure time responses in studies with response-dependent sampling schemes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handling missing within-study correlations in the evaluation
of surrogate endpoints. <em>SIM</em>, <em>42</em>(26), 4738–4762. (<a
href="https://doi.org/10.1002/sim.9886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rigorous evaluation of surrogate endpoints is performed in a trial-level analysis in which the strength of the association between treatment effects on the clinical and surrogate endpoints is quantified across a collection of previously conducted trials. To reduce bias in measures of the performance of the surrogate, the statistical model must account for the sampling error in each trial&#39;s estimated treatment effects and their potential correlation. Unfortunately, these within-study correlations can be difficult to obtain, especially for meta-analysis of published trial results where individual patient data is not available. As such, these terms are frequently partially or completely missing in the analysis. We show that improper handling of these missing terms can meaningfully alter the perceived quality of the surrogate and we introduce novel strategies to handle the missingness.},
  archive      = {J_SIM},
  author       = {Willem Collier and Benjamin Haaland and Lesley Inker and Tom Greene},
  doi          = {10.1002/sim.9886},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4738-4762},
  shortjournal = {Stat. Med.},
  title        = {Handling missing within-study correlations in the evaluation of surrogate endpoints},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correcting prevalence estimation for biased sampling with
testing errors. <em>SIM</em>, <em>42</em>(26), 4713–4737. (<a
href="https://doi.org/10.1002/sim.9885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sampling for prevalence estimation of infection is subject to bias by both oversampling of symptomatic individuals and error-prone tests. This results in naïve estimators of prevalence (ie, proportion of observed infected individuals in the sample) that can be very far from the true proportion of infected. In this work, we present a method of prevalence estimation that reduces both the effect of bias due to testing errors and oversampling of symptomatic individuals, eliminating it altogether in some scenarios. Moreover, this procedure considers stratified errors in which tests have different error rate profiles for symptomatic and asymptomatic individuals. This results in easily implementable algorithms, for which code is provided, that produce better prevalence estimates than other methods (in terms of reducing and/or removing bias), as demonstrated by formal results, simulations, and on COVID-19 data from the Israeli Ministry of Health.},
  archive      = {J_SIM},
  author       = {Lili Zhou and Daniel Andrés Díaz-Pachón and Chen Zhao and J. Sunil Rao and Ola Hössjer},
  doi          = {10.1002/sim.9885},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4713-4737},
  shortjournal = {Stat. Med.},
  title        = {Correcting prevalence estimation for biased sampling with testing errors},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparison of combination methods to create calibrated
ensemble forecasts for seasonal influenza in the u.s. <em>SIM</em>,
<em>42</em>(26), 4696–4712. (<a
href="https://doi.org/10.1002/sim.9884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characteristics of influenza seasons vary substantially from year to year, posing challenges for public health preparation and response. Influenza forecasting is used to inform seasonal outbreak response, which can in turn potentially reduce the impact of an epidemic. The United States Centers for Disease Control and Prevention, in collaboration with external researchers, has run an annual prospective influenza forecasting exercise, known as the FluSight challenge. Uniting theoretical results from the forecasting literature with domain-specific forecasts from influenza outbreaks, we applied parametric forecast combination methods that simultaneously optimize model weights and calibrate the ensemble via a beta transformation and made adjustments to the methods to reduce their complexity. We used the beta-transformed linear pool, the finite beta mixture model, and their equal weight adaptations to produce ensemble forecasts retrospectively for the 2016/2017, 2017/2018, and 2018/2019 influenza seasons in the U.S. We compared their performance to methods that were used in the FluSight challenge to produce the FluSight Network ensemble, namely the equally weighted linear pool and the linear pool. Ensemble forecasts produced from methods with a beta transformation were shown to outperform those from the equally weighted linear pool and the linear pool for all week-ahead targets across in the test seasons based on average log scores. We observed improvements in overall accuracy despite the beta-transformed linear pool or beta mixture methods&#39; modest under-prediction across all targets and seasons. Combination techniques that explicitly adjust for known calibration issues in linear pooling should be considered to improve probabilistic scores in outbreak settings.},
  archive      = {J_SIM},
  author       = {Nutcha Wattanachit and Evan L. Ray and Thomas C. McAndrew and Nicholas G. Reich},
  doi          = {10.1002/sim.9884},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4696-4712},
  shortjournal = {Stat. Med.},
  title        = {Comparison of combination methods to create calibrated ensemble forecasts for seasonal influenza in the U.S.},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subgroup analysis using bernoulli-gated hierarchical
mixtures of experts models. <em>SIM</em>, <em>42</em>(26), 4681–4695.
(<a href="https://doi.org/10.1002/sim.9883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When it is suspected that the treatment effect may only be strong for certain subpopulations, identifying the baseline covariate profiles of subgroups who benefit from such a treatment is of key importance. In this paper, we propose an approach for subgroup analysis by firstly introducing Bernoulli-gated hierarchical mixtures of experts (BHME), a binary-tree structured model to explore heterogeneity of the underlying distribution. We show identifiability of the BHME model and develop an EM-based maximum likelihood method for optimization. The algorithm automatically determines a partition structure with optimal prediction but possibly suboptimal in identifying treatment effect heterogeneity. We then suggest a testing-based postscreening step to further capture effect heterogeneity. Simulation results show that our approach outperforms competing methods on discovery of differential treatment effects and other related metrics. We finally apply the proposed approach to a real dataset from the Tennessee&#39;s Student/Teacher Achievement Ratio project.},
  archive      = {J_SIM},
  author       = {Wei Li and Shanshan Luo and Yangbo He and Zhi Geng},
  doi          = {10.1002/sim.9883},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {26},
  pages        = {4681-4695},
  shortjournal = {Stat. Med.},
  title        = {Subgroup analysis using bernoulli-gated hierarchical mixtures of experts models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of sparse differential dependent functional brain
connectivity. <em>SIM</em>, <em>42</em>(25), 4664–4680. (<a
href="https://doi.org/10.1002/sim.9882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional brain connectivity analysis is an increasingly important technique in neuroscience, psychiatry, and autism research. Functional connectivity can be measured by considering co-activation of brain regions in resting-state functional magnetic resonance imaging (rs-fMRI). We propose a novel Bayesian model to detect differential connections in cross-correlated functional connectivity between region of interest (ROI) pairs. The proposed sparse clustered neighborhood model induces a lower-dimensional sparsity and clustering based on a nonparametric Bayesian approach to model sparse differentially connected ROI pairs. Second, it induces a structured dependence model for modeling potential dependence among ROI pairs. We demonstrate Bayesian inference and performance of the proposed model in simulation studies and compare with a standard model. We utilize the proposed model to contrast functional connectivities between participants with autism spectrum disorder and neurotypical participants using cross-correlated rs-fMRI data from four sites of the Autism Brain Image Data Exchange.},
  archive      = {J_SIM},
  author       = {Nairita Ghosal and Sanjb Basu and Dulal Bhaumik},
  doi          = {10.1002/sim.9882},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4664-4680},
  shortjournal = {Stat. Med.},
  title        = {Detection of sparse differential dependent functional brain connectivity},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Change point detection for high dimensional data via kernel
measure with application to human aging brain data. <em>SIM</em>,
<em>42</em>(25), 4644–4663. (<a
href="https://doi.org/10.1002/sim.9881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the existence and locations of change points has been a broadly encountered task in many statistical application areas. The existing change point detection methods may produce unsatisfactory results for high-dimensional data since certain distributional assumptions are made on data, which are hard to verify in practice. Moreover, some parameters (such as the number of change points) need to be estimated beforehand for some methods, making their powers sensitive to these values. Here, we propose a kernel-based U $$ U $$ -statistic to identify change points (KUCP) for high dimensional data, which is free of distributional assumptions and sup-parameter estimations. Specifically, we employ a kernel function to describe similarities among the subjects and construct a U $$ U $$ -statistic to test the existence of change point for a given location. The asymptotic properties of the -statistic are deduced. We also develop a procedure to locate the change points sequentially via a dichotomy algorithm. Extensive simulations demonstrate that KUCP has higher sensitivity in identifying existence of change points and higher accuracy in locating these change points than its counterparts. We further illustrate its practical utility by analyzing a gene expression data of human brain to detect the time point when gene expression profiles begin to change, which has been reported to be closely related with aging brain.},
  archive      = {J_SIM},
  author       = {Jinjuan Wang and Na Li and Zhen Meng and Qizhai Li},
  doi          = {10.1002/sim.9881},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4644-4663},
  shortjournal = {Stat. Med.},
  title        = {Change point detection for high dimensional data via kernel measure with application to human aging brain data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible quasi-likelihood model for microbiome abundance
count data. <em>SIM</em>, <em>42</em>(25), 4632–4643. (<a
href="https://doi.org/10.1002/sim.9880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a flexible model for microbiome count data. We consider a quasi-likelihood framework, in which we do not make any assumptions on the distribution of the microbiome count except that its variance is an unknown but smooth function of the mean. By comparing our model to the negative binomial generalized linear model (GLM) and Poisson GLM in simulation studies, we show that our flexible quasi-likelihood method yields valid inferential results. Using a real microbiome study, we demonstrate the utility of our method by examining the relationship between adenomas and microbiota. We also provide an R package “fql” for the application of our method.},
  archive      = {J_SIM},
  author       = {Yiming Shi and Huilin Li and Chan Wang and Jun Chen and Hongmei Jiang and Ya-Chen T. Shih and Haixiang Zhang and Yizhe Song and Yang Feng and Lei Liu},
  doi          = {10.1002/sim.9880},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4632-4643},
  shortjournal = {Stat. Med.},
  title        = {A flexible quasi-likelihood model for microbiome abundance count data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A threshold longitudinal tobit quantile regression model for
identification of treatment-sensitive subgroups based on
interval-bounded longitudinal measurements and a continuous covariate.
<em>SIM</em>, <em>42</em>(25), 4618–4631. (<a
href="https://doi.org/10.1002/sim.9879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of a subgroup of patients who may be sensitive to a specific treatment is an important problem in precision medicine. This article considers the case where the treatment effect is assessed by longitudinal measurements, such as quality of life scores assessed over the duration of a clinical trial, and the subset is determined by a continuous baseline covariate, such as age and expression level of a biomarker. Recently, a linear mixed threshold regression model has been proposed but it assumes the longitudinal measurements are normally distributed. In many applications, longitudinal measurements, such as quality of life data obtained from answers to questions on a Likert scale, may be restricted in a fixed interval because of the floor and ceiling effects and, therefore, may be skewed. In this article, a threshold longitudinal Tobit quantile regression model is proposed and a computational approach based on alternating direction method of multipliers algorithm is developed for the estimation of parameters in the model. In addition, a random weighting method is employed to estimate the variances of the parameter estimators. The proposed procedures are evaluated through simulation studies and applications to the data from clinical trials.},
  archive      = {J_SIM},
  author       = {Zhanfeng Wang and Tao Li and Liqun Xiao and Dongsheng Tu},
  doi          = {10.1002/sim.9879},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4618-4631},
  shortjournal = {Stat. Med.},
  title        = {A threshold longitudinal tobit quantile regression model for identification of treatment-sensitive subgroups based on interval-bounded longitudinal measurements and a continuous covariate},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate likelihood and pseudo-likelihood inference in
meta-analysis of diagnostic accuracy studies accounting for disease
prevalence and study design. <em>SIM</em>, <em>42</em>(25), 4602–4617.
(<a href="https://doi.org/10.1002/sim.9878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bivariate random-effects models represent a recommended approach for meta-analysis of diagnostic test accuracy, jointly modeling study-specific sensitivity and specificity. As the severity of the disease status can vary across studies, a proper analysis should account for the dependence of the accuracy measures on the disease prevalence. To this aim, trivariate generalized linear mixed-effects models have been proposed in the literature, although computational difficulties strongly limit their applicability. In addition, the attention has been mainly paid to cohort studies, where the study-specific disease prevalence can be estimated from, while information from case-control studies is often neglected. To overcome such limits, this article introduces a trivariate approximate normal model, which accounts for disease prevalence along with accuracy measures in cohort studies and sensitivity and specificity in case-control studies. The model represents an extension of the bivariate normal mixed-effects model originally developed for meta-analysis not accounting for disease prevalence, under an approximate normal within-study distribution for the logit of estimated sensitivity and specificity. The components of the approximate within-study covariance matrix are derived and the likelihood function is obtained in closed-form. The approximate likelihood approach is compared to that based on the exact within-study distribution and to its modifications following a pseudo-likelihood strategy aimed at reducing the computational effort. The comparison is based on simulation studies in a variety of scenarios, and illustrated in a meta-analysis about the accuracy of a test to diagnose fungal infection and a meta-analysis of a noninvasive test to detect colorectal cancer.},
  archive      = {J_SIM},
  author       = {Annamaria Guolo},
  doi          = {10.1002/sim.9878},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4602-4617},
  shortjournal = {Stat. Med.},
  title        = {Approximate likelihood and pseudo-likelihood inference in meta-analysis of diagnostic accuracy studies accounting for disease prevalence and study design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selection of a statistical analysis method for the glasgow
outcome scale-extended endpoint for estimating the probability of
favorable outcome in future severe TBI clinical trials. <em>SIM</em>,
<em>42</em>(25), 4582–4601. (<a
href="https://doi.org/10.1002/sim.9877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Glasgow outcome scale-extended (GOS-E), an ordinal scale measure, is often selected as the endpoint for clinical trials of traumatic brain injury (TBI). Traditionally, GOS-E is analyzed as a fixed dichotomy with favorable outcome defined as GOS-E ≥ 5 and unfavorable outcome as GOS-E &lt; 5. More recent studies have defined favorable vs unfavorable outcome utilizing a sliding dichotomy of the GOS-E that defines a favorable outcome as better than a subject&#39;s predicted prognosis at baseline. Both dichotomous approaches result in loss of statistical and clinical information. To improve on power, Yeatts et al proposed a sliding scoring of the GOS-E as the distance from the cutoff for favorable/unfavorable outcomes, and therefore used more information found in the original GOS-E to estimate the probability of favorable outcome. We used data from a published TBI trial to explore the ramifications to trial operating characteristics by analyzing the sliding scoring of the GOS-E as either dichotomous, continuous, or ordinal. We illustrated a connection between the ordinal data and time-to-event (TTE) data to allow use of Bayesian software that utilizes TTE-based modeling. The simulation results showed that the continuous method with continuity correction offers higher power and lower mean squared error for estimating the probability of favorable outcome compared to the dichotomous method, and similar power but higher precision compared to the ordinal method. Therefore, we recommended that future severe TBI clinical trials consider analyzing the sliding scoring of the GOS-E endpoint as continuous with continuity correction.},
  archive      = {J_SIM},
  author       = {Yu Wang and Sharon D. Yeatts and Renee&#39; H. Martin and Robert Silbergleit and Gaylan L. Rockswold and William G. Barsan and Frederick K. Korley and Sarah Rockswold and Byron J. Gajewski},
  doi          = {10.1002/sim.9877},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4582-4601},
  shortjournal = {Stat. Med.},
  title        = {Selection of a statistical analysis method for the glasgow outcome scale-extended endpoint for estimating the probability of favorable outcome in future severe TBI clinical trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustering of trajectories with mixed effects classification
model: Inference taking into account classification uncertainties.
<em>SIM</em>, <em>42</em>(25), 4570–4581. (<a
href="https://doi.org/10.1002/sim.9876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying patient biomarker trajectories into groups has become frequent in clinical research. Mixed effects classification models can be used to model the heterogeneity of longitudinal data. The estimated parameters of typical trajectories and the partition can be provided by the classification version of the expectation maximization algorithm, named CEM. However, the variance of the parameter estimates obtained underestimates the true variance because classification uncertainties are not taken into account. This article takes into account these uncertainties by using the stochastic EM algorithm (SEM), a stochastic version of the CEM algorithm, after convergence of the CEM algorithm. The simulations showed correct coverage probabilities of the 95% confidence intervals (close to 95% except for scenarios with high bias in typical trajectories). The method was applied on a trial, called low-cyclo, that compared the effects of low vs standard cyclosporine A doses on creatinine levels after cardiac transplantation. It identified groups of patients for whom low-dose cyclosporine may be relevant, but with high uncertainty on the dose-effect estimate.},
  archive      = {J_SIM},
  author       = {Charlotte Dugourd and Amna Abichou-Klich and René Ecochard and Fabien Subtil},
  doi          = {10.1002/sim.9876},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4570-4581},
  shortjournal = {Stat. Med.},
  title        = {Clustering of trajectories with mixed effects classification model: Inference taking into account classification uncertainties},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shrinkage estimators of the spatial relative risk function.
<em>SIM</em>, <em>42</em>(25), 4556–4569. (<a
href="https://doi.org/10.1002/sim.9875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial relative risk function describes differences in the geographical distribution of two types of points, such as locations of cases and controls in an epidemiological study. It is defined as the ratio of the two underlying densities. Estimation of spatial relative risk is typically done using kernel estimates of these densities, but this procedure is often challenging in practice because of the high degree of spatial inhomogeneity in the distributions. This makes it difficult to obtain estimates of the relative risk that are stable in areas of sparse data while retaining necessary detail elsewhere, and consequently difficult to distinguish true risk hotspots from stochastic bumps in the risk function. We study shrinkage estimators of the spatial relative risk function to address these problems. In particular, we propose a new lasso-type estimator that shrinks a standard kernel estimator of the log-relative risk function towards zero. The shrinkage tuning parameter can be adjusted to help quantify the degree of evidence for the existence of risk hotspots, or selected to optimize a cross-validation criterion. The performance of the lasso estimator is encouraging both on a simulation study and on real-world examples.},
  archive      = {J_SIM},
  author       = {Martin L. Hazelton},
  doi          = {10.1002/sim.9875},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4556-4569},
  shortjournal = {Stat. Med.},
  title        = {Shrinkage estimators of the spatial relative risk function},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of the time of exposure based on interval and
censored data using the ε-accelerated EM algorithm. <em>SIM</em>,
<em>42</em>(25), 4542–4555. (<a
href="https://doi.org/10.1002/sim.9874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately estimating the timing of pathogen exposure plays a crucial role in outbreak control for emerging infectious diseases, including the source identification, contact tracing, and vaccine research and development. However, since surveillance activities often collect data retrospectively after symptoms have appeared, obtaining accurate data on the timing of disease onset is difficult in practice and can involve “coarse” observations, such as interval or censored data. To address this challenge, we propose a novel likelihood function, tailored to coarsely observed data in rapid outbreak surveillance, along with an optimization method based on an ε $$ \varepsilon $$ -accelerated EM algorithm for faster convergence to find maximum likelihood estimates (MLEs). The covariance matrix of MLEs is also discussed using a nonparametric bootstrap approach. In terms of bias and mean-squared error, the performance of our proposed method is evaluated through extensive numerical experiments, as well as its application to a series of epidemiological surveillance focused on cases of mass food poisoning. The experiments show that our method exhibits less bias than conventional methods, providing greater efficiency across all scenarios.},
  archive      = {J_SIM},
  author       = {Daisuke Yoneoka and Takayuki Kawashima and Yuta Tanoue and Shuhei Nomura and Akifumi Eguchi},
  doi          = {10.1002/sim.9874},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4542-4555},
  shortjournal = {Stat. Med.},
  title        = {Estimation of the time of exposure based on interval and censored data using the ε-accelerated EM algorithm},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confidence intervals for the cox model test error from
cross-validation. <em>SIM</em>, <em>42</em>(25), 4532–4541. (<a
href="https://doi.org/10.1002/sim.9873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-validation (CV) is one of the most widely used techniques in statistical learning for estimating the test error of a model, but its behavior is not yet fully understood. It has been shown that standard confidence intervals for test error using estimates from CV may have coverage below nominal levels. This phenomenon occurs because each sample is used in both the training and testing procedures during CV and as a result, the CV estimates of the errors become correlated. Without accounting for this correlation, the estimate of the variance is smaller than it should be. One way to mitigate this issue is by estimating the mean squared error of the prediction error instead using nested CV. This approach has been shown to achieve superior coverage compared to intervals derived from standard CV. In this work, we generalize the nested CV idea to the Cox proportional hazards model and explore various choices of test error for this setting.},
  archive      = {J_SIM},
  author       = {Min Woo Sun and Robert Tibshirani},
  doi          = {10.1002/sim.9873},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4532-4541},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals for the cox model test error from cross-validation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian design and analysis of two-arm cluster randomized
trials using assurance. <em>SIM</em>, <em>42</em>(25), 4517–4531. (<a
href="https://doi.org/10.1002/sim.9871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the design of a two-arm superiority cluster randomized controlled trial (RCT) with a continuous outcome. We detail Bayesian inference for the analysis of the trial using a linear mixed-effects model. The treatment is compared to control using the posterior distribution for the treatment effect. We develop the form of the assurance to choose the sample size based on this analysis, and its evaluation using a two loop Monte Carlo sampling scheme. We assess the proposed approach, considering the effect of different forms of prior distribution, and the number of Monte Carlo samples needed in both loops for accurate determination of the assurance and sample size. Based on this assessment, we provide general advice on each of these choices. We apply the approach to the choice of sample size for a cluster RCT into poststroke incontinence, and compare the resulting sample size to that from assurance based on a Wald test for the treatment effect. The Bayesian approach to design and analysis developed in this article can offer advantages in terms of an increase in the robustness of the chosen sample size to parameter mis-specification and reduced sample sizes if prior information indicates the treatment effect is likely to be larger than the minimal clinically important difference.},
  archive      = {J_SIM},
  author       = {Kevin J. Wilson},
  doi          = {10.1002/sim.9871},
  journal      = {Statistics in Medicine},
  month        = {11},
  number       = {25},
  pages        = {4517-4531},
  shortjournal = {Stat. Med.},
  title        = {Bayesian design and analysis of two-arm cluster randomized trials using assurance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring the performance of prediction models to
personalize treatment choice: Defining observed and predicted pairwise
treatment effects. <em>SIM</em>, <em>42</em>(24), 4514–4515. (<a
href="https://doi.org/10.1002/sim.9719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {David van Klaveren and Carolien C. H. M. Maas and David M. Kent},
  doi          = {10.1002/sim.9719},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4514-4515},
  shortjournal = {Stat. Med.},
  title        = {Measuring the performance of prediction models to personalize treatment choice: Defining observed and predicted pairwise treatment effects},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A guide to regression discontinuity designs in medical
applications. <em>SIM</em>, <em>42</em>(24), 4484–4513. (<a
href="https://doi.org/10.1002/sim.9861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a practical guide for the analysis of regression discontinuity (RD) designs in biomedical contexts. We begin by introducing key concepts, assumptions, and estimands within both the continuity-based framework and the local randomization framework. We then discuss modern estimation and inference methods within both frameworks, including approaches for bandwidth or local neighborhood selection, optimal treatment effect point estimation, and robust bias-corrected inference methods for uncertainty quantification. We also overview empirical falsification tests that can be used to support key assumptions. Our discussion focuses on two particular features that are relevant in biomedical research: (i) fuzzy RD designs, which often arise when therapeutic treatments are based on clinical guidelines, but patients with scores near the cutoff are treated contrary to the assignment rule; and (ii) RD designs with discrete scores, which are ubiquitous in biomedical applications. We illustrate our discussion with three empirical applications: the effect CD4 guidelines for anti-retroviral therapy on retention of HIV patients in South Africa, the effect of genetic guidelines for chemotherapy on breast cancer recurrence in the United States, and the effects of age-based patient cost-sharing on healthcare utilization in Taiwan. Complete replication materials employing publicly available data and statistical software in Python , R and Stata are provided, offering researchers all necessary tools to conduct an RD analysis.},
  archive      = {J_SIM},
  author       = {Matias D. Cattaneo and Luke Keele and Rocío Titiunik},
  doi          = {10.1002/sim.9861},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4484-4513},
  shortjournal = {Stat. Med.},
  title        = {A guide to regression discontinuity designs in medical applications},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forecasting emergency department waiting time using a state
space representation. <em>SIM</em>, <em>42</em>(24), 4458–4483. (<a
href="https://doi.org/10.1002/sim.9870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The provision of waiting time information in emergency departments (ED) has become an increasingly popular practice due to its positive impact on patient experience and ED demand management. However, little scientific attention has been given to the quality and quantity of waiting time information presented to patients. To improve both aspects, we propose a set of state space models with flexible error structures to forecast ED waiting time for low acuity patients. Our approach utilizes a Bayesian framework to generate uncertainties associated with the forecasts. We find that the state-space models with flexible error structures significantly improve forecast accuracy of ED waiting time compared to the benchmark, which is the rolling average model. Specifically, incorporating time-varying and correlated error terms reduces the root mean squared errors of the benchmark by 10%. Furthermore, treating zero-recorded waiting times as unobserved values improves forecast performance. Our proposed model has the ability to provide patient-centric waiting time information. By offering more accurate and informative waiting time information, our model can help patients make better informed decisions and ultimately enhance their ED experience.},
  archive      = {J_SIM},
  author       = {Kelly Trinh and Andrew Staib and Anton Pak},
  doi          = {10.1002/sim.9870},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4458-4483},
  shortjournal = {Stat. Med.},
  title        = {Forecasting emergency department waiting time using a state space representation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric probit regression model with misclassified
current status data. <em>SIM</em>, <em>42</em>(24), 4440–4457. (<a
href="https://doi.org/10.1002/sim.9869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current status data arise when each subject under study is examined only once at an observation time, and one only knows the failure status of the event of interest at the observation time rather than the exact failure time. Moreover, the obtained failure status is frequently subject to misclassification due to imperfect tests, yielding misclassified current status data. This article conducts regression analysis of such data with the semiparametric probit model, which serves as an important alternative to existing semiparametric models and has recently received considerable attention in failure time data analysis. We consider the nonparametric maximum likelihood estimation and develop an expectation-maximization (EM) algorithm by incorporating the generalized pool-adjacent-violators (PAV) algorithm to maximize the intractable likelihood function. The resulting estimators of regression parameters are shown to be consistent, asymptotically normal, and semiparametrically efficient. Furthermore, the numerical results in simulation studies indicate that the proposed method performs satisfactorily in finite samples and outperforms the naive method that ignores misclassification. We then apply the proposed method to a real dataset on chlamydia infection.},
  archive      = {J_SIM},
  author       = {Lijun Fang and Shuwei Li and Liuquan Sun and Xinyuan Song},
  doi          = {10.1002/sim.9869},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4440-4457},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric probit regression model with misclassified current status data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Federated causal inference in heterogeneous observational
data. <em>SIM</em>, <em>42</em>(24), 4418–4439. (<a
href="https://doi.org/10.1002/sim.9868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are interested in estimating the effect of a treatment applied to individuals at multiple sites, where data is stored locally for each site. Due to privacy constraints, individual-level data cannot be shared across sites; the sites may also have heterogeneous populations and treatment assignment mechanisms. Motivated by these considerations, we develop federated methods to draw inferences on the average treatment effects of combined data across sites. Our methods first compute summary statistics locally using propensity scores and then aggregate these statistics across sites to obtain point and variance estimators of average treatment effects. We show that these estimators are consistent and asymptotically normal. To achieve these asymptotic properties, we find that the aggregation schemes need to account for the heterogeneity in treatment assignments and in outcomes across sites. We demonstrate the validity of our federated methods through a comparative study of two large medical claims databases.},
  archive      = {J_SIM},
  author       = {Ruoxuan Xiong and Allison Koenecke and Michael Powell and Zhu Shen and Joshua T. Vogelstein and Susan Athey},
  doi          = {10.1002/sim.9868},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4418-4439},
  shortjournal = {Stat. Med.},
  title        = {Federated causal inference in heterogeneous observational data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of bayesian information borrowing methods in
basket trials and a novel proposal of modified
exchangeability-nonexchangeability method. <em>SIM</em>,
<em>42</em>(24), 4392–4417. (<a
href="https://doi.org/10.1002/sim.9867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent innovation in trial design to improve study efficiency has led to the development of basket trials in which a single therapeutic treatment is tested on several patient populations, each of which forms a basket. In a common setting, patients across all baskets share a genetic marker and as such, an assumption can be made that all patients may have a homogeneous response to treatments. Bayesian information borrowing procedures utilize this assumption to draw on information regarding the response in one basket when estimating the response rate in others. This can improve power and precision of estimates particularly in the presence of small sample sizes, however, can come at a cost of biased estimates and an inflation of error rates, bringing into question validity of trial conclusions. We review and compare the performance of several Bayesian borrowing methods, namely: the Bayesian hierarchical model (BHM), calibrated Bayesian hierarchical model (CBHM), exchangeability-nonexchangeability (EXNEX) model and a Bayesian model averaging procedure. A generalization of the CBHM is made to account for unequal sample sizes across baskets. We also propose a modification of the EXNEX model that allows for better control of a type I error. The proposed method uses a data-driven approach to account for the homogeneity of the response data, measured through Hellinger distances. Through an extensive simulation study motivated by a real basket trial, for both equal and unequal sample sizes across baskets, we show that in the presence of a basket with a heterogeneous response, unlike the other methods discussed, this model can control type I error rates to a nominal level whilst yielding improved power.},
  archive      = {J_SIM},
  author       = {Libby Daniells and Pavel Mozgunov and Alun Bedding and Thomas Jaki},
  doi          = {10.1002/sim.9867},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4392-4417},
  shortjournal = {Stat. Med.},
  title        = {A comparison of bayesian information borrowing methods in basket trials and a novel proposal of modified exchangeability-nonexchangeability method},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple bayesian models for missing binary outcomes in
randomized controlled trials. <em>SIM</em>, <em>42</em>(24), 4377–4391.
(<a href="https://doi.org/10.1002/sim.9866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing outcomes are commonly encountered in randomized controlled trials (RCT) involving human subjects and present a risk for substantial bias in the results of a complete case analysis. While response rates for RCTs are typically high there is no agreed upon universal threshold under which the amount of missing data is deemed to not be a threat to inference. We focus here on binary outcomes that are possibly missing not at random, that is, the value of the outcome influences its possibility of being observed. Salient information that can assist in addressing these missing outcomes in such situations is the anticipated response rate in each study arm; these can often be anticipated based on prior research in similar populations using similar designs and outcomes. Further, in some areas of human subjects research, we are often confident or we suspect that response rates among RCT participants with successful treatment outcomes will be at least as great as those among participants without successful treatment outcomes. In other settings we may suspect the opposite relationship. This direction of the differential response between those with successful and unsuccessful outcomes can further aid in addressing the missing outcomes. We present simple Bayesian pattern-mixture models that incorporate this information on response rates to analyze the relationship between a binary outcome and an intervention while addressing the missing outcomes. We assess the performance of this method in simulation studies and apply this method to the results of an RCT of a smoking abstinence intervention.},
  archive      = {J_SIM},
  author       = {Adam Kaplan and David Nelson},
  doi          = {10.1002/sim.9866},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4377-4391},
  shortjournal = {Stat. Med.},
  title        = {Simple bayesian models for missing binary outcomes in randomized controlled trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instrumental variable analysis for cost outcome: Application
to the effect of primary care visit on medical cost among low-income
adults. <em>SIM</em>, <em>42</em>(24), 4349–4376. (<a
href="https://doi.org/10.1002/sim.9865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical cost data often consist of zero values as well as extremely right-skewed positive values. A two-part model is a popular choice for analyzing medical cost data, where the first part models the probability of a positive cost using logistic regression and the second part models the positive cost using a lognormal or Gamma distribution. To address the unmeasured confounding in studies on cost outcome under two-part models, two instrumental variable (IV) methods, two-stage residual inclusion (2SRI) and two-stage prediction substitution (2SPS) are widely applied. However, previous literature demonstrated that both the 2SRI and the 2SPS could fail to consistently estimate the causal effect among compliers under standard IV assumptions for binary and survival outcomes. Our simulation studies confirmed that it continued to be the case for a two-part model, which is another nonlinear model. In this article, we develop a model-based IV approach, Instrumental Variable with Two-Part model (IV2P), to obtain a consistent estimate of the causal effect among compliers for cost outcome under standard IV assumptions. In addition, we develop sensitivity analysis approaches to allow the evaluation of the sensitivity of the causal conclusions to potential quantified violations of the exclusion restriction assumption and the randomization of IV assumption. We apply our method to a randomized cash incentive study to evaluate the effect of a primary care visit on medical cost among low-income adults newly covered by a primary care program.},
  archive      = {J_SIM},
  author       = {Dexiang Gao and Junxiao Hu and Cathy J. Bradley and Fan Yang},
  doi          = {10.1002/sim.9865},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4349-4376},
  shortjournal = {Stat. Med.},
  title        = {Instrumental variable analysis for cost outcome: Application to the effect of primary care visit on medical cost among low-income adults},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rank intraclass correlation for clustered data.
<em>SIM</em>, <em>42</em>(24), 4333–4348. (<a
href="https://doi.org/10.1002/sim.9864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered data are common in biomedical research. Observations in the same cluster are often more similar to each other than to observations from other clusters. The intraclass correlation coefficient (ICC), first introduced by R. A. Fisher, is frequently used to measure this degree of similarity. However, the ICC is sensitive to extreme values and skewed distributions, and depends on the scale of the data. It is also not applicable to ordered categorical data. We define the rank ICC as a natural extension of Fisher&#39;s ICC to the rank scale, and describe its corresponding population parameter. The rank ICC is simply interpreted as the rank correlation between a random pair of observations from the same cluster. We also extend the definition when the underlying distribution has more than two hierarchies. We describe estimation and inference procedures, show the asymptotic properties of our estimator, conduct simulations to evaluate its performance, and illustrate our method in three real data examples with skewed data, count data, and three-level ordered categorical data.},
  archive      = {J_SIM},
  author       = {Shengxin Tu and Chun Li and Donglin Zeng and Bryan E. Shepherd},
  doi          = {10.1002/sim.9864},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4333-4348},
  shortjournal = {Stat. Med.},
  title        = {Rank intraclass correlation for clustered data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of conditional power in the presence of auxiliary
data. <em>SIM</em>, <em>42</em>(24), 4319–4332. (<a
href="https://doi.org/10.1002/sim.9863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional power (CP) is a commonly used tool to inform interim decision-making in clinical trials, but the conventional approach using only primary endpoint data to calculate CP may not perform well when the primary endpoint requires a long follow-up period, or the treatment effect size changes during the trial. Several methods have been proposed to use additional short term auxiliary data observed at the interim analysis to improve the CP estimation in these situations, however, they may rely on strong assumptions, have limited applications, or use ad hoc choices of information fraction. In this paper we propose a general framework where the true CP formula is first derived in the presence of auxiliary data, and CP estimation is obtained by substituting the unknown parameters with consistent estimators. We conducted extensive simulations to examine the performance of both proposed and conventional approaches using the true CP as the benchmark. As the proposed approach is based on the true underlying CP, the simulations confirmed its superiority over the conventional approach in terms of efficiency and accuracy, especially if observed auxiliary data reflect the change of treatment effect size. The simulations also indicate that the magnitude of improvement in CP estimation is associated with the correlation between auxiliary and primary endpoints and/or the magnitude of the effect size change during the trial.},
  archive      = {J_SIM},
  author       = {Xin Li and Godwin Yung and Jianchang Lin and Jian Zhu},
  doi          = {10.1002/sim.9863},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4319-4332},
  shortjournal = {Stat. Med.},
  title        = {Estimation of conditional power in the presence of auxiliary data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unified semicompeting risks analysis of hepatitis natural
history through mediation modeling. <em>SIM</em>, <em>42</em>(24),
4301–4318. (<a href="https://doi.org/10.1002/sim.9862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural history of hepatitis B or C is comprised of multiple milestones such as liver cirrhosis and liver cancer. To fully characterize its natural course, semicompeting risks represent a common problem where liver cirrhosis and liver cancer are both of interest, but only the former may be censored by the latter. Copula, frailty and multistate models serve as well-established analytics for semicompeting risks. Here, we cast the semicompeting risks in a mediation framework, with liver cirrhosis as a mediator and liver cancer as an outcome. We define the indirect and direct effects as the effects of an exposure on the liver cancer incidence mediated and not mediated through liver cirrhosis, respectively. With the estimands derived as conditional probabilities, we derive respective expressions under the copula, frailty, and multistate models. Next, we propose estimators based on nonparametric maximum likelihood or U -statistics and establish their asymptotic results. Numerical studies demonstrate that the efficiency of copula models leads to potential bias due to model misspecification. Moreover, the robustness of frailty models is accompanied by a loss in efficiency, and multistate models balance the efficiency and robustness. We demonstrate the utility of the proposed methods by a hepatitis study, showing that hepatitis B and C lead to a higher incidence of liver cancer by increasing liver cirrhosis incidence. Thus, mediation modeling provides a unified framework that accommodates various semicompeting risks models.},
  archive      = {J_SIM},
  author       = {Jih-Chang Yu and Yen-Tsung Huang},
  doi          = {10.1002/sim.9862},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {24},
  pages        = {4301-4318},
  shortjournal = {Stat. Med.},
  title        = {Unified semicompeting risks analysis of hepatitis natural history through mediation modeling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to “estimation of treatment effect in 2-in-1
adaptive design and some of its extensions.” <em>SIM</em>,
<em>42</em>(23), 4299. (<a
href="https://doi.org/10.1002/sim.9849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.9849},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4299},
  shortjournal = {Stat. Med.},
  title        = {Correction to “estimation of treatment effect in 2-in-1 adaptive design and some of its extensions”},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accounting for nonmonotone missing data using inverse
probability weighting. <em>SIM</em>, <em>42</em>(23), 4282–4298. (<a
href="https://doi.org/10.1002/sim.9860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability weighting can be used to correct for missing data. New estimators for the weights in the nonmonotone setting were introduced in 2018. These estimators are the unconstrained maximum likelihood estimator (UMLE) and the constrained Bayesian estimator (CBE), an alternative if UMLE fails to converge. In this work we describe and illustrate these estimators, and examine performance in simulation and in an applied example estimating the effect of anemia on spontaneous preterm birth in the Zambia Preterm Birth Prevention Study. We compare performance with multiple imputation (MI) and focus on the setting of an observational study where inverse probability of treatment weights are used to address confounding. In simulation, weighting was less statistically efficient at the smallest sample size and lowest exposure prevalence examined (n = 1500, 15% respectively) but in other scenarios statistical performance of weighting and MI was similar. Weighting had improved computational efficiency taking, on average, 0.4 and 0.05 times the time for MI in R and SAS, respectively. UMLE was easy to implement in commonly used software and convergence failure occurred just twice in &gt;200 000 simulated cohorts making implementation of CBE unnecessary. In conclusion, weighting is an alternative to MI for nonmonotone missingness, though MI performed as well as or better in terms of bias and statistical efficiency. Weighting&#39;s superior computational efficiency may be preferred with large sample sizes or when using resampling algorithms. As validity of weighting and MI rely on correct specification of different models, both approaches could be implemented to check agreement of results.},
  archive      = {J_SIM},
  author       = {Rachael K. Ross and Stephen R. Cole and Jessie K. Edwards and Daniel Westreich and Julie L. Daniels and Jeffrey S.A. Stringer},
  doi          = {10.1002/sim.9860},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4282-4298},
  shortjournal = {Stat. Med.},
  title        = {Accounting for nonmonotone missing data using inverse probability weighting},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensitivity analysis of g-estimators to invalid instrumental
variables. <em>SIM</em>, <em>42</em>(23), 4257–4281. (<a
href="https://doi.org/10.1002/sim.9859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variables regression is a tool that is commonly used in the analysis of observational data. The instrumental variables are used to make causal inference about the effect of a certain exposure in the presence of unmeasured confounders. A valid instrumental variable is a variable that is associated with the exposure, affects the outcome only through the exposure (exclusion), and is not confounded with the outcome (exogeneity). Unlike the first assumption, the other two are generally untestable and rely on subject-matter knowledge. Therefore, a sensitivity analysis is desirable to assess the impact of assumptions&#39; violation on the estimated parameters. In this paper, we propose and demonstrate a new method of sensitivity analysis for G-estimators in causal linear and non-linear models. We introduce two novel aspects of sensitivity analysis in instrumental variables studies. The first is a single sensitivity parameter that captures violations of exclusion and exogeneity assumptions. The second is an application of the method to non-linear models. The introduced framework is theoretically justified and is illustrated via a simulation study. Finally, we illustrate the method by application to real-world data and provide guidelines on conducting sensitivity analysis.},
  archive      = {J_SIM},
  author       = {Valentin Vancak and Arvid Sjölander},
  doi          = {10.1002/sim.9859},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4257-4281},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analysis of G-estimators to invalid instrumental variables},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation and evaluation of individualized treatment rules
following multiple imputation. <em>SIM</em>, <em>42</em>(23), 4236–4256.
(<a href="https://doi.org/10.1002/sim.9857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An individualized treatment rule (ITR) is a function that inputs patient-level information and outputs a recommended treatment. An important focus of precision medicine is to develop optimal ITRs that maximize a population-level distributional summary. However, guidance for estimating and evaluating optimal ITRs in the presence of missing data is limited. Our work is motivated by the Social Incentives to Encourage Physical Activity and Understand Predictors (STEP UP) study. Participants were randomized to a control or one of three interventions designed to increase physical activity and were given wearable devices to record daily steps as a measure of physical activity. Many participants were missing at least one daily step count during the study period. In the primary analysis of the STEP UP trial, multiple imputation (MI) was used to address missingness in daily step counts. Despite ubiquitous use of MI in practice, it has been given relatively little attention in the context of personalized medicine. We fill this gap by describing two frameworks for estimation and evaluation of an optimal ITR following MI and assessing their performance using simulated data. One framework relies on splitting the data into independent training and testing sets for estimation and evaluation, respectively. The other framework estimates an optimal ITR using the full data and constructs an -out-of- bootstrap confidence interval to evaluate its performance. Finally, we provide an illustrative analysis to estimate and evaluate an optimal ITR from the STEP UP data with a focus on practical considerations such as choosing the number of imputations.},
  archive      = {J_SIM},
  author       = {Jenny Shen and Rebecca A. Hubbard and Kristin A. Linn},
  doi          = {10.1002/sim.9857},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4236-4256},
  shortjournal = {Stat. Med.},
  title        = {Estimation and evaluation of individualized treatment rules following multiple imputation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lean additive frailty model: With an application to
clustering of melanoma in norwegian families. <em>SIM</em>,
<em>42</em>(23), 4207–4235. (<a
href="https://doi.org/10.1002/sim.9856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Additive frailty models are used to model correlated survival data. However, the complexity of the models increases with cluster size to the extent that practical usage becomes increasingly challenging. We present a modification of the additive genetic gamma frailty (AGGF) model, the lean AGGF (L-AGGF) model, which alleviates some of these challenges by using a leaner additive decomposition of the frailty. The performances of the models were compared and evaluated in a simulation study. The L-AGGF model was used to analyze population-wide data on clustering of melanoma in 2 391 125 two-generational Norwegian families, 1960-2015. Using this model, we could analyze the complete data set, while the original model limited the analysis to a restricted data set (with cluster sizes ). We found a substantial clustering of melanoma in Norwegian families and large heterogeneity in melanoma risk across the population, where 52% of the frailty was attributed to the 10% of the population at highest unobserved risk. Due to the improved scalability, the L-AGGF model enables a wider range of analyses of population-wide data compared to the AGGF model. Moreover, the methods outlined here make it possible to perform these analyses in a computationally efficient manner.},
  archive      = {J_SIM},
  author       = {Mari Brathovde and Tron A. Moger and Odd O. Aalen and Tom Grotmol and Marit B. Veierød and Morten Valberg},
  doi          = {10.1002/sim.9856},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4207-4235},
  shortjournal = {Stat. Med.},
  title        = {A lean additive frailty model: With an application to clustering of melanoma in norwegian families},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A time-dependent poisson-gamma model for recruitment
forecasting in multicenter studies. <em>SIM</em>, <em>42</em>(23),
4193–4206. (<a href="https://doi.org/10.1002/sim.9855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting recruitments is a key component of the monitoring phase of multicenter studies. One of the most popular techniques in this field is the Poisson-Gamma recruitment model, a Bayesian technique built on a doubly stochastic Poisson process. This approach is based on the modeling of enrollments as a Poisson process where the recruitment rates are assumed to be constant over time and to follow a common Gamma prior distribution. However, the constant-rate assumption is a restrictive limitation that is rarely appropriate for applications in real studies. In this paper, we illustrate a flexible generalization of this methodology which allows the enrollment rates to vary over time by modeling them through B-splines. We show the suitability of this approach for a wide range of recruitment behaviors in a simulation study and by estimating the recruitment progression of the Canadian Co-infection Cohort.},
  archive      = {J_SIM},
  author       = {Armando Turchetta and Nicolas Savy and David A. Stephens and Erica E.M. Moodie and Marina B. Klein},
  doi          = {10.1002/sim.9855},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4193-4206},
  shortjournal = {Stat. Med.},
  title        = {A time-dependent poisson-gamma model for recruitment forecasting in multicenter studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hypothesis testing procedure for binary and multi-class
f1-scores in the paired design. <em>SIM</em>, <em>42</em>(23),
4177–4192. (<a href="https://doi.org/10.1002/sim.9853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern medicine, medical tests are used for various purposes including diagnosis, disease screening, prognosis, and risk prediction. To quantify the performance of the binary medical test, we often use sensitivity, specificity, and negative and positive predictive values as measures. Additionally, the F 1 $$ {F}_1 $$ -score, which is defined as the harmonic mean of precision (positive predictive value) and recall (sensitivity), has come to be used in the medical field due to its favorable characteristics. The -score has been extended for multi-class classification, and two types of -scores have been proposed for multi-class classification: a micro-averaged -score and a macro-averaged -score. The micro-averaged -score pools per-sample classifications across classes and then calculates the overall -score, whereas the macro-averaged -score computes an arithmetic mean of the -scores for each class. Additionally, Sokolova and Lapalme gave an alternative definition of the macro-averaged -score as the harmonic mean of the arithmetic means of the precision and recall over classes. Although some statistical methods of inference for binary and multi-class -scores have been proposed, the methodology development of hypothesis testing procedure for them has not been fully progressing yet. Therefore, we aim to develop hypothesis testing procedure for comparing two -scores in paired study design based on the large sample multivariate central limit theorem.},
  archive      = {J_SIM},
  author       = {Kanae Takahashi and Kouji Yamamoto and Aya Kuchiba and Ayumi Shintani and Tatsuki Koyama},
  doi          = {10.1002/sim.9853},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4177-4192},
  shortjournal = {Stat. Med.},
  title        = {Hypothesis testing procedure for binary and multi-class f1-scores in the paired design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parametric and nonparametric propensity score estimation in
multilevel observational studies. <em>SIM</em>, <em>42</em>(23),
4147–4176. (<a href="https://doi.org/10.1002/sim.9852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been growing interest in using nonparametric machine learning approaches for propensity score estimation in order to foster robustness against misspecification of the propensity score model. However, the vast majority of studies focused on single-level data settings, and research on nonparametric propensity score estimation in clustered data settings is scarce. In this article, we extend existing research by describing a general algorithm for incorporating random effects into a machine learning model, which we implemented for generalized boosted modeling (GBM). In a simulation study, we investigated the performance of logistic regression, GBM, and Bayesian additive regression trees for inverse probability of treatment weighting (IPW) when the data are clustered, the treatment exposure mechanism is nonlinear, and unmeasured cluster-level confounding is present. For each approach, we compared fixed and random effects propensity score models to single-level models and evaluated their use in both marginal and clustered IPW. We additionally investigated the performance of the standard Super Learner and the balance Super Learner. The results showed that when there was no unmeasured confounding, logistic regression resulted in moderate bias in both marginal and clustered IPW, whereas the nonparametric approaches were unbiased. In presence of cluster-level confounding, fixed and random effects models greatly reduced bias compared to single-level models in marginal IPW, with fixed effects GBM and fixed effects logistic regression performing best. Finally, clustered IPW was overall preferable to marginal IPW and the balance Super Learner outperformed the standard Super Learner, though neither worked as well as their best candidate model.},
  archive      = {J_SIM},
  author       = {Marie Salditt and Steffen Nestler},
  doi          = {10.1002/sim.9852},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4147-4176},
  shortjournal = {Stat. Med.},
  title        = {Parametric and nonparametric propensity score estimation in multilevel observational studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical modeling of diabetic neuropathy: Exploring the
dynamics of nerve mortality. <em>SIM</em>, <em>42</em>(23), 4128–4146.
(<a href="https://doi.org/10.1002/sim.9851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic neuropathy is a disorder characterized by impaired nerve function and reduction of the number of epidermal nerve fibers per epidermal surface. Additionally, as neuropathy related nerve fiber loss and regrowth progresses over time, the two-dimensional spatial arrangement of the nerves becomes more clustered. These observations suggest that with development of neuropathy, the spatial pattern of diminished skin innervation is defined by a thinning process which remains incompletely characterized. We regard samples obtained from healthy controls and subjects suffering from diabetic neuropathy as realisations of planar point processes consisting of nerve entry points and nerve endings, and propose point process models based on spatial thinning to describe the change as neuropathy advances. Initially, the hypothesis that the nerve removal occurs completely at random is tested using independent random thinning of healthy patterns. Then, a dependent parametric thinning model that favors the removal of isolated nerve trees is proposed. Approximate Bayesian computation is used to infer the distribution of the model parameters, and the goodness-of-fit of the models is evaluated using both non-spatial and spatial summary statistics. Our findings suggest that the nerve mortality process changes as neuropathy advances.},
  archive      = {J_SIM},
  author       = {Konstantinos Konstantinou and Farnaz Ghorbanpour and Umberto Picchini and Adam Loavenbruck and Aila Särkkä},
  doi          = {10.1002/sim.9851},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4128-4146},
  shortjournal = {Stat. Med.},
  title        = {Statistical modeling of diabetic neuropathy: Exploring the dynamics of nerve mortality},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the integration of decision trees with mixture cure
model. <em>SIM</em>, <em>42</em>(23), 4111–4127. (<a
href="https://doi.org/10.1002/sim.9850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mixture cure model is widely used to analyze survival data in the presence of a cured subgroup. Standard logistic regression-based approaches to model the incidence may lead to poor predictive accuracy of cure, specifically when the covariate effect is non-linear. Supervised machine learning techniques can be used as a better classifier than the logistic regression due to their ability to capture non-linear patterns in the data. However, the problem of interpret-ability hangs in the balance due to the trade-off between interpret-ability and predictive accuracy. We propose a new mixture cure model where the incidence part is modeled using a decision tree-based classifier and the proportional hazards structure for the latency part is preserved. The proposed model is very easy to interpret, closely mimics the human decision-making process, and provides flexibility to gauge both linear and non-linear covariate effects. For the estimation of model parameters, we develop an expectation maximization algorithm. A detailed simulation study shows that the proposed model outperforms the logistic regression-based and spline regression-based mixture cure models, both in terms of model fitting and evaluating predictive accuracy. An illustrative example with data from a leukemia study is presented to further support our conclusion.},
  archive      = {J_SIM},
  author       = {Wisdom Aselisewine and Suvra Pal},
  doi          = {10.1002/sim.9850},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4111-4127},
  shortjournal = {Stat. Med.},
  title        = {On the integration of decision trees with mixture cure model},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inference for covariate-adjusted time-dependent prognostic
accuracy measures. <em>SIM</em>, <em>42</em>(23), 4082–4110. (<a
href="https://doi.org/10.1002/sim.9848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the prognostic performance of candidate markers for future disease onset or progression is one of the major goals in medical research. A marker&#39;s prognostic performance refers to how well it separates patients at the high or low risk of a future disease state. Often the discriminative performance of a marker is affected by the patient characteristics (covariates). Time-dependent receiver operating characteristic (ROC) curves that ignore the informativeness of the covariates will lead to biased estimates of the accuracy parameters. We propose a time-dependent ROC curve that accounts for the informativeness of the covariates in the case of censored data. We propose inverse probability weighted (IPW) estimators for estimating the proposed accuracy parameters. We investigate the performance of the IPW estimators through simulation studies and real-life data analysis.},
  archive      = {J_SIM},
  author       = {Rajib Dey and J. A. Hanley and P. Saha-Chaudhuri},
  doi          = {10.1002/sim.9848},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4082-4110},
  shortjournal = {Stat. Med.},
  title        = {Inference for covariate-adjusted time-dependent prognostic accuracy measures},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clayton copula for survival data with dependent censoring:
An application to a tuberculosis treatment adherence data. <em>SIM</em>,
<em>42</em>(23), 4057–4081. (<a
href="https://doi.org/10.1002/sim.9858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ignoring the presence of dependent censoring in data analysis can lead to biased estimates, for example, not considering the effect of abandonment of the tuberculosis treatment may influence inferences about the cure probability. In order to assess the relationship between cure and abandonment outcomes, we propose a copula Bayesian approach. Therefore, the main objective of this work is to introduce a Bayesian survival regression model, capable of taking into account the dependent censoring in the adjustment. So, this proposed approach is based on Clayton&#39;s copula, to provide the relation between survival and dependent censoring times. In addition, the Weibull and the piecewise exponential marginal distributions are considered in order to fit the times. A simulation study is carried out to perform comparisons between different scenarios of dependence, different specifications of prior distributions, and comparisons with the maximum likelihood inference. Finally, we apply the proposed approach to a tuberculosis treatment adherence dataset of an HIV cohort from Alvorada-RS, Brazil. Results show that cure and abandonment outcomes are negatively correlated, that is, as long as the chance of abandoning the treatment increases, the chance of tuberculosis cure decreases.},
  archive      = {J_SIM},
  author       = {Silvana Schneider and Rodrigo Citton P. dos Reis and Maicon M. F. Gottselig and Patrícia Fisch and Daniela Riva Knauth and Álvaro Vigo},
  doi          = {10.1002/sim.9858},
  journal      = {Statistics in Medicine},
  month        = {10},
  number       = {23},
  pages        = {4057-4081},
  shortjournal = {Stat. Med.},
  title        = {Clayton copula for survival data with dependent censoring: An application to a tuberculosis treatment adherence data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A corrected smoothed score approach for semiparametric
accelerated failure time model with error-contaminated covariates.
<em>SIM</em>, <em>42</em>(22), 4043–4055. (<a
href="https://doi.org/10.1002/sim.9847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the semiparametric accelerated failure time (AFT) model with multiple covariates measured with error. Existing methods for the AFT model are either inconsistent, computationally intensive, or require stringent assumptions. To overcome these limitations, we develop a correction approach for a general smooth function of error-contaminated variables. We apply this method to the smoothed rank-based score function for the AFT model. The estimator is consistent and asymptotically normal. The finite-sample performance of the method is assessed by simulation studies. The approach is illustrated by application to data from an HIV clinical trial.},
  archive      = {J_SIM},
  author       = {Xiao Song},
  doi          = {10.1002/sim.9847},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4043-4055},
  shortjournal = {Stat. Med.},
  title        = {A corrected smoothed score approach for semiparametric accelerated failure time model with error-contaminated covariates},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring causal mechanisms and quantifying direct and
indirect effects using a joint modeling approach for recurrent and
terminal events. <em>SIM</em>, <em>42</em>(22), 4028–4042. (<a
href="https://doi.org/10.1002/sim.9846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events are commonly encountered in biomedical studies. In many situations, there exist terminal events, such as death, which are potentially related to the recurrent events. Joint models of recurrent and terminal events have been proposed to address the correlation between recurrent events and terminal events. However, there is a dearth of suitable methods to rigorously investigate the causal mechanisms between specific exposures, recurrent events, and terminal events. For example, it is of interest to know how much of the total effect of the primary exposure of interest on the terminal event is through the recurrent events, and whether preventing recurrent event occurrences could lead to better overall survival. In this work, we propose a formal causal mediation analysis method to compute the natural direct and indirect effects. A novel joint modeling approach is used to take the recurrent event process as the mediator and the survival endpoint as the outcome. This new joint modeling approach allows us to relax the commonly used “sequential ignorability” assumption. Simulation studies show that our new model has good finite sample performance in estimating both model parameters and mediation effects. We apply our method to an AIDS study to evaluate how much of the comparative effectiveness of the two treatments and the effect of CD4 counts on the overall survival are mediated by recurrent opportunistic infections.},
  archive      = {J_SIM},
  author       = {Fang Niu and Cheng Zheng and Lei Liu},
  doi          = {10.1002/sim.9846},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4028-4042},
  shortjournal = {Stat. Med.},
  title        = {Exploring causal mechanisms and quantifying direct and indirect effects using a joint modeling approach for recurrent and terminal events},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Significance tests for covariates in the diagnostic accuracy
index of a biomarker against a continuous gold standard. <em>SIM</em>,
<em>42</em>(22), 4015–4027. (<a
href="https://doi.org/10.1002/sim.9845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Receiver operating characteristic (ROC) curve is a popular tool to describe and compare the diagnostic accuracy of biomarkers when a binary-scale gold standard is available. However, there are many examples of diagnostic tests whose gold standards are continuous. Hence, Several extensions of receiver operating characteristic (ROC) curve are proposed to evaluate the diagnostic potential of biomarkers when the gold standard is continuous-scale. Moreover, in evaluating these biomarkers, it is often necessary to consider the effects of covariates on the diagnostic accuracy of the biomarker of interest. Covariates may include subject characteristics, expertise of the test operator, test procedures or aspects of specimen handling. Applying the covariate adjustment to the case that the gold standard is continuous is challenging and has not been addressed in the literature. To fill the gap, we propose two general testing frameworks to account for the covariates effect on diagnostic accuracy. Simulation studies are conducted to compare the proposed tests. Data from a study that assessed three types of imaging modalities with the purpose of detecting neoplastic colon polyps and cancers are used to illustrate the proposed methods.},
  archive      = {J_SIM},
  author       = {Mixia Wu and Xian Sun and Aiyi Liu and Chenchen Peng and Zhaohai Li},
  doi          = {10.1002/sim.9845},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {4015-4027},
  shortjournal = {Stat. Med.},
  title        = {Significance tests for covariates in the diagnostic accuracy index of a biomarker against a continuous gold standard},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Describing complex disease progression using joint latent
class models for multivariate longitudinal markers and clinical
endpoints. <em>SIM</em>, <em>42</em>(22), 3996–4014. (<a
href="https://doi.org/10.1002/sim.9844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodegenerative diseases are characterized by numerous markers of progression and clinical endpoints. For instance, multiple system atrophy (MSA), a rare neurodegenerative synucleinopathy, is characterized by various combinations of progressive autonomic failure and motor dysfunction, and a very poor prognosis. Describing the progression of such complex and multi-dimensional diseases is particularly difficult. One has to simultaneously account for the assessment of multivariate markers over time, the occurrence of clinical endpoints, and a highly suspected heterogeneity between patients. Yet, such description is crucial for understanding the natural history of the disease, staging patients diagnosed with the disease, unravelling subphenotypes, and predicting the prognosis. Through the example of MSA progression, we show how a latent class approach modeling multiple repeated markers and clinical endpoints can help describe complex disease progression and identify subphenotypes for exploring new pathological hypotheses. The proposed joint latent class model includes class-specific multivariate mixed models to handle multivariate repeated biomarkers possibly summarized into latent dimensions and class-and-cause-specific proportional hazard models to handle time-to-event data. Maximum likelihood estimation procedure, validated through simulations is available in the lcmm R package. In the French MSA cohort comprising data of 598 patients during up to 13 years, five subphenotypes of MSA were identified that differ by the sequence and shape of biomarkers degradation, and the associated risk of death. In posterior analyses, the five subphenotypes were used to explore the association between clinical progression and external imaging and fluid biomarkers, while properly accounting for the uncertainty in the subphenotypes membership.},
  archive      = {J_SIM},
  author       = {Cécile Proust-Lima and Tiphaine Saulnier and Viviane Philipps and Anne Pavy-Le Traon and Patrice Péran and Olivier Rascol and Wassilios G. Meissner and Alexandra Foubert-Samier},
  doi          = {10.1002/sim.9844},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3996-4014},
  shortjournal = {Stat. Med.},
  title        = {Describing complex disease progression using joint latent class models for multivariate longitudinal markers and clinical endpoints},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rematching on-the-fly: Sequential matched randomization and
a case for covariate-adjusted randomization. <em>SIM</em>,
<em>42</em>(22), 3981–3995. (<a
href="https://doi.org/10.1002/sim.9843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-adjusted randomization (CAR) can reduce the risk of covariate imbalance and, when accounted for in analysis, increase the power of a trial. Despite CAR advances, stratified randomization remains the most common CAR method. Matched randomization (MR) randomizes treatment assignment within optimally identified matched pairs based on covariates and a distance matrix. When participants enroll sequentially, sequentially matched randomization (SMR) randomizes within matches found “on-the-fly” to meet a pre-specified matching threshold. However, pre-specifying the ideal threshold can be challenging and SMR yields less-optimal matches than MR. We extend SMR to allow multiple participants to be randomized simultaneously, to use a dynamic threshold, and to allow matches to break and rematch if a better match later enrolls (sequential rematched randomization; SRR). In simplified settings and a real-world application, we assess whether these extensions improve covariate balance, estimator/study efficiency, and optimality of matches. We investigate whether adjusting for more covariates can be detrimental upon covariate balance and efficiency as is the case of traditional stratified randomization. As secondary objectives, we use the case study to assess how SMR schemes compare side-by-side with common and related CAR schemes and whether adjusting for covariates in the design can be as powerful as adjusting for covariates in a parametric model. We find each SMR extension, individually and collectively, to improve covariate balance, estimator efficiency, study power, and quality of matches. We provide a case-study where CAR schemes with randomization-based inference can be as and more powerful than non-CAR schemes with parametric adjustment for covariates.},
  archive      = {J_SIM},
  author       = {Jonathan J. Chipman and Lindsay Mayberry and Robert A. Greevy},
  doi          = {10.1002/sim.9843},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3981-3995},
  shortjournal = {Stat. Med.},
  title        = {Rematching on-the-fly: Sequential matched randomization and a case for covariate-adjusted randomization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prior elicitation for gaussian spatial process: An
application to TMS brain mapping. <em>SIM</em>, <em>42</em>(22),
3956–3980. (<a href="https://doi.org/10.1002/sim.9842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power and commensurate prior distributions are informative prior distributions that incorporate historical data as prior knowledge in Bayesian analysis to improve inference about a phenomenon under study. Although these distributions have been developed for analyzing non-spatial data, little or no attention has been given to spatial geostatistical data. In this study, we extend these informative prior distributions to a Gaussian spatial process, which enables the elicitation of prior knowledge from historical geostatistical data for Bayesian analysis. Three informative prior distributions were developed for spatial modeling, and an efficient Markov Chain Monte Carlo algorithm was developed for performing Bayesian analysis. Simulation studies were used to assess the adequacy of the informative prior distributions. Hierarchical models combined with the developed informative prior distributions were applied to analyze transcranial magnetic stimulation (TMS) brain mapping data to gain insights into the spatial pattern of a patient&#39;s response to motor cortex stimulation. The study quantified the uncertainty in motor response and found that the primary motor cortex of the hand is responsible for most of the movement of the right first dorsal interosseous muscle. The findings provide a deeper understanding of the neural mechanisms underlying motor function and ultimately aid the improvement of treatment options for individuals with health issues.},
  archive      = {J_SIM},
  author       = {Osafu Augustine Egbon and Diego Nascimento and Francisco Louzada},
  doi          = {10.1002/sim.9842},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3956-3980},
  shortjournal = {Stat. Med.},
  title        = {Prior elicitation for gaussian spatial process: An application to TMS brain mapping},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-effectiveness analysis under multiple effectiveness
outcomes: A probabilistic approach. <em>SIM</em>, <em>42</em>(22),
3936–3955. (<a href="https://doi.org/10.1002/sim.9841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probability based criteria are proposed for the assessment of cost-effectiveness of a new treatment compared to a standard treatment when there are multiple effectiveness measures. Depending on the preferences of a policy maker, there are several options to define such criteria. Two such metrics are investigated in detail. One metric is defined as the conditional probability that a new treatment is more effective with respect to the multiple effectiveness measures for patients having lower costs under the new treatment. A second metric is defined as the conditional probability that a new treatment is less costly for patients having greater health benefits under the new treatment. The metrics offer considerable flexibility to a policy maker as thresholds for cost and effectiveness can be incorporated into the metrics. Parametric confidence limits are developed using a percentile bootstrap approach assuming multivariate normality for the joint distribution of the log(cost) and effectiveness measures. A non-parametric estimation procedure is also developed using the theory of U-statistics. Numerical results indicate that the proposed confidence limits accurately maintain coverage probabilities. The methodologies are illustrated using a study on the treatment of type two diabetes. Code implementing the proposed methods are provided in the supporting information.},
  archive      = {J_SIM},
  author       = {Aryana Arsham and Ionut Bebu and Thomas Mathew},
  doi          = {10.1002/sim.9841},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3936-3955},
  shortjournal = {Stat. Med.},
  title        = {Cost-effectiveness analysis under multiple effectiveness outcomes: A probabilistic approach},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariate adjustment in randomized clinical trials with
missing covariate and outcome data. <em>SIM</em>, <em>42</em>(22),
3919–3935. (<a href="https://doi.org/10.1002/sim.9840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When analyzing data from randomized clinical trials, covariate adjustment can be used to account for chance imbalance in baseline covariates and to increase precision of the treatment effect estimate. A practical barrier to covariate adjustment is the presence of missing data. In this article, in the light of recent theoretical advancement, we first review several covariate adjustment methods with incomplete covariate data. We investigate the implications of the missing data mechanism on estimating the average treatment effect in randomized clinical trials with continuous or binary outcomes. In parallel, we consider settings where the outcome data are fully observed or are missing at random; in the latter setting, we propose a full weighting approach that combines inverse probability weighting for adjusting missing outcomes and overlap weighting for covariate adjustment. We highlight the importance of including the interaction terms between the missingness indicators and covariates as predictors in the models. We conduct comprehensive simulation studies to examine the finite-sample performance of the proposed methods and compare with a range of common alternatives. We find that conducting the proposed adjustment methods generally improves the precision of treatment effect estimates regardless of the imputation methods when the adjusted covariate is associated with the outcome. We apply the methods to the Childhood Adenotonsillectomy Trial to assess the effect of adenotonsillectomy on neurocognitive functioning scores.},
  archive      = {J_SIM},
  author       = {Chia-Rui Chang and Yue Song and Fan Li and Rui Wang},
  doi          = {10.1002/sim.9840},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3919-3935},
  shortjournal = {Stat. Med.},
  title        = {Covariate adjustment in randomized clinical trials with missing covariate and outcome data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Varying-coefficients for regional quantile via KNN-based
LASSO with applications to health outcome study. <em>SIM</em>,
<em>42</em>(22), 3903–3918. (<a
href="https://doi.org/10.1002/sim.9839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Health outcomes, such as body mass index and cholesterol levels, are known to be dependent on age and exhibit varying effects with their associated risk factors. In this paper, we propose a novel framework for dynamic modeling of the associations between health outcomes and risk factors using varying-coefficients (VC) regional quantile regression via K-nearest neighbors (KNN) fused Lasso, which captures the time-varying effects of age. The proposed method has strong theoretical properties, including a tight estimation error bound and the ability to detect exact clustered patterns under certain regularity conditions. To efficiently solve the resulting optimization problem, we develop an alternating direction method of multipliers (ADMM) algorithm. Our empirical results demonstrate the efficacy of the proposed method in capturing the complex age-dependent associations between health outcomes and their risk factors.},
  archive      = {J_SIM},
  author       = {Seyoung Park and Eun Ryung Lee and Hyokyoung G. Hong},
  doi          = {10.1002/sim.9839},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {22},
  pages        = {3903-3918},
  shortjournal = {Stat. Med.},
  title        = {Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-world intervention graphs for defining, identifying,
and communicating estimands in clinical trials. <em>SIM</em>,
<em>42</em>(21), 3892–3902. (<a
href="https://doi.org/10.1002/sim.9833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confusion often arises when attempting to articulate target estimand(s) of a clinical trial in plain language. We aim to rectify this confusion by using a type of causal graph called the Single-World Intervention Graph (SWIG) to provide a visual representation of the estimand that can be effectively communicated to interdisciplinary stakeholders. These graphs not only display estimands, but also illustrate the assumptions under which a causal estimand is identifiable by presenting the graphical relationships between the treatment, intercurrent events, and clinical outcomes. To demonstrate its usefulness in pharmaceutical research, we present examples of SWIGs for various intercurrent event strategies specified in the ICH E9(R1) addendum, as well as an example from a real-world clinical trial for chronic pain. code to generate all the SWIGs shown is this paper is made available. We advocate clinical trialists adopt the use of SWIGs in their estimand discussions during the planning stages of their studies.},
  archive      = {J_SIM},
  author       = {Alex Ocampo and Jemar R. Bather},
  doi          = {10.1002/sim.9833},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3892-3902},
  shortjournal = {Stat. Med.},
  title        = {Single-world intervention graphs for defining, identifying, and communicating estimands in clinical trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). G-estimation of structural nested mean models for
interval-censored data using pseudo-observations. <em>SIM</em>,
<em>42</em>(21), 3877–3891. (<a
href="https://doi.org/10.1002/sim.9838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two large-scale randomized clinical trials compared fenofibrate and placebo in diabetic patients with pre-existing retinopathy (FIELD study) or risk factors (ACCORD trial) on an intention-to-treat basis and reported a significant reduction in the progression of diabetic retinopathy in the fenofibrate arms. However, their analyses involved complications due to intercurrent events, that is, treatment-switching and interval-censoring. This article addresses these problems involved in estimation of causal effects of long-term use of fibrates in a cohort study that followed patients with type 2 diabetes for 8 years. We propose structural nested mean models (SNMMs) of time-varying treatment effects and pseudo-observation estimators for interval-censored data. The first estimator for SNMMs uses a nonparametric maximum likelihood estimator (MLE) as a pseudo-observation, while the second estimator is based on MLE under a parametric piecewise exponential distribution. Through numerical studies with real and simulated datasets, the pseudo-observations estimators of causal effects using the nonparametric Wellner–Zhan estimator perform well even under dependent interval-censoring. Its application to the diabetes study revealed that the use of fibrates in the first 4 years reduced the risk of diabetic retinopathy but did not support its efficacy beyond 4 years.},
  archive      = {J_SIM},
  author       = {Shiro Tanaka and M. Alan Brookhart and Jason Fine},
  doi          = {10.1002/sim.9838},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3877-3891},
  shortjournal = {Stat. Med.},
  title        = {G-estimation of structural nested mean models for interval-censored data using pseudo-observations},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian multivariate longitudinal model for immune
responses to leishmania: A tick-borne co-infection study. <em>SIM</em>,
<em>42</em>(21), 3860–3876. (<a
href="https://doi.org/10.1002/sim.9837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many Bayesian state-space models for infectious disease processes focus on population infection dynamics (eg, compartmental models), in this work we examine the evolution of infection processes and the complexities of the immune responses within the host using these techniques. We present a joint Bayesian state-space model to better understand how the immune system contributes to the control of Leishmania infantum infections over the disease course. We use longitudinal molecular diagnostic and clinical data of a cohort of dogs to describe population progression rates and present evidence for important drivers of clinical disease. Among these results, we find evidence for the importance of co-infection in disease progression. We also show that as dogs progress through the infection, parasite load is influenced by their age, ectoparasiticide treatment status, and serology. Furthermore, we present evidence that pathogen load information from an earlier point in time influences its future value and that the size of this effect varies depending on the clinical stage of the dog. In addition to characterizing the processes driving disease progression, we predict individual and aggregate patterns of Canine Leishmaniasis progression. Both our findings and the application to individual-level predictions are of direct clinical relevance, presenting possible opportunities for application in veterinary practice and motivating lines of additional investigation to better understand and predict disease progression. Finally, as an important zoonotic human pathogen, these results may support future efforts to prevent and treat human Leishmaniosis.},
  archive      = {J_SIM},
  author       = {Felix M. Pabon-Rodriguez and Grant D. Brown and Breanna M. Scorza and Christine A. Petersen},
  doi          = {10.1002/sim.9837},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3860-3876},
  shortjournal = {Stat. Med.},
  title        = {Bayesian multivariate longitudinal model for immune responses to leishmania: A tick-borne co-infection study},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group sequential testing under instrumented
difference-in-differences approach. <em>SIM</em>, <em>42</em>(21),
3838–3859. (<a href="https://doi.org/10.1002/sim.9836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmeasured confounding is a major obstacle to reliable causal inference based on observational studies. Instrumented difference-in-differences (iDiD), a novel idea connecting instrumental variable and standard DiD, ameliorates the above issue by explicitly leveraging exogenous randomness in an exposure trend. In this article, we utilize the above idea of iDiD, and propose a novel group sequential testing method that provides valid inference even in the presence of unmeasured confounders. At each time point, we estimate the average or conditional average treatment effect under iDiD setting using the data accumulated up to that time point, and test the significance of the treatment effect. We derive the joint distribution of the test statistics under the null using the asymptotic properties of M-estimation, and the group sequential boundaries are obtained using the -spending functions. The performance of our proposed approach is evaluated on both synthetic data and Clinformatics Data Mart Database (OptumInsight, Eden Prairie, MN) to examine the association between rofecoxib and acute myocardial infarction, and our method detects significant adverse effect of rofecoxib much earlier than the time when it was finally withdrawn from the market.},
  archive      = {J_SIM},
  author       = {Samrat Roy and Ting Ye and Ashkan Ertefaie and Tat-Thang Vo and James Flory and Sean Hennessy and Dylan Small},
  doi          = {10.1002/sim.9836},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3838-3859},
  shortjournal = {Stat. Med.},
  title        = {Group sequential testing under instrumented difference-in-differences approach},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel predictions of invasive breast cancer risk in
mammography screening cohorts. <em>SIM</em>, <em>42</em>(21), 3816–3837.
(<a href="https://doi.org/10.1002/sim.9834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mammography screening programs are aimed at reducing mortality due to breast cancer by detecting tumors at an early stage. There is currently interest in moving away from the age-based screening programs, and toward personalized screening based on individual risk factors. To accomplish this, risk prediction models for breast cancer are needed to determine who should be screened, and when. We develop a novel approach using a (random effects) continuous growth model, which we apply to a large population-based, Swedish screening cohort. Unlike existing breast cancer prediction models, this approach explicitly incorporates each woman&#39;s individual screening visits in the prediction. It jointly models invasive breast cancer tumor onset, tumor growth rate, symptomatic detection rate, and screening sensitivity. In addition to predicting the overall risk of invasive breast cancer, this model can make separate predictions regarding specific tumor sizes, and the mode of detection (eg, detected at screening, or through symptoms between screenings). It can also predict how these risks change depending on whether or not a woman will attend her next screening. In our study, we predict, given a future diagnosis, that the probability of having a tumor less than (as opposed to greater than) 10-mm diameter, at detection, will be, on average, 2.6 times higher if a woman in the cohort attends their next screening. This indicates that the model can be used to evaluate the short-term benefit of screening attendance, at an individual level.},
  archive      = {J_SIM},
  author       = {Rickard Strandberg and Kamila Czene and Per Hall and Keith Humphreys},
  doi          = {10.1002/sim.9834},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3816-3837},
  shortjournal = {Stat. Med.},
  title        = {Novel predictions of invasive breast cancer risk in mammography screening cohorts},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Markov-modulated marked poisson processes for modeling
disease dynamics based on medical claims data. <em>SIM</em>,
<em>42</em>(21), 3804–3815. (<a
href="https://doi.org/10.1002/sim.9832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore Markov-modulated marked Poisson processes (MMMPPs) as a natural framework for modeling patients&#39; disease dynamics over time based on medical claims data. In claims data, observations do not only occur at random points in time but are also informative, that is, driven by unobserved disease levels, as poor health conditions usually lead to more frequent interactions with the health care system. Therefore, we model the observation process as a Markov-modulated Poisson process, where the rate of health care interactions is governed by a continuous-time Markov chain. Its states serve as proxies for the patients&#39; latent disease levels and further determine the distribution of additional data collected at each observation time, the so-called marks. Overall, MMMPPs jointly model observations and their informative time points by comprising two state-dependent processes: the observation process (corresponding to the event times) and the mark process (corresponding to event-specific information), which both depend on the underlying states. The approach is illustrated using claims data from patients diagnosed with chronic obstructive pulmonary disease by modeling their drug use and the interval lengths between consecutive physician consultations. The results indicate that MMMPPs are able to detect distinct patterns of health care utilization related to disease processes and reveal interindividual differences in the state-switching dynamics.},
  archive      = {J_SIM},
  author       = {Sina Mews and Bastian Surmann and Lena Hasemann and Svenja Elkenkamp},
  doi          = {10.1002/sim.9832},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3804-3815},
  shortjournal = {Stat. Med.},
  title        = {Markov-modulated marked poisson processes for modeling disease dynamics based on medical claims data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Permutation-based multiple testing corrections for p-values
and confidence intervals for cluster randomized trials. <em>SIM</em>,
<em>42</em>(21), 3786–3803. (<a
href="https://doi.org/10.1002/sim.9831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we derive and compare methods to derive P -values and sets of confidence intervals with strong control of the family-wise error rates and coverage for estimates of treatment effects in cluster randomized trials with multiple outcomes. There are few methods for P -value corrections and deriving confidence intervals, limiting their application in this setting. We discuss the methods of Bonferroni, Holm, and Romano and Wolf and adapt them to cluster randomized trial inference using permutation-based methods with different test statistics. We develop a novel search procedure for confidence set limits using permutation tests to produce a set of confidence intervals under each method of correction. We conduct a simulation-based study to compare family-wise error rates, coverage of confidence sets, and the efficiency of each procedure in comparison to no correction using both model-based standard errors and permutation tests. We show that the Romano-Wolf type procedure has nominal error rates and coverage under non-independent correlation structures and is more efficient than the other methods in a simulation-based study. We also compare results from the analysis of a real-world trial.},
  archive      = {J_SIM},
  author       = {Samuel I. Watson and Joshua O. Akinyemi and Karla Hemming},
  doi          = {10.1002/sim.9831},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3786-3803},
  shortjournal = {Stat. Med.},
  title        = {Permutation-based multiple testing corrections for P-values and confidence intervals for cluster randomized trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximin optimal cluster randomized designs for assessing
treatment effect heterogeneity. <em>SIM</em>, <em>42</em>(21),
3764–3785. (<a href="https://doi.org/10.1002/sim.9830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) are studies where treatment is randomized at the cluster level but outcomes are typically collected at the individual level. When CRTs are employed in pragmatic settings, baseline population characteristics may moderate treatment effects, leading to what is known as heterogeneous treatment effects (HTEs). Pre-specified, hypothesis-driven HTE analyses in CRTs can enable an understanding of how interventions may impact subpopulation outcomes. While closed-form sample size formulas have recently been proposed, assuming known intracluster correlation coefficients (ICCs) for both the covariate and outcome, guidance on optimal cluster randomized designs to ensure maximum power with pre-specified HTE analyses has not yet been developed. We derive new design formulas to determine the cluster size and number of clusters to achieve the locally optimal design (LOD) that minimizes variance for estimating the HTE parameter given a budget constraint. Given the LODs are based on covariate and outcome-ICC values that are usually unknown, we further develop the maximin design for assessing HTE, identifying the combination of design resources that maximize the relative efficiency of the HTE analysis in the worst case scenario. In addition, given the analysis of the average treatment effect is often of primary interest, we also establish optimal designs to accommodate multiple objectives by combining considerations for studying both the average and heterogeneous treatment effects. We illustrate our methods using the context of the Kerala Diabetes Prevention Program CRT, and provide an R Shiny app to facilitate calculation of optimal designs under a wide range of design parameters.},
  archive      = {J_SIM},
  author       = {Mary M. Ryan and Denise Esserman and Fan Li},
  doi          = {10.1002/sim.9830},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3764-3785},
  shortjournal = {Stat. Med.},
  title        = {Maximin optimal cluster randomized designs for assessing treatment effect heterogeneity},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel joint frailty model for hierarchically clustered
binary and survival data. <em>SIM</em>, <em>42</em>(21), 3745–3763. (<a
href="https://doi.org/10.1002/sim.9829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical data arise when observations are clustered into groups. Multilevel models are practically useful in these settings, but these models are elusive in the context of hierarchical data with mixed multivariate outcomes. In this article, we consider binary and survival outcomes and assume the hierarchical structure is induced by clustering of both outcomes within patients and clustering of patients within hospitals which frequently occur in multicenter studies. We introduce a multilevel joint frailty model that analyzes the outcomes simultaneously to jointly estimate their regression parameters and explicitly model within-patient correlation between the outcomes and within-hospital correlation separately for each outcome. Estimation is facilitated by a computationally efficient residual maximum likelihood method that further predicts cluster-specific frailties for both outcomes and circumvents the formidable challenges induced by multidimensional integration that complicates the underlying likelihood. The performance of the model and estimation procedure is investigated via extensive simulation studies. The practical utility of the model is illustrated through simultaneous modeling of disease-free survival and binary endpoint of platelet recovery in a multicenter allogeneic bone marrow transplantation dataset that motivates this study.},
  archive      = {J_SIM},
  author       = {Richard Tawiah and Howard Bondell},
  doi          = {10.1002/sim.9829},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {21},
  pages        = {3745-3763},
  shortjournal = {Stat. Med.},
  title        = {Multilevel joint frailty model for hierarchically clustered binary and survival data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing for an ignorable sampling bias under random double
truncation. <em>SIM</em>, <em>42</em>(20), 3732–3744. (<a
href="https://doi.org/10.1002/sim.9828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical and epidemiological research doubly truncated data often appear. This is the case, for instance, when the data registry is formed by interval sampling. Double truncation generally induces a sampling bias on the target variable, so proper corrections of ordinary estimation and inference procedures must be used. Unfortunately, the nonparametric maximum likelihood estimator of a doubly truncated distribution has several drawbacks, like potential nonexistence and nonuniqueness issues, or large estimation variance. Interestingly, no correction for double truncation is needed when the sampling bias is ignorable, which may occur with interval sampling and other sampling designs. In such a case the ordinary empirical distribution function is a consistent and fully efficient estimator that generally brings remarkable variance improvements compared to the nonparametric maximum likelihood estimator. Thus, identification of such situations is critical for the simple and efficient estimation of the target distribution. In this article, we introduce for the first time formal testing procedures for the null hypothesis of ignorable sampling bias with doubly truncated data. The asymptotic properties of the proposed test statistic are investigated. A bootstrap algorithm to approximate the null distribution of the test in practice is introduced. The finite sample performance of the method is studied in simulated scenarios. Finally, applications to data on onset for childhood cancer and Parkinson&#39;s disease are given. Variance improvements in estimation are discussed and illustrated.},
  archive      = {J_SIM},
  author       = {Jacobo de Uña-Álvarez},
  doi          = {10.1002/sim.9828},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3732-3744},
  shortjournal = {Stat. Med.},
  title        = {Testing for an ignorable sampling bias under random double truncation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Subgroup analysis for longitudinal data based on a partial
linear varying coefficient model with a change plane. <em>SIM</em>,
<em>42</em>(20), 3716–3731. (<a
href="https://doi.org/10.1002/sim.9827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgroup analysis has become an important tool to characterize the treatment effect heterogeneity, and finally towards precision medicine. On the other hand, longitudinal study is widespread in many fields, but subgroup analysis for this data type is still limited. In this article, we study a partial linear varying coefficient model with a change plane, in which the subgroups are defined based on linear combination of grouping variables, and the time-varying effects in different subgroups are estimated to capture the dynamic association between predictors and response. The varying coefficients are approximated by basis functions and the group indicator function is smoothed by kernel function, which are included in the generalized estimating equation for estimation. Asymptotic properties of the estimators for the varying coefficients, the constant coefficients and the change plane coefficients are established. Simulations are conducted to demonstrate the flexibility, efficiency and robustness of the proposed method. Based on the Standard and New Antiepileptic Drugs study, we successfully identify a subgroup in which patients are sensitive to the newer drug in a specific period of time.},
  archive      = {J_SIM},
  author       = {Kecheng Wei and Guoyou Qin and Zhongyi Zhu},
  doi          = {10.1002/sim.9827},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3716-3731},
  shortjournal = {Stat. Med.},
  title        = {Subgroup analysis for longitudinal data based on a partial linear varying coefficient model with a change plane},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An inverse probability weighted regression method that
accounts for right-censoring for causal inference with multiple
treatments and a binary outcome. <em>SIM</em>, <em>42</em>(20),
3699–3715. (<a href="https://doi.org/10.1002/sim.9826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative effectiveness research often involves evaluating the differences in the risks of an event of interest between two or more treatments using observational data. Often, the post-treatment outcome of interest is whether the event happens within a pre-specified time window, which leads to a binary outcome. One source of bias for estimating the causal treatment effect is the presence of confounders, which are usually controlled using propensity score-based methods. An additional source of bias is right-censoring, which occurs when the information on the outcome of interest is not completely available due to dropout, study termination, or treatment switch before the event of interest. We propose an inverse probability weighted regression-based estimator that can simultaneously handle both confounding and right-censoring, calling the method CIPWR, with the letter C highlighting the censoring component. CIPWR estimates the average treatment effects by averaging the predicted outcomes obtained from a logistic regression model that is fitted using a weighted score function. The CIPWR estimator has a double robustness property such that estimation consistency can be achieved when either the model for the outcome or the models for both treatment and censoring are correctly specified. We establish the asymptotic properties of the CIPWR estimator for conducting inference, and compare its finite sample performance with that of several alternatives through simulation studies. The methods under comparison are applied to a cohort of prostate cancer patients from an insurance claims database for comparing the adverse effects of four candidate drugs for advanced stage prostate cancer.},
  archive      = {J_SIM},
  author       = {Youfei Yu and Min Zhang and Bhramar Mukherjee},
  doi          = {10.1002/sim.9826},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3699-3715},
  shortjournal = {Stat. Med.},
  title        = {An inverse probability weighted regression method that accounts for right-censoring for causal inference with multiple treatments and a binary outcome},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Healthcare center clustering for cox’s proportional hazards
model by fusion penalty. <em>SIM</em>, <em>42</em>(20), 3685–3698. (<a
href="https://doi.org/10.1002/sim.9825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been growing research interest in developing methodology to evaluate healthcare centers&#39; performance with respect to patient outcomes. Conventional assessments can be conducted using fixed or random effects models, as seen in provider profiling. We propose a new method, using fusion penalty to cluster healthcare centers with respect to a survival outcome. Without any prior knowledge of the grouping information, the new method provides a desirable data-driven approach for automatically clustering healthcare centers into distinct groups based on their performance. An efficient alternating direction method of multipliers algorithm is developed to implement the proposed method. The validity of our approach is demonstrated through simulation studies, and its practical application is illustrated by analyzing data from the national kidney transplant registry.},
  archive      = {J_SIM},
  author       = {Lili Liu and Kevin He and Di Wang and Shujie Ma and Annie Qu and Lu Lin and J. Philip Miller and Lei Liu},
  doi          = {10.1002/sim.9825},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3685-3698},
  shortjournal = {Stat. Med.},
  title        = {Healthcare center clustering for cox&#39;s proportional hazards model by fusion penalty},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep causal feature extraction and inference with
neuroimaging genetic data. <em>SIM</em>, <em>42</em>(20), 3665–3684. (<a
href="https://doi.org/10.1002/sim.9824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is a severe public health issue in the world. Magnetic Resonance Imaging (MRI) offers a way to study brain differences between AD patients and healthy individuals through feature extraction and comparison. However, in most previous works, the extracted features were not aimed to be causal, hindering biological understanding and interpretation. In order to extract causal features, we propose using instrumental variable (IV) regression with genetic variants as IVs. Specifically, we propose Deep Feature Extraction via Instrumental Variable Regression (DeepFEIVR), which uses a nonlinear neural network to extract causal features from three-dimensional neuroimages to predict an outcome (eg, AD status in our application) while maintaining a linear relationship between the extracted features and IVs. DeepFEIVR not only can handle high dimensional individual-level data for model building, but also is applicable to GWAS summary data to test associations of the extracted features with the outcome in subsequent analysis. In addition, we propose an extension of DeepFEIVR, called DeepFEIVR-CA, for covariate adjustment (CA). We apply DeepFEIVR and DeepFEIVR-CA to the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) individual-level data as training data for model building, then apply to the UK Biobank neuroimaging and the International Genomics of Alzheimer&#39;s Project (IGAP) AD GWAS summary data, showcasing how the extracted causal features are related to AD and various brain endophenotypes.},
  archive      = {J_SIM},
  author       = {Yuchen Yao and Dipnil Charkraborty and Lin Zhang and Xiaotong Shen and Wei Pan},
  doi          = {10.1002/sim.9824},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3665-3684},
  shortjournal = {Stat. Med.},
  title        = {Deep causal feature extraction and inference with neuroimaging genetic data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical inference for the two-sample problem under
likelihood ratio ordering, with application to the ROC curve estimation.
<em>SIM</em>, <em>42</em>(20), 3649–3664. (<a
href="https://doi.org/10.1002/sim.9823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The receiver operating characteristic (ROC) curve is a powerful statistical tool and has been widely applied in medical research. In the ROC curve estimation, a commonly used assumption is that larger the biomarker value, greater severity the disease. In this article, we mathematically interpret “greater severity of the disease” as “larger probability of being diseased.” This in turn is equivalent to assume the likelihood ratio ordering of the biomarker between the diseased and healthy individuals. With this assumption, we first propose a Bernstein polynomial method to model the distributions of both samples; we then estimate the distributions by the maximum empirical likelihood principle. The ROC curve estimate and the associated summary statistics are obtained subsequently. Theoretically, we establish the asymptotic consistency of our estimators. Via extensive numerical studies, we compare the performance of our method with competitive methods. The application of our method is illustrated by a real-data example.},
  archive      = {J_SIM},
  author       = {Dingding Hu and Meng Yuan and Tao Yu and Pengfei Li},
  doi          = {10.1002/sim.9823},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3649-3664},
  shortjournal = {Stat. Med.},
  title        = {Statistical inference for the two-sample problem under likelihood ratio ordering, with application to the ROC curve estimation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disease mapping for spatially semi-continuous data by
estimating equations with application to dengue control. <em>SIM</em>,
<em>42</em>(20), 3636–3648. (<a
href="https://doi.org/10.1002/sim.9822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease mapping is a research field to estimate spatial pattern of disease risks so that areas with elevated risk levels can be identified. The motivation of this article is from a study of dengue fever infection, which causes seasonal epidemics in almost every summer in Taiwan. For analysis of zero-inflated data with spatial correlation and covariates, current methods would either cause a computational burden or miss associations between zero and non-zero responses. In this article, we develop estimating equations for a mixture regression model that accommodates spatial dependence and zero inflation for study of disease propagation. Asymptotic properties for the proposed estimates are established. A simulation study is conducted to evaluate performance of the mixture estimating equations; and a dengue dataset from southern Taiwan is used to illustrate the proposed method.},
  archive      = {J_SIM},
  author       = {Pei-Sheng Lin and Yih-Jeng Yu and Jun Zhu},
  doi          = {10.1002/sim.9822},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3636-3648},
  shortjournal = {Stat. Med.},
  title        = {Disease mapping for spatially semi-continuous data by estimating equations with application to dengue control},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-rank latent matrix-factor prediction modeling for
generalized high-dimensional matrix-variate regression. <em>SIM</em>,
<em>42</em>(20), 3616–3635. (<a
href="https://doi.org/10.1002/sim.9821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by diagnosing the COVID-19 disease using two-dimensional (2D) image biomarkers from computed tomography (CT) scans, we propose a novel latent matrix-factor regression model to predict responses that may come from an exponential distribution family, where covariates include high-dimensional matrix-variate biomarkers. A latent generalized matrix regression (LaGMaR) is formulated, where the latent predictor is a low-dimensional matrix factor score extracted from the low-rank signal of the matrix variate through a cutting-edge matrix factor model. Unlike the general spirit of penalizing vectorization plus the necessity of tuning parameters in the literature, instead, our prediction modeling in LaGMaR conducts dimension reduction that respects the geometric characteristic of intrinsic 2D structure of the matrix covariate and thus avoids iteration. This greatly relieves the computation burden, and meanwhile maintains structural information so that the latent matrix factor feature can perfectly replace the intractable matrix-variate owing to high-dimensionality. The estimation procedure of LaGMaR is subtly derived by transforming the bilinear form matrix factor model onto a high-dimensional vector factor model, so that the method of principle components can be applied. We establish bilinear-form consistency of the estimated matrix coefficient of the latent predictor and consistency of prediction. The proposed approach can be implemented conveniently. Through simulation experiments, the prediction capability of LaGMaR is shown to outperform some existing penalized methods under diverse scenarios of generalized matrix regressions. Through the application to a real COVID-19 dataset, the proposed approach is shown to predict efficiently the COVID-19.},
  archive      = {J_SIM},
  author       = {Yuzhe Zhang and Xu Zhang and Hong Zhang and Aiyi Liu and Catherine C. Liu},
  doi          = {10.1002/sim.9821},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3616-3635},
  shortjournal = {Stat. Med.},
  title        = {Low-rank latent matrix-factor prediction modeling for generalized high-dimensional matrix-variate regression},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating contact network properties by integrating
multiple data sources associated with infectious diseases. <em>SIM</em>,
<em>42</em>(20), 3593–3615. (<a
href="https://doi.org/10.1002/sim.9816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively mitigate the spread of communicable diseases, it is necessary to understand the interactions that enable disease transmission among individuals in a population; we refer to the set of these interactions as a contact network . The structure of the contact network can have profound effects on both the spread of infectious diseases and the effectiveness of control programs. Therefore, understanding the contact network permits more efficient use of resources. Measuring the structure of the network, however, is a challenging problem. We present a Bayesian approach to integrate multiple data sources associated with the transmission of infectious diseases to more precisely and accurately estimate important properties of the contact network. An important aspect of the approach is the use of the congruence class models for networks. We conduct simulation studies modeling pathogens resembling SARS-CoV-2 and HIV to assess the method; subsequently, we apply our approach to HIV data from the University of California San Diego Primary Infection Resource Consortium. Based on simulation studies, we demonstrate that the integration of epidemiological and viral genetic data with risk behavior survey data can lead to large decreases in mean squared error (MSE) in contact network estimates compared to estimates based strictly on risk behavior information. This decrease in MSE is present even in settings where the risk behavior surveys contain measurement error. Through these simulations, we also highlight certain settings where the approach does not improve MSE.},
  archive      = {J_SIM},
  author       = {Ravi Goyal and Nicole Carnegie and Sally Slipher and Philip Turk and Susan J. Little and Victor De Gruttola},
  doi          = {10.1002/sim.9816},
  journal      = {Statistics in Medicine},
  month        = {9},
  number       = {20},
  pages        = {3593-3615},
  shortjournal = {Stat. Med.},
  title        = {Estimating contact network properties by integrating multiple data sources associated with infectious diseases},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size for partially nested designs and other nested or
crossed designs with a continuous outcome when adjusted for baseline.
<em>SIM</em>, <em>42</em>(19), 3568–3592. (<a
href="https://doi.org/10.1002/sim.9820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a randomized controlled trial, outcomes of different subjects may be independent at baseline, but correlated at a follow-up measurement due to treatment. This treatment-related clustering at follow-up can arise for instance because the treatment is given in a group or because subjects are treated individually but by the same therapist (therapist effect). There is substantial literature on the design and analysis of such trials when estimation of the intervention effect is based on a follow-up measurement (eg, directly after treatment or at a later time point). However, often the baseline measurement of the outcome is highly correlated with the follow-up measurement, and this information can be used in the analysis. For a randomized design with a baseline and a follow-up measurement, we compare sample size requirements for analyses with and without adjustment for this baseline measure. We show that adjusting for baseline reduces required sample size. This reduction depends on the variance of the difference between arms at baseline, the variance of this difference at follow-up, and the correlation between the two. From this, we derive sample size formulas for partially or fully nested designs, and cluster randomized trials with treatment as a partially or fully cross-classified factor. Also, we discuss situations where clusters are already present at baseline or where treatment by cluster interaction is present. For the partially nested design, we work out practical design considerations (eg, use of content-matter input, design factors and optimal allocation ratio) and investigate small sample properties of the sample size formula.},
  archive      = {J_SIM},
  author       = {Steven Teerenstra and Jessica Kasza and Ruslan Leontjevas and Andrew B. Forbes},
  doi          = {10.1002/sim.9820},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3568-3592},
  shortjournal = {Stat. Med.},
  title        = {Sample size for partially nested designs and other nested or crossed designs with a continuous outcome when adjusted for baseline},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instrumental variable model average with applications in
mendelian randomization. <em>SIM</em>, <em>42</em>(19), 3547–3567. (<a
href="https://doi.org/10.1002/sim.9819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization is a technique used to examine the causal effect of a modifiable exposure on a trait using an observational study by utilizing genetic variants. The use of many instruments can help to improve the estimation precision but may suffer bias when the instruments are weakly associated with the exposure. To overcome the difficulty of high-dimensionality, we propose a model average estimator which involves using different subsets of instruments (single nucleotide polymorphisms, SNPs) to predict the exposure in the first stage, followed by weighting the submodels&#39; predictions using penalization by common penalty functions such as least absolute shrinkage and selection operator (LASSO), smoothly clipped absolute deviation (SCAD) and minimax concave penalty (MCP). The model averaged predictions are then used as a genetically predicted exposure to obtain the estimation of the causal effect on the response in the second stage. The novelty of our model average estimator also lies in that it allows the number of submodels and the submodels&#39; sizes to grow with the sample size. The practical performance of the estimator is examined in a series of numerical studies. We apply the proposed method on a real genetic dataset investigating the relationship between stature and blood pressure.},
  archive      = {J_SIM},
  author       = {Loraine Liping Seng and Ching-Ti Liu and Jingli Wang and Jialiang Li},
  doi          = {10.1002/sim.9819},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3547-3567},
  shortjournal = {Stat. Med.},
  title        = {Instrumental variable model average with applications in mendelian randomization},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handling misclassified stratification variables in the
analysis of randomised trials with continuous outcomes. <em>SIM</em>,
<em>42</em>(19), 3529–3546. (<a
href="https://doi.org/10.1002/sim.9818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many trials use stratified randomisation, where participants are randomised within strata defined by one or more baseline covariates. While it is important to adjust for stratification variables in the analysis, the appropriate method of adjustment is unclear when stratification variables are affected by misclassification and hence some participants are randomised in the incorrect stratum. We conducted a simulation study to compare methods of adjusting for stratification variables affected by misclassification in the analysis of continuous outcomes when all or only some stratification errors are discovered, and when the treatment effect or treatment-by-covariate interaction effect is of interest. The data were analysed using linear regression with no adjustment, adjustment for the strata used to perform the randomisation (randomisation strata), adjustment for the strata if all errors are corrected (true strata), and adjustment for the strata after some errors are discovered and corrected (updated strata). The unadjusted model performed poorly in all settings. Adjusting for the true strata was optimal, while the relative performance of adjusting for the randomisation strata or the updated strata varied depending on the setting. As the true strata are unlikely to be known with certainty in practice, we recommend using the updated strata for adjustment and performing subgroup analyses, provided the discovery of errors is unlikely to depend on treatment group, as expected in blinded trials. Greater transparency is needed in the reporting of stratification errors and how they were addressed in the analysis.},
  archive      = {J_SIM},
  author       = {Lisa N. Yelland and Jennie Louise and Brennan C. Kahan and Tim P. Morris and Katherine J. Lee and Thomas R. Sullivan},
  doi          = {10.1002/sim.9818},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3529-3546},
  shortjournal = {Stat. Med.},
  title        = {Handling misclassified stratification variables in the analysis of randomised trials with continuous outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Propensity-based standardization to enhance the validation
and interpretation of prediction model discrimination for a target
population. <em>SIM</em>, <em>42</em>(19), 3508–3528. (<a
href="https://doi.org/10.1002/sim.9817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {External validation of the discriminative ability of prediction models is of key importance. However, the interpretation of such evaluations is challenging, as the ability to discriminate depends on both the sample characteristics (ie, case-mix) and the generalizability of predictor coefficients, but most discrimination indices do not provide any insight into their respective contributions. To disentangle differences in discriminative ability across external validation samples due to a lack of model generalizability from differences in sample characteristics, we propose propensity-weighted measures of discrimination. These weighted metrics, which are derived from propensity scores for sample membership, are standardized for case-mix differences between the model development and validation samples, allowing for a fair comparison of discriminative ability in terms of model characteristics in a target population of interest. We illustrate our methods with the validation of eight prediction models for deep vein thrombosis in 12 external validation data sets and assess our methods in a simulation study. In the illustrative example, propensity score standardization reduced between-study heterogeneity of discrimination, indicating that between-study variability was partially attributable to case-mix. The simulation study showed that only flexible propensity-score methods (allowing for non-linear effects) produced unbiased estimates of model discrimination in the target population, and only when the positivity assumption was met. Propensity score-based standardization may facilitate the interpretation of (heterogeneity in) discriminative ability of a prediction model as observed across multiple studies, and may guide model updating strategies for a particular target population. Careful propensity score modeling with attention for non-linear relations is recommended.},
  archive      = {J_SIM},
  author       = {Valentijn M. T. de Jong and Jeroen Hoogland and Karel G. M. Moons and Richard D. Riley and Tri-Long Nguyen and Thomas P. A. Debray},
  doi          = {10.1002/sim.9817},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3508-3528},
  shortjournal = {Stat. Med.},
  title        = {Propensity-based standardization to enhance the validation and interpretation of prediction model discrimination for a target population},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tackling dynamic prediction of death in patients with
recurrent cardiovascular events. <em>SIM</em>, <em>42</em>(19),
3487–3507. (<a href="https://doi.org/10.1002/sim.9815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of cardiovascular disease, recurrent events such as stroke or myocardial infarction (MI) are often encountered, leading to an increase in the risk of death. Accurately evaluating the prognosis of patients and dynamically predicting the risk of death by considering the historical recurrent events can improve medical decisions and lead to better health care outcomes. Recently proposed joint modeling approaches within the Bayesian framework have inspired the development of a dynamic prediction tool, which can be applied for subject-level prediction of death with implementation in software packages. The prediction model incorporates subject heterogeneity with subject-level random effects that account for unobserved time-invariant factors and an extra copula function capturing the part caused by unmeasured time-dependent factors. Thereafter, given the prespecified landmark time , the survival probability for a prediction horizon time of interest can be estimated for each individual. The prediction accuracy is assessed by time-dependent receiving operating characteristic curve and the area under the curve and the Brier score with calibration plots is compared to traditional joint frailty models. Finally, the tool is applied to patients with multiple attacks of stroke or MI in the Cardiovascular Health study and the Atherosclerosis Risk in Communities study for illustration.},
  archive      = {J_SIM},
  author       = {Menglu Liang and Zheng Li and Liang Li and Vernon M. Chinchilli and Lijun Zhang and Ming Wang},
  doi          = {10.1002/sim.9815},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3487-3507},
  shortjournal = {Stat. Med.},
  title        = {Tackling dynamic prediction of death in patients with recurrent cardiovascular events},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying potential significant factors impacting
zero-inflated proportion data. <em>SIM</em>, <em>42</em>(19), 3467–3486.
(<a href="https://doi.org/10.1002/sim.9814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical supervised methods like linear regression and decision trees are not completely adapted for identifying impacting factors on a response variable corresponding to zero-inflated proportion data (ZIPD) that are dependent, continuous and bounded. In this article we propose a within-block permutation-based methodology to identify factors (discrete or continuous) that are significantly correlated with ZIPD, we propose a performance indicator quantifying the percentage of correlation explained by the subset of significant factors, and we show how to predict the ranks of the response variables conditionally on the observation of these factors. The methodology is illustrated on simulated data and on two real data sets dealing with epidemiology. In the first data set, ZIPD correspond to probabilities of transmission of Influenza between horses. In the second data set, ZIPD correspond to probabilities that geographic entities (eg, states and countries) have the same COVID-19 mortality dynamics.},
  archive      = {J_SIM},
  author       = {Mélina Ribaud and Edith Gabriel and Joseph Hughes and Samuel Soubeyrand},
  doi          = {10.1002/sim.9814},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3467-3486},
  shortjournal = {Stat. Med.},
  title        = {Identifying potential significant factors impacting zero-inflated proportion data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Defining and estimating effects in cluster randomized
trials: A methods comparison. <em>SIM</em>, <em>42</em>(19), 3443–3466.
(<a href="https://doi.org/10.1002/sim.9813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across research disciplines, cluster randomized trials (CRTs) are commonly implemented to evaluate interventions delivered to groups of participants, such as communities and clinics. Despite advances in the design and analysis of CRTs, several challenges remain. First, there are many possible ways to specify the causal effect of interest (eg, at the individual-level or at the cluster-level). Second, the theoretical and practical performance of common methods for CRT analysis remain poorly understood. Here, we present a general framework to formally define an array of causal effects in terms of summary measures of counterfactual outcomes. Next, we provide a comprehensive overview of CRT estimators, including the t -test, generalized estimating equations (GEE), augmented-GEE, and targeted maximum likelihood estimation (TMLE). Using finite sample simulations, we illustrate the practical performance of these estimators for different causal effects and when, as commonly occurs, there are limited numbers of clusters of different sizes. Finally, our application to data from the Preterm Birth Initiative (PTBi) study demonstrates the real-world impact of varying cluster sizes and targeting effects at the cluster-level or at the individual-level. Specifically, the relative effect of the PTBi intervention was 0.81 at the cluster-level, corresponding to a 19% reduction in outcome incidence, and was 0.66 at the individual-level, corresponding to a 34% reduction in outcome risk. Given its flexibility to estimate a variety of user-specified effects and ability to adaptively adjust for covariates for precision gains while maintaining Type-I error control, we conclude TMLE is a promising tool for CRT analysis.},
  archive      = {J_SIM},
  author       = {Alejandra Benitez and Maya L. Petersen and Mark J. van der Laan and Nicole Santos and Elizabeth Butrick and Dilys Walker and Rakesh Ghosh and Phelgona Otieno and Peter Waiswa and Laura B. Balzer},
  doi          = {10.1002/sim.9813},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3443-3466},
  shortjournal = {Stat. Med.},
  title        = {Defining and estimating effects in cluster randomized trials: A methods comparison},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rule ensemble method with adaptive group lasso for
heterogeneous treatment effect estimation. <em>SIM</em>,
<em>42</em>(19), 3413–3442. (<a
href="https://doi.org/10.1002/sim.9812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing scientific attention given to precision medicine based on real-world data has led to many recent studies clarifying the relationships between treatment effects and patient characteristics. However, this is challenging because of ubiquitous heterogeneity in the treatment effect for individuals and the real-world data on their backgrounds being complex and noisy. Because of their flexibility, various machine learning (ML) methods have been proposed for estimating heterogeneous treatment effect (HTE). However, most ML methods incorporate black-box models that hamper direct interpretation of the relationships between an individual&#39;s characteristics and treatment effects. This study proposes an ML method for estimating HTE based on the rule ensemble method RuleFit. The main advantages of RuleFit are interpretability and accuracy. However, HTEs are always defined in the potential outcome framework, and RuleFit cannot be applied directly. Thus, we modified RuleFit and proposed a method to estimate HTEs that directly interpret the relationships among the individuals&#39; features from the model. Actual data from an HIV study, the ACTG 175 dataset, was used to illustrate the interpretation based on the ensemble of rules created by the proposed method. The numerical results confirm that the proposed method has high prediction accuracy compared to previous methods, indicating that the proposed method establishes an interpretable model with sufficient prediction accuracy.},
  archive      = {J_SIM},
  author       = {Ke Wan and Kensuke Tanioka and Toshio Shimokawa},
  doi          = {10.1002/sim.9812},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3413-3442},
  shortjournal = {Stat. Med.},
  title        = {Rule ensemble method with adaptive group lasso for heterogeneous treatment effect estimation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size considerations for assessing treatment effect
heterogeneity in randomized trials with heterogeneous intracluster
correlations and variances. <em>SIM</em>, <em>42</em>(19), 3392–3412.
(<a href="https://doi.org/10.1002/sim.9811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important consideration in the design and analysis of randomized trials is the need to account for outcome observations being positively correlated within groups or clusters. Two notable types of designs with this consideration are individually randomized group treatment trials and cluster randomized trials. While sample size methods for testing the average treatment effect are available for both types of designs, methods for detecting treatment effect modification are relatively limited. In this article, we present new sample size formulas for testing treatment effect modification based on either a univariate or multivariate effect modifier in both individually randomized group treatment and cluster randomized trials with a continuous outcome but any types of effect modifier, while accounting for differences across study arms in the outcome variance, outcome intracluster correlation coefficient (ICC) and the cluster size. We consider cases where the effect modifier can be measured at either the individual level or cluster level, and with a univariate effect modifier, our closed-form sample size expressions provide insights into the optimal allocation of groups or clusters to maximize design efficiency. Overall, our results show that the required sample size for testing treatment effect heterogeneity with an individual-level effect modifier can be affected by unequal ICCs and variances between arms, and accounting for such between-arm heterogeneity can lead to more accurate sample size determination. We use simulations to validate our sample size formulas and illustrate their application in the context of two real trials: an individually randomized group treatment trial (the AWARE study) and a cluster randomized trial (the K-DPP study).},
  archive      = {J_SIM},
  author       = {Guangyu Tong and Monica Taljaard and Fan Li},
  doi          = {10.1002/sim.9811},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3392-3412},
  shortjournal = {Stat. Med.},
  title        = {Sample size considerations for assessing treatment effect heterogeneity in randomized trials with heterogeneous intracluster correlations and variances},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-state network meta-analysis of progression and
survival data. <em>SIM</em>, <em>42</em>(19), 3371–3391. (<a
href="https://doi.org/10.1002/sim.9810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple randomized controlled trials, each comparing a subset of competing interventions, can be synthesized by means of a network meta-analysis to estimate relative treatment effects between all interventions in the evidence base. Here we focus on estimating relative treatment effects for time-to-event outcomes. Cancer treatment effectiveness is frequently quantified by analyzing overall survival (OS) and progression-free survival (PFS). We introduce a method for the joint network meta-analysis of PFS and OS that is based on a time-inhomogeneous tri-state (stable, progression, and death) Markov model where time-varying transition rates and relative treatment effects are modeled with parametric survival functions or fractional polynomials. The data needed to run these analyses can be extracted directly from published survival curves. We demonstrate use by applying the methodology to a network of trials for the treatment of non-small-cell lung cancer. The proposed approach allows the joint synthesis of OS and PFS, relaxes the proportional hazards assumption, extends to a network of more than two treatments, and simplifies the parameterization of decision and cost-effectiveness analyses.},
  archive      = {J_SIM},
  author       = {Jeroen P. Jansen and Devin Incerti and Thomas A. Trikalinos},
  doi          = {10.1002/sim.9810},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3371-3391},
  shortjournal = {Stat. Med.},
  title        = {Multi-state network meta-analysis of progression and survival data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian hierarchical sparse factor model for estimating
simultaneous covariance matrices for gestational outcomes in consecutive
pregnancies. <em>SIM</em>, <em>42</em>(19), 3353–3370. (<a
href="https://doi.org/10.1002/sim.9809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance estimation for multiple groups is a key feature for drawing inference from a heterogeneous population. One should seek to share information about common features in the dependence structures across the various groups. In this paper, we introduce a novel approach for estimating the covariance matrices for multiple groups using a hierarchical latent factor model that shrinks the factor loadings across groups toward a global value. Using a sparse spike and slab model on these loading coefficients allows for a sparse formulation of our model. Parameter estimation is accomplished through a Markov chain Monte Carlo scheme, and a model selection approach is used to select the number of factors to use. We validate our model through extensive simulation studies. Finally, we apply our methodology to the NICHD Consecutive Pregnancies Study to estimate the correlations between birth weights and gestational ages of three consecutive birth within four different subgroups (underweight, normal, overweight, and obese) of women.},
  archive      = {J_SIM},
  author       = {Debamita Kundu and Ritendranath Mitra and Paul S. Albert and Jeremy T. Gaskins},
  doi          = {10.1002/sim.9809},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3353-3370},
  shortjournal = {Stat. Med.},
  title        = {A bayesian hierarchical sparse factor model for estimating simultaneous covariance matrices for gestational outcomes in consecutive pregnancies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact inference for fixed effects meta-analysis of 2×2
tables. <em>SIM</em>, <em>42</em>(19), 3333–3352. (<a
href="https://doi.org/10.1002/sim.9808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis of associations between rare outcomes and binary exposures are particularly important in studies of a drug&#39;s potential side-effects. But meta-analysis of the resulting 2 × 2 $$ 2\times 2 $$ contingency tables presents substantial practical difficulties, as analysts are currently forced to pick between “exact” inference—that eliminates concern over using large-sample approximations with small cell counts—and explicitly allowing for heterogeneity of the underlying effects. A controversial example is given by the Avandia meta-analysis (Nissen and Wolski. N Engl J Med. 2007;356(24):2457–2471) of rosiglitazone&#39;s effects on myocardial infarction and death. While the initial Avandia analysis—using simple methods—found a significant effect, its results conflict with subsequent re-analyses that use either exact methods, or that explicitly acknowledge the plausible heterogeneity. In this article, we aim to resolve these difficulties, by providing an exact (albeit conservative) method that is valid under heterogeneity. We also provide a measure of the degree of conservatism, that indicates the approximate extent of the excess coverage. Applied to the Avandia data, we find support for Nissen and Wolski 2007&#39;s original results. Given that our method does not require strong assumptions or large cell counts, and provides intervals around the well-known conditional maximum likelihood estimate, we anticipate that it could be an attractive default method for meta-analysis of tables featuring rare events.},
  archive      = {J_SIM},
  author       = {Spencer Hansen and Kenneth Rice},
  doi          = {10.1002/sim.9808},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3333-3352},
  shortjournal = {Stat. Med.},
  title        = {Exact inference for fixed effects meta-analysis of 2×2 tables},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric pseudo-score and pseudo-likelihood for
evaluating correlate of protection in vaccine trials. <em>SIM</em>,
<em>42</em>(19), 3317–3332. (<a
href="https://doi.org/10.1002/sim.9807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vaccine clinical trials, vaccine efficacy endpoint analysis is usually associated with in high cost or extended study duration, due to the generally low infection rate. Correlate of protection (CoP), which refers to surrogate endpoint, usually immunological response, that can reliably predict the treatment effect, provides a more efficient and less costly approach to evaluate the vaccine. To handle the challenge of the missingness in the unobserved surrogate immune biomarker, the pseudo-score (PS) method, semiparametric method and pseudo-likelihood (PL) method demonstrated their advantages on different aspects. In this article, we propose new methodologies to combine the advantages of PS and PL with semiparametric methods respectively, to achieve higher estimate efficiency, allow continuous baseline predictor variable, and handle multiple surrogate markers. The advantage of our methodologies are demonstrated by a simulation study in different settings and applied to a case study, which eventually can improve the chance of a successful trial.},
  archive      = {J_SIM},
  author       = {Wanying Ma and Mengya Liu and Jian Zhu and Qing Li and Elaine Hoffman and Jianchang Lin},
  doi          = {10.1002/sim.9807},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {19},
  pages        = {3317-3332},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric pseudo-score and pseudo-likelihood for evaluating correlate of protection in vaccine trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to “causal estimands and confidence intervals
associated with wilcoxon-mann-whitney tests in randomized experiments.”
<em>SIM</em>, <em>42</em>(18), 3316. (<a
href="https://doi.org/10.1002/sim.9835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.9835},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3316},
  shortjournal = {Stat. Med.},
  title        = {Correction to “Causal estimands and confidence intervals associated with wilcoxon-mann-whitney tests in randomized experiments”},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncovering circadian rhythms in metabolic longitudinal data:
A bayesian latent class modeling approach. <em>SIM</em>,
<em>42</em>(18), 3302–3315. (<a
href="https://doi.org/10.1002/sim.9806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers in biology and medicine have increasingly focused on characterizing circadian rhythms and their potential impact on disease. Understanding circadian variation in metabolomics, the study of chemical processes involving metabolites may provide insight into important aspects of biological mechanism. Of scientific importance is developing a statistical rigorous approach for characterizing different types of 24-hour patterns among high dimensional longitudinal metabolites. We develop a latent class approach to incorporate variation in 24-hour patterns across metabolites where profiles are modeled with finite mixtures of distinct shape-invariant circadian curves that themselves incorporate variation in amplitude and phase across metabolites. An efficient Markov chain Monte Carlo sampling is used to carry out Bayesian posterior computation. When the model was fit separately by individual to the data from a small group of participants, two distinct 24-hour rhythms were identified, with one being sinusoidal and the other being more complex with multiple peaks. Interestingly, the latent pattern associated with circadian variation (simple sinusoidal curve) had a similar phase across the three participants, while the more complex latent pattern reflecting diurnal variation differed across individual. The results suggested that this modeling framework can be used to separate 24-hour rhythms into an endogenous circadian and one or more exogenous diurnal patterns in describing human metabolism.},
  archive      = {J_SIM},
  author       = {Sungduk Kim and Neil E. Caporaso and Fangyi Gu and Elizabeth B. Klerman and Paul S. Albert},
  doi          = {10.1002/sim.9806},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3302-3315},
  shortjournal = {Stat. Med.},
  title        = {Uncovering circadian rhythms in metabolic longitudinal data: A bayesian latent class modeling approach},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The person-time ratio distribution for the exact monitoring
of adverse events: Historical vs surveillance poisson data.
<em>SIM</em>, <em>42</em>(18), 3283–3301. (<a
href="https://doi.org/10.1002/sim.9805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the postmarket drug and vaccine safety surveillance, when the number of adverse events follows a Poisson distribution, the ratio between the exposed and the unexposed person-time information is the random variable that governs the decision rule about the safety of the drug or vaccine. The probability distribution function of such a ratio is derived in this paper. Exact point and interval estimators for the relative risk are discussed as well as statistical hypothesis testing. To the best of our knowledge, this is the first paper that provides an unbiased estimator for the relative risk based on the person-time ratio. The applicability of this new distribution is illustrated through a real data analysis aimed to detect increased risk of occurrence of Myocarditis/Pericarditis following mRNA COVID-19 vaccination in Manitoba, Canada.},
  archive      = {J_SIM},
  author       = {Ivair R. Silva and Joselito Montalban},
  doi          = {10.1002/sim.9805},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3283-3301},
  shortjournal = {Stat. Med.},
  title        = {The person-time ratio distribution for the exact monitoring of adverse events: Historical vs surveillance poisson data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixture of longitudinal factor analyzers and their
application to the assessment of chronic pain. <em>SIM</em>,
<em>42</em>(18), 3259–3282. (<a
href="https://doi.org/10.1002/sim.9804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate longitudinal data are used in a variety of research areas not only because they allow to analyze time trajectories of multiple indicators, but also to determine how these trajectories are influenced by other covariates. In this article, we propose a mixture of longitudinal factor analyzers. This model could be used to extract latent factors representing multiple longitudinal noisy indicators in heterogeneous longitudinal data and to study the impact of one or several covariates on these latent factors. One of the advantages of this model is that it allows for measurement non-invariance, which arises in practice when the factor structure varies between groups of individuals due to cultural or physiological differences. This is achieved by estimating different factor models for different latent classes. The proposed model could also be used to extract latent classes with different latent factor trajectories over time. Other advantages of the model include its ability to take into account heteroscedasticity of errors in the factor analysis model by estimating different error variances for different latent classes. We first define the mixture of longitudinal factor analyzers and its parameters. Then, we propose an EM algorithm to estimate these parameters. We propose a Bayesian information criterion to identify both the number of components in the mixture and the number of latent factors. We then discuss the comparability of the latent factors obtained between subjects in different latent groups. Finally, we apply the model to simulated and real data of patients with chronic postoperative pain.},
  archive      = {J_SIM},
  author       = {Amine Ounajim and Yousri Slaoui and Pierre-Yves Louis and Maxime Billot and Denis Frasca and Philippe Rigoard},
  doi          = {10.1002/sim.9804},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3259-3282},
  shortjournal = {Stat. Med.},
  title        = {Mixture of longitudinal factor analyzers and their application to the assessment of chronic pain},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental design and power calculation in omics circadian
rhythmicity detection using the cosinor model. <em>SIM</em>,
<em>42</em>(18), 3236–3258. (<a
href="https://doi.org/10.1002/sim.9803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circadian clocks are 24-h endogenous oscillators in physiological and behavioral processes. Though recent transcriptomic studies have been successful in revealing the circadian rhythmicity in gene expression, the power calculation for omics circadian analysis have not been fully explored. In this paper, we develop a statistical method, namely CircaPower, to perform power calculation for circadian pattern detection. Our theoretical framework is determined by three key factors in circadian gene detection: sample size, intrinsic effect size and sampling design. Via simulations, we systematically investigate the impact of these key factors on circadian power calculation. We not only demonstrate that CircaPower is fast and accurate, but also show its underlying cosinor model is robust against variety of violations of model assumptions. In real applications, we demonstrate the performance of CircaPower using mouse pan-tissue data and human post-mortem brain data, and illustrate how to perform circadian power calculation using mouse skeleton muscle RNA-Seq pilot as case study. Our method CircaPower has been implemented in an R package, which is made publicly available on GitHub ( https://github.com/circaPower/circaPower ).},
  archive      = {J_SIM},
  author       = {Wei Zong and Marianne L. Seney and Kyle D. Ketchesin and Michael T. Gorczyca and Andrew C. Liu and Karyn A. Esser and George C. Tseng and Colleen A. McClung and Zhiguang Huo},
  doi          = {10.1002/sim.9803},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3236-3258},
  shortjournal = {Stat. Med.},
  title        = {Experimental design and power calculation in omics circadian rhythmicity detection using the cosinor model},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference for recurrent events via aggregated
marginal odds ratio. <em>SIM</em>, <em>42</em>(18), 3208–3235. (<a
href="https://doi.org/10.1002/sim.9802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers often work with treatments and outcomes that vary over time. For example, psychologists are interested in the curative effect of cognitive behavior therapies on patients&#39; recurrent depression symptoms. While there are various causal effect measures designed for one-time treatment, the causal effect measures for time-varying treatment and recurrent events are relatively under-developed. In this article, a new causal measure is proposed to quantify the causal effect of time-varying treatments on recurrent events. We suggest estimators with robust standard errors that are based on various weight models for both conventional causal measures and the proposed measure in different time settings. We outline the approaches and describe how using some stabilized inverse probability weight models are more advantageous than others. We demonstrate that the proposed causal estimand can be consistently estimated for study periods of moderate length, and the estimation results are compared under different treatment settings with various weight models. We also find that the proposed method is suitable for both absorbing and nonabsorbing treatments. The methods are applied to the 1997 National Longitudinal Study of Youth as an illustrative example.},
  archive      = {J_SIM},
  author       = {Wenling Zhang and Cecilia A. Cotton},
  doi          = {10.1002/sim.9802},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3208-3235},
  shortjournal = {Stat. Med.},
  title        = {Causal inference for recurrent events via aggregated marginal odds ratio},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Developing prediction models to estimate the risk of two
survival outcomes both occurring: A comparison of techniques.
<em>SIM</em>, <em>42</em>(18), 3184–3207. (<a
href="https://doi.org/10.1002/sim.9771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Alexander Pate and Matthew Sperrin and Richard D. Riley and Jamie C. Sergeant and Tjeerd Van Staa and Niels Peek and Mamas A. Mamas and Gregory Y. H. Lip and Martin O&#39;Flaherty and Iain Buchan and Glen P. Martin},
  doi          = {10.1002/sim.9771},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3184-3207},
  shortjournal = {Stat. Med.},
  title        = {Developing prediction models to estimate the risk of two survival outcomes both occurring: A comparison of techniques},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate disease progression modeling with longitudinal
ordinal data. <em>SIM</em>, <em>42</em>(18), 3164–3183. (<a
href="https://doi.org/10.1002/sim.9770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease modeling is an essential tool to describe disease progression and its heterogeneity across patients. Usual approaches use continuous data such as biomarkers to assess progression. Nevertheless, categorical or ordinal data such as item responses in questionnaires also provide insightful information about disease progression. In this work, we propose a disease progression model for ordinal and categorical data. We built it on the principles of disease course mapping, a technique that uniquely describes the variability in both the dynamics of progression and disease heterogeneity from multivariate longitudinal data. This extension can also be seen as an attempt to bridge the gap between longitudinal multivariate models and the field of item response theory. Application to the Parkinson&#39;s progression markers initiative cohort illustrates the benefits of our approach: a fine-grained description of disease progression at the item level, as compared to the aggregated total score, together with improved predictions of the patient&#39;s future visits. The analysis of the heterogeneity across individual trajectories highlights known disease trends such as tremor dominant or postural instability and gait difficulties subtypes of Parkinson&#39;s disease.},
  archive      = {J_SIM},
  author       = {Pierre-Emmanuel Poulet and Stanley Durrleman},
  doi          = {10.1002/sim.9770},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3164-3183},
  shortjournal = {Stat. Med.},
  title        = {Multivariate disease progression modeling with longitudinal ordinal data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supervised structural learning of semiparametric regression
on high-dimensional correlated covariates with applications to eQTL
studies. <em>SIM</em>, <em>42</em>(18), 3145–3163. (<a
href="https://doi.org/10.1002/sim.9769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expression quantitative trait loci (eQTL) studies utilize regression models to explain the variance of gene expressions with genetic loci or single nucleotide polymorphisms (SNPs). However, regression models for eQTL are challenged by the presence of high dimensional non-sparse and correlated SNPs with small effects, and nonlinear relationships between responses and SNPs. Principal component analyses are commonly conducted for dimension reduction without considering responses. Because of that, this non-supervised learning method often does not work well when the focus is on discovery of the response-covariate relationship. We propose a new supervised structural dimensional reduction method for semiparametric regression models with high dimensional and correlated covariates; we extract low-dimensional latent features from a vast number of correlated SNPs while accounting for their relationships, possibly nonlinear, with gene expressions. Our model identifies important SNPs associated with gene expressions and estimates the association parameters via a likelihood-based algorithm. A GTEx data application on a cancer related gene is presented with 18 novel eQTLs detected by our method. In addition, extensive simulations show that our method outperforms the other competing methods in bias, efficiency, and computational cost.},
  archive      = {J_SIM},
  author       = {Wei Liu and Huazhen Lin and Li Liu and Yanyuan Ma and Ying Wei and Yi Li},
  doi          = {10.1002/sim.9769},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3145-3163},
  shortjournal = {Stat. Med.},
  title        = {Supervised structural learning of semiparametric regression on high-dimensional correlated covariates with applications to eQTL studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A combined multilevel factor analysis and covariance
regression model with mixed effects in the mean and variance structure.
<em>SIM</em>, <em>42</em>(18), 3128–3144. (<a
href="https://doi.org/10.1002/sim.9768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Li et al developed a multilevel covariance regression (MCR) model as an extension of the covariance regression model of Hoff and Niu. This model assumes a hierarchical structure for the mean and the covariance matrix. Here, we propose the combined multilevel factor analysis and covariance regression model in a Bayesian framework, simultaneously modeling the MCR model and a multilevel factor analysis (MFA) model. The proposed model replaces the responses in the MCR part with the factor scores coming from an MFA model. Via a simulation study and the analysis of real data, we show that the proposed model is quite efficient when the responses of the MCR model are not measured directly but are latent variables such as the patient experience measurements in our motivating dataset.},
  archive      = {J_SIM},
  author       = {Benedict Orindi and Adrian Quintero and Luk Bruyneel and Baoyue Li and Emmanuel Lesaffre},
  doi          = {10.1002/sim.9768},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3128-3144},
  shortjournal = {Stat. Med.},
  title        = {A combined multilevel factor analysis and covariance regression model with mixed effects in the mean and variance structure},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A double-cox model for non-proportional hazards survival
analysis with frailty. <em>SIM</em>, <em>42</em>(18), 3114–3127. (<a
href="https://doi.org/10.1002/sim.9760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox regression, a semi-parametric method of survival analysis, is extremely popular in biomedical applications. The proportional hazards assumption is a key requirement in the Cox model. To accommodate non-proportional hazards, we propose to parameterize the shape parameter of the baseline hazard function using the additional, separate Cox-regression term which depends on the vector of the covariates. This parametrization retains the general form of the hazard function over the strata and is similar to one in Devarajan and Ebrahimi (Comput Stat Data Anal. 2011;55:667–676) in the case of the Weibull distribution, but differs for other hazard functions. We call this model the double-Cox model. We formally introduce the double-Cox model with shared frailty and investigate, by simulation, the estimation bias and the coverage of the proposed point and interval estimation methods for the Gompertz and the Weibull baseline hazards. For real-life applications with low frailty variance and a large number of clusters, the marginal likelihood estimation is almost unbiased and the profile likelihood-based confidence intervals provide good coverage for all model parameters. We also compare the results from the over-parametrized double-Cox model to those from the standard Cox model with frailty in the case of the scale-only proportional hazards. The model is illustrated on an example of the survival after a diagnosis of type 2 diabetes mellitus. The R programs for fitting the double-Cox model are available on Github.},
  archive      = {J_SIM},
  author       = {Alexander Begun and Elena Kulinskaya and Njabulo Ncube},
  doi          = {10.1002/sim.9760},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3114-3127},
  shortjournal = {Stat. Med.},
  title        = {A double-cox model for non-proportional hazards survival analysis with frailty},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximate bayesian computation for the natural history of
breast cancer, with application to data from a milan cohort study.
<em>SIM</em>, <em>42</em>(18), 3093–3113. (<a
href="https://doi.org/10.1002/sim.9756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore models for the natural history of breast cancer, where the main events of interest are the start of asymptomatic detectability of the disease (through screening) and the time of symptomatic detection (through symptoms). We develop several parametric specifications based on a cure rate structure, and present the results of the analysis of data collected as part of a motivating study from Milan. Participants in the study were part of a regional breast cancer screening program, and their ten-year trajectories were obtained from administrative data available from the Italian national health care system. We first present a tractable model for which we develop the likelihood contributions of the observed trajectories and perform maximum likelihood inference on the latent process. Likelihood based inference is not feasible for more flexible models, and we implement approximate Bayesian computation (ABC) for inference. Issues that arise from the use of ABC for model choice and parameter estimation are discussed, including the problem of choosing appropriate summary statistics. The estimated parameters of the underlying disease process allow for the study of the effect of different examination schedules (age range and frequency of screening examinations) on a population of asymptomatic subjects.},
  archive      = {J_SIM},
  author       = {Laura Bondi and Marco Bonetti and Denitsa Grigorova and Antonio Russo},
  doi          = {10.1002/sim.9756},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3093-3113},
  shortjournal = {Stat. Med.},
  title        = {Approximate bayesian computation for the natural history of breast cancer, with application to data from a milan cohort study},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relative sparsity for medical decision problems.
<em>SIM</em>, <em>42</em>(18), 3067–3092. (<a
href="https://doi.org/10.1002/sim.9755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing statistical methods can estimate a policy, or a mapping from covariates to decisions, which can then instruct decision makers (eg, whether to administer hypotension treatment based on covariates blood pressure and heart rate). There is great interest in using such data-driven policies in healthcare. However, it is often important to explain to the healthcare provider, and to the patient, how a new policy differs from the current standard of care. This end is facilitated if one can pinpoint the aspects of the policy (ie, the parameters for blood pressure and heart rate) that change when moving from the standard of care to the new, suggested policy. To this end, we adapt ideas from Trust Region Policy Optimization (TRPO). In our work, however, unlike in TRPO, the difference between the suggested policy and standard of care is required to be sparse, aiding with interpretability. This yields “relative sparsity,” where, as a function of a tuning parameter, we can approximately control the number of parameters in our suggested policy that differ from their counterparts in the standard of care (eg, heart rate only). We propose a criterion for selecting , perform simulations, and illustrate our method with a real, observational healthcare dataset, deriving a policy that is easy to explain in the context of the current standard of care. Our work promotes the adoption of data-driven decision aids, which have great potential to improve health outcomes.},
  archive      = {J_SIM},
  author       = {Samuel J. Weisenthal and Sally W. Thurston and Ashkan Ertefaie},
  doi          = {10.1002/sim.9755},
  journal      = {Statistics in Medicine},
  month        = {8},
  number       = {18},
  pages        = {3067-3092},
  shortjournal = {Stat. Med.},
  title        = {Relative sparsity for medical decision problems},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully order restricted multi-arm multi-stage clinical trial
design. <em>SIM</em>, <em>42</em>(17), 3050–3066. (<a
href="https://doi.org/10.1002/sim.9767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a multi-arm trial with two or more active treatments plus a control where it is reasonable to assume an order for the treatment effects of the active arms compared to control. For example, the arms could be a high dose and low dose of a new drug and a placebo. The objective of the trial is to compare each active arm to control while maintaining strong control of the type 1 error rate. We show that when the study is powered to identify all promising treatments, a design that uses the order of the treatment effects to calculate the test statistic and to set the order of testing requires a smaller sample size than a design where each active arm is tested against the control arm independently. Under the considered settings, the sample size for a single-stage trial and a two-stage trial was reduced by at least 20%.},
  archive      = {J_SIM},
  author       = {Lauren Kanapka and Anastasia Ivanova},
  doi          = {10.1002/sim.9767},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3050-3066},
  shortjournal = {Stat. Med.},
  title        = {Fully order restricted multi-arm multi-stage clinical trial design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Longitudinal self-learning of individualized treatment rules
in a nutrient supplementation trial with missing data. <em>SIM</em>,
<em>42</em>(17), 3032–3049. (<a
href="https://doi.org/10.1002/sim.9766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal outcomes are prevalent in clinical studies, where the presence of missing data may make the statistical learning of individualized treatment rules (ITRs) a much more challenging task. We analyzed a longitudinal calcium supplementation trial in the ELEMENT Project and established a novel ITR to reduce the risk of adverse outcomes of lead exposure on child growth and development. Lead exposure, particularly in the form of in utero exposure, can seriously impair children&#39;s health, especially their cognitive and neurobehavioral development, which necessitates clinical interventions such as calcium supplementation intake during pregnancy. Using the longitudinal outcomes from a randomized clinical trial of calcium supplementation, we developed a new ITR for daily calcium intake during pregnancy to mitigate persistent lead exposure in children at age 3 years. To overcome the technical challenges posed by missing data, we illustrate a new learning approach, termed longitudinal self-learning (LS-learning), that utilizes longitudinal measurements of child&#39;s blood lead concentration in the derivation of ITR. Our LS-learning method relies on a temporally weighted self-learning paradigm to synergize serially correlated training data sources. The resulting ITR is the first of this kind in precision nutrition that will contribute to the reduction of expected blood lead concentration in children aged 0-3 years should this ITR be implemented to the entire study population of pregnant women.},
  archive      = {J_SIM},
  author       = {Yiwang Zhou and Peter X. K. Song},
  doi          = {10.1002/sim.9766},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3032-3049},
  shortjournal = {Stat. Med.},
  title        = {Longitudinal self-learning of individualized treatment rules in a nutrient supplementation trial with missing data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating biological knowledge in analyses of
environmental mixtures and health. <em>SIM</em>, <em>42</em>(17),
3016–3031. (<a href="https://doi.org/10.1002/sim.9765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key goal of environmental health research is to assess the risk posed by mixtures of pollutants. As epidemiologic studies of mixtures can be expensive to conduct, it behooves researchers to incorporate prior knowledge about mixtures into their analyses. This work extends the Bayesian multiple index model (BMIM), which assumes the exposure-response function is a nonparametric function of a set of linear combinations of pollutants formed with a set of exposure-specific weights. The framework is attractive because it combines the flexibility of response-surface methods with the interpretability of linear index models. We propose three strategies to incorporate prior toxicological knowledge into construction of indices in a BMIM: (a) imposing directional homogeneity constraints on the weights, (b) structuring index weights by exposure transformations, and (c) placing informative priors on the index weights. We propose a novel prior specification that combines spike-and-slab variable selection with an informative Dirichlet distribution based on relative potency factors often derived from previous toxicological studies. In simulations we show that the proposed priors improve inferences when prior information is correct and can protect against misspecification suffered by naïve toxicological models when prior information is incorrect. Moreover, different strategies may be mixed-and-matched for different indices to suit available information (or lack thereof). We demonstrate the proposed methods on an analysis of data from the National Health and Nutrition Examination Survey and incorporate prior information on relative chemical potencies obtained from toxic equivalency factors available in the literature.},
  archive      = {J_SIM},
  author       = {Glen McGee and Ander Wilson and Brent A. Coull and Thomas F. Webster},
  doi          = {10.1002/sim.9765},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {3016-3031},
  shortjournal = {Stat. Med.},
  title        = {Incorporating biological knowledge in analyses of environmental mixtures and health},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian joint model for compositional mediation effect
selection in microbiome data. <em>SIM</em>, <em>42</em>(17), 2999–3015.
(<a href="https://doi.org/10.1002/sim.9764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing multivariate count data generated by high-throughput sequencing technology in microbiome research studies is challenging due to the high-dimensional and compositional structure of the data and overdispersion. In practice, researchers are often interested in investigating how the microbiome may mediate the relation between an assigned treatment and an observed phenotypic response. Existing approaches designed for compositional mediation analysis are unable to simultaneously determine the presence of direct effects, relative indirect effects, and overall indirect effects, while quantifying their uncertainty. We propose a formulation of a Bayesian joint model for compositional data that allows for the identification, estimation, and uncertainty quantification of various causal estimands in high-dimensional mediation analysis. We conduct simulation studies and compare our method&#39;s mediation effects selection performance with existing methods. Finally, we apply our method to a benchmark data set investigating the sub-therapeutic antibiotic treatment effect on body weight in early-life mice.},
  archive      = {J_SIM},
  author       = {Jingyan Fu and Matthew D. Koslovsky and Andreas M. Neophytou and Marina Vannucci},
  doi          = {10.1002/sim.9764},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2999-3015},
  shortjournal = {Stat. Med.},
  title        = {A bayesian joint model for compositional mediation effect selection in microbiome data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate single index modeling of longitudinal data with
multiple responses. <em>SIM</em>, <em>42</em>(17), 2982–2998. (<a
href="https://doi.org/10.1002/sim.9763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical studies, composite indices and/or scores are routinely used for predicting medical conditions of patients. These indices are usually developed from observed data of certain disease risk factors, and it has been demonstrated in the literature that single index models can provide a powerful tool for this purpose. In practice, the observed data of disease risk factors are often longitudinal in the sense that they are collected at multiple time points for individual patients, and there are often multiple aspects of a patient&#39;s medical condition that are of our concern. However, most existing single-index models are developed for cases with independent data and a single response variable, which are inappropriate for the problem just described in which within-subject observations are usually correlated and there are multiple mutually correlated response variables involved. This paper aims to fill this methodological gap by developing a single index model for analyzing longitudinal data with multiple responses. Both theoretical and numerical justifications show that the proposed new method provides an effective solution to the related research problem. It is also demonstrated using a dataset from the English Longitudinal Study of Aging.},
  archive      = {J_SIM},
  author       = {Zibo Tian and Peihua Qiu},
  doi          = {10.1002/sim.9763},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2982-2998},
  shortjournal = {Stat. Med.},
  title        = {Multivariate single index modeling of longitudinal data with multiple responses},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On asymptotic distributions of several test statistics for
familial relatedness in linear mixed models. <em>SIM</em>,
<em>42</em>(17), 2962–2981. (<a
href="https://doi.org/10.1002/sim.9762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the asymptotic distributions of the likelihood ratio test (LRT), the restricted likelihood ratio test (RLRT), the F and the sequence kernel association test (SKAT) statistics for testing an additive effect of the expected familial relatedness (FR) in a linear mixed model are examined based on an eigenvalue approach. First, the covariance structure for modeling the FR effect in a LMM is presented. Then, the multiplicity of eigenvalues for the log-likelihood and restricted log-likelihood is established under a replicate family setting and extended to a more general replicate family setting (GRFS) as well. After that, the asymptotic null distributions of LRT, RLRT, F and SKAT statistics under GRFS are derived. The asymptotic null distribution of SKAT for testing genetic rare variants is also constructed. In addition, a simple formula for sample size calculation is provided based on the restricted maximum likelihood estimate of the effect size for the expected FR. Finally, a power comparison of these test statistics on hypothesis test of the expected FR effect is made via simulation. The four test statistics are also applied to a data set from the UK Biobank.},
  archive      = {J_SIM},
  author       = {Nicholas Devogel and Paul L. Auer and Regina Manansala and Tao Wang},
  doi          = {10.1002/sim.9762},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2962-2981},
  shortjournal = {Stat. Med.},
  title        = {On asymptotic distributions of several test statistics for familial relatedness in linear mixed models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple multi-sample testing under arbitrary covariance
dependency. <em>SIM</em>, <em>42</em>(17), 2944–2961. (<a
href="https://doi.org/10.1002/sim.9761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern high-throughput biomedical devices routinely produce data on a large scale, and the analysis of high-dimensional datasets has become commonplace in biomedical studies. However, given thousands or tens of thousands of measured variables in these datasets, extracting meaningful features poses a challenge. In this article, we propose a procedure to evaluate the strength of the associations between a nominal (categorical) response variable and multiple features simultaneously. Specifically, we propose a framework of large-scale multiple testing under arbitrary correlation dependency among test statistics. First, marginal multinomial regressions are performed for each feature individually. Second, we use an approach of multiple marginal models for each baseline-category pair to establish asymptotic joint normality of the stacked vector of the marginal multinomial regression coefficients. Third, we estimate the (limiting) covariance matrix between the estimated coefficients from all marginal models. Finally, our approach approximates the realized false discovery proportion of a thresholding procedure for the marginal p -values for each baseline-category logit pair. The proposed approach offers a sensible trade-off between the expected numbers of true and false findings. Furthermore, we demonstrate a practical application of the method on hyperspectral imaging data. This dataset is obtained by a matrix-assisted laser desorption/ionization (MALDI) instrument. MALDI demonstrates tremendous potential for clinical diagnosis, particularly for cancer research. In our application, the nominal response categories represent cancer (sub-)types.},
  archive      = {J_SIM},
  author       = {Vladimir Vutov and Thorsten Dickhaus},
  doi          = {10.1002/sim.9761},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2944-2961},
  shortjournal = {Stat. Med.},
  title        = {Multiple multi-sample testing under arbitrary covariance dependency},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tailoring capture-recapture methods to estimate
registry-based case counts based on error-prone diagnostic signals.
<em>SIM</em>, <em>42</em>(17), 2928–2943. (<a
href="https://doi.org/10.1002/sim.9759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surveillance research is of great importance for effective and efficient epidemiological monitoring of case counts and disease prevalence. Taking specific motivation from ongoing efforts to identify recurrent cases based on the Georgia Cancer Registry, we extend recently proposed “anchor stream” sampling design and estimation methodology. Our approach offers a more efficient and defensible alternative to traditional capture-recapture (CRC) methods by leveraging a relatively small random sample of participants whose recurrence status is obtained through a principled application of medical records abstraction. This sample is combined with one or more existing signaling data streams, which may yield data based on arbitrarily non-representative subsets of the full registry population. The key extension developed here accounts for the common problem of false positive or negative diagnostic signals from the existing data stream(s). In particular, we show that the design only requires documentation of positive signals in these non-anchor surveillance streams, and permits valid estimation of the true case count based on an estimable positive predictive value (PPV) parameter. We borrow ideas from the multiple imputation paradigm to provide accompanying standard errors, and develop an adapted Bayesian credible interval approach that yields favorable frequentist coverage properties. We demonstrate the benefits of the proposed methods through simulation studies, and provide a data example targeting estimation of the breast cancer recurrence case count among Metro Atlanta area patients from the Georgia Cancer Registry-based Cancer Recurrence Information and Surveillance Program (CRISP) database.},
  archive      = {J_SIM},
  author       = {Lin Ge and Yuzi Zhang and Kevin C. Ward and Timothy L. Lash and Lance A. Waller and Robert H. Lyles},
  doi          = {10.1002/sim.9759},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2928-2943},
  shortjournal = {Stat. Med.},
  title        = {Tailoring capture-recapture methods to estimate registry-based case counts based on error-prone diagnostic signals},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel joint model of longitudinal continuous and binary
outcomes for hierarchically structured data. <em>SIM</em>,
<em>42</em>(17), 2914–2927. (<a
href="https://doi.org/10.1002/sim.9758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling has been a useful strategy for incorporating latent associations between different types of outcomes simultaneously, often focusing on a longitudinal continuous outcome characterized by an LME submodel and a terminal event subject to a Cox proportional hazard or parametric survival submodel. Applications to hierarchical longitudinal studies have been less frequent, particularly with respect to a binary process, which is commonly specified by a GLMM. Furthermore, many of the joint model developments have not allowed for investigations of nested effects, such as those arising from multicenter studies. To fill this gap, we propose a multilevel joint model that encompasses the LME submodel and GLMM through a Bayesian approach. Motivated by the need for timely detection of pulmonary exacerbation and characterization of irregularly observed lung function measurements in people living with cystic fibrosis (CF) receiving care across multiple centers, we apply the model to the data arising from US CF Foundation Patient Registry. In parallel, we examine the extent of bias induced by a non-hierarchical model. Our simulation study and application results show that incorporating the center effect along with individual stochastic variation over time within the LME submodel improves model estimation and prediction. Given that the center effect is evident in lung function observed in the CF population, accounting for center-specific power parameters by incorporating the symmetric power exponential power (spep) link function in the GLMM can facilitate more accurate conclusions in clinical studies.},
  archive      = {J_SIM},
  author       = {Grace Chen Zhou and Seongho Song and Rhonda D. Szczesniak},
  doi          = {10.1002/sim.9758},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2914-2927},
  shortjournal = {Stat. Med.},
  title        = {Multilevel joint model of longitudinal continuous and binary outcomes for hierarchically structured data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust rank-based meta-analyses for two-sample designs with
application to platelet counts of malaria infection data. <em>SIM</em>,
<em>42</em>(17), 2887–2913. (<a
href="https://doi.org/10.1002/sim.9757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose robust meta-analysis procedures for individual studies that report a broad range of robust summary statistics for a two-sample problem. Summary statistics of individual studies could be presented in different forms including full data, medians of the two samples, the Hodges-Lehman and Wilcoxon estimates of the location shift parameters. Data synthesis is made under both fixed-effect and random-effect meta-analysis models. We systematically compare these robust meta-analysis procedures via simulation studies to meta-analysis procedure based on sample means and variances from individual studies under a wide range of error distributions. We show that the coverage probabilities of the robust meta-analysis confidence intervals are quite close to the nominal confidence level. We also show that mean square error (MSE) of the robust meta-analysis estimator is considerably smaller than that of the non-robust meta-analysis estimator under the contaminated normal, heavy tailed and skewed error distributions. The robust meta-analysis procedures are then applied to platelet count reduction for malaria infected patients in Ghana.},
  archive      = {J_SIM},
  author       = {Yanda Lang and Joseph W. McKean and Omer Ozturk},
  doi          = {10.1002/sim.9757},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {17},
  pages        = {2887-2913},
  shortjournal = {Stat. Med.},
  title        = {Robust rank-based meta-analyses for two-sample designs with application to platelet counts of malaria infection data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Issues with the expected information matrix of linear mixed
models provided by popular statistical packages under missingness at
random dropout. <em>SIM</em>, <em>42</em>(16), 2873–2885. (<a
href="https://doi.org/10.1002/sim.9754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Likelihood-based methods ignoring missingness at random (MAR) produce consistent estimates provided that the whole likelihood model is correct. However, the expected information matrix (EIM) depends on the missingness mechanism. It has been shown that calculating the EIM by considering the missing data pattern as fixed (naive EIM) is incorrect under MAR, but the observed information matrix (OIM) is valid under any MAR missingness mechanism. In longitudinal studies, linear mixed models (LMMs) are routinely applied, often without any reference to missingness. However, most popular statistical packages currently provide precision measures for the fixed effects by inverting only the corresponding submatrix of the OIM (naive OIM), which is effectively equivalent to the naive EIM. In this paper, we analytically derive the correct form of the EIM of LMMs under MAR dropout to compare its differences with the naive EIM, which clarifies why the naive EIM fails under MAR. The asymptotic coverage rate of the naive EIM is numerically calculated for two parameters (population slope and slope difference between two groups) under various dropout mechanisms. The naive EIM can severely underestimate the true variance, especially when the degree of MAR dropout is high. Similar trends emerge under misspecified covariance structure, where, even the full OIM may lead to incorrect inferences and sandwich/bootstrap estimators are generally required. Results from simulation studies and application to real data led to similar conclusions. In LMMs, the full OIM should be preferred to the naive EIM/OIM, though if misspecified covariance structure is suspected, robust estimators should be used.},
  archive      = {J_SIM},
  author       = {Christos Thomadakis and Nikos Pantazis and Giota Touloumi},
  doi          = {10.1002/sim.9754},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2873-2885},
  shortjournal = {Stat. Med.},
  title        = {Issues with the expected information matrix of linear mixed models provided by popular statistical packages under missingness at random dropout},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confounding adjustment in the analysis of augmented
randomized controlled trial with hybrid control arm. <em>SIM</em>,
<em>42</em>(16), 2855–2872. (<a
href="https://doi.org/10.1002/sim.9753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The augmented randomized controlled trial (RCT) with hybrid control arm includes a randomized treatment group (RT), a smaller randomized control group (RC), and a large synthetic control (SC) group from real-world data. This kind of trial is useful when there is logistics and ethics hurdle to conduct a fully powered RCT with equal allocation, or when it is necessary to increase the power of the RCT by incorporating real-world data. A difficulty in the analysis of augmented RCT is that the SC and RC may be systematically different in the distribution of observed and unmeasured confounding factors, causing bias when the two control groups are analyzed together as hybrid controls. We propose to use propensity score (PS) analysis to balance the observed confounders between SC and RC. The possible bias caused by unmeasured confounders can be estimated and tested by analyzing propensity score adjusted outcomes from SC and RC. We also propose a partial bias correction (PBC) procedure to reduce bias from unmeasured confounding. Extensive simulation studies show that the proposed PS + PBC procedures can improve the efficiency and statistical power by effectively incorporating the SC into the RCT data analysis, while still control the estimation bias and Type I error inflation that might arise from unmeasured confounding. We illustrate the proposed statistical procedures with data from an augmented RCT in oncology.},
  archive      = {J_SIM},
  author       = {Liang Li and Thomas Jemielita},
  doi          = {10.1002/sim.9753},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2855-2872},
  shortjournal = {Stat. Med.},
  title        = {Confounding adjustment in the analysis of augmented randomized controlled trial with hybrid control arm},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian multi-arm multi-stage clinical trial design
incorporating information about treatment ordering. <em>SIM</em>,
<em>42</em>(16), 2841–2854. (<a
href="https://doi.org/10.1002/sim.9752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Arm Multi-Stage (MAMS) designs can notably improve efficiency in later stages of drug development, but they can be suboptimal when an order in the effects of the arms can be assumed. In this work, we propose a Bayesian multi-arm multi-stage trial design that selects all promising treatments with high probability and can efficiently incorporate information about the order in the treatment effects as well as incorporate prior knowledge on the treatments. A distinguishing feature of the proposed design is that it allows taking into account the uncertainty of the treatment effect order assumption and does not assume any parametric arm-response model. The design can provide control of the family-wise error rate under specific values of the control mean and we illustrate its operating characteristics in a study of symptomatic asthma. Via simulations, we compare the novel Bayesian design with frequentist multi-arm multi-stage designs and a frequentist order restricted design that does not account for the order uncertainty and demonstrate the gains in the sample sizes the proposed design can provide. We also find that the proposed design is robust to violations of the assumptions on the order.},
  archive      = {J_SIM},
  author       = {Alessandra Serra and Pavel Mozgunov and Thomas Jaki},
  doi          = {10.1002/sim.9752},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2841-2854},
  shortjournal = {Stat. Med.},
  title        = {A bayesian multi-arm multi-stage clinical trial design incorporating information about treatment ordering},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian borrowing for basket trials with longitudinal
outcomes. <em>SIM</em>, <em>42</em>(16), 2819–2840. (<a
href="https://doi.org/10.1002/sim.9751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basket trials are a novel clinical trial design in which a single intervention is investigated in multiple patient subgroups, or “baskets.” They offer the opportunity to share information between subgroups, potentially increasing power to detect treatment effects. Basket trials offer several advantages over running a series of separate trials, including reduced sample sizes, increased efficiency, and reduced costs. Primarily, basket trials have been undertaken in Phase II oncology settings, but could be a promising design in other areas where a shared underlying biological mechanism drives different diseases. One such area is chronic aging-related diseases. However, trials in this area frequently have longitudinal outcomes, and therefore suitable methods are needed to share information in this setting. In this paper, we extend three Bayesian borrowing methods for a basket design with continuous longitudinal endpoints. We demonstrate our methods on a real-world dataset and in a simulation study where the aim is to detect positive basketwise treatment effects. Methods are compared with standalone analysis of each basket without borrowing. Our results confirm that methods that share information can improve power to detect positive treatment effects and increase precision over independent analysis in many scenarios. In highly heterogeneous scenarios, there is a trade-off between increased power and increased risk of type I errors. Our proposed methods for basket trials with continuous longitudinal outcomes aim to facilitate their applicability in the area of aging related diseases. Choice of method should be made based on trial priorities and the expected basketwise distribution of treatment effects.},
  archive      = {J_SIM},
  author       = {Lou E. Whitehead and Oliver Sailer and Miles D. Witham and James M. S. Wason},
  doi          = {10.1002/sim.9751},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2819-2840},
  shortjournal = {Stat. Med.},
  title        = {Bayesian borrowing for basket trials with longitudinal outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Allocation in platform trials to maintain comparability
across time and eligibility. <em>SIM</em>, <em>42</em>(16), 2811–2818.
(<a href="https://doi.org/10.1002/sim.9750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform trials, with arms entering and leaving the trial over time, are complex. In addition to trial changes over time, certain arms in a platform may come with patient restrictions. Both of these issues (time and eligibility) can create biases in comparing active arms to control. The largest of these biases, using non-concurrent controls or including control patients that were ineligible for an active arm, have been extensively discussed in the literature. Here we show that even restricting to concurrent, eligible controls can induce biases if proper allocation ratios are not maintained throughout the platform. We also build on results in Ventz et al. Biostat., 19:199–215, 2018 to describe an algorithm that guarantees comparability between active and control groups in arm analyses in both time and eligibility, and allows for both re-randomization of patients and two-stage randomization procedures. The resulting method is both flexible and easily implemented, allowing robust comparisons when assumptions that underlie alternative randomization methods are in doubt.},
  archive      = {J_SIM},
  author       = {Kert Viele},
  doi          = {10.1002/sim.9750},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2811-2818},
  shortjournal = {Stat. Med.},
  title        = {Allocation in platform trials to maintain comparability across time and eligibility},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biomarker-based bayesian randomized clinical trial design
for identifying a target population. <em>SIM</em>, <em>42</em>(16),
2797–2810. (<a href="https://doi.org/10.1002/sim.9749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenges and potential benefits of incorporating biomarkers into clinical trial designs have been increasingly discussed, in particular to develop new agents for immune-oncology or targeted cancer therapies. To more accurately identify a sensitive subpopulation of patients, in many cases, a larger sample size—and consequently higher development costs and a longer study period—might be required. This article discusses a biomarker-based Bayesian (BM-Bay) randomized clinical trial design that incorporates a predictive biomarker measured on a continuous scale with pre-determined cutoff points or a graded scale to define multiple patient subpopulations. We consider designing interim analyses with suitable decision criteria to achieve correct and efficient identification of a target patient population for developing a new treatment. The proposed decision criteria allow not only the take-in of sensitive subpopulations but also the ruling-out of insensitive ones on the basis of the efficacy evaluation of a time-to-event outcome. Extensive simulation studies are conducted to evaluate the operating characteristics of the proposed method, including the probability of correct identification of the desired subpopulation and the expected number of patients, under a wide range of clinical scenarios. For illustration purposes, we apply the proposed method to design a randomized phase II immune-oncology clinical trial.},
  archive      = {J_SIM},
  author       = {Yasuo Sugitani and Satoshi Morita and Akiyoshi Nakakura and Hideharu Yamamoto},
  doi          = {10.1002/sim.9749},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2797-2810},
  shortjournal = {Stat. Med.},
  title        = {Biomarker-based bayesian randomized clinical trial design for identifying a target population},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size considerations for micro-randomized trials with
binary proximal outcomes. <em>SIM</em>, <em>42</em>(16), 2777–2796. (<a
href="https://doi.org/10.1002/sim.9748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-randomized trials (MRTs) are a novel experimental design for developing mobile health interventions. Participants are repeatedly randomized in an MRT, resulting in longitudinal data with time-varying treatments. Causal excursion effects are the main quantities of interest in MRT primary and secondary analyses. We consider MRTs where the proximal outcome is binary and the randomization probability is constant or time-varying but not data-dependent. We develop a sample size formula for detecting a nonzero marginal excursion effect. We prove that the formula guarantees power under a set of working assumptions. We demonstrate via simulation that violations of certain working assumptions do not affect the power, and for those that do, we point out the direction in which the power changes. We then propose practical guidelines for using the sample size formula. As an illustration, the formula is used to size an MRT on interventions for excessive drinking. The sample size calculator is implemented in R package MRTSampleSizeBinary and an interactive R Shiny app. This work can be used in trial planning for a wide range of MRTs with binary proximal outcomes.},
  archive      = {J_SIM},
  author       = {Eric R. Cohn and Tianchen Qian and Susan A. Murphy},
  doi          = {10.1002/sim.9748},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2777-2796},
  shortjournal = {Stat. Med.},
  title        = {Sample size considerations for micro-randomized trials with binary proximal outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust and fast two-sample test of equal correlations with
an application to differential co-expression. <em>SIM</em>,
<em>42</em>(16), 2760–2776. (<a
href="https://doi.org/10.1002/sim.9747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust and fast two-sample test for equal Pearson correlation coefficients (PCCs) is important in solving many biological problems, including, for example, analysis of differential co-expression. However, few existing methods for this test can achieve robustness against deviation from normal distributions, accuracy under small sample sizes, and computational efficiency simultaneously. Here, we propose a new method for testing differential correlation using a saddlepoint approximation of the residual bootstrap (DICOSAR). To achieve robustness, accuracy, and efficiency, DICOSAR combines the ideas underlying the pooled residual bootstrap, the signed root of a likelihood ratio statistic, and a multivariate saddlepoint approximation. Through a comprehensive simulation study and a real data analysis of gene co-expression, we demonstrate that DICOSAR is accurate and robust in controlling the type I error rate for detecting differential correlation and provides a faster alternative to the bootstrap and permutation methods. We further show that DICOSAR can also be used for testing differential correlation matrices. These results suggest that DICOSAR provides an analytical approach to facilitate rapid testing for the equality of PCCs in large-scale analysis.},
  archive      = {J_SIM},
  author       = {Liang He and Ian Philipp and Stephanie Webster and Jacob v. B. Hjelmborg and Alexander M. Kulminski},
  doi          = {10.1002/sim.9747},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2760-2776},
  shortjournal = {Stat. Med.},
  title        = {A robust and fast two-sample test of equal correlations with an application to differential co-expression},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Saddlepoint approximations to score test statistics in
logistic regression for analyzing genome-wide association studies.
<em>SIM</em>, <em>42</em>(16), 2746–2759. (<a
href="https://doi.org/10.1002/sim.9746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate saddlepoint approximations of tail probabilities of the score test statistic in logistic regression for genome-wide association studies. The inaccuracy in the normal approximation of the score test statistic increases with increasing imbalance in the response and with decreasing minor allele counts. Applying saddlepoint approximation methods greatly improve the accuracy, even far out in the tails of the distribution. By using exact results for a simple logistic regression model, as well as simulations for models with nuisance parameters, we compare double saddlepoint methods for computing two-sided -values and mid- -values. These methods are also compared to a recent single saddlepoint procedure. We investigate the methods further on data from UK Biobank with skin and soft tissue infections as phenotype, using both common and rare variants.},
  archive      = {J_SIM},
  author       = {Pål V. Johnsen and Øyvind Bakke and Thea Bjørnland and Andrew Thomas DeWan and Mette Langaas},
  doi          = {10.1002/sim.9746},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2746-2759},
  shortjournal = {Stat. Med.},
  title        = {Saddlepoint approximations to score test statistics in logistic regression for analyzing genome-wide association studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantile partially linear additive model for data with
dropouts and an application to modeling cognitive decline. <em>SIM</em>,
<em>42</em>(16), 2729–2745. (<a
href="https://doi.org/10.1002/sim.9745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Alzheimer&#39;s Coordinating Center Uniform Data Set includes test results from a battery of cognitive exams. Motivated by the need to model the cognitive ability of low-performing patients we create a composite score from ten tests and propose to model this score using a partially linear quantile regression model for longitudinal studies with non-ignorable dropouts. Quantile regression allows for modeling non-central tendencies. The partially linear model accommodates nonlinear relationships between some of the covariates and cognitive ability. The data set includes patients that leave the study prior to the conclusion. Ignoring such dropouts will result in biased estimates if the probability of dropout depends on the response. To handle this challenge, we propose a weighted quantile regression estimator where the weights are inversely proportional to the estimated probability a subject remains in the study. We prove that this weighted estimator is a consistent and efficient estimator of both linear and nonlinear effects.},
  archive      = {J_SIM},
  author       = {Adam Maidman and Lan Wang and Xiao-Hua Zhou and Ben Sherwood},
  doi          = {10.1002/sim.9745},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {16},
  pages        = {2729-2745},
  shortjournal = {Stat. Med.},
  title        = {Quantile partially linear additive model for data with dropouts and an application to modeling cognitive decline},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of bias-adjusted generalized estimating
equations for sparse binary data in small-sample longitudinal studies.
<em>SIM</em>, <em>42</em>(15), 2711–2727. (<a
href="https://doi.org/10.1002/sim.9744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a generalized estimating equation (GEE) can lead to a bias in regression coefficients for a small sample or sparse data. The bias-corrected GEE (BCGEE) and penalized GEE (PGEE) were proposed to resolve the small-sample bias. Moreover, the standard sandwich covariance estimator leads to a bias of standard error for small samples; several modified covariance estimators have been proposed to address this issue. We review the modified GEEs and modified covariance estimators, and evaluate their performance in sparse binary data from small-sample longitudinal studies. The simulation results showed that GEE and BCGEE often failed to achieve convergence, whereas the convergence proportion for PGEE was quite high. The bias for the regression coefficients was generally in the ascending order of PGEE BCGEE GEE. However, PGEE and BCGEE did not sufficiently remove the bias involving 20–30 subjects with unequal exposure levels with a 5% response rate. The coverage probability (CP) of the confidence interval for BCGEE was relatively poor compared with GEE and PGEE. The CP with the sandwich covariance estimator deteriorated regardless of the GEE methods under the small sample size and low response rate, whereas the CP with the modified covariance estimators—such as Morel&#39;s method—was relatively acceptable. PGEE will be the reasonable way for analyzing sparse binary data in small-sample studies. Instead of using the standard sandwich covariance estimator, one should always apply the modified covariance estimators for analyzing these data.},
  archive      = {J_SIM},
  author       = {Masahiko Gosho and Ryota Ishii and Hisashi Noma and Kazushi Maruo},
  doi          = {10.1002/sim.9744},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2711-2727},
  shortjournal = {Stat. Med.},
  title        = {A comparison of bias-adjusted generalized estimating equations for sparse binary data in small-sample longitudinal studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Models for cluster randomized designs using ranked set
sampling. <em>SIM</em>, <em>42</em>(15), 2692–2710. (<a
href="https://doi.org/10.1002/sim.9743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized designs (CRD) provide a rigorous development for randomization principles for studies where treatments are allocated to cluster units rather than the individual subjects within clusters. It is known that CRDs are less efficient than completely randomized designs since the randomization of treatment allocation is applied to the cluster units. To mitigate this problem, we embed a ranked set sampling design from survey sampling studies into CRD for the selection of both cluster and subsampling units. We show that ranking groups in ranked set sampling act like a covariate, reduce the expected mean squared cluster error, and increase the precision of the sampling design. We provide an optimality result to determine the sample sizes at cluster and sub-sample level. We apply the proposed sampling design to a dental study on human tooth size, and to a longitudinal study from an education intervention program.},
  archive      = {J_SIM},
  author       = {Omer Ozturk and Olena Kravchuk and Richard Jarrett},
  doi          = {10.1002/sim.9743},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2692-2710},
  shortjournal = {Stat. Med.},
  title        = {Models for cluster randomized designs using ranked set sampling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A non-parametric bayesian approach for adjusting partial
compliance in sequential decision making. <em>SIM</em>, <em>42</em>(15),
2661–2691. (<a href="https://doi.org/10.1002/sim.9742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for estimating the mean outcome under a given sequential treatment rule often rely on intention-to-treat analyses, which estimate the effect of following a certain treatment rule regardless of compliance behavior of patients. There are two major concerns with intention-to-treat analyses: (1) the estimated effects are often biased toward the null effect; (2) the results are not generalizable and reproducible due to the potentially differential compliance behavior. These are particularly problematic in settings with a high level of non-compliance, such as substance use disorder studies. Our work is motivated by the Adaptive Treatment for Alcohol and Cocaine Dependence study (ENGAGE), which is a multi-stage trial that aimed to construct optimal treatment strategies to engage patients in therapy. Due to the relatively low level of compliance in this trial, intention-to-treat analyses essentially estimate the effect of being randomized to a certain treatment, instead of the actual effect of the treatment. We obviate this challenge by defining the target parameter as the mean outcome under a dynamic treatment regime conditional on a potential compliance stratum. We propose a flexible non-parametric Bayesian approach based on principal stratification, which consists of a Gaussian copula model for the joint distribution of the potential compliances, and a Dirichlet process mixture model for the treatment sequence specific outcomes. We conduct extensive simulation studies which highlight the utility of our approach in the context of multi-stage randomized trials. We show robustness of our estimator to non-linear and non-Gaussian settings as well.},
  archive      = {J_SIM},
  author       = {Indrabati Bhattacharya and Brent A. Johnson and William J. Artman and Andrew Wilson and Kevin G. Lynch and James R. McKay and Ashkan Ertefaie},
  doi          = {10.1002/sim.9742},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2661-2691},
  shortjournal = {Stat. Med.},
  title        = {A non-parametric bayesian approach for adjusting partial compliance in sequential decision making},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Propensity score analysis with local balance. <em>SIM</em>,
<em>42</em>(15), 2637–2660. (<a
href="https://doi.org/10.1002/sim.9741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most propensity score (PS) analysis methods rely on a correctly specified parametric PS model, which may result in biased estimation of the average treatment effect (ATE) when the model is misspecified. More flexible nonparametric models for treatment assignment alleviate this issue, but they do not always guarantee covariate balance. Methods that force balance in the means of covariates and their transformations between the treatment groups, termed global balance in this article, do not always lead to unbiased estimation of ATE. Their estimated propensity scores only ensure global balance but not the balancing property, which is defined as the conditional independence between treatment assignment and covariates given the propensity score. The balancing property implies not only global balance but also local balance—the mean balance of covariates in propensity score stratified sub-populations. Local balance implies global balance, but the reverse is false. We propose the propensity score with local balance (PSLB) methodology, which incorporates nonparametric propensity score models and optimizes local balance. Extensive numerical studies showed that the proposed method can substantially outperform existing methods that estimate the propensity score by optimizing global balance, when the model is misspecified. The proposed method is implemented in the R package PSLB .},
  archive      = {J_SIM},
  author       = {Yan Li and Liang Li},
  doi          = {10.1002/sim.9741},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2637-2660},
  shortjournal = {Stat. Med.},
  title        = {Propensity score analysis with local balance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly structured sparsity for grouped multivariate
responses with application to functional outcome score modeling.
<em>SIM</em>, <em>42</em>(15), 2619–2636. (<a
href="https://doi.org/10.1002/sim.9740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is motivated by the need to accurately model a vector of responses related to pediatric functional status using administrative health data from inpatient rehabilitation visits. The components of the responses have known and structured interrelationships. To make use of these relationships in modeling, we develop a two-pronged regularization approach to borrow information across the responses. The first component of our approach encourages joint selection of the effects of each variable across possibly overlapping groups of related responses and the second component encourages shrinkage of effects towards each other for related responses. As the responses in our motivating study are not normally-distributed, our approach does not rely on an assumption of multivariate normality of the responses. We show that with an adaptive version of our penalty, our approach results in the same asymptotic distribution of estimates as if we had known in advance which variables have non-zero effects and which variables have the same effects across some outcomes. We demonstrate the performance of our method in extensive numerical studies and in an application in the prediction of functional status of pediatric patients using administrative health data in a population of children with neurological injury or illness at a large children&#39;s hospital.},
  archive      = {J_SIM},
  author       = {Jared D. Huling and Jennifer P. Lundine and Julie C. Leonard},
  doi          = {10.1002/sim.9740},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2619-2636},
  shortjournal = {Stat. Med.},
  title        = {Doubly structured sparsity for grouped multivariate responses with application to functional outcome score modeling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the parameter estimation of box-cox transformation cure
model. <em>SIM</em>, <em>42</em>(15), 2600–2618. (<a
href="https://doi.org/10.1002/sim.9739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an improved estimation method for the Box-Cox transformation (BCT) cure rate model parameters. Specifically, we propose a generic maximum likelihood estimation algorithm through a non-linear conjugate gradient (NCG) method with an efficient line search technique. We then apply the proposed NCG algorithm to BCT cure model. Through a detailed simulation study, we compare the model fitting results of the NCG algorithm with those obtained by the existing expectation maximization (EM) algorithm. First, we show that our proposed NCG algorithm allows simultaneous maximization of all model parameters unlike the EM algorithm when the likelihood surface is flat with respect to the BCT index parameter. Then, we show that the NCG algorithm results in smaller bias and noticeably smaller root mean square error of the estimates of the model parameters that are associated with the cure rate. This results in more accurate and precise inference on the cure rate. In addition, we show that when the sample size is large the NCG algorithm, which only needs the computation of the gradient and not the Hessian, takes less CPU time to produce the estimates. These advantages of the NCG algorithm allows us to conclude that the NCG method should be the preferred estimation method over the already existing EM algorithm in the context of BCT cure model. Finally, we apply the NCG algorithm to analyze a well-known melanoma data and show that it results in a better fit when compared to the EM algorithm.},
  archive      = {J_SIM},
  author       = {Suvra Pal and Souvik Roy},
  doi          = {10.1002/sim.9739},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2600-2618},
  shortjournal = {Stat. Med.},
  title        = {On the parameter estimation of box-cox transformation cure model},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Designing superiority trials with window mean survival time
as a primary endpoint. <em>SIM</em>, <em>42</em>(15), 2590–2599. (<a
href="https://doi.org/10.1002/sim.9738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Window mean survival time (WMST), a simple extension of restricted mean survival time (RMST), allows for clinicians to evaluate the mean survival difference between treatment groups in specific windows of time during the follow-up period of a trial. The advantages of WMST are numerous. Not only does it produce estimates of treatment effect that can be meaningfully interpreted, but also has power advantages over competing methods when hazards are non-proportional (NPH). WMST, like RMST, is currently underutilized due to clinicians&#39; lack of familiarity with tests comparing mean survival times and the lack of tools to facilitate trial design with this endpoint. The aim of this article is to provide investigators with insights and software to design trials with WMST as the primary endpoint. Functions for performing power and sample size calculations are provided in the survWMST package in R available on GitHub.},
  archive      = {J_SIM},
  author       = {Mitchell Paukner and Richard Chappell},
  doi          = {10.1002/sim.9738},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2590-2599},
  shortjournal = {Stat. Med.},
  title        = {Designing superiority trials with window mean survival time as a primary endpoint},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised inference for nonparametric logistic
regression. <em>SIM</em>, <em>42</em>(15), 2573–2589. (<a
href="https://doi.org/10.1002/sim.9737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the nonparametric function in nonparametric logistic regression under semi-supervised framework, where a relatively small size labeled data set collected by case-control sampling and a relatively large size of unlabeled data containing only observations of predictors are available. This problem arises in various applications when the outcome variable is expensive or difficult to be observed directly. A two-stage nonparametric semi-supervised estimator based on spline method is proposed to estimate the target regression function by maximizing the likelihood function of the labeled case-control data. The unlabeled data are used in the first stage for estimating the density function that involves in the likelihood function. The consistency and functional asymptotic normality of the semi-supervised two-stage estimator are established under mild conditions. The proposed method, by making use of the unlabeled data, produces more efficient estimation of the target function than the traditional supervised counterpart. The performance of the proposed method is evaluated through extensive simulation studies. An application is illustrated with an analysis of a skin segmentation data.},
  archive      = {J_SIM},
  author       = {Tong Wang and Wenlu Tang and Yuanyuan Lin and Wen Su},
  doi          = {10.1002/sim.9737},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2573-2589},
  shortjournal = {Stat. Med.},
  title        = {Semi-supervised inference for nonparametric logistic regression},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The mean residual life model for the right-censored data in
the presence of covariate measurement errors. <em>SIM</em>,
<em>42</em>(15), 2557–2572. (<a
href="https://doi.org/10.1002/sim.9736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we consider the mean residual life regression model in the presence of covariate measurement errors. In the whole cohort, the surrogate variable of the error-prone covariate is available for each subject, while the instrumental variable (IV), which is related to the underlying true covariates, is measured only for some subjects, the calibration sample. Without specifying distributions of measurement errors but assuming that the IV is missing at random, we develop two estimation methods, the IV calibration and cohort estimators, for the regression parameters by solving estimation equations (EEs) based on the calibration sample and cohort sample, respectively. To improve estimation efficiency, a synthetic estimator is derived by applying the generalized method of moment for all EEs. The large sample properties of the proposed estimators are established and their finite sample performance are evaluated via simulation studies. Simulation results show that the cohort and synthetic estimators outperform the IV calibration estimator and the relative efficiency of the cohort and synthetic estimators mainly depends on the missing rate of IV. In the case of low missing rate, the synthetic estimator is more efficient than the cohort estimator, while the result can be reversed when the missing rate is high. We illustrate the proposed method by application to data from the patients with stage 5 chronic kidney disease in Taiwan.},
  archive      = {J_SIM},
  author       = {Chyong-Mei Chen and Shuo-Chun Weng and Jia-Ren Tsai and Pao-sheng Shen},
  doi          = {10.1002/sim.9736},
  journal      = {Statistics in Medicine},
  month        = {7},
  number       = {15},
  pages        = {2557-2572},
  shortjournal = {Stat. Med.},
  title        = {The mean residual life model for the right-censored data in the presence of covariate measurement errors},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Partially linear bayesian modeling of longitudinal rank and
time-to-event data using accelerated failure time model with application
to brain tumor data. <em>SIM</em>, <em>42</em>(14), 2521–2556. (<a
href="https://doi.org/10.1002/sim.9735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling of longitudinal rank and time-to-event data with random effects model using a Bayesian approach is presented. Accelerated failure time (AFT) models can be used for the analysis of time-to-event data to estimate the effects of covariates on acceleration/deceleration of the survival time. The parametric AFT models require determining the event time distribution. So, we suppose that the time variable is modeled with Weibull AFT distribution. In many real-life applications, it is difficult to determine the appropriate distribution. To avoid this restriction, several semiparametric AFT models were proposed, containing spline-based model. So, we propose a flexible extension of the accelerated failure time model. Furthermore, the usual joint linear model, a joint partially linear model, is also considered containing the nonlinear effect of time on the longitudinal rank responses and nonlinear and time-dependent effects of covariates on the hazard. Also, a Bayesian approach that yields Bayesian estimates of the model&#39;s parameters is used. Some simulation studies are conducted to estimate parameters of the considered models. The model is applied to a real brain tumor patient&#39;s data set that underwent surgery. The results of analyzing data are presented to represent the method.},
  archive      = {J_SIM},
  author       = {Maryam Aghayerashti and Ehsan Bahrami Samani and Ahmad Pour-Rashidi},
  doi          = {10.1002/sim.9735},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2521-2556},
  shortjournal = {Stat. Med.},
  title        = {Partially linear bayesian modeling of longitudinal rank and time-to-event data using accelerated failure time model with application to brain tumor data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Point estimation for adaptive trial designs II: Practical
considerations and guidance. <em>SIM</em>, <em>42</em>(14), 2496–2520.
(<a href="https://doi.org/10.1002/sim.9734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In adaptive clinical trials, the conventional end-of-trial point estimate of a treatment effect is prone to bias, that is, a systematic tendency to deviate from its true value. As stated in recent FDA guidance on adaptive designs, it is desirable to report estimates of treatment effects that reduce or remove this bias. However, it may be unclear which of the available estimators are preferable, and their use remains rare in practice. This article is the second in a two-part series that studies the issue of bias in point estimation for adaptive trials. Part I provided a methodological review of approaches to remove or reduce the potential bias in point estimation for adaptive designs. In part II, we discuss how bias can affect standard estimators and assess the negative impact this can have. We review current practice for reporting point estimates and illustrate the computation of different estimators using a real adaptive trial example (including code), which we use as a basis for a simulation study. We show that while on average the values of these estimators can be similar, for a particular trial realization they can give noticeably different values for the estimated treatment effect. Finally, we propose guidelines for researchers around the choice of estimators and the reporting of estimates following an adaptive design. The issue of bias should be considered throughout the whole lifecycle of an adaptive design, with the estimation strategy prespecified in the statistical analysis plan. When available, unbiased or bias-reduced estimates are to be preferred.},
  archive      = {J_SIM},
  author       = {David S. Robertson and Babak Choodari-Oskooei and Munya Dimairo and Laura Flight and Philip Pallmann and Thomas Jaki},
  doi          = {10.1002/sim.9734},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2496-2520},
  shortjournal = {Stat. Med.},
  title        = {Point estimation for adaptive trial designs II: Practical considerations and guidance},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online error rate control for platform trials. <em>SIM</em>,
<em>42</em>(14), 2475–2495. (<a
href="https://doi.org/10.1002/sim.9733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Platform trials evaluate multiple experimental treatments under a single master protocol, where new treatment arms are added to the trial over time. Given the multiple treatment comparisons, there is the potential for inflation of the overall type I error rate, which is complicated by the fact that the hypotheses are tested at different times and are not necessarily pre-specified. Online error rate control methodology provides a possible solution to the problem of multiplicity for platform trials where a relatively large number of hypotheses are expected to be tested over time. In the online multiple hypothesis testing framework, hypotheses are tested one-by-one over time, where at each time-step an analyst decides whether to reject the current null hypothesis without knowledge of future tests but based solely on past decisions. Methodology has recently been developed for online control of the false discovery rate as well as the familywise error rate (FWER). In this article, we describe how to apply online error rate control to the platform trial setting, present extensive simulation results, and give some recommendations for the use of this new methodology in practice. We show that the algorithms for online error rate control can have a substantially lower FWER than uncorrected testing, while still achieving noticeable gains in power when compared with the use of a Bonferroni correction. We also illustrate how online error rate control would have impacted a currently ongoing platform trial.},
  archive      = {J_SIM},
  author       = {David S. Robertson and James M. S. Wason and Franz König and Martin Posch and Thomas Jaki},
  doi          = {10.1002/sim.9733},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2475-2495},
  shortjournal = {Stat. Med.},
  title        = {Online error rate control for platform trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and maximum likelihood based inference of
interval-censored data with unknown upper limits and time-dependent
covariates. <em>SIM</em>, <em>42</em>(14), 2455–2474. (<a
href="https://doi.org/10.1002/sim.9732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the nature of study design or other reasons, the upper limits of the interval-censored data with multiple visits are unknown. A naïve approach is to treat the last observed time as the exact event time, which may induce biased estimators of the model parameters. In this paper, we first develop a Cox model with time-dependent covariates for the event time and a proportional hazards model with frailty for the gap time. We then construct the upper limits using the latent gap times to resolve the issue of interval-censored event time data with unknown upper limits. A data-augmentation technique and a Monte Carlo EM (MCEM) algorithm are developed to facilitate computation. Theoretical properties of the computational algorithm are also investigated. Additionally, new model comparison criteria are developed to assess the fit of the gap time data as well as the fit of the event time data conditional on the gap time data. Our proposed method compares favorably with competing methods in both simulation study and real data analysis.},
  archive      = {J_SIM},
  author       = {Jing Wu and Lijiang Geng and Angela Starkweather and Ming-Hui Chen},
  doi          = {10.1002/sim.9732},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2455-2474},
  shortjournal = {Stat. Med.},
  title        = {Modeling and maximum likelihood based inference of interval-censored data with unknown upper limits and time-dependent covariates},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Summarizing empirical information on between-study
heterogeneity for bayesian random-effects meta-analysis. <em>SIM</em>,
<em>42</em>(14), 2439–2454. (<a
href="https://doi.org/10.1002/sim.9731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Bayesian meta-analysis, the specification of prior probabilities for the between-study heterogeneity is commonly required, and is of particular benefit in situations where only few studies are included. Among the considerations in the set-up of such prior distributions, the consultation of available empirical data on a set of relevant past analyses sometimes plays a role. How exactly to summarize historical data sensibly is not immediately obvious; in particular, the investigation of an empirical collection of heterogeneity estimates will not target the actual problem and will usually only be of limited use. The commonly used normal-normal hierarchical model for random-effects meta-analysis is extended to infer a heterogeneity prior. Using an example data set, we demonstrate how to fit a distribution to empirically observed heterogeneity data from a set of meta-analyses. Considerations also include the choice of a parametric distribution family. Here, we focus on simple and readily applicable approaches to then translate these into (prior) probability distributions.},
  archive      = {J_SIM},
  author       = {Christian Röver and Sibylle Sturtz and Jona Lilienthal and Ralf Bender and Tim Friede},
  doi          = {10.1002/sim.9731},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2439-2454},
  shortjournal = {Stat. Med.},
  title        = {Summarizing empirical information on between-study heterogeneity for bayesian random-effects meta-analysis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploration of model misspecification in latent class
methods for longitudinal data: Correlation structure matters.
<em>SIM</em>, <em>42</em>(14), 2420–2438. (<a
href="https://doi.org/10.1002/sim.9730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling longitudinal trajectories and identifying latent classes of trajectories is of great interest in biomedical research, and software to identify latent classes of such is readily available for latent class trajectory analysis (LCTA), growth mixture modeling (GMM) and covariance pattern mixture models (CPMM). In biomedical applications, the level of within-person correlation is often non-negligible, which can impact the model choice and interpretation. LCTA does not incorporate this correlation. GMM does so through random effects, while CPMM specifies a model for within-class marginal covariance matrix. Previous work has investigated the impact of constraining covariance structures, both within and across classes, in GMMs—an approach often used to solve convergence problems. Using simulation, we focused specifically on how misspecification of the temporal correlation structure and strength, but correct variances, impacts class enumeration and parameter estimation under LCTA and CPMM. We found (1) even in the presence of weak correlation, LCTA often does not reproduce original classes, (2) CPMM performs well in class enumeration when the correct correlation structure is selected, and (3) regardless of misspecification of the correlation structure, both LCTA and CPMM give unbiased estimates of the class trajectory parameters when the within-individual correlation is weak and the number of classes is correctly specified. However, the bias increases markedly when the correlation is moderate for LCTA and when the incorrect correlation structure is used for CPMM. This work highlights the importance of correlation alone in obtaining appropriate model interpretations and provides insight into model choice.},
  archive      = {J_SIM},
  author       = {Megan L. Neely and Carl F. Pieper and Bida Gu and Natalia O. Dmitrieva and Jane F. Pendergast},
  doi          = {10.1002/sim.9730},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2420-2438},
  shortjournal = {Stat. Med.},
  title        = {Exploration of model misspecification in latent class methods for longitudinal data: Correlation structure matters},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CRM and partial order CRM with adaptive rescaling for
dose-finding in immunotherapy trials with a continuous outcome.
<em>SIM</em>, <em>42</em>(14), 2409–2419. (<a
href="https://doi.org/10.1002/sim.9729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many phase 1 oncology trials of immunotherapies, no dose-limiting toxicities are observed and the maximum tolerated dose cannot be identified. In these settings, dose-finding can be guided by a biomarker of response rather than the occurrences of dose-limiting toxicity. The recommended phase 2 dose can be defined as the dose with mean response equal to a prespecified value of a continuous response biomarker. To target the mean of a continuous biomarker, we build on the idea of the continual reassessment method and the quasi-Bernoulli likelihood. We extend the design to a problem of finding the recommended phase 2 dose combination in a trial with multiple immunotherapies.},
  archive      = {J_SIM},
  author       = {Pooja T. Saha and Jason P. Fine and Anastasia Ivanova},
  doi          = {10.1002/sim.9729},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2409-2419},
  shortjournal = {Stat. Med.},
  title        = {CRM and partial order CRM with adaptive rescaling for dose-finding in immunotherapy trials with a continuous outcome},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous hypothesis testing for multiple competing risks
in comparative clinical trials. <em>SIM</em>, <em>42</em>(14),
2394–2408. (<a href="https://doi.org/10.1002/sim.9728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competing risks data are commonly encountered in randomized clinical trials or observational studies. Ignoring competing risks in survival analysis leads to biased risk estimates and improper conclusions. Often, one of the competing events is of primary interest and the rest competing events are handled as nuisances. These approaches can be inadequate when multiple competing events have important clinical interpretations and thus of equal interest. For example, in COVID-19 in-patient treatment trials, the outcomes of COVID-19 related hospitalization are either death or discharge from hospital, which have completely different clinical implications and are of equal interest, especially during the pandemic. In this paper we develop nonparametric estimation and simultaneous inferential methods for multiple cumulative incidence functions (CIFs) and corresponding restricted mean times. Based on Monte Carlo simulations and a data analysis of COVID-19 in-patient treatment clinical trial, we demonstrate that the proposed method provides global insights of the treatment effects across multiple endpoints.},
  archive      = {J_SIM},
  author       = {Jiyang Wen and Mei-Cheng Wang and Chen Hu},
  doi          = {10.1002/sim.9728},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2394-2408},
  shortjournal = {Stat. Med.},
  title        = {Simultaneous hypothesis testing for multiple competing risks in comparative clinical trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference with longitudinal data subject to irregular
assessment times. <em>SIM</em>, <em>42</em>(14), 2361–2393. (<a
href="https://doi.org/10.1002/sim.9727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data collected in the context of usual care present a rich source of longitudinal data for research, but often require analyses that simultaneously enable causal inferences from observational data while handling irregular and informative assessment times. An inverse-weighting approach to this was recently proposed, and handles the case where the assessment times are at random (ie, conditionally independent of the outcome process given the observed history). In this paper, we extend the inverse-weighting approach to handle a special case of assessment not at random, where assessment and outcome processes are conditionally independent given past observed covariates and random effects. We use multiple outputation to accomplish the same purpose as inverse-weighting, and apply it to the Liang semi-parametric joint model. Moreover, we develop an alternative joint model that does not require covariates for the outcome model to be known at times where there is no assessment of the outcome. We examine the performance of these methods through simulation and illustrate them through a study of the causal effect of wheezing on time spent playing outdoors among children aged 2–9 years and enrolled in the TargetKids! study.},
  archive      = {J_SIM},
  author       = {Eleanor M. Pullenayegum and Catherine Birken and Jonathon Maguire},
  doi          = {10.1002/sim.9727},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2361-2393},
  shortjournal = {Stat. Med.},
  title        = {Causal inference with longitudinal data subject to irregular assessment times},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards robust and accurate estimates of the incubation time
distribution, with focus on upper tail probabilities and SARS-CoV-2
infection. <em>SIM</em>, <em>42</em>(14), 2341–2360. (<a
href="https://doi.org/10.1002/sim.9726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quarantine length for individuals who have been at risk for infection with SARS-CoV-2 has been based on estimates of the incubation time distribution. The time of infection is often not known exactly, yielding data with an interval censored time origin. We give a detailed account of the data structure, likelihood formulation and assumptions usually made in the literature: (i) the risk of infection is assumed constant on the exposure window and (ii) the incubation time follows a specific parametric distribution. The impact of these assumptions remains unclear, especially for the right tail of the distribution which informs quarantine policy. We quantified bias in percentiles by means of simulation studies that mimic reality as close as possible. If assumption (i) is not correct, then median and upper percentiles are affected similarly, whereas misspecification of the parametric approach (ii) mainly affects upper percentiles. The latter may yield considerable bias. We suggest a semiparametric method that provides more robust estimates without the need of a parametric choice. Additionally, we used a simulation study to evaluate a method that has been suggested if all infection times are left censored. It assumes that the width of the interval from infection to latest possible exposure follows a uniform distribution. This assumption gave biased results in the exponential phase of an outbreak. Our application to open source data suggests that focus should be on the level of information in the observations, as expressed by the width of exposure windows, rather than the number of observations.},
  archive      = {J_SIM},
  author       = {Vera H. Arntzen and Marta Fiocco and Nils Leitzinger and Ronald B. Geskus},
  doi          = {10.1002/sim.9726},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2341-2360},
  shortjournal = {Stat. Med.},
  title        = {Towards robust and accurate estimates of the incubation time distribution, with focus on upper tail probabilities and SARS-CoV-2 infection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Permutation-based true discovery proportions for functional
magnetic resonance imaging cluster analysis. <em>SIM</em>,
<em>42</em>(14), 2311–2340. (<a
href="https://doi.org/10.1002/sim.9725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a permutation-based method for testing a large collection of hypotheses simultaneously. Our method provides lower bounds for the number of true discoveries in any selected subset of hypotheses. These bounds are simultaneously valid with high confidence. The methodology is particularly useful in functional Magnetic Resonance Imaging cluster analysis, where it provides a confidence statement on the percentage of truly activated voxels within clusters of voxels, avoiding the well-known spatial specificity paradox. We offer a user-friendly tool to estimate the percentage of true discoveries for each cluster while controlling the family-wise error rate for multiple testing and taking into account that the cluster was chosen in a data-driven way. The method adapts to the spatial correlation structure that characterizes functional Magnetic Resonance Imaging data, gaining power over parametric approaches.},
  archive      = {J_SIM},
  author       = {Angela Andreella and Jesse Hemerik and Livio Finos and Wouter Weeda and Jelle Goeman},
  doi          = {10.1002/sim.9725},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2311-2340},
  shortjournal = {Stat. Med.},
  title        = {Permutation-based true discovery proportions for functional magnetic resonance imaging cluster analysis},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semiparametric regression analysis of length-biased and
partly interval-censored data with application to an AIDS cohort study.
<em>SIM</em>, <em>42</em>(14), 2293–2310. (<a
href="https://doi.org/10.1002/sim.9724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Length-biased data occur often in many scientific fields, including clinical trials, epidemiology surveys and genome-wide association studies, and many methods have been proposed for their analysis under various situations. In this article, we consider the situation where one faces length-biased and partly interval-censored failure time data under the proportional hazards model, for which it does not seem to exist an established method. For the estimation, we propose an efficient nonparametric maximum likelihood method by incorporating the distribution information of the observed truncation times. For the implementation of the method, a flexible and stable EM algorithm via two-stage data augmentation is developed. By employing the empirical process theory, we establish the asymptotic properties of the resulting estimators. A simulation study conducted to assess the finite-sample performance of the proposed method suggests that it works well and is more efficient than the conditional likelihood approach. An application to an AIDS cohort study is also provided.},
  archive      = {J_SIM},
  author       = {Fan Feng and Shuwei Li and Peijie Wang and Jianguo Sun and Chaofu Ke},
  doi          = {10.1002/sim.9724},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2293-2310},
  shortjournal = {Stat. Med.},
  title        = {Semiparametric regression analysis of length-biased and partly interval-censored data with application to an AIDS cohort study},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiple imputation-based sensitivity analysis approach
for regression analysis with a missing not at random covariate.
<em>SIM</em>, <em>42</em>(14), 2275–2292. (<a
href="https://doi.org/10.1002/sim.9723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing covariate problems are common in biomedical and electrical medical record data studies while evaluating the relationship between a biomarker and certain clinical outcome, when biomarker data are not collected for all subjects. However, missingness mechanism is unverifiable based on observed data. If there is a suspicion of missing not at random (MNAR), researchers often perform sensitivity analysis to evaluate the impact of various missingness mechanisms. Under the selection modeling framework, we propose a sensitivity analysis approach with a standardized sensitivity parameter using a nonparametric multiple imputation strategy. The proposed approach requires fitting two working models to derive two predictive scores: one for predicting missing covariate values and the other for predicting missingness probabilities. For each missing covariate observation, the two predictive scores along with the pre-specified sensitivity parameter are used to define an imputing set. The proposed approach is expected to be robust against mis-specifications of the selection model and the sensitivity parameter since the selection model and the sensitivity parameter are not directly used to impute missing covariate values. A simulation study is conducted to study the performance of the proposed approach when MNAR is induced by Heckman&#39;s selection model. Simulation results show the proposed approach can produce plausible regression coefficient estimates. The proposed sensitivity analysis approach is also applied to evaluate the impact of MNAR on the relationship between post-operative outcomes and incomplete pre-operative Hemoglobin A1c level for patients who underwent carotid intervetion for advanced atherosclerotic disease.},
  archive      = {J_SIM},
  author       = {Chiu-Hsieh Hsu and Yulei He and Chengcheng Hu and Wei Zhou},
  doi          = {10.1002/sim.9723},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {14},
  pages        = {2275-2292},
  shortjournal = {Stat. Med.},
  title        = {A multiple imputation-based sensitivity analysis approach for regression analysis with a missing not at random covariate},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Early detection of ovarian cancer by wavelet analysis of
protein mass spectra. <em>SIM</em>, <em>42</em>(13), 2257–2273. (<a
href="https://doi.org/10.1002/sim.9722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient detection of ovarian cancer at early stages is critical to ensure proper treatments for patients. Among the first-line modalities investigated in studies of early diagnosis are features distilled from protein mass spectra. This method, however, considers only a specific subset of spectral responses and ignores the interplay among protein expression levels, which can also contain diagnostic information. We propose a new modality that automatically searches protein mass spectra for discriminatory features by considering the self-similar nature of the spectra. Self-similarity is assessed by taking a wavelet decomposition of protein mass spectra and estimating the rate of level-wise decay in the energies of the resulting wavelet coefficients. Level-wise energies are estimated in a robust manner using distance variance, and rates are estimated locally via a rolling window approach. This results in a collection of rates that can be used to characterize the interplay among proteins, which can be indicative of cancer presence. Discriminatory descriptors are then selected from these evolutionary rates and used as classifying features. The proposed wavelet-based features are used in conjunction with features proposed in the existing literature for early stage diagnosis of ovarian cancer using two datasets published by the American National Cancer Institute. Including the wavelet-based features from the new modality results in improvements in diagnostic performance for early-stage ovarian cancer detection. This demonstrates the ability of the proposed modality to characterize new ovarian cancer diagnostic information.},
  archive      = {J_SIM},
  author       = {Dixon Vimalajeewa and Scott Alan Bruce and Brani Vidakovic},
  doi          = {10.1002/sim.9722},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2257-2273},
  shortjournal = {Stat. Med.},
  title        = {Early detection of ovarian cancer by wavelet analysis of protein mass spectra},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian approach for two-stage multivariate mendelian
randomization with mixed outcomes. <em>SIM</em>, <em>42</em>(13),
2241–2256. (<a href="https://doi.org/10.1002/sim.9721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many research studies have investigated the relationship between baseline factors or exposures, such as patient demographic and disease characteristics, and study outcomes such as toxicities or quality of life, but results from most of these studies may be problematic because of potential confounding effects (eg, the imbalance in baseline factors or exposures). It is important to study whether the baseline factors or exposures have causal effects on the clinical outcomes, so that clinicians can have better understanding of the diseases and develop personalized medicine. Mendelian randomization (MR) provides an efficient way to estimate the causal effects using genetic instrumental variables to handle confounders, but most of the existing studies focus on a single outcome at a time and ignores the correlation structure of multiple outcomes. Given that clinical outcomes like toxicities and quality of life are usually a mixture of different types of variables, and multiple datasets may be available for such outcomes, it may be much more beneficial to analyze them jointly instead of separately. Some well-established methods are available for building multivariate models on mixed outcomes, but they do not incorporate MR mechanism to deal with the confounders. To overcome these challenges, we propose a Bayesian-based two-stage multivariate MR method for mixed outcomes on multiple datasets, called BMRMO. Using simulation studies and clinical applications on the CO.17 and CO.20 studies, we demonstrate better performance of our approach compared to the commonly used univariate two-stage method.},
  archive      = {J_SIM},
  author       = {Yangqing Deng and Dongsheng Tu and Chris J O&#39;Callaghan and Derek J Jonker and Christos S Karapetis and Jeremy Shapiro and Geoffrey Liu and Wei Xu},
  doi          = {10.1002/sim.9721},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2241-2256},
  shortjournal = {Stat. Med.},
  title        = {A bayesian approach for two-stage multivariate mendelian randomization with mixed outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Studentized permutation method for comparing two restricted
mean survival times with small sample from randomized trials.
<em>SIM</em>, <em>42</em>(13), 2226–2240. (<a
href="https://doi.org/10.1002/sim.9720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent observations, especially in cancer immunotherapy clinical trials with time-to-event outcomes, show that the commonly used proportional hazard assumption is often not justifiable, hampering an appropriate analysis of the data by hazard ratios. An attractive alternative advocated is given by the restricted mean survival time (RMST), which does not rely on any model assumption and can always be interpreted intuitively. Since methods for the RMST based on asymptotic theory suffer from inflated type-I error under small sample sizes, a permutation test was proposed recently leading to more convincing results in simulations. However, classical permutation strategies require an exchangeable data setup between comparison groups which may be limiting in practice. Besides, it is not possible to invert related testing procedures to obtain valid confidence intervals, which can provide more in-depth information. In this paper, we address these limitations by proposing a studentized permutation test as well as respective permutation-based confidence intervals. In an extensive simulation study, we demonstrate the advantage of our new method, especially in situations with relatively small sample sizes and unbalanced groups. Finally, we illustrate the application of the proposed method by re-analyzing data from a recent lung cancer clinical trial.},
  archive      = {J_SIM},
  author       = {Marc Ditzhaus and Menggang Yu and Jin Xu},
  doi          = {10.1002/sim.9720},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2226-2240},
  shortjournal = {Stat. Med.},
  title        = {Studentized permutation method for comparing two restricted mean survival times with small sample from randomized trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference in survival analysis using longitudinal
observational data: Sequential trials and marginal structural models.
<em>SIM</em>, <em>42</em>(13), 2191–2225. (<a
href="https://doi.org/10.1002/sim.9718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal observational data on patients can be used to investigate causal effects of time-varying treatments on time-to-event outcomes. Several methods have been developed for estimating such effects by controlling for the time-dependent confounding that typically occurs. The most commonly used is marginal structural models (MSM) estimated using inverse probability of treatment weights (IPTW) (MSM-IPTW). An alternative, the sequential trials approach, is increasingly popular, and involves creating a sequence of “trials” from new time origins and comparing treatment initiators and non-initiators. Individuals are censored when they deviate from their treatment assignment at the start of each “trial” (initiator or noninitiator), which is accounted for using inverse probability of censoring weights. The analysis uses data combined across trials. We show that the sequential trials approach can estimate the parameters of a particular MSM. The causal estimand that we focus on is the marginal risk difference between the sustained treatment strategies of “always treat” vs “never treat.” We compare how the sequential trials approach and MSM-IPTW estimate this estimand, and discuss their assumptions and how data are used differently. The performance of the two approaches is compared in a simulation study. The sequential trials approach, which tends to involve less extreme weights than MSM-IPTW, results in greater efficiency for estimating the marginal risk difference at most follow-up times, but this can, in certain scenarios, be reversed at later time points and relies on modelling assumptions. We apply the methods to longitudinal observational data from the UK Cystic Fibrosis Registry to estimate the effect of dornase alfa on survival.},
  archive      = {J_SIM},
  author       = {Ruth H. Keogh and Jon Michael Gran and Shaun R. Seaman and Gwyneth Davies and Stijn Vansteelandt},
  doi          = {10.1002/sim.9718},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2191-2225},
  shortjournal = {Stat. Med.},
  title        = {Causal inference in survival analysis using longitudinal observational data: Sequential trials and marginal structural models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pitfalls of the concordance index for survival outcomes.
<em>SIM</em>, <em>42</em>(13), 2179–2190. (<a
href="https://doi.org/10.1002/sim.9717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prognostic models are useful tools for assessing a patient&#39;s risk of experiencing adverse health events. In practice, these models must be validated before implementation to ensure that they are clinically useful. The concordance index (C-Index) is a popular statistic that is used for model validation, and it is often applied to models with binary or survival outcome variables. In this paper, we summarize existing criticism of the C-Index and show that many limitations are accentuated when applied to survival outcomes, and to continuous outcomes more generally. We present several examples that show the challenges in achieving high concordance with survival outcomes, and we argue that the C-Index is often not clinically meaningful in this setting. We derive a relationship between the concordance probability and the coefficient of determination under an ordinary least squares model with normally distributed predictors, which highlights the limitations of the C-Index for continuous outcomes. Finally, we recommend existing alternatives that more closely align with common uses of survival models.},
  archive      = {J_SIM},
  author       = {Nicholas Hartman and Sehee Kim and Kevin He and John D. Kalbfleisch},
  doi          = {10.1002/sim.9717},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2179-2190},
  shortjournal = {Stat. Med.},
  title        = {Pitfalls of the concordance index for survival outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjusting for informative cluster size in pseudo-value-based
regression approaches with clustered time to event data. <em>SIM</em>,
<em>42</em>(13), 2162–2178. (<a
href="https://doi.org/10.1002/sim.9716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Informative cluster size (ICS) arises in situations with clustered data where a latent relationship exists between the number of participants in a cluster and the outcome measures. Although this phenomenon has been sporadically reported in the statistical literature for nearly two decades now, further exploration is needed in certain statistical methodologies to avoid potentially misleading inferences. For inference about population quantities without covariates, inverse cluster size reweightings are often employed to adjust for ICS. Further, to study the effect of covariates on disease progression described by a multistate model, the pseudo-value regression technique has gained popularity in time-to-event data analysis. We seek to answer the question: “How to apply pseudo-value regression to clustered time-to-event data when cluster size is informative?” ICS adjustment by the reweighting method can be performed in two steps; estimation of marginal functions of the multistate model and fitting the estimating equations based on pseudo-value responses, leading to four possible strategies. We present theoretical arguments and thorough simulation experiments to ascertain the correct strategy for adjusting for ICS. A further extension of our methodology is implemented to include informativeness induced by the intracluster group size. We demonstrate the methods in two real-world applications: (i) to determine predictors of tooth survival in a periodontal study and (ii) to identify indicators of ambulatory recovery in spinal cord injury patients who participated in locomotor-training rehabilitation.},
  archive      = {J_SIM},
  author       = {Samuel Anyaso-Samuel and Somnath Datta},
  doi          = {10.1002/sim.9716},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2162-2178},
  shortjournal = {Stat. Med.},
  title        = {Adjusting for informative cluster size in pseudo-value-based regression approaches with clustered time to event data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalized maximum likelihood inference under the mixture
cure model in sparse data. <em>SIM</em>, <em>42</em>(13), 2134–2161. (<a
href="https://doi.org/10.1002/sim.9715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Changchang Xu and Shelley B. Bull},
  doi          = {10.1002/sim.9715},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2134-2161},
  shortjournal = {Stat. Med.},
  title        = {Penalized maximum likelihood inference under the mixture cure model in sparse data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SpiderLearner: An ensemble approach to gaussian graphical
model estimation. <em>SIM</em>, <em>42</em>(13), 2116–2133. (<a
href="https://doi.org/10.1002/sim.9714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian graphical models (GGMs) are a popular form of network model in which nodes represent features in multivariate normal data and edges reflect conditional dependencies between these features. GGM estimation is an active area of research. Currently available tools for GGM estimation require investigators to make several choices regarding algorithms, scoring criteria, and tuning parameters. An estimated GGM may be highly sensitive to these choices, and the accuracy of each method can vary based on structural characteristics of the network such as topology, degree distribution, and density. Because these characteristics are a priori unknown, it is not straightforward to establish universal guidelines for choosing a GGM estimation method. We address this problem by introducing SpiderLearner, an ensemble method that constructs a consensus network from multiple estimated GGMs. Given a set of candidate methods, SpiderLearner estimates the optimal convex combination of results from each method using a likelihood-based loss function. -fold cross-validation is applied in this process, reducing the risk of overfitting. In simulations, SpiderLearner performs better than or comparably to the best candidate methods according to a variety of metrics, including relative Frobenius norm and out-of-sample likelihood. We apply SpiderLearner to publicly available ovarian cancer gene expression data including 2013 participants from 13 diverse studies, demonstrating our tool&#39;s potential to identify biomarkers of complex disease. SpiderLearner is implemented as flexible, extensible, open-source code in the R package ensembleGGM at https://github.com/katehoffshutta/ensembleGGM .},
  archive      = {J_SIM},
  author       = {Katherine H. Shutta and Laura B. Balzer and Denise M. Scholtens and Raji Balasubramanian},
  doi          = {10.1002/sim.9714},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2116-2133},
  shortjournal = {Stat. Med.},
  title        = {SpiderLearner: An ensemble approach to gaussian graphical model estimation},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of two approaches to dynamic prediction: Joint
modeling and landmark modeling. <em>SIM</em>, <em>42</em>(13),
2101–2115. (<a href="https://doi.org/10.1002/sim.9713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint modeling and landmark modeling are two mainstream approaches to dynamic prediction in longitudinal studies, that is, the prediction of a clinical event using longitudinally measured predictor variables available up to the time of prediction. It is an important research question to the methodological research field and also to practical users to understand which approach can produce more accurate prediction. There were few previous studies on this topic, and the majority of results seemed to favor joint modeling. However, these studies were conducted in scenarios where the data were simulated from the joint models, partly due to the widely recognized methodological difficulty on whether there exists a general joint distribution of longitudinal and survival data so that the landmark models, which consists of infinitely many working regression models for survival, hold simultaneously. As a result, the landmark models always worked under misspecification, which caused difficulty in interpreting the comparison. In this paper, we solve this problem by using a novel algorithm to generate longitudinal and survival data that satisfies the working assumptions of the landmark models. This innovation makes it possible for a “fair” comparison of joint modeling and landmark modeling in terms of model specification. Our simulation results demonstrate that the relative performance of these two modeling approaches depends on the data settings and one does not always dominate the other in terms of prediction accuracy. These findings stress the importance of methodological development for both approaches. The related methodology is illustrated with a kidney transplantation dataset.},
  archive      = {J_SIM},
  author       = {Wenhao Li and Liang Li and Brad C. Astor},
  doi          = {10.1002/sim.9713},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2101-2115},
  shortjournal = {Stat. Med.},
  title        = {A comparison of two approaches to dynamic prediction: Joint modeling and landmark modeling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disease progression based feature screening for
ultrahigh-dimensional survival-associated biomarkers. <em>SIM</em>,
<em>42</em>(13), 2082–2100. (<a
href="https://doi.org/10.1002/sim.9712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increased availability of ultrahigh-dimensional biomarker data and the high demand of identifying biomarkers importantly related to survival outcomes made feature screening methods commonplace in the analysis of cancer genome data. When survival outcomes include endpoints of overall survival (OS) and time-to-progression (TTP), a high concordance is typically found in both endpoints in cancer studies, namely, patients&#39; OS would most likely be extended when tumour progression is delayed. Existing screening procedures are often performed on a single survival endpoint only and may result in biased selection of features for OS in ignorance of disease progression. We propose a novel feature screening method by incorporating information of TTP into the selection of important biomarker predictors for more accurate inference of OS subsequent to disease progression. The proposal is based on the rank of correlation between individual features and the conditional distribution of OS given observations of TTP. It is advantageous for its flexible model nature, which requires no marginal model assumption for each endpoint, and its minimal computational cost for implementation. Theoretical results show its ranking consistency, sure screening and false rate control properties. Simulation results demonstrate that the proposed screener leads to more accurate feature selection than the method without considering the prior observations of disease progression. An application to breast cancer genome data illustrates its practical utility and facilitates disease classification using selected biomarker predictors.},
  archive      = {J_SIM},
  author       = {Mengjiao Peng and Liming Xiang},
  doi          = {10.1002/sim.9712},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2082-2100},
  shortjournal = {Stat. Med.},
  title        = {Disease progression based feature screening for ultrahigh-dimensional survival-associated biomarkers},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel causal mediation analysis approach for zero-inflated
mediators. <em>SIM</em>, <em>42</em>(13), 2061–2081. (<a
href="https://doi.org/10.1002/sim.9689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analyses play important roles in making causal inference in biomedical research to examine causal pathways that may be mediated by one or more intermediate variables (ie, mediators). Although mediation frameworks have been well established such as counterfactual-outcomes (ie, potential-outcomes) models and traditional linear mediation models, little effort has been devoted to dealing with mediators with zero-inflated structures due to challenges associated with excessive zeros. We develop a novel mediation modeling approach to address zero-inflated mediators containing true zeros and false zeros. The new approach can decompose the total mediation effect into two components induced by zero-inflated structures: the first component is attributable to the change in the mediator on its numerical scale which is a sum of two causal pathways and the second component is attributable only to its binary change from zero to a non-zero status. An extensive simulation study is conducted to assess the performance and it shows that the proposed approach outperforms existing standard causal mediation analysis approaches. We also showcase the application of the proposed approach to a real study in comparison with a standard causal mediation analysis approach.},
  archive      = {J_SIM},
  author       = {Meilin Jiang and Seonjoo Lee and A. James O&#39;Malley and Yaakov Stern and Zhigang Li},
  doi          = {10.1002/sim.9689},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2061-2081},
  shortjournal = {Stat. Med.},
  title        = {A novel causal mediation analysis approach for zero-inflated mediators},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of n-of-1 trials using bayesian distributed lag
model with autocorrelated errors. <em>SIM</em>, <em>42</em>(13),
2044–2060. (<a href="https://doi.org/10.1002/sim.9676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An N-of-1 trial is a multi-period crossover trial performed in a single individual, with a primary goal to estimate treatment effect on the individual instead of population-level mean responses. As in a conventional crossover trial, it is critical to understand carryover effects of the treatment in an N-of-1 trial, especially when no washout periods between treatment periods are instituted to reduce trial duration. To deal with this issue in situations where a high volume of measurements are made during the study, we introduce a novel Bayesian distributed lag model that facilitates the estimation of carryover effects, while accounting for temporal correlations using an autoregressive model. Specifically, we propose a prior variance-covariance structure on the lag coefficients to address collinearity caused by the fact that treatment exposures are typically identical on successive days. A connection between the proposed Bayesian model and penalized regression is noted. Simulation results demonstrate that the proposed model substantially reduces the root mean squared error in the estimation of carryover effects and immediate effects when compared to other existing methods, while being comparable in the estimation of the total effects. We also apply the proposed method to assess the extent of carryover effects of light therapies in relieving depressive symptoms in cancer survivors.},
  archive      = {J_SIM},
  author       = {Ziwei Liao and Min Qian and Ian M. Kronish and Ying Kuen Cheung},
  doi          = {10.1002/sim.9676},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2044-2060},
  shortjournal = {Stat. Med.},
  title        = {Analysis of N-of-1 trials using bayesian distributed lag model with autocorrelated errors},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensitivity analysis using bias functions for studies
extending inferences from a randomized trial to a target population.
<em>SIM</em>, <em>42</em>(13), 2029–2043. (<a
href="https://doi.org/10.1002/sim.9550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extending (i.e., generalizing or transporting) causal inferences from a randomized trial to a target population requires assumptions that randomized and nonrandomized individuals are exchangeable conditional on baseline covariates. These assumptions are made on the basis of background knowledge, which is often uncertain or controversial, and need to be subjected to sensitivity analysis. We present simple methods for sensitivity analyses that directly parameterize violations of the assumptions using bias functions and do not require detailed background knowledge about specific unknown or unmeasured determinants of the outcome or modifiers of the treatment effect. We show how the methods can be applied to non-nested trial designs, where the trial data are combined with a separately obtained sample of nonrandomized individuals, as well as to nested trial designs, where the trial is embedded within a cohort sampled from the target population.},
  archive      = {J_SIM},
  author       = {Issa J. Dahabreh and James M. Robins and Sebastien J.-P. A. Haneuse and Iman Saeed and Sarah E. Robertson and Elizabeth A. Stuart and Miguel A. Hernán},
  doi          = {10.1002/sim.9550},
  journal      = {Statistics in Medicine},
  month        = {6},
  number       = {13},
  pages        = {2029-2043},
  shortjournal = {Stat. Med.},
  title        = {Sensitivity analysis using bias functions for studies extending inferences from a randomized trial to a target population},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correction to “controlled pattern imputation for sensitivity
analysis of longitudinal binary and ordinal outcomes with nonignorable
dropout.” <em>SIM</em>, <em>42</em>(12), 2027–2028. (<a
href="https://doi.org/10.1002/sim.9708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  doi          = {10.1002/sim.9708},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2027-2028},
  shortjournal = {Stat. Med.},
  title        = {Correction to “Controlled pattern imputation for sensitivity analysis of longitudinal binary and ordinal outcomes with nonignorable dropout”},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian generalized linear low rank regression models for
the detection of vaccine-adverse event associations. <em>SIM</em>,
<em>42</em>(12), 2009–2026. (<a
href="https://doi.org/10.1002/sim.9711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generalized linear low-rank mixed model (GLLRM) for the analysis of both high-dimensional and sparse responses and covariates where the responses may be binary, counts, or continuous. This development is motivated by the problem of identifying vaccine-adverse event associations in post-market drug safety databases, where an adverse event is any untoward medical occurrence or health problem that occurs during or following vaccination. The GLLRM is a generalization of a generalized linear mixed model in that it integrates a factor analysis model to describe the dependence among responses and a low-rank matrix to approximate the high-dimensional regression coefficient matrix. A sampling procedure combining the Gibbs sampler and Metropolis and Gamerman algorithms is employed to obtain posterior estimates of the regression coefficients and other model parameters. Testing of response-covariate pair associations is based on the posterior distribution of the corresponding regression coefficients. Monte Carlo simulation studies are conducted to examine the finite-sample performance of the proposed procedures on binary and count outcomes. We further illustrate the GLLRM via a real data example based on the Vaccine Adverse Event Reporting System.},
  archive      = {J_SIM},
  author       = {Paloma Hauser and Xianming Tan and Fang Chen and Joseph G. Ibrahim},
  doi          = {10.1002/sim.9711},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {2009-2026},
  shortjournal = {Stat. Med.},
  title        = {Bayesian generalized linear low rank regression models for the detection of vaccine-adverse event associations},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonparametric estimation of marked survival data in the
presence of dependent censoring. <em>SIM</em>, <em>42</em>(12),
1995–2008. (<a href="https://doi.org/10.1002/sim.9710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider nonparametrically estimating the joint distribution of a survival time and mark variable, where the survival time is subject to right censoring and the mark variable is only observed when the survival time is not censored. The possibility of dependent censoring is allowed for using inverse probability of censoring weights. The proposed estimator is shown to be consistent and asymptotically normal. Finite sample behavior of the proposed methods are investigated via simulation study. Finally, we illustrate the nonparametric estimator from a recent HIV vaccine efficacy trial.},
  archive      = {J_SIM},
  author       = {Busola Sanusi and Jianwen Cai and Michael G. Hudgens},
  doi          = {10.1002/sim.9710},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1995-2008},
  shortjournal = {Stat. Med.},
  title        = {Nonparametric estimation of marked survival data in the presence of dependent censoring},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New late-emphasis and combination tests based on infimum and
supremum logrank statistics with application in oncology trials.
<em>SIM</em>, <em>42</em>(12), 1981–1994. (<a
href="https://doi.org/10.1002/sim.9709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunotherapy cancer clinical trials routinely feature an initial period during which the treatment is given without evident therapeutic benefit, which may be followed by a period during which an effective therapy reduces the hazard for event occurrence. The nature of this treatment effect is incompatible with the proportional hazards assumption, which has prompted much work on the development of alternative effect measures of frameworks for testing. We consider tests based on individual and combination of early- and late-emphasis infimum and supremum logrank statistics, describe how they can be implemented, and evaluate their performance in simulation studies. Through this work and illustrative applications we conclude that this class of test statistics offers a new and powerful framework for assessing treatment effects in cancer clinical trials involving immunotherapies.},
  archive      = {J_SIM},
  author       = {Jean Marie Boher and Thomas Filleron and Pierre Bunouf and Richard J. Cook},
  doi          = {10.1002/sim.9709},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1981-1994},
  shortjournal = {Stat. Med.},
  title        = {New late-emphasis and combination tests based on infimum and supremum logrank statistics with application in oncology trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-homogeneous continuous-time markov chain with
covariates: Applications to ambulatory hypertension monitoring.
<em>SIM</em>, <em>42</em>(12), 1965–1980. (<a
href="https://doi.org/10.1002/sim.9707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypertension significantly increases the risk for many health conditions including heart disease and stroke. Hypertensive patients often have continuous measurements of their blood pressure to better understand how it fluctuates over the day. The continuous-time Markov chain (CTMC) is commonly used to study repeated measurements with categorical outcomes. However, the standard CTMC may be restrictive, because the rates of transitions between states are assumed to be constant through time, while the transition rates for describing the dynamics of hypertension are likely to be changing over time. In addition, the applications of CTMC rarely account for the effects of other covariates on state transitions. In this article, we considered a non-homogeneous continuous-time Markov chain with two states to analyze changes in hypertension while accounting for multiple covariates. The explicit formulas for the transition probability matrix as well as the corresponding likelihood function were derived. In addition, we proposed a maximum likelihood estimation algorithm for estimating the parameters in the time-dependent rate function. Lastly, the model performance was demonstrated through both a simulation study and application to ambulatory blood pressure data.},
  archive      = {J_SIM},
  author       = {Joonha Chang and Hei Kit Chan and Jeffrey Lin and Wenyaw Chan},
  doi          = {10.1002/sim.9707},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1965-1980},
  shortjournal = {Stat. Med.},
  title        = {Non-homogeneous continuous-time markov chain with covariates: Applications to ambulatory hypertension monitoring},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference in survival analysis under deterministic
missingness of confounders in register data. <em>SIM</em>,
<em>42</em>(12), 1946–1964. (<a
href="https://doi.org/10.1002/sim.9706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term register data offer unique opportunities to explore causal effects of treatments on time-to-event outcomes, in well-characterized populations with minimum loss of follow-up. However, the structure of the data may pose methodological challenges. Motivated by the Swedish Renal Registry and estimation of survival differences for renal replacement therapies, we focus on the particular case when an important confounder is not recorded in the early period of the register, so that the entry date to the register deterministically predicts confounder missingness. In addition, an evolving composition of the treatment arms populations, and suspected improved survival outcomes in later periods lead to informative administrative censoring, unless the entry date is appropriately accounted for. We investigate different consequences of these issues on causal effect estimation following multiple imputation of the missing covariate data. We analyse the performance of different combinations of imputation models and estimation methods for the population average survival. We further evaluate the sensitivity of our results to the nature of censoring and misspecification of fitted models. We find that an imputation model including the cumulative baseline hazard, event indicator, covariates and interactions between the cumulative baseline hazard and covariates, followed by regression standardization, leads to the best estimation results overall, in simulations. Standardization has two advantages over inverse probability of treatment weighting here: it can directly account for the informative censoring by including the entry date as a covariate in the outcome model, and allows for straightforward variance computation using readily available software.},
  archive      = {J_SIM},
  author       = {Iuliana Ciocănea-Teodorescu and Els Goetghebeur and Ingeborg Waernbaum and Staffan Schön and Erin E. Gabriel},
  doi          = {10.1002/sim.9706},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1946-1964},
  shortjournal = {Stat. Med.},
  title        = {Causal inference in survival analysis under deterministic missingness of confounders in register data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple hypothesis screening using mixtures of non-local
distributions with applications to genomic studies. <em>SIM</em>,
<em>42</em>(12), 1931–1945. (<a
href="https://doi.org/10.1002/sim.9705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of large-scale datasets, especially in biomedical contexts, frequently involves a principled screening of multiple hypotheses. The celebrated two-group model jointly models the distribution of the test statistics with mixtures of two competing densities, the null and the alternative distributions. We investigate the use of weighted densities and, in particular, non-local densities as working alternative distributions, to enforce separation from the null and thus refine the screening procedure. We show how these weighted alternatives improve various operating characteristics, such as the Bayesian false discovery rate, of the resulting tests for a fixed mixture proportion with respect to a local, unweighted likelihood approach. Parametric and nonparametric model specifications are proposed, along with efficient samplers for posterior inference. By means of a simulation study, we exhibit how our model compares with both well-established and state-of-the-art alternatives in terms of various operating characteristics. Finally, to illustrate the versatility of our method, we conduct three differential expression analyses with publicly-available datasets from genomic studies of heterogeneous nature.},
  archive      = {J_SIM},
  author       = {Francesco Denti and Stefano Peluso and Michele Guindani and Antonietta Mira},
  doi          = {10.1002/sim.9705},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1931-1945},
  shortjournal = {Stat. Med.},
  title        = {Multiple hypothesis screening using mixtures of non-local distributions with applications to genomic studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-level copula joint model for joint analysis of
longitudinal and competing risks data. <em>SIM</em>, <em>42</em>(12),
1909–1930. (<a href="https://doi.org/10.1002/sim.9704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a two-level copula joint model to analyze clinical data with multiple disparate continuous longitudinal outcomes and multiple event-times in the presence of competing risks. At the first level, we use a copula to model the dependence between competing latent event-times, in the process constructing the submodel for the observed event-time, and employ the Gaussian copula to construct the submodel for the longitudinal outcomes that accounts for their conditional dependence; these submodels are glued together at the second level via the Gaussian copula to construct a joint model that incorporates conditional dependence between the observed event-time and the longitudinal outcomes. To have the flexibility to accommodate skewed data and examine possibly different covariate effects on quantiles of a non-Gaussian outcome, we propose linear quantile mixed models for the continuous longitudinal data. We adopt a Bayesian framework for model estimation and inference via Markov Chain Monte Carlo sampling. We examine the performance of the copula joint model through a simulation study and show that our proposed method outperforms the conventional approach assuming conditional independence with smaller biases and better coverage probabilities of the Bayesian credible intervals. Finally, we carry out an analysis of clinical data on renal transplantation for illustration.},
  archive      = {J_SIM},
  author       = {Xiaoming Lu and Thierry Chekouo and Hua Shen and Alexander R. de Leon},
  doi          = {10.1002/sim.9704},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1909-1930},
  shortjournal = {Stat. Med.},
  title        = {A two-level copula joint model for joint analysis of longitudinal and competing risks data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalized smoothing splines resolve the curvature
identifiability problem in age-period-cohort models with unequal
intervals. <em>SIM</em>, <em>42</em>(12), 1888–1908. (<a
href="https://doi.org/10.1002/sim.9703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age-period-cohort (APC) models are frequently used in a variety of health and demographic-related outcomes. Fitting and interpreting APC models to data in equal intervals (equal age and period widths) is nontrivial due to the structural link between the three temporal effects (given two, the third can always be found) causing the well-known identification problem. The usual method for resolving the structural link identification problem is to base a model on identifiable quantities. It is common to find health and demographic data in unequal intervals, this creates further identification problems on top of the structural link. We highlight the new issues by showing that curvatures which were identifiable for equal intervals are no longer identifiable for unequal data. Furthermore, through extensive simulation studies, we show how previous methods for unequal APC models are not always appropriate due to their sensitivity to the choice of functions used to approximate the true temporal functions. We propose a new method for modeling unequal APC data using penalized smoothing splines. Our proposal effectively resolves the curvature identification issue that arises and is robust to the choice of the approximating function. To demonstrate the effectiveness of our proposal, we conclude with an application to UK all-cause mortality data from the Human mortality database.},
  archive      = {J_SIM},
  author       = {Connor Gascoigne and Theresa Smith},
  doi          = {10.1002/sim.9703},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1888-1908},
  shortjournal = {Stat. Med.},
  title        = {Penalized smoothing splines resolve the curvature identifiability problem in age-period-cohort models with unequal intervals},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Defining estimands in clinical trials: A unified procedure.
<em>SIM</em>, <em>42</em>(12), 1869–1887. (<a
href="https://doi.org/10.1002/sim.9702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ICH E9 (R1) addendum proposes five strategies to define estimands by addressing intercurrent events. However, mathematical forms of these targeted quantities are lacking, which might lead to discordance between statisticians who estimate these quantities and clinicians, drug sponsors, and regulators who interpret them. To improve the concordance, we provide a unified four-step procedure for constructing the mathematical estimands. We apply the procedure for each strategy to derive the mathematical estimands and compare the five strategies in practical interpretations, data collection, and analytical methods. Finally, we show that the procedure can help ease tasks of defining estimands in settings with multiple types of intercurrent events using two real clinical trials.},
  archive      = {J_SIM},
  author       = {Shasha Han and Xiao-Hua Zhou},
  doi          = {10.1002/sim.9702},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {12},
  pages        = {1869-1887},
  shortjournal = {Stat. Med.},
  title        = {Defining estimands in clinical trials: A unified procedure},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confidence intervals for prevalence estimates from complex
surveys with imperfect assays. <em>SIM</em>, <em>42</em>(11), 1822–1867.
(<a href="https://doi.org/10.1002/sim.9701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are established methods for estimating disease prevalence with associated confidence intervals for complex surveys with perfect assays, or simple random sample surveys with imperfect assays. We develop and study methods for the complicated case of complex surveys with imperfect assays. The new methods use the melding method to combine gamma intervals for directly standardized rates and established adjustments for imperfect assays by estimating sensitivity and specificity. One of the new methods appears to have at least nominal coverage in all simulated scenarios. We compare our new methods to established methods in special cases (complex surveys with perfect assays or simple surveys with imperfect assays). In some simulations, our methods appear to guarantee coverage, while competing methods have much lower than nominal coverage, especially when overall prevalence is very low. In other settings, our methods are shown to have higher than nominal coverage. We apply our method to a seroprevalence survey of SARS-CoV-2 in undiagnosed adults in the United States between May and July 2020.},
  archive      = {J_SIM},
  author       = {Damon M. Bayer and Michael P. Fay and Barry I. Graubard},
  doi          = {10.1002/sim.9701},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1822-1867},
  shortjournal = {Stat. Med.},
  title        = {Confidence intervals for prevalence estimates from complex surveys with imperfect assays},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size calculation for randomized trials via inverse
probability of response weighting when outcome data are missing at
random. <em>SIM</em>, <em>42</em>(11), 1802–1821. (<a
href="https://doi.org/10.1002/sim.9700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized trials are an established method to evaluate the causal effects of interventions. Despite concerted efforts to retain all trial participants, some missing outcome data are often inevitable. It is unclear how best to account for missing outcome data in sample size calculations. A standard approach is to inflate the sample size by the inverse of one minus the anticipated dropout probability. However, the performance of this approach in the presence of informative outcome missingness has not been well-studied. We investigate sample size calculation when outcome data are missing at random given the randomized intervention group and fully observed baseline covariates under an inverse probability of response weighted (IPRW) estimating equations approach. Using M-estimation theory, we derive sample size formulas for both individually randomized and cluster randomized trials (CRTs). We illustrate the proposed method by calculating a sample size for a CRT designed to detect a difference in HIV testing strategies under an IPRW approach. We additionally develop an R shiny app to facilitate implementation of the sample size formulas.},
  archive      = {J_SIM},
  author       = {Linda J. Harrison and Rui Wang},
  doi          = {10.1002/sim.9700},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1802-1821},
  shortjournal = {Stat. Med.},
  title        = {Sample size calculation for randomized trials via inverse probability of response weighting when outcome data are missing at random},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting multivariate structured additive distributional
regression models. <em>SIM</em>, <em>42</em>(11), 1779–1801. (<a
href="https://doi.org/10.1002/sim.9699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a model-based boosting approach for multivariate distributional regression within the framework of generalized additive models for location, scale, and shape. Our approach enables the simultaneous modeling of all distribution parameters of an arbitrary parametric distribution of a multivariate response conditional on explanatory variables, while being applicable to potentially high-dimensional data. Moreover, the boosting algorithm incorporates data-driven variable selection, taking various different types of effects into account. As a special merit of our approach, it allows for modeling the association between multiple continuous or discrete outcomes through the relevant covariates. After a detailed simulation study investigating estimation and prediction performance, we demonstrate the full flexibility of our approach in three diverse biomedical applications. The first is based on high-dimensional genomic cohort data from the UK Biobank, considering a bivariate binary response (chronic ischemic heart disease and high cholesterol). Here, we are able to identify genetic variants that are informative for the association between cholesterol and heart disease. The second application considers the demand for health care in Australia with the number of consultations and the number of prescribed medications as a bivariate count response. The third application analyses two dimensions of childhood undernutrition in Nigeria as a bivariate response and we find that the correlation between the two undernutrition scores is considerably different depending on the child&#39;s age and the region the child lives in.},
  archive      = {J_SIM},
  author       = {Annika Strömer and Nadja Klein and Christian Staerk and Hannah Klinkhammer and Andreas Mayr},
  doi          = {10.1002/sim.9699},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1779-1801},
  shortjournal = {Stat. Med.},
  title        = {Boosting multivariate structured additive distributional regression models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible template matching for observational study design.
<em>SIM</em>, <em>42</em>(11), 1760–1778. (<a
href="https://doi.org/10.1002/sim.9698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching is a popular design for inferring causal effect with observational data. Unlike model-based approaches, it is a nonparametric method to group treated and control subjects with similar characteristics together, hence to re-create a randomization-like scenario. The application of matched design for real world data may be limited by: (1) the causal estimand of interest; (2) the sample size of different treatment arms. We propose a flexible design of matching, based on the idea of template matching, to overcome these challenges. It first identifies the template group which is representative of the target population, then match subjects from the original data to this template group and make inference. We provide theoretical justification on how it unbiasedly estimates the average treatment effect using matched pairs and the average treatment effect on the treated when the treatment group has a bigger sample size. We also propose using the triplet matching algorithm to improve matching quality and devise a practical strategy to select the template size. One major advantage of matched design is that it allows both randomization-based or model-based inference, with the former being more robust. For the commonly used binary outcome in medical research, we adopt a randomization inference framework of attributable effects in matched data, which allows heterogeneous effects and can incorporate sensitivity analysis for unmeasured confounding. We apply our design and analytical strategy to a trauma care evaluation study.},
  archive      = {J_SIM},
  author       = {Ruochen Zhao and Bo Lu},
  doi          = {10.1002/sim.9698},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1760-1778},
  shortjournal = {Stat. Med.},
  title        = {Flexible template matching for observational study design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian (meta-)regression model for treatment effects on
the risk difference scale. <em>SIM</em>, <em>42</em>(11), 1741–1759. (<a
href="https://doi.org/10.1002/sim.9697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical settings, the absolute risk reduction due to treatment that can be expected in a particular patient is of key interest. However, logistic regression, the default regression model for trials with a binary outcome, produces estimates of the effect of treatment measured as a difference in log odds. We explored options to estimate treatment effects directly as a difference in risk, specifically in the network meta-analysis setting. We propose a novel Bayesian (meta-)regression model for binary outcomes on the additive risk scale. The model allows treatment effects, covariate effects, interactions and variance parameters to be estimated directly on the linear scale of clinical interest. We compared effect estimates from this model to (1) a previously proposed additive risk model by Warn, Thompson and Spiegelhalter (“WTS model”) and (2) backtransforming the predictions from a logistic model to the natural scale after regression. The models were compared in a network meta-analysis of 20 hepatitis C trials, as well as in the analysis of simulated single trial settings. The resulting estimates diverged, in particular for small sample sizes or true risks close to 0% or 100%. Researchers should be aware that modelling untransformed risk can yield very different results from default logistic models. The treatment effect in participants with such extreme predicted risks weighed more heavily on the overall treatment effect estimate from our proposed model compared to the WTS model. In our network meta-analysis, this sensitivity of our proposed model was needed to detect all information in the data.},
  archive      = {J_SIM},
  author       = {Doranne Thomassen and Ewout Steyerberg and Saskia le Cessie},
  doi          = {10.1002/sim.9697},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1741-1759},
  shortjournal = {Stat. Med.},
  title        = {A bayesian (meta-)regression model for treatment effects on the risk difference scale},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian approach to study design and analysis with type i
error rate control for response variables of mixed types. <em>SIM</em>,
<em>42</em>(11), 1722–1740. (<a
href="https://doi.org/10.1002/sim.9696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been increased interest in the design and analysis of studies consisting of multiple response variables of mixed types. For example, in clinical trials, it is desirable to establish efficacy for a treatment effect in primary and secondary outcomes. In this article, we develop Bayesian approaches for hypothesis testing and study planning for data consisting of multiple response variables of mixed types with covariates. We assume that the responses are correlated via a Gaussian copula, and that the model for each response is, marginally, a generalized linear model (GLM). Taking a fully Bayesian approach, the proposed method enables inference based on the joint posterior distribution of the parameters. Under some mild conditions, we show that the joint distribution of the posterior probabilities under any Bayesian analysis converges to a Gaussian copula distribution as the sample size tends to infinity. Using this result, we develop an approach to control the type I error rate under multiple testing. Simulation results indicate that the method is more powerful than conducting marginal regression models and correcting for multiplicity using the Bonferroni-Holm Method. We also develop a Bayesian approach to sample size determination in the presence of response variables of mixed types, extending the concept of probability of success (POS) to multiple response variables of mixed types.},
  archive      = {J_SIM},
  author       = {Ethan M. Alt and Matthew A. Psioda and Joseph G. Ibrahim},
  doi          = {10.1002/sim.9696},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1722-1740},
  shortjournal = {Stat. Med.},
  title        = {A bayesian approach to study design and analysis with type i error rate control for response variables of mixed types},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian estimation and testing in random-effects
meta-analysis of rare binary events allowing for flexible group
variability. <em>SIM</em>, <em>42</em>(11), 1699–1721. (<a
href="https://doi.org/10.1002/sim.9695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rare binary events data arise frequently in medical research. Due to lack of statistical power in individual studies involving such data, meta-analysis has become an increasingly important tool for combining results from multiple independent studies. However, traditional meta-analysis methods often report severely biased estimates in such rare-event settings. Moreover, many rely on models assuming a pre-specified direction for variability between control and treatment groups for mathematical convenience, which may be violated in practice. Based on a flexible random-effects model that removes the assumption about the direction, we propose new Bayesian procedures for estimating and testing the overall treatment effect and inter-study heterogeneity. Our Markov chain Monte Carlo algorithm employs Pólya-Gamma augmentation so that all conditionals are known distributions, greatly facilitating computational efficiency. Our simulation shows that the proposed approach generally reports less biased and more stable estimates compared to existing methods. We further illustrate our approach using two real examples, one using rosiglitazone data from 56 studies and the other using stomach ulcers data from 41 studies.},
  archive      = {J_SIM},
  author       = {Ming Zhang and Jackson Barth and Johan Lim and Xinlei Wang},
  doi          = {10.1002/sim.9695},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1699-1721},
  shortjournal = {Stat. Med.},
  title        = {Bayesian estimation and testing in random-effects meta-analysis of rare binary events allowing for flexible group variability},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulating and estimating agreement in the presence of
multiple raters and covariates. <em>SIM</em>, <em>42</em>(11),
1687–1698. (<a href="https://doi.org/10.1002/sim.9694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohen&#39;s and Fleiss&#39;s kappa are popular estimators for assessing agreement among two and multiple raters, respectively, for a binary response. While additional methods have been developed to account for multiple raters and covariates, they are not always applicable, rarely used, and none simplify to Cohen&#39;s kappa. Furthermore, there are no methods to simulate Bernoulli observations under the kappa agreement structure such that the developed methods could be adequately assessed. This manuscript overcomes these shortfalls. First, we developed a model-based estimator for kappa that accommodates multiple raters and covariates through a generalized linear mixed model and encompasses Cohen&#39;s kappa as a special case. Second, we created a framework to simulate dependent Bernoulli observations that upholds all 2-tuple pair of rater&#39;s kappa agreement structure and includes covariates. We used this framework to assess our method when kappa was nonzero. Simulations showed that Cohen&#39;s and Fleiss&#39;s kappa estimates were inflated unlike our model-based kappa. We analyzed an Alzheimer&#39;s disease neuroimaging study and the classic cervical cancer pathology study. The proposed model-based kappa and advancement in simulation methodology demonstrates that the popular approaches of Cohen&#39;s and Fleiss&#39;s kappa are poised to yield invalid conclusions while our work overcomes shortfalls, leading to improved inferences.},
  archive      = {J_SIM},
  author       = {Katelyn A. McKenzie and Jonathan D. Mahnken},
  doi          = {10.1002/sim.9694},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1687-1698},
  shortjournal = {Stat. Med.},
  title        = {Simulating and estimating agreement in the presence of multiple raters and covariates},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mediation analysis in the presence of continuous exposure
measurement error. <em>SIM</em>, <em>42</em>(11), 1669–1686. (<a
href="https://doi.org/10.1002/sim.9693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The difference method is used in mediation analysis to quantify the extent to which a mediator explains the mechanisms underlying the pathway between an exposure and an outcome. In many health science studies, the exposures are almost never measured without error, which can result in biased effect estimates. This article investigates methods for mediation analysis when a continuous exposure is mismeasured. Under a linear exposure measurement error model, we prove that the bias of indirect effect and mediation proportion can go in either direction but the mediation proportion is usually be less biased when the associations between the exposure and its error-prone counterpart are similar with and without adjustment for the mediator. We further propose methods to adjust for exposure measurement error with continuous and binary outcomes. The proposed approaches require a main study/validation study design where in the validation study, data are available for characterizing the relationship between the true exposure and its error-prone counterpart. The proposed approaches are then applied to the Health Professional Follow-up Study, 1986-2016, to investigate the impact of body mass index (BMI) as a mediator for mediating the effect of physical activity on the risk of cardiovascular diseases. Our results reveal that physical activity is significantly associated with a lower risk of cardiovascular disease incidence, and approximately half of the total effect of physical activity is mediated by BMI after accounting for exposure measurement error. Extensive simulation studies are conducted to demonstrate the validity and efficiency of the proposed approaches in finite samples.},
  archive      = {J_SIM},
  author       = {Chao Cheng and Donna Spiegelman and Fan Li},
  doi          = {10.1002/sim.9693},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1669-1686},
  shortjournal = {Stat. Med.},
  title        = {Mediation analysis in the presence of continuous exposure measurement error},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparisons of statistical methods for handling attrition in
a follow-up visit with complex survey sampling. <em>SIM</em>,
<em>42</em>(11), 1641–1668. (<a
href="https://doi.org/10.1002/sim.9692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design-based analysis, which accounts for the design features of the study, is commonly used to conduct data analysis in studies with complex survey sampling, such as the Hispanic Community Health Study/Study of Latinos (HCHS/SOL). In this type of longitudinal study, attrition has often been a problem. Although there have been various statistical approaches proposed to handle attrition, such as inverse probability weighting (IPW), non-response cell weighting (NRCW), multiple imputation (MI), and full information maximum likelihood (FIML) approach, there has not been a systematic assessment of these methods to compare their performance in design-based analyses. In this article, we perform extensive simulation studies and compare the performance of different missing data methods in linear and generalized linear population models, and under different missing data mechanism. We find that the design-based analysis is able to produce valid estimation and statistical inference when the missing data are handled appropriately using IPW, NRCW, MI, or FIML approach under missing-completely-at-random or missing-at-random missing mechanism and when the missingness model is correctly specified or over-specified. We also illustrate the use of these methods using data from HCHS/SOL.},
  archive      = {J_SIM},
  author       = {Jianwen Cai and Donglin Zeng and Haolin Li and Nicole M. Butera and Pedro L. Baldoni and Poulami Maitra and Li Dong},
  doi          = {10.1002/sim.9692},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {11},
  pages        = {1641-1668},
  shortjournal = {Stat. Med.},
  title        = {Comparisons of statistical methods for handling attrition in a follow-up visit with complex survey sampling},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying important gene signatures of BMI using network
structure-aided nonparametric quantile regression. <em>SIM</em>,
<em>42</em>(10), 1625–1639. (<a
href="https://doi.org/10.1002/sim.9691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on identifying genomics risk factors of higher body mass index (BMI) incorporating a priori information, such as biological pathways. However, the commonly used methods to incorporate prior information provide a model for the mean function of the outcome and rely on unmet assumptions. To address these concerns, we propose a method for nonparametric additive quantile regression with network regularization to incorporate the information encoded by known networks. To account for nonlinear associations, we approximate the unknown additive functional effect of each predictor with the expansion of a B-spline basis. We implement the group Lasso penalty to obtain a sparse model. We define the network-constrained penalty by the total norm of the difference between the effect functions of any two linked genes in the known network. We further propose an efficient computation procedure to solve the optimization problem that arises in our model. Simulation studies show that our proposed method performs well in identifying more truly associated genes and less falsely associated genes than alternative approaches. We apply the proposed method to analyze the microarray gene-expression dataset in the Framingham Heart Study and identify several 75 percentile BMI associated genes. In conclusion, our proposed approach efficiently identifies the outcome-associated variables in a nonparametric additive quantile regression framework by leveraging known network information.},
  archive      = {J_SIM},
  author       = {Peitao Wu and Josée Dupuis and Ching-Ti Liu},
  doi          = {10.1002/sim.9691},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1625-1639},
  shortjournal = {Stat. Med.},
  title        = {Identifying important gene signatures of BMI using network structure-aided nonparametric quantile regression},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impact of correlations between prioritized outcomes on the
net benefit and its estimate by generalized pairwise comparisons.
<em>SIM</em>, <em>42</em>(10), 1606–1624. (<a
href="https://doi.org/10.1002/sim.9690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefit-risk balance is gaining interest in clinical trials. For the comprehensive assessment of benefits and risks, generalized pairwise comparisons are increasingly used to estimate the net benefit based on multiple prioritized outcomes. Although previous research has demonstrated that the correlations between the outcomes impact the net benefit and its estimate, the direction and magnitude of this impact remain unclear. In this study, we investigated the impact of correlations between two binary or Gaussian variables on the true net benefit values via theoretical and numerical analyses. We also explored the impact of correlations between survival and categorical variables on the net benefit estimates based on four existing methods (Gehan, Péron, Gehan with correction, and Péron with correction) in the presence of right censoring via simulation and application to actual oncology clinical trial data. Our theoretical and numerical analyses revealed that the true net benefit values were impacted by the correlations in various directions depending on the outcome distributions. With binary endpoints, this direction was governed by a simple rule with a threshold of 50% for a favorable outcome. Our simulation showed that the net benefit estimates based on Gehan&#39;s or Péron&#39;s scoring rule could be substantially biased in the presence of right censoring, and that the direction and magnitude of this bias were associated with the outcome correlations. The recently proposed correction method greatly reduced this bias, even in the presence of strong outcome correlations. The impact of correlations should be carefully considered when interpreting the net benefit and its estimate.},
  archive      = {J_SIM},
  author       = {Kanako Fuyama and Mitsunori Ogawa and Junki Mizusawa and Yukihide Kanemitsu and Shin Fujita and Takuya Kawahara and Kentaro Sakamaki and Koji Oba},
  doi          = {10.1002/sim.9690},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1606-1624},
  shortjournal = {Stat. Med.},
  title        = {Impact of correlations between prioritized outcomes on the net benefit and its estimate by generalized pairwise comparisons},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient model-free approach to interaction screening
for high dimensional data. <em>SIM</em>, <em>42</em>(10), 1583–1605. (<a
href="https://doi.org/10.1002/sim.9688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An innovated model-free interaction screening procedure called the MCVIS is proposed for high dimensional data analysis. Specifically, we adopt the introduced MCV index for quantifying the importance of an interaction effect among predictors. Our proposed method is fully nonparametric and is capable of successfully selecting interactions even if the signal of parental main effects is weak. The MCVIS procedure has many distinctive features: (i) it can work with discrete, categorical and continuous covariates; (ii) it can deal with both categorical and continuous response, even handle the missing response; (iii) it is robust for heavy-tailed distributions, thus well accommodates heterogeneity typically caused by high dimensionality; (iv) it enjoys the sure screening and ranking consistency properties, therefore achieves dimension reduction without information loss. In another respect, computational feasibility is a top concern in high dimensional data analysis, by transforming our MCV into several variants, the MCVIS procedure is simple and fast to implement. Extensive numerical experiments and comparisons confirm the effectiveness and wide applicability of our MCVIS procedure. We further illustrate the proposed methodology by empirical study of two real datasets. Supplementary materials are available online.},
  archive      = {J_SIM},
  author       = {Wei Xiong and Han Pan and Jianrong Wang and Maozai Tian},
  doi          = {10.1002/sim.9688},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1583-1605},
  shortjournal = {Stat. Med.},
  title        = {An efficient model-free approach to interaction screening for high dimensional data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate functional data clustering using adaptive
density peak detection. <em>SIM</em>, <em>42</em>(10), 1565–1582. (<a
href="https://doi.org/10.1002/sim.9687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering for multivariate functional data is a challenging problem since the data are represented by a set of curves and functions belonging to an infinite-dimensional space. In this article, we propose a novel clustering method for multivariate functional data using an adaptive density peak detection technique. It is a quick cluster center identification algorithm based on the two measures of each functional data observation: the functional density estimate and the distance to the closest observation with a higher functional density. We suggest two types of functional density estimators for multivariate functional data. The first one is a functional k $$ k $$ -nearest neighbor density estimator based on (a) an L2 distance between raw functional curves, or (b) a semimetric of multivariate functional principal components. The second one is a -nearest neighbor density estimator based on multivariate functional principal scores. Our clustering method is computationally fast since it does not need an iterative process. The flexibility and advantages of the method are examined by comparing it with other existing clustering methods in simulation studies. A user-friendly R package FADPclust is developed for public use. Finally, our method is applied to a real case study in lung cancer research.},
  archive      = {J_SIM},
  author       = {Rui Ren and Kuangnan Fang and Qingzhao Zhang and Xiaofeng Wang},
  doi          = {10.1002/sim.9687},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1565-1582},
  shortjournal = {Stat. Med.},
  title        = {Multivariate functional data clustering using adaptive density peak detection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ranking of average treatment effects with generalized random
forests for time-to-event outcomes. <em>SIM</em>, <em>42</em>(10),
1542–1564. (<a href="https://doi.org/10.1002/sim.9686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linkage between drug claims data and clinical outcome allows a data-driven experimental approach to drug repurposing. We develop an estimation procedure based on generalized random forests for estimation of time-point specific average treatment effects in a time-to-event setting with competing risks. To handle right-censoring, we propose a two-step procedure for estimation, applying inverse probability weighting to construct time-point specific weighted outcomes as input for the generalized random forest. The generalized random forests adaptively handle covariate effects on the treatment assignment by applying a splitting rule that targets a causal parameter. Using simulated data we demonstrate that the method is effective for a causal search through a list of treatments to be ranked according to the magnitude of their effect on clinical outcome. We illustrate the method using the Danish national health registries where it is of interest to discover drugs with an unexpected protective effect against relapse of severe depression.},
  archive      = {J_SIM},
  author       = {Helene C. W. Rytgaard and Claus T. Ekstrøm and Lars V. Kessing and Thomas A. Gerds},
  doi          = {10.1002/sim.9686},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1542-1564},
  shortjournal = {Stat. Med.},
  title        = {Ranking of average treatment effects with generalized random forests for time-to-event outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Impute-then-exclude versus exclude-then-impute: Lessons when
imputing a variable used both in cohort creation and as an independent
variable in the analysis model. <em>SIM</em>, <em>42</em>(10),
1525–1541. (<a href="https://doi.org/10.1002/sim.9685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examined the setting in which a variable that is subject to missingness is used both as an inclusion/exclusion criterion for creating the analytic sample and subsequently as the primary exposure in the analysis model that is of scientific interest. An example is cancer stage, where patients with stage IV cancer are often excluded from the analytic sample, and cancer stage (I to III) is an exposure variable in the analysis model. We considered two analytic strategies. The first strategy, referred to as “exclude-then-impute,” excludes subjects for whom the observed value of the target variable is equal to the specified value and then uses multiple imputation to complete the data in the resultant sample. The second strategy, referred to as “impute-then-exclude,” first uses multiple imputation to complete the data and then excludes subjects based on the observed or filled-in values in the completed samples. Monte Carlo simulations were used to compare five methods (one based on “exclude-then-impute” and four based on “impute-then-exclude”) along with the use of a complete case analysis. We considered both missing completely at random and missing at random missing data mechanisms. We found that an impute-then-exclude strategy using substantive model compatible fully conditional specification tended to have superior performance across 72 different scenarios. We illustrated the application of these methods using empirical data on patients hospitalized with heart failure when heart failure subtype was used for cohort creation (excluding subjects with heart failure with preserved ejection fraction) and was also an exposure in the analysis model.},
  archive      = {J_SIM},
  author       = {Peter C. Austin and Daniele Giardiello and Stef van Buuren},
  doi          = {10.1002/sim.9685},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1525-1541},
  shortjournal = {Stat. Med.},
  title        = {Impute-then-exclude versus exclude-then-impute: Lessons when imputing a variable used both in cohort creation and as an independent variable in the analysis model},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mining adverse events in large frequency tables with
ontology, with an application to the vaccine adverse event reporting
system. <em>SIM</em>, <em>42</em>(10), 1512–1524. (<a
href="https://doi.org/10.1002/sim.9684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many statistical methods have been applied to VAERS (vaccine adverse event reporting system) database to study the safety of COVID-19 vaccines. However, none of these methods considered the adverse event (AE) ontology. The AE ontology contains important information about biological similarities between AEs. In this paper, we develop a model to estimate vaccine-AE associations while incorporating the AE ontology. We model a group of AEs using the zero-inflated negative binomial model and then estimate the vaccine-AE association using the empirical Bayes approach. This model handles the AE count data with excess zeros and allows borrowing information from related AEs. The proposed approach was evaluated by simulation studies and was further illustrated by an application to the Vaccine Adverse Event Reporting System (VAERS) dataset. The proposed method is implemented in an R package available at https://github.com/umich-biostatistics/zGPS.AO .},
  archive      = {J_SIM},
  author       = {Bangyao Zhao and Lili Zhao},
  doi          = {10.1002/sim.9684},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1512-1524},
  shortjournal = {Stat. Med.},
  title        = {Mining adverse events in large frequency tables with ontology, with an application to the vaccine adverse event reporting system},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate functional mixed model with MRI data: An
application to alzheimer’s disease. <em>SIM</em>, <em>42</em>(10),
1492–1511. (<a href="https://doi.org/10.1002/sim.9683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s Disease (AD) is the leading cause of dementia and impairment in various domains. Recent AD studies, (ie, Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) study), collect multimodal data, including longitudinal neurological assessments and magnetic resonance imaging (MRI) data, to better study the disease progression. Adopting early interventions is essential to slow AD progression for subjects with mild cognitive impairment (MCI). It is of particular interest to develop an AD predictive model that leverages multimodal data and provides accurate personalized predictions. In this article, we propose a multivariate functional mixed model with MRI data (MFMM-MRI) that simultaneously models longitudinal neurological assessments, baseline MRI data, and the survival outcome (ie, dementia onset) for subjects with MCI at baseline. Two functional forms (the random-effects model and instantaneous model) linking the longitudinal and survival process are investigated. We use Markov Chain Monte Carlo (MCMC) method based on No-U-Turn Sampling (NUTS) algorithm to obtain posterior samples. We develop a dynamic prediction framework that provides accurate personalized predictions of longitudinal trajectories and survival probability. We apply MFMM-MRI to the ADNI study and identify significant associations among longitudinal outcomes, MRI data, and the risk of dementia onset. The instantaneous model with voxels from the whole brain has the best prediction performance among all candidate models. The simulation study supports the validity of the estimation and dynamic prediction method.},
  archive      = {J_SIM},
  author       = {Haotian Zou and Luo Xiao and Donglin Zeng and Sheng Luo},
  doi          = {10.1002/sim.9683},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1492-1511},
  shortjournal = {Stat. Med.},
  title        = {Multivariate functional mixed model with MRI data: An application to alzheimer&#39;s disease},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group sequential multi-arm multi-stage trial design with
treatment selection. <em>SIM</em>, <em>42</em>(10), 1480–1491. (<a
href="https://doi.org/10.1002/sim.9682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-arm trial allows simultaneous comparison of multiple experimental treatments with a common control and provides a substantial efficiency advantage compared to the traditional randomized controlled trial. Many novel multi-arm multi-stage (MAMS) clinical trial designs have been proposed. However, a major hurdle to adopting the group sequential MAMS routinely is the computational effort of obtaining total sample size and sequential stopping boundaries. In this paper, we develop a group sequential MAMS trial design based on the sequential conditional probability ratio test. The proposed method provides analytical solutions for futility and efficacy boundaries to an arbitrary number of stages and arms. Thus, it avoids complicated computational effort for the methods proposed by Magirr et al. Simulation results showed that the proposed method has several advantages compared to the methods implemented in R package MAMS by Magirr et al.},
  archive      = {J_SIM},
  author       = {Jianrong Wu and Yimei Li and Liang Zhu},
  doi          = {10.1002/sim.9682},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1480-1491},
  shortjournal = {Stat. Med.},
  title        = {Group sequential multi-arm multi-stage trial design with treatment selection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of different methods to adjust survival curves
for confounders. <em>SIM</em>, <em>42</em>(10), 1461–1479. (<a
href="https://doi.org/10.1002/sim.9681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Treatment specific survival curves are an important tool to illustrate the treatment effect in studies with time-to-event outcomes. In non-randomized studies, unadjusted estimates can lead to biased depictions due to confounding. Multiple methods to adjust survival curves for confounders exist. However, it is currently unclear which method is the most appropriate in which situation. Our goal is to compare forms of inverse probability of treatment weighting, the G-Formula, propensity score matching, empirical likelihood estimation and augmented estimators as well as their pseudo-values based counterparts in different scenarios with a focus on their bias and goodness-of-fit. We provide a short review of all methods and illustrate their usage by contrasting the survival of smokers and non-smokers, using data from the German Epidemiological Trial on Ankle-Brachial-Index. Subsequently, we compare the methods using a Monte-Carlo simulation. We consider scenarios in which correctly or incorrectly specified models for describing the treatment assignment and the time-to-event outcome are used with varying sample sizes. The bias and goodness-of-fit is determined by taking the entire survival curve into account. When used properly, all methods showed no systematic bias in medium to large samples. Cox regression based methods, however, showed systematic bias in small samples. The goodness-of-fit varied greatly between different methods and scenarios. Methods utilizing an outcome model were more efficient than other techniques, while augmented estimators using an additional treatment assignment model were unbiased when either model was correct with a goodness-of-fit comparable to other methods. These “doubly-robust” methods have important advantages in every considered scenario.},
  archive      = {J_SIM},
  author       = {Robin Denz and Renate Klaaßen-Mielke and Nina Timmesfeld},
  doi          = {10.1002/sim.9681},
  journal      = {Statistics in Medicine},
  month        = {5},
  number       = {10},
  pages        = {1461-1479},
  shortjournal = {Stat. Med.},
  title        = {A comparison of different methods to adjust survival curves for confounders},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian hierarchical model for signal extraction from
protein microarrays. <em>SIM</em>, <em>42</em>(9), 1445–1460. (<a
href="https://doi.org/10.1002/sim.9680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein microarrays are a promising technology that measure protein levels in serum or plasma samples. Due to their high technical variability and high variation in protein levels across serum samples in any population, directly answering biological questions of interest using protein microarray measurements is challenging. Analyzing preprocessed data and within-sample ranks of protein levels can mitigate the impact of between-sample variation. As for any analysis, ranks are sensitive to preprocessing, but loss function based ranks that accommodate major structural relations and components of uncertainty are very effective. Bayesian modeling with full posterior distributions for quantities of interest produce the most effective ranks. Such Bayesian models have been developed for other assays, for example, DNA microarrays, but modeling assumptions for these assays are not appropriate for protein microarrays. Consequently, we develop and evaluate a Bayesian model to extract the full posterior distribution of normalized protein levels and associated ranks for protein microarrays, and show that it fits well to data from two studies that use protein microarrays produced by different manufacturing processes. We validate the model via simulation and demonstrate the downstream impact of using estimates from this model to obtain optimal ranks.},
  archive      = {J_SIM},
  author       = {Sophie Bérubé and Tamaki Kobayashi and Amy Wesolowski and Douglas E. Norris and Ingo Ruczinski and William J. Moss and Thomas A. Louis},
  doi          = {10.1002/sim.9680},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1445-1460},
  shortjournal = {Stat. Med.},
  title        = {A bayesian hierarchical model for signal extraction from protein microarrays},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast estimation of mixed-effects location-scale regression
models. <em>SIM</em>, <em>42</em>(9), 1430–1444. (<a
href="https://doi.org/10.1002/sim.9679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a result of advances in data collection technology and study design, modern longitudinal datasets can be much larger than they historically have been. Such “intensive&quot; longitudinal datasets are rich enough to allow for detailed modeling of the variance of a response as well as the mean, and a flexible class of models called mixed-effects location-scale (MELS) regression models are commonly used to do so. However, fitting MELS models can pose computational challenges related to the numerical evaluation of multi-dimensional integrals; the slow runtime of current methods is inconvenient for data analysis and makes bootstrap inference impractical. In this paper, we introduce a new fitting technique, called FastRegLS, that is considerably faster than existing techniques while still providing consistent estimators for the model parameters.},
  archive      = {J_SIM},
  author       = {Nathan Gill and Donald Hedeker},
  doi          = {10.1002/sim.9679},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1430-1444},
  shortjournal = {Stat. Med.},
  title        = {Fast estimation of mixed-effects location-scale regression models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-specific inference for penalized regression using
local false discovery rates. <em>SIM</em>, <em>42</em>(9), 1412–1429.
(<a href="https://doi.org/10.1002/sim.9678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penalized regression methods such as the lasso are a popular approach to analyzing high-dimensional data. One attractive property of the lasso is that it naturally performs variable selection. An important area of concern, however, is the reliability of these selections. Motivated by local false discovery rate methodology from the large-scale hypothesis testing literature, we propose a method for calculating a local false discovery rate for each variable under consideration by the lasso model. These rates can be used to assess the reliability of an individual feature, or to estimate the model&#39;s overall false discovery rate. The method can be used for any level of regularization. This is particularly useful for models with a few highly significant features but a high overall false discovery rate, a relatively common occurrence when using cross validation to select a model. It is also flexible enough to be applied to many varieties of penalized likelihoods including generalized linear models and Cox regression, and a variety of penalties, including the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD) penalty. We demonstrate the validity of this approach and contrast it with other inferential methods for penalized regression as well as with local false discovery rates for univariate hypothesis tests. Finally, we show the practical utility of our method by applying it to a case study involving gene expression in breast cancer patients.},
  archive      = {J_SIM},
  author       = {Ryan Miller and Patrick Breheny},
  doi          = {10.1002/sim.9678},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1412-1429},
  shortjournal = {Stat. Med.},
  title        = {Feature-specific inference for penalized regression using local false discovery rates},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional concordance-assisted learning under matched
case-control design for combining biomarkers for population screening.
<em>SIM</em>, <em>42</em>(9), 1398–1411. (<a
href="https://doi.org/10.1002/sim.9677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating promising biomarkers into cancer screening practices for early-detection is increasingly appealing because of the unsatisfactory performance of current cancer screening strategies. The matched case-control design is commonly adopted in biomarker development studies to evaluate the discriminative power of biomarker candidates, with an intention to eliminate confounding effects. Data from matched case-control studies have been routinely analyzed by the conditional logistic regression, although the assumed logit link between biomarker combinations and disease risk may not always hold. We propose a conditional concordance-assisted learning method, which is distribution-free, for identifying an optimal combination of biomarkers to discriminate cases and controls. We are particularly interested in combinations with a clinically and practically meaningful specificity to prevent disease-free subjects from unnecessary and possibly intrusive diagnostic procedures, which is a top priority for cancer population screening. We establish asymptotic properties for the derived combination and confirm its favorable finite sample performance in simulations. We apply the proposed method to the prostate cancer data from the carotene and retinol efficacy trial (CARET).},
  archive      = {J_SIM},
  author       = {Wen Li and Ruosha Li and Qingxiang Yan and Ziding Feng and Jing Ning},
  doi          = {10.1002/sim.9677},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1398-1411},
  shortjournal = {Stat. Med.},
  title        = {Conditional concordance-assisted learning under matched case-control design for combining biomarkers for population screening},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multistate models as a framework for estimand specification
in clinical trials of complex processes. <em>SIM</em>, <em>42</em>(9),
1368–1397. (<a href="https://doi.org/10.1002/sim.9675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensity-based multistate models provide a useful framework for characterizing disease processes, the introduction of interventions, loss to followup, and other complications arising in the conduct of randomized trials studying complex life history processes. Within this framework we discuss the issues involved in the specification of estimands and show the limiting values of common estimators of marginal process features based on cumulative incidence function regression models. When intercurrent events arise we stress the need to carefully define the target estimand and the importance of avoiding targets of inference that are not interpretable in the real world. This has implications for analyses, but also the design of clinical trials where protocols may help in the interpretation of estimands based on marginal features.},
  archive      = {J_SIM},
  author       = {Alexandra Bühler and Richard J. Cook and Jerald F. Lawless},
  doi          = {10.1002/sim.9675},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1368-1397},
  shortjournal = {Stat. Med.},
  title        = {Multistate models as a framework for estimand specification in clinical trials of complex processes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and modeling for drug combination experiments with
order effects. <em>SIM</em>, <em>42</em>(9), 1353–1367. (<a
href="https://doi.org/10.1002/sim.9674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinations of drugs are now ubiquitous in treating complex diseases such as cancer and HIV due to their potential for enhanced efficacy and reduced side effects. The traditional combination experiments of drugs focus primarily on the dose effects of the constituent drugs. However, with the doses of drugs remaining unchanged, different sequences of drug administration may also affect the efficacy endpoint. Such drug effects shall be called as order effects. The common order-effect linear models are usually inadequate for analyzing combination experiments due to the nonlinear relationships and complex interactions among drugs. In this article, we propose a random field model for order-effect modeling. This model is flexible, allowing nonlinearities, and interaction effects to be incorporated with a small number of model parameters. Moreover, we propose a subtle experimental design that will collect good quality data for modeling the order effects of drugs with a reasonable run size. A real-data analysis and simulation studies are given to demonstrate that the proposed design and model are effective in predicting the optimal drug sequences in administration.},
  archive      = {J_SIM},
  author       = {Hengzhen Huang and Min-Qian Liu and Ming T. Tan and Hong-Bin Fang},
  doi          = {10.1002/sim.9674},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1353-1367},
  shortjournal = {Stat. Med.},
  title        = {Design and modeling for drug combination experiments with order effects},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model misspecification and robust analysis for
outcome-dependent sampling designs under generalized linear models.
<em>SIM</em>, <em>42</em>(9), 1338–1352. (<a
href="https://doi.org/10.1002/sim.9673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outcome-dependent sampling (ODS) is a commonly used class of sampling designs to increase estimation efficiency in settings where response information (and possibly adjuster covariates) is available, but the exposure is expensive and/or cumbersome to collect. We focus on ODS within the context of a two-phase study, where in Phase One the response and adjuster covariate information is collected on a large cohort that is representative of the target population, but the expensive exposure variable is not yet measured. In Phase Two, using response information from Phase One, we selectively oversample a subset of informative subjects in whom we collect expensive exposure information. Importantly, the Phase Two sample is no longer representative, and we must use ascertainment-correcting analysis procedures for valid inferences. In this paper, we focus on likelihood-based analysis procedures, particularly a conditional-likelihood approach and a full-likelihood approach. Whereas the full-likelihood retains incomplete Phase One data for subjects not selected into Phase Two, the conditional-likelihood explicitly conditions on Phase Two sample selection (ie, it is a “complete case” analysis procedure). These designs and analysis procedures are typically implemented assuming a known, parametric model for the response distribution. However, in this paper, we approach analyses implementing a novel semi-parametric extension to generalized linear models (SPGLM) to develop likelihood-based procedures with improved robustness to misspecification of distributional assumptions. We specifically focus on the common setting where standard GLM distributional assumptions are not satisfied (eg, misspecified mean/variance relationship). We aim to provide practical design guidance and flexible tools for practitioners in these settings.},
  archive      = {J_SIM},
  author       = {Jacob M. Maronge and Jonathan S. Schildcrout and Paul J. Rathouz},
  doi          = {10.1002/sim.9673},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1338-1352},
  shortjournal = {Stat. Med.},
  title        = {Model misspecification and robust analysis for outcome-dependent sampling designs under generalized linear models},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulated annealing for balancing covariates. <em>SIM</em>,
<em>42</em>(9), 1323–1337. (<a
href="https://doi.org/10.1002/sim.9672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate balance is one of the fundamental issues in designing experiments for treatment comparisons, especially in randomized clinical trials. In this article, we introduce a new class of covariate-adaptive procedures based on the Simulated Annealing algorithm aimed at balancing the allocations of two competing treatments across a set of pre-specified covariates. Due to the nature of the simulated annealing, these designs are intrinsically randomized, thus completely unpredictable, and very flexible: they can manage both quantitative and qualitative factors and be implemented in a static version as well as sequentially. The properties of the suggested proposal are described, showing a significant improvement in terms of covariate balance and inferential accuracy with respect to all the other procedures proposed in the literature. An illustrative example based on real data is also discussed.},
  archive      = {J_SIM},
  author       = {Alessandro Baldi Antognini and Marco Novelli and Maroussa Zagoraiou},
  doi          = {10.1002/sim.9672},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1323-1337},
  shortjournal = {Stat. Med.},
  title        = {Simulated annealing for balancing covariates},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New c-indices for assessing importance of longitudinal
biomarkers in fitting competing risks survival data in the presence of
partially masked causes. <em>SIM</em>, <em>42</em>(9), 1308–1322. (<a
href="https://doi.org/10.1002/sim.9671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Competing risks survival data in the presence of partially masked causes are frequently encountered in medical research or clinical trials. When longitudinal biomarkers are also available, it is of great clinical importance to examine associations between the longitudinal biomarkers and the cause-specific survival outcomes. In this article, we propose a cause-specific C-index for joint models of longitudinal and competing risks survival data accounting for masked causes. We also develop a posterior predictive algorithm for computing the out-of-sample cause-specific C-index using Markov chain Monte Carlo samples from the joint posterior of the in-sample longitudinal and competing risks survival data. We further construct the C-index to quantify the strength of association between the longitudinal and cause-specific survival data, or between the out-of-sample longitudinal and survival data. Empirical performance of the proposed assessment criteria is examined through an extensive simulation study. An in-depth analysis of the real data from large cancer prevention trials is carried out to demonstrate the usefulness of the proposed methodology.},
  archive      = {J_SIM},
  author       = {Md. Tuhin Sheikh and Ming-Hui Chen and Jonathan A. Gelfond and Wei Sun and Joseph G. Ibrahim},
  doi          = {10.1002/sim.9671},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1308-1322},
  shortjournal = {Stat. Med.},
  title        = {New C-indices for assessing importance of longitudinal biomarkers in fitting competing risks survival data in the presence of partially masked causes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured time-dependent inverse regression (STIR).
<em>SIM</em>, <em>42</em>(9), 1289–1307. (<a
href="https://doi.org/10.1002/sim.9670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study structured time-dependent inverse regression (STIR), a novel sufficient dimension reduction model, to analyze longitudinally measured, correlated biomarkers in relation to an outcome. The time structure is accommodated in an inverse regression model for the markers that can be applied both to equally and unequally spaced time points for each sample. The inverse regression structure also naturally accommodates retrospectively sampled markers, that is, markers measured in case-control studies. We estimate the corresponding linear combinations of the markers, the reduction, using least squares. We show that under additional distributional assumptions the reduction contains sufficient information about the outcome. In extensive simulations the STIR linear combinations perform well in predictive models based on samples of realistic size. A Wald-type test for association of a particular marker with outcome at any time point based on the STIR reduction has better power overall than assessing associations based on logistic or linear regression models that include all longitudinally measured markers as independent predictors. As illustrations we estimate the STIR reductions for a cohort study of diabetes and hyperlipidemia and a case-control study of brain cancer with multiple longitudinally measured biomarkers. We assess the STIR reductions&#39; predictive performance and identify outcome-associated biomarkers.},
  archive      = {J_SIM},
  author       = {Minsun Song and Efstathia Bura and Roman Parzer and Ruth M. Pfeiffer},
  doi          = {10.1002/sim.9670},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {9},
  pages        = {1289-1307},
  shortjournal = {Stat. Med.},
  title        = {Structured time-dependent inverse regression (STIR)},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In praise of prais-winsten: An evaluation of methods used to
account for autocorrelation in interrupted time series. <em>SIM</em>,
<em>42</em>(8), 1277–1288. (<a
href="https://doi.org/10.1002/sim.9669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interrupted time series are increasingly being used to assess the population impact of public health interventions. These data are usually correlated over time (auto correlated) and this must be accounted for in the analysis. Typically, this is done using either the Prais-Winsten method, the Newey-West method, or autoregressive-moving-average (ARMA) modeling. In this paper, we illustrate these methods via a study of pneumococcal vaccine introduction and explore their performance under 20 simulated autocorrelation scenarios with sample sizes ranging between 20 and 300. We show that in terms of mean square error, the Prais-Winsten and ARMA methods perform best, while in terms of coverage the Prais-Winsten method generally performs better than other methods. All three methods are unbiased. As well as having good statistical properties, the Prais-Winsten method is attractive because it is decision-free and produces a single measure of autocorrelation that can be compared between studies and used to guide sample size calculations. We would therefore encourage analysts to consider using this simple method to analyze interrupted time series.},
  archive      = {J_SIM},
  author       = {C Bottomley and M Ooko and A Gasparrini and RH Keogh},
  doi          = {10.1002/sim.9669},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1277-1288},
  shortjournal = {Stat. Med.},
  title        = {In praise of prais-winsten: An evaluation of methods used to account for autocorrelation in interrupted time series},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Absolute and relative risk estimation in the presence of
outcome ascertainment gaps and competing risks. <em>SIM</em>,
<em>42</em>(8), 1263–1276. (<a
href="https://doi.org/10.1002/sim.9668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete coverage by cancer registries can lead to an underreporting of cancers and a resulting bias in risk estimates. When registries are defined by geographic region, gaps in observation can arise for individuals who reside outside of or migrate from the total registry catchment area. Moreover, the exact periods of non-observation for an individual may be unknown due to intermittent reporting of residential histories. The motivating example for this work is the U.S. Radiologic Technologist (USRT) study which ascertained cancer outcomes for a national cohort through 43 state/regional registries; similar gaps in outcome ascertainment can appear in other registry or electronic health record- based cohort studies. We propose a two-step procedure for estimating relative and absolute risk in these settings. First, using a mover stayer model fitted to individuals&#39; known residential history, we obtain individual posterior probabilities of residing outside the registry catchment area each year. Second, we incorporate these probabilities in the survival data likelihood for competing risks to account for unobserved events. We assess the performance of the proposed method in extensive simulation studies. Compared to several simple alternative approaches, the proposed method reduces bias and improves efficiency. Finally, we apply the proposed method to a study of first primary lung cancers in the USRT cohort.},
  archive      = {J_SIM},
  author       = {Danping Liu and Emily Wu and Joanna H. Shih and Cari M. Kitahara and Li C. Cheung},
  doi          = {10.1002/sim.9668},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1263-1276},
  shortjournal = {Stat. Med.},
  title        = {Absolute and relative risk estimation in the presence of outcome ascertainment gaps and competing risks},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible class of generalized joint frailty models for the
analysis of survival endpoints. <em>SIM</em>, <em>42</em>(8), 1233–1262.
(<a href="https://doi.org/10.1002/sim.9667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on shared frailty models for correlated failure times, as well as joint frailty models for the simultaneous analysis of recurrent events (eg, appearance of new cancerous lesions or hospital readmissions) and a major terminal event (typically, death). As extensions of the Cox model, these joint models usually assume a frailty proportional hazards model for each of the recurrent and terminal event processes. In order to extend these models beyond the proportional hazards assumption, our proposal is to replace these proportional hazards models with generalized survival models, for which the survival function is modeled as a linear predictor through a link function. Depending on the link function considered, these can be reduced to proportional hazards, proportional odds, additive hazards, or probit models. We first consider a fully parametric framework for the time and covariate effects. For proportional and additive hazards models, our approach also allows the use of smooth functions for baseline hazard functions and time-varying coefficients. The dependence between recurrent and terminal event processes is modeled by conditioning on a shared frailty acting differently on the two processes. Parameter estimates are provided using the maximum (penalized) likelihood method, implemented in the R package frailtypack (function GenfrailtyPenal ). We perform simulation studies to assess the method, which is also illustrated on real datasets.},
  archive      = {J_SIM},
  author       = {Jocelyn Chauvet and Virginie Rondeau},
  doi          = {10.1002/sim.9667},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1233-1262},
  shortjournal = {Stat. Med.},
  title        = {A flexible class of generalized joint frailty models for the analysis of survival endpoints},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-phase designs with current status data. <em>SIM</em>,
<em>42</em>(8), 1207–1232. (<a
href="https://doi.org/10.1002/sim.9666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the design and analysis of two-phase studies aiming to assess the relation between a fixed (eg, genetic) marker and an event time under current status observation. We consider a common setting in which a phase I sample is comprised of a large cohort of individuals with outcome (ie, current status) data and a vector of inexpensive covariates. Stored biospecimens for individuals in the phase I sample can be assayed to record the marker of interest for individuals selected in a phase II sub-sample. The design challenge is then to select the phase II sub-sample in order to maximize the precision of the marker effect on the time of interest under a proportional hazards model. This problem has not been examined before for current status data and the role of the assessment time is highlighted. Inference based on likelihood and inverse probability weighted estimating functions are considered, with designs centered on score-based residuals, extreme current status observations, or stratified sampling schemes. Data from a registry of patients with psoriatic arthritis is used in an illustration where we study the risk of diabetes as a comorbidity.},
  archive      = {J_SIM},
  author       = {Fangya Mao and Richard J. Cook},
  doi          = {10.1002/sim.9666},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1207-1232},
  shortjournal = {Stat. Med.},
  title        = {Two-phase designs with current status data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring the performance of prediction models to
personalize treatment choice. <em>SIM</em>, <em>42</em>(8), 1188–1206.
(<a href="https://doi.org/10.1002/sim.9665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When data are available from individual patients receiving either a treatment or a control intervention in a randomized trial, various statistical and machine learning methods can be used to develop models for predicting future outcomes under the two conditions, and thus to predict treatment effect at the patient level. These predictions can subsequently guide personalized treatment choices. Although several methods for validating prediction models are available, little attention has been given to measuring the performance of predictions of personalized treatment effect. In this article, we propose a range of measures that can be used to this end. We start by defining two dimensions of model accuracy for treatment effects, for a single outcome: discrimination for benefit and calibration for benefit. We then amalgamate these two dimensions into an additional concept, decision accuracy, which quantifies the model&#39;s ability to identify patients for whom the benefit from treatment exceeds a given threshold. Subsequently, we propose a series of performance measures related to these dimensions and discuss estimating procedures, focusing on randomized data. Our methods are applicable for continuous or binary outcomes, for any type of prediction model, as long as it uses baseline covariates to predict outcomes under treatment and control. We illustrate all methods using two simulated datasets and a real dataset from a trial in depression. We implement all methods in the R package predieval . Results suggest that the proposed measures can be useful in evaluating and comparing the performance of competing models in predicting individualized treatment effect.},
  archive      = {J_SIM},
  author       = {Orestis Efthimiou and Jeroen Hoogland and Thomas P.A. Debray and Michael Seo and Toshiaki A. Furukawa and Matthias Egger and Ian R. White},
  doi          = {10.1002/sim.9665},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1188-1206},
  shortjournal = {Stat. Med.},
  title        = {Measuring the performance of prediction models to personalize treatment choice},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The impact of correlated exposures and missing data on
multiple informant models used to identify critical exposure windows.
<em>SIM</em>, <em>42</em>(8), 1171–1187. (<a
href="https://doi.org/10.1002/sim.9664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been heightened interest in identifying critical windows of exposure for adverse health outcomes; that is, time points during which exposures have the greatest impact on a person&#39;s health. Multiple informant models implemented using generalized estimating equations (MIM GEEs) have been applied to address this research question because they enable statistical comparisons of differences in associations across exposure windows. As interest rises in using MIMs, the feasibility and appropriateness of their application under settings of correlated exposures and partially missing exposure measurements requires further examination. We evaluated the impact of correlation between exposure measurements and missing exposure data on the power and differences in association estimated by the MIM GEE and an inverse probability weighted extension to account for informatively missing exposures. We assessed these operating characteristics under a variety of correlation structures, sample sizes, and missing data mechanisms considering various exposure-outcome scenarios. We showed that applying MIM GEEs maintains higher power when there is a single critical window of exposure and exposure measures are not highly correlated, but may result in low power and bias under other settings. We applied these methods to a study of pregnant women living with HIV to explore differences in association between trimester-specific viral load and infant neurodevelopment.},
  archive      = {J_SIM},
  author       = {Jemar R. Bather and Nicholas J. Horton and Brent A. Coull and Paige L. Williams},
  doi          = {10.1002/sim.9664},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1171-1187},
  shortjournal = {Stat. Med.},
  title        = {The impact of correlated exposures and missing data on multiple informant models used to identify critical exposure windows},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The personalised randomized controlled trial: Evaluation of
a new trial design. <em>SIM</em>, <em>42</em>(8), 1156–1170. (<a
href="https://doi.org/10.1002/sim.9663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In some clinical scenarios, for example, severe sepsis caused by extensively drug resistant bacteria, there is uncertainty between many common treatments, but a conventional multiarm randomized trial is not possible because individual participants may not be eligible to receive certain treatments. The Personalised Randomized Controlled Trial design allows each participant to be randomized between a “personalised randomization list” of treatments that are suitable for them. The primary aim is to produce treatment rankings that can guide choice of treatment, rather than focusing on the estimates of relative treatment effects. Here we use simulation to assess several novel analysis approaches for this innovative trial design. One of the approaches is like a network meta-analysis, where participants with the same personalised randomization list are like a trial, and both direct and indirect evidence are used. We evaluate this proposed analysis and compare it with analyses making less use of indirect evidence. We also propose new performance measures including the expected improvement in outcome if the trial&#39;s rankings are used to inform future treatment rather than random choice. We conclude that analysis of a personalized randomized controlled trial can be performed by pooling data from different types of participants and is robust to moderate subgroup-by-intervention interactions based on the parameters of our simulation. The proposed approach performs well with respect to estimation bias and coverage. It provides an overall treatment ranking list with reasonable precision, and is likely to improve outcome on average if used to determine intervention policies and guide individual clinical decisions.},
  archive      = {J_SIM},
  author       = {Kim May Lee and Rebecca M. Turner and Guy E. Thwaites and A. Sarah Walker and Ian R. White},
  doi          = {10.1002/sim.9663},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1156-1170},
  shortjournal = {Stat. Med.},
  title        = {The personalised randomized controlled trial: Evaluation of a new trial design},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On assessing survival benefit of immunotherapy using
long-term restricted mean survival time. <em>SIM</em>, <em>42</em>(8),
1139–1155. (<a href="https://doi.org/10.1002/sim.9662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pattern of the difference between two survival curves we often observe in randomized clinical trials for evaluating immunotherapy is not proportional hazards; the treatment effect typically appears several months after the initiation of the treatment (ie, delayed difference pattern). The commonly used logrank test and hazard ratio estimation approach will be suboptimal concerning testing and estimation for those trials. The long-term restricted mean survival time (LT-RMST) approach is a promising alternative for detecting the treatment effect that potentially appears later in the study. A challenge in employing the LT-RMST approach is that it must specify a lower end of the time window in addition to a truncation time point that the RMST requires. There are several investigations and suggestions regarding the choice of the truncation time point for the RMST. However, little has been investigated to address the choice of the lower end of the time window. In this paper, we propose a flexible LT-RMST-based test/estimation approach that does not require users to specify a lower end of the time window. Numerical studies demonstrated that the potential power loss by adopting this flexibility was minimal, compared to the standard LT-RMST approach using a prespecified lower end of the time window. The proposed method is flexible and can offer higher power than the RMST-based approach when the delayed treatment effect is expected. Also, it provides a robust estimate of the magnitude of the treatment effect and its confidence interval that corresponds to the test result.},
  archive      = {J_SIM},
  author       = {Miki Horiguchi and Lu Tian and Hajime Uno},
  doi          = {10.1002/sim.9662},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1139-1155},
  shortjournal = {Stat. Med.},
  title        = {On assessing survival benefit of immunotherapy using long-term restricted mean survival time},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical approaches to bayesian sample size determination
in non-inferiority trials with binary outcomes. <em>SIM</em>,
<em>42</em>(8), 1127–1138. (<a
href="https://doi.org/10.1002/sim.9661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian analysis of a non-inferiority trial is advantageous in allowing direct probability statements to be made about the relative treatment difference rather than relying on an arbitrary and often poorly justified non-inferiority margin. When the primary analysis will be Bayesian, a Bayesian approach to sample size determination will often be appropriate for consistency with the analysis. We demonstrate three Bayesian approaches to choosing sample size for non-inferiority trials with binary outcomes and review their advantages and disadvantages. First, we present a predictive power approach for determining sample size using the probability that the trial will produce a convincing result in the final analysis. Next, we determine sample size by considering the expected posterior probability of non-inferiority in the trial. Finally, we demonstrate a precision-based approach. We apply these methods to a non-inferiority trial in antiretroviral therapy for treatment of HIV-infected children. A predictive power approach would be most accessible in practical settings, because it is analogous to the standard frequentist approach. Sample sizes are larger than with frequentist calculations unless an informative analysis prior is specified, because appropriate allowance is made for uncertainty in the assumed design parameters, ignored in frequentist calculations. An expected posterior probability approach will lead to a smaller sample size and is appropriate when the focus is on estimating posterior probability rather than on testing. A precision-based approach would be useful when sample size is restricted by limits on recruitment or costs, but it would be difficult to decide on sample size using this approach alone.},
  archive      = {J_SIM},
  author       = {Rebecca M. Turner and Michelle N. Clements and Matteo Quartagno and Victoria Cornelius and Suzie Cro and Deborah Ford and Conor D. Tweed and A. Sarah Walker and Ian R. White},
  doi          = {10.1002/sim.9661},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1127-1138},
  shortjournal = {Stat. Med.},
  title        = {Practical approaches to bayesian sample size determination in non-inferiority trials with binary outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple assessments of non-inferiority trials with ordinal
endpoints. <em>SIM</em>, <em>42</em>(8), 1113–1126. (<a
href="https://doi.org/10.1002/sim.9660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-inferiority (NI) trials are implemented when there is a practical demand to search for alternatives to standard therapies, such as to reduce side effects. An experimental treatment is considered non-inferior to the standard treatment when it exhibits clinically non-significant loss of efficacy. Ordinal categorical responses are frequently observed in clinical trials. It has been reported that responses measured using an ordinal scale produce more informative analysis than when responses collapse into binary outcomes. We study the NI trials using ordinal endpoints. We propose a latent variable model for ordinal categorical responses. Based on the proposed latent variable model, the mean efficacy of the different treatments is denoted by the corresponding mean parameter of the underlying continuous distributions. A two-step procedure is proposed for model identification and parameter estimation. A non-inferiority analysis can then be conducted based on the latent variable model and the corresponding estimation procedure. We also develop a method and an algorithm to produce an optimal sample size configuration based on the proposed testing procedure. Two clinical examples are provided for demonstrative purposes.},
  archive      = {J_SIM},
  author       = {Wenfu Xu and Yuli Hou and Tong-Yu Lu},
  doi          = {10.1002/sim.9660},
  journal      = {Statistics in Medicine},
  month        = {4},
  number       = {8},
  pages        = {1113-1126},
  shortjournal = {Stat. Med.},
  title        = {Multiple assessments of non-inferiority trials with ordinal endpoints},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel approach to assess dynamic treatment regimes
embedded in a SMART with an ordinal outcome. <em>SIM</em>,
<em>42</em>(7), 1096–1111. (<a
href="https://doi.org/10.1002/sim.9659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential multiple assignment randomized trials (SMARTs) are used to construct data-driven optimal intervention strategies for subjects based on their intervention and covariate histories in different branches of health and behavioral sciences where a sequence of interventions is given to a participant. Sequential intervention strategies are often called dynamic treatment regimes (DTR). In the existing literature, the majority of the analysis methodologies for SMART data assume a continuous primary outcome. However, ordinal outcomes are also quite common in clinical practice. In this work, first, we introduce the notion of generalized odds ratio ( ) to compare two DTRs embedded in a SMART with an ordinal outcome and discuss some combinatorial properties of this measure. Next, we propose a likelihood-based approach to estimate from SMART data, and derive the asymptotic properties of its estimate. We discuss alternative ways to estimate using concordant-discordant pairs and two-sample -statistic. We derive the required sample size formula for designing SMARTs with ordinal outcomes based on . A simulation study shows the performance of the estimated in terms of the estimated power corresponding to the derived sample size. The methodology is applied to analyze data from the SMART+ study, conducted in the UK, to improve carbohydrate periodization behavior in athletes using a menu planner mobile application, Hexis Performance. A freely available Shiny web app using R is provided to make the proposed methodology accessible to other researchers and practitioners.},
  archive      = {J_SIM},
  author       = {Palash Ghosh and Xiaoxi Yan and Bibhas Chakraborty},
  doi          = {10.1002/sim.9659},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1096-1111},
  shortjournal = {Stat. Med.},
  title        = {A novel approach to assess dynamic treatment regimes embedded in a SMART with an ordinal outcome},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Propensity score matching after multiple imputation when a
confounder has missing data. <em>SIM</em>, <em>42</em>(7), 1082–1095.
(<a href="https://doi.org/10.1002/sim.9658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges when using observational data for causal inference is the presence of confounding. A classic approach to account for confounding is the use of propensity score techniques that provide consistent estimators of the causal treatment effect under four common identifiability assumptions for causal effects, including that of no unmeasured confounding. Propensity score matching is a very popular approach which, in its simplest form, involves matching each treated patient to an untreated patient with a similar estimated propensity score, that is, probability of receiving the treatment. The treatment effect can then be estimated by comparing treated and untreated patients within the matched dataset. When missing data arises, a popular approach is to apply multiple imputation to handle the missingness. The combination of propensity score matching and multiple imputation is increasingly applied in practice. However, in this article we demonstrate that combining multiple imputation and propensity score matching can lead to over-coverage of the confidence interval for the treatment effect estimate. We explore the cause of this over-coverage and we evaluate, in this context, the performance of a correction to Rubin&#39;s rules for multiple imputation proposed by finding that this correction removes the over-coverage.},
  archive      = {J_SIM},
  author       = {Corentin Ségalas and Clémence Leyrat and James R. Carpenter and Elizabeth Williamson},
  doi          = {10.1002/sim.9658},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1082-1095},
  shortjournal = {Stat. Med.},
  title        = {Propensity score matching after multiple imputation when a confounder has missing data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individual frailty excess hazard models in cancer
epidemiology. <em>SIM</em>, <em>42</em>(7), 1066–1081. (<a
href="https://doi.org/10.1002/sim.9657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unobserved individual heterogeneity is a common challenge in population cancer survival studies. This heterogeneity is usually associated with the combination of model misspecification and the failure to record truly relevant variables. We investigate the effects of unobserved individual heterogeneity in the context of excess hazard models, one of the main tools in cancer epidemiology. We propose an individual excess hazard frailty model to account for individual heterogeneity. This represents an extension of frailty modeling to the relative survival framework. In order to facilitate the inference on the parameters of the proposed model, we select frailty distributions which produce closed-form expressions of the marginal hazard and survival functions. The resulting model allows for an intuitive interpretation, in which the frailties induce a selection of the healthier individuals among survivors. We model the excess hazard using a flexible parametric model with a general hazard structure which facilitates the inclusion of time-dependent effects. We illustrate the performance of the proposed methodology through a simulation study. We present a real-data example using data from lung cancer patients diagnosed in England, and discuss the impact of not accounting for unobserved heterogeneity on the estimation of net survival. The methodology is implemented in the R package IFNS .},
  archive      = {J_SIM},
  author       = {Francisco J. Rubio and Hein Putter and Aurélien Belot},
  doi          = {10.1002/sim.9657},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1066-1081},
  shortjournal = {Stat. Med.},
  title        = {Individual frailty excess hazard models in cancer epidemiology},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the predictive performance of subtyping: A
criterion for cluster mean-based prediction. <em>SIM</em>,
<em>42</em>(7), 1045–1065. (<a
href="https://doi.org/10.1002/sim.9656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneity is a frequent issue in population data analyses in medicine, biology, and the social sciences. A common approach for handling heterogeneity is to use a clustering algorithm to group similar samples, considering samples within the same group to be homogeneous. This approach is known as “subtyping” or “subgrouping.” Methods for evaluating the validity of subtyping have yet to be fully established. In this study, we propose the cost of cluster mean-based prediction (CCMP) as a metric for evaluating the accuracy of predictions based on subtyping. By selecting the minimum CCMP among several candidate clustering results, the optimal subtype classification in terms of prediction accuracy can be determined. The computational implementation of the CCMP is validated with numerical experiments. We also examine some properties of subtype classification selected by CCMP.},
  archive      = {J_SIM},
  author       = {Kentaro Katahira},
  doi          = {10.1002/sim.9656},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1045-1065},
  shortjournal = {Stat. Med.},
  title        = {Evaluating the predictive performance of subtyping: A criterion for cluster mean-based prediction},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized online ensemble machine learning with
applications for dynamic data streams. <em>SIM</em>, <em>42</em>(7),
1013–1044. (<a href="https://doi.org/10.1002/sim.9655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we introduce the personalized online super learner (POSL), an online personalizable ensemble machine learning algorithm for streaming data. POSL optimizes predictions with respect to baseline covariates, so personalization can vary from completely individualized, that is, optimization with respect to subject ID, to many individuals, that is, optimization with respect to common baseline covariates. As an online algorithm, POSL learns in real time. As a super learner, POSL is grounded in statistical optimality theory and can leverage a diversity of candidate algorithms, including online algorithms with different training and update times, fixed/offline algorithms that are not updated during POSL&#39;s fitting procedure, pooled algorithms that learn from many individuals&#39; time series, and individualized algorithms that learn from within a single time series. POSL&#39;s ensembling of the candidates can depend on the amount of data collected, the stationarity of the time series, and the mutual characteristics of a group of time series. Depending on the underlying data-generating process and the information available in the data, POSL is able to adapt to learning across samples, through time, or both. For a range of simulations that reflect realistic forecasting scenarios and in a medical application, we examine the performance of POSL relative to other current ensembling and online learning methods. We show that POSL is able to provide reliable predictions for both short and long time series, and it&#39;s able to adjust to changing data-generating environments. We further cultivate POSL&#39;s practicality by extending it to settings where time series dynamically enter and exit.},
  archive      = {J_SIM},
  author       = {Ivana Malenica and Rachael V. Phillips and Antoine Chambaz and Alan E. Hubbard and Romain Pirracchio and Mark J. van der Laan},
  doi          = {10.1002/sim.9655},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {1013-1044},
  shortjournal = {Stat. Med.},
  title        = {Personalized online ensemble machine learning with applications for dynamic data streams},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling the evolution of deaths from infectious diseases
with functional data models: The case of COVID-19 in brazil.
<em>SIM</em>, <em>42</em>(7), 993–1012. (<a
href="https://doi.org/10.1002/sim.9654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we apply statistical methods for functional data to explore the heterogeneity in the registered number of deaths of COVID-19, over time. The cumulative daily number of deaths in regions across Brazil is treated as continuous curves (functional data). The first stage of the analysis applies clustering methods for functional data to identify and describe potential heterogeneity in the curves and their functional derivatives. The estimated clusters are labeled with different “levels of alert” to identify cities in a possible critical situation. In the second stage of the analysis, we apply a functional quantile regression model for the death curves to explore the associations with functional rates of vaccination and stringency and also with several scalar geographical, socioeconomic and demographic covariates. The proposed model gave a better curve fit at different levels of the cumulative number of deaths when compared to a functional regression model based on ordinary least squares. Our results add to the understanding of the development of COVID-19 death counts.},
  archive      = {J_SIM},
  author       = {Julian A. A. Collazos and Ronaldo Dias and Marcelo C. Medeiros},
  doi          = {10.1002/sim.9654},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {993-1012},
  shortjournal = {Stat. Med.},
  title        = {Modeling the evolution of deaths from infectious diseases with functional data models: The case of COVID-19 in brazil},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust causal inference of drug-drug interactions.
<em>SIM</em>, <em>42</em>(7), 970–992. (<a
href="https://doi.org/10.1002/sim.9653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in developing causal inference methods for multi-valued treatments with a focus on pairwise average treatment effects. Here we focus on a clinically important, yet less-studied estimand: causal drug-drug interactions (DDIs), which quantifies the degree to which the causal effect of drug A is altered by the presence versus the absence of drug B. Confounding adjustment when studying the effects of DDIs can be accomplished via inverse probability of treatment weighting (IPTW), a standard approach originally developed for binary treatments and later generalized to multi-valued treatments. However, this approach generally results in biased results when the propensity score model is misspecified. Motivated by the need for more robust techniques, we propose two empirical likelihood-based weighting approaches that allow for specifying a set of propensity score models, with the second method balancing user-specified covariates directly, by incorporating additional, nonparametric constraints. The resulting estimators from both methods are consistent when the postulated set of propensity score models contains a correct one; this property has been termed multiple robustness. In this paper, we derive two multiply-robust estimators of the causal DDI, and develop inference procedures. We then evaluate their finite sample performance through simulation. The results demonstrate that the proposed estimators outperform the standard IPTW method in terms of both robustness and efficiency. Finally, we apply the proposed methods to evaluate the impact of renin-angiotensin system inhibitors (RAS-I) on the comparative nephrotoxicity of nonsteroidal anti-inflammatory drugs (NSAID) and opioids, using data derived from electronic medical records from a large multi-hospital health system.},
  archive      = {J_SIM},
  author       = {Di Shu and Peisong Han and Sean Hennessy and Todd A Miano},
  doi          = {10.1002/sim.9653},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {970-992},
  shortjournal = {Stat. Med.},
  title        = {Robust causal inference of drug-drug interactions},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariate-specific evaluation of continuous biomarker.
<em>SIM</em>, <em>42</em>(7), 953–969. (<a
href="https://doi.org/10.1002/sim.9652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic tests usually need to operate at a high sensitivity or specificity level in practice. Accordingly, specificity at the controlled sensitivity, or vice versa, is a clinically sensible performance metric for evaluating continuous biomarkers. Meanwhile, the performance of a biomarker may vary across sub-populations as defined by covariates, and covariate-specific evaluation can be informative. In this article, we develop a novel modeling and estimation method for covariate-specific specificity at a controlled sensitivity level. Unlike existing methods which typically adopt elaborate models of covariate effects over the entire biomarker distribution, our approach models covariate effects locally at a specific sensitivity level of interest. We also extend our proposed model to handle the whole continuum of sensitivities via dynamic regression and derive covariate-specific ROC curves. We provide the variance estimation through bootstrapping. The asymptotic properties are established. We conduct extensive simulation studies to evaluate the performance of our proposed methods in comparison with existing methods, and further illustrate the applications in two clinical studies for aggressive prostate cancer.},
  archive      = {J_SIM},
  author       = {Ziyi Li and Yijian Huang and Dattatraya Patil and Mark Rubin and Martin G. Sanda},
  doi          = {10.1002/sim.9652},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {953-969},
  shortjournal = {Stat. Med.},
  title        = {Covariate-specific evaluation of continuous biomarker},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ratio and difference of average hazard with survival weight:
New measures to quantify survival benefit of new therapy. <em>SIM</em>,
<em>42</em>(7), 936–952. (<a
href="https://doi.org/10.1002/sim.9651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hazard ratio (HR) has been the most popular measure to quantify the magnitude of treatment effect on time-to-event outcomes in clinical research. However, the traditional Cox&#39;s HR approach has several drawbacks. One major issue is that there is no clear interpretation when the proportional hazards (PH) assumption does not hold, because the estimated HR is affected by study-specific censoring time distribution in non-PH cases. Another major issue is that the lack of a group-specific absolute hazard value in each group obscures the clinical significance of the magnitude of the treatment effect. Given these, we propose average hazard with survival weight (AH-SW) as a summary metric of event time distribution and will use difference in AH-SW (DAH-SW) or ratio of AH-SW (RAH-SW) to quantify the treatment effect magnitude. The AH-SW is interpreted as a person-time incidence rate that does not depend on random censoring. It is defined as the ratio of cumulative incidence probability and restricted mean survival time (RMST), which can be estimated non-parametrically. Numerical studies demonstrate that DAH-SW and RAH-SW offer almost identical power to Cox&#39;s HR-based tests under PH scenarios and can be more powerful for delayed-difference patterns often seen in immunotherapy trials. Like median and RMST differences, the proposed approach is a good model-free alternative to the HR-based approach for evaluating the treatment effect magnitude. Such a model-free measure will increase the likelihood that results from clinical studies are correctly interpreted and generalized to future populations.},
  archive      = {J_SIM},
  author       = {Hajime Uno and Miki Horiguchi},
  doi          = {10.1002/sim.9651},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {936-952},
  shortjournal = {Stat. Med.},
  title        = {Ratio and difference of average hazard with survival weight: New measures to quantify survival benefit of new therapy},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical strategies for operationalizing optimal allocation
in stratified cluster-based outcome-dependent sampling designs.
<em>SIM</em>, <em>42</em>(7), 917–935. (<a
href="https://doi.org/10.1002/sim.9650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster-based outcome-dependent sampling (ODS) has the potential to yield efficiency gains when the outcome of interest is relatively rare, and resource constraints allow only a certain number of clusters to be visited for data collection. Previous research has shown that when the intended analysis is inverse-probability weighted generalized estimating equations, and the number of clusters that can be sampled is fixed, optimal allocation of the (cluster-level) sample size across strata defined by auxiliary variables readily available at the design stage has the potential to increase efficiency in the estimation of the parameter(s) of interest. In such a setting, the optimal allocation formulae depend on quantities that are unknown in practice, currently making such designs difficult to implement. In this paper, we consider a two-wave adaptive sampling approach, in which data is collected from a first wave sample, and subsequently used to compute the optimal second wave stratum-specific sample sizes. We consider two strategies for estimating the necessary components using the first wave data: an inverse-probability weighting (IPW) approach and a multiple imputation (MI) approach. In a comprehensive simulation study, we show that the adaptive sampling approach performs well, and that the MI approach yields designs that are very near-optimal, regardless of the covariate type. The IPW approach, on the other hand, has mixed results. Finally, we illustrate the proposed adaptive sampling procedures with data on maternal characteristics and birth outcomes among women enrolled in the Safer Deliveries program in Zanzibar, Tanzania.},
  archive      = {J_SIM},
  author       = {Sara Sauer and Bethany Hedt-Gauthier and Sebastien Haneuse},
  doi          = {10.1002/sim.9650},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {7},
  pages        = {917-935},
  shortjournal = {Stat. Med.},
  title        = {Practical strategies for operationalizing optimal allocation in stratified cluster-based outcome-dependent sampling designs},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fitting additive risk models using auxiliary information.
<em>SIM</em>, <em>42</em>(6), 894–916. (<a
href="https://doi.org/10.1002/sim.9649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a growing interest in incorporating auxiliary summary information from external studies into the analysis of internal individual-level data. In this paper, we propose an adaptive estimation procedure for an additive risk model to integrate auxiliary subgroup survival information via a penalized method of moments technique. Our approach can accommodate information from heterogeneous data. Parameters to quantify the magnitude of potential incomparability between internal data and external auxiliary information are introduced in our framework while nonzero components of these parameters suggest a violation of the homogeneity assumption. We further develop an efficient computational algorithm to solve the numerical optimization problem by profiling out the nuisance parameters. In an asymptotic sense, our method can be as efficient as if all the incomparable auxiliary information is accurately acknowledged and has been automatically excluded from consideration. The asymptotic normality of the proposed estimator of the regression coefficients is established, with an explicit formula for the asymptotic variance-covariance matrix that can be consistently estimated from the data. Simulation studies show that the proposed method yields a substantial gain in statistical efficiency over the conventional method using the internal data only, and reduces estimation biases when the given auxiliary survival information is incomparable. We illustrate the proposed method with a lung cancer survival study.},
  archive      = {J_SIM},
  author       = {Jie Ding and Jialiang Li and Yang Han and Ian W. McKeague and Xiaoguang Wang},
  doi          = {10.1002/sim.9649},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {894-916},
  shortjournal = {Stat. Med.},
  title        = {Fitting additive risk models using auxiliary information},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individualized net benefit estimation and meta-analysis
using generalized pairwise comparisons in n-of-1 trials. <em>SIM</em>,
<em>42</em>(6), 878–893. (<a
href="https://doi.org/10.1002/sim.9648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Joris Giai and Julien Péron and Matthieu Roustit and Jean-Luc Cracowski and Pascal Roy and Brice Ozenne and Marc Buyse and Delphine Maucort-Boulch},
  doi          = {10.1002/sim.9648},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {878-893},
  shortjournal = {Stat. Med.},
  title        = {Individualized net benefit estimation and meta-analysis using generalized pairwise comparisons in N-of-1 trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Regression analysis of logistic model with latent variables.
<em>SIM</em>, <em>42</em>(6), 860–877. (<a
href="https://doi.org/10.1002/sim.9647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a joint modeling approach to investigating the effects of social-psychological factors on the onset of depression. The proposed model comprises two components. The first one is a confirmatory factor analysis model that summarizes latent factors through multiple correlated observed variables. The second one is a logistic regression model that investigates the effects of observed and latent influence factors on the occurrence of depression. We develop a hybrid procedure based on the borrow-strength estimation procedure and the weighted score function to estimate the model parameters. The asymptotic properties of the proposed estimators are established. Simulation studies demonstrate that the method we proposed performs well. An application to a study concerning the social-psychological factors of depression is provided.},
  archive      = {J_SIM},
  author       = {Yuan Ye and Zhongchun Liu and Deng Pan and Yuanshan Wu},
  doi          = {10.1002/sim.9647},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {860-877},
  shortjournal = {Stat. Med.},
  title        = {Regression analysis of logistic model with latent variables},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A network approach to compute hypervolume under receiver
operating characteristic manifold for multi-class biomarkers.
<em>SIM</em>, <em>42</em>(6), 834–859. (<a
href="https://doi.org/10.1002/sim.9646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation of hypervolume under ROC manifold (HUM) is necessary to evaluate biomarkers for their capability to discriminate among multiple disease types or diagnostic groups. However the original definition of HUM involves multiple integration and thus a medical investigation for multi-class receiver operating characteristic (ROC) analysis could suffer from huge computational cost when the formula is implemented naively. We introduce a novel graph-based approach to compute HUM efficiently in this article. The computational method avoids the time-consuming multiple summation when sample size or the number of categories is large. We conduct extensive simulation studies to demonstrate the improvement of our method over existing R packages. We apply our method to two real biomedical data sets to illustrate its application.},
  archive      = {J_SIM},
  author       = {Qunqiang Feng and Pan Liu and Pei-Fen Kuan and Fei Zou and Jianan Chen and Jialiang Li},
  doi          = {10.1002/sim.9646},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {834-859},
  shortjournal = {Stat. Med.},
  title        = {A network approach to compute hypervolume under receiver operating characteristic manifold for multi-class biomarkers},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cox models with time-varying covariates and partly-interval
censoring–a maximum penalised likelihood approach. <em>SIM</em>,
<em>42</em>(6), 815–833. (<a
href="https://doi.org/10.1002/sim.9645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying covariates can be important predictors when model based predictions are considered. A Cox model that includes time-varying covariates is usually referred to as an extended Cox model. When only right censoring is presented in the observed survival times, the conventional partial likelihood method is still applicable to estimate the regression coefficients of an extended Cox model. However, if there are interval-censored survival times, then the partial likelihood method is not directly available unless an imputation, such as the middle point imputation, is used to replaced the left- and interval-censored data. However, such imputation methods are well known for causing biases. This paper considers fitting of the extended Cox models using the maximum penalised likelihood method allowing observed survival times to be partly interval censored, where a penalty function is used to regularise the baseline hazard estimate. We present simulation studies to demonstrate the performance of our proposed method, and illustrate our method with applications to two real datasets from medical research.},
  archive      = {J_SIM},
  author       = {Annabel Webb and Jun Ma},
  doi          = {10.1002/sim.9645},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {815-833},
  shortjournal = {Stat. Med.},
  title        = {Cox models with time-varying covariates and partly-interval censoring–A maximum penalised likelihood approach},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Doubly robust estimation of the hazard difference for
competing risks data. <em>SIM</em>, <em>42</em>(6), 799–814. (<a
href="https://doi.org/10.1002/sim.9644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the conditional treatment effect for competing risks data in observational studies. We derive the efficient score for the treatment effect using modern semiparametric theory, as well as two doubly robust scores with respect to (1) the assumed propensity score for treatment and the censoring model, and (2) the outcome models for the competing risks. An important property regarding the estimators is rate double robustness, in addition to the classical model double robustness. Rate double robustness enables the use of machine learning and nonparametric methods in order to estimate the nuisance parameters, while preserving the root- asymptotic normality of the estimated treatment effect for inferential purposes. We study the performance of the estimators using simulation. The estimators are applied to the data from a cohort of Japanese men in Hawaii followed since 1960s in order to study the effect of mid-life drinking behavior on late life cognitive outcomes. The approaches developed in this article are implemented in the R package “HazardDiff”.},
  archive      = {J_SIM},
  author       = {Denise Rava and Ronghui Xu},
  doi          = {10.1002/sim.9644},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {799-814},
  shortjournal = {Stat. Med.},
  title        = {Doubly robust estimation of the hazard difference for competing risks data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A likelihood-based sensitivity analysis for publication bias
on the summary receiver operating characteristic in meta-analysis of
diagnostic test accuracy. <em>SIM</em>, <em>42</em>(6), 781–798. (<a
href="https://doi.org/10.1002/sim.9643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In meta-analysis of diagnostic test accuracy, the summary receiver operating characteristic (SROC) curve is a recommended method to summarize the diagnostic capacity of a medical test in the presence of study-specific cutoff values. The SROC curve can be estimated by bivariate modeling of pairs of sensitivity and specificity across multiple diagnostic studies, and the area under the SROC curve (SAUC) gives the aggregate estimate of diagnostic test accuracy. However, publication bias is a major threat to the validity of the estimates. To make inference of the impact of publication bias on the SROC curve or the SAUC, we propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis of Copas. In the proposed method, the SROC curve or the SAUC are estimated by maximizing the likelihood constrained by different values of the marginal probability of selective publication under different mechanisms of selective publication. A cutoff-dependent selection function is developed to model the selective publication mechanism via the -type statistics or -value of the linear combination of the logit-transformed sensitivity and specificity from the published studies. It allows us to model selective publication suggested by the funnel plots of sensitivity, specificity, or diagnostic odds ratio, which are often observed in practice. A real meta-analysis of diagnostic test accuracy is re-analyzed to illustrate the proposed method, and simulation studies are conducted to evaluate its performance.},
  archive      = {J_SIM},
  author       = {Yi Zhou and Ao Huang and Satoshi Hattori},
  doi          = {10.1002/sim.9643},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {781-798},
  shortjournal = {Stat. Med.},
  title        = {A likelihood-based sensitivity analysis for publication bias on the summary receiver operating characteristic in meta-analysis of diagnostic test accuracy},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable importance evaluation with personalized odds ratio
for machine learning model interpretability with applications to
electronic health records-based mortality prediction. <em>SIM</em>,
<em>42</em>(6), 761–780. (<a
href="https://doi.org/10.1002/sim.9642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretability of machine learning models, even though with an excellent prediction performance, remains a challenge in practical applications. The model interpretability and variable importance for well-performed supervised machine learning models are investigated in this study. With the commonly accepted concept of odds ratio (OR), we propose a novel and computationally efficient Variable Importance evaluation framework based on the Personalized Odds Ratio (VIPOR). It is a model-agnostic interpretation method that can be used to evaluate variable importance both locally and globally. Locally, the variable importance is quantified by the personalized odds ratio (POR), which can account for subject heterogeneity in machine learning. Globally, we utilize a hierarchical tree to group the predictors into five groups: completely positive, completely negative, positive dominated, negative dominated, and neutral groups. The relative importance of predictors within each group is ranked based on different statistics of PORs across subjects for different application purposes. For illustration, we apply the proposed VIPOR method to interpreting a multilayer perceptron (MLP) model, which aims to predict the mortality of subarachnoid hemorrhage (SAH) patients using real-world electronic health records (EHR) data. We compare the important variables derived from MLP with other machine learning models, including tree-based models and the L1-regularized logistic regression model. The top importance variables are consistently identified by VIPOR across different prediction models. Comparisons with existing interpretation methods are also conducted and discussed based on publicly available data sets.},
  archive      = {J_SIM},
  author       = {Duo Yu and Hulin Wu},
  doi          = {10.1002/sim.9642},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {761-780},
  shortjournal = {Stat. Med.},
  title        = {Variable importance evaluation with personalized odds ratio for machine learning model interpretability with applications to electronic health records-based mortality prediction},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing dental fluorosis data using a novel bayesian model
for clustered longitudinal ordinal outcomes with an inflated category.
<em>SIM</em>, <em>42</em>(6), 745–760. (<a
href="https://doi.org/10.1002/sim.9641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian hurdle mixed-effects model to analyze longitudinal ordinal data under a complex multilevel structure. This research was motivated by the dataset gathered from the Iowa Fluoride Study (IFS) in order to establish the relationships between fluorosis status and potential risk/protective factors. Dental fluorosis is characterized by spots on tooth enamel and is due to ingestion of excessive fluoride intake during enamel formation. Observations are collected from multiple surface zones on each tooth and on all available teeth of children from the studied cohort, which are longitudinally observed at ages 9, 13, and 17. The data not only exhibit a complex hierarchical structure, but also have a large proportion of zero values that are likely to follow different statistical patterns from non-zero categories. Therefore, we develop a hurdle model to consider the zero category separately, while a proportional odds model is used for the positive categories. The estimated parameters are obtained from a Gibbs sampler implemented by the OpenBUGS software. Our model is compared with two popular methods for ordinal data: the proportional odds model and the partial proportional odds model. We perform a comprehensive analysis of the IFS data and evaluate the accuracy and effectiveness of our methodology through simulation studies. Our discoveries provide novel insights to statisticians and dental practitioners about the associations between patient and clinical characteristics and dental fluorosis.},
  archive      = {J_SIM},
  author       = {Tong Kang and Jeremy Gaskins and Steven Levy and Somnath Datta},
  doi          = {10.1002/sim.9641},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {745-760},
  shortjournal = {Stat. Med.},
  title        = {Analyzing dental fluorosis data using a novel bayesian model for clustered longitudinal ordinal outcomes with an inflated category},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group testing regression analysis with covariates and
specimens subject to missingness. <em>SIM</em>, <em>42</em>(6), 731–744.
(<a href="https://doi.org/10.1002/sim.9640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop parametric estimators of a conditional prevalence in the group testing context. Group testing is applied when a binary outcome variable, often a disease indicator, is assessed by testing a specimen for the presence of the disease. Instead of testing all individual specimens separately, these are pooled in groups and the grouped specimens are tested for the disease, which permits to significantly reduce the number of tests to be performed. Various techniques have been developed in the literature for estimating a conditional prevalence from group testing data, but most of them are not valid when the data are subject to missingness. We consider this problem in the case where the specimen and the covariates are subject to nonmonotone missingness. We propose parametric estimators of the conditional prevalence, establish identifiability conditions for a logistic missing not at random model, and introduce an ignorable missing at random model. In theory, our estimators could be applied with multiple covariates missing, but in practice, they face numerical challenges when more than one covariate is missing for given individuals. We illustrate the method on simulated data and on a dataset from the Demographics and Health Survey.},
  archive      = {J_SIM},
  author       = {Aurore Delaigle and Ruoxu Tan},
  doi          = {10.1002/sim.9640},
  journal      = {Statistics in Medicine},
  month        = {3},
  number       = {6},
  pages        = {731-744},
  shortjournal = {Stat. Med.},
  title        = {Group testing regression analysis with covariates and specimens subject to missingness},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum to: Fusion designs and estimators for treatment
effects. <em>SIM</em>, <em>42</em>(5), 730. (<a
href="https://doi.org/10.1002/sim.9634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Alexander Breskin and Stephen R. Cole and Jessie K. Edwards and Ron Brookmeyer and Joseph J. Eron and Adimora A. Adimora},
  doi          = {10.1002/sim.9634},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {730},
  shortjournal = {Stat. Med.},
  title        = {Corrigendum to: Fusion designs and estimators for treatment effects},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coupling an individual adaptive-decision model with a SIRV
model of influenza vaccination reveals new insights for epidemic
control. <em>SIM</em>, <em>42</em>(5), 716–729. (<a
href="https://doi.org/10.1002/sim.9639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Past seasonal influenza epidemics and vaccination experience may affect individuals&#39; decisions on whether to be vaccinated or not, decisions that may be constantly reassessed in relation to recent influenza related experience. To understand the potentially complex interaction between experience and decisions and whether the vaccination rate is likely to reach a critical coverage level or not, we construct an adaptive-decision model. This model is then coupled with an influenza vaccination dynamics (SIRV) model to explore the interaction between individuals&#39; decision-making and an influenza epidemic. Nonlinear least squares estimation is used to obtain the best-fit parameter values in the SIRV model based on data on new influenza-like illness (ILI) cases in Texas. Uncertainty and sensitivity analyses are then carried out to determine the impact of key parameters of the adaptive decision-making model on the ILI epidemic. The results showed that the necessary critical coverage rate of ILI vaccination could not be reached by voluntary vaccination. However, it could be reached in the fourth year if mass media reports improved individuals&#39; memory of past vaccination experience. Individuals&#39; memory of past vaccination experience, the proportion with histories of past vaccinations and the perceived cost of vaccination are important factors determining whether an ILI epidemic can be effectively controlled or not. Therefore, health authorities should guide people to improve their memory of past vaccination experience through media reports, publish timely data on annual vaccination proportions and adjust relevant measures to appropriately reduce vaccination perceived cost, in order to effectively control an ILI epidemic.},
  archive      = {J_SIM},
  author       = {Qinling Yan and Robert A. Cheke and Sanyi Tang},
  doi          = {10.1002/sim.9639},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {716-729},
  shortjournal = {Stat. Med.},
  title        = {Coupling an individual adaptive-decision model with a SIRV model of influenza vaccination reveals new insights for epidemic control},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation and visualization of heterogeneous treatment
effects for multiple outcomes. <em>SIM</em>, <em>42</em>(5), 693–715.
(<a href="https://doi.org/10.1002/sim.9638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider two-arm comparison in clinical trials. The objective is to identify a population with characteristics that make the treatment effective. Such a population is called a subgroup. This identification can be made by estimating the treatment effect and identifying the interactions between treatments and covariates. For a single outcome, there are several ways available to identify the subgroups. There are also multiple outcomes, but they are difficult to interpret and cannot be applied to outcomes other than continuous values. In this paper, we thus propose a new method that allows for a straightforward interpretation of subgroups and deals with both continuous and binary outcomes. The proposed method introduces latent variables and adds Lasso sparsity constraints to the estimated loadings to facilitate the interpretation of the relationship between outcomes and covariates. The interpretation of the subgroups is made by visualizing treatment effects and latent variables. Since we are performing sparse estimation, we can interpret the covariates related to the treatment effects and subgroups. Finally, simulation and real data examples demonstrate the effectiveness of the proposed method.},
  archive      = {J_SIM},
  author       = {Shintaro Yuki and Kensuke Tanioka and Hiroshi Yadohisa},
  doi          = {10.1002/sim.9638},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {693-715},
  shortjournal = {Stat. Med.},
  title        = {Estimation and visualization of heterogeneous treatment effects for multiple outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tree-based modeling approach for matched case-control
studies. <em>SIM</em>, <em>42</em>(5), 676–692. (<a
href="https://doi.org/10.1002/sim.9637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional logistic regression (CLR) is the indisputable standard method for the analysis of matched case-control studies. However, CLR is strongly restricted with respect to the inclusion of non-linear effects and interactions of confounding variables. A novel tree-based modeling method is proposed which accounts for this issue and provides a flexible framework allowing for a more complex confounding structure. The proposed machine learning model is fitted within the framework of CLR and, therefore, allows to account for the matched strata in the data. A simulation study demonstrates the efficacy of the method. Furthermore, for illustration the method is applied to a matched case-control study on cervical cancer.},
  archive      = {J_SIM},
  author       = {Gunther Schauberger and Luana Fiengo Tanaka and Moritz Berger},
  doi          = {10.1002/sim.9637},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {676-692},
  shortjournal = {Stat. Med.},
  title        = {A tree-based modeling approach for matched case-control studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel robust estimation for high-dimensional precision
matrices. <em>SIM</em>, <em>42</em>(5), 656–675. (<a
href="https://doi.org/10.1002/sim.9636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new robust estimation of precision matrices for high-dimensional data when the number of variables is larger than the sample size. Different from the existing methods in literature, the proposed model combines the technique of modified Cholesky decomposition (MCD) with the robust generalized M-estimators. The MCD reparameterizes a precision matrix and transforms its estimation into solving a series of linear regressions, in which the commonly used robust techniques can be conveniently incorporated. Additionally, the proposed method adopts the model averaging idea to address the ordering issue in the MCD approach, resulting in an accurate estimation for precision matrices. Simulations and real data analysis are conducted to illustrate the merits of the proposed estimator.},
  archive      = {J_SIM},
  author       = {Shaoxin Wang and Chaoping Xie and Xiaoning Kang},
  doi          = {10.1002/sim.9636},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {656-675},
  shortjournal = {Stat. Med.},
  title        = {A novel robust estimation for high-dimensional precision matrices},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling longitudinal change in biomarkers using data from a
complex survey sampling design: An application to the hispanic community
health study/study of latinos. <em>SIM</em>, <em>42</em>(5), 632–655.
(<a href="https://doi.org/10.1002/sim.9635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational cohort studies, there is frequently interest in modeling longitudinal change in a biomarker (ie, physiological measure indicative of metabolic dysregulation or disease; eg, blood pressure) in the absence of treatment (ie, medication), and its association with modifiable risk factors expected to affect health (eg, body mass index). However, individuals may start treatment during the study period, and consequently biomarker values observed while on treatment may be different than those that would have been observed in the absence of treatment. If treated individuals are excluded from analysis, then effect estimates may be biased if treated individuals differ systematically from untreated individuals. We addressed this concern in the setting of the Hispanic Community Health Study/Study of Latinos (HCHS/SOL), an observational cohort study that employed a complex survey sampling design to enable inference to a finite target population. We considered biomarker values measured while on treatment to be missing data, and applied missing data methodology (inverse probability weighting (IPW) and doubly robust estimation) to this problem. The proposed methods leverage information collected between study visits on when individuals started treatment, by adapting IPW and doubly robust approaches to model the treatment mechanism using survival analysis methods. This methodology also incorporates sampling weights and uses a bootstrap approach to estimate standard errors accounting for the complex survey sampling design. We investigated variance estimation for these methods, conducted simulation studies to assess statistical performance in finite samples, and applied the methodology to model temporal change in blood pressure in HCHS/SOL.},
  archive      = {J_SIM},
  author       = {Nicole M. Butera and Donglin Zeng and Gerardo Heiss and Jianwen Cai},
  doi          = {10.1002/sim.9635},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {632-655},
  shortjournal = {Stat. Med.},
  title        = {Modeling longitudinal change in biomarkers using data from a complex survey sampling design: An application to the hispanic community health Study/Study of latinos},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjusting for both sequential testing and systematic error
in safety surveillance using observational data: Empirical calibration
and MaxSPRT. <em>SIM</em>, <em>42</em>(5), 619–631. (<a
href="https://doi.org/10.1002/sim.9631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-approval safety surveillance of medical products using observational healthcare data can help identify safety issues beyond those found in pre-approval trials. When testing sequentially as data accrue, maximum sequential probability ratio testing (MaxSPRT) is a common approach to maintaining nominal type 1 error. However, the true type 1 error may still deviate from the specified one because of systematic error due to the observational nature of the analysis. This systematic error may persist even after controlling for known confounders. Here we propose to address this issue by combing MaxSPRT with empirical calibration. In empirical calibration, we assume uncertainty about the systematic error in our analysis, the source of uncertainty commonly overlooked in practice. We infer a probability distribution of systematic error by relying on a large set of negative controls: exposure-outcome pairs where no causal effect is believed to exist. Integrating this distribution into our test statistics has previously been shown to restore type 1 error to nominal. Here we show how we can calibrate the critical value central to MaxSPRT. We evaluate this novel approach using simulations and real electronic health records, using H1N1 vaccinations during the 2009–2010 season as an example. Results show that combining empirical calibration with MaxSPRT restores nominal type 1 error. In our real-world example, adjusting for systematic error using empirical calibration has a larger impact than, and hence is just as essential as, adjusting for sequential testing using MaxSPRT. We recommend performing both, using the method described here.},
  archive      = {J_SIM},
  author       = {Martijn J. Schuemie and Fan Bu and Akihiko Nishimura and Marc A. Suchard},
  doi          = {10.1002/sim.9631},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {619-631},
  shortjournal = {Stat. Med.},
  title        = {Adjusting for both sequential testing and systematic error in safety surveillance using observational data: Empirical calibration and MaxSPRT},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Marginal structural models with monotonicity constraints: A
case study in out-of-hospital cardiac arrest patients. <em>SIM</em>,
<em>42</em>(5), 603–618. (<a
href="https://doi.org/10.1002/sim.9612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with estimating the probability of a binary counterfactual outcome as a function of a continuous covariate under monotonicity constraints. We are motivated by the study of out-of-hospital cardiac arrest patients which aims to estimate the counterfactual 30-day survival probability if either all patients had received, or if none of the patients had received bystander cardiopulmonary resuscitation (CPR), as a function of the ambulance response time. It is natural to assume that the counterfactual 30-day survival probability cannot increase with increasing ambulance response time. We model the monotone relationship with a marginal structural model and B-splines. We then derive an estimating equation for the parameters of interest which however further relies on an auxiliary regression model for the observed 30-day survival probabilities. The predictions of the observed 30-day survival probabilities are used as pseudo-values for the unobserved counterfactual 30-day survival status. The methods are illustrated and contrasted with an unconstrained modeling approach in large-scale Danish registry data.},
  archive      = {J_SIM},
  author       = {Liis Starkopf and Shahzleen Rajan and Theis Lange and Thomas Alexander Gerds},
  doi          = {10.1002/sim.9612},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {603-618},
  shortjournal = {Stat. Med.},
  title        = {Marginal structural models with monotonicity constraints: A case study in out-of-hospital cardiac arrest patients},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tony johnson—memories and a tribute. <em>SIM</em>,
<em>42</em>(5), 600–602. (<a
href="https://doi.org/10.1002/sim.9616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Laurence S. Freedman and Theodore Colton},
  doi          = {10.1002/sim.9616},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {600-602},
  shortjournal = {Stat. Med.},
  title        = {Tony johnson—memories and a tribute},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obituary: Anthony leonard johnson (1943-2022). <em>SIM</em>,
<em>42</em>(5), 597–599. (<a
href="https://doi.org/10.1002/sim.9614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SIM},
  author       = {Vern Farewell},
  doi          = {10.1002/sim.9614},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {5},
  pages        = {597-599},
  shortjournal = {Stat. Med.},
  title        = {Obituary: Anthony leonard johnson (1943-2022)},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint regression modelling of intensity and timing of
accelerometer counts. <em>SIM</em>, <em>42</em>(4), 579–595. (<a
href="https://doi.org/10.1002/sim.9633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerometers are commonly used in human medical and public health research to measure physical movement, which is relevant in a wide range of studies, from physical activity and sleep behaviours studies, to identification of movement patterns in people affected by diseases of the locomotor system and prediction of risk of injury in high performance sports. The accelerometer output provides the intensity (activity count) and timing (timestamp) of the movement, which can be used to define bouts of activity (periods of sustained movement of a given intensity). In some contexts, it may be important to include both dimensions to obtain a broader and deeper understanding of the phenomenon under study. Such is the case of a large-scale epidemiological investigation on the daily and weekly physical activity behaviours of school-aged children enrolled in the UK Millennium Cohort Study, which has motivated the present article. I present a statistical approach to joint modelling of intensity and timing of activity bouts that takes advantage of the circular nature of the timing. The model, which accounts for the longitudinal structure of the observations, is remarkably simple to implement using standard statistical software.},
  archive      = {J_SIM},
  author       = {Marco Geraci},
  doi          = {10.1002/sim.9633},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {579-595},
  shortjournal = {Stat. Med.},
  title        = {Joint regression modelling of intensity and timing of accelerometer counts},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Power analyses for stepped wedge designs with multivariate
continuous outcomes. <em>SIM</em>, <em>42</em>(4), 559–578. (<a
href="https://doi.org/10.1002/sim.9632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate outcomes are common in pragmatic cluster randomized trials. While sample size calculation procedures for multivariate outcomes exist under parallel assignment, none have been developed for a stepped wedge design. In this article, we present computationally efficient power and sample size procedures for stepped wedge cluster randomized trials (SW-CRTs) with multivariate outcomes that differentiate the within-period and between-period intracluster correlation coefficients (ICCs). Under a multivariate linear mixed model, we derive the joint distribution of the intervention test statistics which can be used for determining power under different hypotheses and provide an example using the commonly utilized intersection-union test for co-primary outcomes. Simplifications under a common treatment effect and common ICCs across endpoints and an extension to closed-cohort designs are also provided. Finally, under the common ICC across endpoints assumption, we formally prove that the multivariate linear mixed model leads to a more efficient treatment effect estimator compared to the univariate linear mixed model, providing a rigorous justification on the use of the former with multivariate outcomes. We illustrate application of the proposed methods using data from an existing SW-CRT and present extensive simulations to validate the methods.},
  archive      = {J_SIM},
  author       = {Kendra Davis-Plourde and Monica Taljaard and Fan Li},
  doi          = {10.1002/sim.9632},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {559-578},
  shortjournal = {Stat. Med.},
  title        = {Power analyses for stepped wedge designs with multivariate continuous outcomes},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization of the two-stage group sequential three-arm
gold-standard design for non-inferiority trials. <em>SIM</em>,
<em>42</em>(4), 536–558. (<a
href="https://doi.org/10.1002/sim.9630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If design parameters are chosen appropriately, group sequential trial designs are known to be able to reduce the expected sample size under the alternative hypothesis compared to single-stage designs. The same holds true for the so-called ‘gold-standard’ design for non-inferiority trials, a design involving an experimental group, an active control group, and a placebo group. However, choosing design parameters that maximize the advantages of a two-stage approach for the three-arm gold-standard design for non-inferiority trials is not a straightforward task. In particular, optimal choices of futility boundaries for this design have not been thoroughly discussed in existing literature. We present a variation of the hierarchical testing procedure, which allows for the incorporation of binding futility boundaries at interim analyses. We show that this procedure maintains strong control of the family-wise type I error rate. Within this framework, we consider the futility and efficacy boundaries as well as the sample size allocation ratios as optimization parameters. This allows the investigation of the efficiency gain from including the option to stop for futility in addition to the ability to stop for efficacy. To analyze the extended designs, optimality criteria that include the design&#39;s performance under the alternative as well as the null hypothesis are introduced. On top of this, we discuss methods to limit the allocation of placebo patients in the trial while maintaining relatively good operating characteristics. The results of our numerical optimization procedure are discussed and a comparison of different approaches to designing a three-arm gold-standard non-inferiority trial is provided.},
  archive      = {J_SIM},
  author       = {Jan Meis and Maximilian Pilz and Carolin Herrmann and Björn Bokelmann and Geraldine Rauch and Meinhard Kieser},
  doi          = {10.1002/sim.9630},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {536-558},
  shortjournal = {Stat. Med.},
  title        = {Optimization of the two-stage group sequential three-arm gold-standard design for non-inferiority trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Consistent inverse probability of treatment weighted
estimation of the average treatment effect with mismeasured
time-dependent confounders. <em>SIM</em>, <em>42</em>(4), 517–535. (<a
href="https://doi.org/10.1002/sim.9629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal studies, the inverse probability of treatment weighted (IPTW) method is commonly employed to estimate the effect of time-dependent treatments on an outcome of interest. However, it has been documented that when the confounders are subject to measurement error, the naive IPTW method which simply ignores measurement error leads to biased treatment effect estimation. In the existing literature, there is a lack of measurement error correction methods that fully remove measurement error effect and produce consistent treatment effect estimation. In this article, we develop a novel consistent IPTW estimation procedure for longitudinal studies. The key step of the proposed method is to use the observed data to construct a corrected function that is unbiased of the unknown IPTW function. Simulation studies reveal that the proposed method outperforms the existing consistent and approximate measurement error correction methods for IPTW estimation of the average treatment effect. Finally, we apply the proposed method to analyze a real dataset.},
  archive      = {J_SIM},
  author       = {Ying Yan and Mingchen Ren},
  doi          = {10.1002/sim.9629},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {517-535},
  shortjournal = {Stat. Med.},
  title        = {Consistent inverse probability of treatment weighted estimation of the average treatment effect with mismeasured time-dependent confounders},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dealing with confounding in observational studies: A scoping
review of methods evaluated in simulation studies with single-point
exposure. <em>SIM</em>, <em>42</em>(4), 487–516. (<a
href="https://doi.org/10.1002/sim.9628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article was to perform a scoping review of methods available for dealing with confounding when analyzing the effect of health care treatments with single-point exposure in observational data. We aim to provide an overview of methods and their performance assessed by simulation studies indexed in PubMed. We searched PubMed for simulation studies published until January 2021. Our search was restricted to studies evaluating binary treatments and binary and/or continuous outcomes. Information was extracted on the methods&#39; assumptions, performance, and technical properties. Of 28,548 identified references, 127 studies were eligible for inclusion. Of them, 84 assessed 14 different methods (ie, groups of estimators that share assumptions and implementation) for dealing with measured confounding, and 43 assessed 10 different methods for dealing with unmeasured confounding. Results suggest that there are large differences in performance between methods and that the performance of a specific method is highly dependent on the estimator. Furthermore, the methods&#39; assumptions regarding the specific data features also substantially influence the methods&#39; performance. Finally, the methods result in different estimands (ie, target of inference), which can even vary within methods. In conclusion, when choosing a method to adjust for measured or unmeasured confounding it is important to choose the most appropriate estimand, while considering the population of interest, data structure, and whether the plausibility of the methods&#39; required assumptions hold.},
  archive      = {J_SIM},
  author       = {Anita Natalia Varga and Alejandra Elizabeth Guevara Morel and Joran Lokkerbol and Johanna Maria van Dongen and Maurits Willem van Tulder and Judith Ekkina Bosmans},
  doi          = {10.1002/sim.9628},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {487-516},
  shortjournal = {Stat. Med.},
  title        = {Dealing with confounding in observational studies: A scoping review of methods evaluated in simulation studies with single-point exposure},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Refined moderation analysis with categorical outcomes in
precision medicine. <em>SIM</em>, <em>42</em>(4), 470–486. (<a
href="https://doi.org/10.1002/sim.9627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moderation analysis is an integral part of precision medicine research. Concerning moderation analysis with categorical outcomes, we start with an interesting observation, which shows that heterogeneous treatment effects could be equivalently estimated via a role exchange between the outcome and the treatment variable in logistic regression models. Hence two estimators of moderating effects can be obtained. We then established the joint asymptotic normality for the two estimators, on which basis refined inference can be made for moderation analysis. The improved precision is helpful in addressing the lack-of-power problem that is common in search of moderators. The above-mentioned results hold for both experimental and observational data. We investigate the proposed method by simulation and provide an illustration with data from a randomized trial on wart treatment.},
  archive      = {J_SIM},
  author       = {Xiaogang Su and Youngjoo Cho and Liqiang Ni and Lei Liu and Elise Dusseldorp},
  doi          = {10.1002/sim.9627},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {470-486},
  shortjournal = {Stat. Med.},
  title        = {Refined moderation analysis with categorical outcomes in precision medicine},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjusted closed-form confidence interval formulas for
network meta-analysis with a small number of studies. <em>SIM</em>,
<em>42</em>(4), 457–469. (<a
href="https://doi.org/10.1002/sim.9626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive simple formulas for closed-form confidence intervals for the Wald statistic, likelihood ratio statistic, and score statistic for network meta-analysis (NMA). Additionally, we consider resolutions of concerns that network meta-analyzes with a small number of studies cannot maintain a nominal confidence level. For bias adjustment in analyzes with a small number of studies, the Bartlett-type adjustment is a well-known method. Many Bartlett-type adjustment-type methods are based on maximum likelihood estimators (MLEs). However, NMA often uses restricted MLEs that have not been extensively discussed with respect to the Bartlett-type adjustment. In this article, we propose a Bartlett-type adjustment method for the Wald statistic, likelihood ratio statistic, and score statistic when nuisance parameters are estimated by not only the maximum likelihood method but also the restricted maximum likelihood method. We can compute closed-form confidence intervals adjusted using the Bartlett-type adjustment immediately without any numerical calculations (eg, bootstrap method). Additionally, we propose a higher-order adjustment by applying the bootstrap method to Bartlett-type adjusted statistics. Using a computer simulation, we confirmed that the adjusted confidence intervals maintained a nominal confidence level. Additionally, we confirmed that the confidence intervals of the Wald statistic, likelihood ratio statistic, and score statistic based on the restricted maximum likelihood method performed well without further bootstrap adjustment and the performances of the three adjusted confidence intervals were comparable. Finally, we demonstrated that confidence intervals were adjusted for actual NMA. In the actual NMA, the adjusted confidence intervals of the Wald statistic were wider, the adjusted confidence intervals of the likelihood ratio statistic were also wider, and the adjusted confidence intervals of the score statistic were narrower. We recommend using the likelihood ratio test statistic with the restricted maximum likelihood estimator; however, just in case, we recommend applying the Bartlett-type adjustment to remove the second order bias. From demonstrations in actual studies, we confirmed that the adjusted confidence intervals improved compared with the naive confidence intervals.},
  archive      = {J_SIM},
  author       = {Masahiro Kojima},
  doi          = {10.1002/sim.9626},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {457-469},
  shortjournal = {Stat. Med.},
  title        = {Adjusted closed-form confidence interval formulas for network meta-analysis with a small number of studies},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised clustering of quaternion time series:
Application to gait analysis in multiple sclerosis using motion sensor
data. <em>SIM</em>, <em>42</em>(4), 433–456. (<a
href="https://doi.org/10.1002/sim.9625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent approaches in gait analysis involve the use of wearable motion sensors to extract spatio-temporal parameters that characterize multiple aspects of an individual&#39;s gait. In particular, the medical community could largely benefit from this type of devices as they could provide the clinicians with a valuable tool for assessing gait impairment. Motion sensor data are however complex and there is an urgent unmet need to develop sound statistical methods for analyzing such data and extracting clinically relevant information. In this article, we measure gait by following the hip rotation over time and the resulting statistical unit is a time series of unit quaternions. We explore the possibility to form groups of patients with similar walking impairment by taking into account their walking data and their global decease severity with semi-supervised clustering. We generalize a compromise-based method named hclustcompro to unit quaternion time series by combining it with the proper dissimilarity quaternion dynamic time warping. We apply this method on patients diagnosed with multiple sclerosis to form groups of patients with similar walking deficiencies while accounting for the clinical assessment of their overall disability. We also compare the compromise-based clustering approach with the method mergeTrees that falls into a sub-class of ensemble clustering named collaborative clustering. The results provide a first proof of both the interest of using wearable motion sensors for assessing gait impairment and the use of prior knowledge to guide the clustering process. It also demonstrates that compromise-based clustering is a more appropriate approach in this context.},
  archive      = {J_SIM},
  author       = {Pierre Drouin and Aymeric Stamm and Laurent Chevreuil and Vincent Graillot and Laetitia Barbin and Pierre-Antoine Gourraud and David-Axel Laplaud and Lise Bellanger},
  doi          = {10.1002/sim.9625},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {433-456},
  shortjournal = {Stat. Med.},
  title        = {Semi-supervised clustering of quaternion time series: Application to gait analysis in multiple sclerosis using motion sensor data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mediation analysis with multiple mediators under unmeasured
mediator-outcome confounding. <em>SIM</em>, <em>42</em>(4), 422–432. (<a
href="https://doi.org/10.1002/sim.9624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often of interest in the health and social sciences to investigate the joint mediation effects of multiple post-exposure mediating variables. Identification of such joint mediation effects generally require no unmeasured confounding of the outcome with respect to the whole set of mediators. As the number of mediators under consideration grows, this key assumption is likely to be violated as it is often infeasible to intervene on any of the mediators. In this article, we develop a simple two-step method of moments estimation procedure to assess mediation with multiple mediators simultaneously in the presence of potential unmeasured mediator-outcome confounding. Our identification result leverages heterogeneity of the population exposure effect on the mediators, which is plausible under a variety of empirical settings. The proposed estimators are illustrated through both simulations and an application to evaluate the mediating effects of post-traumatic stress disorder symptoms in the association between self-efficacy and fatigue among health care workers during the COVID-19 outbreak.},
  archive      = {J_SIM},
  author       = {Deshanee Senevirathne Wickramarachchi and Laura Huey Mien Lim and Baoluo Sun},
  doi          = {10.1002/sim.9624},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {422-432},
  shortjournal = {Stat. Med.},
  title        = {Mediation analysis with multiple mediators under unmeasured mediator-outcome confounding},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goodness-of-fit test for a parametric mixture cure model
with partly interval-censored data. <em>SIM</em>, <em>42</em>(4),
407–421. (<a href="https://doi.org/10.1002/sim.9623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partly interval-censored event time data arise naturally in medical, biological, sociological and demographic studies. In practice, some patients may be immune from the event of interest, invoking a cure model for survival analysis. Choosing an appropriate parametric distribution for the failure time of susceptible patients is an important step to fully structure the mixture cure model. In the literature, goodness-of-fit tests for survival models are usually restricted to uncensored or right-censored data. We fill in this gap by proposing a new goodness-of-fit test dealing with partly interval-censored data under mixture cure models. Specifically, we investigate whether a parametric distribution can fit the susceptible part by using a Cramér-von Mises type of test, and establish the asymptotic distribution of the test . Empirically, the critical value is determined from the bootstrap resamples. The proposed test, compared to the traditional leveraged bootstrap approach, yields superior practical results under various settings in extensive simulation studies. Two clinical data sets are analyzed to illustrate our method.},
  archive      = {J_SIM},
  author       = {Ziqi Geng and Jialiang Li and Yi Niu and Xiaoguang Wang},
  doi          = {10.1002/sim.9623},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {4},
  pages        = {407-421},
  shortjournal = {Stat. Med.},
  title        = {Goodness-of-fit test for a parametric mixture cure model with partly interval-censored data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the distribution of ratio of paired event times
in phase II oncology trials. <em>SIM</em>, <em>42</em>(3), 388–406. (<a
href="https://doi.org/10.1002/sim.9622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of new anti-cancer agents which are cytostatic, new endpoints are needed to better measure treatment efficacy in phase II trials. For this purpose, Von Hoff (1998) proposed the growth modulation index (GMI), that is, the ratio between times to progression or progression-free survival times in two successive treatment lines. An essential task in studies using GMI as an endpoint is to estimate the distribution of GMI. Traditional methods for survival data have been used for estimating the GMI distribution because censoring is common for GMI data. However, we point out that the independent censoring assumption required by traditional survival methods is always violated for GMI, which may lead to severely biased results. In this paper, we construct both nonparametric and parametric estimators for the distribution of GMI, accounting for the dependent censoring of GMI. Extensive simulation studies show that our nonparametric estimators perform well in practical situations and outperform existing estimators, and our parametric estimators perform better than our nonparametric estimators and existing estimators when the parametric model is correctly specified. A phase II clinical trial using GMI as the primary endpoint is provided for illustration.},
  archive      = {J_SIM},
  author       = {Li Chen and Mark Burkard and Jianrong Wu and Jill M. Kolesar and Chi Wang},
  doi          = {10.1002/sim.9622},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {388-406},
  shortjournal = {Stat. Med.},
  title        = {Estimating the distribution of ratio of paired event times in phase II oncology trials},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An exact regression-based approach for the estimation of
natural direct and indirect effects with a binary outcome and a
continuous mediator. <em>SIM</em>, <em>42</em>(3), 353–387. (<a
href="https://doi.org/10.1002/sim.9621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the causal mediation framework, a number of parametric regression-based approaches have been introduced in recent years for estimating natural direct and indirect effects for a binary outcome in an exact manner, without invoking simplifying assumptions based on the rareness or commonness of the outcome. However, most of these works have focused on a binary mediator. In this article, we aim at a continuous mediator and introduce an exact approach for the estimation of natural effects on the odds ratio, risk ratio, and risk difference scales. Our approach relies on logistic and linear models for the outcome and mediator, respectively, and uses numerical integration to calculate the nested counterfactual probabilities underlying the definition of natural effects. Formulas for the delta method standard errors for all effects estimators are provided. The performance of our proposed exact estimators was evaluated in simulation studies that featured scenarios with different levels of outcome rareness/commonness, including a marginally but not conditionally rare outcome scenario. Furthermore, we evaluated the merit of Firth&#39;s penalization to mitigate the bias in the logistic regression coefficients estimators for the smallest outcome prevalences and sample sizes investigated. Using a SAS macro provided, we implemented our approach to assess the effect of placental abruption on low birth weight mediated by gestational age. We found that our exact natural effects estimators worked properly in both simulated and real data applications.},
  archive      = {J_SIM},
  author       = {Mariia Samoilenko and Geneviève Lefebvre},
  doi          = {10.1002/sim.9621},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {353-387},
  shortjournal = {Stat. Med.},
  title        = {An exact regression-based approach for the estimation of natural direct and indirect effects with a binary outcome and a continuous mediator},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A systematic review and evaluation of statistical methods
for group variable selection. <em>SIM</em>, <em>42</em>(3), 331–352. (<a
href="https://doi.org/10.1002/sim.9620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review condenses the knowledge on variable selection methods implemented in R and appropriate for datasets with grouped features. The focus is on regularized regressions identified through a systematic review of the literature, following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. A total of 14 methods are discussed, most of which use penalty terms to perform group variable selection. Depending on how the methods account for the group structure, they can be classified into knowledge and data-driven approaches. The first encompass group-level and bi-level selection methods, while two-step approaches and collinearity-tolerant methods constitute the second category. The identified methods are briefly explained and their performance compared in a simulation study. This comparison demonstrated that group-level selection methods, such as the group minimax concave penalty , are superior to other methods in selecting relevant variable groups but are inferior in identifying important individual variables in scenarios where not all variables in the groups are predictive. This can be better achieved by bi-level selection methods such as group bridge . Two-step and collinearity-tolerant approaches such as elastic net and ordered homogeneity pursuit least absolute shrinkage and selection operator are inferior to knowledge-driven methods but provide results without requiring prior knowledge. Possible applications in proteomics are considered, leading to suggestions on which method to use depending on existing prior knowledge and research question.},
  archive      = {J_SIM},
  author       = {Gregor Buch and Andreas Schulz and Irene Schmidtmann and Konstantin Strauch and Philipp S. Wild},
  doi          = {10.1002/sim.9620},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {331-352},
  shortjournal = {Stat. Med.},
  title        = {A systematic review and evaluation of statistical methods for group variable selection},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaussian variational approximate inference for joint models
of longitudinal biomarkers and a survival outcome. <em>SIM</em>,
<em>42</em>(3), 316–330. (<a
href="https://doi.org/10.1002/sim.9619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shared random effects joint model is one of the most widely used approaches to study the associations between longitudinal biomarkers and a survival outcome and make dynamic risk predictions using the longitudinally measured biomarkers. Various types of joint models have been developed under different settings in the past decades. One major limitation of joint models is that they could be computationally expensive for complex models where the number of the shared random effects is large. Moreover, the inferential accuracy of joint models could also be diminished for complex models due to approximation errors. However, complex models are frequently needed in practice, for example, when the longitudinal biomarkers have nonlinear trajectories over time or the number of longitudinal biomarkers of interest is large. In this article, we propose a novel Gaussian variational approximate inference approach for fitting joint models, which significantly improves computational efficiency while maintaining inferential accuracy. We conduct extensive simulation studies to evaluate the performance of our proposed method and compare it to existing methods. The performance of our proposed method is further demonstrated on a dataset of patients with primary biliary cirrhosis.},
  archive      = {J_SIM},
  author       = {Jieqi Tu and Jiehuan Sun},
  doi          = {10.1002/sim.9619},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {316-330},
  shortjournal = {Stat. Med.},
  title        = {Gaussian variational approximate inference for joint models of longitudinal biomarkers and a survival outcome},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A nonparametric simultaneous confidence band for biomarker
effect on the restricted mean survival time. <em>SIM</em>,
<em>42</em>(3), 297–315. (<a
href="https://doi.org/10.1002/sim.9618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Study of prognostic and predictive biomarkers plays an important role in the design and analysis of clinical trials. The Cox proportional hazards model is often used to study the biomarker main effect and the treatment-biomarker interaction effect for survival data. The estimated effects can be biased if the proportional hazards assumption is violated. The restricted mean survival time is becoming popular in clinical studies for having a clear intuitive interpretation. In this article, we first propose nonparametric methods to make statistical inference for the one-sample problem of the biomarker effect on the restricted mean survival time; we then extend the methods to the two-sample problem for studying the difference in the biomarker effects between treatment groups in clinical trials. For a given biomarker, the restricted mean survival time is estimated by kernel smoothing methods with the inverse probability of censoring weights. We prove the consistency for the estimates and develop simultaneous confidence bands for the biomarker effects on the restricted mean survival time. The simultaneous confidence bands are evaluated in extensive simulation studies and are found to have good finite sample performance. We then apply the proposed methods to a breast cancer study conducted by the Breast International Group (BIG) to illustrate how the Ki67 biomarker, a protein marker of cell proliferation, affects the survival time of patients, compared between the treatment groups.},
  archive      = {J_SIM},
  author       = {Wen Teng and Wenyu Jiang and Bingshu E. Chen},
  doi          = {10.1002/sim.9618},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {297-315},
  shortjournal = {Stat. Med.},
  title        = {A nonparametric simultaneous confidence band for biomarker effect on the restricted mean survival time},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal confidence intervals for the relative risk and odds
ratio. <em>SIM</em>, <em>42</em>(3), 281–296. (<a
href="https://doi.org/10.1002/sim.9617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relative risk and odds ratio are widely used in many fields, including biomedical research, to compare two treatments. Extensive research has been done to infer the two parameters through approximate or exact confidence intervals. However, these intervals may be liberal or conservative. A natural question is whether the intervals can be further improved in maintaining the correct confidence coefficient of an approximate interval or shortening an exact but conservative interval. In this article, when two independent binomials are observed we offer an effort to improve any of the existing intervals by applying the -function method. In particular, if the given interval is approximate, then the improved interval is exact; if the given interval is exact, then the improved interval is a subset of the given interval. This method is also applied multiple times to the improved intervals until the final resultant interval cannot be shortened any further. To demonstrate the effectiveness of the method, we use three real datasets to illustrate in detail how several good intervals in practice are improved. Two exact intervals are then recommended for estimating each of the two parameters in different scenarios.},
  archive      = {J_SIM},
  author       = {Weizhen Wang and Shuiyun Lu and Tianfa Xie},
  doi          = {10.1002/sim.9617},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {281-296},
  shortjournal = {Stat. Med.},
  title        = {Optimal confidence intervals for the relative risk and odds ratio},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized mean residual life models for survival data with
missing censoring indicators. <em>SIM</em>, <em>42</em>(3), 264–280. (<a
href="https://doi.org/10.1002/sim.9615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life (MRL) function is an important and attractive alternative to the hazard function for characterizing the distribution of a time-to-event variable. In this article, we study the modeling and inference of a family of generalized MRL models for right-censored survival data with censoring indicators missing at random. To estimate the model parameters, augmented inverse probability weighted estimating equation approaches are developed, in which the non-missingness probability and the conditional probability of an uncensored observation are estimated by parametric methods or nonparametric kernel smoothing techniques. Asymptotic properties of the proposed estimators are established and finite sample performance is evaluated by extensive simulation studies. An application to brain cancer data is presented to illustrate the proposed methods.},
  archive      = {J_SIM},
  author       = {Wenwen Li and Huijuan Ma and David Faraggi and Gregg E. Dinse},
  doi          = {10.1002/sim.9615},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {264-280},
  shortjournal = {Stat. Med.},
  title        = {Generalized mean residual life models for survival data with missing censoring indicators},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian additive regression trees for multivariate skewed
responses. <em>SIM</em>, <em>42</em>(3), 246–263. (<a
href="https://doi.org/10.1002/sim.9613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a nonparametric regression approach for univariate and multivariate skewed responses using Bayesian additive regression trees (BART). Existing BART methods use ensembles of decision trees to model a mean function, and have become popular recently due to their high prediction accuracy and ease of use. The usual assumption of a univariate Gaussian error distribution, however, is restrictive in many biomedical applications. Motivated by an oral health study, we provide a useful extension of BART, the skewBART model, to address this problem. We then extend skewBART to allow for multivariate responses, with information shared across the decision trees associated with different responses within the same subject. The methodology accommodates within-subject association, and allows varying skewness parameters for the varying multivariate responses. We illustrate the benefits of our multivariate skewBART proposal over existing alternatives via simulation studies and application to the oral health dataset with bivariate highly skewed responses. Our methodology is implementable via the R package skewBART , available on GitHub.},
  archive      = {J_SIM},
  author       = {Seungha Um and Antonio R. Linero and Debajyoti Sinha and Dipankar Bandyopadhyay},
  doi          = {10.1002/sim.9613},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {246-263},
  shortjournal = {Stat. Med.},
  title        = {Bayesian additive regression trees for multivariate skewed responses},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measures of explained variation under the mixture cure model
for survival data. <em>SIM</em>, <em>42</em>(3), 228–245. (<a
href="https://doi.org/10.1002/sim.9611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explained variation is well understood under linear regression models and has been extended to models for survival data. In this article, we consider the mixture cure models. We propose two approaches to define explained variation under the mixture cure models, one based on the Kullback-Leibler information gain and the other based on residual sum of squares. We show that the proposed measures have desired properties as measures of explained variation, similar to those under other regression models. A simulation study is conducted to demonstrate the properties of the proposed measures. They are also applied to real data analyses to illustrate the use of explained variation.},
  archive      = {J_SIM},
  author       = {Yingwei Peng and Yuyao Wang and Ronghui Xu},
  doi          = {10.1002/sim.9611},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {228-245},
  shortjournal = {Stat. Med.},
  title        = {Measures of explained variation under the mixture cure model for survival data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RECeUS: Ratio estimation of censored uncured subjects, a
different approach for assessing cure model appropriateness in studies
with long-term survivors. <em>SIM</em>, <em>42</em>(3), 209–227. (<a
href="https://doi.org/10.1002/sim.9610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need to model a cure fraction, the proportion of a cohort not susceptible to the event of interest, arises in a variety of contexts including tumor relapse in oncology. Existing methodology assumes that follow-up is long enough for all uncured subjects to have experienced the event of interest at the time of analysis, and researchers have demonstrated that fitting cure models without sufficient follow-up leads to bias. Few statistical methods exist to evaluate sufficient follow-up, and they can exhibit poor performance and lead users to falsely conclude sufficient follow-up, leading to bias, or to falsely claim insufficient follow-up, possibly leading to additional, costly data collection. We propose a new quantitative statistic (RECeUS) to evaluate whether cure models may be appropriate to apply to censored data. Specifically, we propose that the estimated proportion of censored uncured subjects in a study can be used to evaluate cure model appropriateness. We evaluated the performance of RECeUS against existing methods via simulation and with two data examples, and we observe that RECeUS displays superior performance. In simulated and real-world settings, RECeUS correctly identifies both situations in which data appear appropriate for cure modeling and when data seem inappropriate for fitting cure models.},
  archive      = {J_SIM},
  author       = {Subodh Selukar and Megan Othus},
  doi          = {10.1002/sim.9610},
  journal      = {Statistics in Medicine},
  month        = {2},
  number       = {3},
  pages        = {209-227},
  shortjournal = {Stat. Med.},
  title        = {RECeUS: Ratio estimation of censored uncured subjects, a different approach for assessing cure model appropriateness in studies with long-term survivors},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection for discrete survival model with frailty
in presence of left truncation and right censoring: Studying association
of environmental toxicants on time-to-pregnancy. <em>SIM</em>,
<em>42</em>(2), 193–208. (<a
href="https://doi.org/10.1002/sim.9609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the association between mixtures of environmental toxicants and time-to-pregnancy (TTP) is an important scientific question as sufficient evidence has emerged about the impact of individual toxicants on reproductive health and that individuals are exposed to a whole host of toxicants rather than an individual toxicant. Assessing mixtures of chemical effects on TTP poses significant statistical challenges, namely (i) TTP being a discrete survival outcome, typically subject to left truncation and right censoring, (ii) chemical exposures being strongly correlated, (iii) appropriate transformation to account for some lipid-binding chemicals, (iv) non-linear effects of some chemicals, and (v) high percentage of concentration below the limit of detection (LOD) for some chemicals. We propose a discrete frailty modeling framework (named Discnet) that allows selection of correlated covariates while appropriately addressing the methodological issues mentioned above. Discnet is shown to have better and stable false negative and false positive rates compared to alternative methods in various simulation settings. We did a detailed analysis of the pre-conception endocrine disrupting chemicals and TTP from the LIFE study and found that older females, female exposure to cotinine (smoking), DDT conferred a delay in getting pregnant, which was consistent across various approaches to account for LOD as well as non-linear associations.},
  archive      = {J_SIM},
  author       = {Abhisek Saha and Rajeshwari Sundaram},
  doi          = {10.1002/sim.9609},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {193-208},
  shortjournal = {Stat. Med.},
  title        = {Variable selection for discrete survival model with frailty in presence of left truncation and right censoring: Studying association of environmental toxicants on time-to-pregnancy},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Double robust estimation of optimal partially adaptive
treatment strategies: An application to breast cancer treatment using
hormonal therapy. <em>SIM</em>, <em>42</em>(2), 178–192. (<a
href="https://doi.org/10.1002/sim.9608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision medicine aims to tailor treatment decisions according to patients&#39; characteristics. G-estimation and dynamic weighted ordinary least squares are double robust methods to identify optimal adaptive treatment strategies. It is underappreciated that they require modeling all existing treatment-confounder interactions to be consistent. Identifying optimal partially adaptive treatment strategies that tailor treatments according to only a few covariates, ignoring some interactions, may be preferable in practice. Building on G-estimation and dWOLS, we propose estimators of such partially adaptive strategies and demonstrate their double robustness. We investigate these estimators in a simulation study. Using data maintained by the Centre des Maladies du Sein, we estimate a partially adaptive treatment strategy for tailoring hormonal therapy use in breast cancer patients. R software implementing our estimators is provided.},
  archive      = {J_SIM},
  author       = {Denis Talbot and Erica E.M. Moodie and Caroline Diorio},
  doi          = {10.1002/sim.9608},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {178-192},
  shortjournal = {Stat. Med.},
  title        = {Double robust estimation of optimal partially adaptive treatment strategies: An application to breast cancer treatment using hormonal therapy},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparing and combining data from immune assays based on
left-censored multivariate normal model assuming common assay
differences across settings. <em>SIM</em>, <em>42</em>(2), 164–177. (<a
href="https://doi.org/10.1002/sim.9607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vaccine research towards the prevention of infectious diseases, immune response biomarkers serve as an important tool for comparing and ranking vaccine candidates based on their immunogenicity and predicted protective effect. However, analyses of immune response outcomes can be complicated by differences across assays when immune response data are acquired from multiple groups/laboratories. Motivated by a real-world problem to accommodate the use of two different neutralization assays in COVID-19 vaccine trials, we propose methods based on left-censored multivariate normal model assuming common assay differences across settings, to adjust for differences between assays with respect to measurement error and the lower limit of detection. Our proposed methods integrate external paired-sample data with bridging assumptions to achieve two objectives, both using pooled data acquired from different assays: (i) comparing immunogenicity between vaccine regimens, and (ii) evaluating correlates of risk. In simulation studies, for the first objective, our method leads to unbiased calibrated assay mean with good coverage of bootstrap confidence interval, as well as valid test for immunogenicity comparison, while the alternative method assuming constant calibration model between assays leads to biased estimate of assay mean with undercoverage problem and invalid test with inflated type-I error; for the second objective, in the presence of noticeable left-censoring rate, our proposed method can drastically outperform the existing method that ignores left-censoring, in terms of reduced bias and improved precision. We apply the proposed methods to SARS-CoV-2 spike-pseudotyped virus neutralization assay data generated in vaccine and convalescent samples by two different laboratories.},
  archive      = {J_SIM},
  author       = {Ying Huang and Yunda Huang},
  doi          = {10.1002/sim.9607},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {164-177},
  shortjournal = {Stat. Med.},
  title        = {Comparing and combining data from immune assays based on left-censored multivariate normal model assuming common assay differences across settings},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adjusting for treatment selection in phase II/III clinical
trials with time to event data. <em>SIM</em>, <em>42</em>(2), 146–163.
(<a href="https://doi.org/10.1002/sim.9606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase II/III clinical trials are efficient two-stage designs that test multiple experimental treatments. In stage 1, patients are allocated to the control and all experimental treatments, with the data collected from them used to select experimental treatments to continue to stage 2. Patients recruited in stage 2 are allocated to the selected treatments and the control. Combined data of stage 1 and stage 2 are used for a confirmatory phase III analysis. Appropriate analysis needs to adjust for selection bias of the stage 1 data. Point estimators exist for normally distributed outcome data. Extending these estimators to time to event data is not straightforward because treatment selection is based on correlated treatment effects and stage 1 patients who do not get events in stage 1 are followed-up in stage 2. We have derived an approximately uniformly minimum variance conditional unbiased estimator (UMVCUE) and compared its biases and mean squared errors to existing bias adjusted estimators. In simulations, one existing bias adjusted estimator has similar properties as the practically unbiased UMVCUE while the others can have noticeable biases but they are less variable than the UMVCUE. For confirmatory phase II/III clinical trials where unbiased estimators are desired, we recommend the UMVCUE or the existing estimator with which it has similar properties.},
  archive      = {J_SIM},
  author       = {Josephine N. Khan and Peter K. Kimani and Ekkehard Glimm and Nigel Stallard},
  doi          = {10.1002/sim.9606},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {146-163},
  shortjournal = {Stat. Med.},
  title        = {Adjusting for treatment selection in phase II/III clinical trials with time to event data},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Point estimation for adaptive trial designs i: A
methodological review. <em>SIM</em>, <em>42</em>(2), 122–145. (<a
href="https://doi.org/10.1002/sim.9605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent FDA guidance on adaptive clinical trial designs defines bias as “a systematic tendency for the estimate of treatment effect to deviate from its true value,” and states that it is desirable to obtain and report estimates of treatment effects that reduce or remove this bias. The conventional end-of-trial point estimates of the treatment effects are prone to bias in many adaptive designs, because they do not take into account the potential and realized trial adaptations. While much of the methodological developments on adaptive designs have tended to focus on control of type I error rates and power considerations, in contrast the question of biased estimation has received relatively less attention. This article is the first in a two-part series that studies the issue of potential bias in point estimation for adaptive trials. Part I provides a comprehensive review of the methods to remove or reduce the potential bias in point estimation of treatment effects for adaptive designs, while part II illustrates how to implement these in practice and proposes a set of guidelines for trial statisticians. The methods reviewed in this article can be broadly classified into unbiased and bias-reduced estimation, and we also provide a classification of estimators by the type of adaptive design. We compare the proposed methods, highlight available software and code, and discuss potential methodological gaps in the literature.},
  archive      = {J_SIM},
  author       = {David S. Robertson and Babak Choodari-Oskooei and Munya Dimairo and Laura Flight and Philip Pallmann and Thomas Jaki},
  doi          = {10.1002/sim.9605},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {122-145},
  shortjournal = {Stat. Med.},
  title        = {Point estimation for adaptive trial designs i: A methodological review},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatially informed bayesian neural network for
neurodegenerative diseases classification. <em>SIM</em>, <em>42</em>(2),
105–121. (<a href="https://doi.org/10.1002/sim.9604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) plays an increasingly important role in the diagnosis and prognosis of neurodegenerative diseases. One field of extensive clinical use of MRI is the accurate and automated classification of degenerative disorders. Most of current classification studies either do not mirror medical practice where patients may exhibit early stages of the disease, comorbidities, or atypical variants, or they are not able to produce probabilistic predictions nor account for uncertainty. Also, the spatial heterogeneity of the brain alterations caused by neurodegenerative processes is not usually considered, despite the spatial configuration of the neuronal loss is a characteristic hallmark for each disorder. In this article, we propose a classification technique that incorporates uncertainty and spatial information for distinguishing between healthy subjects and patients from four distinct neurodegenerative diseases: Alzheimer&#39;s disease, mild cognitive impairment, Parkinson&#39;s disease, and Multiple Sclerosis. We introduce a spatially informed Bayesian neural network (SBNN) that combines a three-dimensional neural network to extract neurodegeneration features from MRI, Bayesian inference to account for uncertainty in diagnosis, and a spatially informed MRI image using hidden Markov random fields to encode cerebral spatial information. The SBNN model demonstrates that classification accuracy increases up to 25% by including a spatially informed MRI scan. Furthermore, the SBNN provides a robust probabilistic diagnosis that resembles clinical decision-making and can account for the heterogeneous medical presentations of neurodegenerative disorders.},
  archive      = {J_SIM},
  author       = {David Payares-Garcia and Jorge Mateu and Wiebke Schick},
  doi          = {10.1002/sim.9604},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {2},
  pages        = {105-121},
  shortjournal = {Stat. Med.},
  title        = {Spatially informed bayesian neural network for neurodegenerative diseases classification},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-sample test with g-modeling and its applications.
<em>SIM</em>, <em>42</em>(1), 89–104. (<a
href="https://doi.org/10.1002/sim.9603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real data analyses involve two-sample comparisons in location or in distribution. Most existing methods focus on problems where observations are independently and identically distributed in each group. However, in some applications the observed data are not identically distributed but associated with some unobserved parameters which are identically distributed. To address this challenge, we propose a novel two-sample testing procedure as a combination of the g $$ g $$ -modeling density estimation introduced by Efron and the two-sample Kolmogorov-Smirnov test. We also propose efficient bootstrap algorithms to estimate the statistical significance for such tests. We demonstrate the utility of the proposed approach with two biostatistical applications: the analysis of surgical nodes data with binomial model and differential expression analysis of single-cell RNA sequencing data with zero-inflated Poisson model.},
  archive      = {J_SIM},
  author       = {Jingyi Zhai and Hui Jiang},
  doi          = {10.1002/sim.9603},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {89-104},
  shortjournal = {Stat. Med.},
  title        = {Two-sample test with g-modeling and its applications},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using a surrogate with heterogeneous utility to test for a
treatment effect. <em>SIM</em>, <em>42</em>(1), 68–88. (<a
href="https://doi.org/10.1002/sim.9602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary benefit of identifying a valid surrogate marker is the ability to use it in a future trial to test for a treatment effect with shorter follow-up time or less cost. However, previous work has demonstrated potential heterogeneity in the utility of a surrogate marker. When such heterogeneity exists, existing methods that use the surrogate to test for a treatment effect while ignoring this heterogeneity may lead to inaccurate conclusions about the treatment effect, particularly when the patient population in the new study has a different mix of characteristics than the study used to evaluate the utility of the surrogate marker. In this article, we develop a novel test for a treatment effect using surrogate marker information that accounts for heterogeneity in the utility of the surrogate. We compare our testing procedure to a test that uses primary outcome information (gold standard) and a test that uses surrogate marker information, but ignores heterogeneity. We demonstrate the validity of our approach and derive the asymptotic properties of our estimator and variance estimates. Simulation studies examine the finite sample properties of our testing procedure and demonstrate when our proposed approach can outperform the testing approach that ignores heterogeneity. We illustrate our methods using data from an AIDS clinical trial to test for a treatment effect using CD4 count as a surrogate marker for RNA.},
  archive      = {J_SIM},
  author       = {Layla Parast and Tianxi Cai and Lu Tian},
  doi          = {10.1002/sim.9602},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {68-88},
  shortjournal = {Stat. Med.},
  title        = {Using a surrogate with heterogeneous utility to test for a treatment effect},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic impairment classification through arrayed
comparisons. <em>SIM</em>, <em>42</em>(1), 52–67. (<a
href="https://doi.org/10.1002/sim.9601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multivariate normative comparison (MNC) method has been used for identifying cognitive impairment. When participants&#39; cognitive brain domains are evaluated regularly, the longitudinal MNC (LMNC) has been introduced to correct for the intercorrelation among repeated assessments of multiple cognitive domains in the same participant. However, it may not be practical to wait until the end of study for diagnosis. For example, in participants of the Multicenter AIDS Cohort Study (MACS), cognitive functioning has been evaluated repeatedly for more than 35 years. Therefore, it is optimal to identify cognitive impairment at each assessment, while the family-wise error rate (FWER) is controlled with unknown number of assessments in future. In this work, we propose to use the difference of consecutive LMNC test statistics to construct independent tests. Frequency modeling can help predict how many assessments each participant will have, so Bonferroni-type correction can be easily adapted. A chi-squared test is used under the assumption of multivariate normality, and permutation test is proposed where this assumption is violated. We showed through simulation and the MACS data that our method controlled FWER below a predetermined level.},
  archive      = {J_SIM},
  author       = {Zheng Wang and Zi Wang and Lingyun Lyu and Yu Cheng and Eric C. Seaberg and Samantha A. Molsberry and Ann Ragin and James T. Becker},
  doi          = {10.1002/sim.9601},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {52-67},
  shortjournal = {Stat. Med.},
  title        = {Dynamic impairment classification through arrayed comparisons},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing positivity violations in causal effect estimation
using gaussian process priors. <em>SIM</em>, <em>42</em>(1), 33–51. (<a
href="https://doi.org/10.1002/sim.9600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies, causal inference relies on several key identifying assumptions. One identifiability condition is the positivity assumption, which requires the probability of treatment be bounded away from 0 and 1. That is, for every covariate combination, it should be possible to observe both treated and control subjects the covariate distributions should overlap between treatment arms. If the positivity assumption is violated, population-level causal inference necessarily involves some extrapolation. Ideally, a greater amount of uncertainty about the causal effect estimate should be reflected in such situations. With that goal in mind, we construct a Gaussian process model for estimating treatment effects in the presence of practical violations of positivity. Advantages of our method include minimal distributional assumptions, a cohesive model for estimating treatment effects, and more uncertainty associated with areas in the covariate space where there is less overlap. We assess the performance of our approach with respect to bias and efficiency using simulation studies. The method is then applied to a study of critically ill female patients to examine the effect of undergoing right heart catheterization.},
  archive      = {J_SIM},
  author       = {Angela Yaqian Zhu and Nandita Mitra and Jason Roy},
  doi          = {10.1002/sim.9600},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {33-51},
  shortjournal = {Stat. Med.},
  title        = {Addressing positivity violations in causal effect estimation using gaussian process priors},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian graphical modeling for heterogeneous causal
effects. <em>SIM</em>, <em>42</em>(1), 15–32. (<a
href="https://doi.org/10.1002/sim.9599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in current medical research to develop personalized treatments using a molecular-based approach. The broad goal is to implement a more precise and targeted decision-making process, relative to traditional treatments based primarily on clinical diagnoses. Specifically, we consider patients affected by Acute Myeloid Leukemia (AML), an hematological cancer characterized by uncontrolled proliferation of hematopoietic stem cells in the bone marrow. Because AML responds poorly to chemotherapeutic treatments, the development of targeted therapies is essential to improve patients&#39; prospects. In particular, the dataset we analyze contains the levels of proteins involved in cell cycle regulation and linked to the progression of the disease. We evaluate treatment effects within a causal framework represented by a Directed Acyclic Graph (DAG) model, whose vertices are the protein levels in the network. A major obstacle in implementing the above program is represented by individual heterogeneity. We address this issue through a Dirichlet Process (DP) mixture of Gaussian DAG-models where both the graphical structure as well as the allied model parameters are regarded as uncertain. Our procedure determines a clustering structure of the units reflecting the underlying heterogeneity, and produces subject-specific estimates of causal effects based on Bayesian Model Averaging (BMA). With reference to the AML dataset, we identify different effects of protein regulation among individuals; moreover, our method clusters patients into groups that exhibit only mild similarities with traditional categories based on morphological features.},
  archive      = {J_SIM},
  author       = {Federico Castelletti and Guido Consonni},
  doi          = {10.1002/sim.9599},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {15-32},
  shortjournal = {Stat. Med.},
  title        = {Bayesian graphical modeling for heterogeneous causal effects},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The scale transformed power prior for use with historical
data from a different outcome model. <em>SIM</em>, <em>42</em>(1), 1–14.
(<a href="https://doi.org/10.1002/sim.9598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop the scale transformed power prior for settings where historical and current data involve different data types, such as binary and continuous data. This situation arises often in clinical trials, for example, when historical data involve binary responses and the current data involve some other type of continuous or discrete outcome. The power prior, proposed by Ibrahim and Chen, does not address the issue of different data types. Herein, we develop a new type of power prior, which we call the scale transformed power prior (straPP). The straPP is constructed by transforming the power prior for the historical data by rescaling the parameter using a function of the Fisher information matrices for the historical and current data models, thereby shifting the scale of the parameter vector from that of the historical to that of the current data. Examples are presented to motivate the need for such a transformation, and simulation studies are presented to illustrate the performance advantages of the straPP over the power prior and other informative and noninformative priors. A real dataset from a clinical trial undertaken to study a novel transitional care model for stroke survivors is used to illustrate the methodology.},
  archive      = {J_SIM},
  author       = {Ethan M. Alt and Brady Nifong and Xinxin Chen and Matthew A. Psioda and Joseph G. Ibrahim},
  doi          = {10.1002/sim.9598},
  journal      = {Statistics in Medicine},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Stat. Med.},
  title        = {The scale transformed power prior for use with historical data from a different outcome model},
  volume       = {42},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
