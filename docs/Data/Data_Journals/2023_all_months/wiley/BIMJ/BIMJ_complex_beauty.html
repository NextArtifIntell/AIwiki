<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIMJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="bimj---120">BIMJ - 120</h2>
<ul>
<li><details>
<summary>
(2023h). Cover picture: Biometrical journal 8’23. <em>BIMJ</em>,
<em>65</em>(8), 2370081. (<a
href="https://doi.org/10.1002/bimj.202370081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370081},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2370081},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 8&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size calculation for one-armed clinical trials with
clustered data and binary outcome. <em>BIMJ</em>, <em>65</em>(8),
2300123. (<a href="https://doi.org/10.1002/bimj.202300123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The formula of Fleiss and Cuzick (1979) to estimate the intraclass correlation coefficient is applied to reduce the task of sample size calculation for clustered data with binary outcome. It is demonstrated that this approach reduces the complexity of sample size calculation to the determination of the null and alternative hypothesis and the formulation of the quantitative influence of the belonging to the same cluster on the therapy success probability.},
  archive      = {J_BIMJ},
  author       = {Maximilian Pilz},
  doi          = {10.1002/bimj.202300123},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300123},
  shortjournal = {Bio. J.},
  title        = {Sample size calculation for one-armed clinical trials with clustered data and binary outcome},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A scalable approach for short-term disease forecasting in
high spatial resolution areal data. <em>BIMJ</em>, <em>65</em>(8),
2300096. (<a href="https://doi.org/10.1002/bimj.202300096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term disease forecasting at specific discrete spatial resolutions has become a high-impact decision-support tool in health planning. However, when the number of areas is very large obtaining predictions can be computationally intensive or even unfeasible using standard spatiotemporal models. The purpose of this paper is to provide a method for short-term predictions in high-dimensional areal data based on a newly proposed “divide-and-conquer” approach. We assess the predictive performance of this method and other classical spatiotemporal models in a validation study that uses cancer mortality data for the 7907 municipalities of continental Spain. The new proposal outperforms traditional models in terms of mean absolute error, root mean square error, and interval score when forecasting cancer mortality 1, 2, and 3 years ahead. Models are implemented in a fully Bayesian framework using the well-known integrated nested Laplace estimation technique.},
  archive      = {J_BIMJ},
  author       = {Erick Orozco-Acosta and Andrea Riebler and Aritz Adin and Maria D. Ugarte},
  doi          = {10.1002/bimj.202300096},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300096},
  shortjournal = {Bio. J.},
  title        = {A scalable approach for short-term disease forecasting in high spatial resolution areal data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A joint bayesian framework for missing data and measurement
error using integrated nested laplace approximations. <em>BIMJ</em>,
<em>65</em>(8), 2300078. (<a
href="https://doi.org/10.1002/bimj.202300078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurement error (ME) and missing values in covariates are often unavoidable in disciplines that deal with data, and both problems have separately received considerable attention during the past decades. However, while most researchers are familiar with methods for treating missing data, accounting for ME in covariates of regression models is less common. In addition, ME and missing data are typically treated as two separate problems, despite practical and theoretical similarities. Here, we exploit the fact that missing data in a continuous covariate is an extreme case of classical ME, allowing us to use existing methodology that accounts for ME via a Bayesian framework that employs integrated nested Laplace approximations (INLA) and thus to simultaneously account for both ME and missing data in the same covariate. As a useful by-product, we present an approach to handle missing data in INLA since this corresponds to the special case when no ME is present. In addition, we show how to account for Berkson ME in the same framework. In its broadest generality, the proposed joint Bayesian framework can thus account for Berkson ME, classical ME, and missing data, or any combination of these in the same or different continuous covariates of the family of regression models that are feasible with INLA. The approach is exemplified using both simulated and real data. We provide extensive and fully reproducible Supporting Information with thoroughly documented examples using R-INLA and inlabru .},
  archive      = {J_BIMJ},
  author       = {Emma Skarstein and Sara Martino and Stefanie Muff},
  doi          = {10.1002/bimj.202300078},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300078},
  shortjournal = {Bio. J.},
  title        = {A joint bayesian framework for missing data and measurement error using integrated nested laplace approximations},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The marginality principle revisited: Should “higher-order”
terms always be accompanied by “lower-order” terms in regression
analyses? <em>BIMJ</em>, <em>65</em>(8), 2300069. (<a
href="https://doi.org/10.1002/bimj.202300069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The marginality principle guides analysts to avoid omitting lower-order terms from models in which higher-order terms are included as covariates. Lower-order terms are viewed as “marginal” to higher-order terms. We consider how this principle applies to three cases: regression models that may include the ratio of two measured variables; polynomial transformations of a measured variable; and factorial arrangements of defined interventions. For each case, we show that which terms or transformations are considered to be lower-order, and therefore marginal, depends on the scale of measurement, which is frequently arbitrary. Understanding the implications of this point leads to an intuitive understanding of the curse of dimensionality. We conclude that the marginality principle may be useful to analysts in some specific cases but caution against invoking it as a context-free recipe.},
  archive      = {J_BIMJ},
  author       = {Tim P. Morris and Maarten van Smeden and Tra My Pham},
  doi          = {10.1002/bimj.202300069},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300069},
  shortjournal = {Bio. J.},
  title        = {The marginality principle revisited: Should “higher-order” terms always be accompanied by “lower-order” terms in regression analyses?},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fiducial-based confidence interval for the linear
combination of multinomial probabilities. <em>BIMJ</em>, <em>65</em>(8),
2300065. (<a href="https://doi.org/10.1002/bimj.202300065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across a broad set of applications, system outcomes may be summarized as probabilities in confusion or contingency tables. In settings with more than two outcomes (e.g., stages of cancer), these outcomes represent multinomial experiments. Measures to summarize system performance have been presented as linear combinations of the resulting multinomial probabilities. Statistical inference on the linear combination of multinomial probabilities has been focused on large-sample and parametric settings and not small-sample settings. Such inference is valuable, however, especially in settings such as those resulting from pilot or low-cost studies. To address this gap, we leverage the fiducial approach to derive confidence intervals around the linear combination of multinomial parameters with desirable frequentist properties. One of the original arguments against the fiducial approach was its inability to extend to multiparameter settings. Therefore, the great novelty of this work is both the derived interval and the logical framework for applying the fiducial approach in multiparameter settings. Through simulation, we demonstrate that the proposed method maintains a minimum coverage of , unlike the bootstrap and large-sample methods, at comparable interval lengths. Finally, we illustrate its use in a medical problem of selecting classifiers for diagnosing chronic allograph nephropathy in postkidney transplant patients.},
  archive      = {J_BIMJ},
  author       = {Katherine A. Batterton and Christine M. Schubert and Richard L. Warr},
  doi          = {10.1002/bimj.202300065},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300065},
  shortjournal = {Bio. J.},
  title        = {A fiducial-based confidence interval for the linear combination of multinomial probabilities},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Letter to the editor on “new weighting methods when cases
are only a subset of events in a nested case-control study” by qian m.
Zhou, xuan wang, yingye zheng, and tianxi cai. <em>BIMJ</em>,
<em>65</em>(8), 2300053. (<a
href="https://doi.org/10.1002/bimj.202300053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Francesca Graziano and Maria Grazia Valsecchi and Paola Rebora},
  doi          = {10.1002/bimj.202300053},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300053},
  shortjournal = {Bio. J.},
  title        = {Letter to the editor on “New weighting methods when cases are only a subset of events in a nested case-control study” by qian m. zhou, xuan wang, yingye zheng, and tianxi cai},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On “reflections on the concept of optimality of single
decision point treatment regimes.” <em>BIMJ</em>, <em>65</em>(8),
2300027. (<a href="https://doi.org/10.1002/bimj.202300027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This is a discussion of “Reflections on the concept of optimality of single decision point treatment regimes” by Trung Dung Tran, Ariel Alonso Abad, Geert Verbeke, Geert Molenberghs, and Iven Van Mechelen. The authors propose a thoughtful consideration of optimization targets and the implications of such targets for the resulting optimal treatment rule. However, we contest the assertation that targets of optimization have been overlooked and suggest additional considerations that researchers must contemplate as part of a complete framework for learning about optimal treatment regimes.},
  archive      = {J_BIMJ},
  author       = {Erica E. M. Moodie and Denis Talbot},
  doi          = {10.1002/bimj.202300027},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300027},
  shortjournal = {Bio. J.},
  title        = {On “Reflections on the concept of optimality of single decision point treatment regimes”},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On near-redundancy and identifiability of parametric hazard
regression models under censoring. <em>BIMJ</em>, <em>65</em>(8),
2300006. (<a href="https://doi.org/10.1002/bimj.202300006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study parametric inference on a rich class of hazard regression models in the presence of right-censoring. Previous literature has reported some inferential challenges, such as multimodal or flat likelihood surfaces, in this class of models for some particular data sets. We formalize the study of these inferential problems by linking them to the concepts of near-redundancy and practical nonidentifiability of parameters. We show that the maximum likelihood estimators of the parameters in this class of models are consistent and asymptotically normal. Thus, the inferential problems in this class of models are related to the finite-sample scenario, where it is difficult to distinguish between the fitted model and a nested nonidentifiable (i.e., parameter-redundant) model. We propose a method for detecting near-redundancy, based on distances between probability distributions. We also employ methods used in other areas for detecting practical nonidentifiability and near-redundancy, including the inspection of the profile likelihood function and the Hessian method. For cases where inferential problems are detected, we discuss alternatives such as using model selection tools to identify simpler models that do not exhibit these inferential problems, increasing the sample size, or extending the follow-up time. We illustrate the performance of the proposed methods through a simulation study. Our simulation study reveals a link between the presence of near-redundancy and practical nonidentifiability. Two illustrative applications using real data, with and without inferential problems, are presented.},
  archive      = {J_BIMJ},
  author       = {Francisco J. Rubio and Jorge A. Espindola and José A. Montoya},
  doi          = {10.1002/bimj.202300006},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2300006},
  shortjournal = {Bio. J.},
  title        = {On near-redundancy and identifiability of parametric hazard regression models under censoring},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Opportunities and challenges with decentralized trials in
neuroscience. <em>BIMJ</em>, <em>65</em>(8), 2200370. (<a
href="https://doi.org/10.1002/bimj.202200370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized clinical trials (DCTs), that is, studies integrating elements of telemedicine and mobile/local healthcare providers allowing for home-based assessments, are an important concept to make studies more resilient and more patient-centric by taking into consideration participant&#39;s views and shifting trial activities to better meet the needs of trial participants. There are however, not only advantages but also challenges associated with DCTs. An area to be addressed by appropriate statistical methodology is the integration of data resulting from a possible mix of home and clinic assessments at different visits for the same variable, especially in adjusting for sources of possible systematic differences. One source of systematic bias may be how a participant perceives their disease and treatment in their home versus in a clinical setting. In this paper, we will discuss these issues with a focus on Neuroscience when participants have the choice between home and clinic assessments to illustrate how to identify systematic biases and describe appropriate approaches to maintain clinical trial scientific rigor. We will describe the benefits and challenges of DCTs in Neuroscience and then describe the relevance of home versus clinic assessments using the estimand framework. We outline several options to enable home assessments in a study. Results of simulations will be presented to help deciding between design and analysis options in a simple scenario where there might be differences in response between clinic and home assessments.},
  archive      = {J_BIMJ},
  author       = {Hans Ulrich Burger and Tom Van de Casteele and Khadija Rerhou Rantell and Patricia Corey-Lisle and Nikolaos Sfikas and Markus Abt},
  doi          = {10.1002/bimj.202200370},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200370},
  shortjournal = {Bio. J.},
  title        = {Opportunities and challenges with decentralized trials in neuroscience},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation in optimal treatment regimes based on mean
residual lifetimes with right-censored data. <em>BIMJ</em>,
<em>65</em>(8), 2200340. (<a
href="https://doi.org/10.1002/bimj.202200340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optimal individualized treatment regime (ITR) is a decision rule in allocating the best treatment to each patient and, hence, maximizing overall benefits. In this paper, we propose a novel framework based on nonparametric inverse probability weighting (IPW) and augmented inverse probability weighting (AIPW) estimators of the value function when the data are subject to right censoring. In contrast to most existing approaches that are designed to maximize the expected survival time under a binary treatment framework, the proposed method targets maximizing the mean residual lifetime of patients. Specifically, the proposed IPW method searches the optimal ITR by maximizing an estimator for the overall population outcome directly, without specifying the regression model for the conditional mean residual lifetime, whereas the AIPW method integrates the model information of the mean residual lifetime to improve the robustness. Furthermore, to overcome the computational difficulty in a nonsmooth value estimator, smoothed IPW and AIPW estimators are constructed. In theory, we establish the asymptotic properties of the proposed method under suitable regularity conditions. The empirical performances of the proposed IPW and AIPW estimators are evaluated using simulation studies and are further illustrated with an application to the real-world data set from the Acquired Immunodeficiency Syndrome Clinical Trial Group Protocol 175 (ACTG175).},
  archive      = {J_BIMJ},
  author       = {Zhishuai Liu and Zishu Zhan and Cunjie Lin and Baqun Zhang},
  doi          = {10.1002/bimj.202200340},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200340},
  shortjournal = {Bio. J.},
  title        = {Estimation in optimal treatment regimes based on mean residual lifetimes with right-censored data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporation of healthy volunteers data on receptor
occupancy into a phase II proof-of-concept trial using a bayesian
dynamic borrowing design. <em>BIMJ</em>, <em>65</em>(8), 2200305. (<a
href="https://doi.org/10.1002/bimj.202200305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Receptor occupancy in targeted tissues measures the proportion of receptors occupied by a drug at equilibrium and is sometimes used as a surrogate of drug efficacy to inform dose selection in clinical trials. We propose to incorporate data on receptor occupancy from a phase I study in healthy volunteers into a phase II proof-of-concept study in patients, with the objective of using all the available evidence to make informed decisions. A minimal physiologically based pharmacokinetic modeling is used to model receptor occupancy in healthy volunteers and to predict it in the patients of a phase II proof-of-concept study, taking into account the variability of the population parameters and the specific differences arising from the pathological condition compared to healthy volunteers. Then, given an estimated relationship between receptor occupancy and the clinical endpoint, an informative prior distribution is derived for the clinical endpoint in both the treatment and control arms of the phase II study. These distributions are incorporated into a Bayesian dynamic borrowing design to supplement concurrent phase II trial data. A simulation study in immuno-inflammation demonstrates that the proposed design increases the power of the study while maintaining a type I error at acceptable levels for realistic values of the clinical endpoint.},
  archive      = {J_BIMJ},
  author       = {Fulvio Di Stefano and Christelle Rodrigues and Stephanie Galtier and Sandrine Guilleminot and Veronique Robert and Mauro Gasparini and Gaelle Saint-Hilary},
  doi          = {10.1002/bimj.202200305},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200305},
  shortjournal = {Bio. J.},
  title        = {Incorporation of healthy volunteers data on receptor occupancy into a phase II proof-of-concept trial using a bayesian dynamic borrowing design},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability of clinical prediction models developed using
statistical or machine learning methods. <em>BIMJ</em>, <em>65</em>(8),
2200302. (<a href="https://doi.org/10.1002/bimj.202200302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical prediction models estimate an individual&#39;s risk of a particular health outcome. A developed model is a consequence of the development dataset and model-building strategy, including the sample size, number of predictors, and analysis method (e.g., regression or machine learning). We raise the concern that many models are developed using small datasets that lead to instability in the model and its predictions (estimated risks). We define four levels of model stability in estimated risks moving from the overall mean to the individual level. Through simulation and case studies of statistical and machine learning approaches, we show instability in a model&#39;s estimated risks is often considerable, and ultimately manifests itself as miscalibration of predictions in new data. Therefore, we recommend researchers always examine instability at the model development stage and propose instability plots and measures to do so. This entails repeating the model-building steps (those used to develop the original prediction model) in each of multiple (e.g., 1000) bootstrap samples, to produce multiple bootstrap models, and deriving (i) a prediction instability plot of bootstrap model versus original model predictions; (ii) the mean absolute prediction error (mean absolute difference between individuals’ original and bootstrap model predictions), and (iii) calibration, classification, and decision curve instability plots of bootstrap models applied in the original sample. A case study illustrates how these instability assessments help reassure (or not) whether model predictions are likely to be reliable (or not), while informing a model&#39;s critical appraisal (risk of bias rating), fairness, and further validation requirements.},
  archive      = {J_BIMJ},
  author       = {Richard D. Riley and Gary S. Collins},
  doi          = {10.1002/bimj.202200302},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200302},
  shortjournal = {Bio. J.},
  title        = {Stability of clinical prediction models developed using statistical or machine learning methods},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Response-adaptive randomization for multiarm clinical trials
using context-dependent information measures. <em>BIMJ</em>,
<em>65</em>(8), 2200301. (<a
href="https://doi.org/10.1002/bimj.202200301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theoretical-information approach applied to the clinical trial designs appeared to bring several advantages when tackling a problem of finding a balance between power and expected number of successes (ENS). In particular, it was shown that the built-in parameter of the weight function allows finding the desired trade-off between the statistical power and number of treated patients in the context of small population Phase II clinical trials. However, in real clinical trials, randomized designs are more preferable. The goal of this research is to introduce randomization to a deterministic entropy-based sequential trial procedure generalized to multiarm setting. Several methods of randomization applied to an entropy-based design are investigated in terms of statistical power and ENS. Namely, the four design types are considered: (a) deterministic procedures, (b) naive randomization using the inverse of entropy criteria as weights, (c) block randomization, and (d) randomized penalty parameter. The randomized entropy-based designs are compared to randomized Gittins index (GI) and fixed randomization (FR). After the comprehensive simulation study, the following conclusion on block randomization is made: for both entropy-based and GI-based block randomization designs the degree of randomization induced by forward-looking procedures is insufficient to achieve a decent statistical power. Therefore, we propose an adjustment for the forward-looking procedure that improves power with almost no cost in terms of ENS. In addition, the properties of randomization procedures based on randomly drawn penalty parameter are also thoroughly investigated.},
  archive      = {J_BIMJ},
  author       = {Ksenia Kasianova and Mark Kelbert and Pavel Mozgunov},
  doi          = {10.1002/bimj.202200301},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200301},
  shortjournal = {Bio. J.},
  title        = {Response-adaptive randomization for multiarm clinical trials using context-dependent information measures},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiplicity adjustments for the dunnett procedure under
heterokcedasticity. <em>BIMJ</em>, <em>65</em>(8), 2200300. (<a
href="https://doi.org/10.1002/bimj.202200300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a simulation-based method for computing the multiplicity adjusted p -values and critical constants for the Dunnett procedure for comparing treatments with a control under heteroskedasticity. The Welch–Satterthwaite test statistics used in this procedure do not have a simple multivariate t -distribution because their denominators are mixtures of chi-squares and are correlated because of the common control treatment sample variance present in all denominators. The joint distribution of the denominators of the test statistics is approximated by correlated chi-square variables and is generated using a novel algorithm proposed in this paper. This approximation is used to derive critical constants or adjusted p -values. The familywise error rate (FWER) of the proposed method is compared with some existing methods via simulation under different heteroskedastic scenarios. The results show that our proposed method controls the FWER most accurately, whereas other methods are either too conservative or liberal or control the FWER less accurately. The different methods considered are illustrated on a real data set.},
  archive      = {J_BIMJ},
  author       = {Ajit C. Tamhane and Dong Xi},
  doi          = {10.1002/bimj.202200300},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200300},
  shortjournal = {Bio. J.},
  title        = {Multiplicity adjustments for the dunnett procedure under heterokcedasticity},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reflections on the concept of optimality of single decision
point treatment regimes. <em>BIMJ</em>, <em>65</em>(8), 2200285. (<a
href="https://doi.org/10.1002/bimj.202200285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many areas, applied researchers as well as practitioners have to choose between different solutions for a problem at hand; this calls for optimal decision rules to settle the choices involved. As a key example, one may think of the search for optimal treatment regimes (OTRs) in clinical research, that specify which treatment alternative should be administered to each patient under study. Motivated by the fact that the concept of optimality of decision rules in general and treatment regimes in particular has received so far relatively little attention and discussion, we will present a number of reflections on it, starting from the basics of any optimization problem. Specifically, we will analyze the search space and the to be optimized criterion function underlying the search of single decision point OTRs, along with the many choice aspects that show up in their specification. Special attention is paid to formal characteristics and properties as well as to substantive concerns and hypotheses that may guide these choices. We illustrate with a few empirical examples taken from the literature. Finally, we discuss how the presented reflections may help sharpen statistical thinking about optimality of decision rules for treatment assignment and to facilitate the dialogue between the statistical consultant and the applied researcher in search of an OTR.},
  archive      = {J_BIMJ},
  author       = {Trung Dung Tran and Ariel Alonso Abad and Geert Verbeke and Geert Molenberghs and Iven Van Mechelen},
  doi          = {10.1002/bimj.202200285},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200285},
  shortjournal = {Bio. J.},
  title        = {Reflections on the concept of optimality of single decision point treatment regimes},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multivariate reference and tolerance regions based on
conditional transformation models: Application to glycemic markers.
<em>BIMJ</em>, <em>65</em>(8), 2200229. (<a
href="https://doi.org/10.1002/bimj.202200229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reference interval is the most widely used medical decision-making, constituting a central tool in determining whether an individual is healthy or not. When the results of several continuous diagnostic tests are available for the same patient, their clinical interpretation is more reliable if a multivariate reference region (MVR) is available rather than multiple univariate reference intervals. MVRs, defined as regions containing 95% of the results of healthy subjects, extend the concept of the reference interval to the multivariate setting. However, they are rarely used in clinical practice owing to difficulties associated with their interpretability and the restrictions inherent to the assumption of a Gaussian distribution. Further statistical research is thus needed to make MVRs more applicable and easier for physicians to interpret. Since the joint distribution of diagnostic test results may well change with patient characteristics independent of disease status, MVRs adjusted for covariates are desirable. The present work introduces a novel formulation for MVRs based on multivariate conditional transformation models (MCTMs). Additionally, we take into account the estimation uncertainty of such MVRs by means of tolerance regions. These conditional MVRs imply no parametric restriction on the response, and potentially nonlinear continuous covariate effects can be estimated. MCTMs allow the estimation of the effects of covariates on the joint distribution of multivariate response variables and on these variables&#39; marginal distributions, via the use of most likely transformation estimation. Our contributions proved reliable when tested with simulated data and for a real data application with two glycemic markers.},
  archive      = {J_BIMJ},
  author       = {Óscar Lado-Baleato and Carmen Cadarso-Suárez and Thomas Kneib and Francisco Gude},
  doi          = {10.1002/bimj.202200229},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200229},
  shortjournal = {Bio. J.},
  title        = {Multivariate reference and tolerance regions based on conditional transformation models: Application to glycemic markers},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal decomposition maps: An exploratory tool for designing
area-level interventions aimed at reducing health disparities.
<em>BIMJ</em>, <em>65</em>(8), 2200213. (<a
href="https://doi.org/10.1002/bimj.202200213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods for decomposition analyses have been developed to partition between-group differences into explained and unexplained portions. In this paper, we introduce the concept of causal decomposition maps, which allow researchers to test the effect of area-level interventions on disease maps before implementation. These maps quantify the impact of interventions that aim to reduce differences in health outcomes between groups and illustrate how the disease map might change under different interventions. We adapt a new causal decomposition analysis method for the disease mapping context. Through the specification of a Bayesian hierarchical outcome model, we obtain counterfactual small area estimates of age-adjusted rates and reliable estimates of decomposition quantities. We present two formulations of the outcome model, with the second allowing for spatial interference of the intervention. Our method is utilized to determine whether the addition of gyms in different sets of rural ZIP codes could reduce any of the rural–urban difference in age-adjusted colorectal cancer incidence rates in Iowa ZIP codes.},
  archive      = {J_BIMJ},
  author       = {Melissa J. Smith and Mary E. Charlton and Jacob J. Oleson},
  doi          = {10.1002/bimj.202200213},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200213},
  shortjournal = {Bio. J.},
  title        = {Causal decomposition maps: An exploratory tool for designing area-level interventions aimed at reducing health disparities},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unbiased and efficient estimation of causal treatment
effects in crossover trials. <em>BIMJ</em>, <em>65</em>(8), 2200170. (<a
href="https://doi.org/10.1002/bimj.202200170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce causal inference reasoning to crossover trials, with a focus on thorough QT (TQT) studies. For such trials, we propose different sets of assumptions and consider their impact on the modeling strategy and estimation procedure. We show that unbiased estimates of a causal treatment effect are obtained by a g-computation approach in combination with weighted least squares predictions from a working regression model. Only a few natural requirements on the working regression and weighting matrix are needed for the result to hold. It follows that a large class of Gaussian linear mixed working models lead to unbiased estimates of a causal treatment effect, even if they do not capture the true data-generating mechanism. We compare a range of working regression models in a simulation study where data are simulated from a complex data-generating mechanism with input parameters estimated on a real TQT data set. In this setting, we find that for all practical purposes working models adjusting for baseline QTc measurements have comparable performance. Specifically, this is observed for working models that are by default too simplistic to capture the true data-generating mechanism. Crossover trials and particularly TQT studies can be analyzed efficiently using simple working regression models without biasing the estimates for the causal parameters of interest.},
  archive      = {J_BIMJ},
  author       = {Jeppe Ekstrand Halkjær Madsen and Thomas Scheike and Christian Pipper},
  doi          = {10.1002/bimj.202200170},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200170},
  shortjournal = {Bio. J.},
  title        = {Unbiased and efficient estimation of causal treatment effects in crossover trials},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spearman-like correlation measure adjusting for covariates
in bivariate survival data. <em>BIMJ</em>, <em>65</em>(8), 2200137. (<a
href="https://doi.org/10.1002/bimj.202200137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an extension of Spearman&#39;s correlation for censored continuous and discrete data that permits covariate adjustment. Previously proposed nonparametric and semiparametric Spearman&#39;s correlation estimators require either nonparametric estimation of the bivariate survival surface or parametric assumptions about the dependence structure. In practice, nonparametric estimation of the bivariate survival surface is difficult, and parametric assumptions about the correlation structure may not be satisfied. Therefore, we propose a method that requires neither and uses only the marginal survival distributions. Our method estimates the correlation of probability-scale residuals, which has been shown to equal Spearman&#39;s correlation when there is no censoring. Because this method relies only on marginal distributions, it tends to be less variable than the previously suggested nonparametric estimators, and the confidence intervals are easily constructed. Although under censoring, it is biased for Spearman&#39;s correlation as our simulations show, it performs well under moderate censoring, with a smaller mean squared error than nonparametric approaches. We also extend it to partial (adjusted), conditional, and partial-conditional correlation, which makes it particularly relevant for practical applications. We apply our method to estimate the correlation between time to viral failure and time to regimen change in a multisite cohort of persons living with HIV in Latin America.},
  archive      = {J_BIMJ},
  author       = {Svetlana K. Eden and Chun Li and Bryan E. Shepherd},
  doi          = {10.1002/bimj.202200137},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200137},
  shortjournal = {Bio. J.},
  title        = {Spearman-like correlation measure adjusting for covariates in bivariate survival data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). State-space prior distribution for parameter of
nonhomogeneous poisson spatiotemporal models. <em>BIMJ</em>,
<em>65</em>(8), 2200125. (<a
href="https://doi.org/10.1002/bimj.202200125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new class of nonhomogeneous Poisson spatiotemporal model. In this approach, we use a state-space model-based prior distribution to handle the scale and shape parameters of the Weibull intensity function. The proposed prior distribution enables the inclusion of changes in the behavior of the intensity function over time. In defining the spatial correlation function of the model, we include anisotropy via spatial deformation. We estimate the model parameters from a Bayesian perspective, employ the Markov chain Monte Carlo approach, and validate this estimation procedure through a simulation exercise. Finally, the extreme rainfall in the southern semiarid region in northeastern Brazil is analyzed using the R10mm index. The proposed model showed better fit and prediction ability than did other nonhomogeneous Poisson spatiotemporal models available in the literature. This improvement in performance is mainly due to the flexibility of the intensity function that is achieved by allowing the incorporation, in time, of the climatic characteristics of this region.},
  archive      = {J_BIMJ},
  author       = {Fidel Ernesto Castro Morales},
  doi          = {10.1002/bimj.202200125},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200125},
  shortjournal = {Bio. J.},
  title        = {State-space prior distribution for parameter of nonhomogeneous poisson spatiotemporal models},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trial arm outcome variance difference after dropout as an
indicator of missing-not-at-random bias in randomized controlled trials.
<em>BIMJ</em>, <em>65</em>(8), 2200116. (<a
href="https://doi.org/10.1002/bimj.202200116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized controlled trials (RCTs) are vulnerable to bias from missing data. When outcomes are missing not at random (MNAR), estimates from complete case analysis (CCA) and multiple imputation (MI) may be biased. There is no statistical test for distinguishing between outcomes missing at random (MAR) and MNAR. Current strategies rely on comparing dropout proportions and covariate distributions, and using auxiliary information to assess the likelihood of dropout being associated with the outcome. We propose using the observed variance difference across trial arms as a tool for assessing the risk of dropout being MNAR in RCTs with continuous outcomes. In an RCT, at randomization, the distributions of all covariates should be equal in the populations randomized to the intervention and control arms. Under the assumption of homogeneous treatment effects and homoskedastic outcome errors, the variance of the outcome will also be equal in the two populations over the course of follow-up. We show that under MAR dropout, the observed outcome variances, conditional on the variables included in the model, are equal across trial arms, whereas MNAR dropout may result in unequal variances. Consequently, unequal observed conditional trial arm variances are an indicator of MNAR dropout and possible bias of the estimated treatment effect. Heterogeneous treatment effects or heteroskedastic outcome errors are another potential cause of observing different outcome variances. We show that for longitudinal data, we can isolate the effect of MNAR outcome-dependent dropout by considering the variance difference at baseline in the same set of patients who are observed at final follow-up. We illustrate our method in simulation for CCA and MI, and in applications using individual-level data and summary data.},
  archive      = {J_BIMJ},
  author       = {Audinga-Dea Hazewinkel and Kate Tilling and Kaitlin H. Wade and Tom Palmer},
  doi          = {10.1002/bimj.202200116},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200116},
  shortjournal = {Bio. J.},
  title        = {Trial arm outcome variance difference after dropout as an indicator of missing-not-at-random bias in randomized controlled trials},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size planning for multiple contrast tests.
<em>BIMJ</em>, <em>65</em>(8), 2200081. (<a
href="https://doi.org/10.1002/bimj.202200081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sample size calculations for two (independent) samples are well established and applied in (pre-)clinical research. When planning several samples, which is common in, for example, preclinical studies, sample size planning tools based on analysis of variance methods are available. Since the underlying effect sizes of these methods are often hard to interpret and to provide for the sample size planning, we employ multiple contrast test procedures for sample size computations in both parametric (under normality assumption) and nonparametric designs using Steel-type tests. Since the exact distributions of the test statistics are unknown under the alternative and variance heterogeneity, we use approximate solutions. Furthermore, since no closed formula for the sample size is available, we use numerical approximations for their computation. Extensive simulation studies are finally conducted to assess the quality of the approximations. It turns out that the methods are accurate in the sense that the multiple contrast test procedures reach the target power to detect the alternative of interest with the sample size computed. The developed procedures are a valuable tool to plan (pre-)clinical trials with several samples and are easily accessible in publicly available software.},
  archive      = {J_BIMJ},
  author       = {Anna Pöhlmann and Frank Konietschke},
  doi          = {10.1002/bimj.202200081},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2200081},
  shortjournal = {Bio. J.},
  title        = {Sample size planning for multiple contrast tests},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A zero-inflated endemic–epidemic model with an application
to measles time series in germany. <em>BIMJ</em>, <em>65</em>(8),
2100408. (<a href="https://doi.org/10.1002/bimj.202100408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data with an excess of zeros are often encountered when modeling infectious disease occurrence. The degree of zero inflation can vary over time due to nonepidemic periods as well as by age group or region. A well-established approach to analyze multivariate incidence time series is the endemic–epidemic modeling framework, also known as the HHH approach. However, it assumes Poisson or negative binomial distributions and is thus not tailored to surveillance data with excess zeros. Here, we propose a multivariate zero-inflated endemic–epidemic model with random effects that extends HHH. Parameters of both the zero-inflation probability and the HHH part of this mixture model can be estimated jointly and efficiently via (penalized) maximum likelihood inference using analytical derivatives. We found proper convergence and good coverage of confidence intervals in simulation studies. An application to measles counts in the 16 German states, 2005–2018, showed that zero inflation is more pronounced in the Eastern states characterized by a higher vaccination coverage. Probabilistic forecasts of measles cases improved when accounting for zero inflation. We anticipate zero-inflated HHH models to be a useful extension also for other applications and provide an implementation in an R package.},
  archive      = {J_BIMJ},
  author       = {Junyi Lu and Sebastian Meyer},
  doi          = {10.1002/bimj.202100408},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2100408},
  shortjournal = {Bio. J.},
  title        = {A zero-inflated endemic–epidemic model with an application to measles time series in germany},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating causal effects in observational studies for
survival data with a cure fraction using propensity score adjustment.
<em>BIMJ</em>, <em>65</em>(8), 2100357. (<a
href="https://doi.org/10.1002/bimj.202100357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies, covariates are often confounding factors for treatment assignment. Such covariates need to be adjusted to estimate the causal treatment effect. For observational studies with survival outcomes, it is usually more challenging to adjust for the confounding covariates for causal effect estimation because of censoring. The challenge becomes even thornier when there exists a nonignorable cure fraction in the population. In this paper, we propose a causal effect estimation approach in observational studies for survival data with a cure fraction. We extend the absolute treatment effects on survival outcomes—including the restricted average causal effect and SPCE—to survival outcomes with cure fractions, and construct the corresponding causal effect estimators based on propensity score stratification. We prove the asymptotic properties of the proposed estimators and conduct simulation studies to evaluate their performances. As an illustration, the method is applied to a stomach cancer study.},
  archive      = {J_BIMJ},
  author       = {Ziwen Wang and Chenguang Wang and Xiaoguang Wang},
  doi          = {10.1002/bimj.202100357},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2100357},
  shortjournal = {Bio. J.},
  title        = {Estimating causal effects in observational studies for survival data with a cure fraction using propensity score adjustment},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M-quantile regression shrinkage and selection via the lasso
and elastic net to assess the effect of meteorology and traffic on air
quality. <em>BIMJ</em>, <em>65</em>(8), 2100355. (<a
href="https://doi.org/10.1002/bimj.202100355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we intersect data on size-selected particulate matter (PM) with vehicular traffic counts and a comprehensive set of meteorological covariates to study the effect of traffic on air quality. To this end, we develop an M-quantile regression model with Lasso and Elastic Net penalizations. This allows (i) to identify the best proxy for vehicular traffic via model selection, (ii) to investigate the relationship between fine PM concentration and the covariates at different M-quantiles of the conditional response distribution, and (iii) to be robust to the presence of outliers. Heterogeneity in the data is accounted by fitting a B-spline on the effect of the day of the year. Analytic and bootstrap-based variance estimates of the regression coefficients are provided, together with a numerical evaluation of the proposed estimation procedure. Empirical results show that atmospheric stability is responsible for the most significant effect on fine PM concentration: this effect changes at different levels of the conditional response distribution and is relatively weaker on the tails. On the other hand, model selection allows to identify the best proxy for vehicular traffic whose effect remains essentially the same at different levels of the conditional response distribution.},
  archive      = {J_BIMJ},
  author       = {M. Giovanna Ranalli and Nicola Salvati and Lea Petrella and Francesco Pantalone},
  doi          = {10.1002/bimj.202100355},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2100355},
  shortjournal = {Bio. J.},
  title        = {M-quantile regression shrinkage and selection via the lasso and elastic net to assess the effect of meteorology and traffic on air quality},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian approach for mixed effects state-space models
under skewness and heavy tails. <em>BIMJ</em>, <em>65</em>(8), 2100302.
(<a href="https://doi.org/10.1002/bimj.202100302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human immunodeficiency virus (HIV) dynamics have been the focus of epidemiological and biostatistical research during the past decades to understand the progression of acquired immunodeficiency syndrome (AIDS) in the population. Although there are several approaches for modeling HIV dynamics, one of the most popular is based on Gaussian mixed-effects models because of its simplicity from the implementation and interpretation viewpoints. However, in some situations, Gaussian mixed-effects models cannot (a) capture serial correlation existing in longitudinal data, (b) deal with missing observations properly, and (c) accommodate skewness and heavy tails frequently presented in patients&#39; profiles. For those cases, mixed-effects state-space models (MESSM) become a powerful tool for modeling correlated observations, including HIV dynamics, because of their flexibility in modeling the unobserved states and the observations in a simple way. Consequently, our proposal considers an MESSM where the observations&#39; error distribution is a skew- t . This new approach is more flexible and can accommodate data sets exhibiting skewness and heavy tails. Under the Bayesian paradigm, an efficient Markov chain Monte Carlo algorithm is implemented. To evaluate the properties of the proposed models, we carried out some exciting simulation studies, including missing data in the generated data sets. Finally, we illustrate our approach with an application in the AIDS Clinical Trial Group Study 315 (ACTG-315) clinical trial data set.},
  archive      = {J_BIMJ},
  author       = {Lina L. Hernandez-Velasco and Carlos A. Abanto-Valle and Dipak K. Dey and Luis M. Castro},
  doi          = {10.1002/bimj.202100302},
  journal      = {Biometrical Journal},
  month        = {12},
  number       = {8},
  pages        = {2100302},
  shortjournal = {Bio. J.},
  title        = {A bayesian approach for mixed effects state-space models under skewness and heavy tails},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023g). Cover picture: Biometrical journal 7’23. <em>BIMJ</em>,
<em>65</em>(7), 2370071. (<a
href="https://doi.org/10.1002/bimj.202370071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370071},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2370071},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 7&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating cancer screening programs using survival
analysis. <em>BIMJ</em>, <em>65</em>(7), 2200344. (<a
href="https://doi.org/10.1002/bimj.202200344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main purpose of cancer screening programs is to provide early treatment to patients that are diagnosed with cancer on a screening test, thus increasing their chances of survival. To test this hypothesis directly, one should compare the survival of screen-detected cases to the survival of their counterparts not included to the program. In this study, we develop a general notation and use it to formally define the comparison of interest. We explain why the naive comparison between screen-detected and interval cases is biased and show that the total bias that arises in this case can be decomposed as a sum of lead time bias, length time bias, and bias due to overdetection. With respect to the estimation, we show what can be estimated using existing methods. To fill in the missing gap, we develop a new nonparametric estimator that allows us to estimate the survival of the control group, that is, the survival of cancer cases that would be screen-detected among those not included to the program. By joining the proposed estimator with existing methods, we show that the contrast of interest can be estimated without neglecting any of the biases. Our approach is illustrated using simulations and empirical data.},
  archive      = {J_BIMJ},
  author       = {Bor Vratanar and Maja Pohar Perme},
  doi          = {10.1002/bimj.202200344},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200344},
  shortjournal = {Bio. J.},
  title        = {Evaluating cancer screening programs using survival analysis},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adjusted coefficient of determination (R2) for
generalized linear mixed models in one go. <em>BIMJ</em>,
<em>65</em>(7), 2200290. (<a
href="https://doi.org/10.1002/bimj.202200290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coefficient of determination ( R 2 ) is a common measure of goodness of fit for linear models. Various proposals have been made for extension of this measure to generalized linear and mixed models. When the model has random effects or correlated residual effects, the observed responses are correlated. This paper proposes a new coefficient of determination for this setting that accounts for any such correlation. A key advantage of the proposed method is that it only requires the fit of the model under consideration, with no need to also fit a null model. Also, the approach entails a bias correction in the estimator assessing the variance explained by fixed effects. Three examples are used to illustrate new measure. A simulation shows that the proposed estimator of the new coefficient of determination has only minimal bias.},
  archive      = {J_BIMJ},
  author       = {Hans-Peter Piepho},
  doi          = {10.1002/bimj.202200290},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200290},
  shortjournal = {Bio. J.},
  title        = {An adjusted coefficient of determination (R2) for generalized linear mixed models in one go},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian adaptive design for dual-agent phase i–II
oncology trials integrating efficacy data across stages. <em>BIMJ</em>,
<em>65</em>(7), 2200288. (<a
href="https://doi.org/10.1002/bimj.202200288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combination of several anticancer treatments has typically been presumed to have enhanced drug activity. Motivated by a real clinical trial, this paper considers phase I–II dose finding designs for dual-agent combinations, where one main objective is to characterize both the toxicity and efficacy profiles. We propose a two-stage Bayesian adaptive design that accommodates a change of patient population in-between. In stage I, we estimate a maximum tolerated dose combination using the escalation with overdose control (EWOC) principle. This is followed by a stage II, conducted in a new yet relevant patient population, to find the most efficacious dose combination. We implement a robust Bayesian hierarchical random-effects model to allow sharing of information on the efficacy across stages, assuming that the related parameters are either exchangeable or nonexchangeable. Under the assumption of exchangeability, a random-effects distribution is specified for the main effects parameters to capture uncertainty about the between-stage differences. The inclusion of nonexchangeability assumption further enables that the stage-specific efficacy parameters have their own priors. The proposed methodology is assessed with an extensive simulation study. Our results suggest a general improvement of the operating characteristics for the efficacy assessment, under a conservative assumption about the exchangeability of the parameters a priori.},
  archive      = {J_BIMJ},
  author       = {José L. Jiménez and Haiyan Zheng},
  doi          = {10.1002/bimj.202200288},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200288},
  shortjournal = {Bio. J.},
  title        = {A bayesian adaptive design for dual-agent phase I–II oncology trials integrating efficacy data across stages},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exposing the confounding in experimental designs to
understand and evaluate them, and formulating linear mixed models for
analyzing the data from a designed experiment. <em>BIMJ</em>,
<em>65</em>(7), 2200284. (<a
href="https://doi.org/10.1002/bimj.202200284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparative experiments involve the allocation of treatments to units, ideally by randomization. This necessarily confounds treatment information with unit information, which we distinguish from the other forms of information blending, in particular aliasing and marginality. We outline a factor-allocation paradigm for describing experimental designs with the aim of (i) exhibiting the confounding in a design, using analysis-of-variance-like tables, so as to understand and evaluate the design and (ii) formulating a linear mixed model based on the factor allocation that the design involves. The approach exhibits the dispersal of treatments information between units sources, allows designers a choice in the strategy that they adopt for including block-treatment interactions, clarifies differences between experiments, accommodates systematic allocation of factors, and provides a consolidated analysis of nonorthogonal designs. It provides insights into the process of designing experiments and issues that commonly arise with designs. The paradigm has pedagogical advantages and is implemented using the R package dae .},
  archive      = {J_BIMJ},
  author       = {Christopher James Brien and Renata Alcarde Sermarini and Clarice Garcia Borges Demétrio},
  doi          = {10.1002/bimj.202200284},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200284},
  shortjournal = {Bio. J.},
  title        = {Exposing the confounding in experimental designs to understand and evaluate them, and formulating linear mixed models for analyzing the data from a designed experiment},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the prevalence of two or more diseases using
outcomes from multiplex group testing. <em>BIMJ</em>, <em>65</em>(7),
2200270. (<a href="https://doi.org/10.1002/bimj.202200270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When screening a population for infectious diseases, pooling individual specimens (e.g., blood, swabs, urine, etc.) can provide enormous cost savings when compared to testing specimens individually. In the biostatistics literature, testing pools of specimens is commonly known as group testing or pooled testing. Although estimating a population-level prevalence with group testing data has received a large amount of attention, most of this work has focused on applications involving a single disease, such as human immunodeficiency virus. Modern methods of screening now involve testing pools and individuals for multiple diseases simultaneously through the use of multiplex assays. Hou et al. (2017, Biometrics , 73 , 656–665) and Hou et al. (2020, Biostatistics , 21 , 417–431) recently proposed group testing protocols for multiplex assays and derived relevant case identification characteristics, including the expected number of tests and those which quantify classification accuracy. In this article, we describe Bayesian methods to estimate population-level disease probabilities from implementing these protocols or any other multiplex group testing protocol which might be carried out in practice. Our estimation methods can be used with multiplex assays for two or more diseases while incorporating the possibility of test misclassification for each disease. We use chlamydia and gonorrhea testing data collected at the State Hygienic Laboratory at the University of Iowa to illustrate our work. We also provide an online R resource practitioners can use to implement the methods in this article.},
  archive      = {J_BIMJ},
  author       = {Md S. Warasi and Joshua M. Tebbs and Christopher S. McMahan and Christopher R. Bilder},
  doi          = {10.1002/bimj.202200270},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200270},
  shortjournal = {Bio. J.},
  title        = {Estimating the prevalence of two or more diseases using outcomes from multiplex group testing},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TITE-gBOIN-ET: Time-to-event generalized bayesian optimal
interval design to accelerate dose-finding accounting for ordinal graded
efficacy and toxicity outcomes. <em>BIMJ</em>, <em>65</em>(7), 2200265.
(<a href="https://doi.org/10.1002/bimj.202200265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the primary objectives of an oncology dose-finding trial for novel therapies, such as molecular-targeted agents and immune-oncology therapies, is to identify an optimal dose (OD) that is tolerable and therapeutically beneficial for subjects in subsequent clinical trials. These new therapeutic agents appear more likely to induce multiple low or moderate-grade toxicities than dose-limiting toxicities. Besides, for efficacy, evaluating the overall response and long-term stable disease in solid tumors and considering the difference between complete remission and partial remission in lymphoma are preferable. It is also essential to accelerate early-stage trials to shorten the entire period of drug development. However, it is often challenging to make real-time adaptive decisions due to late-onset outcomes, fast accrual rates, and differences in outcome evaluation periods for efficacy and toxicity. To solve the issues, we propose a time-to-event generalized Bayesian optimal interval design to accelerate dose finding, accounting for efficacy and toxicity grades. The new design named “TITE-gBOIN-ET” design is model-assisted and straightforward to implement in actual oncology dose-finding trials. Simulation studies show that the TITE-gBOIN-ET design significantly shortens the trial duration compared with the designs without sequential enrollment while having comparable or higher performance in the percentage of correct OD selection and the average number of patients allocated to the ODs across various realistic settings.},
  archive      = {J_BIMJ},
  author       = {Kentaro Takeda and Yusuke Yamaguchi and Masataka Taguri and Satoshi Morita},
  doi          = {10.1002/bimj.202200265},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200265},
  shortjournal = {Bio. J.},
  title        = {TITE-gBOIN-ET: Time-to-event generalized bayesian optimal interval design to accelerate dose-finding accounting for ordinal graded efficacy and toxicity outcomes},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biomarker-based precision dose finding for immunotherapy
combined with radiotherapy. <em>BIMJ</em>, <em>65</em>(7), 2200246. (<a
href="https://doi.org/10.1002/bimj.202200246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent success of sequential administration of immunotherapy following radiotherapy (RT), often referred to as immunoRT, has sparked the urgent need for novel clinical trial designs to accommodate the unique features of immunoRT. For this purpose, we propose a Bayesian phase I/II design for immunotherapy administered after standard-dose RT to identify the optimal dose that is personalized for each patient according to his/her measurements of PD-L1 expression at baseline and post-RT. We model the immune response, toxicity, and efficacy as functions of dose and patient&#39;s baseline and post-RT PD-L1 expression profile. We quantify the desirability of the dose using a utility function and propose a two-stage dose-finding algorithm to find the personalized optimal dose. Simulation studies show that our proposed design has good operating characteristics, with a high probability of identifying the personalized optimal dose.},
  archive      = {J_BIMJ},
  author       = {Li-Hsiang Lin and Yan Han and Rui Zhang and Beibei Guo},
  doi          = {10.1002/bimj.202200246},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200246},
  shortjournal = {Bio. J.},
  title        = {Biomarker-based precision dose finding for immunotherapy combined with radiotherapy},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive data collection for intraindividual studies
affected by adherence. <em>BIMJ</em>, <em>65</em>(7), 2200203. (<a
href="https://doi.org/10.1002/bimj.202200203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the use of mobile technologies in ecological momentary assessments (EMAs) and interventions has made it easier to collect data suitable for intraindividual variability studies in the medical field. Nevertheless, especially when self-reports are used during the data collection process, there are difficulties in balancing data quality and the burden placed on the subject. In this paper, we address this problem for a specific EMA setting that aims to submit a demanding task to subjects at high/low values of a self-reported variable. We adopt a dynamic approach inspired by control chart methods and design optimization techniques to obtain an EMA triggering mechanism for data collection that considers both the individual variability of the self-reported variable and of the adherence. We test the algorithm in both a simulation setting and with real, large-scale data from a tinnitus longitudinal study. A Wilcoxon signed rank test shows that the algorithm tends to have both a higher F 1 score and utility than a random schedule and a rule-based algorithm with static thresholds, which are the current state-of-the-art approaches. In conclusion, the algorithm is proven effective in balancing data quality and the burden placed on the participants, especially in studies where data collection is impacted by adherence.},
  archive      = {J_BIMJ},
  author       = {Greta Monacelli and Lili Zhang and Winfried Schlee and Berthold Langguth and Tomás E. Ward and Thomas B. Murphy},
  doi          = {10.1002/bimj.202200203},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200203},
  shortjournal = {Bio. J.},
  title        = {Adaptive data collection for intraindividual studies affected by adherence},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exact sample size determination for a single poisson random
sample. <em>BIMJ</em>, <em>65</em>(7), 2200183. (<a
href="https://doi.org/10.1002/bimj.202200183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical power analysis for sample size determination is typically performed in clinical trials. A “hybrid” classical Bayesian or a “fully Bayesian” approach can be alternatively used in order to add flexibility to the design assumptions needed at the planning stage of the study and to explicitly incorporate prior information in the procedure. In this paper, we exploit and compare these approaches to obtain the optimal sample size of a single-arm trial based on Poisson data. We adopt exact methods to establish the rejection of the null hypothesis within a frequentist or a Bayesian perspective and suggest the use of a conservative criterion for sample size determination that accounts for the not strictly monotonic behavior of the power function in the presence of discrete data. A Shiny web app in R has been developed to provide a user-friendly interface to easily compute the optimal sample size according to the proposed criteria and to assure the reproducibility of the results.},
  archive      = {J_BIMJ},
  author       = {Susanna Gentile and Valeria Sambucini},
  doi          = {10.1002/bimj.202200183},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200183},
  shortjournal = {Bio. J.},
  title        = {Exact sample size determination for a single poisson random sample},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous confidence intervals from randomization tests
with application in testing bioequivalence with multiple endpoints.
<em>BIMJ</em>, <em>65</em>(7), 2200082. (<a
href="https://doi.org/10.1002/bimj.202200082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method to construct simultaneous confidence intervals for a parameter vector from inverting a series of randomization tests (RT). The randomization tests are facilitated by an efficient multivariate Robbins–Monro procedure that takes the correlation information of all components into account. The estimation method does not require any distributional assumption of the population other than the existence of the second moments. The resulting simultaneous confidence intervals are not necessarily symmetric about the point estimate of the parameter vector but possess the property of equal tails in all dimensions. In particular, we present the constructing the mean vector of one population and the difference between two mean vectors of two populations. Extensive simulation is conducted to show numerical comparison with four methods. We illustrate the application of the proposed method to test bioequivalence with multiple endpoints on some real data.},
  archive      = {J_BIMJ},
  author       = {Abdisa G. Dufera and Cui Xiong and Jin Xu},
  doi          = {10.1002/bimj.202200082},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200082},
  shortjournal = {Bio. J.},
  title        = {Simultaneous confidence intervals from randomization tests with application in testing bioequivalence with multiple endpoints},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smoothed quantile regression for partially functional linear
models in high dimensions. <em>BIMJ</em>, <em>65</em>(7), 2200060. (<a
href="https://doi.org/10.1002/bimj.202200060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Practitioners of current data analysis are regularly confronted with the situation where the heavy-tailed skewed response is related to both multiple functional predictors and high-dimensional scalar covariates. We propose a new class of partially functional penalized convolution-type smoothed quantile regression to characterize the conditional quantile level between a scalar response and predictors of both functional and scalar types. The new approach overcomes the lack of smoothness and severe convexity of the standard quantile empirical loss, considerably improving the computing efficiency of partially functional quantile regression. We investigate a folded concave penalized estimator for simultaneous variable selection and estimation by the modified local adaptive majorize-minimization (LAMM) algorithm. The functional predictors can be dense or sparse and are approximated by the principal component basis. Under mild conditions, the consistency and oracle properties of the resulting estimators are established. Simulation studies demonstrate a competitive performance against the partially functional standard penalized quantile regression. A real application using Alzheimer&#39;s Disease Neuroimaging Initiative data is utilized to illustrate the practicality of the proposed model.},
  archive      = {J_BIMJ},
  author       = {Zhihao Wang and Yongxin Bai and Wolfgang K. Härdle and Maozai Tian},
  doi          = {10.1002/bimj.202200060},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200060},
  shortjournal = {Bio. J.},
  title        = {Smoothed quantile regression for partially functional linear models in high dimensions},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparison of statistical models to predict age-standardized
cancer incidence in switzerland. <em>BIMJ</em>, <em>65</em>(7), 2200046.
(<a href="https://doi.org/10.1002/bimj.202200046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study compares the performance of statistical methods for predicting age-standardized cancer incidence, including Poisson generalized linear models, age-period-cohort (APC) and Bayesian age-period-cohort (BAPC) models, autoregressive integrated moving average (ARIMA) time series, and simple linear models. The methods are evaluated via leave-future-out cross-validation, and performance is assessed using the normalized root mean square error, interval score, and coverage of prediction intervals. Methods were applied to cancer incidence from the three Swiss cancer registries of Geneva, Neuchatel, and Vaud combined, considering the five most frequent cancer sites: breast, colorectal, lung, prostate, and skin melanoma and bringing all other sites together in a final group. Best overall performance was achieved by ARIMA models, followed by linear regression models. Prediction methods based on model selection using the Akaike information criterion resulted in overfitting. The widely used APC and BAPC models were found to be suboptimal for prediction, particularly in the case of a trend reversal in incidence, as it was observed for prostate cancer. In general, we do not recommend predicting cancer incidence for periods far into the future but rather updating predictions regularly.},
  archive      = {J_BIMJ},
  author       = {Bastien Trächsel and Valentin Rousson and Jean-Luc Bulliard and Isabella Locatelli},
  doi          = {10.1002/bimj.202200046},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2200046},
  shortjournal = {Bio. J.},
  title        = {Comparison of statistical models to predict age-standardized cancer incidence in switzerland},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A frequentist approach to dynamic borrowing. <em>BIMJ</em>,
<em>65</em>(7), 2100406. (<a
href="https://doi.org/10.1002/bimj.202100406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been growing interest in leveraging external control data to augment a randomized control group data in clinical trials and enable more informative decision making. In recent years, the quality and availability of real-world data have improved steadily as external controls. However, information borrowing by directly pooling such external controls with randomized controls may lead to biased estimates of the treatment effect. Dynamic borrowing methods under the Bayesian framework have been proposed to better control the false positive error. However, the numerical computation and, especially, parameter tuning, of those Bayesian dynamic borrowing methods remain a challenge in practice. In this paper, we present a frequentist interpretation of a Bayesian commensurate prior borrowing approach and describe intrinsic challenges associated with this method from the perspective of optimization. Motivated by this observation, we propose a new dynamic borrowing approach using adaptive lasso. The treatment effect estimate derived from this method follows a known asymptotic distribution, which can be used to construct confidence intervals and conduct hypothesis tests. The finite sample performance of the method is evaluated through extensive Monte Carlo simulations under different settings. We observed highly competitive performance of adaptive lasso compared to Bayesian approaches. Methods for selecting tuning parameters are also thoroughly discussed based on results from numerical studies and an illustration example.},
  archive      = {J_BIMJ},
  author       = {Ruilin Li and Ray Lin and Jiangeng Huang and Lu Tian and Jiawen Zhu},
  doi          = {10.1002/bimj.202100406},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2100406},
  shortjournal = {Bio. J.},
  title        = {A frequentist approach to dynamic borrowing},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blinded sample size recalculation in multiple composite
population designs with normal data and baseline adjustments.
<em>BIMJ</em>, <em>65</em>(7), 2000326. (<a
href="https://doi.org/10.1002/bimj.202000326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing interest in subpopulation analysis has led to the development of various new trial designs and analysis methods in the fields of personalized medicine and targeted therapies. In this paper, subpopulations are defined in terms of an accumulation of disjoint population subsets and will therefore be called composite populations. The proposed trial design is applicable to any set of composite populations, considering normally distributed endpoints and random baseline covariates. Treatment effects for composite populations are tested by combining p -values, calculated on the subset levels, using the inverse normal combination function to generate test statistics for those composite populations while the closed testing procedure accounts for multiple testing. Critical boundaries for intersection hypothesis tests are derived using multivariate normal distributions, reflecting the joint distribution of composite population test statistics given no treatment effect exists. For sample size calculation and sample size, recalculation multivariate normal distributions are derived which describe the joint distribution of composite population test statistics under an assumed alternative hypothesis. Simulations demonstrate the absence of any practical relevant inflation of the type I error rate. The target power after sample size recalculation is typically met or close to being met.},
  archive      = {J_BIMJ},
  author       = {Roland G. Gera and Tim Friede},
  doi          = {10.1002/bimj.202000326},
  journal      = {Biometrical Journal},
  month        = {10},
  number       = {7},
  pages        = {2000326},
  shortjournal = {Bio. J.},
  title        = {Blinded sample size recalculation in multiple composite population designs with normal data and baseline adjustments},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023f). Cover picture: Biometrical journal 6’23. <em>BIMJ</em>,
<em>65</em>(6), 2370061. (<a
href="https://doi.org/10.1002/bimj.202370061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370061},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2370061},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 6&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous modeling of multivariate heterogeneous
responses and heteroskedasticity via a two-stage composite likelihood.
<em>BIMJ</em>, <em>65</em>(6), 2200029. (<a
href="https://doi.org/10.1002/bimj.202200029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate heterogeneous responses and heteroskedasticity have attracted increasing attention in recent years. In genome-wide association studies, effective simultaneous modeling of multiple phenotypes would improve statistical power and interpretability. However, a flexible common modeling system for heterogeneous data types can pose computational difficulties. Here we build upon a previous method for multivariate probit estimation using a two-stage composite likelihood that exhibits favorable computational time while retaining attractive parameter estimation properties. We extend this approach to incorporate multivariate responses of heterogeneous data types (binary and continuous), and possible heteroskedasticity. Although the approach has wide applications, it would be particularly useful for genomics, precision medicine, or individual biomedical prediction. Using a genomics example, we explore statistical power and confirm that the approach performs well for hypothesis testing and coverage percentages under a wide variety of settings. The approach has the potential to better leverage genomics data and provide interpretable inference for pleiotropy, in which a locus is associated with multiple traits.},
  archive      = {J_BIMJ},
  author       = {Bryan W. Ting and Fred A. Wright and Yi-Hui Zhou},
  doi          = {10.1002/bimj.202200029},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2200029},
  shortjournal = {Bio. J.},
  title        = {Simultaneous modeling of multivariate heterogeneous responses and heteroskedasticity via a two-stage composite likelihood},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An approximate bayesian approach for estimation of the
instantaneous reproduction number under misreported epidemic data.
<em>BIMJ</em>, <em>65</em>(6), 2200024. (<a
href="https://doi.org/10.1002/bimj.202200024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In epidemic models, the effective reproduction number is of central importance to assess the transmission dynamics of an infectious disease and to orient health intervention strategies. Publicly shared data during an outbreak often suffers from two sources of misreporting (underreporting and delay in reporting) that should not be overlooked when estimating epidemiological parameters. The main statistical challenge in models that intrinsically account for a misreporting process lies in the joint estimation of the time-varying reproduction number and the delay/underreporting parameters. Existing Bayesian approaches typically rely on Markov chain Monte Carlo algorithms that are extremely costly from a computational perspective. We propose a much faster alternative based on Laplacian-P-splines (LPS) that combines Bayesian penalized B-splines for flexible and smooth estimation of the instantaneous reproduction number and Laplace approximations to selected posterior distributions for fast computation. Assuming a known generation interval distribution, the incidence at a given calendar time is governed by the epidemic renewal equation and the delay structure is specified through a composite link framework. Laplace approximations to the conditional posterior of the spline vector are obtained from analytical versions of the gradient and Hessian of the log-likelihood, implying a drastic speed-up in the computation of posterior estimates. Furthermore, the proposed LPS approach can be used to obtain point estimates and approximate credible intervals for the delay and reporting probabilities. Simulation of epidemics with different combinations for the underreporting rate and delay structure (one-day, two-day, and weekend delays) show that the proposed LPS methodology delivers fast and accurate estimates outperforming existing methods that do not take into account underreporting and delay patterns. Finally, LPS is illustrated in two real case studies of epidemic outbreaks.},
  archive      = {J_BIMJ},
  author       = {Oswaldo Gressani and Christel Faes and Niel Hens},
  doi          = {10.1002/bimj.202200024},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2200024},
  shortjournal = {Bio. J.},
  title        = {An approximate bayesian approach for estimation of the instantaneous reproduction number under misreported epidemic data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using real-world data to predict health outcomes—the
prediction design: Application and sample size planning. <em>BIMJ</em>,
<em>65</em>(6), 2200023. (<a
href="https://doi.org/10.1002/bimj.202200023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gold standard for investigating the efficacy of a new therapy is a (pragmatic) randomized controlled trial (RCT). This approach is costly, time-consuming, and not always practicable. At the same time, huge quantities of available patient-level control condition data in analyzable format of (former) RCTs or real-world data (RWD) are neglected. Therefore, alternative study designs are desirable. The design presented here consists of setting up a prediction model for determining treatment effects under the control condition for future patients. When a new treatment is intended to be tested against a control treatment, a single-arm trial for the new therapy is conducted. The treatment effect is then evaluated by comparing the outcomes of the single-arm trial against the predicted outcomes under the control condition. While there are obvious advantages of this design compared to classical RCTs (increased efficiency, lower cost, alleviating participants’ fear of being on control treatment), there are several sources of bias. Our aim is to investigate whether and how such a design—the prediction design—may be used to provide information on treatment effects by leveraging external data sources. For this purpose, we investigated under what assumptions linear prediction models could be used to predict the counterfactual of patients precisely enough to construct a test and an appropriate sample size formula for evaluating the average treatment effect in the population of a new study. A user-friendly R Shiny application (available at: https://web.imbi.uni-heidelberg.de/PredictionDesignR/ ) facilitates the application of the proposed methods, while a real-world application example illustrates them.},
  archive      = {J_BIMJ},
  author       = {Stella Erdmann and Dominic Edelmann and Meinhard Kieser},
  doi          = {10.1002/bimj.202200023},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2200023},
  shortjournal = {Bio. J.},
  title        = {Using real-world data to predict health outcomes—The prediction design: Application and sample size planning},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clustered restricted mean survival time regression.
<em>BIMJ</em>, <em>65</em>(6), 2200002. (<a
href="https://doi.org/10.1002/bimj.202200002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multicenter randomized trials or multilevel observational studies, the Cox regression model has long been the primary approach to study the effects of covariates on time-to-event outcomes. A critical assumption of the Cox model is the proportionality of the hazard functions for modeled covariates, violations of which can result in ambiguous interpretations of the hazard ratio estimates. To address this issue, the restricted mean survival time (RMST), defined as the mean survival time up to a fixed time in a target population, has been recommended as a model-free target parameter. In this article, we generalize the RMST regression model to clustered data by directly modeling the RMST as a continuous function of restriction times with covariates while properly accounting for within-cluster correlations to achieve valid inference. The proposed method estimates regression coefficients via weighted generalized estimating equations, coupled with a cluster-robust sandwich variance estimator to achieve asymptotically valid inference with a sufficient number of clusters. In small-sample scenarios where a limited number of clusters are available, however, the proposed sandwich variance estimator can exhibit negative bias in capturing the variability of regression coefficient estimates. To overcome this limitation, we further propose and examine bias-corrected sandwich variance estimators to reduce the negative bias of the cluster-robust sandwich variance estimator. We study the finite-sample operating characteristics of proposed methods through simulations and reanalyze two multicenter randomized trials.},
  archive      = {J_BIMJ},
  author       = {Xinyuan Chen and Michael O. Harhay and Fan Li},
  doi          = {10.1002/bimj.202200002},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2200002},
  shortjournal = {Bio. J.},
  title        = {Clustered restricted mean survival time regression},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparison of cohort and nested case-control designs for
estimating the effect of time-varying drug exposure on the risk of
adverse event in the presence of ties. <em>BIMJ</em>, <em>65</em>(6),
2100384. (<a href="https://doi.org/10.1002/bimj.202100384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohort and nested case-control (NCC) designs are frequently used in pharmacoepidemiology to assess the associations of drug exposure that can vary over time with the risk of an adverse event. Although it is typically expected that estimates from NCC analyses are similar to those from the full cohort analysis, with moderate loss of precision, only few studies have actually compared their respective performance for estimating the effects of time-varying exposures (TVE). We used simulations to compare the properties of the resulting estimators of these designs for both time-invariant exposure and TVE. We varied exposure prevalence, proportion of subjects experiencing the event, hazard ratio, and control-to-case ratio and considered matching on confounders. Using both designs, we also estimated the real-world associations of time-invariant ever use of menopausal hormone therapy (MHT) at baseline and updated, time-varying MHT use with breast cancer incidence. In all simulated scenarios, the cohort-based estimates had small relative bias and greater precision than the NCC design. NCC estimates displayed bias to the null that decreased with a greater number of controls per case. This bias markedly increased with higher proportion of events. Bias was seen with Breslow&#39;s and Efron&#39;s approximations for handling tied event times but was greatly reduced with the exact method or when NCC analyses were matched on confounders. When analyzing the MHT-breast cancer association, differences between the two designs were consistent with simulated data. Once ties were taken correctly into account, NCC estimates were very similar to those of the full cohort analysis.},
  archive      = {J_BIMJ},
  author       = {Liliane Manitchoko and Michal Abrahamowicz and Pascale Tubert-Bitter and Jacques Benichou and Anne C. M. Thiébaut},
  doi          = {10.1002/bimj.202100384},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2100384},
  shortjournal = {Bio. J.},
  title        = {Comparison of cohort and nested case-control designs for estimating the effect of time-varying drug exposure on the risk of adverse event in the presence of ties},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Influence of cluster-period cells in stepped wedge cluster
randomized trials. <em>BIMJ</em>, <em>65</em>(6), 2100383. (<a
href="https://doi.org/10.1002/bimj.202100383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge cluster randomized trials (SWCRT) are increasingly used for the evaluation of complex interventions in health services research. They randomly allocate treatments to clusters that switch to intervention under investigation at variable time points without returning to control condition. The resulting unbalanced allocation over time periods and the uncertainty about the underlying correlation structures at cluster-level renders designing and analyzing SWCRTs a challenge. Adjusting for time trends is recommended, appropriate parameterizations depend on the particular context. For sample size calculation, the covariance structure and covariance parameters are usually assumed to be known. These assumptions greatly affect the influence single cluster-period cells have on the effect estimate. Thus, it is important to understand how cluster-period cells contribute to the treatment effect estimate. We therefore discuss two measures of cell influence. These are functions of the design characteristics and covariance structure only and can thus be calculated at the planning stage: the coefficient matrix as discussed by Matthews and Forbes and information content (IC) as introduced by Kasza and Forbes. The main result is a new formula for IC that is more general and computationally more efficient. The formula applies to any generalized least squares estimator, especially for any type of time trend adjustment or nonblock diagonal matrices. We further show a functional relationship between IC and the coefficient matrix. We give two examples that tie in with current literature. All discussed tools and methods are implemented in the R package SteppedPower .},
  archive      = {J_BIMJ},
  author       = {Philipp Mildenberger and Jochem König},
  doi          = {10.1002/bimj.202100383},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2100383},
  shortjournal = {Bio. J.},
  title        = {Influence of cluster-period cells in stepped wedge cluster randomized trials},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning and differential equations for modeling
changes in individual-level latent dynamics between observation periods.
<em>BIMJ</em>, <em>65</em>(6), 2100381. (<a
href="https://doi.org/10.1002/bimj.202100381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When modeling longitudinal biomedical data, often dimensionality reduction as well as dynamic modeling in the resulting latent representation is needed. This can be achieved by artificial neural networks for dimension reduction and differential equations for dynamic modeling of individual-level trajectories. However, such approaches so far assume that parameters of individual-level dynamics are constant throughout the observation period. Motivated by an application from psychological resilience research, we propose an extension where different sets of differential equation parameters are allowed for observation subperiods. Still, estimation for intra-individual subperiods is coupled for being able to fit the model also with a relatively small dataset. We subsequently derive prediction targets from individual dynamic models of resilience in the application. These serve as outcomes for predicting resilience from characteristics of individuals, measured at baseline and a follow-up time point, and selecting a small set of important predictors. Our approach is seen to successfully identify individual-level parameters of dynamic models that allow to stably select predictors, that is, resilience factors. Furthermore, we can identify those characteristics of individuals that are the most promising for updates at follow-up, which might inform future study design. This underlines the usefulness of our proposed deep dynamic modeling approach with changes in parameters between observation subperiods.},
  archive      = {J_BIMJ},
  author       = {Göran Köber and Raffael Kalisch and Lara M.C. Puhlmann and Andrea Chmitorz and Anita Schick and Harald Binder},
  doi          = {10.1002/bimj.202100381},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2100381},
  shortjournal = {Bio. J.},
  title        = {Deep learning and differential equations for modeling changes in individual-level latent dynamics between observation periods},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individual risk prediction: Comparing random forests with
cox proportional-hazards model by a simulation study. <em>BIMJ</em>,
<em>65</em>(6), 2100380. (<a
href="https://doi.org/10.1002/bimj.202100380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With big data becoming widely available in healthcare, machine learning algorithms such as random forest (RF) that ignores time-to-event information and random survival forest (RSF) that handles right-censored data are used for individual risk prediction alternatively to the Cox proportional hazards (Cox-PH) model. We aimed to systematically compare RF and RSF with Cox-PH. RSF with three split criteria [log-rank (RSF-LR), log-rank score (RSF-LRS), maximally selected rank statistics (RSF-MSR)]; RF, Cox-PH, and Cox-PH with splines (Cox-S) were evaluated through a simulation study based on real data. One hundred eighty scenarios were investigated assuming different associations between the predictors and the outcome (linear/linear and interactions/nonlinear/nonlinear and interactions), training sample sizes (500/1000/5000), censoring rates (50%/75%/93%), hazard functions (increasing/decreasing/constant), and number of predictors (seven, 15 including noise variables). Methods&#39; performance was evaluated with time-dependent area under curve and integrated Brier score. In all scenarios, RF had the worst performance. In scenarios with a low number of events (⩽70), Cox-PH was at least noninferior to RSF, whereas under linearity assumption it outperformed RSF. Under the presence of interactions, RSF performed better than Cox-PH as the number of events increased whereas Cox-S reached at least similar performance with RSF under nonlinear effects. RSF-LRS performed slightly worse than RSF-LR and RSF-MSR when including noise variables and interaction effects. When applied to real data, models incorporating survival time performed better. Although RSF algorithms are a promising alternative to conventional Cox-PH as data complexity increases, they require a higher number of events for training. In time-to-event analysis, algorithms that consider survival time should be used.},
  archive      = {J_BIMJ},
  author       = {Valia Baralou and Natasa Kalpourtzi and Giota Touloumi},
  doi          = {10.1002/bimj.202100380},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2100380},
  shortjournal = {Bio. J.},
  title        = {Individual risk prediction: Comparing random forests with cox proportional-hazards model by a simulation study},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep transformation models for functional outcome prediction
after acute ischemic stroke. <em>BIMJ</em>, <em>65</em>(6), 2100379. (<a
href="https://doi.org/10.1002/bimj.202100379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many medical applications, interpretable models with high prediction performance are sought. Often, those models are required to handle semistructured data like tabular and image data. We show how to apply deep transformation models (DTMs) for distributional regression that fulfill these requirements. DTMs allow the data analyst to specify (deep) neural networks for different input modalities making them applicable to various research questions. Like statistical models, DTMs can provide interpretable effect estimates while achieving the state-of-the-art prediction performance of deep neural networks. In addition, the construction of ensembles of DTMs that retain model structure and interpretability allows quantifying epistemic and aleatoric uncertainty. In this study, we compare several DTMs, including baseline-adjusted models, trained on a semistructured data set of 407 stroke patients with the aim to predict ordinal functional outcome three months after stroke. We follow statistical principles of model-building to achieve an adequate trade-off between interpretability and flexibility while assessing the relative importance of the involved data modalities. We evaluate the models for an ordinal and dichotomized version of the outcome as used in clinical practice. We show that both tabular clinical and brain imaging data are useful for functional outcome prediction, whereas models based on tabular data only outperform those based on imaging data only. There is no substantial evidence for improved prediction when combining both data modalities. Overall, we highlight that DTMs provide a powerful, interpretable approach to analyzing semistructured data and that they have the potential to support clinical decision-making.},
  archive      = {J_BIMJ},
  author       = {Lisa Herzog and Lucas Kook and Andrea Götschi and Katrin Petermann and Martin Hänsel and Janne Hamann and Oliver Dürr and Susanne Wegener and Beate Sick},
  doi          = {10.1002/bimj.202100379},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2100379},
  shortjournal = {Bio. J.},
  title        = {Deep transformation models for functional outcome prediction after acute ischemic stroke},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling time-varying recruitment rates in multicenter
clinical trials. <em>BIMJ</em>, <em>65</em>(6), 2100377. (<a
href="https://doi.org/10.1002/bimj.202100377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicenter phase II/III clinical trials are large-scale operations that often include hundreds of recruiting centers in several countries. Therefore, the operational aspects of a trial must be thoroughly planned and closely monitored to ensure better oversight and study conduct. Predicting patient recruitment plays a pivotal role in trial monitoring as it informs how many people are expected to be recruited on a given day. As such, study teams may rely on predictions to assess progress and detect any deviations from the original plan that might put the study and potentially, patients at risk. Recruitment predictions are often based on a Poisson-Gamma model that assumes that centers have a constant recruitment rate throughout the trial. The model has useful features and has extensively been used, yet its main assumption is often unrealistic. A nonhomogeneous Poisson process has been recently proposed that can incorporate time-varying recruitment rates. In this work, we predict patient recruitment using both approaches and assess the impact of said assumption in a real-world setting where studies may not necessarily have constant center-specific recruitment rates. The paper showcases experience from modeling recruitment in trials sponsored by AstraZeneca between 2005 and 2018. In these data, there is evidence of time-varying recruitment rates. The predictive performance of models that account for both constant and time-varying recruitment effects is presented. Following a descriptive analysis of data, we assess model performance and investigate the impact of model misspecification.},
  archive      = {J_BIMJ},
  author       = {Aris Perperoglou and Youyi Zhang and Dimitra-Kleio Kipourou},
  doi          = {10.1002/bimj.202100377},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2100377},
  shortjournal = {Bio. J.},
  title        = {Modeling time-varying recruitment rates in multicenter clinical trials},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A note of feature screening via a rank-based coefficient of
correlation. <em>BIMJ</em>, <em>65</em>(6), 2100373. (<a
href="https://doi.org/10.1002/bimj.202100373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature screening is a useful and popular tool to detect informative predictors for ultrahigh-dimensional data before developing statistical analysis or constructing statistical models. While a large body of feature screening procedures has been developed, most methods are restricted to examine either continuous or discrete responses. Moreover, even though many model-free feature screening methods have been proposed, additional assumptions are imposed in those methods to ensure their theoretical results. To address those difficulties and provide simple implementation, in this paper we extend the rank-based coefficient of correlation to develop a feature screening procedure. We show that this new screening criterion is able to deal with continuous and binary responses. Theoretically, the sure screening property is established to justify the proposed method. Simulation studies demonstrate that the predictors with nonlinear and oscillatory trajectories are successfully retained regardless of the distribution of the response. Finally, the proposed method is implemented to analyze two microarray datasets.},
  archive      = {J_BIMJ},
  author       = {Li-Pang Chen},
  doi          = {10.1002/bimj.202100373},
  journal      = {Biometrical Journal},
  month        = {8},
  number       = {6},
  pages        = {2100373},
  shortjournal = {Bio. J.},
  title        = {A note of feature screening via a rank-based coefficient of correlation},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023e). Cover picture: Biometrical journal 5’23. <em>BIMJ</em>,
<em>65</em>(5), 2370051. (<a
href="https://doi.org/10.1002/bimj.202370051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370051},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2370051},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 5&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bayesian approach for subgroup analysis. <em>BIMJ</em>,
<em>65</em>(5), 2200231. (<a
href="https://doi.org/10.1002/bimj.202200231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several penalization approaches have been developed to identify homogeneous subgroups based on a regression model with subject-specific intercepts in subgroup analysis. These methods often apply concave penalty functions to pairwise comparisons of the intercepts, such that the subjects with similar intercept values are assigned to the same group, which is very similar to the procedure of the penalization approaches for variable selection. Since the Bayesian methods are commonly used in variable selection, it is worth considering the corresponding approaches to subgroup analysis in the Bayesian framework. In this paper, a Bayesian hierarchical model with appropriate prior structures is developed for the pairwise differences of intercepts based on a regression model with subject-specific intercepts, which can automatically detect and identify homogeneous subgroups. A Gibbs sampling algorithm is also provided to select the hyperparameter and estimate the intercepts and coefficients of the covariates simultaneously, which is computationally efficient for pairwise comparisons compared to the time-consuming procedures for parameter estimation of the penalization methods (e.g., alternating direction method of multiplier) in the case of large sample sizes. The effectiveness and usefulness of the proposed Bayesian method are evaluated through simulation studies and analysis of a Cleveland Heart Disease Dataset.},
  archive      = {J_BIMJ},
  author       = {Nan Li and Wensheng Zhu},
  doi          = {10.1002/bimj.202200231},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200231},
  shortjournal = {Bio. J.},
  title        = {A bayesian approach for subgroup analysis},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On efficient posterior inference in normalized power prior
bayesian analysis. <em>BIMJ</em>, <em>65</em>(5), 2200194. (<a
href="https://doi.org/10.1002/bimj.202200194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power prior has been widely used to discount the amount of information borrowed from historical data in the design and analysis of clinical trials. It is realized by raising the likelihood function of the historical data to a power parameter δ ∈ [ 0 , 1 ] $\delta \in [0, 1]$ , which quantifies the heterogeneity between the historical and the new study. In a fully Bayesian approach, a natural extension is to assign a hyperprior to δ such that the posterior of δ can reflect the degree of similarity between the historical and current data. To comply with the likelihood principle, an extra normalizing factor needs to be calculated and such prior is known as the normalized power prior. However, the normalizing factor involves an integral of a prior multiplied by a fractional likelihood and needs to be computed repeatedly over different δ during the posterior sampling. This makes its use prohibitive in practice for most elaborate models. This work provides an efficient framework to implement the normalized power prior in clinical studies. It bypasses the aforementioned efforts by sampling from the power prior with and only. Such a posterior sampling procedure can facilitate the use of a random δ with adaptive borrowing capability in general models. The numerical efficiency of the proposed method is illustrated via extensive simulation studies, a toxicological study, and an oncology study.},
  archive      = {J_BIMJ},
  author       = {Zifei Han and Qiang Zhang and Min Wang and Keying Ye and Ming-Hui Chen},
  doi          = {10.1002/bimj.202200194},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200194},
  shortjournal = {Bio. J.},
  title        = {On efficient posterior inference in normalized power prior bayesian analysis},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Buckley–james boosting model based on extreme learning
machine and random survival forests. <em>BIMJ</em>, <em>65</em>(5),
2200153. (<a href="https://doi.org/10.1002/bimj.202200153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Buckley–James (BJ) model is a typical semiparametric accelerated failure time model, which is closely related to the ordinary least squares method and easy to be constructed. However, traditional BJ model built on linearity assumption only captures simple linear relationships, while it has difficulty in processing nonlinear problems. To overcome this difficulty, in this paper, we develop a novel regression model for right-censored survival data within the learning framework of BJ model, basing on random survival forests (RSF), extreme learning machine (ELM), and L 2 boosting algorithm. The proposed method, referred to as ELM-based BJ boosting model, employs RSF for covariates imputation first, then develops a new ensemble of ELMs—ELM-based boosting algorithm for regression by ensemble scheme of L 2 boosting, and finally, uses the output function of the proposed ELM-based boosting model to replace the linear combination of covariates in BJ model. Due to fitting the logarithm of survival time with covariates by the nonparametric ELM-based boosting method instead of the least square method, the ELM-based BJ boosting model can capture both linear covariate effects and nonlinear covariate effects. In both simulation studies and real data applications, in terms of concordance index and integrated Brier sore, the proposed ELM-based BJ boosting model can outperform traditional BJ model, two kinds of BJ boosting models proposed by Wang et al., RSF, and Cox proportional hazards model.},
  archive      = {J_BIMJ},
  author       = {Jianfen Kong and Shuhong Zhang},
  doi          = {10.1002/bimj.202200153},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200153},
  shortjournal = {Bio. J.},
  title        = {Buckley–James boosting model based on extreme learning machine and random survival forests},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hidden population size estimation and diagnostics using two
respondent-driven samples with applications in armenia. <em>BIMJ</em>,
<em>65</em>(5), 2200136. (<a
href="https://doi.org/10.1002/bimj.202200136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the size of hidden populations is essential to understand the magnitude of social and healthcare needs, risk behaviors, and disease burden. However, due to the hidden nature of these populations, they are difficult to survey, and there are no gold standard size estimation methods. Many different methods and variations exist, and diagnostic tools are needed to help researchers assess method-specific assumptions as well as compare between methods. Further, because many necessary mathematical assumptions are unrealistic for real survey implementation, assessment of how robust methods are to deviations from the stated assumptions is essential. We describe diagnostics and assess the performance of a new population size estimation method, capture–recapture with successive sampling population size estimation (CR-SS-PSE), which we apply to data from 3 years of studies from three cities and three hidden populations in Armenia. CR-SS-PSE relies on data from two sequential respondent-driven sampling surveys and extends the successive sampling population size estimation (SS-PSE) framework by using the number of individuals in the overlap between the two surveys and a model for the successive sampling process to estimate population size. We demonstrate that CR-SS-PSE is more robust to violations of successive sampling assumptions than SS-PSE. Further, we compare the CR-SS-PSE estimates to population size estimations using other common methods, including unique object and service multipliers, wisdom of the crowd, and two-source capture–recapture to illustrate volatility across estimation methods.},
  archive      = {J_BIMJ},
  author       = {Brian J. Kim and Lisa G. Johnston and Trdat Grigoryan and Arshak Papoyan and Samvel Grigoryan and Katherine R. McLaughlin},
  doi          = {10.1002/bimj.202200136},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200136},
  shortjournal = {Bio. J.},
  title        = {Hidden population size estimation and diagnostics using two respondent-driven samples with applications in armenia},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A censored quantile regression approach for relative
survival analysis: Relative survival quantile regression. <em>BIMJ</em>,
<em>65</em>(5), 2200127. (<a
href="https://doi.org/10.1002/bimj.202200127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a censored quantile regression model for the analysis of relative survival data. We create a hybrid data set consisting of the study observations and counterpart randomly sampled pseudopopulation observations imputed from population life tables that adjust for expected mortality. We then fit a censored quantile regression model to the hybrid data incorporating demographic variables (e.g., age, biologic sex, calendar time) corresponding to the population life tables of demographically-similar individuals, a population versus study covariate, and its interactions with the variables of interest. These latter variables can be interpreted as relative survival parameters that depict the differences in failure quantiles between the study participants and their population counterparts.},
  archive      = {J_BIMJ},
  author       = {John M. Williamson and Hung-Mo Lin and Robert H. Lyles},
  doi          = {10.1002/bimj.202200127},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200127},
  shortjournal = {Bio. J.},
  title        = {A censored quantile regression approach for relative survival analysis: Relative survival quantile regression},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal effect estimation for multivariate continuous
treatments. <em>BIMJ</em>, <em>65</em>(5), 2200122. (<a
href="https://doi.org/10.1002/bimj.202200122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference is widely used in various fields, such as biology, psychology, and economics, etc. In observational studies, balancing the covariates is an important step in estimating the causal effect. This study extends the one-dimensional entropy balancing method to multiple dimensions to balance the covariates. Both parametric and nonparametric methods are proposed to estimate the causal effect of multivariate continuous treatments and theoretical properties of the two estimations are provided. Furthermore, the simulation results show that the proposed method is better than other methods in various cases. Finally, the proposed method is applied to analyze the impact of the duration and frequency of smoking on medical expenditure. The results from the parametric method indicate that the frequency of smoking increases medical expenditure while the duration of smoking does not. The results from the nonparametric method indicate that there is a short-term downward trend and then a long-term upward trend as the duration and frequency of smoking increase.},
  archive      = {J_BIMJ},
  author       = {Juan Chen and Yingchun Zhou},
  doi          = {10.1002/bimj.202200122},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200122},
  shortjournal = {Bio. J.},
  title        = {Causal effect estimation for multivariate continuous treatments},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of the multilevel MIMIC model to the multilevel
regression and mixed ANOVA model for the estimation and testing of a
cross-level interaction effect: A simulation study. <em>BIMJ</em>,
<em>65</em>(5), 2200112. (<a
href="https://doi.org/10.1002/bimj.202200112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When observing data on a patient-reported outcome measure in, for example, clinical trials, the variables observed are often correlated and intended to measure a latent variable. In addition, such data are also often characterized by a hierarchical structure, meaning that the outcome is repeatedly measured within patients. To analyze such data, it is important to use an appropriate statistical model, such as structural equation modeling (SEM). However, researchers may rely on simpler statistical models that are applied to an aggregated data structure. For example, correlated variables are combined into one sum score that approximates a latent variable. This may have implications when, for example, the sum score consists of indicators that relate differently to the latent variable being measured. This study compares three models that can be applied to analyze such data: the multilevel multiple indicators multiple causes (ML-MIMIC) model, a univariate multilevel model, and a mixed analysis of variance (ANOVA) model. The focus is on the estimation of a cross-level interaction effect that presents the difference over time on the patient-reported outcome between two treatment groups. The ML-MIMIC model is an SEM-type model that considers the relationship between the indicators and the latent variable in a multilevel setting, whereas the univariate multilevel and mixed ANOVA model rely on sum scores to approximate the latent variable. In addition, the mixed ANOVA model uses aggregated second-level means as outcome. This study showed that the ML-MIMIC model produced unbiased cross-level interaction effect estimates when the relationships between the indicators and the latent variable being measured varied across indicators. In contrast, under similar conditions, the univariate multilevel and mixed ANOVA model underestimated the cross-level interaction effect.},
  archive      = {J_BIMJ},
  author       = {Rob Kessels and Mirjam Moerbeek},
  doi          = {10.1002/bimj.202200112},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200112},
  shortjournal = {Bio. J.},
  title        = {A comparison of the multilevel MIMIC model to the multilevel regression and mixed ANOVA model for the estimation and testing of a cross-level interaction effect: A simulation study},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous confidence intervals for quantile differences
of several heterogeneous normal populations: With application to vitamin
d supplement treatment on colorectal cancer patients. <em>BIMJ</em>,
<em>65</em>(5), 2200083. (<a
href="https://doi.org/10.1002/bimj.202200083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, comparing the q-quantiles of several normal populations are more advantageous than comparing their means. In this paper, we consider the problem of constructing simultaneous confidence intervals (SCIs) for quantile differences of several heterogeneous normal distributions. To the best of our knowledge, this problem remains unsolved. We propose a novel method for constructing a set of SCI. We propose two new sets of SCI by using the proposed technique and discuss two classic and two simulation-based SCIs. We show that the proposed classic SCIs are conservative for all population parameters configuration. We also show that the simulation-based SCIs have correct coverage probability asymptotically. We then compare these six sets of SCI in terms of average volume and coverage probability via an extensive simulation study. Results show that one of the proposed classic SCI can be recommended. Finally, the proposed methods are illustrated by a real example that is a vitamin D study on colorectal cancer patients.},
  archive      = {J_BIMJ},
  author       = {Ahad Malekzadeh and Mahmood Kharrati-Kopaei},
  doi          = {10.1002/bimj.202200083},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200083},
  shortjournal = {Bio. J.},
  title        = {Simultaneous confidence intervals for quantile differences of several heterogeneous normal populations: With application to vitamin d supplement treatment on colorectal cancer patients},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the use of cross-validation for the calibration of the
adaptive lasso. <em>BIMJ</em>, <em>65</em>(5), 2200047. (<a
href="https://doi.org/10.1002/bimj.202200047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-validation is the standard method for hyperparameter tuning, or calibration, of machine learning algorithms. The adaptive lasso is a popular class of penalized approaches based on weighted L 1 -norm penalties, with weights derived from an initial estimate of the model parameter. Although it violates the paramount principle of cross-validation, according to which no information from the hold-out test set should be used when constructing the model on the training set, a “naive” cross-validation scheme is often implemented for the calibration of the adaptive lasso. The unsuitability of this naive cross-validation scheme in this context has not been well documented in the literature. In this work, we recall why the naive scheme is theoretically unsuitable and how proper cross-validation should be implemented in this particular context. Using both synthetic and real-world examples and considering several versions of the adaptive lasso, we illustrate the flaws of the naive scheme in practice. In particular, we show that it can lead to the selection of adaptive lasso estimates that perform substantially worse than those selected via a proper scheme in terms of both support recovery and prediction error. In other words, our results show that the theoretical unsuitability of the naive scheme translates into suboptimality in practice, and call for abandoning it.},
  archive      = {J_BIMJ},
  author       = {Nadim Ballout and Lola Etievant and Vivian Viallon},
  doi          = {10.1002/bimj.202200047},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200047},
  shortjournal = {Bio. J.},
  title        = {On the use of cross-validation for the calibration of the adaptive lasso},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hidden markov model for continuous longitudinal data with
missing responses and dropout. <em>BIMJ</em>, <em>65</em>(5), 2200016.
(<a href="https://doi.org/10.1002/bimj.202200016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a hidden Markov model for multivariate continuous longitudinal responses with covariates that accounts for three different types of missing pattern: (I) partially missing outcomes at a given time occasion, (II) completely missing outcomes at a given time occasion (intermittent pattern), and (III) dropout before the end of the period of observation (monotone pattern). The missing-at-random (MAR) assumption is formulated to deal with the first two types of missingness, while to account for the informative dropout, we rely on an extra absorbing state. Estimation of the model parameters is based on the maximum likelihood method that is implemented by an expectation-maximization (EM) algorithm relying on suitable recursions. The proposal is illustrated by a Monte Carlo simulation study and an application based on historical data on primary biliary cholangitis.},
  archive      = {J_BIMJ},
  author       = {Silvia Pandolfi and Francesco Bartolucci and Fulvia Pennoni},
  doi          = {10.1002/bimj.202200016},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2200016},
  shortjournal = {Bio. J.},
  title        = {A hidden markov model for continuous longitudinal data with missing responses and dropout},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mean residual life cure models for right-censored data with
and without length-biased sampling. <em>BIMJ</em>, <em>65</em>(5),
2100368. (<a href="https://doi.org/10.1002/bimj.202100368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a semiparametric mean residual life mixture cure model for right-censored survival data with a cured fraction. The model employs the proportional mean residual life model to describe the effects of covariates on the mean residual time of uncured subjects and the logistic regression model to describe the effects of covariates on the cure rate. We develop estimating equations to estimate the proposed cure model for the right-censored data with and without length-biased sampling, the latter is often found in prevalent cohort studies. In particular, we propose two estimating equations to estimate the effects of covariates in the cure rate and a method to combine them to improve the estimation efficiency. The consistency and asymptotic normality of the proposed estimates are established. The finite sample performance of the estimates is confirmed with simulations. The proposed estimation methods are applied to a clinical trial study on melanoma and a prevalent cohort study on early-onset type 2 diabetes mellitus.},
  archive      = {J_BIMJ},
  author       = {Chyong-Mei Chen and Hsin-Jen Chen and Yingwei Peng},
  doi          = {10.1002/bimj.202100368},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2100368},
  shortjournal = {Bio. J.},
  title        = {Mean residual life cure models for right-censored data with and without length-biased sampling},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monte carlo sensitivity analysis for unmeasured confounding
in dynamic treatment regimes. <em>BIMJ</em>, <em>65</em>(5), 2100359.
(<a href="https://doi.org/10.1002/bimj.202100359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven methods for personalizing treatment assignment have garnered much attention from clinicians and researchers. Dynamic treatment regimes formalize this through a sequence of decision rules that map individual patient characteristics to a recommended treatment. Observational studies are commonly used for estimating dynamic treatment regimes due to the potentially prohibitive costs of conducting sequential multiple assignment randomized trials. However, estimating a dynamic treatment regime from observational data can lead to bias in the estimated regime due to unmeasured confounding. Sensitivity analyses are useful for assessing how robust the conclusions of the study are to a potential unmeasured confounder. A Monte Carlo sensitivity analysis is a probabilistic approach that involves positing and sampling from distributions for the parameters governing the bias. We propose a method for performing a Monte Carlo sensitivity analysis of the bias due to unmeasured confounding in the estimation of dynamic treatment regimes. We demonstrate the performance of the proposed procedure with a simulation study and apply it to an observational study examining tailoring the use of antidepressant medication for reducing symptoms of depression using data from Kaiser Permanente Washington.},
  archive      = {J_BIMJ},
  author       = {Eric J. Rose and Erica E. M. Moodie and Susan M. Shortreed},
  doi          = {10.1002/bimj.202100359},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2100359},
  shortjournal = {Bio. J.},
  title        = {Monte carlo sensitivity analysis for unmeasured confounding in dynamic treatment regimes},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizing treatment effects with incomplete covariates:
Identifying assumptions and multiple imputation algorithms.
<em>BIMJ</em>, <em>65</em>(5), 2100294. (<a
href="https://doi.org/10.1002/bimj.202100294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the problem of generalizing a causal effect estimated on a randomized controlled trial (RCT) to a target population described by a set of covariates from observational data. Available methods such as inverse propensity sampling weighting are not designed to handle missing values, which are however common in both data sources. In addition to coupling the assumptions for causal effect identifiability and for the missing values mechanism and to defining appropriate estimation strategies, one difficulty is to consider the specific structure of the data with two sources and treatment and outcome only available in the RCT. We propose three multiple imputation strategies to handle missing values when generalizing treatment effects, each handling the multisource structure of the problem differently (separate imputation, joint imputation with fixed effect, joint imputation ignoring source information). As an alternative to multiple imputation, we also propose a direct estimation approach that treats incomplete covariates as semidiscrete variables. The multiple imputation strategies and the latter alternative rely on different sets of assumptions concerning the impact of missing values on identifiability. We discuss these assumptions and assess the methods through an extensive simulation study. This work is motivated by the analysis of a large registry of over 20,000 major trauma patients and an RCT studying the effect of tranexamic acid administration on mortality in major trauma patients admitted to intensive care units. The analysis illustrates how the missing values handling can impact the conclusion about the effect generalized from the RCT to the target population.},
  archive      = {J_BIMJ},
  author       = {Imke Mayer and Julie Josse},
  doi          = {10.1002/bimj.202100294},
  journal      = {Biometrical Journal},
  month        = {6},
  number       = {5},
  pages        = {2100294},
  shortjournal = {Bio. J.},
  title        = {Generalizing treatment effects with incomplete covariates: Identifying assumptions and multiple imputation algorithms},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Cover picture: Biometrical journal 4’23. <em>BIMJ</em>,
<em>65</em>(4), 2370041. (<a
href="https://doi.org/10.1002/bimj.202370041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370041},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2370041},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 4&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical issues in drug development by stephen senn,
third edition, new york: John wiley &amp; sons. 2021. 640 p. USD 120.00.
ISBN: 978-1-119-23857-7. <em>BIMJ</em>, <em>65</em>(4), 2300028. (<a
href="https://doi.org/10.1002/bimj.202300028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Martina Mittlböck},
  doi          = {10.1002/bimj.202300028},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2300028},
  shortjournal = {Bio. J.},
  title        = {Statistical issues in drug development by stephen senn, third edition, new york: john wiley &amp; sons. 2021. 640 p. USD 120.00. ISBN: 978-1-119-23857-7},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Confidence intervals for discrete data in clinical research
by vivek pradhan, ashis k. Gangopadhyay, sandeep m. Menon, cynthia basu,
tathagata banerjee. Boca raton, FL: Chapman &amp; hall/CRC press. 2022.
240 pages. ISBN: 9781138048980 (hbk). ISBN 9781315169859 (ebk). List
price: £99.99. <em>BIMJ</em>, <em>65</em>(4), 2300026. (<a
href="https://doi.org/10.1002/bimj.202300026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Tim Friede},
  doi          = {10.1002/bimj.202300026},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2300026},
  shortjournal = {Bio. J.},
  title        = {Confidence intervals for discrete data in clinical research by vivek pradhan, ashis k. gangopadhyay, sandeep m. menon, cynthia basu, tathagata banerjee. boca raton, FL: chapman &amp; Hall/CRC press. 2022. 240 pages. ISBN: 9781138048980 (hbk). ISBN 9781315169859 (ebk). list price: £99.99},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear mixed models. A practical guide using statistical
software by brady t. West, kathleen b. Welch, andrzej t. Gałecki, third
edition, new york: Chapman &amp; hall/CRC. 2022. 489 pages. ISBN:
978-1-0031-8106-4. List price: £ 84.99. <em>BIMJ</em>, <em>65</em>(4),
2300025. (<a href="https://doi.org/10.1002/bimj.202300025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Andreas Ziegler},
  doi          = {10.1002/bimj.202300025},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2300025},
  shortjournal = {Bio. J.},
  title        = {Linear mixed models. a practical guide using statistical software by brady t. west, kathleen b. welch, andrzej t. gałecki, third edition, new york: chapman &amp; Hall/CRC. 2022. 489 pages. ISBN: 978-1-0031-8106-4. list price: £ 84.99},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Letter to the editor regarding the paper “new weighting
methods when cases are only a subset of events in a nested case-control
study” by qian m. Zhou, xuan wang, yingye zheng, and tianxi cai.
<em>BIMJ</em>, <em>65</em>(4), 2200360. (<a
href="https://doi.org/10.1002/bimj.202200360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  author       = {Dominic Edelmann and Kristin Ohneberg and Natalia Becker and Axel Benner and Martin Schumacher},
  doi          = {10.1002/bimj.202200360},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2200360},
  shortjournal = {Bio. J.},
  title        = {Letter to the editor regarding the paper “New weighting methods when cases are only a subset of events in a nested case-control study” by qian m. zhou, xuan wang, yingye zheng, and tianxi cai},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bias-reduced estimators of conditional odds ratios in
matched case-control studies with unmatched confounding. <em>BIMJ</em>,
<em>65</em>(4), 2200133. (<a
href="https://doi.org/10.1002/bimj.202200133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study bias-reduced estimators of exponentially transformed parameters in general linear models (GLMs) and show how they can be used to obtain bias-reduced conditional (or unconditional) odds ratios in matched case-control studies. Two options are considered and compared: the explicit approach and the implicit approach. The implicit approach is based on the modified score function where bias-reduced estimates are obtained by using iterative procedures to solve the modified score equations. The explicit approach is shown to be a one-step approximation of this iterative procedure. To apply these approaches for the conditional analysis of matched case-control studies, with potentially unmatched confounding and with several exposures, we utilize the relation between the conditional likelihood and the likelihood of the unconditional logit binomial GLM for matched pairs and Cox partial likelihood for matched sets with appropriately setup data. The properties of the estimators are evaluated by using a large Monte Carlo simulation study and an illustration of a real dataset is shown. Researchers reporting the results on the exponentiated scale should use bias-reduced estimators since otherwise the effects can be under or overestimated, where the magnitude of the bias is especially large in studies with smaller sample sizes.},
  archive      = {J_BIMJ},
  author       = {Rok Blagus},
  doi          = {10.1002/bimj.202200133},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2200133},
  shortjournal = {Bio. J.},
  title        = {Bias-reduced estimators of conditional odds ratios in matched case-control studies with unmatched confounding},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stratified modestly weighted log-rank tests in settings with
an anticipated delayed separation of survival curves. <em>BIMJ</em>,
<em>65</em>(4), 2200126. (<a
href="https://doi.org/10.1002/bimj.202200126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Delayed separation of survival curves is a common occurrence in confirmatory studies in immuno-oncology. Many novel statistical methods that aim to efficiently capture potential long-term survival improvements have been proposed in recent years. However, the vast majority do not consider stratification, which is a major limitation considering that most large confirmatory studies currently employ a stratified primary analysis. In this article, we combine recently proposed weighted log-rank tests that have been designed to work well under a delayed separation of survival curves, with stratification by a baseline variable. The aim is to increase the efficiency of the test when the stratifying variable is highly prognostic for survival. As there are many potential ways to combine the two techniques, we compare several possibilities in an extensive simulation study. We also apply the techniques retrospectively to two recent randomized clinical trials.},
  archive      = {J_BIMJ},
  author       = {Dominic Magirr and José L. Jiménez},
  doi          = {10.1002/bimj.202200126},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2200126},
  shortjournal = {Bio. J.},
  title        = {Stratified modestly weighted log-rank tests in settings with an anticipated delayed separation of survival curves},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial correlated incidence modeling with zero inflation.
<em>BIMJ</em>, <em>65</em>(4), 2200090. (<a
href="https://doi.org/10.1002/bimj.202200090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease mapping models have been popularly used to model disease incidence with spatial correlation. In disease mapping models, zero inflation is an important issue, which often occurs in disease incidence datasets with high proportions of zero disease count. It is originated from limited survey coverage or unadvanced testing equipment, which makes some regions have no observed patients. Then excessive zeros recorded in the disease incidence dataset would mess up the true distributions of disease incidence and lead to inaccurate estimates. To address this issue, a zero-inflated disease mapping model is developed in this work. In this model, a zero-inflated process using Bernoulli indicators is assumed to characterize whether the zero inflation occurs for each region. For regions without zero inflation, a coherent and generative disease mapping model is applied for mapping the spatially correlated disease incidence. Independent spatial random effects are incorporated in both processes to account for the spatial patterns of zero inflation and disease incidence. External covariates are also considered in both processes to better explain the disease count data. To estimate the model, a Markov chain Monte Carlo algorithm is proposed. We evaluate model performance via a variety of simulation experiments. Finally, a Lyme disease dataset of Virginia is analyzed to illustrate the application of the proposed model.},
  archive      = {J_BIMJ},
  author       = {Feifei Wang and Haofeng Li and Han Wang and Yang Li},
  doi          = {10.1002/bimj.202200090},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2200090},
  shortjournal = {Bio. J.},
  title        = {Spatial correlated incidence modeling with zero inflation},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast approximations of pseudo-observations in the context of
right censoring and interval censoring. <em>BIMJ</em>, <em>65</em>(4),
2200071. (<a href="https://doi.org/10.1002/bimj.202200071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of right-censored and interval-censored data, we develop asymptotic formulas to compute pseudo-observations for the survival function and the restricted mean survival time (RMST). These formulas are based on the original estimators and do not involve computation of the jackknife estimators. For right-censored data, Von Mises expansions of the Kaplan–Meier estimator are used to derive the pseudo-observations. For interval-censored data, a general class of parametric models for the survival function is studied. An asymptotic representation of the pseudo-observations is derived involving the Hessian matrix and the score vector. Theoretical results that justify the use of pseudo-observations in regression are also derived. The formula is illustrated on the piecewise-constant-hazard model for the RMST. The proposed approximations are extremely accurate, even for small sample sizes, as illustrated by Monte Carlo simulations and real data. We also study the gain in terms of computation time, as compared to the original jackknife method, which can be substantial for a large dataset.},
  archive      = {J_BIMJ},
  author       = {Olivier Bouaziz},
  doi          = {10.1002/bimj.202200071},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2200071},
  shortjournal = {Bio. J.},
  title        = {Fast approximations of pseudo-observations in the context of right censoring and interval censoring},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expected life years compared to the general population.
<em>BIMJ</em>, <em>65</em>(4), 2200070. (<a
href="https://doi.org/10.1002/bimj.202200070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For cohorts with long-term follow-up, the number of years lost due to a certain disease yields a measure with a simple and appealing interpretation. Recently, an overview of the methodology used for this goal has been published, and two measures have been proposed. In this work, we consider a third option that may be useful in settings in which the other two are inappropriate. In all three measures, the survival of the given dataset is compared to the expected survival in the general population which is calculated using external mortality tables. We thoroughly analyze the differences between the three measures, their assumptions, interpretation, and the corresponding estimators. The first measure is defined in a competing risk setting and assumes an excess hazard compared to the population, while the other two measures also allow estimation for groups that live better than the general population. In this case, the observed survival of the patients is compared to that in the population. The starting point of this comparison depends on whether the entry into the study is a hazard changing event (e.g., disease diagnosis or the age at which the inclusion criteria were met). Focusing on the newly defined life years difference measure, we study the estimation of the variance and consider the possible challenges (e.g., extrapolation) that occur in practice. We illustrate its use with a dataset of French Olympic athletes. Finally, an efficient R implementation has been developed for all three measures which make this work easily available to subsequent users.},
  archive      = {J_BIMJ},
  author       = {Damjan Manevski and Nina Ružić Gorenjec and Per Kragh Andersen and Maja Pohar Perme},
  doi          = {10.1002/bimj.202200070},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2200070},
  shortjournal = {Bio. J.},
  title        = {Expected life years compared to the general population},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample size calculation for the combination test under
nonproportional hazards. <em>BIMJ</em>, <em>65</em>(4), 2100403. (<a
href="https://doi.org/10.1002/bimj.202100403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For sample size calculation in clinical trials with survival endpoints, the logrank test, which is the optimal method under the proportional hazard (PH) assumption, is predominantly used. In reality, the PH assumption may not hold. For example, in immuno-oncology trials, delayed treatment effects are often expected. The sample size without considering the potential violation of the PH assumption may lead to an underpowered study. In recent years, combination tests such as the maximum weighted logrank test have received great attention because of their robust performance in various hazards scenarios. In this paper, we propose a flexible simulation-free procedure to calculate the sample size using combination tests. The procedure extends the Lakatos&#39; Markov model and allows for complex situations encountered in a clinical trial, like staggered entry, dropouts, etc. We evaluate the procedure using two maximum weighted logrank tests, one projection-type test, and three other commonly used tests under various hazards scenarios. The simulation studies show that the proposed method can achieve the target power for all compared tests in most scenarios. The combination tests exhibit robust performance under correct specification and misspecification scenarios and are highly recommended when the hazard-changing patterns are unknown beforehand. Finally, we demonstrate our method using two clinical trial examples and provide suggestions about the sample size calculations under nonproportional hazards.},
  archive      = {J_BIMJ},
  author       = {Huan Cheng and Jianghua He},
  doi          = {10.1002/bimj.202100403},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2100403},
  shortjournal = {Bio. J.},
  title        = {Sample size calculation for the combination test under nonproportional hazards},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian design for minimizing prediction uncertainty in
bivariate spatial responses with applications to air quality monitoring.
<em>BIMJ</em>, <em>65</em>(4), 2100386. (<a
href="https://doi.org/10.1002/bimj.202100386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based geostatistical design involves the selection of locations to collect data to minimize an expected loss function over a set of all possible locations. The loss function is specified to reflect the aim of data collection, which, for geostatistical studies, could be to minimize the prediction uncertainty at unobserved locations. In this paper, we propose a new approach to design such studies via a loss function derived through considering the entropy about the model predictions and the parameters of the model. The approach includes a multivariate extension to generalized linear spatial models, and thus can be used to design experiments with more than one response. Unfortunately, evaluating our proposed loss function is computationally expensive so we provide an approximation such that our approach can be adopted to design realistically sized geostatistical studies. This is demonstrated through a simulated study and through designing an air quality monitoring program in Queensland, Australia. The results show that our designs remain highly efficient in achieving each experimental objective individually, providing an ideal compromise between the two objectives. Accordingly, we advocate that our approach could be adopted more generally in model-based geostatistical design.},
  archive      = {J_BIMJ},
  author       = {S. G. J. Senarathne and Werner G. Müller and James M. McGree},
  doi          = {10.1002/bimj.202100386},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2100386},
  shortjournal = {Bio. J.},
  title        = {Bayesian design for minimizing prediction uncertainty in bivariate spatial responses with applications to air quality monitoring},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian estimation of two-part joint models for a
longitudinal semicontinuous biomarker and a terminal event with INLA:
Interests for cancer clinical trial evaluation. <em>BIMJ</em>,
<em>65</em>(4), 2100322. (<a
href="https://doi.org/10.1002/bimj.202100322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-part joint models for a longitudinal semicontinuous biomarker and a terminal event have been recently introduced based on frequentist estimation. The biomarker distribution is decomposed into a probability of positive value and the expected value among positive values. Shared random effects can represent the association structure between the biomarker and the terminal event. The computational burden increases compared to standard joint models with a single regression model for the biomarker. In this context, the frequentist estimation implemented in the R package frailtypack can be challenging for complex models (i.e., a large number of parameters and dimension of the random effects). As an alternative, we propose a Bayesian estimation of two-part joint models based on the Integrated Nested Laplace Approximation (INLA) algorithm to alleviate the computational burden and fit more complex models. Our simulation studies confirm that INLA provides accurate approximation of posterior estimates and to reduced computation time and variability of estimates compared to frailtypack in the situations considered. We contrast the Bayesian and frequentist approaches in the analysis of two randomized cancer clinical trials (GERCOR and PRIME studies), where INLA has a reduced variability for the association between the biomarker and the risk of event. Moreover, the Bayesian approach was able to characterize subgroups of patients associated with different responses to treatment in the PRIME study. Our study suggests that the Bayesian approach using the INLA algorithm enables to fit complex joint models that might be of interest in a wide range of clinical applications.},
  archive      = {J_BIMJ},
  author       = {Denis Rustand and Janet van Niekerk and Håvard Rue and Christophe Tournigand and Virginie Rondeau and Laurent Briollais},
  doi          = {10.1002/bimj.202100322},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2100322},
  shortjournal = {Bio. J.},
  title        = {Bayesian estimation of two-part joint models for a longitudinal semicontinuous biomarker and a terminal event with INLA: Interests for cancer clinical trial evaluation},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A classification model for continuous responses: Identifying
risk perception groups on health-related activities. <em>BIMJ</em>,
<em>65</em>(4), 2100222. (<a
href="https://doi.org/10.1002/bimj.202100222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current literature on latent variable models, much effort has been put on the development of dichotomous and polytomous cognitive diagnostic models (CDMs) for assessments. Recently, the possibility of using continuous responses in CDMs has been brought to discussion. But no Bayesian approach has been developed yet for the analysis of CDMs when responses are continuous. Our work is the first Bayesian framework for the continuous deterministic inputs, noisy, and gate (DINA) model. We also propose new interpretations for item parameters in this DINA model, which makes the analysis more interpretable than before. In addition, we have conducted several simulations to evaluate the performance of the continuous DINA model through our Bayesian approach. Then, we have applied the proposed DINA model to a real data example of risk perceptions for individuals over a range of health-related activities. The application results exemplify the high potential of the use of the proposed continuous DINA model to classify individuals in the study.},
  archive      = {J_BIMJ},
  author       = {Eduardo S. B. de Oliveira and Xiaojing Wang and Jorge L. Bazán},
  doi          = {10.1002/bimj.202100222},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2100222},
  shortjournal = {Bio. J.},
  title        = {A classification model for continuous responses: Identifying risk perception groups on health-related activities},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Correcting for heterogeneity and non-comparability bias in
multicenter clinical trials with a rescaled random-effect excess hazard
model. <em>BIMJ</em>, <em>65</em>(4), 2100210. (<a
href="https://doi.org/10.1002/bimj.202100210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the presence of competing causes of event occurrence (e.g., death), the interest might not only be in the overall survival but also in the so-called net survival, that is, the hypothetical survival that would be observed if the disease under study were the only possible cause of death. Net survival estimation is commonly based on the excess hazard approach in which the hazard rate of individuals is assumed to be the sum of a disease-specific and expected hazard rate, supposed to be correctly approximated by the mortality rates obtained from general population life tables. However, this assumption might not be realistic if the study participants are not comparable with the general population. Also, the hierarchical structure of the data can induces a correlation between the outcomes of individuals coming from the same clusters (e.g., hospital, registry). We proposed an excess hazard model that corrects simultaneously for these two sources of bias, instead of dealing with them independently as before. We assessed the performance of this new model and compared it with three similar models, using extensive simulation study, as well as an application to breast cancer data from a multicenter clinical trial. The new model performed better than the others in terms of bias, root mean square error, and empirical coverage rate. The proposed approach might be useful to account simultaneously for the hierarchical structure of the data and the non-comparability bias in studies such as long-term multicenter clinical trials, when there is interest in the estimation of net survival.},
  archive      = {J_BIMJ},
  author       = {Juste A. Goungounga and Nathalie Grafféo and Hadrien Charvat and Roch Giorgi},
  doi          = {10.1002/bimj.202100210},
  journal      = {Biometrical Journal},
  month        = {4},
  number       = {4},
  pages        = {2100210},
  shortjournal = {Bio. J.},
  title        = {Correcting for heterogeneity and non-comparability bias in multicenter clinical trials with a rescaled random-effect excess hazard model},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Cover picture: Biometrical journal 3’23. <em>BIMJ</em>,
<em>65</em>(3), 2370031. (<a
href="https://doi.org/10.1002/bimj.202370031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370031},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2370031},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 3&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random-effects meta-analysis models for the odds ratio in
the case of rare events under different data-generating models: A
simulation study. <em>BIMJ</em>, <em>65</em>(3), 2200132. (<a
href="https://doi.org/10.1002/bimj.202200132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis of binary data is challenging when the event under investigation is rare, and standard models for random-effects meta-analysis perform poorly in such settings. In this simulation study, we investigate the performance of different random-effects meta-analysis models in terms of point and interval estimation of the pooled log odds ratio in rare events meta-analysis. First and foremost, we evaluate the performance of a hypergeometric-normal model from the family of generalized linear mixed models (GLMMs), which has been recommended, but has not yet been thoroughly investigated for rare events meta-analysis. Performance of this model is compared to performance of the beta-binomial model, which yielded favorable results in previous simulation studies, and to the performance of models that are frequently used in rare events meta-analysis, such as the inverse variance model and the Mantel–Haenszel method. In addition to considering a large number of simulation parameters inspired by real-world data settings, we study the comparative performance of the meta-analytic models under two different data-generating models (DGMs) that have been used in past simulation studies. The results of this study show that the hypergeometric-normal GLMM is useful for meta-analysis of rare events when moderate to large heterogeneity is present. In addition, our study reveals important insights with regard to the performance of the beta-binomial model under different DGMs from the binomial-normal family. In particular, we demonstrate that although misalignment of the beta-binomial model with the DGM affects its performance, it shows more robustness to the DGM than its competitors.},
  archive      = {J_BIMJ},
  author       = {Katrin Jansen and Heinz Holling},
  doi          = {10.1002/bimj.202200132},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2200132},
  shortjournal = {Bio. J.},
  title        = {Random-effects meta-analysis models for the odds ratio in the case of rare events under different data-generating models: A simulation study},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving sandwich variance estimation for marginal cox
analysis of cluster randomized trials. <em>BIMJ</em>, <em>65</em>(3),
2200113. (<a href="https://doi.org/10.1002/bimj.202200113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) frequently recruit a small number of clusters, therefore necessitating the application of small-sample corrections for valid inference. A recent systematic review indicated that CRTs reporting right-censored, time-to-event outcomes are not uncommon and that the marginal Cox proportional hazards model is one of the common approaches used for primary analysis. While small-sample corrections have been studied under marginal models with continuous, binary, and count outcomes, no prior research has been devoted to the development and evaluation of bias-corrected sandwich variance estimators when clustered time-to-event outcomes are analyzed by the marginal Cox model. To improve current practice, we propose nine bias-corrected sandwich variance estimators for the analysis of CRTs using the marginal Cox model and report on a simulation study to evaluate their small-sample properties. Our results indicate that the optimal choice of bias-corrected sandwich variance estimator for CRTs with survival outcomes can depend on the variability of cluster sizes and can also slightly differ whether it is evaluated according to relative bias or type I error rate. Finally, we illustrate the new variance estimators in a real-world CRT where the conclusion about intervention effectiveness differs depending on the use of small-sample bias corrections. The proposed sandwich variance estimators are implemented in an R package CoxBcv .},
  archive      = {J_BIMJ},
  author       = {Xueqi Wang and Elizabeth L. Turner and Fan Li},
  doi          = {10.1002/bimj.202200113},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2200113},
  shortjournal = {Bio. J.},
  title        = {Improving sandwich variance estimation for marginal cox analysis of cluster randomized trials},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-free conditional screening for ultrahigh-dimensional
survival data via conditional distance correlation. <em>BIMJ</em>,
<em>65</em>(3), 2200089. (<a
href="https://doi.org/10.1002/bimj.202200089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to select the active variables that have significant impact on the event of interest is a very important and meaningful problem in the statistical analysis of ultrahigh-dimensional data. In many applications, researchers often know that a certain set of covariates are active variables from some previous investigations and experiences. With the knowledge of the important prior knowledge of active variables, we propose a model-free conditional screening procedure for ultrahigh dimensional survival data based on conditional distance correlation. The proposed procedure can effectively detect the hidden active variables that are jointly important but are weakly correlated with the response. Moreover, it performs well when covariates are strongly correlated with each other. We establish the sure screening property and the ranking consistency of the proposed method and conduct extensive simulation studies, which suggests that the proposed procedure works well for practical situations. Then, we illustrate the new approach through a real dataset from the diffuse large-B-cell lymphoma study S1 .},
  archive      = {J_BIMJ},
  author       = {Hengjian Cui and Yanyan Liu and Guangcai Mao and Jing Zhang},
  doi          = {10.1002/bimj.202200089},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2200089},
  shortjournal = {Bio. J.},
  title        = {Model-free conditional screening for ultrahigh-dimensional survival data via conditional distance correlation},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian and influence function-based empirical likelihoods
for inference of sensitivity to the early diseased stage in diagnostic
tests. <em>BIMJ</em>, <em>65</em>(3), 2200021. (<a
href="https://doi.org/10.1002/bimj.202200021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, a disease process might involve three ordinal diagnostic stages: the normal healthy stage, the early stage of the disease, and the stage of full development of the disease. Early detection is critical for some diseases since it often means an optimal time window for therapeutic treatments of the diseases. In this study, we propose a new influence function-based empirical likelihood method and Bayesian empirical likelihood methods to construct confidence/credible intervals for the sensitivity of a test to patients in the early diseased stage given a specificity and a sensitivity of the test to patients in the fully diseased stage. Numerical studies are performed to compare the finite sample performances of the proposed approaches with existing methods. The proposed methods are shown to outperform existing methods in terms of coverage probability. A real dataset from the Alzheimer&#39;s Disease Neuroimaging Initiative (ANDI) is used to illustrate the proposed methods.},
  archive      = {J_BIMJ},
  author       = {Yan Hai and Shuangfei Shi and Gengsheng Qin and for the Alzheimer&#39;s Disease Neuroimaging},
  doi          = {10.1002/bimj.202200021},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2200021},
  shortjournal = {Bio. J.},
  title        = {Bayesian and influence function-based empirical likelihoods for inference of sensitivity to the early diseased stage in diagnostic tests},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using mortality to predict incidence for rare and lethal
cancers in very small areas. <em>BIMJ</em>, <em>65</em>(3), 2200017. (<a
href="https://doi.org/10.1002/bimj.202200017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incidence and mortality figures are needed to get a comprehensive overview of cancer burden. In many countries, cancer mortality figures are routinely recorded by statistical offices, whereas incidence depends on regional cancer registries. However, due to the complexity of updating cancer registries, incidence numbers become available 3 or 4 years later than mortality figures. It is, therefore, necessary to develop reliable procedures to predict cancer incidence at least until the period when mortality data are available. Most of the methods proposed in the literature are designed to predict total cancer (except nonmelanoma skin cancer) or major cancer sites. However, less frequent lethal cancers, such as brain cancer, are generally excluded from predictions because the scarce number of cases makes it difficult to use univariate models. Our proposal comes to fill this gap and consists of modeling jointly incidence and mortality data using spatio-temporal models with spatial and age shared components. This approach allows for predicting lethal cancers improving the performance of individual models when data are scarce by taking advantage of the high correlation between incidence and mortality. A fully Bayesian approach based on integrated nested Laplace approximations is considered for model fitting and inference. A validation process is also conducted to assess the performance of alternative models. We use the new proposals to predict brain cancer incidence rates by gender and age groups in the health units of Navarre and Basque Country (Spain) during the period 2005–2008.},
  archive      = {J_BIMJ},
  author       = {Jaione Etxeberria and Tomás Goicoa and Maria D. Ugarte},
  doi          = {10.1002/bimj.202200017},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2200017},
  shortjournal = {Bio. J.},
  title        = {Using mortality to predict incidence for rare and lethal cancers in very small areas},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Some new results on cox–czanner divergence and their
applications in survival studies. <em>BIMJ</em>, <em>65</em>(3),
2200008. (<a href="https://doi.org/10.1002/bimj.202200008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present communication, we propose a quantile-based measure for the divergence between two survival functions. This can also be used in a dynamic way where the divergence between survival functions varies with time. Several new properties of the proposed measure are investigated with suitable examples. The behavior of the measure for various reliability models is also investigated. A real data analysis is employed to compare the relative efficacy of two treatment groups using the proposed divergence measure.},
  archive      = {J_BIMJ},
  author       = {Unnikrishnan Nair N. and Silpa Subhash and S. M. Sunoj},
  doi          = {10.1002/bimj.202200008},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2200008},
  shortjournal = {Bio. J.},
  title        = {Some new results on Cox–Czanner divergence and their applications in survival studies},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incorporating infectious duration-dependent transmission
into bayesian epidemic models. <em>BIMJ</em>, <em>65</em>(3), 2100401.
(<a href="https://doi.org/10.1002/bimj.202100401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compartmental models are commonly used to describe the spread of infectious diseases by estimating the probabilities of transitions between important disease states. A significant challenge in fitting Bayesian compartmental models lies in the need to estimate the duration of the infectious period, based on limited data providing only symptom onset date or another proxy for the start of infectiousness. Commonly, the exponential distribution is used to describe the infectious duration, an overly simplistic approach, which is not biologically plausible. More flexible distributions can be used, but parameter identifiability and computational cost can worsen for moderately sized or large epidemics. In this article, we present a novel approach, which considers a curve of transmissibility over a fixed infectious duration. The incorporation of infectious duration-dependent (IDD) transmissibility, which decays to zero during the infectious period, is biologically reasonable for many viral infections and fixing the length of the infectious period eases computational complexity in model fitting. Through simulation, we evaluate different functional forms of IDD transmissibility curves and show that the proposed approach offers improved estimation of the time-varying reproductive number. We illustrate the benefit of our approach through a new analysis of the 1995 outbreak of Ebola Virus Disease in the Democratic Republic of the Congo.},
  archive      = {J_BIMJ},
  author       = {Caitlin Ward and Grant D. Brown and Jacob J. Oleson},
  doi          = {10.1002/bimj.202100401},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2100401},
  shortjournal = {Bio. J.},
  title        = {Incorporating infectious duration-dependent transmission into bayesian epidemic models},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing recurrent and nonrecurrent terminal events data in
discrete time. <em>BIMJ</em>, <em>65</em>(3), 2100361. (<a
href="https://doi.org/10.1002/bimj.202100361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint analysis of recurrent and nonrecurrent terminal events has attracted substantial attention in literature. However, there lacks formal methodology for such analysis when the event time data are on discrete scales, even though some modeling and inference strategies have been developed for discrete-time survival analysis. We propose a discrete-time joint modeling approach for the analysis of recurrent and terminal events where the two types of events may be correlated with each other. The proposed joint modeling assumes a shared frailty to account for the dependence among recurrent events and between the recurrent and the terminal terminal events. Also, the joint modeling allows for time-dependent covariates and rich families of transformation models for the recurrent and terminal events. A major advantage of our approach is that it does not assume a distribution for the frailty, nor does it assume a Poisson process for the analysis of the recurrent event. The utility of the proposed analysis is illustrated by simulation studies and two real applications, where the application to the biochemists&#39; rank promotion data jointly analyzes the biochemists&#39; citation numbers and times to rank promotion, and the application to the scleroderma lung study data jointly analyzes the adverse events and off-drug time among patients with the symptomatic scleroderma-related interstitial lung disease.},
  archive      = {J_BIMJ},
  author       = {Chi-Chung Wen and Yi-Hau Chen},
  doi          = {10.1002/bimj.202100361},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2100361},
  shortjournal = {Bio. J.},
  title        = {Analyzing recurrent and nonrecurrent terminal events data in discrete time},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthesizing secondary data into survival analysis to
improve estimation efficiency. <em>BIMJ</em>, <em>65</em>(3), 2100326.
(<a href="https://doi.org/10.1002/bimj.202100326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accelerated failure time (AFT) model and Cox proportional hazards (PH) model are broadly used for survival endpoints of primary interest. However, the estimation efficiency from those models can be further enhanced by incorporating the information from secondary outcomes that are increasingly available and highly correlated with primary outcomes. Those secondary outcomes could be longitudinal laboratory measures collected from doctor visits or cross-sectional disease-relevant variables, which are believed to contain extra information related to primary survival endpoints to a certain extent. In this paper, we develop a two-stage estimation framework to combine a survival model with a secondary model that contains secondary outcomes, named as the empirical-likelihood-based weighting (ELW), which comprises two weighting schemes accommodated to the AFT model (ELW-AFT) and the Cox PH model (ELW-Cox), respectively. This innovative framework is flexibly adaptive to secondary outcomes with complex data features, and it leads to more efficient parameter estimation in the survival model even if the secondary model is misspecified. Extensive simulation studies showcase more efficiency gain from ELW compared to conventional approaches, and an application in the Atherosclerosis Risk in Communities study also demonstrates the superiority of ELW by successfully detecting risk factors at the time of hospitalization for acute myocardial infarction.},
  archive      = {J_BIMJ},
  author       = {Chixiang Chen and Tonghui Yu and Biyi Shen and Ming Wang},
  doi          = {10.1002/bimj.202100326},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2100326},
  shortjournal = {Bio. J.},
  title        = {Synthesizing secondary data into survival analysis to improve estimation efficiency},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flexible cloglog links for binomial regression models as an
alternative for imbalanced medical data. <em>BIMJ</em>, <em>65</em>(3),
2100325. (<a href="https://doi.org/10.1002/bimj.202100325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complementary log-log link was originally introduced in 1922 to R. A. Fisher, long before the logit and probit links. While the last two links are symmetric, the complementary log-log link is an asymmetrical link without a parameter associated with it. Several asymmetrical links with an extra parameter were proposed in the literature over last few years to deal with imbalanced data in binomial regression (when one of the classes is much smaller than the other); however, these do not necessarily have the cloglog link as a special case, with the exception of the link based on the generalized extreme value distribution. In this paper, we introduce flexible cloglog links for modeling binomial regression models that include an extra parameter associated with the link that explains some unbalancing for binomial outcomes. For all cases, the cloglog is a special case or the reciprocal version loglog link is obtained. A Bayesian Markov chain Monte Carlo inference approach is developed. Simulations study to evaluate the performance of the proposed algorithm is conducted and prior sensitivity analysis for the extra parameter shows that a uniform prior is the most convenient for all models. Additionally, two applications in medical data (age at menarche and pulmonary infection) illustrate the advantages of the proposed models.},
  archive      = {J_BIMJ},
  author       = {Jessica S.B. Alves and Jorge L. Bazán and Reinaldo B. Arellano-Valle},
  doi          = {10.1002/bimj.202100325},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2100325},
  shortjournal = {Bio. J.},
  title        = {Flexible cloglog links for binomial regression models as an alternative for imbalanced medical data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A latent class model to multiply impute missing treatment
indicators in observational studies when inferences of the treatment
effect are made using propensity score matching. <em>BIMJ</em>,
<em>65</em>(3), 2100284. (<a
href="https://doi.org/10.1002/bimj.202100284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysts often estimate treatment effects in observational studies using propensity score matching techniques. When there are missing covariate values, analysts can multiply impute the missing data to create m completed data sets. Analysts can then estimate propensity scores on each of the completed data sets, and use these to estimate treatment effects. However, there has been relatively little attention on developing imputation models to deal with the additional problem of missing treatment indicators, perhaps due to the consequences of generating implausible imputations. However, simply ignoring the missing treatment values, akin to a complete case analysis, could also lead to problems when estimating treatment effects. We propose a latent class model to multiply impute missing treatment indicators. We illustrate its performance through simulations and with data taken from a study on determinants of children&#39;s cognitive development. This approach is seen to obtain treatment effect estimates closer to the true treatment effect than when employing conventional imputation procedures as well as compared to a complete case analysis.},
  archive      = {J_BIMJ},
  author       = {Robin Mitra},
  doi          = {10.1002/bimj.202100284},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2100284},
  shortjournal = {Bio. J.},
  title        = {A latent class model to multiply impute missing treatment indicators in observational studies when inferences of the treatment effect are made using propensity score matching},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rectangular tolerance regions and multivariate normal
reference regions in laboratory medicine. <em>BIMJ</em>, <em>65</em>(3),
2100180. (<a href="https://doi.org/10.1002/bimj.202100180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reference intervals are widely used in the interpretation of results of biochemical and physiological tests of patients. When there are multiple biochemical analytes measured from each subject, a multivariate reference region is needed. Because of their greater specificity against false positives, such reference regions are more desirable than separate univariate reference intervals that disregard the cross-correlations between variables. Traditionally, under multivariate normality, reference regions have been constructed as ellipsoidal regions. This approach suffers from a major drawback: it cannot detect component-wise extreme observations. In the present work, procedures are developed to construct rectangular reference regions in the multivariate normal setup. The construction is based on the criteria for tolerance intervals. The problems addressed include the computation of a rectangular tolerance region and simultaneous tolerance intervals. Also addressed is the computation of mixed reference intervals that include both two-sided and one-sided limits, simultaneously. A parametric bootstrap approach is used in the computations, and the accuracy of the proposed methodology is assessed using estimated coverage probabilities. The problem of sample size determination is also addressed, and the results are illustrated using examples that call for the computation of reference regions.},
  archive      = {J_BIMJ},
  author       = {Michael Daniel Lucagbo and Thomas Mathew},
  doi          = {10.1002/bimj.202100180},
  journal      = {Biometrical Journal},
  month        = {3},
  number       = {3},
  pages        = {2100180},
  shortjournal = {Bio. J.},
  title        = {Rectangular tolerance regions and multivariate normal reference regions in laboratory medicine},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Cover picture: Biometrical journal 2’23. <em>BIMJ</em>,
<em>65</em>(2), 2370021. (<a
href="https://doi.org/10.1002/bimj.202370021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370021},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2370021},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 2&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A likelihood ratio test for completed sampling in population
size estimation studies. <em>BIMJ</em>, <em>65</em>(2), 2200129. (<a
href="https://doi.org/10.1002/bimj.202200129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a likelihood ratio test to assess that sampling has been completed in closed population size estimation studies. More precisely, we assess if the expected number of subjects that have never been sampled is below a user-specified threshold. The likelihood ratio test statistic has a nonstandard distribution under the null hypothesis. Critical values can be easily approximated and tabulated, and they do not depend on model specification. We illustrate in a simulation study and three real data examples, one of which involves ascertainment bias of amyotrophic lateral sclerosis in Gulf War veterans.},
  archive      = {J_BIMJ},
  author       = {Alessio Farcomeni},
  doi          = {10.1002/bimj.202200129},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2200129},
  shortjournal = {Bio. J.},
  title        = {A likelihood ratio test for completed sampling in population size estimation studies},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal stein-type goodness-of-fit tests for count data.
<em>BIMJ</em>, <em>65</em>(2), 2200073. (<a
href="https://doi.org/10.1002/bimj.202200073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Common count distributions, such as the Poisson (binomial) distribution for unbounded (bounded) counts considered here, can be characterized by appropriate Stein identities. These identities, in turn, might be utilized to define a corresponding goodness-of-fit (GoF) test, the test statistic of which involves the computation of weighted means for a user-selected weight function f . Here, the choice of f should be done with respect to the relevant alternative scenario, as it will have great impact on the GoF-test&#39;s performance. We derive the asymptotics of both the Poisson and binomial Stein-type GoF-statistic for general count distributions (we also briefly consider the negative-binomial case), such that the asymptotic power is easily computed for arbitrary alternatives. This allows for an efficient implementation of optimal Stein tests, that is, which are most powerful within a given class of weight functions. The performance and application of the optimal Stein-type GoF-tests is investigated by simulations and several medical data examples.},
  archive      = {J_BIMJ},
  author       = {Christian H. Weiß and Pedro Puig and Boris Aleksandrov},
  doi          = {10.1002/bimj.202200073},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2200073},
  shortjournal = {Bio. J.},
  title        = {Optimal stein-type goodness-of-fit tests for count data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical review of animal trials—a guideline.
<em>BIMJ</em>, <em>65</em>(2), 2200061. (<a
href="https://doi.org/10.1002/bimj.202200061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Any experiment involving living organisms requires justification of the need and moral defensibleness of the study. Statistical planning, design, and sample size calculation of the experiment are no less important review criteria than general medical and ethical points to consider. Errors made in the statistical planning and data evaluation phase can have severe consequences on both results and conclusions. They might proliferate and thus impact future trials—an unintended outcome of fundamental research with profound ethical consequences. Unified statistical standards are currently missing for animal review boards in Germany. In order to accompany, we developed a biometric form to be filled and handed in with the proposal at the concerned local authority on animal welfare. It addresses relevant points to consider for biostatistical planning of animal experiments and can help both the applicants and the reviewers in overseeing the entire experiment(s) planned. Furthermore, the form might also aid in meeting the current standards set by the 3+3R&#39;s principle of animal experimentation: Replacement, Reduction, Refinement as well as Robustness, Registration, and Reporting. The form has already been in use by the concerned local authority of animal welfare in Berlin, Germany. In addition, we provide reference to our user guide giving more detailed explanation and examples for each section of the biometric form. Unifying the set of biostatistical aspects will help both the applicants and the reviewers to equal standards and increase quality of preclinical research projects, also for translational, multicenter, or international studies.},
  archive      = {J_BIMJ},
  author       = {Sophie K. Piper and Dario Zocholl and Ulf Toelch and Robert Roehle and Andrea Stroux and Johanna Hoessler and Anne Zinke and Frank Konietschke},
  doi          = {10.1002/bimj.202200061},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2200061},
  shortjournal = {Bio. J.},
  title        = {Statistical review of animal trials—A guideline},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing estimation methods for integrating probability and
nonprobability survey samples with machine-learning techniques. An
application to a survey on the impact of the COVID-19 pandemic in spain.
<em>BIMJ</em>, <em>65</em>(2), 2200035. (<a
href="https://doi.org/10.1002/bimj.202200035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web surveys have replaced Face-to-Face and computer assisted telephone interviewing (CATI) as the main mode of data collection in most countries. This trend was reinforced as a consequence of COVID-19 pandemic-related restrictions. However, this mode still faces significant limitations in obtaining probability-based samples of the general population. For this reason, most web surveys rely on nonprobability survey designs. Whereas probability-based designs continue to be the gold standard in survey sampling, nonprobability web surveys may still prove useful in some situations. For instance, when small subpopulations are the group under study and probability sampling is unlikely to meet sample size requirements, complementing a small probability sample with a larger nonprobability one may improve the efficiency of the estimates. Nonprobability samples may also be designed as a mean for compensating for known biases in probability-based web survey samples by purposely targeting respondent profiles that tend to be underrepresented in these surveys. This is the case in the Survey on the impact of the COVID-19 pandemic in Spain (ESPACOV) that motivates this paper. In this paper, we propose a methodology for combining probability and nonprobability web-based survey samples with the help of machine-learning techniques. We then assess the efficiency of the resulting estimates by comparing them with other strategies that have been used before. Our simulation study and the application of the proposed estimation method to the second wave of the ESPACOV Survey allow us to conclude that this is the best option for reducing the biases observed in our data.},
  archive      = {J_BIMJ},
  author       = {María del Mar Rueda and Sara Pasadas-del-Amo and Beatriz Cobo Rodríguez and Luis Castro-Martín and Ramón Ferri-García},
  doi          = {10.1002/bimj.202200035},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2200035},
  shortjournal = {Bio. J.},
  title        = {Enhancing estimation methods for integrating probability and nonprobability survey samples with machine-learning techniques. an application to a survey on the impact of the COVID-19 pandemic in spain},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalized pairwise comparisons for censored data: An
overview. <em>BIMJ</em>, <em>65</em>(2), 2100354. (<a
href="https://doi.org/10.1002/bimj.202100354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The method of generalized pairwise comparisons (GPC) is an extension of the well-known nonparametric Wilcoxon–Mann–Whitney test for comparing two groups of observations. Multiple generalizations of Wilcoxon–Mann–Whitney test and other GPC methods have been proposed over the years to handle censored data. These methods apply different approaches to handling loss of information due to censoring: ignoring noninformative pairwise comparisons due to censoring (Gehan, Harrell, and Buyse); imputation using estimates of the survival distribution (Efron, Péron, and Latta); or inverse probability of censoring weighting (IPCW, Datta and Dong). Based on the GPC statistic, a measure of treatment effect, the “net benefit,” can be defined. It quantifies the difference between the probabilities that a randomly selected individual from one group is doing better than an individual from the other group. This paper aims at evaluating GPC methods for censored data, both in the context of hypothesis testing and estimation, and providing recommendations related to their choice in various situations. The methods that ignore uninformative pairs have comparable power to more complex and computationally demanding methods in situations of low censoring, and are slightly superior for high proportions (&gt;40%) of censoring. If one is interested in estimation of the net benefit, Harrell&#39;s c index is an unbiased estimator if the proportional hazards assumption holds. Otherwise, the imputation (Efron or Peron) or IPCW (Datta, Dong) methods provide unbiased estimators in case of proportions of drop-out censoring up to 60%.},
  archive      = {J_BIMJ},
  author       = {Vaiva Deltuvaite-Thomas and Johan Verbeeck and Tomasz Burzykowski and Marc Buyse and Christophe Tournigand and Geert Molenberghs and Olivier Thas},
  doi          = {10.1002/bimj.202100354},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2100354},
  shortjournal = {Bio. J.},
  title        = {Generalized pairwise comparisons for censored data: An overview},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the equivalence of one-inflated zero-truncated and
zero-truncated one-inflated count data likelihoods. <em>BIMJ</em>,
<em>65</em>(2), 2100343. (<a
href="https://doi.org/10.1002/bimj.202100343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-inflation in zero-truncated count data has recently found considerable attention. There are currently two views in the literature. In the first approach, the untruncated model is considered as one-inflated whereas in the second approach the truncated model is viewed as one-inflated. Here, we show that both models have identical model spaces as well as identical maximum likelihoods. Consequences of population size estimation are illuminated, and the findings are illustrated at hand of two case studies.},
  archive      = {J_BIMJ},
  author       = {Dankmar Böhning},
  doi          = {10.1002/bimj.202100343},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2100343},
  shortjournal = {Bio. J.},
  title        = {On the equivalence of one-inflated zero-truncated and zero-truncated one-inflated count data likelihoods},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multistate modeling and structure selection for multitype
recurrent events and terminal event data. <em>BIMJ</em>, <em>65</em>(2),
2100334. (<a href="https://doi.org/10.1002/bimj.202100334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cardiovascular disease studies, a large number of risk factors are measured but it often remains unknown whether all of them are relevant variables and whether the impact of these variables is changing with time or remains constant. In addition, more than one kind of cardiovascular disease events can be observed in the same patient and events of different types are possibly correlated. It is expected that different kinds of events are associated with different covariates and the forms of covariate effects also vary between event types. To tackle these problems, we proposed a multistate modeling framework for the joint analysis of multitype recurrent events and terminal event. Model structure selection is performed to identify covariates with time-varying coefficients, time-independent coefficients, and null effects. This helps in understanding the disease process as it can detect relevant covariates and identify the temporal dynamics of the covariate effects. It also provides a more parsimonious model to achieve better risk prediction. The performance of the proposed model and selection method is evaluated in numerical studies and illustrated on a real dataset from the Atherosclerosis Risk in Communities study.},
  archive      = {J_BIMJ},
  author       = {Chuoxin Ma and Chunyu Wang and Jianxin Pan},
  doi          = {10.1002/bimj.202100334},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2100334},
  shortjournal = {Bio. J.},
  title        = {Multistate modeling and structure selection for multitype recurrent events and terminal event data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple two-sample testing under arbitrary covariance
dependency with an application in imaging mass spectrometry.
<em>BIMJ</em>, <em>65</em>(2), 2100328. (<a
href="https://doi.org/10.1002/bimj.202100328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale hypothesis testing has become a ubiquitous problem in high-dimensional statistical inference, with broad applications in various scientific disciplines. One relevant application is constituted by imaging mass spectrometry (IMS) association studies, where a large number of tests are performed simultaneously in order to identify molecular masses that are associated with a particular phenotype, for example, a cancer subtype. Mass spectra obtained from matrix-assisted laser desorption/ionization (MALDI) experiments are dependent, when considered as statistical quantities. False discovery proportion (FDP) estimation and  control under arbitrary dependency structure among test statistics is an active topic in modern multiple testing research. In this context, we are concerned with the evaluation of associations between the binary outcome variable (describing the phenotype) and multiple predictors derived from MALDI measurements. We propose an inference procedure in which the correlation matrix of the test statistics is utilized. The approach is based on multiple marginal models. Specifically, we fit a marginal logistic regression model for each predictor individually. Asymptotic joint normality of the stacked vector of the marginal regression coefficients is established under standard regularity assumptions, and their (limiting) correlation matrix is estimated. The proposed method extracts common factors from the resulting empirical correlation matrix. Finally, we estimate the realized FDP of a thresholding procedure for the marginal p -values. We demonstrate a practical application of the proposed workflow to MALDI IMS data in an oncological context.},
  archive      = {J_BIMJ},
  author       = {Vladimir Vutov and Thorsten Dickhaus},
  doi          = {10.1002/bimj.202100328},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2100328},
  shortjournal = {Bio. J.},
  title        = {Multiple two-sample testing under arbitrary covariance dependency with an application in imaging mass spectrometry},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-dimensional feature selection in competing risks
modeling: A stable approach using a split-and-merge ensemble algorithm.
<em>BIMJ</em>, <em>65</em>(2), 2100164. (<a
href="https://doi.org/10.1002/bimj.202100164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection is critical in competing risks regression with high-dimensional data. Although penalized variable selection methods and other machine learning-based approaches have been developed, many of these methods often suffer from instability in practice. This paper proposes a novel method named Random Approximate Elastic Net (RAEN). Under the proportional subdistribution hazards model, RAEN provides a stable and generalizable solution to the large-p-small-n variable selection problem for competing risks data. Our general framework allows the proposed algorithm to be applicable to other time-to-event regression models, including competing risks quantile regression and accelerated failure time models. We show that variable selection and parameter estimation improved markedly using the new computationally intensive algorithm through extensive simulations. A user-friendly R package RAEN is developed for public use. We also apply our method to a cancer study to identify influential genes associated with the death or progression from bladder cancer.},
  archive      = {J_BIMJ},
  author       = {Han Sun and Xiaofeng Wang},
  doi          = {10.1002/bimj.202100164},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2100164},
  shortjournal = {Bio. J.},
  title        = {High-dimensional feature selection in competing risks modeling: A stable approach using a split-and-merge ensemble algorithm},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model misspecification and bias for inverse probability
weighting estimators of average causal effects. <em>BIMJ</em>,
<em>65</em>(2), 2100118. (<a
href="https://doi.org/10.1002/bimj.202100118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Commonly used semiparametric estimators of causal effects specify parametric models for the propensity score (PS) and the conditional outcome. An example is an augmented inverse probability weighting (IPW) estimator, frequently referred to as a doubly robust estimator, because it is consistent if at least one of the two models is correctly specified. However, in many observational studies, the role of the parametric models is often not to provide a representation of the data-generating process but rather to facilitate the adjustment for confounding, making the assumption of at least one true model unlikely to hold. In this paper, we propose a crude analytical approach to study the large-sample bias of estimators when the models are assumed to be approximations of the data-generating process, namely, when all models are misspecified. We apply our approach to three prototypical estimators of the average causal effect, two IPW estimators, using a misspecified PS model, and an augmented IPW (AIPW) estimator, using misspecified models for the outcome regression (OR) and the PS. For the two IPW estimators, we show that normalization, in addition to having a smaller variance, also offers some protection against bias due to model misspecification. To analyze the question of when the use of two misspecified models is better than one we derive necessary and sufficient conditions for when the AIPW estimator has a smaller bias than a simple IPW estimator and when it has a smaller bias than an IPW estimator with normalized weights. If the misspecification of the outcome model is moderate, the comparisons of the biases of the IPW and AIPW estimators show that the AIPW estimator has a smaller bias than the IPW estimators. However, all biases include a scaling with the PS-model error and we suggest caution in modeling the PS whenever such a model is involved. For numerical and finite sample illustrations, we include three simulation studies and corresponding approximations of the large-sample biases. In a dataset from the National Health and Nutrition Examination Survey, we estimate the effect of smoking on blood lead levels.},
  archive      = {J_BIMJ},
  author       = {Ingeborg Waernbaum and Laura Pazzagli},
  doi          = {10.1002/bimj.202100118},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2100118},
  shortjournal = {Bio. J.},
  title        = {Model misspecification and bias for inverse probability weighting estimators of average causal effects},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A nonparametric method for classification trees using
grouped covariates. <em>BIMJ</em>, <em>65</em>(2), 2100107. (<a
href="https://doi.org/10.1002/bimj.202100107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A group of variables are commonly seen in diagnostic medicine when multiple prognostic factors are aggregated into a composite score to represent the risk profile. A model selection method considers these covariates as all-in or all-out types. Model selection procedures for grouped covariates and their applications have thrived in recent years, in part because of the development of genetic research in which gene–gene or gene–environment interactions and regulatory network pathways are considered groups of individual variables. However, little has been discussed on how to utilize grouped covariates to grow a classification tree. In this paper, we propose a nonparametric method to address the selection of split variables for grouped covariates and their following selection of split points. Comprehensive simulations were implemented to show the superiority of our procedures compared to a commonly used recursive partition algorithm. The practical use of our method is demonstrated through a real data analysis that uses a group of prognostic factors to classify the successful mobilization of peripheral blood stem cells.},
  archive      = {J_BIMJ},
  author       = {Feng-Chang Lin and Yu-Shan Shih and Yuan-Bin Yu},
  doi          = {10.1002/bimj.202100107},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2100107},
  shortjournal = {Bio. J.},
  title        = {A nonparametric method for classification trees using grouped covariates},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blinded sample size recalculation in adaptive enrichment
designs. <em>BIMJ</em>, <em>65</em>(2), 2000345. (<a
href="https://doi.org/10.1002/bimj.202000345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the precision medicine era, (prespecified) subgroup analyses are an integral part of clinical trials. Incorporating multiple populations and hypotheses in the design and analysis plan, adaptive designs promise flexibility and efficiency in such trials. Adaptations include (unblinded) interim analyses (IAs) or blinded sample size reviews. An IA offers the possibility to select promising subgroups and reallocate sample size in further stages. Trials with these features are known as adaptive enrichment designs. Such complex designs comprise many nuisance parameters, such as prevalences of the subgroups and variances of the outcomes in the subgroups. Additionally, a number of design options including the timepoint of the sample size review and timepoint of the IA have to be selected. Here, for normally distributed endpoints, we propose a strategy combining blinded sample size recalculation and adaptive enrichment at an IA, that is, at an early timepoint nuisance parameters are reestimated and the sample size is adjusted while subgroup selection and enrichment is performed later. We discuss implications of different scenarios concerning the variances as well as the timepoints of blinded review and IA and investigate the design characteristics in simulations. The proposed method maintains the desired power if planning assumptions were inaccurate and reduces the sample size and variability of the final sample size when an enrichment is performed. Having two separate timepoints for blinded sample size review and IA improves the timing of the latter and increases the probability to correctly enrich a subgroup.},
  archive      = {J_BIMJ},
  author       = {Marius Placzek and Tim Friede},
  doi          = {10.1002/bimj.202000345},
  journal      = {Biometrical Journal},
  month        = {2},
  number       = {2},
  pages        = {2000345},
  shortjournal = {Bio. J.},
  title        = {Blinded sample size recalculation in adaptive enrichment designs},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Cover picture: Biometrical journal 1’23. <em>BIMJ</em>,
<em>65</em>(1), 2370011. (<a
href="https://doi.org/10.1002/bimj.202370011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIMJ},
  doi          = {10.1002/bimj.202370011},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2370011},
  shortjournal = {Bio. J.},
  title        = {Cover picture: Biometrical journal 1&#39;23},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian time-varying autoregressive models of COVID-19
epidemics. <em>BIMJ</em>, <em>65</em>(1), 2200054. (<a
href="https://doi.org/10.1002/bimj.202200054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has highlighted the importance of reliable statistical models which, based on the available data, can provide accurate forecasts and impact analysis of alternative policy measures. Here we propose Bayesian time-dependent Poisson autoregressive models that include time-varying coefficients to estimate the effect of policy covariates on disease counts. The model is applied to the observed series of new positive cases in Italy and in the United States. The results suggest that our proposed models are capable of capturing nonlinear growth of disease counts. We also find that policy measures and, in particular, closure policies and the distribution of vaccines, lead to a significant reduction in disease counts in both countries.},
  archive      = {J_BIMJ},
  author       = {Paolo Giudici and Barbara Tarantino and Arkaprava Roy},
  doi          = {10.1002/bimj.202200054},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2200054},
  shortjournal = {Bio. J.},
  title        = {Bayesian time-varying autoregressive models of COVID-19 epidemics},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the relevance of prognostic information for clinical
trials: A theoretical quantification. <em>BIMJ</em>, <em>65</em>(1),
2100349. (<a href="https://doi.org/10.1002/bimj.202100349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The question of how individual patient data from cohort studies or historical clinical trials can be leveraged for designing more powerful, or smaller yet equally powerful, clinical trials becomes increasingly important in the era of digitalization. Today, the traditional statistical analyses approaches may seem questionable to practitioners in light of ubiquitous historical prognostic information. Several methodological developments aim at incorporating historical information in the design and analysis of future clinical trials, most importantly Bayesian information borrowing, propensity score methods, stratification, and covariate adjustment. Adjusting the analysis with respect to a prognostic score, which was obtained from some model applied to historical data, received renewed interest from a machine learning perspective, and we study the potential of this approach for randomized clinical trials. In an idealized situation of a normal outcome in a two-arm trial with 1:1 allocation, we derive a simple sample size reduction formula as a function of two criteria characterizing the prognostic score: (1) the coefficient of determination R 2 on historical data and (2) the correlation ρ between the estimated and the true unknown prognostic scores. While maintaining the same power, the original total sample size n planned for the unadjusted analysis reduces to in an adjusted analysis. Robustness in less ideal situations was assessed empirically. We conclude that there is potential for substantially more powerful or smaller trials, but only when prognostic scores can be accurately estimated.},
  archive      = {J_BIMJ},
  author       = {Sandra Siegfried and Stephen Senn and Torsten Hothorn},
  doi          = {10.1002/bimj.202100349},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100349},
  shortjournal = {Bio. J.},
  title        = {On the relevance of prognostic information for clinical trials: A theoretical quantification},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A mechanistic spatio-temporal modeling of COVID-19 data.
<em>BIMJ</em>, <em>65</em>(1), 2100318. (<a
href="https://doi.org/10.1002/bimj.202100318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the evolution of an epidemic is essential to implement timely and efficient preventive measures. The availability of epidemiological data at a fine spatio-temporal scale is both novel and highly useful in this regard. Indeed, having geocoded data at the case level opens the door to analyze the spread of the disease on an individual basis, allowing the detection of specific outbreaks or, in general, of some interactions between cases that are not observable if aggregated data are used. Point processes are the natural tool to perform such analyses. We analyze a spatio-temporal point pattern of Coronavirus disease 2019 (COVID-19) cases detected in Valencia (Spain) during the first 11 months (February 2020 to January 2021) of the pandemic. In particular, we propose a mechanistic spatio-temporal model for the first-order intensity function of the point process. This model includes separate estimates of the overall temporal and spatial intensities of the model and a spatio-temporal interaction term. For the latter, while similar studies have considered different forms of this term solely based on the physical distances between the events, we have also incorporated mobility data to better capture the characteristics of human populations. The results suggest that there has only been a mild level of spatio-temporal interaction between cases in the study area, which to a large extent corresponds to people living in the same residential location. Extending our proposed model to larger areas could help us gain knowledge on the propagation of COVID-19 across cities with high mobility levels.},
  archive      = {J_BIMJ},
  author       = {Álvaro Briz-Redón and Adina Iftimi and Jorge Mateu and Carolina Romero-García},
  doi          = {10.1002/bimj.202100318},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100318},
  shortjournal = {Bio. J.},
  title        = {A mechanistic spatio-temporal modeling of COVID-19 data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable selection for nonparametric additive cox model with
interval-censored data. <em>BIMJ</em>, <em>65</em>(1), 2100310. (<a
href="https://doi.org/10.1002/bimj.202100310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard Cox model is perhaps the most commonly used model for regression analysis of failure time data but it has some limitations such as the assumption on linear covariate effects. To relax this, the nonparametric additive Cox model, which allows for nonlinear covariate effects, is often employed, and this paper will discuss variable selection and structure estimation for this general model. For the problem, we propose a penalized sieve maximum likelihood approach with the use of Bernstein polynomials approximation and group penalization. To implement the proposed method, an efficient group coordinate descent algorithm is developed and can be easily carried out for both low- and high-dimensional scenarios. Furthermore, a simulation study is performed to assess the performance of the presented approach and suggests that it works well in practice. The proposed method is applied to an Alzheimer&#39;s disease study for identifying important and relevant genetic factors.},
  archive      = {J_BIMJ},
  author       = {Tian Tian and Jianguo Sun},
  doi          = {10.1002/bimj.202100310},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100310},
  shortjournal = {Bio. J.},
  title        = {Variable selection for nonparametric additive cox model with interval-censored data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A q-q plot aids interpretation of the false discovery rate.
<em>BIMJ</em>, <em>65</em>(1), 2100309. (<a
href="https://doi.org/10.1002/bimj.202100309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {False discovery rates are routinely controlled by application of the Benjamini–Hochberg step-up procedure to a set of p -values. A method is demonstrated for representing the values so obtained (the BH-FDRs) on a quantile–quantile (Q-Q) plot of the p -values transformed to the negative-logarithmic scale. Recognition of this connection between the BH-FDR and the Q-Q plot facilitates both understanding of the meaning of the BH-FDR and interpretation of the BH-FDR in a particular data set.},
  archive      = {J_BIMJ},
  author       = {Nicholas W. Galwey},
  doi          = {10.1002/bimj.202100309},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100309},
  shortjournal = {Bio. J.},
  title        = {A Q-Q plot aids interpretation of the false discovery rate},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sequential test for assessing the effectiveness of
response strategies during an emerging epidemic. <em>BIMJ</em>,
<em>65</em>(1), 2100293. (<a
href="https://doi.org/10.1002/bimj.202100293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In epidemiology, the fatality rate is an important indicator of disease severity and has been used to evaluate the effects of new treatments. During an emerging epidemic with limited resources, monitoring the changes in fatality rate can also provide signals on the evaluation of government policies and healthcare quality, which helps to guide public health decision. A statistical test is developed in this paper to detect changes in fatality rate over time during the course of an emerging infectious disease. A major advantage of the proposed test is that it only requires the regularly reported numbers of deaths and recoveries, which meets the actual need as detailed surveillance data are hard to collect during the course of an emerging epidemic especially the deadly infectious diseases with large magnitude. In addition, with the sequential testing procedure, the effective measures can be detected at the earliest possible time to provide guidance to policymakers for swift action. Simulation studies showed that the proposed test performs well and is sensitive in picking up changes in the fatality rate. The test is applied to the 2014–2016 Ebola outbreak in Sierra Leone for illustration.},
  archive      = {J_BIMJ},
  author       = {K.F. Lam and Yuanke Qu},
  doi          = {10.1002/bimj.202100293},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100293},
  shortjournal = {Bio. J.},
  title        = {A sequential test for assessing the effectiveness of response strategies during an emerging epidemic},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A spatial model to jointly analyze self-reported survey data
of COVID-19 symptoms and official COVID-19 incidence data.
<em>BIMJ</em>, <em>65</em>(1), 2100186. (<a
href="https://doi.org/10.1002/bimj.202100186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a joint spatial modeling framework to improve estimation of the spatial distribution of the latent COVID-19 incidence in Belgium, based on test-confirmed COVID-19 cases and crowd-sourced symptoms data as reported in a large-scale online survey. Correction is envisioned for stochastic dependence between the survey&#39;s response rate and spatial COVID-19 incidence, commonly known as preferential sampling, but not found significant. Results show that an online survey can provide valuable auxiliary data to optimize spatial COVID-19 incidence estimation based on confirmed cases in situations with limited testing capacity. Furthermore, it is shown that an online survey on COVID-19 symptoms with a sufficiently large sample size per spatial entity is capable of pinpointing the same locations that appear as test-confirmed clusters, approximately 1 week earlier. We conclude that a large-scale online study provides an inexpensive and flexible method to collect timely information of an epidemic during its early phase, which can be used by policy makers in an early phase of an epidemic and in conjunction with other monitoring systems.},
  archive      = {J_BIMJ},
  author       = {Maren Vranckx and Christel Faes and Geert Molenberghs and Niel Hens and Philippe Beutels and Pierre Van Damme and Jan Aerts and Oana Petrof and Koen Pepermans and Thomas Neyens},
  doi          = {10.1002/bimj.202100186},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100186},
  shortjournal = {Bio. J.},
  title        = {A spatial model to jointly analyze self-reported survey data of COVID-19 symptoms and official COVID-19 incidence data},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Penalized estimation of a class of single-index
varying-coefficient models for integrative genomic analysis.
<em>BIMJ</em>, <em>65</em>(1), 2100139. (<a
href="https://doi.org/10.1002/bimj.202100139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological advances have made it possible to collect high-dimensional genomic data along with clinical data on a large number of subjects. In the studies of chronic diseases such as cancer, it is of great interest to integrate clinical and genomic data to build a comprehensive understanding of the disease mechanisms. Despite extensive studies on integrative analysis, it remains an ongoing challenge to model the interaction effects between clinical and genomic variables, due to high dimensionality of the data and heterogeneity across data types. In this paper, we propose an integrative approach that models interaction effects using a single-index varying-coefficient model, where the effects of genomic features can be modified by clinical variables. We propose a penalized approach for separate selection of main and interaction effects. Notably, the proposed methods can be applied to right-censored survival outcomes based on a Cox proportional hazards model. We demonstrate the advantages of the proposed methods through extensive simulation studies and provide applications to a motivating cancer genomic study.},
  archive      = {J_BIMJ},
  author       = {Hoi Min Ng and Binyan Jiang and Kin Yau Wong},
  doi          = {10.1002/bimj.202100139},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100139},
  shortjournal = {Bio. J.},
  title        = {Penalized estimation of a class of single-index varying-coefficient models for integrative genomic analysis},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A powerful global test for spliceQTL effects. <em>BIMJ</em>,
<em>65</em>(1), 2100123. (<a
href="https://doi.org/10.1002/bimj.202100123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methods to test for effects of single nucleotide polymorphisms (SNPs) on exon inclusion exist but often rely on testing of associations between multiple exon–SNP pairs, with sometimes subsequent summarization of results at the gene level. Such approaches require heavy multiple testing corrections and detect mostly events with large effect sizes. We propose here a test to find spliceQTL (splicing quantitative trait loci) effects that takes all exons and all SNPs into account simultaneously. For any chosen gene, this score-based test looks for an association between the set of exon expressions and the set of SNPs, via a random-effects model framework. It is efficient to compute and can be used if the number of SNPs is larger than the number of samples. In addition, the test is powerful in detecting effects that are relatively small for individual exon–SNP pairs but are observed for many pairs. Furthermore, test results are more often replicated across datasets than pairwise testing results. This makes our test more robust to exon–SNP pair-specific effects, which do not extend to multiple pairs within the same gene. We conclude that the test we propose here offers more power and better replicability in the search for spliceQTL effects.},
  archive      = {J_BIMJ},
  author       = {Renee X. de Menezes and Armin Rauschenberger and Peter A. C. &#39;t Hoen and Marianne A. Jonker},
  doi          = {10.1002/bimj.202100123},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2100123},
  shortjournal = {Bio. J.},
  title        = {A powerful global test for spliceQTL effects},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goodness-of-fit tests in proportional hazards models with
random effects. <em>BIMJ</em>, <em>65</em>(1), 2000353. (<a
href="https://doi.org/10.1002/bimj.202000353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with testing the functional form of the covariate effects in a Cox proportional hazards model with random effects. We assume that the responses are clustered and incomplete due to right censoring. The estimation of the model under the null (parametric covariate effect) and the alternative (nonparametric effect) is performed using the full marginal likelihood. Under the alternative, the nonparametric covariate effects are estimated using orthogonal expansions. The test statistic is the likelihood ratio statistic, and its distribution is approximated using a bootstrap method. The performance of the proposed testing procedure is studied through simulations. The method is also applied on two real data sets one from biomedical research and one from veterinary medicine.},
  archive      = {J_BIMJ},
  author       = {Wenceslao González-Manteiga and María Dolores Martínez-Miranda and Ingrid Van Keilegom},
  doi          = {10.1002/bimj.202000353},
  journal      = {Biometrical Journal},
  month        = {1},
  number       = {1},
  pages        = {2000353},
  shortjournal = {Bio. J.},
  title        = {Goodness-of-fit tests in proportional hazards models with random effects},
  volume       = {65},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
