<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COIN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="coin---48">COIN - 48</h2>
<ul>
<li><details>
<summary>
(2023). Retina disease prediction using modified convolutional
neural network based on inception-ResNet model with support vector
machine classifier. <em>COIN</em>, <em>39</em>(6), 1088–1111. (<a
href="https://doi.org/10.1111/coin.12601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence and deep learning have aided ocular disease through experiments including automatic illness recognition from images of the iris, fundus, or retina. Automated diagnosis systems (ADSs) provide services for the benefit of humanity and are essential in the early detection of harmful diseases. In fact, early detection is essential to avoid total blindness. In real life, several diagnostic tests such as visual ocular tonometry, retinal exam, and acuity test are performed, but they are conclusively time demanding and stressful for the patient. To consume time and detect the retinal disease earlier, an efficient prediction method is designed. In this proposed model, the first process is data collection that consists of a retinal disease dataset for testing and training. The second process is pre-processing, which executes image resizing and noise filter for feature extraction. The third step is feature extraction, which extracts the image&#39;s form, size, color, and texture for classification with CNN based on Inception-ResNet V2. The classification process is done by using the SVM with the extracted features. The prediction of diseases is classified such as normal, cataract, glaucoma, and retinal disease. The suggested model&#39;s performance is assessed using performance indicators such as accuracy, error, sensitivity, precision, and so forth. The suggested model&#39;s accuracy, error, sensitivity, and precision are 0.96, 0.962, 0.964, and 0.04, respectively, higher than existing techniques such as VGG16, Mobilenet V1, ResNet, and AlexNet. Thus, the proposed model instantly predicts retinal disease.},
  archive      = {J_COIN},
  author       = {Arushi Jain and Vishal Bhatnagar and Annavarapu Chandra Sekhara Rao and Manju Khari},
  doi          = {10.1111/coin.12601},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1088-1111},
  shortjournal = {Comput. Intell.},
  title        = {Retina disease prediction using modified convolutional neural network based on inception-ResNet model with support vector machine classifier},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated skin lesion detection and classification using
fused deep convolutional neural network on dermoscopic images.
<em>COIN</em>, <em>39</em>(6), 1073–1087. (<a
href="https://doi.org/10.1111/coin.12590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer becomes a deadly disease that affect people of all ages globally. The availability of various types of benign and malignant melanoma makes the skin lesion diagnostic process difficult. Since the visual inspection of skin cancer is costlier and lengthy process, it is needed to design automatic diagnosis model to classify skin lesions accurately and promptly. Computer-aided diagnosis models can be employed to identify the presence of skin lesions using dermoscopic images. The automatic identification of skin lesions can assist the doctors and enable the detection process at an efficient and faster rate. With this motivation, this article presents an automated skin lesion detection and classification using fused deep convolutional neural network (ASDC-FDCNN) on dermoscopic images. The ASDC-FDCNN technique aims to identify the existence of skin lesions from dermoscopic images. The ASDC-FDCNN model involves the design of two deep learning models namely VGG19 and ResNet152 models. Besides, the fusion based feature extraction process is performed to derive feature vectors. In addition, the DCNN technique was employed as classifier for identifying the presence or absence of skin lesions. The performance validation of the ASDC-FDCNN technique takes place utilizing benchmark skin lesion dataset. A comparative results analysis reported the better performance of the ASDC-FDCNN model over the recent technique with respect to various measures.},
  archive      = {J_COIN},
  author       = {Rayappa Priyanka Pramila and Radhakrishnan Subhashini},
  doi          = {10.1111/coin.12590},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1073-1087},
  shortjournal = {Comput. Intell.},
  title        = {Automated skin lesion detection and classification using fused deep convolutional neural network on dermoscopic images},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SVM-ABC based cancer microarray (gene expression) hybrid
method for data classification. <em>COIN</em>, <em>39</em>(6),
1054–1072. (<a href="https://doi.org/10.1111/coin.12589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarray technology presents a challenge due to the large dimensionality of the data, which can be difficult to interpret. To address this challenge, the article proposes a feature extraction-based cancer classification technique coupled with artificial bee colony optimization (ABC) algorithm. The ABC-support vector machine (SVM) method is used to classify the lung cancer datasets and compared them with existing techniques in terms of precision, recall, F-measure, and accuracy. The proposed ABC-SVM has the advantage of dealing with complex nonlinear data, providing good flexibility. Simulation analysis was conducted with 30% of the data reserved for testing the proposed method. The results indicate that the proposed attribute classification technique, which uses fewer genes, performs better than other modalities. The classifiers, such as naïve Bayes, multi-class SVM, and linear discriminant analysis, were also compared and the proposed method outperformed these classifiers and state-of-the-art techniques. Overall, this study demonstrates the potential of using intelligent algorithms and feature extraction techniques to improve the accuracy of cancer diagnosis using microarray gene expression data.},
  archive      = {J_COIN},
  author       = {Punam Gulande and R N Awale},
  doi          = {10.1111/coin.12589},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1054-1072},
  shortjournal = {Comput. Intell.},
  title        = {SVM-ABC based cancer microarray (gene expression) hybrid method for data classification},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic detection of microaneurysms using a novel
segmentation algorithm based on deep learning techniques. <em>COIN</em>,
<em>39</em>(6), 1039–1053. (<a
href="https://doi.org/10.1111/coin.12588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microaneurysms is the first stage of diabetic retinopathy (DR) and it plays a vital role in the computerized diagnosis. However, it is difficult to automatically detect microaneurysms in fundus images due to the complicated background and various illumination reasons. The motivation behind this, is the number of increases in diabetic patients is very large when compared with the number of ophthalmologists. The FSCA-UNET (Frequency Spatial Channel Attention UNET) segmentation model, is proposed and it is an improvement over UNET. We first use the frequency channel attention mechanism to analyze the features that were extracted from the first stage of the convolution layer, and we obtain good results. Then, we included a spatial attention map with frequency attention, also known as FSCA-UNET, which makes use of inter-spatial connections between features. Our deep neural model with an encoder-decoder structure termed FSCA-UNET produced more accurate results. Our novel algorithm outdated the performance measures of the existing segmentation algorithms. The proposed segmentation algorithm was trained and tested on Indian Diabetic Retinopathy Image Dataset (IDRiD), and E-ophtha Dataset and we got promising results in terms of sensitivity, specificity, dice coefficient, precision, F1 score, and accuracy.},
  archive      = {J_COIN},
  author       = {T. Monisha Birlin and C. Divya and J. John Livingston},
  doi          = {10.1111/coin.12588},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1039-1053},
  shortjournal = {Comput. Intell.},
  title        = {Automatic detection of microaneurysms using a novel segmentation algorithm based on deep learning techniques},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An explainable deep learning model for prediction of
early-stage chronic kidney disease. <em>COIN</em>, <em>39</em>(6),
1022–1038. (<a href="https://doi.org/10.1111/coin.12587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic kidney disease (CKD) is a major public health concern with rising prevalence and huge costs associated with dialysis and transplantation. Early prediction of CKD can reduce the patient&#39;s risk of CKD progression to end-stage kidney failure. Artificial intelligence offers more intelligent and expert healthcare services in disease diagnosis. In this work, a deep learning model is built using deep neural networks (DNN) with an adaptive moment estimation optimization function to predict early-stage CKD. The health care applications require interpretability over the predictions of the black-box model to build conviction towards the model&#39;s prediction. Hence, the predictions of the DNN-CKD model are explained by the local interpretable model-agnostic explainer (LIME). The diagnostic patient data is trained on five layered DNN with three hidden layers. Over the unseen data, the DNN-CKD model yields an accuracy of 98.75% and a roc_auc score of 98.86% in detecting CKD risk. The explanation revealed by the LIME algorithm echoes the influence of each feature on the prediction made by the DNN-CKD model over the given CKD data. With its interpretability and accuracy, the proposed system may effectively help medical experts in the early diagnosis of CKD.},
  archive      = {J_COIN},
  author       = {Vinothini Arumugham and Baghavathi Priya Sankaralingam and Uma Maheswari Jayachandran and Komanduri Venkata Sesha Sai Rama Krishna and Selvanayaki Sundarraj and Moulana Mohammed},
  doi          = {10.1111/coin.12587},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1022-1038},
  shortjournal = {Comput. Intell.},
  title        = {An explainable deep learning model for prediction of early-stage chronic kidney disease},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual neural network-assisted one-class classification
algorithm for melanoma recognition with imbalanced data. <em>COIN</em>,
<em>39</em>(6), 1004–1021. (<a
href="https://doi.org/10.1111/coin.12578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer, also known as melanoma, is a deadly form of skin cancer that can significantly improve survival rates when diagnosed at an early stage. It is usually diagnosed visually from dermoscopic images, and such visual assessment of skin cancer by the naked eye is a challenging and arduous task. Therefore, the detection of melanoma from dermoscopic images using trained artificial intelligence models is of great importance today. However, since melanoma is a rare disease, existing databases of skin lesions often contain highly unbalanced numbers of benign and malignant samples. In this paper, we propose a new one-class classification-based skin lesion classification strategy for small and unbalanced datasets. One-class classification (OCC) is a special case of multi-classification. OCC aims to learn a descriptive paradigm from positive class data (true data) during training and reject pseudo data (fake data) that do not conform to the paradigm during inference. OCC has great potential for application in anomaly detection problems. We have analyzed several approaches to the OCC task in recent years and propose a new design paradigm for the OCC problem, taking into account the unbalanced data set of the melanoma classification task. We have designed an improved OCC network based on this design paradigm, where the network is based on the architecture of a residual neural network, combining the coding and decoding idea of variational self-encoder and the adversarial training idea of an adversarial neural network, using binary cross-entropy as the loss function and introducing the channel attention mechanism. Tests on several publicly available dermatology datasets show that this improved OCC network addresses the unbalanced dataset situation in melanoma image classification to some extent while having relatively excellent performance. Compared with some traditional networks, it can obtain more stable training results and perform more consistently on complex datasets.},
  archive      = {J_COIN},
  author       = {Lisu Yu and Yifei Wang and Liyu Zhou and Jinsheng Wu and Zhenghai Wang},
  doi          = {10.1111/coin.12578},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {1004-1021},
  shortjournal = {Comput. Intell.},
  title        = {Residual neural network-assisted one-class classification algorithm for melanoma recognition with imbalanced data},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extreme learning machine algorithm-based model for lung
cancer classification from histopathological real-time images.
<em>COIN</em>, <em>39</em>(6), 974–1003. (<a
href="https://doi.org/10.1111/coin.12576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present scenario, developing an automatic and credible diagnostic system to analyze lung cancer type, stage, and level from computed tomography (C.T.) images is a very challenging task, even for experienced pathologists, due to the nonuniform illumination and artifacts. The nonuniform illumination and artifacts are the low-frequency changes in image intensity that arise from the sensor and the person&#39;s movement while recording the C.T. scanned images. Although numerous machine learning techniques are used to improve the effectiveness of automatic lung cancer diagnostic systems, the classification accuracy of these systems still needs significant improvement to satisfy the real-time requirement of the diagnostic situations. A new extreme learning machine (ELM) algorithm-based model (hereafter called XlmNet) is proposed to classify the histopathology scans effectively. XlmNet utilizes The Cancer Imaging Archive (TCIA) dataset. After data collection, the initial stage in XlmNet is preprocessing, including noise removal, histogram equalization, and quality-improved image. The enhanced Profuse Clustering (EPC) method is implemented for segmenting the affected regions from C.T. scans by image segment using superpixel clustering. The statistical attributes are extracted by using Principal Component Analysis (PCA). ELM classifier helps in classifying the lung nodules. The empirical results of the XlmNet model are related to some advanced classifiers concerning performance metrics. The evaluations of XlmNet on the TCIA dataset reveal that XlmNet outperforms other classification networks with the Accuracy of 0.965, a sensitivity of 0.964, a specificity of 0.865, a precision of 0.962, a Jaccard similarity score (JSS) of 0.95.},
  archive      = {J_COIN},
  author       = {M. Grace John and S. Baskar},
  doi          = {10.1111/coin.12576},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {974-1003},
  shortjournal = {Comput. Intell.},
  title        = {Extreme learning machine algorithm-based model for lung cancer classification from histopathological real-time images},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Smart analysis of anxiety people and their activities using
heterogeneous quasiperiodic process. <em>COIN</em>, <em>39</em>(6),
950–973. (<a href="https://doi.org/10.1111/coin.12574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase in anxiety levels worldwide can be described as a serious global health threat. Around 500 million people suffer from mental disorders and are suffering from depression, and other mental-oriented disabilities. The new technological paradigms such as the Internet of Things (IoT) were employed for detecting, and treating these disorders, which are being proposed, developed, and provide new capabilities to detect, assess and care for anxious people. These possibilities lead to several issues that are identified, which relate to patient privacy and confidentiality, security challenges such as data security, and the organization of IoT systems. To rectify these issues, we implement the Smart analysis of anxious people and their activities using Heterogeneous computing with virtual sensing. This system consists of the health application featuring the technique of the internet of things, heterogeneous computing, Cognitive Quasiperiodic motion, and Pentagonal tiling. This system introduces the internet of things-assisted smart analysis of anxiety against people with their activities. It performs the optimization analysis for improving the levels. The observational results focus on how to deal with the issues which were overcome in the analysis levels of the anxiety disorders, which show that time spent using the heterogeneous computing with the virtual sense proves more accuracy (92.3%), specificity (Degrees of anxiety severity by 20%), precision (Analysis of anxiety stress level by 65%), and recall (Anxiety chances percentile by 65.34%). The proposed model is simulated, and the outcomes are compared with the prevailing methods for evaluating the parameters like accuracy, end-end-end delay, energy consumption, network lifetime, and throughput.},
  archive      = {J_COIN},
  author       = {Ludi Zhao and Xuting Guo and Guanpeng Song},
  doi          = {10.1111/coin.12574},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {950-973},
  shortjournal = {Comput. Intell.},
  title        = {Smart analysis of anxiety people and their activities using heterogeneous quasiperiodic process},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing CT images for detecting lung cancer by applying
the computational intelligence-based optimization techniques.
<em>COIN</em>, <em>39</em>(6), 930–949. (<a
href="https://doi.org/10.1111/coin.12567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer is the most critical disease because it affects both men and women. Most of the time, lung cancer leads to death due to less health care and medical attention. In addition, lung cancer is difficult to identify in earlier stages due to the low-level symptoms and risk factors. To overcome the complexity, effective techniques must predict lung cancer earlier. To attain the problem statement, an lung cancer identification system is developed with the help of a meta-heuristic algorithm. The CT imageries obtained from the CIA database are analyzed step by step. The gathered image noise is removed by applying the mean filter, and the affected regions are segmented with the help of the Butterfly Optimization Algorithm-based K-Means Clustering (BOAKMC) algorithm. Afterward, various statistical features are derived, and the Supervised Jaya Optimized Rough Set related Feature Selection (SJORSFS) process is used to select the lung features. Finally, the lung cancer is identified using Autoencoder based Recurrent Neural Network (ARNN) classification algorithm, successfully recognizing the lung cancer features. Then the system&#39;s efficiency is evaluated using a MATLAB setup; here, 3000 are treated as training images and 2043 for testing images. The effective training enhances overall lung cancer prediction accuracy by up to 99.15%.},
  archive      = {J_COIN},
  author       = {Mohamed Shakeel Pethuraj and Burhanuddin bin Mohd Aboobaider and Lizawati Binti Salahuddin},
  doi          = {10.1111/coin.12567},
  journal      = {Computational Intelligence},
  month        = {12},
  number       = {6},
  pages        = {930-949},
  shortjournal = {Comput. Intell.},
  title        = {Analyzing CT images for detecting lung cancer by applying the computational intelligence-based optimization techniques},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel feature ranking algorithm for text classification:
Brilliant probabilistic feature selector (BPFS). <em>COIN</em>,
<em>39</em>(5), 900–926. (<a
href="https://doi.org/10.1111/coin.12599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification (TC) is a very crucial task in this century of high-volume text datasets. Feature selection (FS) is one of the most important stages in TC studies. In the literature, numerous feature selection methods are recommended for TC. In the TC domain, filter-based FS methods are commonly utilized to select a more informative feature subsets. Each method uses a scoring system that is based on its algorithm to order the features. The classification process is then carried out by choosing the top-N features. However, each method&#39;s feature order is distinct from the others. Each method selects by giving the qualities that are critical to its algorithm a high score, but it does not select by giving the features that are unimportant a low value. In this paper, we proposed a novel filter-based FS method namely, brilliant probabilistic feature selector (BPFS), to assign a fair score and select informative features. While the BPFS method selects unique features, it also aims to select sparse features by assigning higher scores than common features. Extensive experimental studies using three effective classifiers decision tree (DT), support vector machines (SVM), and multinomial naive bayes (MNB) on four widely used datasets named Reuters-21,578, 20Newsgroup, Enron1, and Polarity with different characteristics demonstrate the success of the BPFS method. For feature dimensions, 20, 50, 100, 200, 500, and 1000 dimensions were used. The experimental results on different benchmark datasets show that the BPFS method is more successful than the well-known and recent FS methods according to Micro-F1 and Macro-F1 scores.},
  archive      = {J_COIN},
  author       = {Bekir Parlak},
  doi          = {10.1111/coin.12599},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {900-926},
  shortjournal = {Comput. Intell.},
  title        = {A novel feature ranking algorithm for text classification: Brilliant probabilistic feature selector (BPFS)},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computation of persistent homology on streaming data using
topological data summaries. <em>COIN</em>, <em>39</em>(5), 860–899. (<a
href="https://doi.org/10.1111/coin.12597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent homology is a computationally intensive and yet extremely powerful tool for Topological Data Analysis. Applying the tool on potentially infinite sequence of data objects is a challenging task. For this reason, persistent homology and data stream mining have long been two important but disjoint areas of data science. The first computational model, that was recently introduced to bridge the gap between the two areas, is useful for detecting steady or gradual changes in data streams, such as certain genomic modifications during the evolution of species. However, that model is not suitable for applications that encounter abrupt changes of extremely short duration. This paper presents another model for computing persistent homology on streaming data that addresses the shortcoming of the previous work. The model is validated on the important real-world application of network anomaly detection. It is shown that in addition to detecting the occurrence of anomalies or attacks in computer networks, the proposed model is able to visually identify several types of traffic. Moreover, the model can accurately detect abrupt changes of extremely short as well as longer duration in the network traffic. These capabilities are not achievable by the previous model or by traditional data mining techniques.},
  archive      = {J_COIN},
  author       = {Anindya Moitra and Nicholas O. Malott and Philip A. Wilsey},
  doi          = {10.1111/coin.12597},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {860-899},
  shortjournal = {Comput. Intell.},
  title        = {Computation of persistent homology on streaming data using topological data summaries},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An attention-based deep learning model for credibility
assessment of online health information. <em>COIN</em>, <em>39</em>(5),
832–859. (<a href="https://doi.org/10.1111/coin.12596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the surge of searching and reading online health-based articles, maintaining the quality and credibility of online health-based articles has become crucial. The circulation of deceptive health information on numerous social media sites can mislead people and can potentially cause adverse effects on people&#39;s health. To address these problems, this work uses deep learning approaches to automate the assessment and scoring of online health-related articles&#39; credibility. The paper proposed an Attention-based Recurrent Multichannel Convolutional Neural Network (ARMCNN) model. The proposed model incorporates a BiLSTM layer, a multichannel CNN layer, and an attention layer and predicts the credibility of online health information. To perform a reliable evaluation of the presented model, we utilize the health articles reviewed by the experts, compiled in a labeled dataset termed “Pubhealth,” which consists of thousands of health articles. The results are evaluated using five performance measures, accuracy, precision, recall, f1-score, and area under the ROC curve (AUC). Furthermore, we extensively compared the proposed model with different deep learning and machine learning models such as Long short-term memory (LSTM), Bidirectional LSTM, CNN (Convolutional neural network), and RNN-CNN. The experimental results showed that the proposed model produced state-of-the-art performance on the used dataset by achieving an accuracy of 0.88, precision of 0.92, recall of 0.87, f1-score of 0.90, and AUC of 0.94. Further, the proposed model yielded better performance than other benchmarked techniques for the credibility assessment of online health articles.},
  archive      = {J_COIN},
  author       = {Swarup Padhy and Santosh Singh Rathore},
  doi          = {10.1111/coin.12596},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {832-859},
  shortjournal = {Comput. Intell.},
  title        = {An attention-based deep learning model for credibility assessment of online health information},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Classification analysis of burnout people’s brain images
using ontology-based speculative sense model. <em>COIN</em>,
<em>39</em>(5), 806–831. (<a
href="https://doi.org/10.1111/coin.12595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Burnout is a state of exhaustion that results from prolonged, excessive workplace stress. This can be examined with the biological explications of burnout and physical consequences and classified against prolonged vigorous activities. The research aims to classify burnout people&#39;s brain images against prolonged emotional activities using ontology analysis of treatment and prevention and intermediate layers formation based on a speculative sense model. In this segment, the Ontology analysis of Treatment and prevention and intermediate layers formation based on a hypothetical sense model is employed for burnout people&#39;s classification analysis. The methodology is performed in the platform of ontology creation and performs the classification analysis. The calculation analysis found the result, and the brain images were classified. The classification analysis of burnout people&#39;s brain images, separation of prolonged vigorous activities, and the ontology creation for treatment and prevention against burnout people&#39;s brain images were obtained. The analysis received the result, and the results of the precision, recall, storage, computation time, specificity, and classification of burnout people&#39;s brain images were obtained. Furthermore, all these Ontology analysis of Treatment and prevention and intermediate layers formation based on a hypothetical sense model had the prediction sensitivity (SN) over 50% and specificity (SP) over 90%. The Classification of Burnout People&#39;s Brain performance comparison shows that the proposed system is much more successful than existing methods, especially on a scoring accuracy of 98%.},
  archive      = {J_COIN},
  author       = {Chandrakirishnan Balakrishnan Sivaparthipan and Priyan Malarvizhi Kumar and Thota Chandu and BalaAnand Muthu and Mohammed Hasan Ali and Boris Tomaš},
  doi          = {10.1111/coin.12595},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {806-831},
  shortjournal = {Comput. Intell.},
  title        = {Classification analysis of burnout people&#39;s brain images using ontology-based speculative sense model},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An intelligent hierarchical residual attention
learning-based conjoined twin neural network for alzheimer’s stage
detection and prediction. <em>COIN</em>, <em>39</em>(5), 783–805. (<a
href="https://doi.org/10.1111/coin.12594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disorder (AD) causes permanent impairment in the brain&#39;s memory of the cellular system, leading to the initiation of dementia. Earlier detection of Alzheimer&#39;s disease in the initial stages is challenging for researchers. Deep learning and machine learning-based techniques can help resolve many issues associated with brain imaging exploration. Brain MR Images (Brain-MRI) are used to detect Alzheimer&#39;s in computable research work. To correctly categorize the stages of Alzheimer&#39;s disease, discriminative features need to be extracted from the MR images. Recently, many studies have used deep learning methods for the early detection of this disorder. However, overfitting degrades the deep learning method&#39;s performance because the dataset&#39;s selection images are smaller and imbalanced. Some studies could not reach more discriminative and effectual attention-aware features for Alzheimer&#39;s stage classification to increase the model performance. In this paper, we develop a novel hierarchical residual attention learning-inspired multistage conjoined twin network (HRAL-CTNN) to classify the stages of Alzheimer&#39;s. We used augmentation approaches to scale insufficient and imbalanced data. The HRAL-CTNN is efficiently overcoming the issues of not obtaining efficient attention-aware and generative features for Alzheimer&#39;s stage classification. The proposed model solved the problem of redundant features by extracting attentive discriminant features, and scaling imbalance data by data augmentation, after that training and validation using HRAL-CTNN. The execution of this proposed work has been performed on the ADNI MRI dataset. This work achieved outstanding accuracy of 99.97 0.01% and F1 score of 99.30 0.02% for Alzheimer&#39;s stage classification. This model proposed by our group outperformed the existing related studies in terms of the model&#39;s performance score.},
  archive      = {J_COIN},
  author       = {Venkatesh Gauri Shankar and Dilip Singh Sisodia and Preeti Chandrakar},
  doi          = {10.1111/coin.12594},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {783-805},
  shortjournal = {Comput. Intell.},
  title        = {An intelligent hierarchical residual attention learning-based conjoined twin neural network for alzheimer&#39;s stage detection and prediction},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online short text clustering using infinite extensions of
discrete mixture models. <em>COIN</em>, <em>39</em>(5), 759–782. (<a
href="https://doi.org/10.1111/coin.12593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text clustering is one of the fundamental tasks in natural language processing. Different from traditional documents, short texts are ambiguous and sparse due to their short form and the lack of recurrence in word usage from one text to another, making it very challenging to apply conventional machine learning algorithms directly. In this article, we propose two novel approaches for short texts clustering: collapsed Gibbs sampling infinite generalized Dirichlet multinomial mixture model infinite GSGDMM) and collapsed Gibbs sampling infinite Beta-Liouville multinomial mixture model (infinite GSBLMM). We adopt two flexible and practical priors to the multinomial distribution where in the first one the generalized Dirichlet distribution is integrated, while the second one is based on the Beta-Liouville distribution. We evaluate the proposed approaches on two famous benchmark datasets, namely, Google News and Tweet. The experimental results demonstrate the effectiveness of our models compared to basic approaches that use Dirichlet priors. We further propose to improve the performance of our methods with an online clustering procedure. We also evaluate the performance of our methods for the outlier detection task, in which we achieve accurate results.},
  archive      = {J_COIN},
  author       = {Samar Hannachi and Fatma Najar and Hafsa Ennajari and Nizar Bouguila},
  doi          = {10.1111/coin.12593},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {759-782},
  shortjournal = {Comput. Intell.},
  title        = {Online short text clustering using infinite extensions of discrete mixture models},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional neural networks combined with feature
selection for radio-frequency fingerprinting. <em>COIN</em>,
<em>39</em>(5), 734–758. (<a
href="https://doi.org/10.1111/coin.12592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio-frequency fingerprinting is a technique for the authentication and identification of wireless devices using their intrinsic physical features and an analysis of the digitized signal collected during transmission. The technique is based on the fact that the unique physical features of the devices generate discriminating features in the transmitted signal, which can then be analyzed using signal-processing and machine-learning algorithms. Deep learning and more specifically convolutional neural networks (CNNs) have been successfully applied to the problem of radio-frequency fingerprinting using a spectral domain representation of the signal. A potential problem is the large size of the data to be processed, because this size impacts on the processing time during the application of the CNN. We propose an approach to addressing this problem, based on dimensionality reduction using feature-selection algorithms before the spectrum domain representation is given as an input to the CNN. The approach is applied to two public data sets of radio-frequency devices using different feature-selection algorithms for different values of the signal-to-noise ratio. The results show that the approach is able to achieve not only a shorter processing time; it also provides a superior classification performance in comparison to the direct application of CNNs.},
  archive      = {J_COIN},
  author       = {Gianmarco Baldini and Irene Amerini and Franc Dimc and Fausto Bonavitacola},
  doi          = {10.1111/coin.12592},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {734-758},
  shortjournal = {Comput. Intell.},
  title        = {Convolutional neural networks combined with feature selection for radio-frequency fingerprinting},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Policy generation network for zero-shot policy learning.
<em>COIN</em>, <em>39</em>(5), 707–733. (<a
href="https://doi.org/10.1111/coin.12591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong reinforcement learning is able to continually accumulate shared knowledge by estimating the inter-task relationships based on training data for the learned tasks in order to accelerate learning for new tasks by knowledge reuse. The existing methods employ a linear model to represent the inter-task relationships by incorporating task features in order to accomplish a new task without any learning. But these methods may be ineffective for general scenarios, where linear models build inter-task relationships from low-dimensional task features to high-dimensional policy parameters space. Also, the deficiency of calculating errors from objective function may arise in the lifelong reinforcement learning process when some errors of policy parameters restrain others due to inter-parameter correlation. In this paper, we develop a policy generation network that nonlinearly models the inter-task relationships by mapping low-dimensional task features to the high-dimensional policy parameters, in order to represent the shared knowledge more effectively. At the same time, we propose a novel objective function of lifelong reinforcement learning to relieve the deficiency of calculating errors by adding weight constraints for errors. We empirically demonstrate that our method improves the zero-shot policy performance across a variety of dynamical systems.},
  archive      = {J_COIN},
  author       = {Yiming Qian and Fengyi Zhang and Zhiyong Liu},
  doi          = {10.1111/coin.12591},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {707-733},
  shortjournal = {Comput. Intell.},
  title        = {Policy generation network for zero-shot policy learning},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-channel ensembled deep convolutional neural network
for facial expression recognition in the wild. <em>COIN</em>,
<em>39</em>(5), 666–706. (<a
href="https://doi.org/10.1111/coin.12586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) in the wild is an active and challenging field of research. A system for automatic FER finds use in a wide range of applications related to advanced human–computer interaction (HCI), human–robot interaction (HRI), human behavioral analysis, gaming and entertainment, etc. Since their inception, convolutional neural networks (CNNs) have attained state-of-the-art accuracy in the facial analysis task. However, recognizing facial expressions in the wild with high confidence running on a low-cost embedded device remains challenging. To this end, this study presents an efficient dual-channel ensembled deep CNN (DCE-DCNN) for FER in the wild. Initially, two DCNNs, namely the DCNN G $$ {\mathrm{DCNN}}_G $$ and DCNN S $$ {\mathrm{DCNN}}_S $$ , are trained separately on the grayscale and Scharr-convolved vertical gradient facial images, respectively. The proposed network later integrates the two pre-trained DCNNs to obtain the dual-channel integrated DCNN (DCI-DCNN). Finally, all three neural networks, namely the , and DCI-DCNN, are jointly fine-tuned to get a single dual-channel-multi-output model. The multi-output model produces three prediction scores for the given input facial image. The prediction scores are thus fused using the max-voting ensemble scheme to obtain the DCE-DCNN with the final classification label. On the FER2013, RAF-DB, NCAER-S, AffectNet, and CKPlus benchmark FER datasets, the proposed DCE-DCNN consistently outperforms the two individual DCNNs and numerous state-of-the-art CNNs. Moreover, the network achieves competitive recognition accuracy on all four FER in the wild datasets with reduced memory storage size and parameters. The proposed DCE-DCNN model with high throughput on resource-limited embedded devices is suitable for applications that seek real-time classification of facial expressions in the wild with high confidence.},
  archive      = {J_COIN},
  author       = {Sumeet Saurav and Ravi Saini and Sanjay Singh},
  doi          = {10.1111/coin.12586},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {666-706},
  shortjournal = {Comput. Intell.},
  title        = {A dual-channel ensembled deep convolutional neural network for facial expression recognition in the wild},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of acoustic source localization with adaptive
neural network using distance mating-based red deer algorithm.
<em>COIN</em>, <em>39</em>(5), 632–665. (<a
href="https://doi.org/10.1111/coin.12571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multichannel, audio processing approaches are widely examined in human–computer interaction, autonomous robots, audio surveillance, and teleconferencing systems. The numerous applications are linked to the speech technology and acoustic analysis area. Much attention is received to the active speakers and spatial localization of acoustic sources on the acoustic sensor arrays. Baseline approaches provide negotiable performance in a real-world comprised of far-field/near-field monitoring, reverberant and noisy environments, and also the outdoor/indoor scenarios. A practical system to detect defects in complex structures is the time difference mapping (TDM) technique. The significant scope of the research is to search the location using the minimum distance point in the time difference database to be apart from the verification point. In the case of the improved “time difference mapping (I-TDM)” technique and traditional “time difference mapping (T-TDM)” technique, the denser grids and vast database permit increased accuracy. In the database, if the location points are not present, then the accurate localization of the I-TDM and T-TDM techniques is damaged. Hence, to handle these problems, this article plans to develop acoustic source localization according to the deep learning strategy. The audio dataset is gathered from the benchmark source called the SSLR dataset and is initially subjected to preprocessing, which involves artifact removal and smoothing for effective processing. Further, the adaptive convolutional neural network (CNN)-based feature set creation is performed. Here, the adaptive CNN is accomplished by the improved optimization algorithm called distance mating-based red deer algorithm (DM-RDA). With this trained feature set, the acoustic source localization is done by the weight updated deep neural network, in which the same DM-RDA is used for optimizing the training weight. The simulation outcome proves that the designed model produced enhanced performance compared to other traditional source localization estimators.},
  archive      = {J_COIN},
  author       = {E. Bharat Babu and D. Hari Krishna and S. Munavvar Hussain and Santhosh Kumar Veeramalla},
  doi          = {10.1111/coin.12571},
  journal      = {Computational Intelligence},
  month        = {10},
  number       = {5},
  pages        = {632-665},
  shortjournal = {Comput. Intell.},
  title        = {Development of acoustic source localization with adaptive neural network using distance mating-based red deer algorithm},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimal cluster based security and resilience of smart
home environments using hybrid soft computing techniques. <em>COIN</em>,
<em>39</em>(4), 607–630. (<a
href="https://doi.org/10.1111/coin.12560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart home is the main part of smart intelligent system here the remote users share the sensitive information through an insecure medium to access such smart devices, which becomes security issues. The recent user authentication protocols have used to solve those problems and provide secure communication. Consumer traffic increase the risk of illegal user as legal user and radio channels are extra vulnerable to listeners. For further security enhancement, we proposed an optimal cluster based remote user authentication (OCRUA) protocol for smart home environment using hybrid soft computing techniques. The first contribution of proposed protocol is to introduce squirrel induced butterfly optimization (SBO) algorithm for cluster formation, which groups the smart devices. Then, we compute the cluster head (CH) using the teacher learning based deep neural network (TL-DNN) based on multiple design constraints. The second contribution is to illustrate remote user authentication using optimal elliptic curve cryptography (OECC) which encrypts the sensitive information before forward to gateway. At long last, the concert of planned OCRUA protocol evaluates use different replication scenarios and shows the effectiveness over the existing state-of-art protocols.},
  archive      = {J_COIN},
  author       = {Thapasimuthu Rajeswari Saravanan and Jeyaraj Kalaivani and Govindan Nagarajan},
  doi          = {10.1111/coin.12560},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {607-630},
  shortjournal = {Comput. Intell.},
  title        = {An optimal cluster based security and resilience of smart home environments using hybrid soft computing techniques},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid approach to implement multi-robotic navigation system
using neural network, fuzzy logic, and bio-inspired optimization
methodologies. <em>COIN</em>, <em>39</em>(4), 592–606. (<a
href="https://doi.org/10.1111/coin.12547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile robots have been increasingly popular in a variety of industries in recent years due to their ability to move in variable situations and perform routine jobs effectively. Path planning, without a dispute, performs a crucial part in multi-robot navigation, making it one of the very foremost investigated issues in robotics. In recent times, meta-heuristic strategies have been intensively investigated to tackle path planning issues in the similar way that optimizing issues were handled, or to design the optimal path for such multi-robotics to travel from the initial point to such goal. The fundamental purpose of portable multi-robot guidance is to navigate a mobile robot across a crowded area from initial point to target position while maintaining a safe route and creating optimum length for the path. Various strategies for robot navigational path planning were investigated by scientists in this field. This work seeks to discuss bio-inspired methods that are exploited to optimize hybrid neuro-fuzzy analysis which is the combination of neural network and fuzzy logic is optimized using the particle swarm optimization technique in real-time scenarios. Several optimization approaches of bio-inspired techniques are explained briefly. Its simulation findings, which are displayed for two simulated scenarios reveal that hybridization increases multi-robot navigation accuracy in terms of navigation duration and length of the path.},
  archive      = {J_COIN},
  author       = {Shahanaz Ayub and Navneet Singh and Md. Zair Hussain and Mohd Ashraf and Dinesh Kumar Singh and Anandakumar Haldorai},
  doi          = {10.1111/coin.12547},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {592-606},
  shortjournal = {Comput. Intell.},
  title        = {Hybrid approach to implement multi-robotic navigation system using neural network, fuzzy logic, and bio-inspired optimization methodologies},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Addressing image and poisson noise deconvolution problem
using deep learning approaches. <em>COIN</em>, <em>39</em>(4), 577–591.
(<a href="https://doi.org/10.1111/coin.12510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital images are more important in numerous contemporary applications, and the need for images in the technical field is also increasing drastically. It is used to recognize signatures and faces in many industries and is applicable for intelligent departments. The images are usually associated with the noise content; this may happen due to the instrument imperfections, troubleshooting while collecting data from the acquisition process, and another natural phenomenon. Poisson noise, also known as photon noise, is caused in the images due to the statistical essence of electromagnetic waves. X-ray, visible light, and gamma rays are electromagnetic waves. The enhancement of the convolution model in addressing images is challenging due to the various constituents such as optical aberrations, noise level, and optical setup. The modeling configuration of the image is attained using the point spread function (PSF), which is responsible for the system&#39;s impulse response. The quality image is retrieved by denoising and super-resolution (SR) methods; these methods simultaneously eliminate the noise content from the images. A Richardson–Lucy and alternating direction method of multipliers type of non-blind iterative algorithmic approaches associated with the PSF performance in addressing image is comparatively analyzed. The deep learning approach, convolutional neural networks (CNNs), is also employed to understand the nonlinear mapping relationship between the observed data and ground reality. The performance of the various network approaches is compared in this article. The result obtained shows that the deep learning CNNs achieved higher accuracy in producing denoising images. The goal of the proposed system model is to remove the interference noise in images. The high-resolution images are obtained by implementing a SR-based CNN model.},
  archive      = {J_COIN},
  author       = {Mohammad Haider Syed and Kamal Upreti and Mohammad Shahnawaz Nasir and Mohammad Shabbir Alam and Arvind Kumar Sharma},
  doi          = {10.1111/coin.12510},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {577-591},
  shortjournal = {Comput. Intell.},
  title        = {Addressing image and poisson noise deconvolution problem using deep learning approaches},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection of cardiac amyloidosis on electrocardiogram images
using machine learning and deep learning techniques. <em>COIN</em>,
<em>39</em>(4), 554–576. (<a
href="https://doi.org/10.1111/coin.12505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac amyloidosis is an uncommon disease that has been known for a long time. Moreover, modern advancement in noninvasive imaging of heart via ultrasound, magnetic resonance imaging has enhanced the detection of secret cardiac amyloidosis in patients identified with the heart disease. This article focused on detecting the heart disease especially cardiac amyloidosis on electro cardio gram images using recent technology of both machine learning and deep learning approaches. In addition, apart from detecting the disease on images, we are categorizing the heart images as normal and cardiac amyloidosis if any deviations occur. For CA disease identification along with its classification, 300 cardiac images have taken and those images are analyzed using machine learning algorithms namely nearest centroid, gradient boosting and random forest. Several metrics such as precision, recall, f-score, sensitivity, accuracy, and confusion matrix based on binary classification which classifies the images into positive (CA) and negative (non-CA) are estimated. Among these approaches, gradient boosting method achieves 95% accuracy as better outcomes which measure the model performance in detecting cardiac amyloidosis disease as well as ECG images are categorized into either normal or abnormal via classification metrics. Furthermore, we applied deep learning based neural network “DeepNet” model is applied on augmented data along with CNN which attains 93% accuracy in CA disease identification.},
  archive      = {J_COIN},
  author       = {Gladys Jebakumari Gnanadurai and Arun Raaza and Rajendran Velayutham and Sathish Kumar Palani and Ebenezer Abishek Bramwell},
  doi          = {10.1111/coin.12505},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {554-576},
  shortjournal = {Comput. Intell.},
  title        = {Detection of cardiac amyloidosis on electrocardiogram images using machine learning and deep learning techniques},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Processing capacity-based decision mechanism edge computing
model for IoT applications. <em>COIN</em>, <em>39</em>(4), 532–553. (<a
href="https://doi.org/10.1111/coin.12541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The handling of complex tasks in IoT applications becomes difficult due to the limited availability of resources in most IoT devices. There arises a need to offload the IoT tasks with huge processing and storage to resource enriched edge and cloud. In edge computing, factors such as arrival rate, nature and size of task, network conditions, platform differences and energy consumption of IoT end devices impacts in deciding an optimal offloading mechanism. A model is developed to make a dynamic decision for offloading of tasks to edge and cloud or local execution by computing the expected time, energy consumption and processing capacity. This dynamic decision is proposed as processing capacity-based decision mechanism (PCDM) which takes the offloading decisions on new tasks by scheduling all the available devices based on processing capacity. The target devices are then selected for task execution with respect to energy consumption, task size and network time. PCDM is developed in the EDGECloudSim simulator for four different applications from various categories such as time sensitiveness, smaller in size and less energy consumption. The PCDM offloading methodology is experimented through simulations to compare with multi-criteria decision support mechanism for IoT offloading (MEDICI). Strategies based on task weightage termed as PCDM-AI, PCDM-SI, PCDM-AN, and PCDM-SN are developed and compared against the five baseline existing strategies namely IoT-P, Edge-P, Cloud-P, Random-P, and Probabilistic-P. These nine strategies are again developed using MEDICI with the same parameters of PCDM. Finally, all the approaches using PCDM and MEDICI are compared against each other for four different applications. From the simulation results, it is inferred that every application has unique approach performing better in terms of response time, total task execution, energy consumption of device, and total energy consumption of applications.},
  archive      = {J_COIN},
  author       = {S. Premkumar and A.N. Sigappi},
  doi          = {10.1111/coin.12541},
  journal      = {Computational Intelligence},
  month        = {8},
  number       = {4},
  pages        = {532-553},
  shortjournal = {Comput. Intell.},
  title        = {Processing capacity-based decision mechanism edge computing model for IoT applications},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel classification-based shilling attack detection
approach for multi-criteria recommender systems. <em>COIN</em>,
<em>39</em>(3), 499–528. (<a
href="https://doi.org/10.1111/coin.12579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems are emerging techniques guiding individuals with provided referrals by considering their past rating behaviors. By collecting multi-criteria preferences concentrating on distinguishing perspectives of the items, a new extension of traditional recommenders, multi-criteria recommender systems reveal how much a user likes an item and why user likes it; thus, they can improve predictive accuracy. However, these systems might be more vulnerable to malicious attacks than traditional ones, as they expose multiple dimensions of user opinions on items. Attackers might try to inject fake profiles into these systems to skew the recommendation results in favor of some particular items or to bring the system into discredit. Although several methods exist to defend systems against such attacks for traditional recommenders, achieving robust systems by capturing shill profiles remains elusive for multi-criteria rating-based ones. Therefore, in this study, we first consider a prominent and novel attack type, that is, the power-item attack model, and introduce its four distinct variants adapted for multi-criteria data collections. Then, we propose a classification method detecting shill profiles based on various generic and model-based user attributes, most of which are new features usually related to item popularity and distribution of rating values. The experiments conducted on three benchmark datasets conclude that the proposed method successfully detects attack profiles from genuine users even with a small selected size and attack size. The empirical outcomes also demonstrate that item popularity and user characteristics based on their rating profiles are highly beneficial features in capturing shilling attack profiles.},
  archive      = {J_COIN},
  author       = {Tugba Turkoglu Kaya and Emre Yalcin and Cihan Kaleli},
  doi          = {10.1111/coin.12579},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {499-528},
  shortjournal = {Comput. Intell.},
  title        = {A novel classification-based shilling attack detection approach for multi-criteria recommender systems},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete wavelet transform based branched deep hybrid
network for environmental noise classification. <em>COIN</em>,
<em>39</em>(3), 478–498. (<a
href="https://doi.org/10.1111/coin.12577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With ever growing urbanization, the environmental noise is becoming hazardous. Vehicular traffic, locomotives, heavy machinery in industry, and construction processes are the major sources of noise pollution. It has adverse effects on the health of humans as well as that of the wild life. World Health Organization (WHO) puts noise pollution as the second major cause of illness due to environmental reasons. The effects of noise pollution on the quality of life are usually ignored. Due to this reason it is common, even in the first world countries, to have the WHO&#39;s peak noise standards violated in residential areas. Therefore, there is a need to have a real time, portable and easy to replicate, mechanism to monitor the noise sources. In this work, we propose a novel architecture of a deep neural network to classify a 10-class environmental noise data called URBANSOUND8K. This network is comprised of three components, (1) one dimensional two level Discrete Wavelet Transform (DWT) component, (2) branched component for feature extraction through auto-encoders, and (3) LSTM and fully-connected layers based classification component. With all components combined, we call this network DWTNet. By embedding the DWT component as a part of network, we eliminate the need of prior data conversion into spectral and/or spectro-temporal domains. The efficiency of DWTNet is comparable to the state of the art networks with significantly lower number of trainable parameters. We analyze the contribution of classification accuracy. We further study some of the classification results individually and show that some of the mis-classifications are actually multi-class classifications with distributed decision confidence.},
  archive      = {J_COIN},
  author       = {Syed Aamir Ali Shah and Abdul Bais and Abdulaziz Alashaikh and Eisa Alanazi},
  doi          = {10.1111/coin.12577},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {478-498},
  shortjournal = {Comput. Intell.},
  title        = {Discrete wavelet transform based branched deep hybrid network for environmental noise classification},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReSE-net: Enhanced UNet architecture for lung segmentation
in chest radiography images. <em>COIN</em>, <em>39</em>(3), 456–477. (<a
href="https://doi.org/10.1111/coin.12575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic lung segmentation in the chest x-ray is important for computer aided diagnosis. It helps in the surgical planning and diagnosis of pulmonary diseases. Lung shape, size, overlapped area, and opacities make lung segmentation arduous. In this article, we have proposed a UNet-based model for lung segmentation. We have evaluated the model on difficult datasets that have chest radiographs of patients affected by tuberculosis and other severe abnormalities. Three chest radiography datasets and a CT-scan dataset are used to prove the model generalization. The proposed model efficiently uses the residual learning and attention mechanisms to improve the segmentation results against the original UNet for the dice coefficient index (DCI) and Jaccard index. We have also performed an ablation study to highlight the impact of the attention mechanism in the proposed model. The model obtained a 97.62% DCI, 95.43% Jaccard index, and a 4.00 Hausdorff distance on the Montgomery County dataset. While on the Shenzhen and NIH datasets, it achieved a 95.71% and 95.75% DCI, 91.90% and 91.95% Jaccard index, and a 5.23 and 5.20 Hausdorff distance, respectively. The proposed model has achieved better or comparable performance against other state-of-the-art models.},
  archive      = {J_COIN},
  author       = {Tarun Agrawal and Prakash Choudhary},
  doi          = {10.1111/coin.12575},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {456-477},
  shortjournal = {Comput. Intell.},
  title        = {ReSE-net: Enhanced UNet architecture for lung segmentation in chest radiography images},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed system anomaly detection using deep
learning-based log analysis. <em>COIN</em>, <em>39</em>(3), 433–455. (<a
href="https://doi.org/10.1111/coin.12573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection is a key step in ensuring the security and reliability of large-scale distributed systems. Analyzing system logs through artificial intelligence methods can quickly detect anomalies and thus help maintenance personnel to maintain system security. Most of the current works only focus on the temporal or spatial features of distributed system logs, and they cannot sufficiently extract the global features of distributed system logs to achieve a good correct rate of anomaly detection. To further address the shortcomings of existing methods, this paper proposes a deep learning model with global spatiotemporal features to detect the presence of anomalies in distributed system logs. First, we extract semi-structured log events from log templates and model them as natural language. In addition, we focus on the temporal characteristics of logs using the bidirectional long short-term memory network and the spatial invocation characteristics of logs using the Transformer. Extensive experimental evaluations show the advantages of our proposed model for distributed system log anomaly detection tasks. The optimal F1-Score on three open-source datasets and our own collected distributed system datasets reach 98.04%, 94.34%, 88.16%, and 97.40%, respectively.},
  archive      = {J_COIN},
  author       = {Pengfei Han and Huakang Li and Gang Xue and Chao Zhang},
  doi          = {10.1111/coin.12573},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {433-455},
  shortjournal = {Comput. Intell.},
  title        = {Distributed system anomaly detection using deep learning-based log analysis},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving robustness of industrial object detection by
automatic generation of synthetic images from CAD models. <em>COIN</em>,
<em>39</em>(3), 415–432. (<a
href="https://doi.org/10.1111/coin.12572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection (OD) is used for visual quality control in factories. Images that compose training datasets are often collected directly from the production line and labeled with bounding boxes manually. Such data represent well the inference context but might lack diversity, implying a risk of overfitting. To address this issue, we propose a dataset construction method based on an automated pipeline, which receives a CAD model of an object and returns a set of realistic synthetic labeled images (code publicly available). Our approach can be easily used by non-expert users and is relevant for industrial applications, where CAD models are widely available. We performed experiments to compare the use of datasets obtained by the two different ways—collecting and labeling real images or applying the proposed automated pipeline—in the classification of five different industrial parts. To ensure that both approaches can be used without deep learning expertise, all training parameters were kept fixed during these experiments. In our results, both methods were successful for some objects but failed for others. However, we have shown that the combined use of real and synthetic images led to better results. This finding has the potential to make industrial OD models more robust to poor data collection and labeling errors, without increasing the difficulty of the training process.},
  archive      = {J_COIN},
  author       = {Igor Garcia Ballhausen Sampaio and José Viterbo and Joris Guerin},
  doi          = {10.1111/coin.12572},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {415-432},
  shortjournal = {Comput. Intell.},
  title        = {Improving robustness of industrial object detection by automatic generation of synthetic images from CAD models},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The phenomenon of decision oscillation: A new consequence of
pathology in game trees. <em>COIN</em>, <em>39</em>(3), 402–414. (<a
href="https://doi.org/10.1111/coin.12570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random minimaxing studies the consequences of using a random number for scoring the leaf nodes of a full width game tree and then computing the best move using the standard minimax procedure. Experiments in Chess showed that the strength of play increases as the depth of the lookahead is increased. Previous research by the authors provided a partial explanation of why random minimaxing can strengthen play by showing that, when one move dominates another move, then the dominating move is more likely to be chosen by minimax. This paper examines a special case of determining the move probability when domination does not occur. Specifically, we show that, under a uniform branching game tree model, whether the probability that one move is chosen rather than another depends not only on the branching factors of the moves involved, but also on whether the number of ply searched is odd or even. This is a new type of game tree pathology, where the minimax procedure will change its mind as to which move is best, independently of the true value of the game, and oscillate between moves as the depth of lookahead alternates between odd and even.},
  archive      = {J_COIN},
  author       = {Mark Levene and Trevor Fenner},
  doi          = {10.1111/coin.12570},
  journal      = {Computational Intelligence},
  month        = {6},
  number       = {3},
  pages        = {402-414},
  shortjournal = {Comput. Intell.},
  title        = {The phenomenon of decision oscillation: A new consequence of pathology in game trees},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep-CoV: An integrated deep learning model to detect
COVID-19 using chest x-ray and CT images. <em>COIN</em>, <em>39</em>(2),
369–400. (<a href="https://doi.org/10.1111/coin.12568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 virus has fatal effect on lung function and due to its rapidity the early detection is necessary at the moment. The radiographic images have already been used by the researchers for the early diagnosis of COVID-19. Though several existing research exhibited very good performance with either x-ray or computer tomography (CT) images, to the best of our knowledge no such work has reported the assembled performance of both x-ray and CT images. Thus increase in accuracy with higher scalability is the main concern of the recent research. In this article, an integrated deep learning model has been developed for detection of COVID-19 at an early stage using both chest x-ray and CT images. The lack of publicly available data about COVID-19 disease motivates the authors to combine three benchmark datasets into a single dataset of large size. The proposed model has applied various transfer learning techniques for feature extraction and to find out the best suite. Finally the capsule network is used to categorize the sub-dataset into COVID positive and normal patients. The experimental results show that, the best performance exhibits by the ResNet50 with capsule network as an extractor-classifier pair with the combined dataset, which is composed of 575 numbers of x-ray images and 930 numbers of CT images. The proposed model achieves accuracy of 98.2% and 97.8% with x-ray and CT images, respectively, and an average of 98%.},
  archive      = {J_COIN},
  author       = {Sanjib Roy and Ayan Kumar Das},
  doi          = {10.1111/coin.12568},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {369-400},
  shortjournal = {Comput. Intell.},
  title        = {Deep-CoV: An integrated deep learning model to detect COVID-19 using chest X-ray and CT images},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-armed bandit heterogeneous ensemble learning for
imbalanced data. <em>COIN</em>, <em>39</em>(2), 344–368. (<a
href="https://doi.org/10.1111/coin.12566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most widely used approaches to the class-imbalanced issue is ensemble learning. The base classifier is trained using an unbalanced training set in the conventional ensemble learning approach. We are unable to select the best suitable resampling method or base classifier for the training set, despite the fact that researchers have examined employing resampling strategies to balance the training set. A multi-armed bandit heterogeneous ensemble framework was developed as a solution to these issues. This framework employs the multi-armed bandit technique to pick the best base classifier and resampling techniques to build a heterogeneous ensemble model. To obtain training sets, we first employ the bagging technique. Then, we use the instances from the out-of-bag set as the validation set. In general, we consider the basic classifier combination with the highest validation set score to be the best model on the bagging subset and add it to the pool of model. The classification performance of the multi-armed bandit heterogeneous ensemble model is then assessed using 30 real-world imbalanced data sets that were gathered from UCI, KEEL, and HDDT. The experimental results demonstrate that, under the two assessment metrics of AUC and Kappa, the proposed heterogeneous ensemble model performs competitively with other nine state-of-the-art ensemble learning methods. At the same time, the findings of the experiment are confirmed by the statistical findings of the Friedman test and Holm&#39;s post-hoc test.},
  archive      = {J_COIN},
  author       = {Qi Dai and Jian-wei Liu and Jiapeng Yang},
  doi          = {10.1111/coin.12566},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {344-368},
  shortjournal = {Comput. Intell.},
  title        = {Multi-armed bandit heterogeneous ensemble learning for imbalanced data},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight deep learning with feature weighting for
activity recognition. <em>COIN</em>, <em>39</em>(2), 315–343. (<a
href="https://doi.org/10.1111/coin.12565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of deep learning, numerous models have been proposed for human activity recognition to achieve state-of-the-art recognition on wearable sensor data. Despite the improved accuracy achieved by previous deep learning models, activity recognition remains a challenge. This challenge is often attributed to the complexity of some specific activity patterns. Existing deep learning models proposed to address this have often recorded high overall recognition accuracy, while low recall and precision are often recorded on some individual activities due to the complexity of their patterns. Some existing models that have focused on tackling these issues are always bulky and complex. Since most embedded systems have resource constraints in terms of their processor, memory and battery capacity, it is paramount to propose efficient lightweight activity recognition models that require limited resources consumption, and still capable of achieving state-of-the-art recognition of activities, with high individual recall and precision. This research proposes a high performance, low footprint deep learning model with a squeeze and excitation block to address this challenge. The squeeze and excitation block consist of a global average-pooling layer and two fully connected layers, which were placed to extract the flattened features in the model, with best-fit reduction ratios in the squeeze and excitation block. The squeeze and excitation block served as channel-wise attention, which adjusted the weight of each channel to build more robust representations, which enabled our network to become more responsive to essential features while suppressing less important ones. By using the best-fit reduction ratio in the squeeze and excitation block, the parameters of the fully connected layer were reduced, which helped the model increase responsiveness to essential features. Experiments on three publicly available datasets (PAMAP2, WISDM, and UCI-HAR) showed that the proposed model outperformed existing state-of-the-art with fewer parameters and increased the recall and precision of some individual activities compared to the baseline, and the existing models.},
  archive      = {J_COIN},
  author       = {Ayokunle Olalekan Ige and Mohd Halim Mohd Noor},
  doi          = {10.1111/coin.12565},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {315-343},
  shortjournal = {Comput. Intell.},
  title        = {A lightweight deep learning with feature weighting for activity recognition},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Current status, application, and challenges of the
interpretability of generative adversarial network models.
<em>COIN</em>, <em>39</em>(2), 283–314. (<a
href="https://doi.org/10.1111/coin.12564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative adversarial network (GAN) is one of the most promising methods in the field of unsupervised learning. Model developers, users, and other interested people are highly concerned about the GAN mechanism where the generative model and the discriminative model learn from each other in a gameplay manner, which generates a causal relationship among output features, internal network structure, feature extraction process, and output results. Through the study of the interpretability of GANs, the validity, reliability, and robustness of the application of GANs can be verified, and the weaknesses of the GANs in specific applications can be diagnosed, which can provide support for designing better network structures. It can also improve security and reduce the decision-making and prediction risks brought by GANs. In this article, the study of the interpretability of GANs is explored, and ways of the evaluation of the application effect of GAN interpretability techniques are analyzed. Besides, the effect of interpretable GANs in fields such as medical treatment and military is discussed, and current limitations and future challenges are demonstrated.},
  archive      = {J_COIN},
  author       = {Sulin Wang and Chengqiang Zhao and Lingling Huang and Yuanwei Li and Ruochen Li},
  doi          = {10.1111/coin.12564},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {283-314},
  shortjournal = {Comput. Intell.},
  title        = {Current status, application, and challenges of the interpretability of generative adversarial network models},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep ensemble network model for classifying and predicting
breast cancer. <em>COIN</em>, <em>39</em>(2), 258–282. (<a
href="https://doi.org/10.1111/coin.12563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the leading causes of death among women worldwide. In most cases, the misinterpretation of medical diagnosis plays a vital role in increased fatality rates due to breast cancer. Breast cancer can be diagnosed by classifying tumors. There are two different types of tumors, such as malignant and benign tumors. Identifying the type of tumor is a tedious task, even for experts. Hence, an automated diagnosis is necessary. The role of machine learning in medical diagnosis is eminent as it provides more accurate results in classifying and predicting diseases. In this paper, we propose a deep ensemble network (DEN) method for classifying and predicting breast cancer. This method uses a stacked convolutional neural network, artificial neural network and recurrent neural network as the base classifiers in the ensemble. The random forest algorithm is used as the meta-learner for providing the final prediction. Experimental results show that the proposed DEN technique outperforms all the existing approaches in terms of accuracy, sensitivity, specificity, F-score and area under the curve (AUC) measures. The analysis of variance test proves that the proposed DEN model is statistically more significant than the other existing classification models; thus, the proposed approach may aid in the early detection and diagnosis of breast cancer in women, hence aiding in the development of early treatment techniques to increase survival rate.},
  archive      = {J_COIN},
  author       = {Arul Antran Vijay Subramanian and Jothi Prakash Venugopal},
  doi          = {10.1111/coin.12563},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {258-282},
  shortjournal = {Comput. Intell.},
  title        = {A deep ensemble network model for classifying and predicting breast cancer},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards physician’s experience: Development of machine
learning model for the diagnosis of autism spectrum disorders based on
complex t-spherical fuzzy-weighted zero-inconsistency method.
<em>COIN</em>, <em>39</em>(2), 225–257. (<a
href="https://doi.org/10.1111/coin.12562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorders (ASD) are a diverse group of conditions characterized by difficulty with social interaction and communication. ASD is expected to be a high-risk disease. Recent studies have focused on the diagnosis based on sociodemographic and family characteristics factors. The development of a diagnosis model, which is primarily based on machine learning methods, has been carried out to alleviate the detection of autism. However, they neglected the importance of ASD features in a training dataset, especially because some features have different levels of contributions to the processing data and possess more relevancies to the classification information than others. Such limitations use preprocessing techniques for the construction of the machine learning model, but the role of the physician&#39;s experience towards feature contributions remains limited. However, for certain autism datasets, the relevancies of sociodemographic and family characteristic feature concerning the given class labels should be considered. Accordingly, this study developed a new machine learning model for the diagnosis of ASD based on multi-criteria decision-making (MCDM). By using three methodology phases, the model combines two representative theories, namely, MCDM and machine learning. The identification phase for imbalance ASD dataset and application of pre-possessing stages by imputing missing values, feature selection of sociodemographic and family characteristics, and data imbalanced approach resulted in balanced ASD dataset, including 107,573 cases. The development phase for the new model was achieved by the proposed complex T-spherical fuzzy-weighted zero-inconsistency (CT-SFWZIC) method. CT-SFWZIC was developed based on a new fuzzy set (i.e., complex T-spherical fuzzy) for weighting affected features, and then applied for training and testing the machine learning model considering various complex T-spherical fuzzy membership functions (i.e., T = 1, 2, 3, 5, 7, and 10). The results obtained from a 10-fold cross-validation test for all T values by using nine machine learning classifiers were measured under seven evaluation metrics, namely AUC, accuracy, F1, precision, recall, training time (s), and test time (s). Performance evaluation results reveal that AdaBoost can be used to boost the ASD diagnosis as the best machine learning algorithm for all T values based on all metrics to improve the diagnosis based on physician&#39;s assessment. Under the most extreme evaluation metric, which is accuracy, the results of the AdaBoost classifiers for T = 1, 2, 3, 5, 7, 10 have obtained 0.99948, 0.99934, 0.99930, 0.99939, 0.99910, and 0.99930, respectively.},
  archive      = {J_COIN},
  author       = {Ahmed S. Albahri and Aws A. Zaidan and Hassan A. AlSattar and Rula A. Hamid and Osamah S. Albahri and Sarah Qahtan and Abdulla H. Alamoodi},
  doi          = {10.1111/coin.12562},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {225-257},
  shortjournal = {Comput. Intell.},
  title        = {Towards physician&#39;s experience: Development of machine learning model for the diagnosis of autism spectrum disorders based on complex T-spherical fuzzy-weighted zero-inconsistency method},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big data feature selection using fish and frog optimization.
<em>COIN</em>, <em>39</em>(2), 214–224. (<a
href="https://doi.org/10.1111/coin.12483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data denotes a large amount of data which includes a wide range of such methodologies like big data collection, storage, analysis, and managing the data. Every data collected in this process (homogeneous or heterogeneous considered as data), we called as big data. In this article, fish colony and their social behavior are used recently for developing an algorithm, we called as novel represented as fish swarm optimization algorithm (FSOA), which is based on the fish swarm and its behavior while search for food. The shuffled frog leaping algorithm (SFLA) is one which we introduced recently for finding near optimal solutions. The technique of Hybrid FSO-SFLA is used here for evaluating performance in big data queries.},
  archive      = {J_COIN},
  author       = {Manikandan R. P. S. and Kalpana A. M.},
  doi          = {10.1111/coin.12483},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {214-224},
  shortjournal = {Comput. Intell.},
  title        = {Big data feature selection using fish and frog optimization},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new hybrid image segmentation approach using clustering
and black hole algorithm. <em>COIN</em>, <em>39</em>(2), 194–213. (<a
href="https://doi.org/10.1111/coin.12297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering technique is used in image segmentation because of its simple and easy approach. However, the existing clustering techniques required prior information as input and the performance are entirely dependent on this prior information, which is the main drawback of the clustering approaches. Therefore, many researchers are trying to introduce a novel method with user free parameter. We proposed a clustering method, that is, independent of user parameters and later we used a region merging technique to improve the performance of the clustering output. In this article, we proposed a hybrid image segmentation method which is based on a clustering algorithm and black hole algorithm. In the clustering technique, we have used recursive density estimation technique of surrounding pixels. After clustering technique, presence of small segments may be present and it would give lower a performance of segmentation output. Therefore, a segment is merged with another segment by finding best matched segment. Black hole algorithm concept has been used to define the fitness of each segment and to find the best matching segment. We have compared the proposed method with the other clustering-based segmentation methods and different evaluation indices are used to calculate the performance, and the result proved the effectiveness of the proposed algorithm.},
  archive      = {J_COIN},
  author       = {Nameirakpam Dhanachandra and Y. Jina Chanu and Kh. Manglem Singh},
  doi          = {10.1111/coin.12297},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {194-213},
  shortjournal = {Comput. Intell.},
  title        = {A new hybrid image segmentation approach using clustering and black hole algorithm},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic speaker recognition from speech signal using
bidirectional long-short-term memory recurrent neural network.
<em>COIN</em>, <em>39</em>(2), 170–193. (<a
href="https://doi.org/10.1111/coin.12278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speaker recognition is a major challenge in various languages for researchers. For programmed speaker recognition structure prepared by utilizing ordinary speech, shouting creates a confusion between the enlistment and test, henceforth minimizing the identification execution as extreme vocal exertion is required during shouting. Speaker recognition requires more time for classification of data, accuracy is optimized, and the low root-mean-square error rate is the major problem. The objective of this work is to develop an efficient system of speaker recognition. In this work, an improved method of Wiener filter algorithm is applied for better noise reduction. To obtain the essential feature vector values, Mel-frequency cepstral coefficient feature extraction method is used on the noise-removed signals. Furthermore, input samples are created by using these extracted features after the dimensions have been reduced using probabilistic principal component analysis. Finally, recurrent neural network-bidirectional long-short-term memory is used for the classification to improve the prediction accuracy. For checking the effectiveness, the proposed work is compared with the existing methods based on accuracy, sensitivity, and error rate. The results obtained with the proposed method demonstrate an accuracy of 95.77%.},
  archive      = {J_COIN},
  author       = {Kharibam Jilenkumari Devi and Khelchandra Thongam},
  doi          = {10.1111/coin.12278},
  journal      = {Computational Intelligence},
  month        = {4},
  number       = {2},
  pages        = {170-193},
  shortjournal = {Comput. Intell.},
  title        = {Automatic speaker recognition from speech signal using bidirectional long-short-term memory recurrent neural network},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-based convolutional neural network deep learning
approach for robust malware classification. <em>COIN</em>,
<em>39</em>(1), 145–168. (<a
href="https://doi.org/10.1111/coin.12551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transforming windows files into images and its analysis using machine learning and deep learning have been considered as a state-of-the art works for malware detection and classification. This is mainly due to the fact that image-based malware detection and classification is platform independent, and the recent surge of success of deep learning model performance in image classification. Literature survey shows that convolutional neural network (CNN) deep learning methods are successfully employed for image-based windows malware classification. However, the malwares were embedded in a tiny portion in the overall image representation. Identifying and locating these affected tiny portions is important to achieve a good malware classification accuracy. In this work, a multi-headed attention based approach is integrated to a CNN to locate and identify the tiny infected regions in the overall image. A detailed investigation and analysis of the proposed method was done on a malware image dataset. The performance of the proposed multi-headed attention-based CNN approach was compared with various non-attention-CNN-based approaches on various data splits of training and testing malware image benchmark dataset. In all the data-splits, the attention-based CNN method outperformed non-attention-based CNN methods while ensuring computational efficiency. Most importantly, most of the methods show consistent performance on all the data splits of training and testing and that illuminates multi-headed attention with CNN model&#39;s generalizability to perform on the diverse datasets. With less number of trainable parameters, the proposed method has achieved an accuracy of 99% to classify the 25 malware families and performed better than the existing non-attention based methods. The proposed method can be applied on any operating system and it has the capability to detect packed malware, metamorphic malware, obfuscated malware, malware family variants, and polymorphic malware. In addition, the proposed method is malware file agnostic and avoids usual methods such as disassembly, de-compiling, de-obfuscation, or execution of the malware binary in a virtual environment in detecting malware and classifying malware into their malware family.},
  archive      = {J_COIN},
  author       = {Vinayakumar Ravi and Mamoun Alazab},
  doi          = {10.1111/coin.12551},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {145-168},
  shortjournal = {Comput. Intell.},
  title        = {Attention-based convolutional neural network deep learning approach for robust malware classification},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A privacy-preserving recommendation method with clustering
and locality-sensitive hashing. <em>COIN</em>, <em>39</em>(1), 121–144.
(<a href="https://doi.org/10.1111/coin.12549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, there is a significant increase in information, resulting in information overload. Recommendation systems have been widely adopted, and they can help users find information relevant to their interests. However, a malicious attacker can infer users&#39; private information via recommendations. To solve problems of data sparseness, enormous high-dimensional data, the cold start problem and privacy protection in an intelligent recommender system, this study proposes a privacy-preserving collaborative filtering recommendation method with clustering and locality-sensitive hashing. First, we cluster users according to their characteristic information to obtain sub-rating matrices. We use the latent factor model to predict and fill in the missing ratings in those matrices. Second, we combine the sub-rating matrices into a complete rating matrix, subsequently, we obtained the neighbors of the target user by analyzing the similarity of the users. We use a locality-sensitive hashing algorithm to reduce the dimensionality of the user rating data and build an index that could quickly obtain the neighbors of the target user. Finally, we predict the target user&#39;s ratings and provide recommendations to the target user. Through experiments, our study shows that our method can deal with the problems of data sparseness and cold start problems well and the accuracy of the intelligent recommendation system has been improved. In addition, we use hash techniques to search for the neighbors, which effectively protects the privacy of the user.},
  archive      = {J_COIN},
  author       = {Hanrui Zhang and Qianmu Li and Jiangmin Xu and Shunmei Meng and Jun Hou},
  doi          = {10.1111/coin.12549},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {121-144},
  shortjournal = {Comput. Intell.},
  title        = {A privacy-preserving recommendation method with clustering and locality-sensitive hashing},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A blockchain framework data integrity enhanced recommender
system. <em>COIN</em>, <em>39</em>(1), 104–120. (<a
href="https://doi.org/10.1111/coin.12548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender system for the IoT (RSIoT) has attracted considerable attention. By leveraging emerging technologies such as the Internet of Things (IoT), artificial intelligence, and blockchain, RSIoT improves various indicators of residents&#39; life. However, data integrity threats may affect the accuracy and consistency of the data particularly in the IoT environment where most devices are inherently dynamic and have limited resources that could fail in ensuring the quality of data transmission. Prior work has focused on processing big data and ensuring their integrity by considering cloud storage service as the popular way. In this article, we address integrity of data leveraging blockchain capabilities to ensure the integrity of the critical data. We adapted the Ethereum blockchain to our RCS for ensuring integrity of data during sharing them between doctor and patient without handling their data by third party. We build four smart contracts that enable our system of gaining more advantage of blockchain. We evaluated the performance of our smart contracts in Kovan and Rinkeby test networks. The preliminary results show the feasibility and effectiveness of the proposed solution.},
  archive      = {J_COIN},
  author       = {May Altulyan and Lina Yao and Salil Kanhere and Chaoran Huang},
  doi          = {10.1111/coin.12548},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {104-120},
  shortjournal = {Comput. Intell.},
  title        = {A blockchain framework data integrity enhanced recommender system},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent computation offloading for educational virtual
reality applications in smart campus using MoCell. <em>COIN</em>,
<em>39</em>(1), 82–103. (<a
href="https://doi.org/10.1111/coin.12539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, as the smart campus concept becomes a reality, virtual reality (VR) applications are being applied to connect students with the virtual teaching world via VR devices (VDs), enhancing learning efficiency. Nevertheless, VR applications are latency-sensitive while VDs are subjected to shortcomings to handle many VR applications simultaneously. Fortunately, mobile edge computing (MEC) has been recognized as a promising solution that can bring abundant resources to VDs to relieve hardware limits. However, the computing resources of edge nodes in MEC are limited, and thus how to allocate resources effectively is critical. Meanwhile, some sensitive information of students collected by VD needs to be protected. In view of this, we investigate computation offloading for educational VR applications in MEC-enabled smart campus. The aims of this issue are to optimize the motion-to-photon latency, energy consumption, and resource utilization while satisfying the privacy and security constraints. To this end, we propose a new multi-objective optimization method using MoCell. Eventually, the experimental evaluations are designed to illustrate the effectiveness and superiority our proposed method.},
  archive      = {J_COIN},
  author       = {Peichen Liu and Kai Peng and Peng Tao},
  doi          = {10.1111/coin.12539},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {82-103},
  shortjournal = {Comput. Intell.},
  title        = {Intelligent computation offloading for educational virtual reality applications in smart campus using MoCell},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accident responsibility identification model for internet of
vehicles based on lightweight blockchain. <em>COIN</em>, <em>39</em>(1),
58–81. (<a href="https://doi.org/10.1111/coin.12529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of autonomous vehicle technology has brought a new experience to people&#39;s daily travel. However, if a traffic accident involving autonomous vehicles occurs, it will face difficulties in vehicle accident forensic-preservation, leakage of vehicle owner&#39;s privacy, and identifying legal liabilities. This article proposes an accident responsibility identification model for the Internet of Vehicles based on lightweight blockchain to solve the above problems. This model uses Car Forensics Master to collect evidence from the accident vehicle, and at the same time collects evidence from maintenance service providers, automobile manufacturers, transportation management departments, insurance companies, and other vehicle accident related parties and stores them in the preservation chain. We also use VPKI to protect the autonomous vehicle identity privacy. In order to improve the efficiency of the model and set up authorized access to related entities, the identification of accident liability is jointly completed by the preservation chain and the accident identification chain. In addition, we prove that the protocol proposed in the model has ideal security properties. Finally, we implement the smart contracts in the model through the Solidity language, and evaluate its performance.},
  archive      = {J_COIN},
  author       = {Qing Yao and Taotao Li and Chao Yan and Zhihong Deng},
  doi          = {10.1111/coin.12529},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {58-81},
  shortjournal = {Comput. Intell.},
  title        = {Accident responsibility identification model for internet of vehicles based on lightweight blockchain},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time COVID-19 detection over chest x-ray images in edge
computing. <em>COIN</em>, <em>39</em>(1), 36–57. (<a
href="https://doi.org/10.1111/coin.12528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Severe Coronavirus Disease 2019 (COVID-19) has been a global pandemic which provokes massive devastation to the society, economy, and culture since January 2020. The pandemic demonstrates the inefficiency of superannuated manual detection approaches and inspires novel approaches that detect COVID-19 by classifying chest x-ray (CXR) images with deep learning technology. Although a wide range of researches about bran-new COVID-19 detection methods that classify CXR images with centralized convolutional neural network (CNN) models have been proposed, the latency, privacy, and cost of information transmission between the data resources and the centralized data center will make the detection inefficient. Hence, in this article, a COVID-19 detection scheme via CXR images classification with a lightweight CNN model called MobileNet in edge computing is proposed to alleviate the computing pressure of centralized data center and ameliorate detection efficiency. Specifically, the general framework is introduced first to manifest the overall arrangement of the computing and information services ecosystem. Then, an unsupervised model DCGAN is employed to make up for the small scale of data set. Moreover, the implementation of the MobileNet for CXR images classification is presented at great length. The specific distribution strategy of MobileNet models is followed. The extensive evaluations of the experiments demonstrate the efficiency and accuracy of the proposed scheme for detecting COVID-19 over CXR images in edge computing.},
  archive      = {J_COIN},
  author       = {Weijie Xu and Beijing Chen and Haoyang Shi and Hao Tian and Xiaolong Xu},
  doi          = {10.1111/coin.12528},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {36-57},
  shortjournal = {Comput. Intell.},
  title        = {Real-time COVID-19 detection over chest x-ray images in edge computing},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real geo-time-based secured access computation model for
e-health systems. <em>COIN</em>, <em>39</em>(1), 18–35. (<a
href="https://doi.org/10.1111/coin.12523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Role Back Access Control model (RBAC) allows devices to access cloud services after authentication of requests. However, it does not give priority in Big Data to devices located in certain geolocations. Regarding the crisis in a specific region, RBAC did not provide a facility to give priority access to such geolocations. In this paper, we planned to incorporate Location Time- (GEOTime) based condition alongside Priority Attribute role-based access control model (PARBAC), so requesters can be allowed/prevented from access based on their location and time. The priority concept helped to improve the performance of the existing access model. TIME-PARBAC also ensures service priorities based on geographical condition. For this purpose, the session is encrypted using a secret key. The secret key is created by mapping location, time, speed, acceleration and other information into a unique number, that is, K(Unique_Value) = location, time, speed, accelerator, other information. Spatial entities are used to model objects, user position, and geographically bounded roles. The role is activated based on the position and attributes of the user. To enhance usability and flexibility, we designed a role schema to include the name of the role and the type of role associated with the logical position and the rest of the PARBAC model proposed using official documentation available on the website for Azure internet of things (IoT) Cloud. The implementation results utilizing a health use case signified the importance of geology, time, priority and attribute parameters with supporting features to improve the flexibility of the existing access control model in the IoT Cloud.},
  archive      = {J_COIN},
  author       = {Ajay Kumar and Kumar Abhishek and Chinmay Chakraborty and Joel J. P. C. Rodrigues},
  doi          = {10.1111/coin.12523},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {18-35},
  shortjournal = {Comput. Intell.},
  title        = {Real geo-time-based secured access computation model for e-health systems},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Security alert: Generalized deep multi-view representation
learning for crime forecasting. <em>COIN</em>, <em>39</em>(1), 4–17. (<a
href="https://doi.org/10.1111/coin.12504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crime is a focal problem in modern society, affecting social stability, public safety, economic development, and life quality of residents. Promptly predicting crime occurrence places in a relatively high accuracy is a very important and meaningful research direction. Via the rapid development of social media (e.g., Twitter), the online information can act as a strong supplement for the offline information (crime records). Additionally, the geographic information and taxi flow between communities can model the spatial relationship between communities, which has already been confirmed effective in previous work. In order to efficiently solve crime prediction problem, we propose a generalized deep multi-view representation learning framework for crime forecasting. Our extensive experiments on a 4-month city-wide dataset that consists of 77 communities and 22 crime types show our model improve the prediction accuracy on most crime types.},
  archive      = {J_COIN},
  author       = {Ziwan Zheng and Yu Xia and Xiaocong Chen and Junwei Yao},
  doi          = {10.1111/coin.12504},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {4-17},
  shortjournal = {Comput. Intell.},
  title        = {Security alert: Generalized deep multi-view representation learning for crime forecasting},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Special issue on privacy, security, and trust in
computational intelligence. <em>COIN</em>, <em>39</em>(1), 2–3. (<a
href="https://doi.org/10.1111/coin.12569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COIN},
  author       = {Xuyun Zhang and Deepak Puthal and Chi Yang},
  doi          = {10.1111/coin.12569},
  journal      = {Computational Intelligence},
  month        = {2},
  number       = {1},
  pages        = {2-3},
  shortjournal = {Comput. Intell.},
  title        = {Special issue on privacy, security, and trust in computational intelligence},
  volume       = {39},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
