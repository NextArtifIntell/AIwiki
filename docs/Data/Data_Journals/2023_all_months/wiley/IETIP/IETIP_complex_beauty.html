<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IETIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ietip---315">IETIP - 315</h2>
<ul>
<li><details>
<summary>
(2023). MFFN: Multi-path feedback fusion network for lightweight
image super resolution. <em>IETIP</em>, <em>17</em>(14), 4190–4201. (<a
href="https://doi.org/10.1049/ipr2.12927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural network (CNN) has shown great power in single image super resolution (SISR) reconstruction, and achieving significant improvements over traditional methods. Despite the great success of these CNN-based methods, direct application of these methods to some edge devices is impractical due to the large computational overhead required. To address this problem, a novel, lightweight SISR network focusing on speed and accuracy, called the multi-path feedback fusion network (MFFN), has been designed in this paper. Specifically, in order to extract features more effectively, the authors propose a novel fusion attention feedback block (FAFB) as the main building block of MFFN. The FAFB consists of a backbone branch and several hierarchical branches. The backbone branch is composed of stacked enhanced pixel attentional blocks (EPAB), which are responsible for incremental deep feature learning on the feature map. And the hierarchical branches are responsible for extracting feature maps with different sizes of receptive fields and fusing these feature maps with the features extracted from the trunk branches to achieve multi-scale feature learning, which the authors refer to this design as the multi-scale fusion block (MSFB). Extra enhancement information (EIE) is added to each EPAB input, which enables the backbone branch to learn more effectively. On the other hand, the outputs of the cascade branches are further complemented by an additional feedback fusion enhancement block (FFEB) before being fused with the output of the trunk branches to achieve more comprehensive and accurate feature learning. Numerous experiments have shown that MFFN achieves higher accuracy than other state-of-the-art methods on benchmark test sets.},
  archive      = {J_IETIP},
  author       = {LiXia Xue and JunHui Shen and RongGui Wang and Juan Yang},
  doi          = {10.1049/ipr2.12927},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4190-4201},
  shortjournal = {IET Image Process.},
  title        = {MFFN: Multi-path feedback fusion network for lightweight image super resolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synchronization of boolean networks with chaos-driving and
its application in image cryptosystem. <em>IETIP</em>, <em>17</em>(14),
4176–4189. (<a href="https://doi.org/10.1049/ipr2.12926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a Boolean network model with high dimensional chaos driving and investigates the synchronization of the chaos-driven Boolean network with a semi-tensor product. In order to protect the privacy and ensure the security of image transmission, the synchronization results are utilized in the image cryptosystem to achieve compression and encryption. First, the driving chaos system is coupled with multiple local systems and synchronized with the transmitted encrypted signals. Second, the Boolean network is driven and synchronized with derived chaos signals. Finally, images are encrypted and compressed with chaos-driven Boolean network signals in the transmitter, and then decrypted and recovered with synchronized chaos and Boolean network signals in the recipient. Because of the complexities of high dimensional chaos and Boolean network, the proposed cryptosystem has good security in the secure communication and image process.},
  archive      = {J_IETIP},
  author       = {Peng-Fei Yan and Hao Zhang and Chuan Zhang and Rui-Yun Chang and Yu-Jie Sun},
  doi          = {10.1049/ipr2.12926},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4176-4189},
  shortjournal = {IET Image Process.},
  title        = {Synchronization of boolean networks with chaos-driving and its application in image cryptosystem},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on seamless image stitching based on fast marching
method. <em>IETIP</em>, <em>17</em>(14), 4159–4175. (<a
href="https://doi.org/10.1049/ipr2.12925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image stitching is an important way to achieve large-field high-resolution imaging. The inconsistencies in brightness and structure and defects in ghosting, blurring and misalignment between images, which are inevitable and difficult to eliminate, make a challenge to image stitching, due to the external lighting environment and changes in camera pose and parameters. Here, a novel method is proposed to search for the optimal seamline based on the fast marching method, which can stitch large parallax images with high quality. A feature weight map is first formed based on the similarity in colour, edge, texture and saliency of the images. Then it is used as the cost value of the seamline to search for the optimal seamline by fast marching method. The results show that this new method is more efficient to reduce defects, such as ghosting, misalignment and chromatic aberration, and realize high quality image stitching compared with traditional stitching tools and methods, which provides a new perspective for image stitching technology.},
  archive      = {J_IETIP},
  author       = {Weidong Pan and Anhu Li and Yusheng Wu and Zhaojun Deng and Xingsheng Liu},
  doi          = {10.1049/ipr2.12925},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4159-4175},
  shortjournal = {IET Image Process.},
  title        = {Research on seamless image stitching based on fast marching method},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Key point detection method for fish size measurement based
on deep learning. <em>IETIP</em>, <em>17</em>(14), 4142–4158. (<a
href="https://doi.org/10.1049/ipr2.12924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate fish size measurement in breeding areas is crucial for the fishing industry. Unlike acoustic methods with high equipment cost and low measurement accuracy, current image-based methods offer a promising alternative. However, these image-based methods still face challenges in selecting measurement points. To address this issue and achieve precise measurements of individual fish, this paper introduces an automatic fish size measurement method based on key point detection. We established a Fish-Keypoints dataset and utilized deep learning techniques for the detection of fish and their key points. Using a binocular camera system, we reconstruct a three-dimensional coordinate system to measure key points at the fish&#39;s head and tail, facilitating fish length calculation. The detection model achieves an accuracy of 85.1% in key point detection. The proposed method is tested in both land and underwater environments, demonstrating a relative measurement error of approximately 7% for fish in pools. This confirms the proposed method&#39;s ability to accurately detect measurement points, offering superior accuracy compared to other methods.},
  archive      = {J_IETIP},
  author       = {Yaozhen Yu and Hao Zhang and Fei Yuan},
  doi          = {10.1049/ipr2.12924},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4142-4158},
  shortjournal = {IET Image Process.},
  title        = {Key point detection method for fish size measurement based on deep learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lung segmentation in chest x-ray image using
multi-interaction feature fusion network. <em>IETIP</em>,
<em>17</em>(14), 4129–4141. (<a
href="https://doi.org/10.1049/ipr2.12923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung segmentation is an essential step in a computer-aided diagnosis system for chest radiographs. The lung parenchyma is first segmented in pulmonary computer-aided diagnosis systems to remove the interference of non-lung regions while increasing the effectiveness of the subsequent work. Nevertheless, most medical image segmentation methods nowadays use U-Net and its variants. These variant networks perform poorly in segmentation to detect smaller structures and cannot accurately segment boundary regions. A multi-interaction feature fusion network model based on Kiu-Net is presented in this paper to address this problem. Specifically, U-Net and Ki-Net are first utilized to extract high-level and detailed features of chest images, respectively. Then, cross-residual fusion modules are employed in the network encoding stage to obtain complementary features from these two networks. Second, the global information module is introduced to guarantee the segmented region&#39;s integrity. Finally, in the network decoding stage, the multi-interaction module is presented, which allows to interact with multiple kinds of information, such as global contextual information, branching features, and fused features, to obtain more practical information. The performance of the proposed model was assessed on both the Montgomery County (MC) and Shenzhen datasets, demonstrating its superiority over existing methods according to the experimental results.},
  archive      = {J_IETIP},
  author       = {Xuebin Xu and Meng Lei and Dehua Liu and Muyu Wang and Longbin Lu},
  doi          = {10.1049/ipr2.12923},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4129-4141},
  shortjournal = {IET Image Process.},
  title        = {Lung segmentation in chest X-ray image using multi-interaction feature fusion network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-cues underwater image restoration algorithm combined
with light field technology. <em>IETIP</em>, <em>17</em>(14), 4116–4128.
(<a href="https://doi.org/10.1049/ipr2.12922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images suffer from color distortion, low clarity and halos problems due to light absorption, particle scattering and non-uniform illumination. To address these degradation issues, a multi-cues underwater image restoration algorithm combined with light field technology is proposed. First, based on Epipolar Plane Image, the light field cue transmittance containing depth information is calculated. Then, according to the turbidity of the underwater image, the light field cue transmittance and the polarization cue transmittance are fused to obtain the multi-cues transmittance, which can effectively reduce the effect of particle scattering and color bias. Finally, the background light is estimated through the all-focus operation, which can effectively overcome the distortion of an underwater single image and simultaneously reduce the halo phenomenon. Experimental results show that the method achieves the best results evaluated by UCIQE, UIQM, PSNR, and SSIM, and the restored color under the method is closer to the actual image than other underwater restoration methods.},
  archive      = {J_IETIP},
  author       = {Xudong Zhang and Liwen Cui and Zhiguo Fan and Rui Sun and Yang Li},
  doi          = {10.1049/ipr2.12922},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4116-4128},
  shortjournal = {IET Image Process.},
  title        = {Multi-cues underwater image restoration algorithm combined with light field technology},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Progressive dilation dense residual fusion network for
single-image deraining. <em>IETIP</em>, <em>17</em>(14), 4102–4115. (<a
href="https://doi.org/10.1049/ipr2.12921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rain removal is very important for many applications in computer vision, and it is a challenging problem due to its ill-posed nature, especially for single-image deraining. In order to remove rain streaks more thoroughly, as well as to retain more details, a progressive dilation dense residual fusion network is proposed. The entire network is designed in a cascade manner with multiple fusion blocks. The fusion block consists of a dilation dense residual block (DDRB) and a dense residual feature fusion block (DRFFB), where DDRB is created for feature extraction and DRFFB is mainly designed for feature fusion operation. Meanwhile, detail compensation memory mechanism (DCMM) is leveraged between each of two cascade modules to retain more background details. Compared with previous state-of-the-art methods, extensive experiments show that the proposed method can achieve better results, in terms of rain streaks removal and background details preservation. Furthermore, the authors’ network also shows its superiority for image noise removal.},
  archive      = {J_IETIP},
  author       = {Xiaolin Kong and Tao Gao and Ting Chen and Jing Zhang},
  doi          = {10.1049/ipr2.12921},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4102-4115},
  shortjournal = {IET Image Process.},
  title        = {Progressive dilation dense residual fusion network for single-image deraining},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cuboid-net: A multi-branch convolutional neural network for
joint space-time video super resolution. <em>IETIP</em>,
<em>17</em>(14), 4089–4101. (<a
href="https://doi.org/10.1049/ipr2.12920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for high-resolution videos has been consistently rising across various domains, propelled by continuous advancements in societal. Nonetheless, limitations in imaging and economic factors often result in obtaining low-resolution images. The currently available space-time video super-resolution methods often fail to fully exploit the information existing within the spatio-temporal domain. To address this problem, the issue is tackled by conceptualizing the input low-resolution video as a cuboid structure. An innovative methodology called “Cuboid-Net”, which incorporates a multi-branch convolutional neural network, is introduced. Cuboid-Net is designed to collectively enhance the spatial and temporal resolutions of videos, enabling the extraction of rich and meaningful information across both spatial and temporal dimensions. Specifically, the input video is taken as a cuboid to generate different directional slices as input for different branches of the network. The proposed network contains four modules, that is, a multi-branch-based hybrid feature extraction module, a multi-branch-based reconstruction module, a first-stage quality enhancement module, and a second-stage cross frame quality enhancement module for interpolated frames only. Experimental results demonstrate that the proposed method is not only effective for spatial and temporal super-resolution of video but also for spatial and angular super-resolution of light field.},
  archive      = {J_IETIP},
  author       = {Congrui Fu and Hui Yuan and Hongji Xu and Hao Zhang and Liquan Shen},
  doi          = {10.1049/ipr2.12920},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4089-4101},
  shortjournal = {IET Image Process.},
  title        = {Cuboid-net: A multi-branch convolutional neural network for joint space-time video super resolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FNRegion: A fast NAM-based region extraction algorithm.
<em>IETIP</em>, <em>17</em>(14), 4076–4088. (<a
href="https://doi.org/10.1049/ipr2.12919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region extraction is usually used by many computer vision tasks as a pre-processing step to extract image features. However, how to efficiently extract effective regions remains a challenging problem. In this paper, inspired by the non-symmetry and anti-packing pattern representation model (NAM) and the FatRegion algorithm, a fast NAM-based region extraction algorithm which is called FNRegion is proposed. A NAM-based homogeneous block generation algorithm is first presented to represent an image as a combination of multiple homogeneous blocks, each of which is a square region with visually indistinguishable intra-region colour difference. Then, these homogeneous blocks are merged into larger regions according to their colour and shape information. To group these regions into larger ones in order to progressively build a region tree, a distance function is defined using variety of regional information to measure the distance between adjacent regions. Also, a multi-feature region merging algorithm with linear complexity both in time and space is presented.The proposed algorithm has been evaluated on multiple public datasets in comparison with the state-of-the-art region extraction algorithms. The experimental results show that in the case of almost the same or even less running time as other fast region extraction algorithms, the proposed algorithm is able to extract higher-quality regions.},
  archive      = {J_IETIP},
  author       = {Yunping Zheng and Bowen Yang and Mudar Sarem},
  doi          = {10.1049/ipr2.12919},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4076-4088},
  shortjournal = {IET Image Process.},
  title        = {FNRegion: A fast NAM-based region extraction algorithm},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep-sea image stitching: Using multi-channel fusion and
improved AKAZE. <em>IETIP</em>, <em>17</em>(14), 4061–4075. (<a
href="https://doi.org/10.1049/ipr2.12918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep-sea image is of great significance for exploring seabed resources. However, the information of a single image is limited. Besides, deep-sea image with low contrast and colour distortion further restricts useful feature extraction. To address the issues above, this paper presents a multi-channel fusion and accelerated-KAZE (AKAZE) feature detection algorithm for deep-sea image stitching. First, the authors restore deep-sea image in LAB colour space and RGB colour space, respectively; in LAB space, the authors use homomorphic filtering in L colour channel, and in RGB space, the authors adopt multi-scale Retinex with chromaticity preservation algorithm to adjust the colour information. Then, the authors blend two pre-processed images with dark channel prior weighted coefficient. After that, the authors detect feature points with the AKAZE algorithm and obtain feature descriptors with Boosted Efficient Binary Local Image Descriptor. Finally, the authors match the feature points and warp deep-sea images to obtain the stitched image. Experimental results demonstrate that the authors’ method generates high-quality stitched image with minimized seam. Compared with state-of-the-art algorithms, the proposed method has better quantitative evaluation, visual stitching results, and robustness.},
  archive      = {J_IETIP},
  author       = {Ping Yuan and Chunling Fan and Chuntang Zhang},
  doi          = {10.1049/ipr2.12918},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4061-4075},
  shortjournal = {IET Image Process.},
  title        = {Deep-sea image stitching: Using multi-channel fusion and improved AKAZE},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proximal linearized alternating direction method of
multipliers algorithm for nonconvex image restoration with impulse
noise. <em>IETIP</em>, <em>17</em>(14), 4044–4060. (<a
href="https://doi.org/10.1049/ipr2.12917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration with impulse noise is an important task in image processing. Taking into account the statistical distribution of impulse noise, the ℓ 1 -norm data fidelity and total variation ( ℓ 1 T V $\ell _1TV$ ) model has been widely used in this area. However, the ℓ 1 T V $\ell _1TV$ model usually performs worse when the noise level is high. To overcome this drawback, several nonconvex models have been proposed. In this paper, an efficient iterative algorithm is proposed to solve nonconvex models arising in impulse noise. Compared to existing algorithms, the proposed algorithm is a completely explicit algorithm in which every subproblem has a closed-form solution. The key idea is to transform the original nonconvex models into an equivalent constrained minimization problem with two separable objective functions, where one is differentiable but nonconvex. As a consequence, the proximal linearized alternating direction method of multipliers is employed to solve it. Extensive numerical experiments are presented to demonstrate the efficiency and effectiveness of the proposed algorithm.},
  archive      = {J_IETIP},
  author       = {Yuchao Tang and Shirong Deng and Jigen Peng and Tieyong Zeng},
  doi          = {10.1049/ipr2.12917},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4044-4060},
  shortjournal = {IET Image Process.},
  title        = {Proximal linearized alternating direction method of multipliers algorithm for nonconvex image restoration with impulse noise},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive bistable stochastic resonance based blind watermark
extraction in discrete cosine transform domain. <em>IETIP</em>,
<em>17</em>(14), 4028–4043. (<a
href="https://doi.org/10.1049/ipr2.12916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind watermark extraction in discrete cosine transform (DCT) domain has a wide application prospect as well as a challenging subject. The imperceptibility of watermark signal makes watermark extraction a weak signal reception issue in essence. For DCT coefficients of host image generally disobey Gaussian distribution, at which the performance of linear correlated reception is no longer optimal. Aiming at this, a novel blind watermark extraction scheme combining the uncorrelated reception with adaptive bistable stochastic resonance (ABSR) technique is proposed. First, by block DCT transformation for host image, an additive watermark embedding algorithm is introduced, in which the watermarked image can be converted to one dimensional time domain weak signal (binary watermark image) reception under additive Laplacian noise (selected DCT coefficients). On this basis, through the key technology research on quantitatively cooperative resonance relationship under Laplacian noise, the ABSR system can be implemented by bistable system parameters self-adaptive adjustment, in which the ABSR system output signal will be enhanced rather than be weakened by random noise. Finally, the ABSR-based watermark extraction scheme is investigated, and both the visual effect, bit error ratio performance and robustness of proposed scheme are testified to be superior to that of traditional uncorrelated extraction.},
  archive      = {J_IETIP},
  author       = {Jin Liu and Zan Li and Qiguang Miao and Peihan Qi and Danyang Wang},
  doi          = {10.1049/ipr2.12916},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4028-4043},
  shortjournal = {IET Image Process.},
  title        = {Adaptive bistable stochastic resonance based blind watermark extraction in discrete cosine transform domain},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TranSpeckle: An edge-protected transformer for medical
ultrasound image despeckling. <em>IETIP</em>, <em>17</em>(14),
4014–4027. (<a href="https://doi.org/10.1049/ipr2.12915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transformer, a type of neural architecture, has demonstrated exceptional performance improvements in vision and natural language tasks. While overcoming the disadvantages of limited perceptual field and non-adaptive input content exhibited in CNNs, the computational complexity of the Transformer model increases quadratically with spatial resolution. As such, this model is not frequently employed in image processing tasks such as image denoising, and there is a shortage of studies that investigate ultrasonic image multiplication speckle removal. In light of this, we present TranSpeckle, an effective and efficient despeckle architecture that employs Multi-Dconv Head Transposed Attention and Dconv Feed-Forward Network as the core components of its Transformer block. Multiple Transformer blocks are then utilized to implement a hierarchical encoder-decoder network. TranSpeckle architecture considerably reduces the computational complexity of feature maps while also effectively capturing long-range pixel interactions and local context information. In this study, an edge protection module is combined to augment the edges of ultrasound images. The module incorporates extracted image edge features into the TranSpeckle architecture, which ameliorates the issue of edge information loss engendered by the image despeckling process. Extensive experimental results clearly show that our proposed network outperforms state-of-the-art methods in terms of quantitative metrics and visual quality.},
  archive      = {J_IETIP},
  author       = {Yuqing Chen and Zhitao Guo},
  doi          = {10.1049/ipr2.12915},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4014-4027},
  shortjournal = {IET Image Process.},
  title        = {TranSpeckle: An edge-protected transformer for medical ultrasound image despeckling},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pose-guided adversarial video prediction for image-to-video
person re-identification. <em>IETIP</em>, <em>17</em>(14), 4000–4013.
(<a href="https://doi.org/10.1049/ipr2.12913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image-to-video (I2V) person re-identification (Re-ID) is a cross-modality pedestrian retrieval task, whose crux is to reduce the large modality discrepancy between images and videos. To this end, this paper proposes to predict the following video frames from a single image. Thus, the I2V person Re-ID can be transformed to video-to-video (V2V) Re-ID. Considering that predicting video frames from a single image is an ill-posed problem, this paper proposes two strategies to improve the quality of the predicted videos. First, a pose-guided video prediction pipeline is proposed. The given single image and pedestrian pose are encoded via image encoder and pose encoder, respectively; then, the image feature and pose feature are concatenated as the input of the video decoder. The authors minimize the difference between the predicted video and true video, and simultaneously minimize the difference between the true pose and predicted pose. Second, the conditional adversarial training strategy is employed to generate high-quality video frames. Specifically, the discriminator takes the source image as condition and distinguishes whether the input frames are fake or true following frames of the source image. Experimental results demonstrate that the pose-guided adversarial video prediction can effectively improve accuracy of I2V Re-ID.},
  archive      = {J_IETIP},
  author       = {Yunqi He and Liqiu Chen and Honghu Pan},
  doi          = {10.1049/ipr2.12913},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {4000-4013},
  shortjournal = {IET Image Process.},
  title        = {Pose-guided adversarial video prediction for image-to-video person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small object detection based on hierarchical attention
mechanism and multi-scale separable detection. <em>IETIP</em>,
<em>17</em>(14), 3986–3999. (<a
href="https://doi.org/10.1049/ipr2.12912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of modern detectors to detect small targets is still an unresolved topic compared to their capability of detecting medium and large targets in the field of object detection. Accurately detecting and identifying small objects in the real-world scenario suffer from sub-optimal performance due to various factors such as small target size, complex background, variability in illumination, occlusions, and target distortion. Here, a small object detection method for complex traffic scenarios named deformable local and global attention (DLGADet) is proposed, which seamlessly merges the ability of hierarchical attention mechanisms (HAMs) with the versatility of deformable multi-scale feature fusion, effectively improving recognition and detection performance. First, DLGADet introduces the combination of multi-scale separable detection and multi-scale feature fusion mechanism to obtain richer contextual information for feature fusion while solving the misalignment problem of classification and localisation tasks. Second, a deformation feature extraction module (DFEM) is designed to address the deformation of objects. Finally, a HAM combining global and local attention mechanisms is designed to obtain discriminative features from complex backgrounds. Extensive experiments on three datasets demonstrate the effectiveness of the proposed methods. Code is available at https://github.com/ACAMPUS/DLGADet},
  archive      = {J_IETIP},
  author       = {Yafeng Zhang and Junyang Yu and Yuanyuan Wang and Shuang Tang and Han Li and Zhiyi Xin and Chaoyi Wang and Ziming Zhao},
  doi          = {10.1049/ipr2.12912},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3986-3999},
  shortjournal = {IET Image Process.},
  title        = {Small object detection based on hierarchical attention mechanism and multi-scale separable detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive enhancement method based on stochastic parallel
gradient descent of glioma image. <em>IETIP</em>, <em>17</em>(14),
3976–3985. (<a href="https://doi.org/10.1049/ipr2.12911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumour diagnosis is significant for both physicians and patients, but the low contrast and the artefacts of MRI glioma images always affect the diagnostic accuracy. The existing mainstream image enhancement methods are insufficient in improving contrast and suppressing artefacts simultaneously. To enrich the field of glioma image enhancement, this research proposed a glioma image enhancement method based on histogram modification and total variational using stochastic parallel gradient descent (SPGD) algorithm. Firstly, this method modifies the cumulative distribution function on the image histogram and performs gamma correction on the image according to the modified histogram to obtain a contrast-enhanced image. Then, the method suppresses the artefacts of glioma images by total variational and wavelet denoising algorithm. To get better enhancement images, the optimal parameters in the proposed method are searched by the SPGD algorithm. The statistical studies performed on 580 real glioma images demonstrate that the authors’ approach can outperform the existing mainstream image enhancement methods. The results show that the proposed method increases the discrete entropy of the image by 8.9% and the contrast by 2.8% compared to original images. The enhanced images are produced by the proposed method with a natural appearance, appealing contrast, less degradation, and reasonable detail preservation.},
  archive      = {J_IETIP},
  author       = {Hongfei Wang and Xinhao Peng and ShiQing Ma and Shuai Wang and Chuan Xu and Ping Yang},
  doi          = {10.1049/ipr2.12911},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3976-3985},
  shortjournal = {IET Image Process.},
  title        = {An adaptive enhancement method based on stochastic parallel gradient descent of glioma image},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robust and clinically applicable deep learning model for
early detection of alzheimer’s. <em>IETIP</em>, <em>17</em>(14),
3959–3975. (<a href="https://doi.org/10.1049/ipr2.12910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease, often known as dementia, is a severe neurodegenerative disorder that causes irreversible memory loss by destroying brain cells. People die because there is no specific treatment for this disease. Alzheimer&#39;s is most common among seniors 65 years and older. However, the progress of this disease can be reduced if it can be diagnosed earlier. Recently, artificial intelligence has instilled hope in the diagnosis of Alzheimer&#39;s disease by performing sophisticated analyses on extensive patient datasets, enabling the identification of subtle patterns that may elude human experts. Researchers have investigated various deep learning and machine learning models to diagnose this disease at an early stage using image datasets. In this paper, a new Deep learning (DL) methodology is proposed, where MRI images are fed into the model after applying various pre-processing techniques. The proposed Alzheimer&#39;s disease detection approach adopts transfer learning for multi-class classification using brain MRIs. The MRI Images are classified into four categories: mild dementia (MD), moderate dementia (MOD), very mild dementia (VMD), and non-dementia (ND). The model is implemented and extensive performance analysis is performed. The finding shows that the model obtains 97.31% accuracy. The model outperforms the state-of-the-art models in terms of accuracy, precision, recall, and F-score.},
  archive      = {J_IETIP},
  author       = {Md Masud Rana and Md Manowarul Islam and Md. Alamin Talukder and Md Ashraf Uddin and Sunil Aryal and Naif Alotaibi and Salem A. Alyami and Khondokar Fida Hasan and Mohammad Ali Moni},
  doi          = {10.1049/ipr2.12910},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3959-3975},
  shortjournal = {IET Image Process.},
  title        = {A robust and clinically applicable deep learning model for early detection of alzheimer&#39;s},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel blind tamper detection and localization scheme for
multiple faces in digital images. <em>IETIP</em>, <em>17</em>(14),
3938–3958. (<a href="https://doi.org/10.1049/ipr2.12909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face image manipulation detection (FIMD) is a research area of great interest, widely applicable in fields requiring data security and authentication. Existing FIMD schemes aim to identify manipulations in digital face images, but they possess individual strengths and limitations. Most schemes can only detect specific manipulations under certain conditions, leading to variable success rates across different images. The literature lacks emphasis on detecting manipulations involving multiple faces. This paper introduces a novel blind tamper detection and localization scheme specifically designed for multiple faces in digital images. The proposed multiple faces manipulation detection (MFMD) scheme consists of two stages: face detection and selection, and image watermarking. Through extensive experiments, the MFMD scheme&#39;s performance has been evaluated on various multiple-face images, considering embedding capacity, payload, watermarked image quality, time complexity, and manipulation detection ability. The results demonstrate the MFMD scheme&#39;s efficacy in detecting different types of manipulations for multiple faces in images. Furthermore, the watermarked images exhibit high visual quality, even when multiple faces are present. The scheme&#39;s efficiency recommends it for practical applications, especially in sharing personal images over unsecured networks. This research advances FIMD techniques by addressing the neglected area of multiple-face manipulation detection. With improved accuracy, faster processing times, and resilience against various manipulations, the MFMD scheme offers valuable capabilities for enhancing data security and authentication in real-world scenarios.},
  archive      = {J_IETIP},
  author       = {Rasha Thabit},
  doi          = {10.1049/ipr2.12909},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3938-3958},
  shortjournal = {IET Image Process.},
  title        = {A novel blind tamper detection and localization scheme for multiple faces in digital images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight bus passenger detection model based on YOLOv5.
<em>IETIP</em>, <em>17</em>(14), 3927–3937. (<a
href="https://doi.org/10.1049/ipr2.12908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bus passenger detection algorithm is a key component of a public transportation bus management system. The detection techniques based on the convolutional neural network have been widely used in bus passenger detection. However, they require high memory and computational requirements, which hinder the deployment of bus passenger detectors in the bus system. In this paper, a lightweight bus passenger detection model based on YOLOv5 is introduced. To make the model more lightweight, the inner and outer cross-stage bottleneck modules, called ICB and OCB, respectively, are proposed. The proposed module reduces the quantity of parameter and floating point operations and increases the detection speed. In addition, the neighbour feature attention pooling is adopted to improve detection accuracy. The performance of the lightweight model on the bus passenger dataset is empirically demonstrated. The experiment results demonstrate that the proposed model is lightweight and efficient. Compared lightweight YOLOv5n with the original algorithm, the model weight is reduced by 31% to 2.6M, and the detection speed is increased by 6% to 40FPS without an accuracy drop.},
  archive      = {J_IETIP},
  author       = {Xiaosong Li and Yanxia Wu and Yan Fu and Lidan Zhang and Ruize Hong},
  doi          = {10.1049/ipr2.12908},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3927-3937},
  shortjournal = {IET Image Process.},
  title        = {A lightweight bus passenger detection model based on YOLOv5},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature matching of remote-sensing images based on bilateral
local–global structure consistency. <em>IETIP</em>, <em>17</em>(14),
3909–3926. (<a href="https://doi.org/10.1049/ipr2.12907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of feature matching is to establish accurate correspondences between feature points in different images depicting the same scene. To address the polymorphism of local structures, the authors propose a mismatch removal method using bilateral local–global structural consistency. This method incorporates the problem of mismatch removal into the framework of graph matching, constructs a global affinity matrix using local structural similarity and global affine transformation consistency, and optimizes it using a constrained integer quadratic programming method. To comprehensively describe the local structure, the signature quadratic form distance (SQFD) is used to measure the consistency of the neighbourhood structure. Specifically, the weights of edges are constructed based on the SQFD of the local structure, while the matching correctness of nodes and edges between the two graphs is described using local vector similarity. Furthermore, the consistency of the global affine transformation is evaluated by assessing the consistency of the local neighbourhood affine transformation between different corresponding point pairs. In estimating the local affine transformation, a bilateral correction is performed using a total least-squares (TLS) algorithm to measure the similarity of nodes between the two different graphs. Experimental results demonstrate that the proposed algorithm outperforms state-of-the-art methods in terms of accuracy and effectiveness.},
  archive      = {J_IETIP},
  author       = {Qing-Yan Chen and Da-Zheng Feng},
  doi          = {10.1049/ipr2.12907},
  journal      = {IET Image Processing},
  month        = {12},
  number       = {14},
  pages        = {3909-3926},
  shortjournal = {IET Image Process.},
  title        = {Feature matching of remote-sensing images based on bilateral local–global structure consistency},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SFTN: Fast object detection for aerial images.
<em>IETIP</em>, <em>17</em>(13), 3897–3907. (<a
href="https://doi.org/10.1049/ipr2.12906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of remote sensing image object detection in low latency scenes is of great research significance. To address the problem that the current high-precision object detection algorithm based on a feature pyramid network is slow due to a large number of parameters and complicated computation, a fast remote sensing image object detection method based on a Single-scale Feature Transformation Network (SFTN) is proposed. Firstly, based on the single-scale remote sensing image features, the new channel features are quickly generated by a linear transformation of the original features and convolution kernel clustering optimization using cosine similarity; secondly, in order to obtain multi-scale receptive fields, a parallel residual hole convolution module is designed to cover multi-category remote sensing object scales on the feature map; finally, angle variables are introduced and optimized using angle similarity to effectively improve the object orientation accuracy. The experimental results on different datasets show that the method in this paper improves the detection speed rapidly while ensuring the accuracy of remote sensing image object detection, which is better than many remote sensing image object detection methods. The results demonstrate the reliability and robustness of the method.},
  archive      = {J_IETIP},
  author       = {Li Chen and Fan Zhang and Wei Guo and Tianyang Li and Mingqian Sun},
  doi          = {10.1049/ipr2.12906},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3897-3907},
  shortjournal = {IET Image Process.},
  title        = {SFTN: Fast object detection for aerial images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDAUNet: A simple dual attention mechanism UNet for mixed
noise removal. <em>IETIP</em>, <em>17</em>(13), 3884–3896. (<a
href="https://doi.org/10.1049/ipr2.12905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have demonstrated impressive results in additive white Gaussian noise removal due to their strong fitting ability. However, their performance in mixed noise removal remains unsatisfactory, primarily due to their limited receptive field that focuses only on the local features of images and disregards global information. To ameliorate this issue, recent state-of-the-art approaches employ attention mechanism (AM) to capture the global information. However, most AM based methods still suffer from low computational efficiency. In this paper, a novel model named simple dual attention mechanism UNet (SDAUNet) for mixed noise removal is proposed. In SDAUNet, the UNet architecture is used to gradually acquire multi-scale image features and provide a more comprehensive and accurate representation of the image features than other CNNs. A simple dual attention convolutional block is presented to acquire the global image features that can successfully capture image details with a low burden. The experimental results demonstrate that the SDAUNet model can achieve better measurement metrics and visual performance than other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jielin Jiang and Xiangming Hong and Yingnan Zhao and Xiaonglong Xu and Yan Cui},
  doi          = {10.1049/ipr2.12905},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3884-3896},
  shortjournal = {IET Image Process.},
  title        = {SDAUNet: A simple dual attention mechanism UNet for mixed noise removal},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight deep learning model for real-time face
recognition. <em>IETIP</em>, <em>17</em>(13), 3869–3883. (<a
href="https://doi.org/10.1049/ipr2.12903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight deep learning models for face recognition are becoming increasingly crucial for deployment on resource-constrained devices such as embedded systems or mobile devices. This paper presents a highly efficient and compact deep learning (DL) model that achieves state-of-the-art performance on various face recognition benchmarks. The developed DL model employs one- or few-shot learning to obtain effective feature embeddings and draws inspiration from FaceNet with significant refinements to achieve a memory size of only 3.5 MB—about 30 times smaller than FaceNet—while maintaining high accuracy and real-time performance. The study demonstrates the model&#39;s effectiveness through extensive experiments, which include testing on public datasets and the model&#39;s ability to recognize occluded faces in uncontrolled environments using grayscale input images. Compared to the state-of-the-art lightweight models, the proposed model requires fewer FLOPs (0.06G), has a smaller number of parameters (1.2 M), and occupies a smaller model size (3.5 MB) while achieving a competitive level of recognition accuracy and real-time performance. The results show that the model is well-suited for deployment in embedded domains, including live entrance security checks, driver authorization, and in-class attendance systems. The entire code of FN8 is available on GitHub .},
  archive      = {J_IETIP},
  author       = {Zong-Yue Deng and Hsin-Han Chiang and Li-Wei Kang and Hsiao-Chi Li},
  doi          = {10.1049/ipr2.12903},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3869-3883},
  shortjournal = {IET Image Process.},
  title        = {A lightweight deep learning model for real-time face recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recovery performance improvement of image compressive
sensing using complex-valued vandermonde matrix. <em>IETIP</em>,
<em>17</em>(13), 3856–3868. (<a
href="https://doi.org/10.1049/ipr2.12902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, a novel image-based quantized compressive sensing (QCS) framework based on complex-valued Vandermonde (Vander) matrix is proposed. In the proposed QCS framework, a discrete wavelet transform (DWT) serves as a sparse basis and a partial complex-valued Vander matrix serves as a measurement matrix. The theoretical analysis based on mutual coherence metric of compressive sensing (CS) theory shows that the proposed Vander measurement matrix has the best reconstruction performance among other conventional measurement matrices. The simulation results also show that the recovery quality using the proposed measurement matrix can be greatly improved compared with the other existing real-valued measurement matrices. In particular, the experiment results also show that under the same measurement matrix, the reconstruction performance of Smoothed l 0 norm (SL0) algorithm is better than that of Orthogonal Matching Pursuit (OMP) algorithm, sparsity adaptive matching pursuit (SAMP) algorithm and approximate message passing (AMP) algorithm. In addition, a sparse measurement matrix scheme is further proposed to achieve a trade-off between recovery performance and computational complexity. The theoretical analysis and simulation results both show the proposed image-based QCS is efficient.},
  archive      = {J_IETIP},
  author       = {Weiwei Qiu and Linlin Xue and Zhongpeng Wang},
  doi          = {10.1049/ipr2.12902},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3856-3868},
  shortjournal = {IET Image Process.},
  title        = {Recovery performance improvement of image compressive sensing using complex-valued vandermonde matrix},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cformer: An underwater image enhancement hybrid network
combining convolution and transformer. <em>IETIP</em>, <em>17</em>(13),
3841–3855. (<a href="https://doi.org/10.1049/ipr2.12901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images are the most direct and effective ways to obtain underwater information. However, underwater images typically suffer from contrast reduction and colour distortion due to the absorption and scattering of water by light, which seriously limits the further development of underwater visual tasks. Recently, the convolutional neural network has been extensively applied in underwater image enhancement for its powerful local information extraction capabilities, but due to the locality of convolution operation, it cannot capture the global context well. Although the recently emerging Transformer can capture global context, it cannot model local correlations. Cformer is proposed, which is an Unet-like hybrid network structure. First, a Depth Self-Calibrated block is proposed to extract the local features of the image effectively. Second, a novel Cross-Shaped Enhanced Window Transformer block is proposed. It captures long-range pixel interactions while dramatically reducing the computational complexity of feature maps. Finally, the depth self-calibrated block and the cross-shaped enhanced window Transformer block are ingeniously fused to build a global–local Transformer module. Extensive ablation studies are performed on public underwater datasets to demonstrate the effectiveness of individual components in the network. The qualitative and quantitative comparisons indicate that Cformer achieves superior performance compared to other competitive models.},
  archive      = {J_IETIP},
  author       = {Ruhui Deng and Lei Zhao and Heng Li and Hui Liu},
  doi          = {10.1049/ipr2.12901},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3841-3855},
  shortjournal = {IET Image Process.},
  title        = {Cformer: An underwater image enhancement hybrid network combining convolution and transformer},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware iterative learning for noisy-labeled
medical image segmentation. <em>IETIP</em>, <em>17</em>(13), 3830–3840.
(<a href="https://doi.org/10.1049/ipr2.12900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation from noisy labels is an important task since obtaining high-quality annotations is extremely difficult and expensive. There are a lot of approaches proposed for such task. However, some issues like the overfitting on noisy annotations, the limited learning of boundary features, and no consideration of the corrupted local pixels are still not solved. Therefore, a novel approach named uncertainty-aware iterative learning (UaIL) is proposed for medical image segmentation with noisy labels. UaIL iteratively and jointly trains two deep networks using the original images and their argumented ones through a joint loss function including softened label loss, hard label loss and consistency loss, which encourages UaIL to produce segmentations that are robust to the perturbations in arbitrary semantic space. The uncertainty of labels is estimated based on the predictions in iterative learning, then the original labels are refined, which improves the learning of boundary features in segmentation. To avoid overfitting, a stopping strategy is designed based on the dice coefficient in iterative learning. Experiments on two public datasets verify the effectiveness of UaIL under different levels of annotation noise. Especially, when there are serious noises in the labels, the dice achieved by UaIL is 1.43% to 15.03% higher than the competing approaches on the two public datasets. The UaIL is further verified on a private dataset, which shows its ability of applying in the real application with noisy labels.},
  archive      = {J_IETIP},
  author       = {Pengyi Hao and Kangjian Shi and Shuyuan Tian and Fuli Wu},
  doi          = {10.1049/ipr2.12900},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3830-3840},
  shortjournal = {IET Image Process.},
  title        = {Uncertainty-aware iterative learning for noisy-labeled medical image segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TTN-FCN: A tangut character classification framework by tree
tensor network and fully connected neural network. <em>IETIP</em>,
<em>17</em>(13), 3815–3829. (<a
href="https://doi.org/10.1049/ipr2.12899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of Tangut characters plays a significant role in Western Xia research, and yet it is still a great challenge to recognize Tangut characters accurately due to the less inter-class similarity and smaller intra-class variation of Tangut character images. The main reason is that Tangut characters possess an extremely intricate construct despite some other character recognition methods emerging. For this reason, the authors propose a novel framework for Tangut character classification, named tree tensor network-fully connected neural network (TTN-FCN), in which TTN is embedded to fully connect neural network. Firstly, Tangut images are encoded into quantum product states without entanglement in pre-processing. Then the TTN is adopted to contract quantum product states to intermediate low dimensional quantum states. Finally, low dimensional quantum states are input to the FCN network to perform classification tasks. The Model is evaluated on the Tangut character dataset that is constructed from Tangut character-related documents by scanning and consists of 30,293 Tangut character images with 6077 categories. Experimental results show that TTN-FCN has a faster convergence speed and achieves classification precision (AC) of 99.98% and loss of 0.688% with the max batch size 2042, which outperforms 30 compared networks. Moreover, the proposed model can also be generalized to other character recognition, which enhances its potential for cultural relic research and development.},
  archive      = {J_IETIP},
  author       = {Ziping Ma and Jinlin Ma},
  doi          = {10.1049/ipr2.12899},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3815-3829},
  shortjournal = {IET Image Process.},
  title        = {TTN-FCN: A tangut character classification framework by tree tensor network and fully connected neural network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FPIseg: Iterative segmentation network based on feature
pyramid for few-shot segmentation. <em>IETIP</em>, <em>17</em>(13),
3801–3814. (<a href="https://doi.org/10.1049/ipr2.12898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) enables rapid adaptation to the segmentation task of unseen-classes object based on a few labelled support samples. Currently, the focal point of research in the FSS field is to align features between support and query images, aiming to improve the segmentation performance. However, most existing FSS methods implement such support/query alignment by solely leveraging middle-level feature for generalization, ignoring the category semantic information contained in high-level feature, while pooling operation inevitably lose spatial information of the feature. To alleviate these issues, the authors propose the Iterative Segmentation Network Based on Feature Pyramid (FPIseg), which mainly consists of three modules: Feature Pyramid Fusion Module (FPFM), Region Feature Enhancement Module (RFEM), and Iterative Optimization Segmentation Module (IOSM). Firstly, FPFM fully utilizes the foreground information from the support image to implement support/query alignment under multi-scale, multi-level semantic backgrounds. Secondly, RFEM enhances the foreground detail information of aligned feature to improve generalization ability. Finally, ISOM iteratively segments the query image to optimize the prediction result and improve segmentation performance. Extensive experiments on the PASCAL-5 i and COCO-20 i datasets show that FPIseg achieves considerable segmentation performance under both 1-shot and 5-shot settings.},
  archive      = {J_IETIP},
  author       = {Ronggui Wang and Cong Yang and Juan Yang and Lixia Xue},
  doi          = {10.1049/ipr2.12898},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3801-3814},
  shortjournal = {IET Image Process.},
  title        = {FPIseg: Iterative segmentation network based on feature pyramid for few-shot segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). SaTransformer: Semantic-aware transformer for breast cancer
classification and segmentation. <em>IETIP</em>, <em>17</em>(13),
3789–3800. (<a href="https://doi.org/10.1049/ipr2.12897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer classification and segmentation play an important role in identifying and detecting benign and malignant breast lesions. However, segmentation and classification still face many challenges: 1) The characteristics of cancer itself, such as fuzzy edges, complex backgrounds, and significant changes in size, shape, and intensity distribution make accurate segment and classification challenges. 2) Existing methods ignore the potential relationship between classification and segmentation tasks, due to the classification and segmentation being treated as two separate tasks. To overcome these challenges, in this paper, a novel Semantic-aware transformer (SaTransformer) for breast cancer classification and segmentation is proposed. Specifically, the SaTransformer enables doing the two takes simultaneously through one unified framework. Unlike existing well-known methods, the segmentation and classification information are semantically interactive, reinforcing each other during feature representation learning and improving the ability of feature representation learning while consuming less memory and computational complexity. The SaTransformer is validated on two publicly available breast cancer datasets – BUSI and UDIAT. Experimental results and quantitative evaluations (accuracy: 97.97%, precision: 98.20%, DSC: 86.34%) demonstrate that the SaTransformer outperforms other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Jie Zhang and Zhichao Zhang and Hua Liu and Shiqiang Xu},
  doi          = {10.1049/ipr2.12897},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3789-3800},
  shortjournal = {IET Image Process.},
  title        = {SaTransformer: Semantic-aware transformer for breast cancer classification and segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Image segmentation by selecting eigenvectors based on
extended information entropy. <em>IETIP</em>, <em>17</em>(13),
3777–3788. (<a href="https://doi.org/10.1049/ipr2.12896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For spectral clustering algorithm, the quality of eigenvectors of graph affinity matrix is very important for the clustering result. So, how to obtain high-quality eigenvectors is crucial. In this paper, the authors aim to propose some new measurement methods to evaluate each eigenvector of affinity matrix for spectral selection. Based on extended information entropy, three criteria, i.e. Spectral Distinguishability (SD), Spectral Distinguishability Validity (SDV) and pectral Distinguishability -Degree (SDD), are defined respectively. The compactness of clusters for each eigenvector is measured by SD; SDV is used to remove the inefficient eigenvectors for clustering; SDD is used to evaluate the contribution of eigenvectors to clustering and is exploited to build a selective spectral ensemble scheme. To indicate the merits of the authors’ algorithm, the authors consider varied artificial data and natural images, including Berkeley image segmentation data set as benchmark data set. The authors’ simulation results confirm the superior performance of the proposed method in developing spectral clustering compared to conventional clustering methods and recent eigenvectors-selection-based algorithms.},
  archive      = {J_IETIP},
  author       = {Daming Zhang and Xueyong Zhang and Huayong Liu},
  doi          = {10.1049/ipr2.12896},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3777-3788},
  shortjournal = {IET Image Process.},
  title        = {Image segmentation by selecting eigenvectors based on extended information entropy},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SSTNet: Saliency sparse transformers network with tokenized
dilation for salient object detection. <em>IETIP</em>, <em>17</em>(13),
3759–3776. (<a href="https://doi.org/10.1049/ipr2.12895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vision Transformer structure performs better in salient object detection than the convolutional neural network (CNN)-based approach. Vision Transformer predicts saliency by modelling long-range dependencies from sequence to sequence with convolution-free. It is challenging to distinguish the salient objects&#39; location and obtain structural details for the influence of extracting irrelevant contextual information. A novel saliency sparse Transformer network is proposed to exploit sparse attention to guide saliency prediction. The convolution-like with dilation in the token to token (T2T) module is replaced to achieve relationships in larger regions and to improve contextual information fusion. An adaptive position bias module is designed for the Vision Transformer to make position bias suitable for variable-sized RGB images. A saliency sparse Transformer module is designed to improve the concentration of attention on the global context by selecting the Top-k of the most relevant segments to improve the detection results further. Besides, cross-modality to exploit the complementary RGB and depth modality fusion module (CMF) is used to take advantage of the complementary RGB image features and spatial depth information to enhance the feature fusion performance. Extensive experiments on multiple benchmark datasets demonstrate this method&#39;s effectiveness and superiority that it is suitable for saliency prediction comparable to state-of-the-art RGB and RGB-D saliency methods.},
  archive      = {J_IETIP},
  author       = {Mo Yang and Ziyan Liu and Wen Dong and Ying Wu},
  doi          = {10.1049/ipr2.12895},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3759-3776},
  shortjournal = {IET Image Process.},
  title        = {SSTNet: Saliency sparse transformers network with tokenized dilation for salient object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ARDA-UNIT recurrent dense self-attention block with adaptive
feature fusion for unpaired (unsupervised) image-to-image translation.
<em>IETIP</em>, <em>17</em>(13), 3746–3758. (<a
href="https://doi.org/10.1049/ipr2.12894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most challenging topics in artificial intelligence is image-to-image translation, the purpose of which is generating images close to those in the target domain while preserving the important features of the images in the source domain. In this direction, various types of generative adversarial networks have been developed. ARDA-UNIT, presented in this paper, seeks to meet the main challenges of these networks, that is, producing a high-quality image in a reasonable amount of time, and transferring content between two images with different structures. The proposed recurrent dense self-attention block, applied in ARDA-UNIT&#39;s generator latent space, simultaneously increases its generating capability and decreases the training parameters. ARDA-UNIT has a feature extraction module which feeds both the generator and the discriminator. This module uses a new adaptive feature fusion method which combines multi-scale features in such a way that the characteristics of each scale are preserved. The module also uses a pre-trained CNN that reduces the training parameters. Moreover, a feature similarity loss is introduced that guides the model to change the structure of the source domain in accordance with that in the target domain. Experiments performed on different datasets using FID, KID and IS evaluation criteria have shown that the model reduces computational loads, transfers structures well, and achieves better qualities.},
  archive      = {J_IETIP},
  author       = {Farzane Maghsoudi Ghombavani and Mohammad Javad Fadaeieslam and Farzin Yaghmaee},
  doi          = {10.1049/ipr2.12894},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3746-3758},
  shortjournal = {IET Image Process.},
  title        = {ARDA-UNIT recurrent dense self-attention block with adaptive feature fusion for unpaired (unsupervised) image-to-image translation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An efficient deep multi-task learning structure for
covid-19 disease. <em>IETIP</em>, <em>17</em>(13), 3728–3745. (<a
href="https://doi.org/10.1049/ipr2.12893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 has had a profound global impact, necessitating the development of infection detection systems based on machine learning. This paper presents a Multi-task architecture that addresses the classification and segmentation tasks for COVID-19 detection. The model comprises an encoder for feature representation, a decoder for segmentation, and a multi-layer perceptron for classification. Evaluations conducted on two datasets demonstrate the model&#39;s performance in both classification and segmentation. To enhance efficiency and diagnosis accuracy, CT-scan images undergo pre-processing using image processing algorithms like histogram equalization, median filtering, and mathematical morphology operations. The combination of the median filter pre-processing and the proposed model yields impressive results in the classification task, achieving high accuracy, sensitivity, and specificity, with values of 0.97, 0.97, and 0.96, respectively, for dataset 1, and 0.96 in mentioned metrics for dataset 2. For segmentation, the proposed model, particularly with the average morphology pre-processing, exhibits excellent performance with high accuracy, low mean squared error, high peak signal-to-noise ratio, high structural similarity index, and a mean dice coefficient of 88.86 ± 0.05 for dataset 1, and 87.97 ± 0.02 for dataset 2. Furthermore, the pre-trained models consistently demonstrate the superiority of the median filter and proposed model in the classification task on the same datasets. In conclusion, the proposed multi-task model, incorporating image processing techniques, achieves remarkable results in both classification and segmentation. The utilization of pre-processing algorithms and the multi-task framework significantly contribute to superior performance metrics. This study encourages further exploration of combining diverse image processing algorithms to advance infection diagnosis and treatment.},
  archive      = {J_IETIP},
  author       = {Shirin Kordnoori and Maliheh Sabeti and Hamidreza Mostafaei and Saeed Seyed Agha Banihashemi},
  doi          = {10.1049/ipr2.12893},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3728-3745},
  shortjournal = {IET Image Process.},
  title        = {An efficient deep multi-task learning structure for covid-19 disease},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MDSC-net: A multi-scale depthwise separable convolutional
neural network for skin lesion segmentation. <em>IETIP</em>,
<em>17</em>(13), 3713–3727. (<a
href="https://doi.org/10.1049/ipr2.12892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  author       = {Yun Jiang and Hao Qiao and Zequn Zhang and Meiqi Wang and Wei Yan and Jie Chen},
  doi          = {10.1049/ipr2.12892},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3713-3727},
  shortjournal = {IET Image Process.},
  title        = {MDSC-net: A multi-scale depthwise separable convolutional neural network for skin lesion segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active contour model based on hybrid signed pressure force
function. <em>IETIP</em>, <em>17</em>(13), 3702–3712. (<a
href="https://doi.org/10.1049/ipr2.12891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To segment noisy and multi-target images, an active contour model based on a hybrid signed pressure force function that fuses global and local information of the image is proposed. Firstly, a local signed pressure force function is defined using the local area information of the image. Then, it is combined with the existing global grey density function to construct a new signed pressure force function. Finally, the selective binary and Gaussian filtering regularized level set are modified using the newly defined function. Extensive experiments and comparisons on both synthetic and real images demonstrate that the proposed method is robust to noise and can handle noisy and multi-target images with high accuracy.},
  archive      = {J_IETIP},
  author       = {XinChao Meng and Si Si},
  doi          = {10.1049/ipr2.12891},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3702-3712},
  shortjournal = {IET Image Process.},
  title        = {Active contour model based on hybrid signed pressure force function},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An active learning method based on result quality evaluation
for printed circuit board computed tomography image segmentation.
<em>IETIP</em>, <em>17</em>(13), 3688–3701. (<a
href="https://doi.org/10.1049/ipr2.12888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Element detection is a key step in non-destructive testing of printed circuit board (PCB) based on computed tomography (CT). In recent years, some image segmentation methods based on deep learning have shown great potential in the element segmentation task of PCB CT images, and greatly improved the efficiency and accuracy. However, since image segmentation is based on pixel-level classification, the annotation of training data is difficult and costly. Aiming at this problem, the authors proposed a new active learning method based on the integration of relevant information about segmentation tasks and data variance. In this method, the Result Quality Evaluation Module (RQEM) proposed by us is used to generate task-related information, and an adversarial network is used to generate the difference information between samples and the initial labelled data. Finally, the two parts of information are fused and used as the standard of data selecting. In the PCB CT image element segmentation task, the authors only need to select 12.7% of the whole training set with their proposed method to make Mean Intersection Over Union (MIOU) reach 79.7, which has reached 95% of the optimal performance of 83.7. The in-depth analysis also verifies the effectiveness and stability of the authors’ method.},
  archive      = {J_IETIP},
  author       = {Baojie Song and Kai Qiao and Jie Yang and Shuhao Shi and Jian Chen and Bin Yan},
  doi          = {10.1049/ipr2.12888},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3688-3701},
  shortjournal = {IET Image Process.},
  title        = {An active learning method based on result quality evaluation for printed circuit board computed tomography image segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale and multidirection depth map super resolution
with semantic inference. <em>IETIP</em>, <em>17</em>(13), 3670–3687. (<a
href="https://doi.org/10.1049/ipr2.12877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth map super resolution has been paid much attention in 3D applications due to the limitation of depth sensors. Few textures in objects with clear contours along them is the most important characteristic of depth map. An efficient image representation should be directional, multiscale and anisotropic. From this we propose a novel multiscale and multidirection depth map super resolution framework with semantic inference to improve the quality of depth maps. In this framework, a multiscale and multidirection depth map contour fusion scheme captures and assembles intrinsic geometrical structures through a multiview non-subsampled contourlet transform manner. This scheme not only isolates the discontinuities of contours but retains the smoothness along the contours. The semantic inference is also utilized to segment and label the depth map into objects/backgrounds-level which are coplanar. Furthermore, a semantic-aware label refinement strategy is introduced to correct the rarely inaccurate labels of the label map for upscaling the target pixel with pixels in the same object or background. Experimental results on benchmark depth map dataset demonstrate that the proposed multiscale and multidirection depth map super resolution framework with semantic inference has a significant improvement than the state-of-the-art algorithms both visually and quantitatively.},
  archive      = {J_IETIP},
  author       = {Dan Xu and Xiaopeng Fan and Debin Zhao and Wen Gao},
  doi          = {10.1049/ipr2.12877},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3670-3687},
  shortjournal = {IET Image Process.},
  title        = {Multiscale and multidirection depth map super resolution with semantic inference},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric correction code-based robust image watermarking.
<em>IETIP</em>, <em>17</em>(13), 3660–3669. (<a
href="https://doi.org/10.1049/ipr2.12143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital image watermarking is one of the effective schemes to protect the copyrights of still images. However, the existing watermarking schemes are still not robust enough to the common geometric transformation attacks such as arbitrary rotation, scaling and shifting with desirable hiding capacity. To address this issue, we propose a robust watermarking scheme based on geometric correction codes (GCCs). In this scheme, the watermark and pre-set GCCs are combined and embedded into a cover image to obtain the watermarked image. At the stage of watermark extraction, the watermarked image, under a variety of geometric transformation attacks, can be geometrically corrected by minimising the difference between the extracted and the original GCCs, then the watermark is extracted from the watermarked image. The experiments demonstrate that, compared to the typical watermarking schemes, the proposed scheme achieves much higher robustness to the common geometric transformation attacks and comparable invisibility with the same embedding capacity.},
  archive      = {J_IETIP},
  author       = {Zhili Zhou and Jianyu Zhu and Yuecheng Su and Meimin Wang and Xingming Sun},
  doi          = {10.1049/ipr2.12143},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3660-3669},
  shortjournal = {IET Image Process.},
  title        = {Geometric correction code-based robust image watermarking},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A non-invertible transformation based technique to protect a
fingerprint template. <em>IETIP</em>, <em>17</em>(13), 3645–3659. (<a
href="https://doi.org/10.1049/ipr2.12130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fingerprint-based authentication system provides security to the applications of numerous fields and usually stores minutiae information in the database as a template. It has been observed from the literature that the reconstruction of an original fingerprint is possible from minutiae points information; hence the security of the stored template becomes extremely crucial. Cancellable biometric techniques based on non-invertible transformation protect the stored template. These techniques prevent the reconstruction of original fingerprint data from the compromised template and avoid unauthorized access to the system. In this paper, a technique based on the non-invertible transformation to protect a fingerprint template is proposed. In the technique, minutiae locations in a fingerprint are transformed by using the minutiae&#39;s original locations and orientation information, and a user keyset. A principal component analysis based approach to align the probe and gallery templates of fingerprint images while matching is also proposed. The evaluation of the proposed technique is carried out on seven different fingerprint databases taken from FVC2000, FVC2002, and FVC2004 databases, and its performance is compared with other existing state-of-the-art techniques in the literature. The comparative performance shows that the proposed technique is highly robust and performs exceptionally well compared to other existing techniques.},
  archive      = {J_IETIP},
  author       = {Vivek Singh Baghel and Syed Sadaf Ali and Surya Prakash},
  doi          = {10.1049/ipr2.12130},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3645-3659},
  shortjournal = {IET Image Process.},
  title        = {A non-invertible transformation based technique to protect a fingerprint template},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DMAGNet: Dual-path multi-scale attention guided network for
medical image segmentation. <em>IETIP</em>, <em>17</em>(13), 3631–3644.
(<a href="https://doi.org/10.1049/ipr2.12904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks (CNN)-based automatic segmentation of medical images has become one of the hot topics in clinical disease diagnosis. It is still a challenging task to improve the segmentation accuracy of the network model with the large variation of pathological regions in different patients and the fuzzy boundary of pathological regions. A Dual-path Multi-scale Attention Guided network (DMAGNet) for medical image segmentation is proposed in this paper. First, the Dual-path Multi-scale Attention Fusion Module (DMAF) is proposed as a novel skip connection strategy, which is applied to encode semantic dependencies between high-level and low-level channels. Second, the Multi-scale Normalized Channel Attention Module (MNCA) based on the atrous convolution, normalization channel attention mechanism, and the Depthwise Separable Convolutions (DSConv) is developed to strengthen dependencies between channels. Finally, the encoder–decoder backbone employs the DSConv, as well as the pretrained Resnet34 block is combined in the encoder part to further improve the backbone network performance. Comprehensive experiments on brain, lung, and liver segmentation tasks show that the proposed DMAGNet outperforms the original U-Net method and other advanced methods.},
  archive      = {J_IETIP},
  author       = {Qiulang Ji and Jihong Wang and Caifu Ding and Yuhang Wang and Wen Zhou and Zijie Liu and Chen Yang},
  doi          = {10.1049/ipr2.12904},
  journal      = {IET Image Processing},
  month        = {11},
  number       = {13},
  pages        = {3631-3644},
  shortjournal = {IET Image Process.},
  title        = {DMAGNet: Dual-path multi-scale attention guided network for medical image segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FCUnet: Refined remote sensing image segmentation method
based on a fuzzy deep learning conditional random field network.
<em>IETIP</em>, <em>17</em>(12), 3616–3629. (<a
href="https://doi.org/10.1049/ipr2.12870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is pivotal for the understanding of high-resolution remote sensing images (HRRS). However, because of the inherent uncertainties in remote sensing images and the highly complex resolution of HRRS, ambiguity often occurs among some geographic entities in the segmentation process, and the fine segmentation of HRRS is not considered sufficiently for most existing segmentation methods. Therefore, in this paper, the authors propose a new collaborative neural network structure called fuzzy deep learning conditional random field network (FCUnet) to solve the refined segmentation of HRRS. First, the authors design a fuzzy U-Net classification network to obtain effective feature information, which introduces the fuzzy logic unit into the network to process the ambiguity and uncertainty of HRRS. Then, the authors introduce the conditional random field (CRF) at the end of FCUnet to optimize the image segmentation results. Finally, the authors validated the effectiveness and superiority of their approach on three data sets. The experiment results revealed that FCUnet had better refined segmentation performance and generalization ability than state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Xiangyue Ma and Jindong Xu and Qiangpeng Chong and Shifeng Ou and Haihua Xing and Mengying Ni},
  doi          = {10.1049/ipr2.12870},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3616-3629},
  shortjournal = {IET Image Process.},
  title        = {FCUnet: Refined remote sensing image segmentation method based on a fuzzy deep learning conditional random field network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FRDet: Few-shot object detection via feature reconstruction.
<em>IETIP</em>, <em>17</em>(12), 3599–3615. (<a
href="https://doi.org/10.1049/ipr2.12890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art object detection models rely on large-scale datasets for training to achieve good precision. Without sufficient samples, the model can suffer from severe overfitting. Current explorations in few-shot object detection are mainly divided into meta-learning-based methods and fine-tuning-based methods. However, existing models do not focus on how feature maps should be processed to present more accurate regions of interest (RoIs), leading to many non-supporting RoIs. These non-supporting RoIs can increase the burden of subsequent classification and even lead to misclassification. Additionally, catastrophic forgetting is inevitable in both few-shot object detection models. Many models classify directly in low-dimensional spaces due to insufficient resources, but this transformation of the data space can confuse some categories and lead to misclassification. To address these problems, the Feature Reconstruction Detector (FRDet) is proposed, a simple yet effective fine-tune-based approach for few-shot object detection. FRDet includes a region proposal network (RPN) based on channel attention and space attention called Multi-Attention RPN (MARPN) and a head based on feature reconstruction called Feature Reconstruction Head (FRHead). MARPN utilizes channel attention to suppress non-supporting classes and spatial attention to enhance support classes based on Attention RPN, resulting in fewer but more accurate RoIs. Meanwhile, FRHead utilizes support features to reconstruct query RoI features through a closed-form solution, allowing for a comprehensive and fine-grained comparison. The model was validated on the PASCAL VOC, MS COCO, FSOD, and CUB200 datasets and achieved better results.},
  archive      = {J_IETIP},
  author       = {Zhihao Chen and Yingchi Mao and Yong Qian and Zhenxiang Pan and Shufang Xu},
  doi          = {10.1049/ipr2.12890},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3599-3615},
  shortjournal = {IET Image Process.},
  title        = {FRDet: Few-shot object detection via feature reconstruction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A classification algorithm based on improved meta learning
and transfer learning for few-shot medical images. <em>IETIP</em>,
<em>17</em>(12), 3589–3598. (<a
href="https://doi.org/10.1049/ipr2.12889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, medical image classification algorithm plays an important role in clinical diagnosis. However, due to the scarcity of data labels, small sample size, uneven distribution, and poor domain generalization, many algorithms still have limitations. Therefore, a deep learning training network for disease classification and recognition of multimodal few-shot medical images are proposed, trying to solve the above problems and limitations. The network is based on the idea of meta-learning for training. Specifically, the technology of transfer learning and few-shot learning are used. In the process of building and improving the network structure, the multi-source domain generalization method, which performs well in the field of person re-identification, is combined. Finally, the applicability and effectiveness of the model are verified by using Grad-CAM tool. The experiments show that the accuracy of classification and recognition of the model is better than the advanced model in this field. The concerned areas of model classification are similar or the same as the manually labelled areas. It is of far-reaching significance to improve the efficiency of future clinical auxiliary diagnosis and patient diversion, as well as to promote the development of the Wise Information Technology of Med in the future.},
  archive      = {J_IETIP},
  author       = {Bingjie Zhang and Baolu Gao and Siyuan Liang and Xiaoyang Li and Hao Wang},
  doi          = {10.1049/ipr2.12889},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3589-3598},
  shortjournal = {IET Image Process.},
  title        = {A classification algorithm based on improved meta learning and transfer learning for few-shot medical images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-image snow removal algorithm based on generative
adversarial networks. <em>IETIP</em>, <em>17</em>(12), 3580–3588. (<a
href="https://doi.org/10.1049/ipr2.12887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of snowfall on an image is not only the interference of snow particles but also snow streaks and masking effects (similar to haze). Snowy weather severely reduces the accuracy of computer vision systems. There is a lot of interest in how to effectively remove snow while preserving as much of the original image information as possible. Based on this, the authors propose an effective Generative Adversarial Network (GAN) snow removal algorithm for single images to solve the snow removal failure problem caused by irregular snow particles and snow streaks. Specifically, the authors improve the original GAN network as follows: A novel Transformer module, the Contextual Transformer (CoT) module, is adopted in the residual modules based generator. It effectively uses the contextual information of the snow streaks neighbourhood to restore the texture and information in the noisy image as much as possible based on the focus on snow streaks features. Also, using learnable Regionalized Normalization (RN-L), potentially corrupted and undamaged regions are automatically detected for separate normalization, and global affine transformations are performed to enhance their fusion. In addition, a multi-scale discriminator is used in the discriminator to make the discrimination more adequate and retain more details. Extensive experiments have shown that the authors’ GAN network snow removal algorithm outperforms various current networks on snow removal studies in terms of evaluation metrics on both synthetic and real datasets.},
  archive      = {J_IETIP},
  author       = {Zhijia Zhang and Sinan Wu and Shixian Wang},
  doi          = {10.1049/ipr2.12887},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3580-3588},
  shortjournal = {IET Image Process.},
  title        = {Single-image snow removal algorithm based on generative adversarial networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-oriented feature hallucination for few-shot image
classification. <em>IETIP</em>, <em>17</em>(12), 3564–3579. (<a
href="https://doi.org/10.1049/ipr2.12886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data hallucination generates additional training examples for novel classes to alleviate the data scarcity problem in few-shot learning (FSL). Existing hallucination-based FSL methods normally train a general embedding model first by applying information extracted from base classes that have abundant data. In those methods, hallucinators are then built upon the trained embedding model to generate data for novel classes. However, these hallucination methods usually rely on general-purpose embeddings, limiting their ability to generate task-oriented samples for novel classes. Recent studies have shown that task-specific embedding models, which are adapted to novel tasks, can achieve better classification performance. To improve the performance of example hallucination for tasks, a task-oriented embedding model is used in the proposed method to perform task-oriented generation. After the initialization, the hallucinator is finetuned by applying a task-oriented embedding model with the guidance of a teacher–student mechanism. The proposed task-oriented hallucination method contains two steps. An initial embedding network and an initial hallucinator are trained with a base dataset in the first step. The second step contains a pseudo-labelling process where the base dataset is pseudo-labelled using support data of the few-shot task and a task-oriented fine-tuning process where the embedding network and hallucinator are adjusted simultaneously. Both the embedding network and the hallucinator are updated with the support set and the pseudo-labelled base dataset using knowledge distillation. The experiments are conducted on four popular few-shot datasets. The results demonstrate that the proposed approach outperforms state-of-the-art methods with 0.8% to 4.08% increases in classification accuracy for 5-way 5-shot tasks. It also achieves comparable accuracy to state-of-the-art methods for 5-way 1-shot tasks.},
  archive      = {J_IETIP},
  author       = {Sining Wu and Xiang Gao and Xiaopeng Hu},
  doi          = {10.1049/ipr2.12886},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3564-3579},
  shortjournal = {IET Image Process.},
  title        = {Task-oriented feature hallucination for few-shot image classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DS-net: Dual supervision neural network for image
manipulation localization. <em>IETIP</em>, <em>17</em>(12), 3551–3563.
(<a href="https://doi.org/10.1049/ipr2.12885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of image editing technology, tampering with images has become easier. Maliciously tampered images lead to serious security problems (e.g., when used as evidence). The current mainstream methods of image tampering are divided into three types which are copy-move, splicing and removal. Many image tampering detection methods can only detect one type of image tampering. Additionally, some methods learn features by suppressing image content, which can result in false positives when identifying tampered areas. In this paper, the authors propose a novel framework named the dual supervision neural network (DS-Net) to localize the tampered regions of images tampered by the three tampering methods mentioned above. First, to extract richer multiscale information, the authors add skip connections to the atrous spatial pyramid pooling (ASPP) module. Second, a channel attention mechanism is introduced to dynamically weigh the results generated by ASPP. Finally, the authors build additional supervised branches for high-level features to further enhance the extraction of these high-level features before fusing them with low-level features. The authors conduct experiments on various standard datasets. Through extensive experiments, the results show that the AUC scores reach 86.4% , 95.3% and 99.6% for CASIA, COVERAGE and NIST16 datasets, respectively, and the F1 scores are 56.0% , 73.4% and 82.7% , respectively. The results demonstrate that the authors’ method can accurately locate tampered regions and achieve better performance on various datasets than other methods of the same type.},
  archive      = {J_IETIP},
  author       = {Chenwei Dai and Lichao Su and Bin Wu and Jian Chen},
  doi          = {10.1049/ipr2.12885},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3551-3563},
  shortjournal = {IET Image Process.},
  title        = {DS-net: Dual supervision neural network for image manipulation localization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal object detection via transformer network.
<em>IETIP</em>, <em>17</em>(12), 3541–3550. (<a
href="https://doi.org/10.1049/ipr2.12884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to the fact that single-modal data usually contain limited information, a great deal of effort has been devoted to making use of the complementary information contained in the multi-modal data on various patterns. Thus, this paper is concerned with an object detection method that can fully utilize multi-modal data. First, the method introduces the transformer mechanism to realize the fusion of intra-modal and inter-modal features of different modal data. The aim is to take advantage of the complementarity of data between modalities, which helps to improve the performance of multi-modal object detection. Second, a contrastive loss suitable for contrastive learning is applied. This enables the authors to effectively utilize label information. Extensive experiments are conducted on multiple object detection datasets to demonstrate the effectiveness of our proposed method.},
  archive      = {J_IETIP},
  author       = {Wenbing Liu and Haibo Wang and Quanxue Gao and Zhaorui Zhu},
  doi          = {10.1049/ipr2.12884},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3541-3550},
  shortjournal = {IET Image Process.},
  title        = {Multi-modal object detection via transformer network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CMLocate: A cross-modal automatic visual geo-localization
framework for a natural environment without GNSS information.
<em>IETIP</em>, <em>17</em>(12), 3524–3540. (<a
href="https://doi.org/10.1049/ipr2.12883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new approach to visual geo-localization for natural environments is proposed. The digital elevation model (DEM) data in virtual space is rendered and construct a panoramic skyline database is constructed. By combining the skyline database with real-world image data (used as the “queries” to be localized), visual geo-localization is treated as a cross-modal image retrieval problem for panoramic skyline images, creating a unique new visual geo-localization benchmark for the natural environment. Specifically, the semantic segmentation model named LineNet is proposed, for skyline extractions from query images, which has proven to be robust to a variety of complex natural environments. On the aforementioned benchmarks, the fully automatic method is elaborated for large-scale cross-modal localization using panoramic skyline images. Finally, the compound index is delicately designed to reduce the storage space of the positioning global descriptors and improve the retrieval efficiency. Moreover, the proposed method is proven to outperform most state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Zhuoqun Liu and Fan Guo and Heng Liu and Xiaoyue Xiao and Jin Tang},
  doi          = {10.1049/ipr2.12883},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3524-3540},
  shortjournal = {IET Image Process.},
  title        = {CMLocate: A cross-modal automatic visual geo-localization framework for a natural environment without GNSS information},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An image encryption algorithm based on cascade chaotic map
and DNA coding. <em>IETIP</em>, <em>17</em>(12), 3510–3523. (<a
href="https://doi.org/10.1049/ipr2.12882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2D-Cascade-Transeform-Logistic-Sine Map (2D-CTLSM) with good randomness and large parameter chaos range is obtained by improving the Logistic and Sine map in a cascade manner. By combining the chaotic map with DNA coding, an efficient encryption algorithm based on a chaotic map with chunking dislocation and alternate row diffusion is designed. The shortcomings of zigzag itself are overcome by performing different zigzag transformations in chunks while combining DNA encoding to accomplish the diffusion operation. Due to the excellent properties of DNA encoding, the diffusion operation is performed by the DNA computation method with alternate row diffusion so that each value can complete the diffusion operation twice. The simulation experimental results and security analysis show that the algorithm can effectively resist various typical attacks, such as the information entropy is close to the expected value of 8, the correlation coefficient is close to 0, and the NPCR and UACI values can meet the critical value range, and so on. This is a safe and reliable image encryption algorithm.},
  archive      = {J_IETIP},
  author       = {Jiming Zheng and Tianyu Bao},
  doi          = {10.1049/ipr2.12882},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3510-3523},
  shortjournal = {IET Image Process.},
  title        = {An image encryption algorithm based on cascade chaotic map and DNA coding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A high-performance and lightweight framework for real-time
facial expression recognition. <em>IETIP</em>, <em>17</em>(12),
3500–3509. (<a href="https://doi.org/10.1049/ipr2.12881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition technology has become a powerful tool for conveying human emotions and intentions and is widely used in areas such as assisted driving and intelligent medical care. Due to the limited computing power of current hardware devices and the real-time requirements of application scenarios, this paper proposes a high-performance and lightweight framework for real-time facial expression recognition framework to solve the problem of real-time completion of expression recognition tasks under low hardware costs. To address these issues, this paper first designs a RepVGG and mobileNetV2 dual-channel structure in the feature extraction. It is then input into the MobileViT Block for global feature modelling. Finally, the position vector of the capsule network is used to replace the output of the global pooling, preserving the spatial relationship of the salient features and enhancing the classification effect. Compared with the mainstream facial expression recognition algorithm that cannot get good classification results under low complexity conditions, the model has a significant accuracy improvement while ensuring lightweight. With only 294.60M FLOPS and 0.95M parameters, it achieved an accuracy of 97.53% on the KDEF dataset and 85.56% on the RAF-DB, demonstrating the advanced nature of the algorithm.},
  archive      = {J_IETIP},
  author       = {Xuebin Xu and Chenguang Liu and Shuxin Cao and Longbin Lu},
  doi          = {10.1049/ipr2.12881},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3500-3509},
  shortjournal = {IET Image Process.},
  title        = {A high-performance and lightweight framework for real-time facial expression recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MaskDis r-CNN: An instance segmentation algorithm with
adversarial network for herd pigs. <em>IETIP</em>, <em>17</em>(12),
3488–3499. (<a href="https://doi.org/10.1049/ipr2.12880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current instance segmentation method can achieve satisfactory results in common scenarios. However, under the overlap or partial occlusion between targets caused by the complex scenes, accurate segmentation of pigs remains a challenging task. To address the problem, the authors propose an instance segmentation method based on Mask Scoring region-based convolutional neural networks (R-CNN) (MS R-CNN), which creates the adversarial network called MaskDis in the head branch of MS R-CNN. The MaskDis is trained as a discriminator using a generative adversarial network, and the MS R-CNN model is used as a generator during model training. The adversarial training enables the generator to learn context information and features at the pixel level, which effectively improves the segmentation quality under pigs’ overlapping or dense occlusions scenes. Experimental conducted on the pig object segmentation dataset show that the proposed approach achieves a precision of 92.03%, a recall of 92.18%, and an F1 score of 0.9210. Compared with the basic MS R-CNN model, the approach achieved a 2.25% improvement in precision and 1.18% improvement in F1 score. Furthermore, the improved approach outperformed advanced instance segmentation methods such as YOLACT, Swin Transformer, YOLOv5-seg, and SOLOv2 on COCO evaluation metrics. These experimental results demonstrate the effectiveness of the proposed approach in instance segmentation of pigs in complex scenes, providing technical support for non-contact pig automatic management.},
  archive      = {J_IETIP},
  author       = {Shuqin Tu and Qiantao Zeng and Haofeng Liu and Yun Liang and Xiaolong Liu and Lei Huang and Zhengxin Huang},
  doi          = {10.1049/ipr2.12880},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3488-3499},
  shortjournal = {IET Image Process.},
  title        = {MaskDis R-CNN: An instance segmentation algorithm with adversarial network for herd pigs},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A markov random field based method for removing invalid
unwrapping phase points in 3D reconstruction. <em>IETIP</em>,
<em>17</em>(12), 3477–3487. (<a
href="https://doi.org/10.1049/ipr2.12879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fringe projection profilometry is widely used in 3D structured light due to its fast speed and accuracy. However, in the process of phase unwrapping, it is easy to cause invalid points in the edges and shadows of objects, which leads to error points in 3D reconstruction. To solve this problem, we propose an invalid points removal method based on Markov random fields. Specifically, the proposed method formulates unwrapped phase and mask maps as energy functions and uses iterative methods to minimize them. Furthermore, we validate the proposed method in a monocular structured light system and compare it with existing algorithms. Results show that the proposed method effectively identifies edges and shadows while preserving valid points, and has strong robustness and correctness.},
  archive      = {J_IETIP},
  author       = {Jinfeng Gao and Fengyuan Wu and Cheng Cheng and Chengbai Wu and Yangfan Zhou},
  doi          = {10.1049/ipr2.12879},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3477-3487},
  shortjournal = {IET Image Process.},
  title        = {A markov random field based method for removing invalid unwrapping phase points in 3D reconstruction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved wavelet prediction superresolution reconstruction
based on u-net. <em>IETIP</em>, <em>17</em>(12), 3464–3476. (<a
href="https://doi.org/10.1049/ipr2.12878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning can be used to achieve single-image superresolution (SR) reconstruction. To address problems encountered during this process, such as the number of network parameters, high training requirements on equipment performance, and inability to downsample certain SR images accurately, an image SR reconstruction algorithm based on deep residual network optimization is proposed. The model introduces wavelet transforms based on the original U-Net, where the U-Net is trained to obtain SR wavelet feature images at multiple scales simultaneously. This approach reduces the mapping space for the network to learn low- to high-resolution image mapping, which in turn reduces the training difficulty of the model. In terms of network details, the inverse wavelet transform is used in image upsampling to enhance the sparsity of the reconstruction layer in the original network. The network structure of the U-Net upsampling is adjusted slightly to enable the network to distinguish wavelet images from feature images, thereby improving the richness of the features extracted by the model. The experimental results show that the peak signal-to-noise ratio (PSNR) of the fourfold SR model is 32.35 and 28.68 on the Set5 and Set14 validation sets, respectively. Compared with networks that use wavelet prediction mechanisms, such as the deep wavelet prediction SR (DWSR) and deep wavelet prediction-based residual SR (DWRSR) models, the PSNR for all the tested public datasets is improved by 0.5. The method yields superior results in terms of both visual effect and PSNR, demonstrating the feasibility of the wavelet prediction mechanism in SR reconstruction and thus offering application value and research significance.},
  archive      = {J_IETIP},
  author       = {Jianfang Cao and Zeyu Chen and Hongyan Cui and Xiaofei Ji and Xianhui Wang and Yunchuan Liang and Yun Tian},
  doi          = {10.1049/ipr2.12878},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3464-3476},
  shortjournal = {IET Image Process.},
  title        = {Improved wavelet prediction superresolution reconstruction based on U-net},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). LGANet: Local and global attention are both you need for
action recognition. <em>IETIP</em>, <em>17</em>(12), 3453–3463. (<a
href="https://doi.org/10.1049/ipr2.12876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to redundancy in the spatiotemporal neighborhood and the global dependency between video frames, video recognition remains a challenge. Some prior works have been mainly driven by 3D convolutional neural networks (CNNs) or 2D CNNs with a well-designed module for temporal information. However, convolution-based networks lack the capability to capture the global dependency due to the limited receptive field. Alternatively, transformer for video recognition is proposed to build long-range dependency between frame patches. Nevertheless, most transformer-based networks have significant computational costs because attention is calculated among all the tokens. Based on these observations, we propose an efficient network which we dub LGANet. Unlike conventional CNNs and transformers for video recognition, the LGANet can tackle both spatiotemporal redundancy and dependency by learning local and global token affinity in shallow and deep layers, respectively . Specifically, local attention is implemented in the shallow layers to reduce parameters and eliminate redundancy. In the deep layers, spatial-wise and channel-wise self-attention are embedded to realize the global dependency of high-level features. Moreover, several key designs are made in the multi-head self-attention (MSA) and feed-forward network (FFN). Extensive experiments are conducted on the popular video benchmarks, such as Kinetics-400, Something-Something V1&amp;V2. Without any bells and whistles, the LGANet achieves state-of-the-art performance. The code will be released soon.},
  archive      = {J_IETIP},
  author       = {Hao Wang and Bin Zhao and Wenjia Zhang and Guohua Liu},
  doi          = {10.1049/ipr2.12876},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3453-3463},
  shortjournal = {IET Image Process.},
  title        = {LGANet: Local and global attention are both you need for action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CrossFormer: Multi-scale cross-attention for polyp
segmentation. <em>IETIP</em>, <em>17</em>(12), 3441–3452. (<a
href="https://doi.org/10.1049/ipr2.12875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colonoscopy is a common method for the early detection of colorectal cancer (CRC). The segmentation of colonoscopy imagery is valuable for examining the lesion. However, as colonic polyps have various sizes and shapes, and their morphological characteristics are similar to those of mucosa, it is difficult to segment them accurately. To address this, a novel neural network architecture called CrossFormer is proposed. CrossFormer combines cross-attention and multi-scale methods, which can achieve high-precision automatic segmentation of the polyps. A multi-scale cross-attention module is proposed to enhance the ability to extract context information and learn different features. In addition, a novel channel enhancement module is used to focus on the useful channel information. The model is trained and tested on the Kvasir and CVC-ClinicDB datasets. Experimental results show that the proposed model outperforms most existing polyps segmentation methods.},
  archive      = {J_IETIP},
  author       = {Lifang Chen and Hongze Ge and Jiawei Li},
  doi          = {10.1049/ipr2.12875},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3441-3452},
  shortjournal = {IET Image Process.},
  title        = {CrossFormer: Multi-scale cross-attention for polyp segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving multi-object tracking by full occlusion handle and
adaptive feature fusion. <em>IETIP</em>, <em>17</em>(12), 3423–3440. (<a
href="https://doi.org/10.1049/ipr2.12874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion has always been a challenging research topic in the field of multi-target tracking. The invisibility of the target in full occlusion increases the difficulty of continuous tracking, which makes the recovery failure when the target is re-visible, and ultimately leads to a decrease in tracking accuracy. To address full occlusion problem, an effective multi-object tracking algorithm with full occlusion handle and adaptive fusion features is proposed. Firstly, a spatio-temporal model is established for full occlusion, and a simple, efficient and training-free method is proposed to find full occluded targets. Secondly, local high discrimination features with better stability and independence is proposed to realize effective correlation between targets before and after the full occlusion. Finally, an adaptive feature fusion mechanism is proposed, which can adjust feature structure dynamically according to the occlusion state. The experimental results show that most evaluation metrics of the proposed algorithm are superior to those of some typical algorithms proposed in recent years under full occlusion tracking scenes. The proposed algorithm can realize accurate occluded targets identification and improve tracking robustness under short-term, long-term and frequent full occlusion.},
  archive      = {J_IETIP},
  author       = {Yingying Yue and Yang Yang and Yongtao Yu and Haiyan Liu},
  doi          = {10.1049/ipr2.12874},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3423-3440},
  shortjournal = {IET Image Process.},
  title        = {Improving multi-object tracking by full occlusion handle and adaptive feature fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural surface reconstruction with saliency-guided sampling
in multi-view. <em>IETIP</em>, <em>17</em>(12), 3411–3422. (<a
href="https://doi.org/10.1049/ipr2.12873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a neural surface reconstruction framework is presented. In order to perform neural surface reconstruction using 2D supervision, a weighted random sampling based on saliency is introduced for training the deep neural network. In the proposed method, self-attention is used to detect the saliency of input 2D images. The saliency map, that is, the weight matrix of the weighted random sampling, is used to sample the training samples. As a result, more samples in the reconstructed object area are collected. Moreover, an update strategy for weight based on sampling frequency is adopted to avoid the points that cannot be sampled all the time. The experiments are implemented in real-world 2D images of objects with different material properties and lighting conditions based on the DTU dataset. The results show that the proposed method produces more detailed 3D surfaces, and the rendered results are closer to the raw images visually. In addition, the mean of peak signal-to-noise ratio (PNSR) is also improved.},
  archive      = {J_IETIP},
  author       = {Xiuxiu Li and Yongchen Guo and Haiyan Jin and Jiangbin Zheng},
  doi          = {10.1049/ipr2.12873},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3411-3422},
  shortjournal = {IET Image Process.},
  title        = {Neural surface reconstruction with saliency-guided sampling in multi-view},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DenseGCN: A multi-level and multi-temporal graph
convolutional network for action recognition. <em>IETIP</em>,
<em>17</em>(12), 3401–3410. (<a
href="https://doi.org/10.1049/ipr2.12872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the exponential growth of video data, action recognition has become an increasingly important area of study. Despite various advancements, achieving a balance between detection accuracy and lightness remains a formidable challenge, primarily due to the complexity of existing action recognition models. To address this issue, DenseGCN is developed, a lightweight network designed to optimize accuracy and efficiency. The aim was to create a detection model that has high accuracy while remaining lightweight for real-world applications. DenseGCN operates via a unique three-level feature fusion system. The initial stage involves the Multi-level Fusion Network (MlFN), which contains dense connections and a Spatial-Temporal Fusion Attention module (STF-Att), designed to eliminate bias in feature extraction caused by deep networks. In the next stage, RefineBone tackles optimization issues in low-dimensional feature layers by leveraging high-dimensional feature layers, thus avoiding gradient stacking. Finally, the Multi-temporal Fusion Feature Pyramid Network (MF-FPN) generates a discriminative classification feature map by repetitively combining data from multiple dimensions. This strategy has proven successful in refining the extracted feature, allowing for discriminative feature extraction even with a reduced number of channels. This efficient design not only contributes to further research in developing lightweight networks but also offers enhanced possibilities for real-world implementations. In two large-scale datasets, NTU RGB+D 60 and 120, DenseGCN outperformed other state-of-the-art methods, achieving an accuracy of 92.7% on the X-View benchmark of the NTU RGB+D 60 dataset. The DenseGCN is 10.2 × faster and 10 × smaller than the spatial temporal graph attention network (STGAT) proposed in 2022 while retaining very competitive accuracy. The findings suggest that this model significantly improves the quality of feature extraction. As a result, DenseGCN presents a remarkable balance between accuracy and lightness.},
  archive      = {J_IETIP},
  author       = {Chengzhang Yu and Wenxia Bao},
  doi          = {10.1049/ipr2.12872},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3401-3410},
  shortjournal = {IET Image Process.},
  title        = {DenseGCN: A multi-level and multi-temporal graph convolutional network for action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiteDEKR: End-to-end lite 2D human pose estimation network.
<em>IETIP</em>, <em>17</em>(12), 3392–3400. (<a
href="https://doi.org/10.1049/ipr2.12871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2D human pose estimation plays an important role in human-computer interaction and action recognition. Although the method based on high-resolution network has superior performance, there is still room for improvement in terms of speed and lightweight. Here, a LiteDEKR, a 2D pose estimation method that combines lightweight and accuracy, is proposed by designing a lightweight network based on DEKR and constructing two scientifically valid loss functions. The method, constructs a multi-instance bias regression loss that matches the true distribution of keypoint bias, improves the accuracy of bias regression, and constructs a keypoint similarity loss with the object keypoint similarity index of keypoints as the optimization objective to achieve end-to-end training of the network. In addition, this paper has designed a lightweight DEKR, using LitePose as the backbone network. With the optimization of the above two loss functions, LiteDEKR not only achieves lightweight but also has high accuracy. Comparative experiments on the COCO and CrowdPose datasets show that compared to the current state-of-the-art Contextual Instance Decoupling, LiteDEKR achieves a similar accuracy with only 10% of its network complexity. It also shows better robustness to low-resolution input images.},
  archive      = {J_IETIP},
  author       = {Xueqiang Lv and Wei Hao and Lianghai Tian and Jing Han and Yuzhong Chen and Zangtai Cai},
  doi          = {10.1049/ipr2.12871},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3392-3400},
  shortjournal = {IET Image Process.},
  title        = {LiteDEKR: End-to-end lite 2D human pose estimation network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextual information extraction in brain tumour
segmentation. <em>IETIP</em>, <em>17</em>(12), 3371–3391. (<a
href="https://doi.org/10.1049/ipr2.12869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic brain tumour segmentation in MRI scans aims to separate the brain tumour&#39;s endoscopic core, edema, non-enhancing tumour core, peritumoral edema, and enhancing tumour core from three-dimensional MR voxels. Due to the wide range of brain tumour intensity, shape, location, and size, it is challenging to segment these regions automatically. UNet is the prime three-dimensional CNN network performance source for medical imaging applications like brain tumour segmentation. This research proposes a context aware 3D ARDUNet (Attentional Residual Dropout UNet) network, a modified version of UNet to take advantage of the ResNet and soft attention. A novel residual dropout block (RDB) is implemented in the analytical encoder path to replace traditional UNet convolutional blocks to extract more contextual information. A unique Attentional Residual Dropout Block (ARDB) in the decoder path utilizes skip connections and attention gates to retrieve local and global contextual information. The attention gate enabled the Network to focus on the relevant part of the input image and suppress irrelevant details. Finally, the proposed Network assessed BRATS2018, BRATS2019, and BRATS2020 to some best-in-class segmentation approaches. The proposed Network achieved dice scores of 0.90, 0.92, and 0.93 for the whole tumour. On BRATS2018, BRATS2019, and BRATS2020, tumour core is 0.90, 0.92, 0.93, and enhancing tumour is 0.92, 0.93, 0.94.},
  archive      = {J_IETIP},
  author       = {Muhammad Sultan Zia and Usman Ali Baig and Zaka Ur Rehman and Muhammad Yaqub and Shahzad Ahmed and Yudong Zhang and Shuihua Wang and Rizwan Khan},
  doi          = {10.1049/ipr2.12869},
  journal      = {IET Image Processing},
  month        = {10},
  number       = {12},
  pages        = {3371-3391},
  shortjournal = {IET Image Process.},
  title        = {Contextual information extraction in brain tumour segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stream spatiotemporal networks for skeleton action
recognition. <em>IETIP</em>, <em>17</em>(11), 3358–3370. (<a
href="https://doi.org/10.1049/ipr2.12868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based neural networks have been considered a focus for human action recognition (HAR). It is noteworthy that the existing skeleton-based methods are not capable of combining the spatial and temporal features reasonably to derive more effective high-level representations, and it continues to be a challenging task of learning and representing the skeleton action discriminatively. In this study, a novel two-stream spatiotemporal network (TSTN) is proposed, which is capable of processing the spatial and temporal features respectively and collectively to achieve a better representation and understanding of human action. The temporal branch stacks three gate recurrent unit (GRU) blocks in a new architecture to encode the temporal correlations from different aspects of human action, achieving high-level temporal semantic feature expressions. The spatial branch encodes the spatial features with multi-stacked graph convolutional network (GCN) blocks. Self-attention mechanisms incorporated with the graph structure of the skeleton are explored to add weight influence and structural hints to further enhance the performance. The experimental results verify the effectiveness and superiority of the proposed model in skeleton action recognition; the model reaches state-of-the-art on specific datasets.},
  archive      = {J_IETIP},
  author       = {Lei Wang and Jianwei Zhang and Shanmin Yang and Song Gu},
  doi          = {10.1049/ipr2.12868},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3358-3370},
  shortjournal = {IET Image Process.},
  title        = {Two-stream spatiotemporal networks for skeleton action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detection and location of microaneurysms in fundus images
based on improved YOLOv4 with IFCM. <em>IETIP</em>, <em>17</em>(11),
3349–3357. (<a href="https://doi.org/10.1049/ipr2.12867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is one of the leading causes of blindness for people suffering from diabetes. Microaneurysm (MA) is the initial symptom of DR. MA is a lesion based disease which starts as small red spots on the retina and increases in size as the DR progresses which finally leads to blindness. So eliminating the lesion can effectively prevent DR at an early stage. However, due to complex retinal structure, different brightness and contrast of fundus images with effects of factors such as different patients, environment changes, and difference in acquisition equipment, it is difficult for existing detection algorithms to achieve accurate results of MA detection and location. Therefore, the detection algorithm of improved YOLOv4 (YOLOv4-Pro) was proposed. First, an improved Fuzzy C-Means (IFCM) clustering algorithm was proposed to optimize anchor parameters of target samples to improve matching results between anchors and feature graphs. In order to control noise and improve efficiency, a median filtering method was employed to update the criterion function of the original FCM algorithm, and a K-means algorithm was employed to initialize clustering. Second, a SENet attention module was added in the backbone of YOLOv4 to enhance key information and suppress background, improving the confidence of MA effectively. Finally, the spatial pyramid pooling (SPP) module was added to the neck to enhance the acceptance domain of the output characteristics of the backbone network, and profits separating of important context information. The improved YOLOv4 with IFCM was verified on the Kaggle DR dataset and compared with other methods. Experimental results show that optimizing the prior frame with the IFCM algorithm can make it suitable to frame the Kaggle DR dataset, which improves the detection accuracy of the network by nearly 5%, and provides a nice performance on detection and location of MA in fundus images. This would help ophthalmologists finding the exact location of MA on retina, thereby simplifying the process and eliminating any manual intervention.},
  archive      = {J_IETIP},
  author       = {Weiwei Gao and Bo Fan and Yu Fang and Mingtao Shan and Nan Song},
  doi          = {10.1049/ipr2.12867},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3349-3357},
  shortjournal = {IET Image Process.},
  title        = {Detection and location of microaneurysms in fundus images based on improved YOLOv4 with IFCM},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Adaptive learning unet-based adversarial network with CNN
and transformer for segmentation of hard exudates in diabetes
retinopathy. <em>IETIP</em>, <em>17</em>(11), 3337–3348. (<a
href="https://doi.org/10.1049/ipr2.12865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of hard exudates in early non-proliferative diabetic retinopathy can assist physicians in taking appropriate treatment in a more targeted manner, in order to avoid more serious damage to vision caused by the deterioration of the disease in the later stages. Here, an Adaptive Learning Unet-based adversarial network with Convolutional neural network and Transformer (CT-ALUnet) is proposed for automatic segmentation of hard exudates, combining the excellent local modelling ability of Unet with the global attention mechanism of transformer. Firstly, multi-scale features are extracted through a CNN dual-branch encoder. Then, the information fusion of features at adjacent scale is realized and the fused features are selected adaptively to maintain the overall consistency of features by attention-guided multi-scale fusion blocks (AGMFB). After that, the high-level encoded features are input to transformer blocks to extract global contexts. Finally, these features are fused layer-by-layer to achieve accurate segmentation of hard exudates. In addition, adversarial training is incorporated into the above segmentation model, which improves Dice scores and MIoU scores by 7.5% and 3%, respectively. Experiments demonstrate that CT-ALUnet shows more reliable segmentation and stronger generalization ability than other SOTA methods, which lays a good foundation for computer-assisted diagnosis and assessment of efficacy.},
  archive      = {J_IETIP},
  author       = {Xinfeng Zhang and Jiaming Zhang and Yitian Zhang and Maoshen Jia and Hui Li and Xiaomin Liu},
  doi          = {10.1049/ipr2.12865},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3337-3348},
  shortjournal = {IET Image Process.},
  title        = {Adaptive learning unet-based adversarial network with CNN and transformer for segmentation of hard exudates in diabetes retinopathy},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on neural processes with multiple latent variables.
<em>IETIP</em>, <em>17</em>(11), 3323–3336. (<a
href="https://doi.org/10.1049/ipr2.12864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Process (NP) fully combines the advantages of neural network and Gaussian Process (GP) to provide an efficient method for solving regression problems. Nonetheless, limited by the dimensionality of the latent variable, NP has difficulty fitting the observed data completely and predicting the targets perfectly. To remedy these drawbacks, the authors propose a concise and effective improvement of the latent path of NP, which the authors term Multi-Latent Variables Neural Process (MLNP). MLNP samples multiple latent variables and integrates the representations corresponding to the latent variables in the decoder with adaptive weights. MLNP inherits the desirable property of linear computation scales of NP and learns the approximate distribution over objective functions from contexts more flexibly and accurately. By applying MLNP to 1-D regression, real-world image completion, which can be seen as a 2-D regression task, the authors demonstrate its significant improvement in the accuracy of prediction and contexts fitting capability compared with NP. Through ablation experiments, the authors also verify that the number of latent variables has a great impact on the prediction accuracy and fitting capability of MLNP. Moreover, the authors also analyze the roles played by different latent variables in reconstructing images.},
  archive      = {J_IETIP},
  author       = {Xiao-Han Yu and Shao-Chen Mao and Lei Wang and Shi-Jie Lu and Kun Yu},
  doi          = {10.1049/ipr2.12864},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3323-3336},
  shortjournal = {IET Image Process.},
  title        = {Research on neural processes with multiple latent variables},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cascaded face super-resolution with shape and identity
priors. <em>IETIP</em>, <em>17</em>(11), 3309–3322. (<a
href="https://doi.org/10.1049/ipr2.12863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite impressive progress in face super-resolution (SR), it is an open challenge to reconstruct a reliable SR face that preserves authentic facial characteristics. Here, the problem of super-resolving low-resolution (LR) faces to high-resolution (HR) ones is addressed. To tackle the ill-posed nature of face SR, the cascaded super-resolution network (CSRNet) is proposed to utilize shape and identity priors jointly and progressively, the first to explore multiple priors. Specifically, CSRNet adopts a cascaded structure to transform an LR face to HR face progressively via multiple stages. At each stage, CSRNet forces its output face image to match both the shape priors and identity priors extracted from the ground-truth HR face. The shape priors estimated in one stage are merged into the inputs of its subsequent stage to provide rich information for the face SR. To generate realistic yet discriminative faces, the cascaded super-resolution generative adversarial network (CSRGAN) is also proposed to incorporate the adversarial loss and identification loss into CSRNet. Extensive experiments on popular benchmarks show that the CSRNet and CSRGAN outperform existing face SR state-of-the-art methods, both quantitatively and qualitatively, and detailed ablation studies show the advantage of this method.},
  archive      = {J_IETIP},
  author       = {Dan Zeng and Zelin Li and Xiao Yan and Wen Jiang and Xinshao Wang and Jiang Liu and Bo Tang},
  doi          = {10.1049/ipr2.12863},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3309-3322},
  shortjournal = {IET Image Process.},
  title        = {Cascaded face super-resolution with shape and identity priors},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Three-stream RGB-d salient object detection network based on
cross-level and cross-modal dual-attention fusion. <em>IETIP</em>,
<em>17</em>(11), 3292–3308. (<a
href="https://doi.org/10.1049/ipr2.12862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective integration of RGB and depth map features to improve the performance of RGB-D salient object detection (SOD) has garnered significant research interest. The existing dual-stream models can be used for high-level feature fusion or unidirectionally transferring depth features to RGB features; however, they are unable to fully exploit the differences in modality. Furthermore, owing to the influence of image background information, the generated salient object is affected by background swallow. Herein, a three-stream RGB-D SOD method based on cross-layer and cross-modal dual-attention (CMDA) fusion is proposed. In the encoding stage, the CMDA fusion module is used to fuse RGB and depth features layer by layer. Through this module, merged interactive features may be used to extract the richer features of salient objects, realize the commonality and complementarity of fusion features, and achieve effective cross-modal fusion. In addition, for the decoding stage, a cross-level feature fusion module that introduces global context features into the up-sampling process, reduces the impact of salient objects being swallowed by the background, and helps to accurately detect salient areas is proposed. Three different branch features are used for simultaneous end-to-end training. The experimental results demonstrate that the proposed method outperforms other methods in terms of multiple evaluation metrics on four datasets. Furthermore, the authors visualize the precision–recall curve, F-measure curve, and saliency map, which indicate that the detection effect of the proposed method is superior to those of other methods. During the testing stage, our model ran at 14 frames per second (FPS).},
  archive      = {J_IETIP},
  author       = {Lingbing Meng and Mengya Yuan and Xuehan Shi and Qingqing Liu and Fei Cheng and Lingli Li},
  doi          = {10.1049/ipr2.12862},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3292-3308},
  shortjournal = {IET Image Process.},
  title        = {Three-stream RGB-D salient object detection network based on cross-level and cross-modal dual-attention fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic-guided fusion for multiple object tracking and
RGB-t tracking. <em>IETIP</em>, <em>17</em>(11), 3281–3291. (<a
href="https://doi.org/10.1049/ipr2.12861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The attention mechanism has produced impressive results in object tracking, but for a good trade-off between performance and efficiency, CNN-based approaches still dominate, owing to quadratic complexity of attention. Here, the SGF module is proposed, an efficient feature fusion block for effective object tracking with reduced linear complexity of attention. The proposed method fuses feature with attention in a coarse-to-fine manner. In the low-resolution semantic branch, the top K regions with highest attention scores are selected; in the high-resolution detail branch, attention is only calculated within regions corresponding to the top K regions. Thus, the features from the high-resolution branch can be efficiently fused under the guidance of low-resolution branch. Experiments on RGB and RGB-T datasets with reformed FairMOT and MDNet+RGBT trackers demonstrated the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Xiaohu Liu and Yichuang Luo and Yan Zhang and Zhiyong Lei},
  doi          = {10.1049/ipr2.12861},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3281-3291},
  shortjournal = {IET Image Process.},
  title        = {Semantic-guided fusion for multiple object tracking and RGB-T tracking},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WD-UNeXt: Weight loss function and dropout u-net with
ConvNeXt for automatic segmentation of few shot brain gliomas.
<em>IETIP</em>, <em>17</em>(11), 3271–3280. (<a
href="https://doi.org/10.1049/ipr2.12860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of brain gliomas (BG) is a crucial and challenging task for effective treatment planning in BG therapy. This study presents the weight loss function and dropout U-Net with ConvNeXt block (WD-UNeXt), which precisely segments BG from few shot MRI. The ConvNeXt block, which comprises the main body of the network, is a structure that can extract more detailed features from images. The weight loss function addresses the issue of category imbalance, thereby enhancing the network&#39;s ability to achieve more precise segmentation. The training set of BraTS2019 was used to train the network and apply it to test data. Dice similarity coefficient (DSC), sensitivity (Sen), specificity (Spec) and Hausdorff distance (HD) were used to assess the performance of the method. The experimental results demonstrate that the DSC of whole tumour, tumour core and enhancing tumour reached 0.934, 0.911 and 0.851, respectively. Sen of the sub-regions achieved 0.922, 0.911 and 0.867. Spec and HD reached 1.000, 1.000, 1.000 and 3.224, 2.990, 2.844, respectively. Compared with the performance of state-of-the-art methods, the DSC and HD of WD-UNeXt were improved to varying degrees. Therefore, this method has considerable potential for the segmentation of BG.},
  archive      = {J_IETIP},
  author       = {Ziming Yin and Hongyu Gao and Jinchang Gong and Yuanjun Wang},
  doi          = {10.1049/ipr2.12860},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3271-3280},
  shortjournal = {IET Image Process.},
  title        = {WD-UNeXt: Weight loss function and dropout U-net with ConvNeXt for automatic segmentation of few shot brain gliomas},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learnable interpolation and extrapolation network for fuzzy
pulmonary lobe segmentation. <em>IETIP</em>, <em>17</em>(11), 3258–3270.
(<a href="https://doi.org/10.1049/ipr2.12859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pulmonary lobe segmentation is an important prerequisite for accurately quantifying pulmonary damage in many pulmonary diseases and planning treatment. However, due to the incomplete lobar structures and morphological changes caused by diseases, the lobe segmentation still encounters great challenges. In this study, a Learnable Interpolation and Extrapolation Network (LIE-Net) is proposed to form complete and consecutive fissure surfaces by learning to extract information of the fissures from existing fissure points and absent points (unsegmented points belonging to fissures) to predict the z coordinate of the absent fissure points. The completed pulmonary fissures are further used for accurate pulmonary lobe segmentation. Specifically, LIE-Net takes the coordinate information of existing fissure points (their ( x , y , z ) coordinates) and absent fissure points (their ( x , y ) coordinates) as two independent inputs, and predicts the z coordinates of absent points. The proposed LIE-Net makes voxel-wise predictions based on the spatial structure characteristics of the lung fissure, and is able to provide a consecutive fissure surface in space. According to the evaluation of radiologists, the lobe segmentation performance was remarkably enhanced in approximately 76% of patients in our additional dataset after the application of LIE-Net, especially for those cases with large-scale missing fissures.},
  archive      = {J_IETIP},
  author       = {Xiaochen Fan and Xin Xu and Jianxing Feng and Haixia Huang and Xiang Zuo and Guohou Xu and Guanghui Ma and Bin Chen and Jianbin Wu and Yinhua Huang and Yang Luo},
  doi          = {10.1049/ipr2.12859},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3258-3270},
  shortjournal = {IET Image Process.},
  title        = {Learnable interpolation and extrapolation network for fuzzy pulmonary lobe segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explore the potential of deep learning and hyperchaotic map
in the meaningful visual image encryption scheme. <em>IETIP</em>,
<em>17</em>(11), 3235–3257. (<a
href="https://doi.org/10.1049/ipr2.12858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, meaningful visual image encryption schemes that the plain image is compressed and encrypted and then hidden into the carrier image have received increasing attention. This paper proposes a new meaningful visual image encryption scheme, which consists of three stages: compression (compression network)—encryption (2D-SLC hyperchaotic map)—hiding (matrix encoding). First, the advantages of deep learning are explored. It can compress the width, height, channel, and pixel values of the plain image simultaneously. Second, a new 2D-SLC hyperchaotic map is designed to ensure security. It has a larger chaotic space and better randomness. Finally, to obtain a high-quality cipher image, the secure secret image is hidden in the grey carrier image by matrix encoding. The scheme can compress and encrypt the grey or colour plain image and then hide it in a grey carrier image. In addition, the theoretical peak signal-to-noise ratio (PSNR) between the cipher image and the carrier image is improved from 40.9292 to 42.1785 dB. The total running time is only about 0.35, 0.87 and 3.1 s for a 256 × 256, 512 × 512 and 1024 × 1024 grey or colour plain image, respectively.},
  archive      = {J_IETIP},
  author       = {Wei Chen and Yichuan Wang and Yeqiu Xiao and Xinhong Hei},
  doi          = {10.1049/ipr2.12858},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3235-3257},
  shortjournal = {IET Image Process.},
  title        = {Explore the potential of deep learning and hyperchaotic map in the meaningful visual image encryption scheme},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-light image enhancement for infrared and visible image
fusion. <em>IETIP</em>, <em>17</em>(11), 3216–3234. (<a
href="https://doi.org/10.1049/ipr2.12857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion (IVIF) is an essential branch of image fusion, and enhancing the visible image of IVIF can significantly improve the fusion performance. However, many existing low-light enhancement methods are unsuitable for the visible image enhancement of IVIF. In order to solve this problem, this paper proposes a new visible image enhancement method for IVIF. Firstly, the colour balance and contrast enhancement-based self-calibrated illumination estimation (CCSCE) is proposed to improve the input image&#39;s brightness, contrast, and colour information. Then, the method based on Mutually Guided Image Filtering (muGIF) is adopted to design a strategy to extract details adaptively from the original visible image, which can keep details without introducing additional noise effectively. Finally, the proposed visible image enhancement technique is used for IVIF tasks. In addition, the proposed method can be used for the visible image enhancement of IVIF and other low-light images. Experiment results on different public datasets and IVIF demonstrate the authors’ method&#39;s superiority from both qualitative and quantitative comparisons. The authors’ code will be publicly available at https://github.com/yiqiao666/low-light-enhancement-for-IVIF/tree/master .},
  archive      = {J_IETIP},
  author       = {Yiqiao Zhou and Lisiqi Xie and Kangjian He and Dan Xu and Dapeng Tao and Xu Lin},
  doi          = {10.1049/ipr2.12857},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3216-3234},
  shortjournal = {IET Image Process.},
  title        = {Low-light image enhancement for infrared and visible image fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). River boundary detection and autonomous cruise for unmanned
surface vehicles. <em>IETIP</em>, <em>17</em>(11), 3196–3215. (<a
href="https://doi.org/10.1049/ipr2.12848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of river boundaries is a crucial branch of the intelligent perception of unmanned surface vehicles (USVs), it can be used to determine the driving areas of USVs, and also to ensure driving safety by limiting the effective drivable areas of USVs in the river areas. Aiming to detect the boundaries of incompletely structured river channels, this study proposes a real-time detection method for river boundaries based on a Light Detection and Ranging (LiDAR) sensor. The point clouds that are disturbed by the water surface noise are filtered firstly, and then the spatial and geometric features are extracted separately from the point cloud detected above the water surface. To prevent the error detection and missing detection, the boundary point information is predicted and calibrated in real time by Extended Kalman Filter (EKF). A planning track generation algorithm for coastal autonomous cruise without relying on high-precision maps, and a heading and distance adaptive control method by Proportional-Integral-Derivative (PID), and different driving line generation methods for driving along the narrow river and wide river are proposed respectively. The experimental data verification of river boundary detection shows that the algorithm is accurate, real-time, and robust.},
  archive      = {J_IETIP},
  author       = {Kai Zhang and Min Hu and Fuji Ren and Yanwei Bao and Piao Shi and Daoyang Yu},
  doi          = {10.1049/ipr2.12848},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3196-3215},
  shortjournal = {IET Image Process.},
  title        = {River boundary detection and autonomous cruise for unmanned surface vehicles},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human stability assessment and fall detection based on
dynamic descriptors. <em>IETIP</em>, <em>17</em>(11), 3177–3195. (<a
href="https://doi.org/10.1049/ipr2.12847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fall detection systems use a number of different technologies to achieve their goals. This way, they contribute to better life conditions for the elderly community. The artificial vision is one of these technologies and, within this field, it has gained momentum over the course of the last few years as a consequence of the incorporation of different artificial neural networks (ANN&#39;s). These ANN&#39;s share a common characteristic, they are used to extract descriptors from images and video clips that, properly processed, will determine whether a fall has taken place. These descriptors, which capture kinematic features associated with the fall, are inferred from datasets recorded by young volunteers or actors who simulate falls. Systems based on this concept offer excellent performances in tests which use that kind of datasets. However, given the well-documented differences between these falls and the real ones, concerns about system performances when processing falls of elderly people are raised. This work implements an alternative approach to the classical use of kinematic descriptors. To do it, for the first time to the best of the authors’ knowledge, the authors propose the introduction of human dynamic stability descriptors used in other fields to determine whether a fall has taken place. These descriptors approach the human body in terms of balance and stability; this way, differences between real and simulated falls become irrelevant, as all falls are a direct result of fails in the continuous effort of the body to keep balance, regardless of other considerations. The descriptors are determined by using the information provided by a neural network able to estimate the body centre of mass and the feet projections onto the ground plane, as well as the feet contact status. The theory behind this new approach and its validity is studied in this article with very promising results, as it is able to match or over exceed the performances of previous systems using kinematic descriptors employing available data and, given the independence of this approach from the conditions of the fall, it has the potential to have a better behaviour than classic systems when facing falls of elderly people.},
  archive      = {J_IETIP},
  author       = {Jesús Gutiérrez and Sergio Martin and Victor Rodriguez},
  doi          = {10.1049/ipr2.12847},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3177-3195},
  shortjournal = {IET Image Process.},
  title        = {Human stability assessment and fall detection based on dynamic descriptors},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel gamut expansion method based on combined
global–local mapping for sRGB-to-ProPhoto conversion. <em>IETIP</em>,
<em>17</em>(11), 3165–3176. (<a
href="https://doi.org/10.1049/ipr2.12844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of display technology, more displays can cover the wider gamut, but most of the content they show is based on a small gamut. It is significant to employ gamut expansion (GE) to expand the small gamut images to a wider target gamut. Most of the existing GE methods only use global or local operations to realize the mapping from small gamut to wide gamut. However, the utilization of both global information and local feature is important for GE. In this article, the authors propose a combined global-local gamut expansion network (G-LGENet) for mapping the input standard RGB (sRGB) images to wider ProPhoto RGB space. In G-LGENet, the global colour mapping module first extracts and fuses the global colour priors and learns the mapping of colour information for the corresponding pixels. And then, the local enhancement (LE) is designed to extract the local colour information between the corresponding pixel and neighbourhood pixels. The experimental results on a sRGB-to-ProPhoto dataset have demonstrated that the proposed G-LGENet outperforms the other excellent GE methods qualitatively and visually.},
  archive      = {J_IETIP},
  author       = {Huailin Li and Qinsen Liu and Bangyong Sun and Mengnan Liu},
  doi          = {10.1049/ipr2.12844},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3165-3176},
  shortjournal = {IET Image Process.},
  title        = {A novel gamut expansion method based on combined global–local mapping for sRGB-to-ProPhoto conversion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal predictive classification of alzheimer’s disease
based on attention-combined fusion network: Integrated neuroimaging
modalities and medical examination data. <em>IETIP</em>,
<em>17</em>(11), 3153–3164. (<a
href="https://doi.org/10.1049/ipr2.12841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis of Alzheimer&#39;s disease (AD) plays a key role in preventing and responding to this neurodegenerative disease. It has shown that, compared with a single imaging modality-based classification of AD, synergy exploration among multimodal neuroimages is beneficial for the pathological identification. However, effectively exploiting multimodal information is still a big challenge due to the lack of efficient fusion methods. Herein, a multimodal fusion network based on attention mechanism is proposed, in which magnetic resonance imaging (MRI) and positron emission computed tomography (PET) images are converted into feature vectors with the same dimension, while the demographic information and clinical data are preprocessed and converted into feature vectors through embedding. This attention model can focus on important feature points, fuse the multimodal information more effectively, and thus provide accurate diagnosis and prediction for different pathological stages. The results show that the model achieves an accuracy of 84.1% for triple classification tasks in normal cognition (NC) versus mild cognitive impairment (MCI) versus AD and 93.9% prediction accuracy in stable MCI (sMCI) versus progressive MCI (pMCI). In contrast to the existing multimodal diagnosis methods, our model yields a state-of-the-art accuracy of AD diagnosis, which is powerful and promising in clinical practice.},
  archive      = {J_IETIP},
  author       = {Hui Chen and Huiru Guo and Longqiang Xing and Da Chen and Ting Yuan and Yunpeng Zhang and Xuedian Zhang},
  doi          = {10.1049/ipr2.12841},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3153-3164},
  shortjournal = {IET Image Process.},
  title        = {Multimodal predictive classification of alzheimer&#39;s disease based on attention-combined fusion network: Integrated neuroimaging modalities and medical examination data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effectiveness of specularity removal from hyperspectral
images in the colour spectral measurement of wool fibres.
<em>IETIP</em>, <em>17</em>(11), 3143–3152. (<a
href="https://doi.org/10.1049/ipr2.12839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microscopic hyperspectral imaging technology is a potential non-destructive and non-contact method for colour measurement of micrometre-sized textile fibres. However, specularity on the fibre surface can distort the accurate colour information and affect the accuracy of the colour measurement. This paper proposed a specular-constrained sparse approximation (SCSA) for specular-diffuse reflection separation from hyperspectral images of wool fibres. First, a specular prior map is generated based on the lightness dissimilarity. Then the SCSA model is used to decompose the processed hyperspectral image A into low-rank data L , sparse specularity data S constrained by the specular prior map, sparse noise E, and Gaussian noise N . A non-linear logistic sigmoid function and a sparse approximation of A – L – N to S are used to improve the performance of specularity removal during iterative optimization. The experimental results show that the proposed method significantly preserves diffuse reflectance and texture details in the specular highlight regions to obtain actual spectral reflectance and chromatic values from hyperspectral images of wool fibres.},
  archive      = {J_IETIP},
  author       = {Kebin Qiu and Jiajia Shen and Weiguo Chen and Jiahui Zhang},
  doi          = {10.1049/ipr2.12839},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3143-3152},
  shortjournal = {IET Image Process.},
  title        = {Effectiveness of specularity removal from hyperspectral images in the colour spectral measurement of wool fibres},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based COVID-19 diagnosis using CT scans with
laboratory and physiological parameters. <em>IETIP</em>,
<em>17</em>(11), 3127–3142. (<a
href="https://doi.org/10.1049/ipr2.12837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global economy has been dramatically impacted by COVID-19, which has spread to be a pandemic. COVID-19 virus affects the respiratory system, causing difficulty breathing in the patient. It is crucial to identify and treat infections as soon as possible. Traditional diagnostic reverse transcription-polymerase chain reaction (RT-PCR) methods require more time to find the infection. A high infection rate, slow laboratory analysis, and delayed test results caused the widespread and uncontrolled spread of the disease. This study aims to diagnose the COVID-19 epidemic by leveraging a modified convolutional neural network (CNN) to quickly and safely predict the disease&#39;s appearance from computed tomography (CT) scan images and a laboratory and physiological parameters dataset. A dataset representing 500 patients was used to train, test, and validate the CNN model with results in detecting COVID-19 having an accuracy, sensitivity, specificity, and F1-score of 99.33%, 99.09%, 99.52%, and 99.24%, respectively. These experimental results suggest that our strategy performs better than previously published approaches.},
  archive      = {J_IETIP},
  author       = {Humam Adnan Sameer and Ammar Hussein Mutlag and Sadik Kamel Gharghan},
  doi          = {10.1049/ipr2.12837},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3127-3142},
  shortjournal = {IET Image Process.},
  title        = {Deep learning-based COVID-19 diagnosis using CT scans with laboratory and physiological parameters},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight and stochastic depth residual attention
network for remote sensing scene classification. <em>IETIP</em>,
<em>17</em>(11), 3106–3126. (<a
href="https://doi.org/10.1049/ipr2.12836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid development of satellite technology, high-spatial-resolution remote sensing (HRRS) images have highly complex spatial distributions and multiscale features, making the classification of such images a challenging task. The key to scene classification is to accurately understand the main semantic information contained in images. Convolutional neural networks (CNNs) have outstanding advantages in this field. Deep CNNs (D-CNNs) with better performance tend to have more parameters and higher complexity. However, shallow CNNs have difficulty extracting the key features of complex remote sensing images. In this paper, we propose a lightweight network with a random depth strategy for remote sensing scene classification (LRSCM). We construct a convolutional feature extraction module, DCAB, which incorporates depthwise separable convolutional and inverted residual structures, effectively reducing the numbers of required parameters and computations, and retains and utilizes low-level features. In addition, coordinate attention (CA) is integrated into the module, thereby further improving the network&#39;s ability to extract key local information. To further reduce the complexity of model training, the residual module adopts a stochastic depth strategy, providing the network with a random depth. Comparative experiments on five public datasets show that the LRSCM network can achieve results comparable to those of other state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Xinyu Wang and Haixia Xu and Liming Yuan and Xianbin Wen},
  doi          = {10.1049/ipr2.12836},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3106-3126},
  shortjournal = {IET Image Process.},
  title        = {A lightweight and stochastic depth residual attention network for remote sensing scene classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised depth completion with multi-view geometric
constraints. <em>IETIP</em>, <em>17</em>(11), 3095–3105. (<a
href="https://doi.org/10.1049/ipr2.12834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning-based depth completion is a cost-effective way for 3D environment perception. However, it is also a challenging task because sparse depth may deactivate neural networks. In this paper, a novel Sparse-Dense Depth Consistency Loss (SDDCL) is proposed to penalize not only the estimated depth map with sparse input points but also consecutive completed dense depth maps. Combined with the pose consistency loss, a new self-supervised learning scheme is developed, using multi-view geometric constraints, to achieve more accurate depth completion results. Moreover, to tackle the sparsity issue of input depth, a Quasi Dense Representations (QDR) module with triplet branches for spatial pyramid pooling is proposed to produce more dense feature maps. Extensive experimental results on VOID, NYUv2, and KITTI datasets show that the method outperforms state-of-the-art self-supervised depth completion methods.},
  archive      = {J_IETIP},
  author       = {Mingkang Xiong and Zhenghong Zhang and Jiyuan Liu and Tao Zhang and Huilin Xiong},
  doi          = {10.1049/ipr2.12834},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3095-3105},
  shortjournal = {IET Image Process.},
  title        = {Self-supervised depth completion with multi-view geometric constraints},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multimodal feature fusion image dehazing method with scene
depth prior. <em>IETIP</em>, <em>17</em>(11), 3079–3094. (<a
href="https://doi.org/10.1049/ipr2.12866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current dehazing networks usually only learn haze features in a single-image colour space and often suffer from uneven dehazing, colour, and edge degradation when confronted with different scales of ground objects in the depth space of the scene. The authors propose a multimodal feature fusion image dehazing method with scene depth prior based on a decoder–encoder backbone network. The multimodal feature fusion module was first designed. In this module, affine transformation and polarized self-attention mechanism are used to realize the fusion of image colour and depth prior feature, to improve the representation ability of the model for different scale ground haze feature in-depth space. Then, the feature enhancement module (FEM) is added, and deformable convolution and difference convolution methods are used to enhance the representation ability of the model for the geometric and texture feature of the ground objects. The publicly available dehazing datasets are used for comparison and ablation experiments. The results show that compared with the existing classical dehazing networks, the peak signal-to-noise ratio (PSNR) and SSIM of the authors’ proposed method have been significantly improved, have a more uniform dehazing effect in different depth spaces, and maintain the colour and edge details of the ground objects very well.},
  archive      = {J_IETIP},
  author       = {Zhang Zhengpeng and Cheng Yan and Zhang Shuai and Bu Lijing and Deng Mingjun},
  doi          = {10.1049/ipr2.12866},
  journal      = {IET Image Processing},
  month        = {9},
  number       = {11},
  pages        = {3079-3094},
  shortjournal = {IET Image Process.},
  title        = {A multimodal feature fusion image dehazing method with scene depth prior},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep image segmentation-based method for stitching
ancient-book images without an overlapping region. <em>IETIP</em>,
<em>17</em>(10), 3068–3078. (<a
href="https://doi.org/10.1049/ipr2.12856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With continuous advancements in ancient-book digitization and preservation research, the problems with the stitching of ancient-book images have become increasingly prominent, as traditional feature-mapping-based methods cannot satisfactorily stitch non-overlapping images. To realize the accurate stitching of the left and right pages of ancient-book images, this paper proposes a method for ancient-book image stitching to meet the requirements of their digitization in back-wrapped binding and other binding forms. First, a dataset of the black text frames from ancient-book images was established and then used to train a VGG16-UNet network for the extraction of black text frames. Then, the Douglas–Peucker algorithm was used to fit the black text frames and filter outliers. Finally, a sliding matching algorithm based on the position information of black text frames was proposed for the rectification of misalignments. The results showed that the method achieved a satisfying stitching effect and had good robustness.},
  archive      = {J_IETIP},
  author       = {Genlang Chen and Han Zhou and Gang Huang and Guanghui Song and Jiajian Zhang},
  doi          = {10.1049/ipr2.12856},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {3068-3078},
  shortjournal = {IET Image Process.},
  title        = {A deep image segmentation-based method for stitching ancient-book images without an overlapping region},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recognition of pear leaf disease under complex background
based on DBPNet and modified mobilenetV2. <em>IETIP</em>,
<em>17</em>(10), 3055–3067. (<a
href="https://doi.org/10.1049/ipr2.12855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the challenge of pear leaf disease recognition caused by uneven illumination, overlapping leaves, and other green plants in the background, a two-stage strategy-based framework for pear leaf segmentation and disease classification is proposed. Initially, a double branch polymerization net fusing features of the low-level feature branch and semantic branch is constructed to extract the target diseased pear leaf and eliminate background interference. Then an improved lightweight neural network (Inverted-Inception Efficient-Excitation-and-Filtering-Bottleneck MobileNet-v2, I 2 EMv2) is used to capture multi-scale lesion information and ainhibit invalid feature channels and filter redundant features for the final classification. The experimental results show that the proposed framework can accurately extract the pear leaf region with complete boundary from the complex background, maximize the retention of lesion information, and achieve high-precision pear leaf disease identification. The mean absolute error, F-measure, and intersection over union of leaf segmentation are 0.027, 0.947, and 0.880, respectively, and the average recognition accuracy of leaf disease is 92.05%. Compared with others, the method proposed in this paper has superior performance on segmentation and classification, which provides a reference for pear leaf disease classification under complex background.},
  archive      = {J_IETIP},
  author       = {Xuehui Wu and Zhiwei Luo and Huanliang Xu},
  doi          = {10.1049/ipr2.12855},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {3055-3067},
  shortjournal = {IET Image Process.},
  title        = {Recognition of pear leaf disease under complex background based on DBPNet and modified mobilenetV2},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image segmentation technology based on transformer in
medical decision-making system. <em>IETIP</em>, <em>17</em>(10),
3040–3054. (<a href="https://doi.org/10.1049/ipr2.12854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the improvement in computing power and the development of computer technology, deep learning has pene-trated into various fields of the medical industry. Segmenting lesion areas in medical scans can help clinicians make accurate diagnoses. In particular, convolutional neural networks (CNNs) are a dominant tool in computer vision tasks. They can accurately locate and classify lesion areas. However, due to their inherent inductive bias, CNNs may lack an understanding of long-term dependencies in medical images, leading to less accurate grasping of details in the images. To address this problem, we explored a Transformer-based solution and studied its feasibility in medical imaging tasks (OstT). First, we performed super-resolution reconstruction on the original MRI image of osteosarcoma and improved the texture features of the tissue structure to reduce the error caused by the unclear tissue structure in the image during model training. Then, we propose a Transformer-based method for medical image segmentation. A gated axial attention model is used, which augments existing architectures by introducing an additional control mechanism in the self-attention module to improve segmentation accuracy. Experiments on real datasets show that our method outper-forms existing models such as Unet. It can effectively assist doctors in imaging examinations.},
  archive      = {J_IETIP},
  author       = {Keke He and Fangfang Gou and Jia Wu},
  doi          = {10.1049/ipr2.12854},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {3040-3054},
  shortjournal = {IET Image Process.},
  title        = {Image segmentation technology based on transformer in medical decision-making system},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multiple gated boosting network for multi-organ medical
image segmentation. <em>IETIP</em>, <em>17</em>(10), 3028–3039. (<a
href="https://doi.org/10.1049/ipr2.12852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentations provide important clues for diagnosing diseases. U-shaped neural networks with skip connections have become one of popular frameworks for medical image segmentation. Skip connections really reduce loss of spatial details caused by down-sampling, but they cannot handle well semantic gaps between low- and high-level features. It is quite challenging to accurately separate out long, narrow, and small organs from human bodies. To solve these problems, the authors propose a Multiple Gated Boosting Network (MGB-Net). To boost spatial accuracy, the authors first adopt Gated Recurrent Units (GRU) to design multiple Gated Skip Connections (GSC) at different levels, which efficiently reduce the semantic gap between the shallow and deep features. The Update and Reset gates of GRUs enhance features beneficial to segmentation and suppress information adverse to final results in a recurrent way. To obtain more scale invariances, the authors propose a module of Multi-scale Weighted Channel Attention (MWCA). The module first uses convolutions with different kernel sizes and group numbers to generate multi-scale features, and then adopts learnable weights to emphasize the importance of each scale for capturing attention features. Blocks of Transformer Self-Attention (TSA) are sequentially stacked to extract long-range dependency features. To effectively fuse and boost the features of MWCA and TSA, the authors use GRUs again to propose a Gated Dual Attention module (GDA), which enhances beneficial features and suppresses adverse information in a gated learning way. Experiments show that the authors’ method achieves an average Dice coefficient of 80.66% on the Synapse multi-organ segmentation dataset. The authors’ method outperforms the state-of-the-art methods on medical images. In addition, the authors’ method achieves a Dice segmentation accuracy of 62.77% on difficult objects such as pancreas, significantly exceeding the current average accuracy, so multiple gated boosting (MGB) methods are reliably effective for improving the ability of feature representations. The authors’ code is publicly available at https://github.com/DAgalaxy/MGB-Net .},
  archive      = {J_IETIP},
  author       = {Feiniu Yuan and Zhaoda Tang and Chunmei Wang and Qinghua Huang and Jinting Shi},
  doi          = {10.1049/ipr2.12852},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {3028-3039},
  shortjournal = {IET Image Process.},
  title        = {A multiple gated boosting network for multi-organ medical image segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Water-surface infrared small object detection based on
spatial feature weighting and class balancing method. <em>IETIP</em>,
<em>17</em>(10), 3012–3027. (<a
href="https://doi.org/10.1049/ipr2.12851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared imaging is widely used due to its penetration capability to operate under many weather or lighting condition. However, due to the far distance of aerial view, feature blur, and the scarcity of aerial infrared data, the detection of small infrared targets on the water surface remains a challenging problem. In response to the problem of unclear features, we propose the spatial feature weighting method based on 2D Gaussian distribution. This method increases the weight of the target area by adaptively adjusting the feature activation. Secondly, for the problem of rare aerial perspective infrared data, we propose the cross-spectral data migration method. By introducing the domain difference loss function to optimize the pseudo-label selection process, the range of target domain distribution is expanded, and the adaptability of the detector is improved. Finally, in response to the problem of underfitting caused by category imbalance in transfer learning, we propose the class balancing method that effectively reduces the false detection. Extensive experiments were conducted on both benchmark datasets and the self-built dataset to evaluate the effectiveness and robustness of our method. The proposed method was evaluated with different models and various scenarios, and the results demonstrated the effectiveness.},
  archive      = {J_IETIP},
  author       = {Tian Hui and YueLei Xu and HuaFeng Li and Qing Zhou and Jarhinbek Rasol},
  doi          = {10.1049/ipr2.12851},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {3012-3027},
  shortjournal = {IET Image Process.},
  title        = {Water-surface infrared small object detection based on spatial feature weighting and class balancing method},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CSIT: Channel spatial integrated transformer for human pose
estimation. <em>IETIP</em>, <em>17</em>(10), 3002–3011. (<a
href="https://doi.org/10.1049/ipr2.12850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human keypoints detection is different from general detection tasks and requires networks that can learn visual information and anatomical constraints. Since CNN is excellent in extracting texture features of images and transformer can learn the correlation among keypoints well, many CTPNets (CNN+transformer type human pose estimation networks) have emerged. However, these networks are unconcerned with the processing of the features extracted from the CNN and naturally expand only from the channel dimension, ignoring the spatial features in the visual information that are essential for complex detection tasks like pose estimation. So the channel spatial integrated transformer for human pose estimation, termed CSIT, is proposed. The visual information are summarized as texture and spatial information, and a parallel network is used to expand the feature maps in the channel and spatial dimensions to learn texture features and spatial features respectively. In addition, anatomically constrained information is learned by keypoint embeddings. At the end of the network, the 1D vector representation method with more advanced performance and more compatible with transformer&#39;s characteristics is used to predict keypoints. Experiments show that CSIT outperforms the mainstream CTPNets on the COCO test-dev dataset, and also show satisfactory results on the MPII dataset.},
  archive      = {J_IETIP},
  author       = {Shaohua Li and Haixiang Zhang and Hanjie Ma and Jie Feng and Mingfeng Jiang},
  doi          = {10.1049/ipr2.12850},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {3002-3011},
  shortjournal = {IET Image Process.},
  title        = {CSIT: Channel spatial integrated transformer for human pose estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integration of lightweight cryptography and watermarking
with compression for high speed and reliable communication of digital
images in IoT. <em>IETIP</em>, <em>17</em>(10), 2984–3001. (<a
href="https://doi.org/10.1049/ipr2.12849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has grown rapidly over the past few years. The main challenges for people who design and make these systems are to make them use less energy and use as small hardware as possible. Recording and sending compressed images in IoT systems must be secure. Secure image transmission requires encryption, watermarking, and compression. In choosing these algorithms, one should always consider energy efficiency and low hardware complexity. So, we propose a secure way to send data in IoT systems by combining encryption, watermarking, and compression. Depending on what is needed, we use a lightweight encryption algorithm, discrete wavelet transform (DWT) watermarking, and Discrete cosine transform + Lempel-Ziv-Welch (DCT+LZW) compression. In this method, the main contribution is to make a security pack that is small, strong, and easy to implement for secure image transmission on IoT systems. In the experiments, several images are used to judge how well the proposed integrated method works and how well it compares to other methods. When the proposed method was compared to other methods, the peak signal-to-noise ratio (PSNR) parameter improved by more than 20% in the Lena image and by more than 11% in the Baboon image.},
  archive      = {J_IETIP},
  author       = {Hadi Nazari and Mahmoud Mahlouji Bidgoli and Hossein Ghasvari},
  doi          = {10.1049/ipr2.12849},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2984-3001},
  shortjournal = {IET Image Process.},
  title        = {Integration of lightweight cryptography and watermarking with compression for high speed and reliable communication of digital images in IoT},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Network adaptation for color image semantic segmentation.
<em>IETIP</em>, <em>17</em>(10), 2972–2983. (<a
href="https://doi.org/10.1049/ipr2.12846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image analysis using deep learning has made significant progress in the last few decades, and the importance of pre-processing input images has become evident. However, adapting a network structure suitable for input images has not been considered. In this study, a simple network adaptation method for color image analysis is described. The method is illustrated using semantic segmentation, which mainly takes a color image as input. The method is inspired by chrominance subsampling, which is a practical method for image and video analysis. The human visual system is less sensitive to color differences than it is to brightness, and based on this phenomenon, it is possible to improve existing networks by providing less resolution to chroma information than luminance information in the network encoder design by applying the group convolution concept. The proposed method helps to achieve improved results without changing the complexity of the baseline network model, and is especially helpful in applications with limited resources, such as autonomous driving, augmented reality. Experiments were performed on a combination of datasets (i.e. CamVid, Cityscapes and KITTI-360) and networks (i.e. ENet, ERFNet, Deeplabv3plus with mobilenetv2). The results show that the method improves the performance of existing network structures without increasing the number of parameters.},
  archive      = {J_IETIP},
  author       = {Taeg-Hyun An and Jungyu Kang and Kyoung-Wook Min},
  doi          = {10.1049/ipr2.12846},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2972-2983},
  shortjournal = {IET Image Process.},
  title        = {Network adaptation for color image semantic segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underwater image enhancement via a channel-wise transmission
estimation network. <em>IETIP</em>, <em>17</em>(10), 2958–2971. (<a
href="https://doi.org/10.1049/ipr2.12845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement for image processing and underwater robotic vision have recently attracted much academic attention. However, in most existing methods, underwater image enhancement is completed with a simple assumption: the attenuation coefficients are unified across the color channels. This assumption leads to unstable and visually unpleasing enhancement results. Moreover, these methods cannot be successfully applied to explore relatively independent transmissions from multiple color channels with complimentary feature information. To address these challenges, a novel channel-wise transmission estimation network (CTEN) is proposed, which aims to pioneer the exploration of the transmission difference across the color channels in an underwater scene. Specifically, a color-specific correction module is proposed to automatically quantify the transmission ability of multiple color channels in the underwater environment. Furthermore, a channel-wise transmission estimation module is designed to simultaneously explore the relative independence of multi-color channels and estimate the medium transmissions for each color channel, which represents the attenuation degree of different color radiances after reflecting in the water. Then, a novel residual strategy is introduced to integrate these two modules to complete the underwater enhancement. Using the model, the authors are able to provide an answer as to why channel-wise transmission estimation are better than single transmission estimation and establish a generalization theory to show the effect of the independent transmission estimation model for each color channel. Experiments on several underwater image datasets verify the superiority of the proposed CTEN model.},
  archive      = {J_IETIP},
  author       = {Qiang Wang and Bo Fu and Huijie Fan},
  doi          = {10.1049/ipr2.12845},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2958-2971},
  shortjournal = {IET Image Process.},
  title        = {Underwater image enhancement via a channel-wise transmission estimation network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoGAN: Cooperatively trained conditional and unconditional
GAN for person image generation. <em>IETIP</em>, <em>17</em>(10),
2949–2957. (<a href="https://doi.org/10.1049/ipr2.12843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person image generation aims to synthesize realistic person images that follow the same distribution as the given dataset. Previous attempts can be generally categorized into two classes: conditional GAN and unconditional GAN. The former usually uses pose information as condition to make pose transfer using GAN. The generated person have the same identity as the source person. The latter generates person images from scratch, and the real person images are only used as references for the discriminator. While conditional GAN is widely studied, unconditional GAN is also worth exploring because it can synthesize person image with new identity, which is a useful manner of data augmentation. These two types of generating methods have their different advantages and disadvantages, and sometimes they are complementary. This paper proposes a CoGAN to cooperatively train two types of GANs in an end-to-end framework. The two GANs serve different purposes, and can learn from each other during the cooperative learning procedure. The experimental results on public datasets show that the proposed CoGAN improves the performance of both baseline methods, and achieves competitive results compared with state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Yang Liu and Hao Sheng and Shuai Wang and Yubin Wu and Zhang Xiong},
  doi          = {10.1049/ipr2.12843},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2949-2957},
  shortjournal = {IET Image Process.},
  title        = {CoGAN: Cooperatively trained conditional and unconditional GAN for person image generation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised deep homography with multi-scale global
attention. <em>IETIP</em>, <em>17</em>(10), 2937–2948. (<a
href="https://doi.org/10.1049/ipr2.12842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Homography estimation serves an important role in many computer vision tasks. Depending heavily on hand-craft feature quality, traditional methods degenerate sharply in scenes with low texture. Existing deep homography methods can handle the low-texture problem but are not robust for scenes with low overlap rates and/or illumination changes. This paper proposes a novel unsupervised homography estimation method that can simultaneously handle such low overlap and illumination change. Specifically, a powerful module, named global transformer contextual encoder (GTCE) is first designed, together with a correlation encoder to effectively aggregate global contextual information and reduce matching ambiguity between feature maps. Moreover, a hybrid photo-perceptual loss for unsupervised homography estimation is proposed. The proposed loss function considers alignment information on both pixel level and perceptual level thus helping this network to be more adaptive to various scenes, including normal cases and illumination change cases. The results of extensive experiments on synthetic and real-world datasets demonstrate the superiority of this proposed method over current state-of-the-art solutions especially on challenging scenes with low overlap rates, repetitive patterns and illumination changes.},
  archive      = {J_IETIP},
  author       = {Wei Hu and Chu He and Mingyuan Lin and Haoyu Zhou},
  doi          = {10.1049/ipr2.12842},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2937-2948},
  shortjournal = {IET Image Process.},
  title        = {Unsupervised deep homography with multi-scale global attention},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A progressive segmentation with weight contrast label
enhancement for weakly supervised video salient object detection.
<em>IETIP</em>, <em>17</em>(10), 2925–2936. (<a
href="https://doi.org/10.1049/ipr2.12840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scribble labels have gained increasing attention in the field of weakly supervised video salient object detection (VSOD). Based on scribble labels, latest methods can spread labeled pixels to unlabeled regions using local coherence loss, but predicted objects often lose detail and boundary information. In this work, a novel method based on back-foreground weight contrast is proposed that adds label enhancement points to facilitate the model to learn the edge, detail and location of salient object. Additionally, a new VSOD framework based on global structural localization is introduced. Enhanced scribble labels are used to assist the model for global localization, and then the located regions are finely segmented by the trained model. Extensive experiments demonstrate that the method achieves the state-of-the-art performance on common VSOD datasets, with an improvement of 3.75%, 4.68%, and 0.88% in S-measure, F-measure, and MAE, respectively.},
  archive      = {J_IETIP},
  author       = {Zelin Lu and Haoran Liang and Binwei Xu and Ronghua Liang},
  doi          = {10.1049/ipr2.12840},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2925-2936},
  shortjournal = {IET Image Process.},
  title        = {A progressive segmentation with weight contrast label enhancement for weakly supervised video salient object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A court line extraction algorithm for badminton tournament
videos with horizontal line projection learning. <em>IETIP</em>,
<em>17</em>(10), 2907–2924. (<a
href="https://doi.org/10.1049/ipr2.12838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Court line extraction is one of the important steps in the analysis of sport videos. The court extraction is the foundation of the analysis of badminton video, and an efficient method with horizontal line projection K-means machine learning algorithm to extract court lines from different broadcast badminton tournament videos is proposed in this paper. The horizontal lines are projected into 1-D histogram signal; then the signal is trained to learn the intensity of the histogram signal for locating the positions of the horizontal court lines. After the equations of the horizontal court lines and the court lines in the vertical direction have been formulized, the intersection points of the court lines can be calculated and the court line can be extracted. The experimental results show that the proposed method can extract the court lines more efficiently than that done by the Hough transform-related algorithms, which are widely applied in computer vision and self-driving car applications.},
  archive      = {J_IETIP},
  author       = {Chun-Ta Wei and Shiuh-Ku Weng},
  doi          = {10.1049/ipr2.12838},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2907-2924},
  shortjournal = {IET Image Process.},
  title        = {A court line extraction algorithm for badminton tournament videos with horizontal line projection learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SNENet: An adaptive stego noise extraction network using
parallel dilated convolution for JPEG image steganalysis.
<em>IETIP</em>, <em>17</em>(10), 2894–2906. (<a
href="https://doi.org/10.1049/ipr2.12835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The steganalysis for JPEG image is an important research topic, as the enormous popularity of JPEG image on Internet. However, the stego noise feature extraction process of the existing deep learning-based steganalytic methods are not adaptive enough to the content of the image, which may lead to suboptimal steganalysis performance. In order to solve this issue, an adaptive stego noise extraction network, named SNENet, for JPEG image steganalysis is proposed. The stego noise extraction module of the network is specifically designed for steganalysis, which consists of parallel dilated convolutional layer and inverted bottleneck layer. This specific design expands the receptive field of the network, which makes the extraction of the stego noise more global and adaptive to the content of the image. The experimental results indicate that proposed network outperforms the state-of-the-art steganalytic method by as much as 6.25% for UED-JC and 3.35% for J-UNIWARD. The design of the network is also justified in the extensive ablation experiments.},
  archive      = {J_IETIP},
  author       = {Wentong Fan and Zhenyu Li and Hao Li and Yi Zhang and Xiangyang Luo},
  doi          = {10.1049/ipr2.12835},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2894-2906},
  shortjournal = {IET Image Process.},
  title        = {SNENet: An adaptive stego noise extraction network using parallel dilated convolution for JPEG image steganalysis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-image super-resolution using lightweight
transformer-convolutional neural network hybrid model. <em>IETIP</em>,
<em>17</em>(10), 2881–2893. (<a
href="https://doi.org/10.1049/ipr2.12833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With constant advances in deep learning methods as applied to image processing, deep convolutional neural networks (CNNs) have been widely explored in single-image super-resolution (SISR) problems and have attained significant success. These CNN-based methods cannot fully use the internal and external information of the image. The authors add a lightweight Transformer structure to capture this information. Specifically, the authors apply a dense block structure and residual connection to build a residual dense convolution block (RDCB) that reduces the parameters somewhat and extracts shallow features. The lightweight transformer block (LTB) further extracts features and learns the texture details between the patches through the self-attention mechanism. The LTB comprises an efficient multi-head transformer (EMT) with small graphics processing unit (GPU) memory footprint, and benefits from feature preprocessing by multi-head attention (MA), reduction, and expansion. The EMT significantly reduces the use of GPU resources. In addition, a detail-purifying attention block (DAB) is proposed to explore the context information in the high-resolution (HR) space to recover more details. Extensive evaluations of four benchmark datasets demonstrate the effectiveness of the authors’ proposed model in terms of quantitative metrics and visual effects. The proposed EMT only uses about 40% as much GPU memory as other methods, with better performance.},
  archive      = {J_IETIP},
  author       = {Yuanyuan Liu and Mengtao Yue and Han Yan and Lu Zhu},
  doi          = {10.1049/ipr2.12833},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2881-2893},
  shortjournal = {IET Image Process.},
  title        = {Single-image super-resolution using lightweight transformer-convolutional neural network hybrid model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A domain translation network with contrastive constraint for
unpaired motion image deblurring. <em>IETIP</em>, <em>17</em>(10),
2866–2880. (<a href="https://doi.org/10.1049/ipr2.12832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most motion deblurring methods require a large amount of paired training data, which is nearly unreachable in practice. To overcome the limitation, a domain translation network with contrastive constraint for unpaired motion image deblurring is proposed. First, a domain translation network with two streams, a sharp domain translation stream and a blurred domain translation stream, to handle unpaired sharp and blurred images from the real world is presented. Second, a contrastive constraint loss in the deep intermediate level for the two streams to promote the network to produce deblurring results close to the real sharp image is proposed. Third, distinct loss functions for the two streams to preserve the edge and texture detail of the deblurring image is designed. Extensive experiments on several benchmark datasets demonstrate that the proposed network achieves better visual performance than the current state-of-the-art methods for unpaired motion image deblurring.},
  archive      = {J_IETIP},
  author       = {Bingxin Zhao and Weihong Li},
  doi          = {10.1049/ipr2.12832},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2866-2880},
  shortjournal = {IET Image Process.},
  title        = {A domain translation network with contrastive constraint for unpaired motion image deblurring},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The optimal triangulation method is not really optimal.
<em>IETIP</em>, <em>17</em>(10), 2855–2865. (<a
href="https://doi.org/10.1049/ipr2.12831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triangulation refers to the problem of finding a 3D point from its 2D projections on multiple images taken at different camera poses. For solving this problem, it is a common practice to use the so-called optimal triangulation method. But, the method can be optimal only if we assume no uncertainty in the camera parameters. In this paper, an extensive comparison between various existing methods for the calibrated cameras is performed, in which different poses are considered. Furthermore, uncertainty sensitivity analysis is conducted for extrinsic parameters of cameras. The results show that the optimal triangulation method is actually not the best choice when there is uncertainty in extrinsic parameters. Interestingly, it can be observed that the simple midpoint method works equally well or outperforms the optimal triangulation and many other methods. Apart from its high performance, the midpoint method has a simple closed form solution for multiple views while the optimal triangulation method is hard to be used for more than two views. Therefore, in contrast to the common practice, we argue that the simple midpoint method can be a good choice in the structure-from-motion process where there is uncertainty in extrinsic camera parameters.},
  archive      = {J_IETIP},
  author       = {Seyed-Mahdi Nasiri and Reshad Hosseini and Hadi Moradi},
  doi          = {10.1049/ipr2.12831},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2855-2865},
  shortjournal = {IET Image Process.},
  title        = {The optimal triangulation method is not really optimal},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pyramided and optimized blurred shape model for plant leaf
classification. <em>IETIP</em>, <em>17</em>(10), 2838–2854. (<a
href="https://doi.org/10.1049/ipr2.12830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant leaf classification is a crucial task in the field of computer vision and pattern recognition, with various applications such as plant species identification and disease diagnosis. In this paper, the authors introduce the Pyramid Blurred Shape Model (PBSM) as a new descriptor for plant leaf classification. The PBSM extracts both shape and texture features from plant leaf images, which are combined to define a probability density function for leaf shape. Our experimental results show that the proposed PBSM achieves high classification accuracy, F1-score, and precision-recall results, demonstrating its effectiveness for leaf classification. However, extracting all available features from leaf images can lead to redundant and inessential features, which can degrade the classification performance and computational efficiency. To address this issue, the authors implement grey wolf optimization (GWO)-based feature selection to identify the most informative features for classification. The final set of features is then classified using a list of selected classifiers, further enhancing the performance of the authors’ approach. The authors evaluate their proposed method on three publicly available datasets, namely the Middle European Woods (MEW), Swedish, and Flavia leaf datasets, and achieve high classification accuracies of 96.34%, 96.89%, and 92.41% for the Flavia, Swedish, and MEW leaf datasets, respectively. The authors’ approach outperforms state-of-the-art descriptors in terms of classification accuracy and robustness, demonstrating its potential for real-world applications. Overall, the authors’ proposed PBSM descriptor with feature selection provides a reliable and efficient solution for plant leaf classification. It can contribute to the development of automated plant species identification systems and disease diagnosis, thereby facilitating the conservation and protection of plant species.},
  archive      = {J_IETIP},
  author       = {Rolla Almodfer and Mohammed Mudhsh and Jifei Zhao},
  doi          = {10.1049/ipr2.12830},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2838-2854},
  shortjournal = {IET Image Process.},
  title        = {Pyramided and optimized blurred shape model for plant leaf classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight image matting algorithm based on deep learning.
<em>IETIP</em>, <em>17</em>(10), 2829–2837. (<a
href="https://doi.org/10.1049/ipr2.12829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the problem that the deep learning-based image matting algorithm cannot balance accuracy and model size, a lightweight image matting algorithm based on deep learning is proposed. Considering the limitation of memory and computing resources, and aiming at lightweight. We construct a network and gradually improved it. Firstly, apply deep detachable convolution to deep image matting networks to form faster and stronger encoder and decoder networks. The simultaneous use of depth-separable convolution can also reduce the number of corresponding model parameters and computation. And then attention mechanism is integrated into the model and SE Block was used to assign different weights to feature channels to improve the accuracy of the model. Finally, knowledge distillation scheme is designed in part of the encoder-decoder structure, the corresponding loss function is proposed, and the method of knowledge distillation is used to improve the feature learning ability of the lightweight neural network. Compared with the original deep image matting model, the number of parameters in the new model is greatly reduced without too much loss of accuracy.},
  archive      = {J_IETIP},
  author       = {Xujia Qin and Guang Yang and Qin Shao and Hongbo Zheng and Meiyu Zhang},
  doi          = {10.1049/ipr2.12829},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2829-2837},
  shortjournal = {IET Image Process.},
  title        = {Lightweight image matting algorithm based on deep learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based panoptic segmentation: Recent advances
and perspectives. <em>IETIP</em>, <em>17</em>(10), 2807–2828. (<a
href="https://doi.org/10.1049/ipr2.12853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, panoptic segmentation has drawn increasing amounts of attention, leading to the rapid emergence of numerous related algorithms. A variety of deep neural networks have been used more frequently for panoptic segmentation, which is motivated by the significant success of deep learning methods in other tasks. This article presents a comprehensive exploration of panoptic segmentation, focusing on the analysis and understanding of RGB image data. Initially, the authors introduce the background of panoptic segmentation, including deep learning models and image segmentation. Then, the authors thoroughly cover a variety of panoptic segmentation-related topics, such as datasets connected to the field, evaluation metrics, panoptic segmentation models, and derived subfields based on panoptic segmentation. Finally, the authors examine the difficulties and possibilities in this area and identify its future paths.},
  archive      = {J_IETIP},
  author       = {Yuelong Chuang and Shiqing Zhang and Xiaoming Zhao},
  doi          = {10.1049/ipr2.12853},
  journal      = {IET Image Processing},
  month        = {8},
  number       = {10},
  pages        = {2807-2828},
  shortjournal = {IET Image Process.},
  title        = {Deep learning-based panoptic segmentation: Recent advances and perspectives},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Withdrawn: A computerized doughty predictor framework for
corona virus disease: Combined deep learning based approach.
<em>IETIP</em>, <em>17</em>(9), 2806. (<a
href="https://doi.org/10.1049/ipr2.12554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The above article, published online on 5 December 2022, on Wiley Online Library ( https://onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12554 ), has been withdrawn by agreement between the authors, the journal Editor in Chief, Farzin Deravi, and John Wiley &amp; Sons Ltd. The withdrawal has been agreed due to a technical error at the publisher that caused the article to be mistakenly published online although its publication had been cancelled at the request of the authors.},
  archive      = {J_IETIP},
  author       = {P. Ramya and S.V. Babu},
  doi          = {10.1049/ipr2.12554},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2806},
  shortjournal = {IET Image Process.},
  title        = {Withdrawn: a computerized doughty predictor framework for corona virus disease: combined deep learning based approach},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Latent low-rank representation sparse regression model with
symmetric constraint for unsupervised feature selection. <em>IETIP</em>,
<em>17</em>(9), 2791–2805. (<a
href="https://doi.org/10.1049/ipr2.12828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection is a dimensionality reduction method and has been widely used as an important and indispensable preprocessing step in many tasks. However, real-world data are not only high-dimensional, but also have intrinsic correlations between data points, which are not fully utilized in feature selection. Furthermore, real-world data usually inevitably contain noise or outliers. In order to select features from data more effectively, a sparse regression model based on latent low-rank representation with the symmetric constraint for unsupervised feature selection is proposed. With the coefficient matrix of non-negative symmetric low-rank representation, an affinity matrix characterized by the correlation relationship between data points is adaptively obtained, which reveals the intrinsic geometric relationship, global structure, and discrimination of data points. A latent representation of all data points obtained from this affinity matrix is employed as a pseudo-label, and feature selection is carried out by sparse linear regression. This method performs feature selection in the learned latent space instead of the original data space. An alternating iteration algorithm is designed to solve the proposed model, and its effectiveness and efficiency are verified on several benchmark data sets.},
  archive      = {J_IETIP},
  author       = {Lingli Guo and Xiuhong Chen},
  doi          = {10.1049/ipr2.12828},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2791-2805},
  shortjournal = {IET Image Process.},
  title        = {Latent low-rank representation sparse regression model with symmetric constraint for unsupervised feature selection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TLWSR: Weakly supervised real-world scene text image
super-resolution using text label. <em>IETIP</em>, <em>17</em>(9),
2780–2790. (<a href="https://doi.org/10.1049/ipr2.12827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text image super-resolution (STISR) has recently received considerable attention. Existing STISR methods are applicable to the situation that all the LR-HR pairs are available. However, in real-world scenarios, it is difficult and expensive to collect ground-truth HR labels and align them with LR images, and thus it is essential to find a way to implement weakly supervised learning. We investigate the STISR problem in the situation that only a subset of HR labels is available and design a weak supervision framework using coarse-grained text labels named TLWSR, which combines incomplete supervision and inexact supervision. Specifically, a lightweight text recognition network and connectionist temporal classification loss are used to guide the super-resolution of text images during training. Extensive experiments on the benchmark TextZoom demonstrate that TLWSR generates distinguishable text images and exceeds the fully supervised baseline TSRN in boosting text recognition accuracywith only 50% HR labels available. Meanwhile, TLWSR can be applied to different super-resolution backbones and significantly improves their performance. Furthermore, TLWSR shows good generalization capability to low-quality images on scene text recognition benchmarks, which verifies the effectiveness of this framework. To the authors&#39; knowledge, this is the first work exploring the problem of STISR in weakly supervised scenarios.},
  archive      = {J_IETIP},
  author       = {Qin Shi and Yu Zhu and Chuantao Fang and Dawei Yang},
  doi          = {10.1049/ipr2.12827},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2780-2790},
  shortjournal = {IET Image Process.},
  title        = {TLWSR: Weakly supervised real-world scene text image super-resolution using text label},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time face perception based encoding strategy
optimization method for UHD videos. <em>IETIP</em>, <em>17</em>(9),
2764–2779. (<a href="https://doi.org/10.1049/ipr2.12826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face regions containing rich semantic information appear frequently in the videos. As the video resolution increase dramatically, the face regions will inevitably attract more attentions. This paper proposes a face perception based coding scheme to improve the visual quality of the face regions in UHD videos. A specially tailored face perception model is first utilized to precisely and quickly locate the face regions. Then, a face perception map is generated based on a hierarchical mapping algorithm. Finally, the face perception map is employed as a guidance to optimize the encoding process, including mode decision, block partition and bit allocation. The proposed method is implemented on HEVC to demonstrate the effectiveness. Experimental results on a set of 4K test sequences show that the proposed method can obviously improve the objective and subjective quality of the face regions, while causing only slight quality decline over the rest of the frame. Additionally, the computation required for mode decision and block partition is reduced, thereby saving encoding time cost.},
  archive      = {J_IETIP},
  author       = {Jiang Bi and Lidong Wang and Yu Han and Cheng Zhou},
  doi          = {10.1049/ipr2.12826},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2764-2779},
  shortjournal = {IET Image Process.},
  title        = {Real-time face perception based encoding strategy optimization method for UHD videos},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning global image representation with generalized-mean
pooling and smoothed average precision for large-scale CBIR.
<em>IETIP</em>, <em>17</em>(9), 2748–2763. (<a
href="https://doi.org/10.1049/ipr2.12825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-based image retrieval (CBIR) is the problem of searching for items in an image database that are similar to the query image. Most of the existing image retrieval methods are trained based on metric learning loss functions (e.g. contrastive loss or triplet loss), however, which require the use of hard sample mining strategies (HMS) to better train the model. The HMS implies that picking out hard positive or negative samples increases the complexity of model training and requires a large amount of additional training time. To address this issue, lessons from recent work are leveraged on representation learning and a model called GS is proposed that combines the state-of-the-art Generalized-Mean (GeM) pooling and the smoothed average precision (AP). The entire network can be learned end-to-end by approximating the non-differentiable AP function to a differentiable one-without mining hard samples, only image-level annotations. A model named GSA is also presented which achieves excellent retrieval performance jointly trained by two various loss functions. Experimental results validate the effectiveness of the proposed approach and demonstrate the competitive performance on a common standard image retrieval dataset (Revisited Oxford and Paris).},
  archive      = {J_IETIP},
  author       = {Jinliang Yao and Yongqing Li and Bing Yang and Chenrui Wang},
  doi          = {10.1049/ipr2.12825},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2748-2763},
  shortjournal = {IET Image Process.},
  title        = {Learning global image representation with generalized-mean pooling and smoothed average precision for large-scale CBIR},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GIGAN: Self-supervised GAN for generating the invisible
using cycle transformation and conditional normalization.
<em>IETIP</em>, <em>17</em>(9), 2736–2747. (<a
href="https://doi.org/10.1049/ipr2.12824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objects in a real scene often occlude each other and inferring a complete appearance from the visible part is an important and challenging task. In this paper, the authors propose a self-supervised generative adversarial network GIGAN (GAN for generating the invisible), which can generate the complete appearance of objects without labelled invisible part information. The authors build two cycle transformation networks CycleIncomplete (CycleI) and CycleComplete (CycleC) that share parameters to improve the accuracy of mask completion. This design does not require well-matched training images and can make better use of the limited labelled samples. In addition, the authors propose a conditional normalization module and combine it with the inferred complete mask output. The combination not only enhances the content recovery ability and obtains more realistic outputs, but also improves the efficiency of the generation process. Experimental results show that compared with existing self-supervised learning models, our method achieves l 1 error, mean intersection-over-union (mIOU), and Fréchet inception distance (FID) improvements on the COCOA and KINS datasets.},
  archive      = {J_IETIP},
  author       = {Fengnan Quan and Bo Lang},
  doi          = {10.1049/ipr2.12824},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2736-2747},
  shortjournal = {IET Image Process.},
  title        = {GIGAN: Self-supervised GAN for generating the invisible using cycle transformation and conditional normalization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Multi-scale feature fusion pyramid attention network for
single image dehazing. <em>IETIP</em>, <em>17</em>(9), 2726–2735. (<a
href="https://doi.org/10.1049/ipr2.12823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture and color distortion are common in existing learning-based dehazing algorithms, and it is argued that one of the major reasons is that the shallow features of fog images are underutilized, and the deep features of fog images are insufficient for single image dehazing. In order to provide more texture and color information for image restoration, more shallow features need to be added in the process of image decoding. Therefore, a multi-scale feature fusion pyramid attention network (PAN) for single image dehazing is proposed. In PAN, combined with the attention mechanism, a shallow and deep feature fusion (SDF) strategy is designed. SDF considers multi-scale as well as channel-level fusion to provide feature information under different receptive fields while also highlighting important channels, such as texture and color information. DC is designed as a latent space mapping module to learn a mapping relationship between the latent space representation of the hazy image at low resolution and the corresponding latent space representation of the haze-free image. Additionally, network deconvolution (ND) and deformed convolution network (DCN) are introduced into PAN. The ND module can remove pixel-wise and channel-wise correlation of features, reduce data redundancy to obtain sparse representation of features, and speed up network convergence. The DCN module can use its adaptive receptive field to focus on the area of interest for calculation and play a role in texture feature enhancement. Finally, the perceptual loss is chosen as the regularization item of the loss function, which makes style features of the restored image closer to the real fog-free image. Extensive experiments reveal that the proposed PAN outperforms other existing dehazing methods on real-world and synthetic datasets.},
  archive      = {J_IETIP},
  author       = {Jianlei Liu and Peng Liu and Yuanke Zhang},
  doi          = {10.1049/ipr2.12823},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2726-2735},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale feature fusion pyramid attention network for single image dehazing},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D craniofacial reconstruction framework using elastic
surface deformation based on automatic landmark positioning.
<em>IETIP</em>, <em>17</em>(9), 2710–2725. (<a
href="https://doi.org/10.1049/ipr2.12822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided craniofacial reconstruction (CFR) is a process that aims to estimate facial impressions based on skull remains. It mimics the conventional method with a conceptual model-based framework. The existing problems in CFR are that landmark annotation is expert-dependent, landmark processing in the 3D domain has volumetric challenges, and a method based on a population&#39;s morphological characteristics (templates). A framework with three stages is proposed: Building a craniofacial model, automatic landmark detection, and surface deformation. Machine learning is deployed to draw local surface correlations as landmarks and automatically detects their position. The local surface context is extracted using the Surface Curvature Feature (SCF) as a 3D descriptor. Using a cluster-based filter, the average distance (to the ground truth) of the top 20 points is 0.0326 units. Cluster-based filters are better than mass-radius-based filters and consistently give better pinpoint accuracy, especially in multi-cluster cases. Training data consists of 140,000 SCF for ten landmark classes. The third stage, surface deformation, fits the facial template to the cranial based on the corresponding facial-cranial landmarks. Five experts from the Anthropology department stated that of the reconstruction results, 91.5% could retain the template details and are accepted as the natural shape of the human face.},
  archive      = {J_IETIP},
  author       = {Putu Hendra Suputra and Anggraini Dwi Sensusiati and Myrtati Dyah Artaria and Eko Mulyanto Yuniarno and I Ketut Eddy Purnama},
  doi          = {10.1049/ipr2.12822},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2710-2725},
  shortjournal = {IET Image Process.},
  title        = {3D craniofacial reconstruction framework using elastic surface deformation based on automatic landmark positioning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust registration of aerial and close-range
photogrammetric point clouds using visual context features and scale
consistency. <em>IETIP</em>, <em>17</em>(9), 2698–2709. (<a
href="https://doi.org/10.1049/ipr2.12821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration is of great significance to the reconstruction of high-precision 3D city models. There are some challenges when aligning aerial and close-range photogrammetric point clouds, such as huge view differences caused by the different sights of the sensors, massive noisy points due to the error in dense matching, and scale uncertainty since there are no control points for absolute orientation. To achieve complementary advantages of aerial and close-range point clouds, in this paper, a robust cross-source point clouds registration method is proposed using image visual context features and scale consistency. First, a cross-view image matching method based on image visual context features is proposed to obtain corresponding points. Second, to overcome the challenges of noisy points and scale differences, an outlier filtering method is designed based on scale consistency. Finally, the dual quaternions model considering the scale factor is introduced to solve the spatial transformation model. To analyze the feasibility of this method qualitatively and quantitatively, experiments are conducted using the public scene dataset of Dortmund, Germany, and the scene dataset of Zhengzhou City, China. Three kinds of cross-source point clouds registration experiments are conducted in this paper, including aerial and close-range point clouds registration in Dortmund, and aerial and ground point clouds registrations in both Dortmund and Zhengzhou. The chamfer distances of the three sets of experiments are 4.48 m, 5.97 m and 4.78 m, respectively. The ablation study shows that the outlier filtering method and the dual quaternions model improve the accuracy by at least 14% and 25%, respectively. The experiments demonstrate that the method accomplishes the cross-source point clouds registration in large-scale scenes accurately and efficiently, providing a solid foundation for subsequent fine 3D reconstruction.},
  archive      = {J_IETIP},
  author       = {Guanghan Chu and Dazhao Fan and Yang Dong and Song Ji and Linyu Gu and Dongzi Li and Wu Zhang},
  doi          = {10.1049/ipr2.12821},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2698-2709},
  shortjournal = {IET Image Process.},
  title        = {Robust registration of aerial and close-range photogrammetric point clouds using visual context features and scale consistency},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A coordinate attention enhanced swin transformer for
handwriting recognition of parkinson’s disease. <em>IETIP</em>,
<em>17</em>(9), 2686–2697. (<a
href="https://doi.org/10.1049/ipr2.12820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosing Parkinson&#39;s disease (PD) in its early stages is a significant challenge in medicine. Hand tremors and dysgraphia, which are typical early motor symptoms of PD, can manifest for decades before a formal diagnosis is made. Therefore, handwriting analysis has become an important tool for detecting PD. While many machine learning algorithms have been applied in this area, they struggle to capture the subtle changes in handwriting and must describe features from various perspectives. To address these issues, this paper proposes a Coordinate Attention Enhanced Swin Transformer (CAS Transformer) model for PD handwriting recognition. It establishes the long-term dependence of features on the joint coordinate attention application, which enables the model to more accurately localize the important features of handwriting data and also extract the fuzzy edge features of handwriting images.These characteristics of the CAS Transformer enable it to outperform current advanced deep learning methods in classification, with an accuracy of 92.68% in experiments conducted on two handwritten datasets.},
  archive      = {J_IETIP},
  author       = {Nana Wang and Xuesen Niu and Yiyang Yuan and Yunze Sun and Ran Li and Guoliang You and Aite Zhao},
  doi          = {10.1049/ipr2.12820},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2686-2697},
  shortjournal = {IET Image Process.},
  title        = {A coordinate attention enhanced swin transformer for handwriting recognition of parkinson&#39;s disease},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense video captioning based on local attention.
<em>IETIP</em>, <em>17</em>(9), 2673–2685. (<a
href="https://doi.org/10.1049/ipr2.12819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense video captioning aims to locate multiple events in an untrimmed video and generate captions for each event. Previous methods experienced difficulties in establishing the multimodal feature relationship between frames and captions, resulting in low accuracy of the generated captions. To address this problem, a novel Dense Video Captioning Model Based on Local Attention (DVCL) is proposed. DVCL employs a 2D temporal differential CNN to extract video features, followed by feature encoding using a deformable transformer that establishes the global feature dependence of the input sequence. Then DIoU and TIoU are incorporated into the event proposal match algorithm and evaluation algorithm during training, to yield more accurate event proposals and hence increase the quality of the captions. Furthermore, an LSTM based on local attention is designed to generate captions, enabling each word in the captions to correspond to the relevant frame. Extensive experimental results demonstrate the effectiveness of DVCL. On the ActivityNet Captions dataset, DVCL performs significantly better than other baselines, with improvements of 5.6%, 8.2%, and 15.8% over the best baseline in BLEU4, METEOR, and CIDEr, respectively.},
  archive      = {J_IETIP},
  author       = {Yong Qian and Yingchi Mao and Zhihao Chen and Chang Li and Olano Teah Bloh and Qian Huang},
  doi          = {10.1049/ipr2.12819},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2673-2685},
  shortjournal = {IET Image Process.},
  title        = {Dense video captioning based on local attention},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DLSANet: Facial expression recognition with double-code
LBP-layer spatial-attention network. <em>IETIP</em>, <em>17</em>(9),
2659–2672. (<a href="https://doi.org/10.1049/ipr2.12817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is widely used in many fields. To further improve the accuracy of FER, this paper proposes a method based on double-code LBP-layer spatial-attention network (DLSANet). The backbone model for the DLSANet is an emotion network (ENet), which is modified with a double-code LBP (DLBP) layer and a spatial attention module. The DLBP layer is at the front of the first convolutional layer. More valuable features can be extracted by inputting the image processed by DLBP into convolutional layers. The JAFFE and CK+ datasets are used, which contain seven expressions: happiness, anger, disgust, neutral, fear, sadness, and surprise. The average of fivefold cross-validation shows that DLSANet achieves a recognition accuracy of 93.81% and 98.68% on the JAFFE and CK+ datasets. The experiment reveals that the DLSANet can produce better classification results than state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Xing Guo and Siyuan Lu and Shuihua Wang and Zhihai Lu and Yudong Zhang},
  doi          = {10.1049/ipr2.12817},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2659-2672},
  shortjournal = {IET Image Process.},
  title        = {DLSANet: Facial expression recognition with double-code LBP-layer spatial-attention network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight attention-guided redundancy-reuse network for
real-time semantic segmentation. <em>IETIP</em>, <em>17</em>(9),
2649–2658. (<a href="https://doi.org/10.1049/ipr2.12816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a critical topic in computer vision, and it has numerous practical applications, including mobile devices, autonomous driving, and many other fields. However, in these application scenarios, it is often essential for the segmentation models to achieve a balance between efficiency and performance. A lightweight attention-guided redundancy-reuse network (LARNet) was proposed to address this challenge in this paper. Specifically, the multi-scale asymmetric redundancy reuse (MAR) module was designed as the main component of the encoder for dense encoding of contextual semantic features. Furthermore, the efficient attention fusion (EAF) module was established for multi-scale information fusion via the channel and spatial attention mechanisms in the decoder. A series of experiments were conducted to verify the proposed network. The results of tests on multiple datasets suggest that the network has higher accuracy and faster speed than the existing real-time semantic segmentation methods.},
  archive      = {J_IETIP},
  author       = {Xuegang Hu and Shuhan Xu and Liyuan Jing},
  doi          = {10.1049/ipr2.12816},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2649-2658},
  shortjournal = {IET Image Process.},
  title        = {Lightweight attention-guided redundancy-reuse network for real-time semantic segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object detection network based on dense dilated encoder net.
<em>IETIP</em>, <em>17</em>(9), 2640–2648. (<a
href="https://doi.org/10.1049/ipr2.12811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the authors apply the feature pyramid network (FPN) to the single-stage anchor-free object detection algorithm CenterNet, and the effectiveness of the multi-level feature fusion of FPN for the object detection algorithm is proved by experiments. However, multi-level feature fusion leads to an increase in computational cost. In this regard, this paper proposes an object detection algorithm, called DDE-Net, that does not use multi-level feature fusion and only uses single-level feature for optimization. The key component in it: the dense dilated encoder, which encourages dense information exchange of features between different spatial scales. This paper presents extensive experiments, and DDE-Net shows strong performance compared to that of other popular models on the PASCAL VOC and on the COCO2017 dataset. On the COCO2017 dataset, the authors’ DDE-Net achieves comparable results with its feature pyramids counterpart RetinaNet, while applying the same backbone with smaller params and GFLOPs than RetinaNet. With an image size of 512 × 512, DDE-Net achieves 37.3 AP running at 81 fps on 2080 Ti.},
  archive      = {J_IETIP},
  author       = {Shaohua Liu and Ao Yang and Chundong She and Kang Du},
  doi          = {10.1049/ipr2.12811},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2640-2648},
  shortjournal = {IET Image Process.},
  title        = {Object detection network based on dense dilated encoder net},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Research on identification and classification of grassland
forage based on deep learning and attention mechanisms. <em>IETIP</em>,
<em>17</em>(9), 2628–2639. (<a
href="https://doi.org/10.1049/ipr2.12808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grassland is an important resource for China&#39;s economic development and the main economic source of animal husbandry. The identification and classification of grassland forage is an important part of the improvement of forage varieties and the monitoring of germplasm resources, which can fundamentally solve the problems of poor forage quality and low reproduction rate. For the problem of low accuracy of forage identification and classification, the authors put forward a new 3DSECNN model to remove the preprocessing operation and directly study the images. The authors took forage hyperspectral image (HSI) images on the field and built dataset, used 3DSECNN to train the images to improve the classification effect. The outstanding contributions of this paper are: (1) The authors took high-precision forage HSI images in the field, established a dedicated database of forage HSIs, and expanded the datasets; (2) the process of integrating preprocessing ideas into the network and replacing the traditional method of preprocessing the data and then extracting features; (3) proposing the 3DSECNN model, which adds SENet on the basis of the traditional 3DCNN, strengthens the correlation of the spatial dimension, selects the key features for the classification by calculating the channel weight, inhibits the unimportant information, and achieves the purpose of integrating the preprocessing ideas into the network. The experimental results show the overall accuracy (OA) of 3DSECNN is 94.36%, Precision, Recall, F1-score, Kappa, and Time also showed good levels. The experimental results prove that the 3DSECNN strengthens the correlation between image channels, enhances the performance ability of features, and provides a new method for the identification and classification research of forage.},
  archive      = {J_IETIP},
  author       = {Yilei Liu and Jiangping Liu and Xuanhe Zhao and Xin Pan and Weihong Yan},
  doi          = {10.1049/ipr2.12808},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2628-2639},
  shortjournal = {IET Image Process.},
  title        = {Research on identification and classification of grassland forage based on deep learning and attention mechanisms},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Malignant melanoma dermoscopy image classification method
based on multi-modal medical features. <em>IETIP</em>, <em>17</em>(9),
2611–2627. (<a href="https://doi.org/10.1049/ipr2.12801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin cancer is one of the deadliest cancers, and it has been widely developed worldwide since the last decade. Malignant melanoma is currently the most deadly skin cancer. If malignant melanoma is diagnosed at an early stage, the probability of patients being cured will be greatly improved. At present, most existing skin lesion image classification methods only use deep learning. However, the multi-modal features of skin lesions in the medical domain are not well utilized and integrated. To reduce the classification error of the skin lesion images caused by the complexity and subjectivity of visual interpretation, a malignant melanoma dermoscopy image classification method based on multi-modal medical features is proposed in this paper which is inspired by the fuzzy decision-making process of doctors. It can reduce the subjective difference in the image classification process and assist dermatologists to analyze the skin lesion area. Firstly, the feature detection method based on the extension theory can effectively quantify the difference between different colour features. Then, an interpretable segmentation edge of the skin lesion is established by using the neutrosophic theory which can convert the image into the neutrosophic space. The edge of the skin lesion is captured by applying the Hierarchical Gaussian Mixture Model (HGMM) method. Next, the edge sequence is established by segmenting the edge, and the contour regularity, symmetry, and uniformity of the edge of the skin lesion are analyzed. Finally, the extracted multi-feature sets are used for dermoscopy image classification. Experiments are carried out on real datasets, and the classification accuracy of four kernel functions is verified. The experimental results show that the authors’ method can effectively improve the classification accuracy of benign dermoscopy images and malignant dermoscopy images.},
  archive      = {J_IETIP},
  author       = {Xiaofei Bian and Haiwei Pan and Kejia Zhang and Peng Liu and Chunling Chen},
  doi          = {10.1049/ipr2.12801},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2611-2627},
  shortjournal = {IET Image Process.},
  title        = {Malignant melanoma dermoscopy image classification method based on multi-modal medical features},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DCIFPN: Deformable cross-scale interaction feature pyramid
network for object detection. <em>IETIP</em>, <em>17</em>(9), 2596–2610.
(<a href="https://doi.org/10.1049/ipr2.12800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting multi-scale features is one of the most effective methods to recognize objects of different scales in object detection. Since image pyramid is time-consuming, Feature Pyramid Network (FPN) becomes the most popular component used for obtaining pyramidal features. Despite its effectiveness, there still exist some intrinsic defects. In this work, it is attributed to insufficient information flow and a Deformable Cross-scale Interaction Feature Pyramid Network (DCIFPN) is proposed, which aims to promote the information transfer process with content-aware sampling and dynamic aggregation weights. More specifically, Deformable Semantic Enhancement Module (DSEM) is designed that can construct accurate information flow with dynamic aggregation weights. In addition, Deformable Spatial Refinement Module (DSRM) is proposed to enhance high-level features with low-level location details. When DCIFPN is deployed on RetinaNet and FCOS with ResNet-50, the performance is improved by 1.6 AP and 1.1 AP, respectively, on the challenging MS COCO benchmark. Apart from one-stage detectors, DCIFPN is also applicable to two-stage methods such as Faster R-CNN and Mask R-CNN. Further experiments on Pascal VOC and CrowdHuman datasets can verify the effectiveness and generalization of the method.},
  archive      = {J_IETIP},
  author       = {Junrui Xiao and He Jiang and Zhikai Li and Qingyi Gu},
  doi          = {10.1049/ipr2.12800},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2596-2610},
  shortjournal = {IET Image Process.},
  title        = {DCIFPN: Deformable cross-scale interaction feature pyramid network for object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight network of near cotton-coloured impurity
detection method in raw cotton based on weighted feature fusion.
<em>IETIP</em>, <em>17</em>(9), 2585–2595. (<a
href="https://doi.org/10.1049/ipr2.12788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective is to improve the recognition rate of white and near cotton-coloured impurities in raw cotton against a single white visible light source background. A lightweight detection network model without anchor boxes based on improved YOLO v4-tiny is proposed in this paper based on weighted feature fusion (WFF). The WFF strategy was used to improve the detection accuracy of the improved YOLO v4-tiny algorithm. Meanwhile, to address the disadvantage that the anchor boxes obtained by the K-means algorithm clustering do not have global features, the anchor-free and tiny decoupled head schemes are used instead of the traditional coupled detection head. The improved algorithm was validated on the PASCAL VOC2012 dataset and the self-built raw cotton impurity dataset collected by high-speed camera. On the raw cotton impurity test set, compared to the original YOLO v4-tiny model, the mean average precision (mAP) and frames per second (FPS) of the improved model increased by 10.35% and 6.9%, respectively. The proposed model detects white and near cotton-coloured impurities with an accuracy of up to 98.78% and 98.00%. The experimental results show that the proposed method can effectively detect and identify impurities in raw cotton, which is of great practical significance for foreign matter detection and identification in cotton.},
  archive      = {J_IETIP},
  author       = {Tao Xu and Aisong Ma and Huan Lv and Yang Dai and Shoujin Lin and Haihui Tan},
  doi          = {10.1049/ipr2.12788},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2585-2595},
  shortjournal = {IET Image Process.},
  title        = {A lightweight network of near cotton-coloured impurity detection method in raw cotton based on weighted feature fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A CNN with noise inclined module and denoise framework for
hyperspectral image classification. <em>IETIP</em>, <em>17</em>(9),
2575–2584. (<a href="https://doi.org/10.1049/ipr2.12733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks have been successfully applied in hyperspectral image classification. However, most of prior works adopt general deep architectures while ignore the intrinsic structure of the hyperspectral image, such as the physical noise generation. This would make these deep models unable to generate discriminative features and provide impressive classification performance. To leverage such intrinsic information, this work develops a novel deep learning framework with the noise inclined module and denoise framework for hyperspectral image classification. First, the spectral signature of hyperspectral image is modeled with the physical noise model to describe the high intra-class variance of each class and great overlapping between different classes in the image. Then, a noise inclined module is developed to capture the physical noise within each object and a denoise framework is then followed to remove such noise from the object. Finally, the CNN with noise inclined module and the denoise framework is developed to obtain discriminative features and provides good classification performance of hyperspectral image. Experiments are conducted over two commonly used real-world datasets and the experimental results show the effectiveness of the proposed method. The implementation of the proposed method and other compared methods could be accessed at https://github.com/shendu-sw/noise-physical-framework .},
  archive      = {J_IETIP},
  author       = {Zhiqiang Gong and Ping Zhong and Wen Yao and Weien Zhou and Jiahao Qi and Panhe Hu},
  doi          = {10.1049/ipr2.12733},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2575-2584},
  shortjournal = {IET Image Process.},
  title        = {A CNN with noise inclined module and denoise framework for hyperspectral image classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthetic aperture radar image despeckling using
convolutional neural networks in wavelet domain. <em>IETIP</em>,
<em>17</em>(9), 2561–2574. (<a
href="https://doi.org/10.1049/ipr2.12730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is difficult for a convolutional neural network (CNN) to capture the detailed features of synthetic aperture radar (SAR) images when increasing the network depth. To capture sufficient information for reconstructing image details, the authors propose a multidirectional and multiscale convolutional neural network (MMCNN) in which the wavelet subband is input into each independent subnetwork to be trained. Each subnetwork has few convolution layers and a loss function. When the loss function reaches its optimal value, all subbands are integrated to produce the despeckled SAR image through the inverse Wavelet transform. The proposed MMCNN consisting of multiple subnetworks extracts the detailed features and suppresses speckle noise from different directions and scales; thus, its performance is improved by broadening the network width rather than increasing the depth. Experimental results on synthetic and real SAR images show that the proposed method shows superior performance over the state-of-the-art methods in terms of both quantitative assessments and subjective visual quality, especially for strong speckle noise.},
  archive      = {J_IETIP},
  author       = {Jing Liu and Runchuan Liu},
  doi          = {10.1049/ipr2.12730},
  journal      = {IET Image Processing},
  month        = {7},
  number       = {9},
  pages        = {2561-2574},
  shortjournal = {IET Image Process.},
  title        = {Synthetic aperture radar image despeckling using convolutional neural networks in wavelet domain},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UHA-CycleGAN: Unpaired hybrid attention network based on
CycleGAN for terahertz image super-resolution. <em>IETIP</em>,
<em>17</em>(8), 2547–2559. (<a
href="https://doi.org/10.1049/ipr2.12804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, terahertz imaging technology has been widely used in security, medicine, and other fields. However, the image resolution is low due to the limits of imaging equipment and diffraction. Traditional super-resolution methods based on machine learning use artificial paired image datasets, and their image degradation process is quite different from the actual terahertz imaging mechanism. In the paper, an unpaired hybrid attention network is proposed, using the real unpaired high-resolution and low-resolution terahertz images as the original input, two sub-networks are trained: degenerate reconstruction network and super-resolution network. The degenerate reconstruction network is used to learn the degradation process of the real terahertz imaging system, and reconstruct the generated degradation images into the fixed mapping down-sampled image of high-resolution terahertz image. The super-resolution network uses the generated paired image with fixed correspondence to train the super-resolution network of paired terahertz images, to improve the imaging resolution of the network in the super-resolution work of real low-resolution terahertz images. In addition, the no-reference image quality evaluation system is introduced to objectively evaluate the network performance. The experimental results show that the UHA-CycleGAN network outperforms the traditional paired super-resolution network on both open-source and self-built datasets.},
  archive      = {J_IETIP},
  author       = {Huanyu Liu and Haipeng Guo and Xiaodong Liu},
  doi          = {10.1049/ipr2.12804},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2547-2559},
  shortjournal = {IET Image Process.},
  title        = {UHA-CycleGAN: Unpaired hybrid attention network based on CycleGAN for terahertz image super-resolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel head network and group normalisation help track more
accurately. <em>IETIP</em>, <em>17</em>(8), 2537–2546. (<a
href="https://doi.org/10.1049/ipr2.12815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing Siamese Network trackers rely on a predefined anchor box to predict object position. However, they require complicated hyperparameter settings. The authors directly forecast the object boundary by using the fully convolutional network as the head of the tracking network to solve this issue and simplify the application. The end-to-end design avoids the setting of hyperparameters and candidate boxes. The authors also discovered that the validation loss decreased less than the training loss throughout Siamese training. The authors changed the normalisation layer from batch normalisation to group normalisation to solve this issue. It solved the problem that the loss function is difficult to decrease and increased training efficiency. Test experiments on the tracking dataset, including VOT2019 and GOT10k, show that the authors’ network outperforms DaSiamRPN and SiamFC regarding precision under the same network size and runs at 24 FPS on an AMD 4800 CPU. It also runs at 306 FPS on a 3090 GPU.},
  archive      = {J_IETIP},
  author       = {Jian Wang and Xinyi Tang and Yifan Hao and Dongjie Wu and Xiangzhou Ye and Zheng Li},
  doi          = {10.1049/ipr2.12815},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2537-2546},
  shortjournal = {IET Image Process.},
  title        = {A novel head network and group normalisation help track more accurately},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hybrid domain digital watermarking scheme based on improved
differential evolution algorithm and singular value block embedding.
<em>IETIP</em>, <em>17</em>(8), 2516–2536. (<a
href="https://doi.org/10.1049/ipr2.12814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The internet and related technologies have promoted the development of medical and health fields, especially remote diagnosis. As medical images may be stolen in the transmission process, the patient&#39;s personal information could be leaked. Aiming at the problem of privacy disclosure of colour medical images, a hybrid domain watermarking scheme based on improved differential evolution (DE) and the singular value block embedding (SVBE) is proposed in this study. Through this scheme, the image containing the patient&#39;s information is hidden in the patient&#39;s medical carrier image as a watermark. First, redistributed invariant lifting wavelet transform (RILWT), discrete wavelet transform (DWT), and singular value decomposition (SVD) are used to process the colour medical carrier image, and the high-frequency information in the integer wavelet domain of the watermark is embedded into the processed carrier image using the proposed singular value block embedding method based on the improved differential evolution algorithm. At the same time, to increase the security of the watermark scheme, a digital signature based on the SHA-384 hash function is designed for verification before watermark extraction. Compared with recent related research, the scheme has strong invisibility and robustness.},
  archive      = {J_IETIP},
  author       = {Dawei Liu and Donglin Liu and Bin Wang and Pan Zheng},
  doi          = {10.1049/ipr2.12814},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2516-2536},
  shortjournal = {IET Image Process.},
  title        = {Hybrid domain digital watermarking scheme based on improved differential evolution algorithm and singular value block embedding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel matters: Efficient polyp segmentation with parallel
structured feature augmentation modules. <em>IETIP</em>, <em>17</em>(8),
2503–2515. (<a href="https://doi.org/10.1049/ipr2.12813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large variations of polyp sizes and shapes and the close resemblances of polyps to their surroundings call for features with long-range information in rich scales and strong discrimination. This article proposes two parallel structured modules for building those features. One is the Transformer Inception module (TI) which applies Transformers with different reception fields in parallel to input features and thus enriches them with more long-range information in more scales. The other is the Local-Detail Augmentation module (LDA) which applies the spatial and channel attentions in parallel to each block and thus locally augments the features from two complementary dimensions for more object details. Integrating TI and LDA, a new Transformer encoder based framework, Parallel-Enhanced Network (PENet), is proposed, where LDA is specifically adopted twice in a coarse-to-fine way for accurate prediction. PENet is efficient in segmenting polyps with different sizes and shapes without the interference from the background tissues. Experimental comparisons with state-of-the-arts methods show its merits.},
  archive      = {J_IETIP},
  author       = {Qingqing Guo and Xianyong Fang and Kaibing Wang and Yuqing Shi and Linbo Wang and Enming Zhang and Zhengyi Liu},
  doi          = {10.1049/ipr2.12813},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2503-2515},
  shortjournal = {IET Image Process.},
  title        = {Parallel matters: Efficient polyp segmentation with parallel structured feature augmentation modules},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust facial landmark detection by probability-guided
hourglass network. <em>IETIP</em>, <em>17</em>(8), 2489–2502. (<a
href="https://doi.org/10.1049/ipr2.12812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absence of local features and global shape constraints severely limits the performance of the hourglass network for facial landmark detection in unconstrained environments. Moreover, diverse feature types and scales may result in low accuracy. This paper proposes a probability-guided hourglass network to enhance the shape constraints for robust facial landmark detection. Firstly, a multi-scale pre-processing module is designed to extract features at different scales. Secondly, based on the heatmaps generated by the stacked hourglass network, the coarse localizations are obtained, while the probability maps are generated with local features. Finally, a probability-based boundary regression method is proposed and the hausdorff distance is modified as the loss function to constrain the feature shape. Adaptive weights are also added to the loss function, which can help relieve the data imbalance problem. Subjective and objective experimental results on the challenging datasets show that this method outperforms the state-of-the-arts on unconstrained conditions.},
  archive      = {J_IETIP},
  author       = {Jingyan Fan and Jiuzhen Liang and Hao Liu and Zhan Huan and Zhenjie Hou and Xinwen Zhou},
  doi          = {10.1049/ipr2.12812},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2489-2502},
  shortjournal = {IET Image Process.},
  title        = {Robust facial landmark detection by probability-guided hourglass network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Apron surveillance video coding based on compositing virtual
reference frame with object library. <em>IETIP</em>, <em>17</em>(8),
2475–2488. (<a href="https://doi.org/10.1049/ipr2.12810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A video coding framework for the apron surveillance scene has been proposed in this paper, which aims to improve coding efficiency by eliminating long-term redundancy at the object level. To achieve this goal, this study first develops an existing block-based hybrid video coding framework by exploiting the video redundancy on the object level to perform video coding. Second, an object-library mechanism is designed to collect the representative object images as coding references on larger temporal and spatial scales. Finally, a virtual reference frame, which blends background and foreground references from the object library, is adaptively composited according to the video content to improve the inter-prediction performance. Preliminary experimental results demonstrate that the proposed method achieves a high BD rate reduction of up to 23.97% in apron surveillance video sequences, compared to the standard high efficiency video coding (HEVC).},
  archive      = {J_IETIP},
  author       = {Zonglei Lyu and Bo Zhang},
  doi          = {10.1049/ipr2.12810},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2475-2488},
  shortjournal = {IET Image Process.},
  title        = {Apron surveillance video coding based on compositing virtual reference frame with object library},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel shape classification method using 1-d convolutional
neural networks. <em>IETIP</em>, <em>17</em>(8), 2467–2474. (<a
href="https://doi.org/10.1049/ipr2.12809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the shape classification methods are based on a single closed contour. However, practical shapes always have complex contours, for example, a combination of multiple open contours. How to accurately identify complex shapes is an unsolved problem. In this research, a novel method is proposed to classify complex shapes. The proposed method firstly encodes a complex shape to an angle code and a sparsity code, then input these codes to a 1-D CNN for extracting features and classification. Experiments on two datasets show this novel method is superior in terms of classification accuracy. These two datasets are practical shape dataset collected by this paper on internet and MPEG-7 CE-1 Part B. The proposed method achieves higher classification accuracy than compared methods. In order to show the performance of the proposed method on each class, the accuracy on each class is analyzed. Ablation experiment is conducted to show the contribution of each module in the network. The result shows that each module is meaningful in the network, because without any module the accuracy drops.},
  archive      = {J_IETIP},
  author       = {Xun Zhang and Jingxian Liu and Yalu Zheng and Yan Zheng and Masroor Hussain},
  doi          = {10.1049/ipr2.12809},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2467-2474},
  shortjournal = {IET Image Process.},
  title        = {A novel shape classification method using 1-D convolutional neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymmetric cost aggregation network for efficient stereo
matching. <em>IETIP</em>, <em>17</em>(8), 2450–2466. (<a
href="https://doi.org/10.1049/ipr2.12807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost aggregation is crucial to the accuracy of stereo matching. A reasonable cost aggregation algorithm should aggregate costs within homogeneous regions where pixels have the same or similar disparities. Otherwise, the estimated disparity map is prone to the well-known edge-fattening issue and the problem of losing fine structures. However, current state-of-the-art convolutional neural networks (CNNs) mainly do cost aggregation with square-kernel convolutional layers that learn to adjust their kernel elements to make the actual receptive fields of the aggregated costs adapt to homogeneous regions with various shapes. This is a mechanism that easily leads to the above issues due to the translation equivalence and content-insensitivity properties of CNNs. To tackle these problems, a novel densely connected asymmetric convolution block (Dense-ACB) based on asymmetric convolutions is proposed to explicitly construct receptive fields with various shapes, which effectively alleviates the issues caused by mismatching shapes of receptive fields and homogeneous regions. The proposed Dense-ACB brings new insight to CNN-based stereo matching networks. Based on the proposed cost aggregation method, an efficient and effective stereo matching network is built, which not only achieves competitive overall accuracy compared with state-of-the-art models but also effectively alleviates the edge-fattening problem and preserves fine structures.},
  archive      = {J_IETIP},
  author       = {Zhong Wu and Hong Zhu and Lili He and Dong Wang and Jing Shi and Wenhuan Wu},
  doi          = {10.1049/ipr2.12807},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2450-2466},
  shortjournal = {IET Image Process.},
  title        = {Asymmetric cost aggregation network for efficient stereo matching},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward cross-domain object detection in artwork images using
improved YoloV5 and XGBoosting. <em>IETIP</em>, <em>17</em>(8),
2437–2449. (<a href="https://doi.org/10.1049/ipr2.12806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition in natural images has achieved great success, while recognizing objects in style-images, such as artworks and watercolor images, has not yet achieved great progress. Here, this problem is addressed using cross-domain object detection in style-images, clipart, watercolor, and comic images. In particular, a cross-domain object detection model is proposed using YoloV5 and eXtreme Gradient Boosting (XGBoosting). As detecting difficult instances in cross domain images is a challenging task, XGBoosting is incorporated in this workflow to enhance learning of the proposed model for application on hard-to-detect samples. Several ablation studies are carried out by training and evaluating this model on the StyleObject7K, ClipArt1K, Watercolor2K, and Comic2K datasets. It is empirically established that this proposed model works better than other methods for the above-mentioned datasets.},
  archive      = {J_IETIP},
  author       = {Tasweer Ahmad and Maximilian Schich},
  doi          = {10.1049/ipr2.12806},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2437-2449},
  shortjournal = {IET Image Process.},
  title        = {Toward cross-domain object detection in artwork images using improved YoloV5 and XGBoosting},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MCNN: Conditional focus probability learning to multi-focus
image fusion via mutually coupled neural network. <em>IETIP</em>,
<em>17</em>(8), 2422–2436. (<a
href="https://doi.org/10.1049/ipr2.12805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel conditional focus probability learning model, termed MCNN, is proposed for multi-focus image fusion (MFIF). Given a pair of source images, their conditional focus probabilities can be generated by using the well-trained MCNN, which is further converted into the binary focus masks to directly produce an all-focus image with no postprocessing. To this end, a fully convolutional encoder is designed with two mutually coupled Siamese branches in MCNN, which include a coupling block that bridge between the two branches to provide conditional information to each other, at different layers, such that the encoder can more strongly extract conditional focus features and further encourage the decoder pixel-wisely to give more robust conditional focus probabilities. Moreover, a hybrid loss is designed with a structural sparse fidelity loss and a structural similarity loss to force the network to learn more accurate conditional focus probabilities. Particularly, a convolutional norm with good structural group sparse is proposed, to construct the structural sparse fidelity loss. Simulation results substantiate the superiority of our MCNN over other state-of-the-art, in terms of both visual perception and quantitative evaluation.},
  archive      = {J_IETIP},
  author       = {Chengchao Wang and Yuanyuan Pu and Xue Wang and Chaozhen Ma and Rencan Nie},
  doi          = {10.1049/ipr2.12805},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2422-2436},
  shortjournal = {IET Image Process.},
  title        = {MCNN: Conditional focus probability learning to multi-focus image fusion via mutually coupled neural network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved SwinTrack single target tracking algorithm based on
spatio-temporal feature fusion. <em>IETIP</em>, <em>17</em>(8),
2410–2421. (<a href="https://doi.org/10.1049/ipr2.12803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single target tracking based on computer vision helps to collect, analyse and exploit target information. The SwinTrack algorithm has received widespread attention as one of the twin network algorithms with the best trade-off between tracking accuracy and speed, but it also suffers from the insufficient fusion of deep and shallow features leading to loss of shallow information and insufficient use of temporal information leading to inconsistency between target and template. Semantic information and detailed information are combined and multiple convolutional forms are introduced to propose a multi-level feature fusion strategy to effectively fuse features in space. Besides, based on the idea of feedback, a dynamic template branching approach is also designed to fuse temporal features and enhance the representation of target features. The effectiveness of this method was verified on the OTB100 and GOT10K datasets.},
  archive      = {J_IETIP},
  author       = {Min Zhao and Qiang Yue and Dihua Sun and Yuan Zhong},
  doi          = {10.1049/ipr2.12803},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2410-2421},
  shortjournal = {IET Image Process.},
  title        = {Improved SwinTrack single target tracking algorithm based on spatio-temporal feature fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-based hierarchical pyramid feature fusion
structure for efficient face recognition. <em>IETIP</em>,
<em>17</em>(8), 2399–2409. (<a
href="https://doi.org/10.1049/ipr2.12802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNN) have become the main method for face recognition (FR). To deploy deep CNN models on embedded and mobile devices, several lightweight FR models have been proposed. However, multi-scale facial features are seldom considered in these approaches. To overcome this limitation, an attention-based hierarchical pyramid feature fusion (AHPF) structure was proposed in this paper. Specifically, hierarchical multi-scale features were directly extracted from the backbone based on its pyramidal hierarchy, and the bidirectional cross-scale connection was used to better combine the high-level global features with low-level local features. In addition, instead of simple concatenation or summation, an attention-based feature fusion mechanism was used to highlight the most recognizable facial patches, and to address the unequal contribution to the output during the fusing process. Based on the AHPF structure and efficient backbones, multiple sizes of lightweight FR models were presented, called HSFNet. After an extensive experimental evaluation involving 10 mainstream benchmarks, the proposed models consistently achieved state-of-the-art FR performance compared to other lightweight FR models with same level of model complexity. With only 0.659M parameters and 94.94M FLOPs, our HSFNet-05-M exhibited a performance competitive with recent top-ranked FR models containing up to 4M parameters and 500M FLOPs.},
  archive      = {J_IETIP},
  author       = {Yi Dai and Kai Sun and Wei Huang and Dawei Zhang and Gaojie Dai},
  doi          = {10.1049/ipr2.12802},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2399-2409},
  shortjournal = {IET Image Process.},
  title        = {Attention-based hierarchical pyramid feature fusion structure for efficient face recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel receptive field expansion network for small
object detection. <em>IETIP</em>, <em>17</em>(8), 2385–2398. (<a
href="https://doi.org/10.1049/ipr2.12799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small object detection remains a bottleneck because there is little visual information about them, especially in the deep layers. To improve the detection performance of small objects, here, Swin Transformer is introduced as the model backbone network to extract rich features of small objects. Then, a multilevel receptive field expansion network (MRFENet) is proposed based on the characteristics of different stages in the Swin Transformer. Specifically, a receptive field expansion block (RFEB) is designed to acquire contextual cues and extract detailed information. The RFEB is carefully designed to target the required receptive fields of different layers and further refine the features. MRFENet combined with RFEBs implements the retention of small object context cues and the acquisition of receptive fields for the adaptive detection tasks. Finally, a union loss function is designed to enhance the localization ability. Experiments on the MS COCO dataset demonstrate that the proposed MRFENet has a significant improvement against other state-of-the-art methods, which further validates that MRFENet can effectively utilize small object information.},
  archive      = {J_IETIP},
  author       = {Zhiwei Liu and Menghan Gan and Li Xiong and Xiaofeng Mao and Yue Que},
  doi          = {10.1049/ipr2.12799},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2385-2398},
  shortjournal = {IET Image Process.},
  title        = {Multilevel receptive field expansion network for small object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight method for face expression recognition based
on improved MobileNetV3. <em>IETIP</em>, <em>17</em>(8), 2375–2384. (<a
href="https://doi.org/10.1049/ipr2.12798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition plays a significant role in the application of man–machine interaction. However, existing models typically have shortcomings with numerous parameters, large model sizes, and high computational costs, which are difficult to deploy in resource-constrained devices. This paper proposes a lightweight network based on improved MobileNetV3 to mitigate these disadvantages. Firstly, we adjust the channels in the high-level network to reduce the number of parameters and model size, and then, the coordinate attention mechanism is introduced to the network, which enhances the attention of the network with few parameters and low computing cost. Furthermore, a complementary pooling structure is designed to improve the coordinate attention mechanism, which enables it to assist the network in extracting salient features sufficiently. In addition, the network with the joint loss consisting of the softmax loss and centre loss is trained, which can minimize the intra-class gap and improve the classification performance. Finally, the network is trained and tested on public datasets FERPlus and RAF-DB, with the best accuracy of 87.5% and 86.6%, respectively. The FLOPs, parameters, and the memory storage size are only 0.19GMac, 1.3 M, and 15.9 MB, respectively, which is lighter than most state-of-the-art networks. Code is available at https://github.com/RIS-LAB1/FER-mobilenet .},
  archive      = {J_IETIP},
  author       = {Xunru Liang and Jianfeng Liang and Tao Yin and Xiaoyu Tang},
  doi          = {10.1049/ipr2.12798},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2375-2384},
  shortjournal = {IET Image Process.},
  title        = {A lightweight method for face expression recognition based on improved MobileNetV3},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 4× super-resolution of unsupervised CT images based on GAN.
<em>IETIP</em>, <em>17</em>(8), 2362–2374. (<a
href="https://doi.org/10.1049/ipr2.12797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving the resolution of computed tomography (CT) medical images can help doctors more accurately identify lesions, which is important in clinical diagnosis. In the absence of natural paired datasets of high resolution and low resolution image pairs, we abandoned the conventional Bicubic method and innovatively used a dataset of images of a single resolution to create near-natural high–low-resolution image pairs by designing a deep learning network and utilizing noise injection. In addition, we propose a super-resolution generative adversarial network called KerSRGAN which includes a super-resolution generator, super-resolution discriminator, and super-resolution feature extractor to achieve a 4× super-resolution of CT images. The results of an experimental evaluation show that KerSRGAN achieved superior performance compared to the state-of-the-art methods in terms of a quantitative comparison of non-reference image quality evaluation indicators on the generated 4× super-resolution CT images. Moreover, in terms of an intuitive visual comparison, the images generated by the KerSRGAN method had more precise details and better perceptual quality.},
  archive      = {J_IETIP},
  author       = {Yunhe Li and Lunqiang Chen and Bo Li and Huiyan Zhao},
  doi          = {10.1049/ipr2.12797},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2362-2374},
  shortjournal = {IET Image Process.},
  title        = {4× super-resolution of unsupervised CT images based on GAN},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth-aware lightweight network for RGB-d salient object
detection. <em>IETIP</em>, <em>17</em>(8), 2350–2361. (<a
href="https://doi.org/10.1049/ipr2.12796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D salient object detection (SOD) is to detect salient objects from one RGB image and its depth data. Although related networks have achieved appreciable performance, they are not ideal for mobile devices since they are cumbersome and time-consuming. The existing lightweight networks for RGB-D SOD use depth information as additional input, and integrate depth information with colour image, which achieve impressive performance. However, the quality of depth information is uneven and the acquisition cost is high. To solve this issue, depth-aware strategy is first combined to propose a lightweight SOD model, Depth-Aware Lightweight network (DAL), using only RGB maps as input, which is applied to mobile devices. The DAL&#39;s framework is composed of multi-level feature extraction branch, specially designed channel fusion module (CF) to perceive the depth information, and multi-modal fusion module (MMF) to fuse the information of multi-modal feature maps. The proposed DAL is evaluated on five datasets and it is compared with 14 models. Experimental results demonstrate that the proposed DAL outperforms the state-of-the-art lightweight networks. The proposed DAL has only 5.6 M parameters and inference speed of 39 ms. Compared with the best-performing lightweight method, the proposed DAL has fewer parameters, faster inference speed, and higher accuracy.},
  archive      = {J_IETIP},
  author       = {Liuyi Ling and Yiwen Wang and Chengjun Wang and Shanyong Xu and Yourui Huang},
  doi          = {10.1049/ipr2.12796},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2350-2361},
  shortjournal = {IET Image Process.},
  title        = {Depth-aware lightweight network for RGB-D salient object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generative adversarial network model fused with a
self-attention mechanism for the super-resolution reconstruction of
ancient murals. <em>IETIP</em>, <em>17</em>(8), 2336–2349. (<a
href="https://doi.org/10.1049/ipr2.12795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem that the texture details of low-resolution (LR) digital images of ancient murals are ambiguous persists. To solve this problem, this study proposes a super-resolution (SR) reconstruction method for fuzzy murals based on a generative adversarial network with self-attention (SA). The network uses a blur kernel and realistic noise data to add blur and noise, respectively, to a high-resolution (HR) image to obtain an original LR image. Then, a feature image with the same size as that of the input image is obtained through a SA module. Finally, the feature information extracted from the image is input into the high-resolution image space by using a subpixel convolution layer to realize the image enlargement process from an LR to an HR. Experiments evaluate the proposed approach both objectively and subjectively. The objective evaluation results show that compared with other SR reconstruction algorithms, the proposed algorithm&#39;s peak signal-to-noise ratio (PSNR) is increased by 0.04 to 3.78 dB on average, and its structural similarity is increased by 0.002 to 0.191. A subjective perception evaluation shows that the developed algorithm can better reconstruct the texture details of murals, thus better meeting the visual perception needs of the public. The method proposed in this study can satisfactorily reconstruct the texture details of murals, which may provide technical guidance for the development of mural protection plans. Furthermore, it may be of certain practical significance for the SR reconstruction of ancient murals.},
  archive      = {J_IETIP},
  author       = {Jianfang Cao and Xiaohui Hu and Hongyan Cui and Yunchuan Liang and Zeyu Chen},
  doi          = {10.1049/ipr2.12795},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2336-2349},
  shortjournal = {IET Image Process.},
  title        = {A generative adversarial network model fused with a self-attention mechanism for the super-resolution reconstruction of ancient murals},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adaptive cross-scale transformer based on graph signal
processing for person re-identification. <em>IETIP</em>, <em>17</em>(8),
2321–2335. (<a href="https://doi.org/10.1049/ipr2.12794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting robust feature representation is one of the key challenges for person re-identification (ReID) task. Although convolution neural network (CNN)-based methods have achieved great success, they still cannot handle the part occlusion and misalignment caused by limited receptive field. Recently, pure transformer models have shown its power in the person ReID task. However, current transformer models adopt patches of equal-scale as input, and cannot solve the problem of cross-scale interaction properly. To overcome this problem, an adaptive cross-scale transformer from a perspective of the graph signal, named ACSFormer, is proposed. Specifically, the self-attention module is first treated as an undirected fully connected graph. And then, “node variation” is introduced as an indicator to adaptively merge neighbourhood tokens. To the best of the authors’ knowledge, their ACSFormer is the first work to attempt to combine pure transformers and graph signal processing in the field of person ReID. Extensive evaluations are conducted on three person ReID datasets to validate the performance of ACSFormer. Experiments demonstrate that this ACSFormer performs on par with state-of-the-art CNN-based methods and consistently improves transformer-based baseline, for example, surpassing ViT-baseline by 2.5%, 2.7% and 4.8% mAP on Market1501, DukeMTMC-reID and MSMT17, respectively.},
  archive      = {J_IETIP},
  author       = {Wei Zhou and Yi Hou and Shijun Xu and Shilin Zhou},
  doi          = {10.1049/ipr2.12794},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2321-2335},
  shortjournal = {IET Image Process.},
  title        = {An adaptive cross-scale transformer based on graph signal processing for person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MDR-net: Multiscale dense residual networks for liver image
segmentation. <em>IETIP</em>, <em>17</em>(8), 2309–2320. (<a
href="https://doi.org/10.1049/ipr2.12793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver image segmentation is an attractive topic in the diagnosis and surgical planning of liver diseases. Although deep learning methods have significantly advanced liver segmentation, existing frameworks fail to clearly determine liver boundaries, especially in medical images where various organs have similar grey levels. In this paper, the authors design a multi-scale dense residual network (MDR-Net) for liver segmentation, which consists of two blocks: a liver segmentation network and an edge-aware network. In the segmentation network, the authors introduce a multi-scale residual pooling module combining channel attention (CA) mechanism and depth-wise separable convolution to accommodate liver scale variation. Furthermore, the authors employ an edge-aware loss network to refine edge information and enhance feature representation, which is beneficial to guide the network to iterate towards the ground truth. The authors’ method achieves the best visualization results in qualitative evaluation. In addition, the authors’ method achieves 96.189% on 3D-IRCADb and 96.889% on the CHAOS dataset in quantitative evaluation with respect to the dice index.},
  archive      = {J_IETIP},
  author       = {Lijie Xie and Fubao Zhu and Ni Yao},
  doi          = {10.1049/ipr2.12793},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2309-2320},
  shortjournal = {IET Image Process.},
  title        = {MDR-net: Multiscale dense residual networks for liver image segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-class pest and disease vegetation detection based on
small sample registration. <em>IETIP</em>, <em>17</em>(8), 2299–2308.
(<a href="https://doi.org/10.1049/ipr2.12779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces few-shot anomaly detection (FSAD), a practical and less anomaly detection (AD) method, which can provide a limited number of normal images for each class during training. So far, studies on FSAD have been carried out according to each model, and there is no discussion of commonalities between different types. Depending on how people detect unusual lies, the problematic images are compared to the normal ones. The image alignment method based on different classifications is used to train the target detection model independent of classification, and performed ablation experiments on the pest and disease datasets in different environments for verification. This is the first time the FSAD method has been used to train a single scalable model without the need to train new classifications or adjust parameters. The experimental results show that the application of AUC based on vegetation disease data set and vegetation pest data set in FSAD algorithm is improved by 19.5% compared with the existing algorithm.},
  archive      = {J_IETIP},
  author       = {Liu Jiayao and Wang Linfeng and Wang Yunsheng and An MingMing and Jiang Wenfei and Xu Shipu},
  doi          = {10.1049/ipr2.12779},
  journal      = {IET Image Processing},
  month        = {6},
  number       = {8},
  pages        = {2299-2308},
  shortjournal = {IET Image Process.},
  title        = {Cross-class pest and disease vegetation detection based on small sample registration},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Corrigendum. <em>IETIP</em>, <em>17</em>(7), 2297–2298. (<a
href="https://doi.org/10.1049/ipr2.12775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IETIP},
  author       = {Yingpin Chen},
  doi          = {10.1049/ipr2.12775},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2297-2298},
  shortjournal = {IET Image Process.},
  title        = {Corrigendum},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved BlendMask: Nuclei instance segmentation for medical
microscopy images. <em>IETIP</em>, <em>17</em>(7), 2284–2296. (<a
href="https://doi.org/10.1049/ipr2.12792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nuclei instance segmentation is an important task in medical image analysis involving cell-level pathological analysis, which is of great significance for many biomedical applications. Nuclei segmentation is a challenging task due to edge adhesions and the distribution of numerous tiny dense nuclei. In this work, a nuclei instance segmentation framework, namely, the improved BlendMask is proposed. In this framework, in order to improve the performance of detection and segmentation of dense small objects and adhering nuclei, two components, including dilated convolution aggregation module (DCA) and context information aggregation module (CIA), are designed. The DCA constructs multi-path parallel dilated convolution, which greatly increases the receptive field of the network and the ability to capture multi-scale contextual information. The CIA reduces the information loss in the channel by endowing the network with high-level multi-scale spatial context information. In addition, a novel distributional ranking loss function is given that can effectively alleviate the imbalance between the target and the background. The proposed method is validated on the DSB2018 dataset. Compared to BlendMask, this network improves by 3.6% on AP segmentation metric, and the segmentation performance of this network is superior to that of several recent classic open-source nuclei instance segmentation methods.},
  archive      = {J_IETIP},
  author       = {Juan Wang and Zetao Zhang and Minghu Wu and Yonggang Ye and Sheng Wang and Ye Cao and Hao Yang},
  doi          = {10.1049/ipr2.12792},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2284-2296},
  shortjournal = {IET Image Process.},
  title        = {Improved BlendMask: Nuclei instance segmentation for medical microscopy images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trinity-yolo: High-precision logo detection in the real
world. <em>IETIP</em>, <em>17</em>(7), 2272–2283. (<a
href="https://doi.org/10.1049/ipr2.12791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logo detection has a wide range of applications in the multimedia field, such as video advertising research, brand awareness monitoring and analysis, trademark infringement detection, autonomous driving and intelligent transportation. Compared with other types of images, logo images in the real world have greater diversity in appearance and more complex backgrounds. Therefore, identifying logos from images is a challenge. A strong baseline method Trinity-Yolo, is proposed, which incorporates attention mechanism, stripe pooling and weighted boxes fusion (WBF) into the state-of-the-art Yolov4 framework for large-scale logo detection. The attention mechanism improves the feature extraction ability of the deep detection model, the stripe pooling expands the field of view of the model and the weighted boxes fusion enables the model to obtain excellent corrections when outputting the prediction boxes. Trinity-Yolo can solve the problems of lack of training data, multi-scale objects and inconsistent bounding-box regression. On the dataset LogoDet-3K, the average performance of Trinity-Yolo is 3% higher than that of Yolov4. Compared with other deep detection models, the performance of Trinity-Yolo is improved more. The experimental performance on other existing datasets verifies the effectiveness of this method.},
  archive      = {J_IETIP},
  author       = {KeJi Mao and RunHui Jin and KaiYan Chen and JiaFa Mao and GuangLin Dai},
  doi          = {10.1049/ipr2.12791},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2272-2283},
  shortjournal = {IET Image Process.},
  title        = {Trinity-yolo: High-precision logo detection in the real world},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integration of the latent variable knowledge into deep image
captioning with bayesian modeling. <em>IETIP</em>, <em>17</em>(7),
2256–2271. (<a href="https://doi.org/10.1049/ipr2.12790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image captioning systems assign one or more sentences to images to describe their visual content. Most of these systems use attention-based deep convolutional neural networks and recurrent neural networks (CNN-RNN-Att). However, they must optimally use latent variables and side information within the image concepts. This study aims to integrate a latent variable into image captioning using CNN-RNN-Att. A Bayesian modeling framework is used for this work. As an instance of a latent variable, High-Level Semantic Concepts (HLSCs) of tags are used to implement the proposed model. The Bayesian model output interpretation is to localize the entire image description process and breaks it down into sub-problems. Thus, a baseline descriptor subnet is trained independently for each sub-problem, and it is the only expert in captioning for a given HLSC. The final output is the caption derived from the subnet; its HLSC is closest to the image content. The results indicate that CNN-RNN-Att applied to data localized using HLSCs improves the captioning accuracy of the proposed method, which can be compared to the latest state-of-the-art and most accurate captioning systems.},
  archive      = {J_IETIP},
  author       = {Alireza Barati and Hassan Farsi and Sajad Mohamadzadeh},
  doi          = {10.1049/ipr2.12790},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2256-2271},
  shortjournal = {IET Image Process.},
  title        = {Integration of the latent variable knowledge into deep image captioning with bayesian modeling},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new adaptive window-based guided filtering and
interpolation for polarization image demosaicing. <em>IETIP</em>,
<em>17</em>(7), 2238–2255. (<a
href="https://doi.org/10.1049/ipr2.12789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new adaptive window-based guided filtering and interpolation for polarization image demosaicing. Considering the correlation between the polarization channels, the authors use the total intensity S 0 as a guide image to guide the interpolation of all channels. The guided filtering is performed with a new adaptive window size selection technique. A novel iterative residual interpolation is proposed to construct accurate full resolution polarization. Experiments are conducted with the sample images from the demosaicking algorithms dataset. Objective evaluations demonstrate that the proposed algorithm can reduce root mean squared error by at least 27% over other methods. Subjective visual inspection can also verify the improvements in the clarity of objects in angle and degree of the linear polarization images.},
  archive      = {J_IETIP},
  author       = {Fei Xie and Shumin Liu and Jiajia Chen},
  doi          = {10.1049/ipr2.12789},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2238-2255},
  shortjournal = {IET Image Process.},
  title        = {A new adaptive window-based guided filtering and interpolation for polarization image demosaicing},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AARN: Anchor-guided attention refinement network for inshore
ship detection. <em>IETIP</em>, <em>17</em>(7), 2225–2237. (<a
href="https://doi.org/10.1049/ipr2.12787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inshore ship detection is an important task in several fields, for example, maritime transportation, maritime supervision, and port management. However, due to the diversified categories and locations of different ships and interference of complex surroundings, capturing discriminative characteristics of multi-scale inshore ships for accurate detection is still challenging. Here, an anchor-guided attention refinement network (AARN) is proposed to alleviate the problems by prominently designing an attention feature filter module (AFFM) and an anchor-guided alignment detection module (AADM). In AFFM, of which the attention supervision generated from high-level semantic features, is used to highlight informative target cues and suppress background interference when establishing a four-layer feature pyramid. In AADM, the anchor-aligned features are adopted to eventually identify potential inshore ships, which both alleviates misalignment between refined anchors and pyramidal features and improves the performance further. Extensive experiments conducted on the public Seaships7000 dataset verify the contributions of the proposed modules and the effectiveness of our method for detecting multi-scale inshore ships in comparison to both domain-specific and general CNN-based methods.},
  archive      = {J_IETIP},
  author       = {Di Liu and Yan Zhang and Yan Zhao and Zhiguang Shi and Jinghua Zhang and Yu Zhang and Feng Ling and Yi Zhang},
  doi          = {10.1049/ipr2.12787},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2225-2237},
  shortjournal = {IET Image Process.},
  title        = {AARN: Anchor-guided attention refinement network for inshore ship detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mask-guided image person removal with data synthesis.
<em>IETIP</em>, <em>17</em>(7), 2214–2224. (<a
href="https://doi.org/10.1049/ipr2.12786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a special case of common object removal, image person removal is playing an increasingly important role in social media and criminal investigation domains. Due to the integrity of person area and the complexity of human posture, person removal has its own dilemmas. In this paper, a novel idea is proposed to tackle these problems from the perspective of data synthesis. Concerning the lack of a dedicated dataset for image person removal, two dataset production methods are proposed to automatically generate images, masks and ground truths, respectively. Then, a learning framework similar to local image degradation is proposed so that the masks can be used to guide the feature extraction process and more texture information can be gathered for final prediction. A coarse-to-fine training strategy is further applied to refine the details. The data synthesis and learning framework combine well with each other. Experimental results verify the effectiveness of the method quantitatively and qualitatively, and the trained network proves to have good generalization ability either on real or synthetic images.},
  archive      = {J_IETIP},
  author       = {Yunliang Jiang and Chenyang Gu and Zhenfeng Xue and Xiongtao Zhang and Yong Liu},
  doi          = {10.1049/ipr2.12786},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2214-2224},
  shortjournal = {IET Image Process.},
  title        = {Mask-guided image person removal with data synthesis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive fractional differential algorithm for image edge
enhancement and texture preserve using fuzzy sets. <em>IETIP</em>,
<em>17</em>(7), 2204–2213. (<a
href="https://doi.org/10.1049/ipr2.12785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper uses a fuzzy set scheme to present an adaptive fractional differential algorithm for image edge enhancement and texture preservation. In the proposed algorithm, an image&#39;s membership function and area feature are used to calculate the fuzzy set of images. The function of adaptive fractional differential order (FAFDO) can be constructed by making the linear transformation of the fuzzy set. Then, the fuzzy adaptive fractional differential mask (FAFDM) is obtained by substituting the FAFDO into the fractional differential mask. Finally, the image edge and texture are enhanced and preserved by applying airspace filtering of the FAFDM convolution. The experimental results show that, compared to fractional differential or fuzzy set-based image enhancement algorithms, the proposed algorithm can adaptively enhance the image edge and preserve the image texture by analysing the fuzziness of the image itself.},
  archive      = {J_IETIP},
  author       = {Bo Li and Wei Xie and Langwen Zhang and Xiaoyuan Yu},
  doi          = {10.1049/ipr2.12785},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2204-2213},
  shortjournal = {IET Image Process.},
  title        = {Adaptive fractional differential algorithm for image edge enhancement and texture preserve using fuzzy sets},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RRN: A differential private approach to preserve privacy in
image classification. <em>IETIP</em>, <em>17</em>(7), 2192–2203. (<a
href="https://doi.org/10.1049/ipr2.12784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide application of image classification has given rise to many intelligent systems, such as face recognition systems, which makes our life more convenient. However, the ensuing privacy leakage problem has become increasingly serious. The training of a deep neural network requires lots of data, which may contain sensitive information of users and may be exploited by data collectors. A perturbation algorithm named RRN is proposed for image data based on local differential privacy, which provides a rigorous privacy guarantee. Existing solutions have low accuracy due to the high sensitivity of an image; the authors&#39; method combines the Randomized Response mechanism with the Laplace mechanism to solve this problem. Experiments were conducted on the MNIST and CIFAR-10 datasets to show the effectiveness of the algorithm. Experimental results shows that the model is better than baseline models. The algorithm was also implemented on the commonly used model in deep learning, the VGG model, which can achieve 96.4% accuracy in the non-private version on the CIFAR-10 dataset. The accuracy of the differential private VGG model based on the RRN algorithm is 83% when ε = 0.5 $\varepsilon =0.5$ , which is still excellent. The experimental results show that the RRN algorithm can both preserve privacy and data utility.},
  archive      = {J_IETIP},
  author       = {Zhidong Shen and Ting Zhong and Hui Sun and Baiwen Qi},
  doi          = {10.1049/ipr2.12784},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2192-2203},
  shortjournal = {IET Image Process.},
  title        = {RRN: A differential private approach to preserve privacy in image classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMDA: Multi-person marginal distribution awareness for
monocular 3D pose estimation. <em>IETIP</em>, <em>17</em>(7), 2182–2191.
(<a href="https://doi.org/10.1049/ipr2.12783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing 3D pose representations cannot completely decouple the overlapping two or more human joints of the same type. In this paper, the authors propose a novel 2.5 D representation of the human pose by projecting human joints in 3D space onto the three orthogonal planes. The authors apply for the first time the permutation module to a multi-person 3D human pose estimation task and use Geometric Constraints Loss (GCL) to guide the learning of the model. The authors overcome the negative effects of the inductive bias of convolutional neural networks (CNNs) by aligning the intermediate feature space with the output feature space. The effectiveness of the authors’ approach is validated on the carnegie mellon university (CMU) panoptic dataset and MuPoTS-3D dataset. The authors’ proposed representations can effectively decouple the human joints in their selected data from overlapping human joints.},
  archive      = {J_IETIP},
  author       = {Sheng Liu and Jianghai Shuai and Yang Li and Sidan Du},
  doi          = {10.1049/ipr2.12783},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2182-2191},
  shortjournal = {IET Image Process.},
  title        = {MMDA: Multi-person marginal distribution awareness for monocular 3D pose estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SS-norm: Spectral-spatial normalization for single-domain
generalization with application to retinal vessel segmentation.
<em>IETIP</em>, <em>17</em>(7), 2168–2181. (<a
href="https://doi.org/10.1049/ipr2.12782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinal vessel segmentation is an important computer vision task for eye retinopathy diagnosis. In the real scenarios, most datasets of source domain and target domain have distribution deviation, and the model often fails to generate accurate segmentation results due to the lack of data variation in single-source domain, which damages the generalization ability to unseen target domains and may mislead doctors or artificial intelligence model in the following diseases diagnosis. Feature normalization is one feasible solution which can standardize data into uniform and stable distribution without additional data. However, the existing methods like batch normalization, uniform the data by global parameters. This leads to insufficient representation of important semantic information in the local region. To address this problem, the authors propose the spectral-spatial normalization (SS-Norm) module to enhance the generalization ability of the model. More specifically, the authors perform a discrete cosine transform (DCT) to decompose the feature into multiple frequency components and to analyze the semantic contribution degree of each component. By learning a spectral vector, the authors reweight the frequency components of features and therefore normalize the distribution in the spectral domain. Extensive experiments on six datasets prove the effectiveness of the authors’ methods.},
  archive      = {J_IETIP},
  author       = {Yi-Peng Liu and Dongxu Zeng and Zhanqing Li and Peng Chen and Ronghua Liang},
  doi          = {10.1049/ipr2.12782},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2168-2181},
  shortjournal = {IET Image Process.},
  title        = {SS-norm: Spectral-spatial normalization for single-domain generalization with application to retinal vessel segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DGC-UWnet: Underwater image enhancement based on
computation-efficient convolution and channel shuffle. <em>IETIP</em>,
<em>17</em>(7), 2158–2167. (<a
href="https://doi.org/10.1049/ipr2.12781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image enhancement is receiving increasing attention due to many of the vision research being applied to underwater scenes. To eliminate the impact of complex underwater scenes on imaging, underwater image enhancement algorithm has become an effective solution. However, underwater image enhancement models face a challenge of lightening the model while improving generalizability. Here, DGC-UWnet is proposed to go for both lightweight and enhancement effect. The proposed model is designed by using depthwise convolution, group convolution and channel shuffle (DGC). Ablation experiment shows that compared with standard convolution, DGC decreases model parameters and computational complexity, and improves the generalizability of the model. Qualitative and quantitative comparative experimental results show that comprehensive performances of the model can catch up with or even surpass state-of-the-art (SOTA) algorithms in terms of processing speed, subjective visual perception and objective evaluation metrics. In addition, application test results prove that DGC-UWnet can be used as the pre-processing for underwater applications of other visual algorithms such as improving performance of YOLOv5l.},
  archive      = {J_IETIP},
  author       = {Xuyan Hao and Lixin Liu},
  doi          = {10.1049/ipr2.12781},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2158-2167},
  shortjournal = {IET Image Process.},
  title        = {DGC-UWnet: Underwater image enhancement based on computation-efficient convolution and channel shuffle},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A graph-based edge attention gate medical image segmentation
method. <em>IETIP</em>, <em>17</em>(7), 2142–2157. (<a
href="https://doi.org/10.1049/ipr2.12780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the purpose of solving the problems of missing edges and low segmentation accuracy in medical image segmentation, a medical image segmentation network (EAGC_UNet++) based on residual graph convolution UNet++ with edge attention gate (EAG) is proposed in the study. With UNet++ as the backbone network, the idea of graph theory is introduced into the model. First, the dropout residual graph convolution block (DropRes_GCN Block) and the traditional convolution structure in UNet++ are used as encoders. Second, EAGs are adopted so that the model pays more attention to image edge features during decoding. Finally, aiming at the imbalance problem of positive and negative samples in medical image segmentation, a new weighted loss function is introduced to enhance segmentation accuracy. In the experimental part, three datasets (LiTS2017, ISIC2018, COVID-19 CT scans) were used to evaluate the performances of various models; multiple groups of ablation experiments were designed to verify the effectiveness of each part of the model. The experimental results showed that EAGC_UNet++ had better segmentation performance than the other models under three quantitative evaluation indicators and better solved the problem of missing edges in medical image segmentation.},
  archive      = {J_IETIP},
  author       = {Dechen Hao and Hualing Li},
  doi          = {10.1049/ipr2.12780},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2142-2157},
  shortjournal = {IET Image Process.},
  title        = {A graph-based edge attention gate medical image segmentation method},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image encryption algorithm based on 2D-CLICM chaotic system.
<em>IETIP</em>, <em>17</em>(7), 2127–2141. (<a
href="https://doi.org/10.1049/ipr2.12778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image encryption algorithm based on a two-dimensional Chebyshev-Logistic-Infinite Collapse Map (2D-CLICM) is proposed. A new two-dimensional chaotic map is designed, and numerical studies show that 2D-CLICM has a larger chaotic region, better randomness and more complex chaotic behavior. An image encryption algorithm (CLICM-IE) is developed based on the 2D-CLICM. The randomly generated key is used to generate more random chaotic sequences and the security is greatly improved. To obtain lower computation and time complexity, the whole process performs row encryption for each image pixel. The Arnold transformation preprocesses the image and then the encryption of image pixels is accomplished by controlling the random confusion and diffusion parts with the sequence generated by 2D-CLICM. The results show that the CLICM-IE algorithm is deeply random and unpredictable, and resists a variety of attacks. These results can provide good basis and practical application for the chaos generation and image encryption.},
  archive      = {J_IETIP},
  author       = {Xiaoman Jiang and Guangyu Jiang and Qingke Wang and DongDong Shu},
  doi          = {10.1049/ipr2.12778},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2127-2141},
  shortjournal = {IET Image Process.},
  title        = {Image encryption algorithm based on 2D-CLICM chaotic system},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised learning for ABUS tumor detection using deep
learning method. <em>IETIP</em>, <em>17</em>(7), 2113–2126. (<a
href="https://doi.org/10.1049/ipr2.12777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated breast ultrasound (ABUS) imaging system is a practical technique to automatically scan the whole breast. Automatic tumor detection plays a significant role in the clinic. However, training deep convolutional neural networks (CNNs) for tumor detection needs a large quantity of labeled data. It is time-consuming and expensive to manually annotate tumor positions in ABUS images. In this paper, a novel semi-supervised learning EfficientDet (SSL-E) model is proposed for ABUS tumor detection. Our SSL-E model solves the tumor detection problem from high similarity and serious unbalance between tumors and backgrounds. Considering the image contrast variation and tumor scale variations in ABUS images, color transformation and geometric transformation are employed for data augmentations. Then the consistency between image and its augmented version is developed, thus the robustness of the detector can be improved. Aiming at the problem of serious unbalance between tumors and backgrounds, a novel copy-paste synthesis strategy is designed, which can generate more tumor samples and enhance tumor diversity. This method is tested on 68 tumor volumes and 68 normal volumes, including 43,248 slices (1683 tumor slices and 41,565 normal slices). It obtains a promising result with sensitivity of 90.2% and false positives per image (FPs/I) at 0.15.},
  archive      = {J_IETIP},
  author       = {Yanfeng Li and Zilu Zhang and Zhanyi Cheng and Lin Cheng and Xin Chen},
  doi          = {10.1049/ipr2.12777},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2113-2126},
  shortjournal = {IET Image Process.},
  title        = {Semi-supervised learning for ABUS tumor detection using deep learning method},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multilevel histogram shape-based image watermarking
invariant to geometric attacks. <em>IETIP</em>, <em>17</em>(7),
2097–2112. (<a href="https://doi.org/10.1049/ipr2.12776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a geometrically invariant image watermarking scheme is proposed by exploiting multilevel histogram shapes. The embedding procedure starts by decomposing the host image with the first level Haar wavelet. After that, histograms are extracted from the approximation subband via several rounds, which are used to embed watermark bits. Each round of embedding first extracts a histogram at a specified level. Then the histogram is split into fragments, into which a number of watermark bits can be embedded. In this way, a considerable watermarking capacity is available. Besides, a histogram adjustment in the first embedding round is suggested to guarantee good population of histogram bins. Experimental results support its robustness against various common attacks and geometric attacks. Moreover, the scheme can embed multiple watermark sequences with various robustness and capacity profiles, which enriches its practical applications.},
  archive      = {J_IETIP},
  author       = {Bingwen Feng and Guofeng Li and Zhiquan Luo and Wei Lu},
  doi          = {10.1049/ipr2.12776},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2097-2112},
  shortjournal = {IET Image Process.},
  title        = {Multilevel histogram shape-based image watermarking invariant to geometric attacks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fuzzy rule-based system with decision tree for breast
cancer detection. <em>IETIP</em>, <em>17</em>(7), 2083–2096. (<a
href="https://doi.org/10.1049/ipr2.12774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is possibly the deadliest illness in the world and the risks are gradually increasing. One out of eight women has the chance to be detected with breast cancer in their lifetime. The utmost cause for the higher fatality rates is the prolonged prognosis for the detection of breast cancer. The focus of this study is therefore to develop a better fuzzy expert system for the detection of breast cancer using decision tree analysis for deriving the rule base. For this classification problem, the input features of the dataset are converted into human-understandable terms-linguistic variables. The Mamdani Fuzzy Rule-Based system is deployed as the main inference engine and the centroid method for the defuzzification process to convert the final fuzzy score into class labels- benign (not cancerous) or malignant (cancerous). A decision tree algorithm is applied the creating a novel set of 27 fuzzy rules which are fed into FRBS. The investigation is performed on the publicly available Wisconsin Breast Cancer Dataset. The accuracy obtained by the proposed system is about 97%, recall is 99.58% and precision is about 93%. The experiments on this dataset yield higher performance as compared to the state-of-the-art dataset.},
  archive      = {J_IETIP},
  author       = {Vedika Gupta and Harshit Gaur and Srishti Vashishtha and Uttirna Das and Vivek Kumar Singh and D. Jude Hemanth},
  doi          = {10.1049/ipr2.12774},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2083-2096},
  shortjournal = {IET Image Process.},
  title        = {A fuzzy rule-based system with decision tree for breast cancer detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved LSB image steganography with high imperceptibility
based on cover-stego matching. <em>IETIP</em>, <em>17</em>(7),
2072–2082. (<a href="https://doi.org/10.1049/ipr2.12773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information security is an important factor when critical information is transferred. This paper concentrates on finding a method that can hide a message in an image without making any changes to it. The proposed method is novel and based on the Least Significant Bit (LSB) method with a high imperceptibility and capacity. Initially, the secret message bits are compressed using the LZW technique, then the compressed message bits are embedded into the LSB of the edge pixels in the cover image which have the same bit values. As a result, the cover image pixels remain unchanged and the stego image matches the cover image. The addresses of pixels that contain the compressed secret bits are maintained in a file called “location addresses” which the receiver uses to retrieve the compressed secret message. It is possible that the stego image will not be transmitted if it was previously specified by the sender and the receiver. Image quality metrics are applied to several standard images. The obtained PSNR for the stego image is infinity, SSIM and NCC are one, MSE and AD are zero for different embedding capacities in all images, which ensures the stego image&#39;s excellent imperceptibility.},
  archive      = {J_IETIP},
  author       = {Sama N. M. Al-Faydi and Sahar Khalid Ahmed and Heba N. Y. Al-Talb},
  doi          = {10.1049/ipr2.12773},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2072-2082},
  shortjournal = {IET Image Process.},
  title        = {Improved LSB image steganography with high imperceptibility based on cover-stego matching},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-negative low-rank adaptive preserving sparse matrix
regression model for supervised image feature selection and
classification. <em>IETIP</em>, <em>17</em>(7), 2056–2071. (<a
href="https://doi.org/10.1049/ipr2.12772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse matrix regression (SMR) model for the feature selection method has attracted much attention. However, most existing models do not consider the globality and adaptively preserve the local structure of the image data in projection space. To settle such issues, an adaptive non-negative low-rank preserving SMR model for supervised image feature selection is proposed. It first uses the low-rank representation with non-negative constraint to capture the globality and more discriminative information of image data and makes the error matrix in self-representation of training data sparse. Next, the non-negative low-rank representation coefficients are used to establish a graph matrix learning model to reveal the local manifold structure of the image data. Thus, the proposed model enhances the discriminative ability as well as performs feature selection by the obtained transformation matrix. Finally, an alternating iterative algorithm for solving this model is developed and its convergence and complexity are also analyzed. Experimental results on some image data sets show that the proposed algorithm is effective for images and its recognition ability is obviously superior to other existing methods. In addition, the proposed method is also applied to two scene image classifications to further verify its effectiveness.},
  archive      = {J_IETIP},
  author       = {Xiuhong Chen and Xingyu Zhu and Yun Lu and Zhifang Pu},
  doi          = {10.1049/ipr2.12772},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2056-2071},
  shortjournal = {IET Image Process.},
  title        = {Non-negative low-rank adaptive preserving sparse matrix regression model for supervised image feature selection and classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dark light enhancement for dark scene urban object
recognition. <em>IETIP</em>, <em>17</em>(7), 2043–2055. (<a
href="https://doi.org/10.1049/ipr2.12771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a low-light image enhancement method to improve the performance of autonomous piloting tasks based on deep learning methods. In the low light environment, camera sensors cannot capture enough effective photon signals causing poor performance in vision-based autonomous piloting. Moreover, the lack of training data makes single-frame low-light enhancement and denoising algorithms hard to generalize in real-world scenarios. By analyzing the noise patterns of real-world cameras under low-light environments, a noise generation method is proposed to mimic real-world dark-light noise and generate noisy-clean data pairs for training. A method that can calibrate the camera shot noise parameters is then designed to learn low-light enhancement from the synthesized noisy data pairs. The neural network trained on a synthesized dataset can effectively enhance the dark light image quality. Consequently, the network improves the downstream auto-piloting applications such as object detection and semantic segmentation. Qualitative and quantitative experiments have been conducted to demonstrate that the method outperforms previous methods in low-light enhancement.},
  archive      = {J_IETIP},
  author       = {Xiao Zhou and Xiaobiao Du and Peizhe Ru},
  doi          = {10.1049/ipr2.12771},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2043-2055},
  shortjournal = {IET Image Process.},
  title        = {Dark light enhancement for dark scene urban object recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CFNet: Head detection network based on multi-layer feature
fusion and attention mechanism. <em>IETIP</em>, <em>17</em>(7),
2032–2042. (<a href="https://doi.org/10.1049/ipr2.12770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, head detection has been widely used in target detection, which has a great application value for improving security prevention and control in public places, as well as enhancing target tracking and identification in national defense, criminal investigation, and other fields. However, detecting small targets accurately at long distances is very difficult, and current methods often lack optimization of multi-resolution features. Therefore, the authors propose a one-stage detection network CFNet (cross-layer feature fusion and fusion weight attention network), in which a fusion weight attention mechanism module (FWAM) is proposed to give different weights to the fused features in order to distinguish the importance of different features. The module increases the weights of features that contain strong information so that the fused features are focused on feature points that are beneficial for optimal head detection. Meanwhile, a cross-layer feature fusion module is proposed to fuse information from different resolution feature maps to compensate for the decrease in detection accuracy caused by the omission of information features at low resolution, and a connection network for contextual information fusion is constructed, while weight parameter value settings are introduced to optimize the detection effect after fusion of different resolution features. In order to better reflect the effectiveness of the network, the experiments are performed on the SCUT-HEAD PartA dataset and the Brainwash dataset; the results show that the network the authors proposed is better than the existing comparison methods, which proves the robustness and effectiveness of the network.},
  archive      = {J_IETIP},
  author       = {Jing Han and Xiaoying Wang and Xichang Wang and Xueqiang Lv},
  doi          = {10.1049/ipr2.12770},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2032-2042},
  shortjournal = {IET Image Process.},
  title        = {CFNet: Head detection network based on multi-layer feature fusion and attention mechanism},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A modified eye-in-hand stereo visual control for grasping
unknown objects via scara robot. <em>IETIP</em>, <em>17</em>(7),
2015–2031. (<a href="https://doi.org/10.1049/ipr2.12758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here, the switch proportional integral derivative controller (PID) stereo image-based visual servoing controller is designed for controlling the manipulator robots. The proposed control algorithm separates the rotational and translational motion of the camera, so the stereo image-based visual servoing control could be decomposed into three separate stages with different gains. Despite the increasing computation and complexity of relationships, instead of kinematics, robot dynamics are used to guide the robot to make the simulation more consistent with the real condition. To reduce response time and improve the robot&#39;s trajectory, the object positions in the image space are predicted by the Extended Kalman filter. The results showed that the proposed method produces a more accurate response than the classical methods that use robot kinematics. Adding the trajectory estimation algorithm improves the time response of the system and its industrial application capabilities.},
  archive      = {J_IETIP},
  author       = {Mahmoud Jeddi and Ahmad Reza Khoogar},
  doi          = {10.1049/ipr2.12758},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2015-2031},
  shortjournal = {IET Image Process.},
  title        = {A modified eye-in-hand stereo visual control for grasping unknown objects via scara robot},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facial expression recognition based on improved residual
network. <em>IETIP</em>, <em>17</em>(7), 2005–2014. (<a
href="https://doi.org/10.1049/ipr2.12743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are an important part of human emotional signals and their recognition has become an important topic of research in the field of pattern recognition. Deep learning based methods have achieved great success in the recognition of facial expressions. However, with the evolution of convolution neural networks and the increased network depth, these methods suffer from problems such as degraded network performance and loss of feature information. To address these problems, a novel facial expression recognition algorithm based on an improved residual neural network is proposed. First, a residual neural network is designed to extract deep features while retaining the shallow ones. This can effectively prevent the degradation of network performance. Moreover, when the gradient of the Rectified Linear Units activation function used in the residual module is 0, it will inactivate the neurons and cause a loss of feature information. To address this, the Mish activation function is used instead. The slight allowance for negative values in Mish improves the gradient flow. Next, an inception module is introduced to obtain richer feature information under the same receptive field. Finally, by conducting verification experiments on the public datasets CK+ and KDEF, the authors manage to solve the problems of degraded network performance and insufficient information from extracted features, achieving recognition accuracy rates of 96.37% and 93.38% on the two datasets, respectively.},
  archive      = {J_IETIP},
  author       = {Weiguang Zhang and Xuguang Zhang and Yinggan Tang},
  doi          = {10.1049/ipr2.12743},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {2005-2014},
  shortjournal = {IET Image Process.},
  title        = {Facial expression recognition based on improved residual network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel smoke detection algorithm based on improved mixed
gaussian and YOLOv5 for textile workshop environments. <em>IETIP</em>,
<em>17</em>(7), 1991–2004. (<a
href="https://doi.org/10.1049/ipr2.12719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a lot of flammable materials in the textile workshop, and once a fire occurs, it will cause property damage and casualties. At present, smoke detection in textile workshops mainly relies on temperature-sensing smoke sensors with low detection rate and poor real-time performance, which cannot meet the task of smoke detection in complex environments. Therefore, this paper proposes an improved mixed Gaussian and YOLOv5 smoke detection algorithm for textile workshops. In order to reduce the interference of static background in smoke detection, an improved gaussian mixture algorithm is used to extract suspected smoke areas in video by using the dynamic characteristics of smoke. Then, an adaptive attention module is added to the feature pyramid infrastructure of the YOLOv5 target detection network to improve the multi-scale target recognition ability. In addition, the focal loss function is used to reduce the impact of background and foreground class imbalances on the detection results. The experimental results show that the detection accuracy of the proposed method is 94.7%, and the average detection speed is 66.7 FPS. By comparing with the existing state-of-the-art algorithms, the detection capability of this method has been significantly improved. At the same time, it has high real-time performance and detection accuracy in smoke detection in textile workshops.},
  archive      = {J_IETIP},
  author       = {Xin Chen and Yipeng Xue and Yaolin Zhu and Ruiqing Ma},
  doi          = {10.1049/ipr2.12719},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1991-2004},
  shortjournal = {IET Image Process.},
  title        = {A novel smoke detection algorithm based on improved mixed gaussian and YOLOv5 for textile workshop environments},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A transfer learning-based system for grading breast invasive
ductal carcinoma. <em>IETIP</em>, <em>17</em>(7), 1979–1990. (<a
href="https://doi.org/10.1049/ipr2.12660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast carcinoma is a sort of malignancy that begins in the breast. Breast malignancy cells generally structure a tumour that can routinely be seen on an x-ray or felt like a lump. Despite advances in screening, treatment, and observation that have improved patient endurance rates, breast carcinoma is the most regularly analyzed malignant growth and the subsequent driving reason for malignancy mortality among ladies. Invasive ductal carcinoma is the most boundless breast malignant growth with about 80% of all analyzed cases. It has been found from numerous types of research that artificial intelligence has tremendous capabilities, which is why it is used in various sectors, especially in the healthcare domain. In the initial phase of the medical field, mammography is used for diagnosis, and finding cancer in the case of a dense breast is challenging. The evolution of deep learning and applying the same in the findings are helpful for earlier tracking and medication. The authors have tried to utilize the deep learning concepts for grading breast invasive ductal carcinoma using Transfer Learning in the present work. The authors have used five transfer learning approaches here, namely VGG16, VGG19, InceptionReNetV2, DenseNet121, and DenseNet201 with 50 epochs in the Google Colab platform which has a single 12GB NVIDIA Tesla K80 graphical processing unit (GPU) support that can be used up to 12 h continuously. The dataset used for this work can be openly accessed from http://databiox.com . The experimental results that the authors have received regarding the algorithm&#39;s accuracy are as follows: VGG16 with 92.5%, VGG19 with 89.77%, InceptionReNetV2 with 84.46%, DenseNet121 with 92.64%, DenseNet201 with 85.22%. From the experimental results, it is clear that the DenseNet121 gives the maximum accuracy in terms of cancer grading, whereas the InceptionReNetV2 has minimal accuracy.},
  archive      = {J_IETIP},
  author       = {Radhakrishnan Sujatha and Jyotir Moy Chatterjee and Anastassia Angelopoulou and Epaminondas Kapetanios and Parvathaneni Naga Srinivasu and Duraisamy Jude Hemanth},
  doi          = {10.1049/ipr2.12660},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {7},
  pages        = {1979-1990},
  shortjournal = {IET Image Process.},
  title        = {A transfer learning-based system for grading breast invasive ductal carcinoma},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FARNet: Fragmented affinity reasoning network of text
instances for arbitrary shape text detection. <em>IETIP</em>,
<em>17</em>(6), 1959–1977. (<a
href="https://doi.org/10.1049/ipr2.12769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary shape text detection is a challenging task in scene text recognition. Driven by deep learning and large-scale data sets, the detection method based on connected component (CC) has increasingly gained popularity. However, there are still problems of unclear separation of text instances and incorrect component links. Thus, in this paper, the authors propose a novel component connection method, that is, Fragmented Affinity Reasoning Network of Text Instances (FARNet), for arbitrary shape text detection. The network consists of a Weighted Feature Fusion Pyramid Network (WFFPN), Text Fragments Subgraph (TFS), and Dense Graph Attention Network (DGAT), which can be trained end-to-end. The WFFPN is used to generate text fragments, TFS and DGAT jointly construct an affinity reasoning network. Since the neighbouring boundaries between text instances may blend them into a single instance, the core idea is to use the WFFPN to divide the text instance into a series of rectangular fragments, the affinity reasoning network infers the affinity between fragments and then links them to rebuild text instances. Extensive experiments on seven challenging datasets (ICDAR2015, MSRA-TD500, Totaltext, CTW-1500, ICDAR 2019MLT, ICDAR2019 ArT, and DAST-1500) demonstrate that the proposed text detector achieves state-of-the-art performance in both on polygon datasets and quadrilateral datasets. The code is available at https://github.com/giganticpower/FARNet .},
  archive      = {J_IETIP},
  author       = {Honghui Chen and Pingping Chen and Yuhang Qiu and Nam Ling},
  doi          = {10.1049/ipr2.12769},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1959-1977},
  shortjournal = {IET Image Process.},
  title        = {FARNet: Fragmented affinity reasoning network of text instances for arbitrary shape text detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). A method for extracting interference striations in
lofargram based on decomposition and clustering. <em>IETIP</em>,
<em>17</em>(6), 1951–1958. (<a
href="https://doi.org/10.1049/ipr2.12768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex underwater noise environment, target detection, recognition, and tracking are proceeded through the frequency spectrum analysis of the signals received by sonar systems, in which the lofargram plays a major role. Typically, if the distance of the target experiences far-near-far variation, there will be parabolic interference striations presented in the lofargram. However, the existing striations extraction methods sometimes lack objectivity and fail to extract the wide striations accurately. In this paper, a novel method for wide type interference striations extraction is developed based on efficient decomposition and ensemble clustering. To obtain valuable information, the lofargram is decomposed into smooth background, striations area, and noise, then a multi-phase ensemble clustering algorithm is employed to extract parabolas from the decomposed striations area pixels. Experimental results on simulated and real-life datasets exhibit the effectiveness of the proposed method, and verify that it has comparable performance with prevalent Hough transform while extracting striations.},
  archive      = {J_IETIP},
  author       = {Xinyan Li and Dianpeng Wang and Yubin Tian and Xiangshun Kong},
  doi          = {10.1049/ipr2.12768},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1951-1958},
  shortjournal = {IET Image Process.},
  title        = {A method for extracting interference striations in lofargram based on decomposition and clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Application of multi-party computation and error correction
with image enhancement and convolution neural networks based on cloud
computing. <em>IETIP</em>, <em>17</em>(6), 1931–1950. (<a
href="https://doi.org/10.1049/ipr2.12767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of technology, people work hard on image processing and artificial intelligence which require a lot of computer resources. Instead of simply speeding up calculations with processors, people develop the technology of the Cloud with the Internet which performs high-quality calculations and has quite a huge amount of storage. Thus, in this paper, the authors use a kind of encryption methods, Secure Multi-Party Computation (SMPC), to protect the important information which is based on the absence of a trusted party and does not use specific keys or methods to maintain the confidentiality of data. Furthermore, the authors also use the error correction, Berlekamp–Welch (BW) algorithm, to double-check the correction of the information. To apply the two algorithms to image enhancement and convolution neural networks (CNN), the authors will also discuss the homomorphism in all operations, including addition, multiplication, division etc., and find the best way to encrypt and decrypt the information. Finally, the authors will implement the system with cloud computing to decrease the consumption of computer resources and build a system that can do image enhancement and CNN.},
  archive      = {J_IETIP},
  author       = {Teh-Lu Liao and Chiau-Yuan Peng and Yi-You Hou},
  doi          = {10.1049/ipr2.12767},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1931-1950},
  shortjournal = {IET Image Process.},
  title        = {Application of multi-party computation and error correction with image enhancement and convolution neural networks based on cloud computing},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An accurate shared bicycle detection network based on
faster r-CNN. <em>IETIP</em>, <em>17</em>(6), 1919–1930. (<a
href="https://doi.org/10.1049/ipr2.12766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting shared bicycles is an essential and challenging task. Deep learning has been widely used in object detection tasks in urban scenes, such as vehicle detection. However, deep learning algorithms still face many difficulties and challenges in shared bicycle detection. For example, the problem of large deformation of shared bicycles and the problem of small targets because the camera is far away from the shared bicycles. In order to solve these problems, this study introduces the feature fusion module and deformable convolution into the object detection network, which improves the efficiency of shared bicycle detection. This study proposes an enhanced faster R-CNN network (A classic two-stage object detection network) for shared bicycle detection and a shared bicycle dataset (SBD) is constructed for model training and testing. Compared with the original faster R-CNN, the mean average precision (mAP) of the enhanced method on SBD is improved by 13%, which indicates that the method provided in this study is more suitable for detecting shared bicycles. This study also conducts experiments on the Microsoft Common Objects (COCO) dataset, where this method achieves 40.2% of the mAP, which is 5.8% higher than faster R-CNN before improvement.},
  archive      = {J_IETIP},
  author       = {Lingqiao Li and Xiangkai Wang and Mengyu Yang and Hongwei Zhang},
  doi          = {10.1049/ipr2.12766},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1919-1930},
  shortjournal = {IET Image Process.},
  title        = {An accurate shared bicycle detection network based on faster R-CNN},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A video drowning detection device based on underwater
computer vision. <em>IETIP</em>, <em>17</em>(6), 1905–1918. (<a
href="https://doi.org/10.1049/ipr2.12765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drowning is a significant public health concern. A video drowning detection algorithm is a helpful tool for finding drowning victims. However, there are three challenges that drowning detection research typically encounters: a lack of actual drowning video data, subtle early drowning traits, and a lack of real time. In this paper, the authors propose an underwater computer vision based drowning detection device composed of embedded AI devices, camera, and waterproof case to solve the above problems. The detection device utilizes the high-performance computing of Jetson Nano to realize real-time detection of drowning events through the proposed drowning detection algorithm on the acquired underwater video stream. The proposed drowning detection algorithm primarily consists of two stages: in the first step, to successfully solve the interference of the surroundings and to give a trustworthy basis for video drowning detection, the YOLOv5n network is used to detect the near-vertical human body based on the characteristics of the drowning person. In the second stage, the authors propose a lightweight drowning detection network (DDN) based on a deep Gaussian model for fast feature vector detection. The lightweight DDN is combined with the Gaussian model to detect anomaly in the high-level semantic features, which has higher robustness and solves the lack of drowning videos. The experimental results show that the proposed drowning detection algorithm has good comprehensive performance and practical application value.},
  archive      = {J_IETIP},
  author       = {Tingzhuang Liu and Xinyu He and Linglu He and Fei Yuan},
  doi          = {10.1049/ipr2.12765},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1905-1918},
  shortjournal = {IET Image Process.},
  title        = {A video drowning detection device based on underwater computer vision},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geo-information mapping improves canny edge detection
method. <em>IETIP</em>, <em>17</em>(6), 1893–1904. (<a
href="https://doi.org/10.1049/ipr2.12764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the shortcomings of the current Canny edge detection method in terms of noise removal, threshold setting, and edge recognition, this paper proposes a method for improving Canny edge detection by geo-information mapping. The shortcomings of the traditional Canny edge detection method are analyzed by using the Canny optimal criterion and Tobler&#39;s First Law, which points out the direction of edge detection optimization by using the difference between edge properties and noise properties. The property characteristics and spatial distribution rules of edge points and edge lines are inspected using the geographic information mapping theory and technical methods, and edge identification criteria are defined at two levels of edge points and edge lines. Finally, the method model of improved Canny edge detection is constructed by combining guided filtering. The experimental results show that the improved edge detection method has the advantages of enriched edge details, accurate edge recognition, and strong self-adaptive capability. This is a new attempt of geo-information mapping theory and technical method in image edge detection, which has certain theoretical significance and strong practical guidance.},
  archive      = {J_IETIP},
  author       = {Yang Lijun and Li Mengbo and Wu Tongxin and Bao Youfeng and Li Junhui and Jiang Yi},
  doi          = {10.1049/ipr2.12764},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1893-1904},
  shortjournal = {IET Image Process.},
  title        = {Geo-information mapping improves canny edge detection method},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight image splicing tampering localization method
based on MobileNetV2 and SRM. <em>IETIP</em>, <em>17</em>(6), 1883–1892.
(<a href="https://doi.org/10.1049/ipr2.12763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The architectures of many state-of-the-art local tempering detection models are complexity, and the training process of those models is also time-consuming. Therefore, this paper constructs a lightweight local tampering detection method based on the convolutional network MobileNetV2 and a dual-stream network. Specifically, the algorithm first improves the MobileNetv2, which not only reduces the multiple of its downsampling operator to retain richer traces of image tampering, but also introduces the dilated convolution in it to expand the receptive field of feature maps. The dual-stream network uses RGB stream to extract image tampering features such as strong contrast difference and unnatural tampered boundaries, and implements spatial rich model (SRM) stream to extract image tampered area and noise features of real area. Finally, the features extracted from two streams are fused through an improved attention mechanism called parallel convolutional block attention module (CBAM), which can improve the sensitivity of the model to important features in RGB and SRM. The experimental results show that the proposed algorithm still has higher positioning accuracy than some existing algorithms, while achieving lightweight.},
  archive      = {J_IETIP},
  author       = {Xiaoqian Shi and Ping Li and Hao Wu and Qidong Chen and Haoyu Zhu},
  doi          = {10.1049/ipr2.12763},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1883-1892},
  shortjournal = {IET Image Process.},
  title        = {A lightweight image splicing tampering localization method based on MobileNetV2 and SRM},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransMVU: Multi-view 2D u-nets with transformer for brain
tumour segmentation. <em>IETIP</em>, <em>17</em>(6), 1874–1882. (<a
href="https://doi.org/10.1049/ipr2.12762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation remains particularly challenging for complex and low-contrast anatomical structures, especially in brain MRI glioma segmentation. Gliomas appear with extensive heterogeneity in appearance and location on brain MR images, making robust tumour segmentation extremely challenging and leads to highly variable even in manual segmentation. U-Net has become the de facto standard in medical image segmentation tasks with great success. Previous researches have proposed various U-Net-based 2D Convolutional Neural Networks (2D-CNN) and their 3D variants, called 3D-CNN-based architectures, for capturing contextual information. However, U-Net often has limitations in explicitly modelling long-term dependencies due to the inherent locality of convolution operations. Inspired by the recent success of natural language processing transformers in long-range sequence learning, a multi-view 2D U-Nets with transformer (TransMVU) method is proposed, which combines the advantages of transformer and 2D U-Net. On the one hand, the transformer encodes the tokenized image patches in the CNN feature map into an input sequence for extracting global context for global feature modelling. On the other hand, multi-view 2D U-Nets can provide accurate segmentation with fewer parameters than 3D networks. Experimental results on the BraTS20 dataset demonstrate that our model outperforms state-of-the-art 2D models and classic 3D model.},
  archive      = {J_IETIP},
  author       = {Zengxin Liu and Caiwen Ma and Wenji She and Xuan Wang},
  doi          = {10.1049/ipr2.12762},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1874-1882},
  shortjournal = {IET Image Process.},
  title        = {TransMVU: Multi-view 2D U-nets with transformer for brain tumour segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finger vein recognition based on lightweight convolutional
attention model. <em>IETIP</em>, <em>17</em>(6), 1864–1873. (<a
href="https://doi.org/10.1049/ipr2.12761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are a hot research topic in finger vein recognition. However, most existing research focuses on increasing the depth and width of the convolutional neural network to improve the network&#39;s performance, which has a specific requirement on the computer&#39;s computational power. To reduce the computational burden, a lightweight convolutional attention model (LCAModel) is proposed for finger vein recognition to achieve more accurate visual structure capture by exploiting the sensitivity of the attention mechanism to features. First, the attentional model (AModel) is proposed to extract representative features of images, which mainly utilizes the adaptive mapping on space and channels of the convolutional block attention module (CBAM) to make features distinguishable. In addition, considering the integrity of the features, a convolutional model (CModel) is designed to supplement the features of AModel. Finally, the features obtained from AModel and CModel are fused using an adaptive weighting mechanism to make the features more complete. Here, the obtained features are provided into a support vector machine (SVM) for image classification. The experiments are carried out on two publicly available databases, demonstrating that the proposed network structure requires less computing power and performs better.},
  archive      = {J_IETIP},
  author       = {Zhongxia Zhang and Mingwen Wang},
  doi          = {10.1049/ipr2.12761},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1864-1873},
  shortjournal = {IET Image Process.},
  title        = {Finger vein recognition based on lightweight convolutional attention model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An improved generative adversarial network for remote
sensing image super-resolution. <em>IETIP</em>, <em>17</em>(6),
1852–1863. (<a href="https://doi.org/10.1049/ipr2.12760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial resolution is an important indicator that measures the quality of remote sensing images. Image texture has been successfully recovered by generative adversarial networks in deep learning super-resolution (SR) methods. However, the existing methods are prone to image texture distortion. To solve the above problems, this paper proposes an improved generative adversarial network to enhance the super-resolution reconstruction effect of medium- and low-resolution (LR) remote sensing images. This network is based on the Super Resolution Generative Adversarial Network (SRGAN), which makes great improvements in the structure of the connection between the inside and outside of the residual block and the design of the model loss function. At the same time, the G1–G2–G3 structure between residuals effectively combines the image information of the three scales of small, medium and large. The model loss function can be designed based on the Charbonnier loss function to narrow the pixel distance between the reconstructed remote sensing image and the original image. Furthermore, targeted perceptual loss can direct the network to restore the texture details of the image according to the semantic category. The subjective and objective evaluation of the generated images and the ablation experiments prove that compared with SRGAN and other networks, our method can generate more realistic and reliable textures. Additionally, the indicators [peak signal-to-noise ratio (PSNR), structural similarity (SSIM), multiscale structural similarity (MS-SSIM)] used to measure the quality of the reconstructed image obtain improved objective quantitative evaluation.},
  archive      = {J_IETIP},
  author       = {Jifeng Guo and Feicai Lv and Jiayou Shen and Jing Liu and Mingzhi Wang},
  doi          = {10.1049/ipr2.12760},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1852-1863},
  shortjournal = {IET Image Process.},
  title        = {An improved generative adversarial network for remote sensing image super-resolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A serial attention module-based deep convolutional neural
network for mixed gaussian-impulse removal. <em>IETIP</em>,
<em>17</em>(6), 1837–1851. (<a
href="https://doi.org/10.1049/ipr2.12759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The removal of mixed noise is a challenging task because the attenuation of the noise distribution cannot be described precisely. The coupling of additive white Gaussian noise and impulse noise (IN) is a typical case. At present, most methods use a two-phase strategy, that is, IN detection coupled with additive white Gaussian noise removal, often leading to poor denoising results with an increase in the ratio of IN. In this paper, an effective convolutional neural network (CNN) model is proposed, namely a serial attention module-based CNN (SACNN), for mixed noise removal. In contrast to the existing two-phase methods, SACNN unifies the denoising process into a single CNN framework. In SACNN, residual learning and batch normalization are used to train the model, which speeds up the convergence and improves the mixed noise removal performance. Meanwhile, the serial attention module is applied to better preserve the texture details. The experimental results reveal that SACNN achieves superior quality metrics and visual appearance when compared to several leading approaches.},
  archive      = {J_IETIP},
  author       = {Jielin Jiang and Kang Yang and Xiaolong Xu and Yan Cui},
  doi          = {10.1049/ipr2.12759},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1837-1851},
  shortjournal = {IET Image Process.},
  title        = {A serial attention module-based deep convolutional neural network for mixed gaussian-impulse removal},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-light image enhancement via span correction function and
discrete mapping model. <em>IETIP</em>, <em>17</em>(6), 1812–1836. (<a
href="https://doi.org/10.1049/ipr2.12757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new low-light image enhancement method, which we call the Local Discrete Mapping Method. The new method limits the processing range to small areas with a high information relevance, which can better coordinate the enhancement quality of each area. First, the discrete mapping relationship of pixels (called discrete mapping points) globally occupying a small part of the critical gray value was extracted and designated to keep the enhancement amplitude of each local area consistent. Then, other free mapping points were adjusted according to the local features to achieve the best visual effect in each local area. In addition, this paper also proposes a span correction function that takes the gray span between local pixels as the adjustment object. The function can preserve the gray difference between freely mapped pixels to the maximum and significantly reduce detailed damage in the local area. Finally, we used 1500 test images and eleven objective evaluation indicators in the public dataset to comprehensively test the seven methods. The experimental results showed that the proposed method has an excellent dark area quality enhancement, brightness detail protection, overall noise suppression, and processing speed. It is significantly better than similar methods in terms of visual quality and quantitative testing.},
  archive      = {J_IETIP},
  author       = {Lei He and Shouxin Liu and Wei Long and Yanyan Li},
  doi          = {10.1049/ipr2.12757},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1812-1836},
  shortjournal = {IET Image Process.},
  title        = {Low-light image enhancement via span correction function and discrete mapping model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A light-weight stereo matching network based on multi-scale
features fusion and robust disparity refinement. <em>IETIP</em>,
<em>17</em>(6), 1797–1811. (<a
href="https://doi.org/10.1049/ipr2.12756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional-neural-network based stereo matching methods have achieved significant gains compared to conventional methods in terms of both speed and accuracy. Current state-of-the-art disparity estimation algorithms require many parameters and large amounts of computational resources and are not suited for applications on edge devices. In this paper, an end-to-end light-weight network (LWNet) for fast stereo matching is proposed, which consists of an efficient backbone with multi-scale feature fusion for feature extraction, a 3D U-Net aggregation architecture for disparity computation, and color guidance in a 2D convolutional neural network (CNN) for disparity refinement. MobileNetV2 is adopted as an efficient backbone in feature extraction. The channel attention module is applied to improve the representational capacity of features and multi-resolution information is adaptively incorporated into the cost volume via cross-scale connections. Further, a left-right consistency check and color guidance refinement are introduced and a robust disparity refinement network is designed with skip connections and dilated convolutions to capture global context information and improve disparity estimation accuracy with little computational cost and memory space. Extensive experiments on Scene Flow, KITTI 2015, and KITTI 2012 demonstrate that the proposed LWNet achieves competitive accuracy and speed when compared with state-of-the-art stereo matching methods.},
  archive      = {J_IETIP},
  author       = {Xiaowei Yang and Yong Zhao and Zhiguo Feng and Haiwei Sang and Zhenbo Zhang and Guiying Zhang and Lin He},
  doi          = {10.1049/ipr2.12756},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1797-1811},
  shortjournal = {IET Image Process.},
  title        = {A light-weight stereo matching network based on multi-scale features fusion and robust disparity refinement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BESRGAN: Boundary equilibrium face super-resolution
generative adversarial networks. <em>IETIP</em>, <em>17</em>(6),
1784–1796. (<a href="https://doi.org/10.1049/ipr2.12755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Generative Adversarial Networks (GAN)-based face hallucination algorithms are hard to control the face fidelity of the generated samples, and easily generate flawed faces with unfavourable artefacts and distortions. To address this problem, the authors propose a fidelity-controllable face super-resolution (FSR) network boundary equilibrium face super-resolution generative adversarial networks (BESRGAN), a fidelity ratio is introduced in their network to control how much the adversarial effect the discriminator is put on the generator; therefore, the authors’ network better trades off the objective and perceptual quality. Additionally, the authors design an equilibrium perceptual discriminator to match the perception loss distributions. Under the equilibrium constraint, the discriminator pays more attention to learning fine-grained feature statistics of ground truths, and further guides the generator to produce photo-realistic faces, especially in terms of facial textures. Moreover, the authors propose a novel channel-spatial attention module (CSAM) to eliminate local distortions, by further fusing richer information from the facial prior knowledge and global high-level facial descriptions. Extensive experiments illustrate that the authors’ approach preserves high pixel-wise accuracy while achieving superior visual performance against state-of-the-art methods. Specifically, the peak signal to noise ratio (PSNR) and structural similarity index (SSIM) of the authors’ proposed BESRGAN rise 0.64 dB and 0.02 for CelebA compared with one of the state-of-the-art face super-resolution (FSR) methods.},
  archive      = {J_IETIP},
  author       = {Xinyi Ren and Qiang Hui and Xingke Zhao and Jianping Xiong and Jun Yin},
  doi          = {10.1049/ipr2.12755},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1784-1796},
  shortjournal = {IET Image Process.},
  title        = {BESRGAN: Boundary equilibrium face super-resolution generative adversarial networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal cooperative self-attention network for action
recognition. <em>IETIP</em>, <em>17</em>(6), 1775–1783. (<a
href="https://doi.org/10.1049/ipr2.12754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal human behaviour recognition is a research hotspot in computer vision. To fully use both skeleton and depth data, this paper constructs a new multimodal network identification scheme combined with the self-attention mechanism. The system comprises a transformer-based skeleton self-attention subnetwork and a depth self-attention subnetwork based on CNN. In the skeleton self-attention subnetwork, this paper proposes a motion synergy space feature that can integrate the information of each joint point according to the entirety and synergy of human motion and puts forward a quantitative standard for the contribution degree of each joint motion. In this paper, the results from the skeleton self-attention subnetwork and the depth self-attention subnetwork are integrated and they are verified on the NTU RGB+D and UTD-MHAD datasets. The authors have achieved 90% recognition rate on UTD-MHAD dataset, and the CS recognition rate of the authors’ method on the NTU RGB+D dataset reaches 90.5% and the recognition rate of CV is 94.7%. Experimental results show that the network structure proposed in this paper achieves a high recognition rate, and its performance is better than most current methods.},
  archive      = {J_IETIP},
  author       = {Zhuokun Zhong and Zhenjie Hou and Jiuzhen Liang and En Lin and Haiyong Shi},
  doi          = {10.1049/ipr2.12754},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1775-1783},
  shortjournal = {IET Image Process.},
  title        = {Multimodal cooperative self-attention network for action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrared object detection via patch-tensor model and image
denoising based on weighted truncated schatten-p norm minimization.
<em>IETIP</em>, <em>17</em>(6), 1762–1774. (<a
href="https://doi.org/10.1049/ipr2.12753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nuclear norm minimization (NNM) is a special non-convex rank minimization convex relaxation scheme for image denoising and object detection that requires denoising and background subtraction. Considering excessive shrinkage of rank components and equal treatment of different rank components, NNM is extended to the weighted Schatten- p norm minimization (WSNM) with weights assigned to different singular values. In this paper, a multi-channel weighted truncated WSNM model based on the WSNM optimization framework is proposed for RGB colour image denoising. On the basis of different noise intensities and non-local self-similar patches of each channel of the colour image itself, the proposed model is improved significantly by the optimization methods of superposition and truncation. Meanwhile, it can be generalized to the tensor space and employed to the infrared imaging target detection based on the spatial-temporal tensor model for the first time. And the efficient alternating direction multiplier-based algorithms are developed to solve the proposed model and the accuracy of the algorithm is effectively improved by choosing an adaptive threshold. Extensive experiments on real infrared data verified the proposed method state-of-the-arts and effectiveness.},
  archive      = {J_IETIP},
  author       = {Yun Zhu and Chengjian Gong and Shuwen Liu and Zhiyue Yu and Hanzeng Shao and Gaohang Yu},
  doi          = {10.1049/ipr2.12753},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1762-1774},
  shortjournal = {IET Image Process.},
  title        = {Infrared object detection via patch-tensor model and image denoising based on weighted truncated schatten-p norm minimization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Industrial anomaly detection with multiscale autoencoder and
deep feature extractor-based neural network. <em>IETIP</em>,
<em>17</em>(6), 1752–1761. (<a
href="https://doi.org/10.1049/ipr2.12752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the maturity of deep learning image recognition technology and the popularity of automated production lines, deep learning industrial anomaly detection has become an important research topic in recent years. In this study, an anomaly detection model with a multi-scale autoencoder and deep feature extractor is proposed. This model was confirmed to obtain the highest area under the curve (AUC) in 14 of the 17 industrial detection tasks. In addition, the receiver operating characteristic (ROC) curves show that an appropriate threshold of the proposed model exists, which can achieve a low false-positive rate and maintain a high true-positive rate. Furthermore, the influence of different feature extractors on the method is discussed. It was shown that the proposed method can maintain good detection ability with most of the feature extractor. Therefore, it is suitable for industrial optical inspection systems with different hardware conditions.},
  archive      = {J_IETIP},
  author       = {Ta-Wei Tang and Hakiem Hsu and Kuan-Ming Li},
  doi          = {10.1049/ipr2.12752},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1752-1761},
  shortjournal = {IET Image Process.},
  title        = {Industrial anomaly detection with multiscale autoencoder and deep feature extractor-based neural network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast additive half-quadratic iterative minimization for lp −
lq image smoothing. <em>IETIP</em>, <em>17</em>(6), 1739–1751. (<a
href="https://doi.org/10.1049/ipr2.12751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications, it is important for the authors to remove insignificant image details while preserving the significant structures. This is the so-called image smoothing problem. In this paper, the authors investigate the image smoothing problem using the l p − l q ${l}_p - {l}_q$ optimization model with 0 &lt; p , q ≤ 1 $0 &amp;lt; p,q \le 1$ . The authors employ the fast additive half-quadratic (AHQ) iterative minimization algorithm for solving the l p − l q ${l}_p - {l}_q$ optimization model. The authors discuss the convergence of the AQH iterative minimization algorithm. Experimental results and comparisons are provided to show the efficiency and flexibility of the proposed method in terms of both qualitative and quantitative evaluations.},
  archive      = {J_IETIP},
  author       = {Xueman Sun and Xiaoguang Lv and Guoliang Zhu and Biao Fang and Le Jiang},
  doi          = {10.1049/ipr2.12751},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1739-1751},
  shortjournal = {IET Image Process.},
  title        = {Fast additive half-quadratic iterative minimization for lp − lq image smoothing},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retinex-inspired contrast stretch and detail boosting for
lowlight image enhancement. <em>IETIP</em>, <em>17</em>(6), 1718–1738.
(<a href="https://doi.org/10.1049/ipr2.12750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lowlight images with low brightness and contrast, blurry details usually bring us an uncomfortable visual experience. To promote the quality of these deviation images, this paper presents a new and efficient approach, named MFMR, for enhancing lowlight images in the hue-saturation-value (HSV) colour space. Concretely, the multi-angle filter is first applied to estimate the artifact-free illumination and reflection component of the V-channel. Afterward, the adaptive bi-interval histogram with human visual characteristics and morphological operations is employed to process the former, adaptive gamma correction to process the latter for generating various feature maps. In the end, these feature maps are united via adaptive multi-scale fusion strategy to reconstruct high-quality images, which are characterized by high contrast and brightness, vivid colour, and clearer details. Extensive experiments show that this method is a well-proven low-light image enhancement approach, which outperforms the state-of-the-art comparison methods. Furthermore, the proposed method also can yield satisfying images in the heavy foggy, yellow sand, underwater, and other severe conditions.},
  archive      = {J_IETIP},
  author       = {Haoxiang Lu and Zhenbing Liu and Rushi Lan and Xipeng Pan and Junming Gong},
  doi          = {10.1049/ipr2.12750},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1718-1738},
  shortjournal = {IET Image Process.},
  title        = {Retinex-inspired contrast stretch and detail boosting for lowlight image enhancement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient hyperspectral image classification method for
limited training data. <em>IETIP</em>, <em>17</em>(6), 1709–1717. (<a
href="https://doi.org/10.1049/ipr2.12749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image classification has gained great progress in recent years based on deep learning model and massive training data. However, it is expensive and unpractical to label hyperspectral image data and implement model in constrained environment. To address this problem, this paper proposes an effective ghost module based spectral network for hyperspectral image classification. First, Ghost3D module is adopted to reduce the size of model parameter dramatically by redundant feature maps generation with linear transformation. Then Ghost2D module with channel-wise attention is used to explore informative spectral feature representation. For large field covering, the non-local operation is utilized to promote self-attention. Compared with the state-of-the-art hyperspectral image classification methods, the proposed approach achieves superior performance on three hyperspectral image data sets with fewer sample labelling and less resource consumption.},
  archive      = {J_IETIP},
  author       = {Yitao Ren and Peiyang Jin and Yiyang Li and Keming Mao},
  doi          = {10.1049/ipr2.12749},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1709-1717},
  shortjournal = {IET Image Process.},
  title        = {An efficient hyperspectral image classification method for limited training data},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time seed sorting system via 2D information
entropy-based CNN pruning and TensorRt acceleration. <em>IETIP</em>,
<em>17</em>(6), 1694–1708. (<a
href="https://doi.org/10.1049/ipr2.12747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seed sorting based on deep neural networks is one of the important applications of seed variety identification and quality purification. However, DNNs is difficult to deploy on embedded devices since the consumption of computational and storage resource. To address these problems, this paper proposes a pipeline-style neural network framework for real-time seed sorting. First, we propose a novel algorithm, 2D information entropy, pruning redundant filters to realize structured pruning. Then, the pruning rate of each convolution layer is determined by visualizing the results of 2D entropy. Meanwhile, the pruned network is fine-tuned to recover the performance. Finally, TensorRT is utilized to optimize and accelerate the pruned model for deployment in Jeston Nano. Experiments on two large-scale seed-sorting datasets demonstrate the significant improvement of the proposed method over existing model compression methods. Experimental results on Jeston Nano show that the pruned model 2EFP-E achieves a single image inference speed of 107 FPS, with the best accuracy of 95.94% on the red kidney bean dataset.},
  archive      = {J_IETIP},
  author       = {Chunlei Li and Huanyu Li and Liang Liao and Zhoufeng Liu and Yan Dong},
  doi          = {10.1049/ipr2.12747},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1694-1708},
  shortjournal = {IET Image Process.},
  title        = {Real-time seed sorting system via 2D information entropy-based CNN pruning and TensorRt acceleration},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fused pyramid attention network for single image
super-resolution. <em>IETIP</em>, <em>17</em>(6), 1681–1693. (<a
href="https://doi.org/10.1049/ipr2.12746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image super-resolution, deep neural networks with various attention mechanisms have achieved noticeable performance in recent years, for example, channel attention and layer attention. Although many researchers have achieved good super-resolution results with only a certain style of attention, the divergence and the complementarity focused by multiple attention mechanisms are ignored. In addition, most of these methods fail to utilize the diverse information from multi-scale features. To efficiently manipulate the above rich information, this paper strives to combine multi-scale structure and multi-attention schemes in architecture and module levels for super-resolution. Especially, in the architecture level, a fused pyramid attention network is developed to extract deep features with the multi-scale context information from multiple different sizes of receptive field recurrently with skip connections. For the module level, a fused pyramid attention module is designed to fuse the two attention mechanisms to further refine the deep features with fine-grained information. Compared with the common fusion strategy, the adopted feature fusion structure can maintain better structural information while establishing long-range dependency. Extensive experimental results demonstrate that the proposed network achieves favorable performance quantitatively and visually.},
  archive      = {J_IETIP},
  author       = {Shi Chen and Xiuping Bi and Lefei Zhang},
  doi          = {10.1049/ipr2.12746},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1681-1693},
  shortjournal = {IET Image Process.},
  title        = {Fused pyramid attention network for single image super-resolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual-attention global domain adaptation for mariculture
image enhancement. <em>IETIP</em>, <em>17</em>(6), 1668–1680. (<a
href="https://doi.org/10.1049/ipr2.12745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mariculture image enhancement aims to recover degraded images and meet the requirements of various digital aquaculture systems. However, the existing underwater image enhancement (UIE) cannot suit diverse marine scenarios and leads to sub-optimal results for the real mariculture images. To solve the aforementioned issues, a novel dual-attention global domain-adaptive mariculture image enhancement network (DAMIE) is proposed to improve the quality of degraded images. Specifically, the proposed method consists of two core parts: (1) an innovative depth transfer dual-attention module to aggregate multiple features and bridge the difference between domains; (2) a modified encoder–decoder enhancement network with a global feature vector to reconstruct clean mariculture images. Meanwhile, a semi-supervised adaptive training scheme is utilized to improve the model generalization in different mariculture domains. Extensive experiments demonstrate that the proposed DAMIE can achieve a good performance in terms of quantitative and qualitative metrics. In addition, an ablation study is conducted to analyse the contribution of the key components in the proposed model.},
  archive      = {J_IETIP},
  author       = {Fei Li and Chaojun Cen and Xinxin Zhang and Zhenbo Li},
  doi          = {10.1049/ipr2.12745},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1668-1680},
  shortjournal = {IET Image Process.},
  title        = {Dual-attention global domain adaptation for mariculture image enhancement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Segmenting lung parenchyma from CT images with gray
correlation-based clustering. <em>IETIP</em>, <em>17</em>(6), 1658–1667.
(<a href="https://doi.org/10.1049/ipr2.12744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung segmentation, a prerequisite step of lung disease detection in computer-aided diagnosis system, is a challenging task because of noises, complex structures, as well as large individual differences of lung CT scans. Here, an automatic algorithm for segmenting lungs from thoracic CT images accurately is presented. This scheme consists of three principal steps: image preprocessing, lung extracting and contour correcting. To cope with inhomogeneous intensities of CT images, a novel preprocessing approach based on empirical mode decomposition and bilateral filter is proposed, which has abilities of denoising, smoothing and edge keeping. Lung region is then extracted with a novel gray correlation-based clustering approach. A new lung contour correction technology is finally employed to repair the concave regions caused by pulmonary nodules, vessels and so on. Experimental results show that the preprocessing approach outperforms other methods on image denoising and smoothing. Meanwhile, the lung segmentation algorithm is tested on a group of lung CT images affected with interstitial lung diseases and achieves a high segmentation accuracy. Compared with several existing lung segmentation methods, this algorithm exhibits a better performance on lung segmentation.},
  archive      = {J_IETIP},
  author       = {Caixia Liu and Wanli Xie and Ruibin Zhao and Mingyong Pang},
  doi          = {10.1049/ipr2.12744},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1658-1667},
  shortjournal = {IET Image Process.},
  title        = {Segmenting lung parenchyma from CT images with gray correlation-based clustering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Recent progress in image denoising: A training strategy
perspective. <em>IETIP</em>, <em>17</em>(6), 1627–1657. (<a
href="https://doi.org/10.1049/ipr2.12748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is one of the hottest topics in image restoration area, it has achieved great progress both in terms of quantity and quality in recent years, especially after the wide and intensive application of deep neural networks. In many deep learning based image denoising models, the performance can greatly benefit from the prepared clean/noisy image pairs used for model training, however, it also limits the application of these models in real denoising scenes. Therefore, more and more researchers tend to develop models that can be learned without image pairs, namely the denoising models that can be well generalised in real-world denoising tasks. This motivates to make a survey on the recent development of image denoising methods. In this paper, the typical denoising methods from the perspective of model training are reviewed, the reviewed methods are categorised into four classes: the models need clean/noisy image pairs to train, the models trained on multiple noisy images, the models can be learned from a single noisy image, and the visual transformer based models. The denoising results of different denoisers were compared on some public datasets to discover the performance and advantages. The challenges and future directions in image denoising area are also discussed.},
  archive      = {J_IETIP},
  author       = {Wencong Wu and Mingfei Chen and Yu Xiang and Yungang Zhang and Yang Yang},
  doi          = {10.1049/ipr2.12748},
  journal      = {IET Image Processing},
  month        = {5},
  number       = {6},
  pages        = {1627-1657},
  shortjournal = {IET Image Process.},
  title        = {Recent progress in image denoising: A training strategy perspective},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Medical image blind super-resolution based on improved
degradation process. <em>IETIP</em>, <em>17</em>(5), 1615–1625. (<a
href="https://doi.org/10.1049/ipr2.12742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical diagnosis has high requirements for the resolution of medical images, but most existing medical images super- resolution (SR) methods are performed under a known or specific degradation kernel. However, the difference between the actual degradations and their assumed degradation kernels results in a severe performance drop for the advanced SR methods in real applications. This paper proposes a medical image blind super-resolution model (Med-BSR) based on an improved degradation process to handle this issue. The model makes each of the degradation factors in medical image blind SR, such as blur, noise, and downsampling, more complex and practical. Specifically, the authors use the random select/combine strategy to randomly arrange and combine the type and order of each degradation factor, which significantly expands the degradation space. The authors also improved the loss function of the primary enhanced super-resolution generative adversarial networks (ESRGAN) network. The extensive experimental results demonstrate that the authors’ designed model can accurately restore the natural degradation process, which can reconstruct high-quality SR medical images. It also has a good generalization ability to realistic images simultaneously.},
  archive      = {J_IETIP},
  author       = {Dangguo Shao and Li Qin and Yan Xiang and Lei Ma and Hui Xu},
  doi          = {10.1049/ipr2.12742},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1615-1625},
  shortjournal = {IET Image Process.},
  title        = {Medical image blind super-resolution based on improved degradation process},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAGCN: Generative adversarial graph convolutional network
for non-homogeneous texture extension synthesis. <em>IETIP</em>,
<em>17</em>(5), 1603–1614. (<a
href="https://doi.org/10.1049/ipr2.12741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the non-homogeneous texture synthesis task, the overall visual characteristics should be consistent when extending the local patterns of the exemplar. The existing methods mainly focus on the local visual features of patterns but ignore the relative position features that are important for non-homogeneous texture synthesis. Although these methods have achieved success on homogeneous textures, they cannot perform well on non-homogeneous textures. Thus, it is desirable to model the dependence between pixels to improve the synthesis performance. To ensure synthesis results from both the local detail structure and the overall structure, this paper proposes a non-homogeneous texture extended synthesis model (GAGCN) combining the generate adversarial network (GAN) and the graph convolutional network (GCN). The GAN learns the internal distribution of image patches, which makes the synthetic image have rich local details. The GCN learns the latent dependence between pixels according to the statistical characteristics of the image. Based on this, a novel graph similarity loss is proposed. This loss describes the latent spatial differences between the sample image and the generated image, which helps the model to better capture global features. Experiments show that our method outperforms existing methods on non-homogeneous textures.},
  archive      = {J_IETIP},
  author       = {Shasha Xie and Wenhua Qian and Rencan Nie and Dan Xu and Jinde Cao},
  doi          = {10.1049/ipr2.12741},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1603-1614},
  shortjournal = {IET Image Process.},
  title        = {GAGCN: Generative adversarial graph convolutional network for non-homogeneous texture extension synthesis},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-feature fusion-based strabismus detection for
children. <em>IETIP</em>, <em>17</em>(5), 1590–1602. (<a
href="https://doi.org/10.1049/ipr2.12740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strabismus is a common ophthalmologic disease that affects approximately 1.19% to 5.0% of children; however if the disease is detected early it can be treated effectively. Generally, the automatic detection of strabismus is usually performed only by a single feature, which is, with image deep features or ratio features. However, the accuracy of a strabismus diagnosis with a single feature is unreliable. This study aims to develop an intelligent strabismus detection model driven by corneal light reflection photographs to automatically detect children&#39;s strabismus. The proposed multi-feature fusion model (MFFM) improves the detection performance by fusing the deep features and ratio features extracted from the corneal light reflection photographs to identify strabismus. The experimental results demonstrate that our proposed multi-feature model outperforms all of the single feature models in strabismus detection. The experiments show that the proposed method achieves an accuracy of 97.17%, sensitivity of 96.06%, specificity of 97.79%, and AUC of 0.969 in strabismus detection. Our evidence shows that it greatly improves the performance of strabismus detection.},
  archive      = {J_IETIP},
  author       = {Guiying Zhang and Wenjing Xu and Haotian Gong and Lilei Sun and Cong Li and Huicong Chen and Daoman Xiang},
  doi          = {10.1049/ipr2.12740},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1590-1602},
  shortjournal = {IET Image Process.},
  title        = {Multi-feature fusion-based strabismus detection for children},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GOMT: Multispectral video tracking based on genetic
optimization and multi-features integration. <em>IETIP</em>,
<em>17</em>(5), 1578–1589. (<a
href="https://doi.org/10.1049/ipr2.12739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional visual tracking algorithms on RGB cannot effectively distinguish an object from background with similar colour feature, resulting in the migration problem in the tracking process. In order to solve this problem, a tracking algorithm with spectral information is proposed by using band selection, information fusion, and deep features. Firstly, the correlation analysis of 16 spectral band information is carried out, and genetic optimization method is introduced to obtain two bands with low correlation coefficient and abundant information. Moreover, under the framework of kernel correlation filtering tracking, guided filtering is adopted to fuse the information referring to the target region of the two bands. Besides, the feature maps are generated by histogram of gradient and pretrained visual geometry group network. Finally, target is detected via finding the maximum value of a strong response. The proposed genetic optimization based multifeature tracker is compared with multiple tracking methods including correlation filtering and deep learning. Experimental results with multiple groups of spectral videos and corresponding RGB videos demonstrate that the genetic optimization based multifeature tracker method achieves good results in subjective vision and objective evaluation.},
  archive      = {J_IETIP},
  author       = {Kun Qian and Peng Chen and Dong Zhao},
  doi          = {10.1049/ipr2.12739},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1578-1589},
  shortjournal = {IET Image Process.},
  title        = {GOMT: Multispectral video tracking based on genetic optimization and multi-features integration},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A DCA-based sparse coding for video summarization with MCP.
<em>IETIP</em>, <em>17</em>(5), 1564–1577. (<a
href="https://doi.org/10.1049/ipr2.12738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video summarization offers a summary version that conveys the primary information of a longer video. The main challenges of video summarization are related to keyframe extraction and saliency mapping. Thus, this work proposes a sparse coding model for keyframe extraction and saliency mapping applications. Specifically, the minimax concave penalty (MCP) is utilized as a sparse regularization scheme and the regularized non-convex MCP problem is solved by decomposing MCP into two convex functions and the convex function&#39;s algorithm difference is relied on to solve the resulting sub-problems. The experimental results demonstrate higher compressed keyframes and saliency maps than current state-of-the-art algorithms. In particular, the model attains a lower summary length of 34% and 19% compared to sparse modeling representation selection (SMRS) and sparse modeling using the determinant sparsity measure (SC-det), respectively. In addition, the developed scheme has a shorter computation time, requiring 82% and 33% less time than the ITTI and the dense and sparse reconstruction (DSR) methods.},
  archive      = {J_IETIP},
  author       = {Yujie Li and Zhenni Li and Benying Tan and Shuxue Ding},
  doi          = {10.1049/ipr2.12738},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1564-1577},
  shortjournal = {IET Image Process.},
  title        = {A DCA-based sparse coding for video summarization with MCP},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced three-dimensional model reconstruction based on
local ternary pattern-guided fusion of multi-exposure images.
<em>IETIP</em>, <em>17</em>(5), 1546–1563. (<a
href="https://doi.org/10.1049/ipr2.12737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision applications usually rely on the features extracted from input images with good visibility. Image acquisition systems may produce degraded images with low contrast or distorted colours. For instance, bad weather (haze, fog) can cause images captured outdoor with low visibility. Image processing algorithms generally assume that the input image is the scene radiance. Haze removal, with the recovery of image radiance, ensures reliable features extracted from images and the image processing algorithm can achieve optimal performance. Inspired by the concept of image dehazing, the authors propose an image enhancement method that can be used to improve the visibility of the images. Each original image is first transformed into multiple exposure images by means of gamma-correction operations and adaptive histogram equalization. The transformed images are analyzed by the computation of the local ternary pattern. The image is then enhanced, with each pixel generated from the set of transformed image pixels weighted by a function of the local pattern feature. The authors evaluate their proposed method on four benchmark image dehazing datasets. The quantitative results show that our method outperforms many deterministic algorithms and deep learning models. Moreover, the authors investigate the impact of image enhancement on a practical image-based application—the reconstruction of three-dimensional (3D) model of survey scene. Accurate 3D model reconstruction depends on high-quality images. Degraded images will result in large errors in the reconstructed 3D model. Experimentations have been carried out on outdoor and indoor surveys. Our analysis finds that when fed into the photogrammetry software, the images enhanced by the authors’ method can reconstruct 3D scene models with sub-millimetre mean errors, which are much better than those with the original images. As shown in the visual and quantitative results of 3D model reconstruction, the authors’ proposed method also outperforms other image enhancement methods.},
  archive      = {J_IETIP},
  author       = {Kwok-Leung Chan and Liping Li and Arthur Wing-Tak Leung and Ho-Yin Chan},
  doi          = {10.1049/ipr2.12737},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1546-1563},
  shortjournal = {IET Image Process.},
  title        = {Enhanced three-dimensional model reconstruction based on local ternary pattern-guided fusion of multi-exposure images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Analysis of lung scan imaging using deep multi-task
learning structure for covid-19 disease. <em>IETIP</em>, <em>17</em>(5),
1534–1545. (<a href="https://doi.org/10.1049/ipr2.12736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covid-19 caused by the SARS-CoV2 virus has become a pandemic all over the world. By growing in a number of cases, there is a need for clinical decision-making system based on machine learning models. Most of the previous studies have examined only one task, while the detection and identification of infectious area are conducted simultaneously in the real world. Thus, the present study aims to propose a multi-task model which can perform automatic classification-segmentation for screening Covid-19 pneumonia by using chest CT imaging. This model includes a common encoder for feature representation, one decoder for segmentation, and a multi-layer perceptron for classification, respectively. The proposed model can evaluate three datasets, along with the effect of images size on the output of the model. The outputs were examined in both multi-task and single-task learning. The result indicates that the effect of multi-task is significant in improving the results, which can increase the outputs of each task performance to 95.40% accuracy in classification and 95.40% in segmentation. Further, the model represented the highest results among the state-of-the-art methods. The proposed model can be applied as a primary screening tool to help primary service staff in better referral of the suspected patients to specialists.},
  archive      = {J_IETIP},
  author       = {Shirin Kordnoori and Malihe Sabeti and Hamidreza Mostafaei and Saeed Seyed Agha Banihashemi},
  doi          = {10.1049/ipr2.12736},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1534-1545},
  shortjournal = {IET Image Process.},
  title        = {Analysis of lung scan imaging using deep multi-task learning structure for covid-19 disease},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small insulator target detection based on multi-feature
fusion. <em>IETIP</em>, <em>17</em>(5), 1520–1533. (<a
href="https://doi.org/10.1049/ipr2.12735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proportion of insulators in aerial power patrol images is small and the background of overhead lines is complex, often leading to incomplete and inaccurate detection of insulators. Therefore, an algorithm for detecting insulator targets based on multi-feature fusion is developed in this study. Firstly, a dynamic threshold oriented fast and rotated brief algorithm is proposed, which uses the bag-of-words dictionary model to determine local shape features of the image, applies gradient weighting to the global texture feature vector extracted by the histogram of oriented gradients algorithm and performs radial gradient transformations to get the improved HOG of features. Secondly, the feature vectors are fused serially, the learning machine is trained and the parameters of the support vector machine are optimized using the quantum particle swarm optimization algorithm. Finally, the target area is pre-divided by the selective search algorithm, and the area is classified by the learning machine. The experimental results show that the proposed feature extraction method can describe the image details more accurately than the existing methods, and the average accuracy of the feature extraction classifier can reach 93.7%, which helps to overcome the incomplete detection problem of insulator detection at the aerial work site.},
  archive      = {J_IETIP},
  author       = {Minan Tang and Kai Liang and Jiandong Qiu},
  doi          = {10.1049/ipr2.12735},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1520-1533},
  shortjournal = {IET Image Process.},
  title        = {Small insulator target detection based on multi-feature fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A xanthoceras sorbifolium crack segmentation method based
on an improved level set. <em>IETIP</em>, <em>17</em>(5), 1510–1519. (<a
href="https://doi.org/10.1049/ipr2.12734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dehiscence of the Xanthoceras sorbifolium (X. sorbifolium) may lead to seeds jump out and economic loss. The shape and the degree of the crack will provide the relevant elements for the study of the X. sorbifolium dehiscence and duly picked. Therefore, an improved level set method is proposed for X. sorbifolium crack segmentation. The problems of intensity inhomogeneity and so on, which pose challenges for accurate crack segmentation. The local Gaussian distribution fitting method has a good segmentation effect, but it is sensitive to the initial contour and does not use gradient information, which affects the accurate location of the edge. Aiming at the above problems and the scene of crack segmentation, this paper firstly adopts histogram threshold method to obtain the initial contour automatically. Secondly, the energy function is constructed by combining local and edge information. Finally, the double-well potential function is used to reduce the oscillation and distortion of the method. In this paper, the experiment results show that the average boundary precision is 88.25% and average segmentation time of each image is 12.7s of the proposed method. After comprehensive analysis of IoU and boundary recall, the method in this paper achieves better results.},
  archive      = {J_IETIP},
  author       = {Dan Zhang and Tieshan Li and C.L. Philip Chen and Li Wang},
  doi          = {10.1049/ipr2.12734},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1510-1519},
  shortjournal = {IET Image Process.},
  title        = {A xanthoceras sorbifolium crack segmentation method based on an improved level set},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint transformer progressive self-calibration network for
low light enhancement. <em>IETIP</em>, <em>17</em>(5), 1493–1509. (<a
href="https://doi.org/10.1049/ipr2.12732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the lighting conditions are poor and the environmental light is weak, the image captured by the imaging device often has lower brightness and is accompanied by a lot of noise. The paper designs a progressive self-calibration network model (PSCNet) for recovering high-quality low-light-enhanced images. First, shallow features in low-light images can be better focused and extracted with the help of attention mechanism. Next, the feature mapping is passed to the encoder and decoder modules, where the transformer and encoder-decoder jump connection structures can be better combined with the semantic information of the context to learn rich deep feature information. Finally, the self-calibration module can adaptively cascade the features decoded by the decoder and input them into the residual attention module quickly and accurately. Meanwhile, the LBP features of the image are also fused into the feature information of the residual attention module to enhance the detailed texture information of the image. Qualitative analysis and quantitative comparison of a large number of experimental results show that this method outperforms existing methods.},
  archive      = {J_IETIP},
  author       = {Junyu Fan and Jinjiang Li and Zhen Hua and Linwei Fan},
  doi          = {10.1049/ipr2.12732},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1493-1509},
  shortjournal = {IET Image Process.},
  title        = {Joint transformer progressive self-calibration network for low light enhancement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust image compression-encryption via scrambled block
bernoulli sampling with diffusion noise. <em>IETIP</em>, <em>17</em>(5),
1478–1492. (<a href="https://doi.org/10.1049/ipr2.12731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed an image compression-encryption scheme based on compressive sensing theory, which achieves high security, strong robustness, and high rate-distortion performance. First, the denoising preprocessing strategy is applied at the encoder side, which can enhance the rate-distortion performance without sacrificing security and robustness. Second, the preprocessed image is randomly down-sampled using scrambled block Bernoulli sampling with diffusion noise (SBBS-DN), which is generated by combining a hyper-chaotic system and SHA256 hash of the plain image. Third, a deep-learned plug-and-play is embedded prior for plain image reconstruction at the decoder side. Simulation results show that the proposed scheme has desirable security performance (being resistant to different attacks), high R-D performance (PSNR gains over 1.3 dB than JPEG at 0.50 bpp compression ratio), and high error resilience (reconstructed 29.92 dB at 0.50 bpp compression ratio even with 50% bit loss).},
  archive      = {J_IETIP},
  author       = {Zan Chen and Chaocheng Ma and Tao Wang and Yuanjing Feng and Xingsong Hou and Xueming Qian},
  doi          = {10.1049/ipr2.12731},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1478-1492},
  shortjournal = {IET Image Process.},
  title        = {Robust image compression-encryption via scrambled block bernoulli sampling with diffusion noise},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DTFA: Adversarial attack with discrete cosine transform
noise and target features on deep neural networks. <em>IETIP</em>,
<em>17</em>(5), 1464–1477. (<a
href="https://doi.org/10.1049/ipr2.12727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image recognition on deep neural network is vulnerable to adversarial sample attacks. The adversarial attack accuracy is low when only limited queries on the target are allowed with the current black box environment. This paper proposes a target adversarial attack algorithm discrete cosine transform-mean target feature attack (DTFA) based on the target features and a limited-area sampling method. The algorithm first examines the original image and a target image to generate an initial adversarial example. Then the disturbance is sampled from the low-frequency region intercepted by Gaussian noise after discrete cosine transform. The authors determine the size of the disturbance according to the difference between the adversarial example and the original image with consideration of the number of iterations and the position of the target feature region. The disturbance is applied on the initial adversarial example to generate the new adversarial example with the difference from the original image reduced. To evaluate the proposed algorithm, based on the common image classification model InceptionV3, and with identical queries accessing the same target model, the authors conduct experiments to compare the attack effectiveness of DTFA and the benchmark algorithms on the same image and target datasets. Experimental results show that the generated adversarial examples by the proposed algorithm are superior to 94% of those by the similar attack algorithms with less than 10,000 access queries on the target model.},
  archive      = {J_IETIP},
  author       = {Dong Yang and Wei Chen and Songjie Wei},
  doi          = {10.1049/ipr2.12727},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1464-1477},
  shortjournal = {IET Image Process.},
  title        = {DTFA: Adversarial attack with discrete cosine transform noise and target features on deep neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage learning framework for single image deraining.
<em>IETIP</em>, <em>17</em>(5), 1449–1463. (<a
href="https://doi.org/10.1049/ipr2.12726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image deraining methods have been extensively studied for its ability to remarkably improve the performance of computer vision tasks in rainy environments. However, most existing rain removal methods still have two major drawbacks which are hindering the technology development. First, the rain streaks are seriously coupled with the background information in a single rainy image, which leads to incorrect identification of rain streaks by many methods and further makes the loss of texture details in the rain removal results. Second, they spend excessive computational cost, which is not conducive to practical applications. To address these issues, a progressive separation network (PSN) is proposed by decomposing the rain removal task into two stages, the bilateral grid learning stage and the joint feature refinement stage, from a novel perspective. The bilateral grid learning stage is designed to expand the distance between the rain streaks and the background information while preserving the image edge details to guide the subsequent refinement. For the joint feature refinement stage, a dual-path interaction module is constructed to dynamically and gradually decouple the rain streak content and the intermediate features of the clear image details. In addition, an activation-free feature refinement block is designed to further improve the computational efficiency by removing or replacing the activation function without loss of accuracy. Extensive experiments on synthetic and real datasets show that PSN outperforms state-of-the-art rain removal methods in terms of quantitative accuracy and subjective visual quality. Furthermore, competitive results are derived by extending PSN to the defogging task.},
  archive      = {J_IETIP},
  author       = {Rui Jiang and Yaoshun Li and Cheng Chen and Wei Liu},
  doi          = {10.1049/ipr2.12726},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1449-1463},
  shortjournal = {IET Image Process.},
  title        = {Two-stage learning framework for single image deraining},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SRI3D: Two-stream inflated 3D ConvNet based on sparse
regularization for action recognition. <em>IETIP</em>, <em>17</em>(5),
1438–1448. (<a href="https://doi.org/10.1049/ipr2.12725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although most state-of-the-art action recognition models have adopted a two-stream 3D convolutional structure as a backbone network, few works have studied the impact of loss functions on action recognition models. In addition, sparsity is used as a key prior knowledge in many fields. However, as far as is known, no one has studied the influence of the sparsity of network output on the output of deep learning-based action recognition models. Therefore, this paper proposes a novel two-stream inflated 3D ConvNet based on the sparse regularization (SRI3D) model for action recognition. In order to allow the network to learn the sparsity of output, the ℓ 1 norm is embedded in the loss function in regularization form in a plug-and-play manner. It can make the classification result after the fusion of the two-stream network only be the category with the highest confidence in one of the streams and not the other cases. The proposed loss function based on sparse regularization makes the output vector of the neural network as sparse as possible so that the classification results will not be ambiguous. Experimental results show that compared with other state-of-the-art models, this SRI3D has a competitive advantage on Kinetics-400, Something-Something V2, UCF-101 and HMDB-51.},
  archive      = {J_IETIP},
  author       = {Zhaoqilin Yang and Gaoyun An and Ruichen Zhang and Zhenxing Zheng and Qiuqi Ruan},
  doi          = {10.1049/ipr2.12725},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1438-1448},
  shortjournal = {IET Image Process.},
  title        = {SRI3D: Two-stream inflated 3D ConvNet based on sparse regularization for action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Independency-enhancing adversarial active learning.
<em>IETIP</em>, <em>17</em>(5), 1427–1437. (<a
href="https://doi.org/10.1049/ipr2.12724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The core idea of active learning is to obtain higher model performance with less annotation cost. This paper proposes an independency-enhancing adversarial active learning method. Independency-enhancing adversarial active learning is different from the previous methods and pays more attention to sample independence. Specifically, it is believed that the informativeness of a group of samples is related to sample independence rather than the simple sum of the informativeness of each sample in the group. Therefore, an independent sample selection module based on hierarchical clustering is designed to ensure sample independence. An adversarial approach is used to learn the feature representation of a sample and use the predicted loss value to label the state of the sample. Finally, samples are selected according to the uncertainty of the samples, the diversity of the samples and the independence of the samples. The experimental results on four datasets (CIFAR-100, Caltech-101, Cityscapes and BDD100K) demonstrate the effectiveness and superiority of independency-enhancing adversarial active learning.},
  archive      = {J_IETIP},
  author       = {Jifeng Guo and Zhiqi Pang and Miaoyuan Bai and Yanbang Xiao and Jian Zhang},
  doi          = {10.1049/ipr2.12724},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1427-1437},
  shortjournal = {IET Image Process.},
  title        = {Independency-enhancing adversarial active learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantitative feature classification for breast ultrasound
images using improved naive bayes. <em>IETIP</em>, <em>17</em>(5),
1417–1426. (<a href="https://doi.org/10.1049/ipr2.12723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantitative feature classification of breast ultrasound images can explore the intrinsic connection between image features and lesions, and become an important basis for the diagnosis of tumor nature in clinical medicine. However, the tumor features extracted from breast ultrasound images are generally based on the overall situation, which has an impact on the classification of features in ultrasound images. Therefore, a quantitative feature classification algorithm for breast ultrasound images using improved Naive Bayes (NB) is proposed. First, histogram features, grayscale co-generation matrix and other texture features are extracted from the ultrasound image. Second, NB is combined with decision tree calculation to improve the traditional NB algorithm. Finally, feature classification is optimized using improved NB algorithm to achieve high accuracy in the classification of ultrasound images. The results show that the contour of the lesion recognition results obtained by the proposed algorithm is the most complete, and the other regions can maximize the retention of the texture features of the original image. The classification accuracy is high, and the classification time of quantitative features of breast ultrasound images is short. It has application value in the field of breast ultrasound diagnosis.},
  archive      = {J_IETIP},
  author       = {Xiaofeng Li and Yupeng Sang and Xianmin Ma and Yingjie Cai},
  doi          = {10.1049/ipr2.12723},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1417-1426},
  shortjournal = {IET Image Process.},
  title        = {Quantitative feature classification for breast ultrasound images using improved naive bayes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-scale homography estimation based on dual feature
aggregation transformer. <em>IETIP</em>, <em>17</em>(5), 1403–1416. (<a
href="https://doi.org/10.1049/ipr2.12722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of registration in image stitching task directly affects the performance of subsequent stages. Traditional registration methods rely heavily on the quality of the features when calculating the homography matrix, resulting in alignment failures in low-texture or low-overlap scenes due to extracting insufficient features. On the other hand, existing DNN-based methods for homography estimation are more robust in multiple scenes but previous work usually employs an overly simple convolutional network structure to directly regress the homography, ignoring the redundant information contained in the feature maps so that their prediction accuracy is inferior to the traditional methods in simple scenarios. To overcome the disadvantages of the two methods, a Multi-scale structure is proposed to extract feature maps at three scales and design two modules to handle the matrix prediction respectively. The DFA-T module analyzes semantic information on the high-level features to accomplish coarse-grained alignment while the Contextual Correlation module on the bottom level to accomplish more accurate alignment. Experiments demonstrate that this method provides more accurate alignment results than the existing state-of-the-art DNN-based methods and outperforms traditional algorithms with more stable results in some extreme scenarios.},
  archive      = {J_IETIP},
  author       = {Ying Li and Kehan Chen and Shilei Sun and Chu He},
  doi          = {10.1049/ipr2.12722},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1403-1416},
  shortjournal = {IET Image Process.},
  title        = {Multi-scale homography estimation based on dual feature aggregation transformer},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-feature fusion attention network for single image
super-resolution. <em>IETIP</em>, <em>17</em>(5), 1389–1402. (<a
href="https://doi.org/10.1049/ipr2.12721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Image Super-Resolution algorithms have made enormous progress in recent years. However, many previous Convolution Neural Network (CNN) based Super-Resolution algorithms only stack uniform convolution layers of fixed kernel size, and frequently ignore inherent multi-scale properties of the images, resulting in unsatisfactory reconstruction results. Here, a multi-feature fusion attention network (MFFAN) is proposed for capturing information at diverse scales. MFFAN is composed of multiple efficient sparse residual group (ESRG) modules. Several multi-scale feature fusion blocks (MSFFB) are constructed using a cascade manner in each ESRG module and it is capable of exploiting various cross scales information. Subsequently, a local-global spatial attention block (LGSAB) is inserted at the tail of the ESRG module for further improving the interaction of inter-pixel, which strengths essential features and suppresses irrelevant information. Additionally, owing to the fact that only feeding final output into the reconstruction layer has exacerbated the long-range dependency problems, an enhanced hierarchy feature fusion block (EHFFB) is designed to fuse low-level information and high-level semantic information. Experiment results indicate that the proposed MFFAN is competitive in comparison to several state-of-the-art algorithms.},
  archive      = {J_IETIP},
  author       = {Jiacheng Chen and Wanliang Wang and Fangsen Xing and Hangyao Tu},
  doi          = {10.1049/ipr2.12721},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1389-1402},
  shortjournal = {IET Image Process.},
  title        = {Multi-feature fusion attention network for single image super-resolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time video anomaly detection for smart surveillance.
<em>IETIP</em>, <em>17</em>(5), 1375–1388. (<a
href="https://doi.org/10.1049/ipr2.12720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human monitoring of surveillance cameras for anomaly detection may be a monotonous task as it requires constant attention to judge if the captured activities are anomalous or suspicious. This paper exploits background subtraction (BS), convolutional autoencoder, and object detection for a fully automated surveillance system. BS was performed by modelling each pixel as a mixture of Gaussians (MoG) to concatenate only the higher-order learning in the foreground. Next, the foreground objects are fed to the convolutional autoencoders to filter out abnormal events from normal ones and automatically identify signs of threat and violence in real time. Then, object detection is introduced on the entire scene and the region of interest is highlighted with a bounding box to minimize human intervention in video stream processing. At recognition time, the network generates an alarm for the presence of an anomaly to notify of the identification of potentially suspicious actions. Finally, the complete system is validated upon several benchmark datasets and proved to be robust for complex video anomaly detection. The (AUC) average area under the curve for the frame-level evaluation for all benchmarks is 94.94%. The best improvement ratio of AUC between the proposed system and state-of-the-art methods is 7.7%.},
  archive      = {J_IETIP},
  author       = {Manal Mostafa Ali},
  doi          = {10.1049/ipr2.12720},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1375-1388},
  shortjournal = {IET Image Process.},
  title        = {Real-time video anomaly detection for smart surveillance},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mobile phone screen surface scratch detection based on
optimized YOLOv5 model (OYm). <em>IETIP</em>, <em>17</em>(5), 1364–1374.
(<a href="https://doi.org/10.1049/ipr2.12718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve phone screen surface detection efficiency, an optimized YOLOv5s model (OYm) based on GhostNet(YOLOv5GHOSTs) and BottleneckCSP is proposed. For a given target sample, OYm could effectively reduce the computation of GFLOPS and detection time by optimizing the network structure. The detection results show that the mean average precision_0.5 (mAP_0.5) exceeds 95%, and the average detection rate is 16 ms. Compared with the traditional YOLOv5s model, the loss of average accuracy is ensured to be controlled within 3%, the detection frame rate of OYm is risen by 56.25%, and GFLOPS is decreased by 64.2%. The principle of OYm is explained in detail, and the proposed model is then experimentally validated.},
  archive      = {J_IETIP},
  author       = {Jian Zhao and Bolin Zhu and Mo Peng and Lingling Li},
  doi          = {10.1049/ipr2.12718},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1364-1374},
  shortjournal = {IET Image Process.},
  title        = {Mobile phone screen surface scratch detection based on optimized YOLOv5 model (OYm)},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provable image encryption scheme via trapdoor permutation.
<em>IETIP</em>, <em>17</em>(5), 1355–1363. (<a
href="https://doi.org/10.1049/ipr2.12717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, due to the advancement in networks, it has become easy to share multimedia information such as images, voice, and videos in real-time. In the process of multimedia storage and transmission, it is crucial to ensure the security and privacy of multimedia data. The attendant issue is how to encrypt images or videos safely and efficiently. Here, a provable security model of image encryption is presented. Then, a new public image encryption scheme based on one-way trapdoor permutation, with security in the presence of honest adversaries, is proposed. Furthermore, random reusable technology is adopted to improve efficiency and shorten ciphertext. Finally, the simulation results show the validity of the encryption scheme.},
  archive      = {J_IETIP},
  author       = {Dongmei Li and Haikuan Liu},
  doi          = {10.1049/ipr2.12717},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1355-1363},
  shortjournal = {IET Image Process.},
  title        = {Provable image encryption scheme via trapdoor permutation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Template matching via bipartite graph and graph attention
mechanism. <em>IETIP</em>, <em>17</em>(5), 1346–1354. (<a
href="https://doi.org/10.1049/ipr2.12716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing template matching algorithms are global matching between the template target and the search region, which makes the matching process retain a lot of unfavourable background information and ignore the structure and local information of the template target. To address this problem, a template matching algorithm based on bipartite graph and graph attention mechanism is proposed in this paper. The algorithm models the similarity matching problem between template features and search region features as a complete bipartite graph, realises local-to-local information transfer between the two, and uses the graph attention mechanism to apply weights between local information to obtain a learnable embedding network module. In addition, in terms of feature representation, a multi-level feature fusion module based on CNN is introduced, which improves the representation of a target by fusing features with different representational meanings of the target. Experimental results on several typical datasets show that the proposed algorithm achieves leading performance in terms of accuracy and efficiency compared to the two state-of-the-art CNN-based template matching algorithms, Deep-DIM and QATM.},
  archive      = {J_IETIP},
  author       = {Kai Zhao and Binbing He and Wei Lei and Yuan Zhu},
  doi          = {10.1049/ipr2.12716},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1346-1354},
  shortjournal = {IET Image Process.},
  title        = {Template matching via bipartite graph and graph attention mechanism},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An end-to-end steel surface defect detection approach via
swin transformer. <em>IETIP</em>, <em>17</em>(5), 1334–1345. (<a
href="https://doi.org/10.1049/ipr2.12715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from most current studies using convolutional neural network (CNN), a deep learning detection method for steel plate surface defects based on Transformer is researched. This paper presents a process, network structure and detection method for steel strip surface defect detection. A Swin Transformer was used to extract hierarchical features in the detection system. Then feature pyramid networks (FPN) was used to fuse the above features to form multi-scale feature maps, and region proposal network (RPN) was adopted to generate the generate region of interest (ROI) of defects. Finally, the ROI head was used to generate defect category information and its precise location. Here, ablation experiments were conducted to explore the impact of different backbone networks and the number of stages of Swin Transformer and FPN on target detection performance to prove the rationality and efficiency of the network. On the NEU-DET dataset, the proposed algorithm achieved 81.1% mAP, which is higher than that of classic CNN detection methods such as Faster R-CNN, SSD, Yolo v3 and RepPoints.},
  archive      = {J_IETIP},
  author       = {Bo Tang and Zi-Kai Song and Wei Sun and Xing-Dong Wang},
  doi          = {10.1049/ipr2.12715},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1334-1345},
  shortjournal = {IET Image Process.},
  title        = {An end-to-end steel surface defect detection approach via swin transformer},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiscale anchor box and optimized classification with
faster r-CNN for object detection. <em>IETIP</em>, <em>17</em>(5),
1322–1333. (<a href="https://doi.org/10.1049/ipr2.12714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the two-stage object detector as a faster region-convolutional neural network (Faster R-CNN), upgrading the accuracy of object recognition depends on the proposal box, which is generated by the region proposal algorithms. Due to the limitations of the anchor setting of Faster RCNN, the size of the proposal box generated by the region proposal network (RPN) used is large, which would easily cause a great number of overflows in the sliding search. To improve the accuracy of object detection and remit the overflow problem of the anchor box, multi-scale anchor box and moving overflow anchor box strategies are introduced here. Then, to increase the positive sample range of the foreground, the hierarchical weight cross entropy classification function is set for binary classification in the RPN network. These strategies could improve the accuracy of object detection. The experimental result achieves 76.2% AP on the Pascal VOC 2007(VOC 07) dataset, which is 2.7% higher than the Faster R-CNN. The result of the Pascal VOC 2012(VOC 12) test, we achieve 75.6% AP , is improved by 2.5% compared with the Faster R-CNN.},
  archive      = {J_IETIP},
  author       = {Sheng-Ye Wang and Zhong Qu},
  doi          = {10.1049/ipr2.12714},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1322-1333},
  shortjournal = {IET Image Process.},
  title        = {Multiscale anchor box and optimized classification with faster R-CNN for object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A survey on end-to-end point cloud learning. <em>IETIP</em>,
<em>17</em>(5), 1307–1321. (<a
href="https://doi.org/10.1049/ipr2.12729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud is an important expression form of three-dimensional (3D) data. It has enjoyed continuous development and attracted increasing attention due to its wide applications in many areas, such as artificial intelligence, deep learning, autonomous driving and tracking. Recently, there is a large number of end-to-end point cloud-based deep learning methods being proposed which are successful in the 3D domain. In order to better use point cloud data for analysis and to explore future research directions, this paper presents a comprehensive review of existing methods and publicly available datasets, with a focus on the methods and research status of using point cloud data as direct input. The background of point cloud is first introduced, including data acquisition methods, basic concepts, and challenges. Following that, the deep learning methods based on point cloud data are investigated and analysed according to classification, detection and tracking, and segmentation. Furthermore, the existing public datasets and evaluation metrics are introduced. Finally, promising research directions are proposed in conjunction with existing methods.},
  archive      = {J_IETIP},
  author       = {Xikai Tang and Fangzheng Huang and Chao Li and Dayan Ban},
  doi          = {10.1049/ipr2.12729},
  journal      = {IET Image Processing},
  month        = {4},
  number       = {5},
  pages        = {1307-1321},
  shortjournal = {IET Image Process.},
  title        = {A survey on end-to-end point cloud learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Complex artefact suppression for sparse reconstruction based
on compensation approach in x-ray computed tomography. <em>IETIP</em>,
<em>17</em>(4), 1291–1306. (<a
href="https://doi.org/10.1049/ipr2.12713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To provide high-quality imaging by decreasing the sparse view imaging artefact from the computed tomography (CT) images, this study addresses a new artefact suppression technique for sparse data for both single material and multi-material objects that have diverse materials. It begins with a pre-reconstructed image and a network of target patches that have been trained beforehand and then uses the forward projection (FP) approach to resolve the structural mutation brought on by sparse-view projection. As an edge-preserving operator to commit to the forward operator for sinogram correction, the bilateral filter was used. Both simulated and actual data have been gathered and evaluated in experiments. The suggested forward operator and normalized compensation (FONC) method produce results that have far smaller artefact and errors than those of more traditional techniques. For simulation # blade, the Normalized Mean Square Distance (NMSD) of the proposed method was reduced by 8.94%, Structural Similarity Index (SSIM) and the Universal Quality Index (UQI) were increased by 78.17% and 80.49%, respectively, which also demonstrate better uniformity of the results to practical data # pan, where the root mean squared error was reduced by 13.93%. SSIM and UQI were increased by 25.66% and 37.02%, respectively. The results conclusively show that the planned strategies are successful in eliminating artefact for irregular objects.},
  archive      = {J_IETIP},
  author       = {Fuqiang Yang and Dinghua Zhang and Kuidong Huang and Yao Yang and Zhixiang Li},
  doi          = {10.1049/ipr2.12713},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1291-1306},
  shortjournal = {IET Image Process.},
  title        = {Complex artefact suppression for sparse reconstruction based on compensation approach in X-ray computed tomography},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gesture recognition algorithm based on multi-scale feature
fusion in RGB-d images. <em>IETIP</em>, <em>17</em>(4), 1280–1290. (<a
href="https://doi.org/10.1049/ipr2.12712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sensor technology and artificial intelligence, the video gesture recognition technology under the background of big data makes human-computer interaction more natural and flexible, bringing richer interactive experience to teaching, on-board control, electronic games, etc. In order to perform robust recognition under the conditions of illumination change, background clutter, rapid movement, partial occlusion, an algorithm based on multi-level feature fusion of two-stream convolutional neural network is proposed, which includes three main steps. Firstly, the Kinect sensor obtains RGB-D images to establish a gesture database. At the same time, data enhancement is performed on training and test sets. Then, a model of multi-level feature fusion of two-stream convolutional neural network is established and trained. Experiments result show that the proposed network model can robustly track and recognize gestures, and compared with the single-channel model, the average detection accuracy is improved by 1.08%, and mean average precision (mAP) is improved by 3.56%. The average recognition rate of gestures under occlusion and different light intensity was 93.98%. Finally, in the ASL dataset, LaRED dataset, and 1-miohand dataset, recognition accuracy shows satisfactory performances compared to the other method.},
  archive      = {J_IETIP},
  author       = {Ying Sun and Yaoqing Weng and Bowen Luo and Gongfa Li and Bo Tao and Du Jiang and Disi Chen},
  doi          = {10.1049/ipr2.12712},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1280-1290},
  shortjournal = {IET Image Process.},
  title        = {Gesture recognition algorithm based on multi-scale feature fusion in RGB-D images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deeply supervised vestibule segmentation network for CT
images with global context-aware pyramid feature extraction.
<em>IETIP</em>, <em>17</em>(4), 1267–1279. (<a
href="https://doi.org/10.1049/ipr2.12711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate vestibule segmentation for CT images is of great significance for the clinical diagnosis of congenital ear malformations and cochlear implant. However, it is still a challenging task due to extremely small size and irregular shape of vestibule. Here, a vestibule segmentation network for CT images is proposed under the basic encoder-decoder framework. Firstly, a residual block based on channel attention mechanism, named Res-CA block, is designed to guide the network to enhance the important features for the segmentation tasks while suppressing the irrelevant ones. And then, a global context-aware pyramid feature extraction (GCPFE) module is proposed to capture multi-receptive-field global context information. Finally, active contour with elastic (ACE) loss function is adopted to guide network learning more detailed information of the boundary. Furthermore, deep supervision (DS) mechanism is employed to locate the boundaries finely, improving the robustness of the network. The experiments are conducted on the self-established VestibuleDataset and UHRCT-Dataset, as well as publicly available retinal dataset, namely DRIVE, to comprehensively verify the robustness and generalization capability of the proposed segmentation network. The experimental results show that the proposed network can achieve a superior performance.},
  archive      = {J_IETIP},
  author       = {Meijuan Chen and Li Zhuo and Ziyao Zhu and Hongxia Yin and Xiaoguang Li and Zhenchang Wang},
  doi          = {10.1049/ipr2.12711},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1267-1279},
  shortjournal = {IET Image Process.},
  title        = {Deeply supervised vestibule segmentation network for CT images with global context-aware pyramid feature extraction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-domain autonomous driving dataset: Towards enhancing
the generalization of the convolutional neural networks in new
environments. <em>IETIP</em>, <em>17</em>(4), 1253–1266. (<a
href="https://doi.org/10.1049/ipr2.12710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a large-scale dataset called the Iran Autonomous Driving Dataset (IADD) is presented, aiming to improve the generalization capability of the deep networks outside of their training domains. The IADD focuses on 2D object detection and contains more than 97,000 annotated images, covering six common object classes in the field of autonomous vehicles. To improve the generalization of the models, a wide variety of driving conditions and domains, including the city and suburban road settings, adverse weather conditions, and various traffic flows, are presented in the IADD images. The results of exhaustive evaluations conducted on several state-of-the-art convolutional neural networks reveal that not only the trained architectures have performed successfully on test data of the IADD, but also they have upheld high precision in the assessments of generalization capability. In order to challenge the models, broad range of simulations have been performed in the CARLA software environment; which due to the synthetic nature of the simulated images, severe domain shifts have been observed between the CARLA and the IADD. Also, the cross-domain evaluation results have confirmed the efficacy of the IADD in enhancing the generalization ability of the deep learning models. The dataset is available in: https://github.com/ahv1373/IADD .},
  archive      = {J_IETIP},
  author       = {Amir Khosravian and Abdollah Amirkhani and Masoud Masih-Tehrani and Alireza Yazdanijoo},
  doi          = {10.1049/ipr2.12710},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1253-1266},
  shortjournal = {IET Image Process.},
  title        = {Multi-domain autonomous driving dataset: Towards enhancing the generalization of the convolutional neural networks in new environments},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SACNet: Shuffling atrous convolutional u-net for medical
image segmentation. <em>IETIP</em>, <em>17</em>(4), 1236–1252. (<a
href="https://doi.org/10.1049/ipr2.12709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical images exhibit multi-granularity and high obscurity along boundaries. As representative work, the U-Net and its variants exhibit two shortcomings on medical image segmentation: (a) they expand the range of reception fields by applying addition or concatenate operators to features with different reception fields, which disrupts the distribution of the essential feature of objects; (b) they utilize the downsampling or atrous convolution to characterize multi-granular features of objects, which can obtain a large range of reception fields but leads to blur boundaries of objects. A Shuffling Atrous Convolutional U-Net (SACNet) for circumventing those issues is proposed. The significant component of SACNet is the Shuffling Atrous Convolution (SAC) module, which fuses different atrous convolutional layers together by using a shuffle concatenate operation, so that the features from the same channel (which correspond to the same attribute of objects) are merged together. Besides the SAC modules, SACNet utilizes an EP module during the fine and medium levels to enhance the boundaries of objects, and utilizes a Transformer module during the coarse level to capture an overall correlation of pixels. Experiments on three medical image segmentation tasks: abdominal organ, cardiac, and skin lesion segmentation demonstrate that, SACNet outperforms several state-of-the-art methods and facilitates easy transplant to other semantic segmentation tasks.},
  archive      = {J_IETIP},
  author       = {Shaofan Wang and Yukun Liu and Yanfeng Sun and Baocai Yin},
  doi          = {10.1049/ipr2.12709},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1236-1252},
  shortjournal = {IET Image Process.},
  title        = {SACNet: Shuffling atrous convolutional U-net for medical image segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual- and triple-stream RESUNET/UNET architectures for
multi-modal liver segmentation. <em>IETIP</em>, <em>17</em>(4),
1224–1235. (<a href="https://doi.org/10.1049/ipr2.12708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning image segmentation has become an important field of interest in recent years, especially when it comes to medical images. Segmentation of medical image modalities such as magnetic resonance imaging (MRI) and computed tomography (CT) can benefit diagnosis accuracy, speed up diagnosis process, and decrease workload. The most famously used deep learning models in the medical image segmentation are the UNET-based models, which have been repeatedly proven to provide a high percentage of accuracy in medical image segmentation. But, most of the available datasets contain a single modality and thus are not big enough to train complex architectures. Lately, it has been shown that using multiple modalities with multiple streams architectures can provide higher accuracy more than single modality with a single stream architecture. In this paper, the benefits of dual-stream and triple-stream architectures are demonstrated when processing multiple modalities. This work shows that dual stream can achieve dice of 0.97 on CT images and 0.89 on MRI images, while in triple stream architectures can achieve dice of 0.97 on CT images and 0.96 on MRI images. To the best of our knowledge, these are the best results to date.},
  archive      = {J_IETIP},
  author       = {Hagar Louye Elghazy and Mohamed Waleed Fakhr},
  doi          = {10.1049/ipr2.12708},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1224-1235},
  shortjournal = {IET Image Process.},
  title        = {Dual- and triple-stream RESUNET/UNET architectures for multi-modal liver segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDNet: Image-based sonar detection network for multi-scale
objects. <em>IETIP</em>, <em>17</em>(4), 1208–1223. (<a
href="https://doi.org/10.1049/ipr2.12707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Underwater Vehicle (AUV) carrying sonar for object detection has become one of the main ways of ocean exploration. However, object detection in sonar images always faces the problems of balancing detection accuracy and efficiency. To this end, this paper proposes an efficient underwater object detection network for sonar images named SDNet. In the model, the authors construct a new feature extraction network based on RepVGG to balance the detection accuracy and speed. By combining channel attention with RepVGG, useful information of high-order feature maps captured can be selectively paid attention to. Then, the authors design a feature fusion network to efficiently converge the location and semantic information of multi-scale feature maps. In the network, the authors propose a lightweight cross stage partial network for sonar (CSP_S) module suitable for sonar images, which can enhance the model&#39;s feature fusion capability and simplify the model. Finally, to reduce the conflict between classification and regression tasks, the authors leverage the Decoupled Head for the sonar object classification and localization. By testing on the self-built Underwater Sonar Dataset underwater sonar dataset (USD) and the public sonar dataset sonar common target detection dataset (SCTD), the detection accuracy of SDNet reaches 99.52% and 95.20%, respectively. Moreover, the detection speed reaches 114 frames per second (FPS) and 138 FPS, respectively. The experimental results show that SDNet can effectively balance the sonar detection accuracy and efficiency. Code is available at https://github.com/Yukino111/sdnet-pytorch .},
  archive      = {J_IETIP},
  author       = {Pengfei Shi and Huanru Sun and Yuanxue Xin and Qi He and Xiaotian Wang},
  doi          = {10.1049/ipr2.12707},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1208-1223},
  shortjournal = {IET Image Process.},
  title        = {SDNet: Image-based sonar detection network for multi-scale objects},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image encryption based on fractional discrete cosine
transform and DWT with interplane arrangements in dost domain.
<em>IETIP</em>, <em>17</em>(4), 1195–1207. (<a
href="https://doi.org/10.1049/ipr2.12706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel color image encryption scheme based on fractional discrete cosine transform (FrDCT) and discrete wavelet transform (DWT) with interplane arrangements in the discrete orthonormal Stockwell transform (DOST) domain is presented. Color images are encrypted using the keys of FrDCT and coefficients of interplane arrangement of DWT sub-bands. These keys are applied on three independent planes, that is, R, G, and B. To ensure the correct decryption of the encrypted image, every key need to be clearly understood in the correct sequence and its exact values. FrDCT has the advantageous feature of storing real-valued coefficients, and the energy of the image can be represented with very few coefficients. Therefore, transmitting and storing the encrypted image becomes easy. FrDCT may be considered a real-valued cosine version of fractional Fourier transform (FrFT), which is used successfully for storage and transmission due to its property of compacting energy distribution. Furthermore, for more robustness, interplane arrangements are explored. The concept of interplane arrangements has not been substantially reported in the existing literature. The comparison of the technique with other similar encryption techniques available in the literature has been carried out by performing different experiments to examine the efficacy of the proposed method. It is found that the proposed method outperforms other existing techniques.},
  archive      = {J_IETIP},
  author       = {Gaurav Sundaram and Bhaskar Panna and Rajib Kumar Jha and Sumit Kumar},
  doi          = {10.1049/ipr2.12706},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1195-1207},
  shortjournal = {IET Image Process.},
  title        = {Image encryption based on fractional discrete cosine transform and DWT with interplane arrangements in dost domain},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-based fall detection in bus compartment scene.
<em>IETIP</em>, <em>17</em>(4), 1181–1194. (<a
href="https://doi.org/10.1049/ipr2.12705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bus, as an important means of public transportation, has always received attention for its safety. When an emergency occurs, fall behaviour is one of representative features to help alarm. However, compared with other open and fixed scenes, the bus compartment has its own characteristics, such as crowded, closed, in moving, and so on. Considering the problems of people blocked as well as light changes, this paper proposes a new image-based fall detection method in bus compartment scene, which can be divided into three parts: (1) Human detection, where object detection and pose estimation algorithms are combined, and then both global and local human features can be obtained; (2) fall discrimination, where both of the fall discrimination conditions and fall discrimination network are designed; (3) alert, where an alarm strategy is designed. Experiments are done in a real bus, and the Nvidia Jetson Xavier NX module is used to analyse the videos and images. Results finally show that the proposed fall detection method in bus compartment scenes can achieve 90% accuracy.},
  archive      = {J_IETIP},
  author       = {Xiaoping Zhang and Jiahui Ji and Li Wang and Zhonghe He and Shida Liu},
  doi          = {10.1049/ipr2.12705},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1181-1194},
  shortjournal = {IET Image Process.},
  title        = {Image-based fall detection in bus compartment scene},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WCE-DCGAN: A data augmentation method based on wireless
capsule endoscopy images for gastrointestinal disease detection.
<em>IETIP</em>, <em>17</em>(4), 1170–1180. (<a
href="https://doi.org/10.1049/ipr2.12704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless capsule endoscopy (WCE) is becoming more popular in clinical settings as a safe and painless gastrointestinal examination. Existing studies on automatic detection of lesions in WCE images have the problems of small dataset size and uneven distribution of numbers in terms of categories, which often leads to overfitting of the model and severely limits the performance improvement of the object detection network on WCE images. The traditional data enhancement methods such as flipping and local erasure have limitations and cannot achieve good generalization results. Therefore, a WCE-DCGAN network was proposed in this paper to generate WCE images from existing WCE images. Using the images generated by this network and the original images as the input of the object detection network, there are different degrees of performance improvement on SSD, YOLOv5, and YOLOv4, and the average recognition accuracy of 97.25% can be achieved on SSD. Meanwhile, images generated by WCE-DCGAN not only enlarge the size of the data set, but also have the characteristics of diversity, which makes the model have a good generalization effect.},
  archive      = {J_IETIP},
  author       = {Zhiguo Xiao and Jia Lu and Xiaokun Wang and Nianfeng Li and Yuying Wang and Nan Zhao},
  doi          = {10.1049/ipr2.12704},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1170-1180},
  shortjournal = {IET Image Process.},
  title        = {WCE-DCGAN: A data augmentation method based on wireless capsule endoscopy images for gastrointestinal disease detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An edge thinning algorithm based on newly defined
single-pixel edge patterns. <em>IETIP</em>, <em>17</em>(4), 1161–1169.
(<a href="https://doi.org/10.1049/ipr2.12703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the uniformity of one-pixel width and continuity of the thinned edges, this paper proposes an edge thinning algorithm acting on grey-scale edge images based on 24 self-defined single-pixel connection patterns. First, for binary or blurred grey-scale gradient edge images, a distance–greyscale coupling algorithm is proposed to achieve gradient enhancement in the edge width direction. Then the elimination rules of noise points and gradient calculation method are given. Secondly, the marking rules of the first three pixels of each edge are given. The next pixel to be marked must meet that the new last three pixels belong to the 24 connection modes. Whether the qualified pixels are retained depends on its grey value and the local edge gradient. The algorithm is tested on four types of images. The results show that the proposed method can guarantee uniform, smooth, and connected one-pixel-wide lines that lie at the centre of the initial edges. The algorithm and the existing algorithms are performed on portrait image and four scenarios of the indoor datasets. Five evaluation indicators are statistically analyzed to prove the feasibility and effectiveness of the proposed algorithm.},
  archive      = {J_IETIP},
  author       = {Lijuan Ren and Xionghui Wang and Nina Wang and Guangpeng Zhang and Yongchang Li and Zhijian Yang},
  doi          = {10.1049/ipr2.12703},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1161-1169},
  shortjournal = {IET Image Process.},
  title        = {An edge thinning algorithm based on newly defined single-pixel edge patterns},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Underwater image enhancement using a mixed generative
adversarial network. <em>IETIP</em>, <em>17</em>(4), 1149–1160. (<a
href="https://doi.org/10.1049/ipr2.12702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images intuitively reflect the underwater environment information. However, underwater images have defects such as colour distortion and low contrast, which seriously affect the processing of complex underwater visual tasks. Here, a novel mixed model called mixed underwater image generative adversarial network (MUGAN) is presented, consisting of a generator and corresponding discriminator. The generator is built following the U-shaped architecture, where a mixed block of convolution and self-attention is developed. It effectively exploits the complementarity between the two paradigms. In addition, a dual discriminator is employed to induce the generator to produce realistic images at both the global semantic and local detail levels, which is only discriminates based on the patch-level information. Meanwhile, a multi-term loss function is formulated to supervise adversarial training by evaluating the perceptual quality of an image based on its global content, local texture and illumination smoothness. To validate the proposed approach, extensive experiments are conducted on the public underwater datasets. MUGAN achieves promising performance in terms of colour, contrast and naturalness, showing a significant improvement over other competitive models in visual quality and quantitative metrics.},
  archive      = {J_IETIP},
  author       = {Delang Mu and Heng Li and Hui Liu and Ling Dong and Guoyin Zhang},
  doi          = {10.1049/ipr2.12702},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1149-1160},
  shortjournal = {IET Image Process.},
  title        = {Underwater image enhancement using a mixed generative adversarial network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMFuse: A multi-scale infrared and visible images fusion
algorithm based on morphological reconstruction and membership
filtering. <em>IETIP</em>, <em>17</em>(4), 1126–1148. (<a
href="https://doi.org/10.1049/ipr2.12701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a multi-scale transformation method based on morphological reconstruction and membership filtering, termed as MMFuse, to fuse infrared and visible images. This method employs a fuzzy c-means clustering algorithm for multi-scale decomposition by introducing morphological reconstruction operations and modifying member partitions to ensure noise resistance and image detail preservation. In addition, the MMFuse utilises the image attributes of layers as their fusion weights at each scale for adaptive feature fusion, which reduces the difficulty of manual adjustment of fusion weights. Moreover, on the basis of histogram enhancement, a visible image enhancement method is proposed, which can help exploit additional texture details in low-light visible images and transfer these details to the fused image. The experiments performed on public datasets indicates that the MMFuse can generate sharp and clean fused images with high robustness and good fusion results for the images corrupted by different noises. Moreover, the results of this method appear as high-quality visible images with clear highlighted infrared targets.},
  archive      = {J_IETIP},
  author       = {Liangjun Zhao and Hao Yang and Linlu Dong and Liping Zheng and Manlike Asiya and Fengling Zheng},
  doi          = {10.1049/ipr2.12701},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1126-1148},
  shortjournal = {IET Image Process.},
  title        = {MMFuse: A multi-scale infrared and visible images fusion algorithm based on morphological reconstruction and membership filtering},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic facial expression recognition combining texture
and shape features from prominent facial regions. <em>IETIP</em>,
<em>17</em>(4), 1111–1125. (<a
href="https://doi.org/10.1049/ipr2.12700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression is one form of communication which being non-verbal in nature precedes verbal communication in both origin and conception. Most of the existing methods for Automatic Facial Expression Recognition (AFER) are mainly focused on global feature extraction assuming that all facial regions contribute equal amount of discriminative information to predict the expression class. The detection and localization of facial regions that have significant contribution to expression recognition and extraction of highly discriminative feature distribution from those regions are not fully explored. The key contributions of the proposed work are developing novel feature distribution upon combining the discriminative power of shape and texture feature; determining the contribution of facial regions and identifying the prominent facial regions that hold abstract and highly discriminative information for expression recognition. The shape and texture features taken into consideration are Local Phase Quantization (LPQ), Local Binary Pattern (LBP), and Histogram of Oriented Gradients (HOG). Multiclass Support Vector Machine (MSVM) is used while one versus one classification. The proposed work is implemented on CK+, KDEF, and JAFFE benchmark facial expression datasets. The recognition rate of the proposed work is 94.2% on CK+ and 93.7% on KDEF, which is significantly more than the existing handcrafted feature-based methods.},
  archive      = {J_IETIP},
  author       = {Naveen Kumar H N and A Suresh Kumar and Guru Prasad M S and Mohd Asif Shah},
  doi          = {10.1049/ipr2.12700},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1111-1125},
  shortjournal = {IET Image Process.},
  title        = {Automatic facial expression recognition combining texture and shape features from prominent facial regions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-local neural networks combined with local
importance-based pooling for space-time video super-resolution.
<em>IETIP</em>, <em>17</em>(4), 1097–1110. (<a
href="https://doi.org/10.1049/ipr2.12699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with convolutional operation, non-local operation can directly capture long-range dependencies and thus has a larger receptive field. However, the computation and memory consumption of non-local operation is much higher than convolutional operation, so it cannot be used repeatedly as a general component directly. In this paper, in order to balance the accuracy and computational complexity of non-local enhancement, the non-local operation is simplified based on local importance-based pooling, which can dynamically extract discriminative features during the down-sampling process by learning adaptive weights. Such simplified non-local enhancement is able to prevent unacceptable computational consumption caused by directly processing the entire feature maps containing a large number of features. In order to verify the effectiveness of the proposed method, 2D and 3D feature extraction blocks are constructed based on the simplified non-local operations, and they are stacked as feature extraction networks for space-time video super-resolution task, which aims to increase resolution in both time and space simultaneously. Extensive experiments demonstrate that the proposed simplified non-local networks can effectively improve the performance of space-time video super-resolution task both quantitatively and qualitatively.},
  archive      = {J_IETIP},
  author       = {Dacheng Zhang and Weimin Lei and Wei Zhang and Xinyi Chen},
  doi          = {10.1049/ipr2.12699},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1097-1110},
  shortjournal = {IET Image Process.},
  title        = {Non-local neural networks combined with local importance-based pooling for space-time video super-resolution},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A quantitative evaluation of lung nodule spiculation based
on image enhancement. <em>IETIP</em>, <em>17</em>(4), 1086–1096. (<a
href="https://doi.org/10.1049/ipr2.12698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer manifests itself as lung nodules at an early stage. Segmentation of lung nodules and quantitative evaluation of spiculation can assist physicians in distinguishing benign and malignant lung nodules. The identification of malignant nodules for early diagnosis and treatment can improve the survival rate of patients. In this paper, a quantitative evaluation method of lung nodule spiculation is proposed based on image enhancement. The proposed method combines super-resolution reconstruction-based image enhancement techniques with lung nodule segmentation algorithms to improve the accuracy of segmentation and to quantify the degree of distinctness of the spiculation of lung nodule, which can provide a reliable basis for computer-aided diagnosis and treatment of lung nodules. First, the method uses Laplacian pyramid image restoration technique and Gaussian differential scale-invariant feature to enhance the details and edge information of lung CT images. Then the improved Random Walk algorithm is used to segment the enhanced lung images and extract lung nodules. Finally, the spiculation index, which measures the spiculation of breast nodules, is used to quantify the spiculation of nodules. The experimental results show that the method can effectively segment lung nodules and quantitatively evaluate the spiculation of lung nodules.},
  archive      = {J_IETIP},
  author       = {Juliang Tao and Yongli Wang and Xiaoyun Ding},
  doi          = {10.1049/ipr2.12698},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1086-1096},
  shortjournal = {IET Image Process.},
  title        = {A quantitative evaluation of lung nodule spiculation based on image enhancement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel computer vision-based approach for monitoring safety
harness use in construction. <em>IETIP</em>, <em>17</em>(4), 1071–1085.
(<a href="https://doi.org/10.1049/ipr2.12696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falling from a height is the most common accident on construction sites. Vision-based techniques can be used to automatically monitor the construction sites and give early warnings. In this study, a lightweight object detection method, Efficient-YOLOv5, was proposed for detecting whether workers are wearing safety harnesses when working at height. Furthermore, a matching-recheck strategy was proposed to improve the mean average precision (mAP). The safety status evaluation model was designed to evaluate the safety status of workers in different construction scenarios. An edge computing-based security monitoring and alarm system suitable for deployment on construction sites was proposed to assist manual management. Efficient-YOLOv5 was trained and evaluated on our newly created dataset. Experiments demonstrated that our proposed method outperformed other comparison methods, as the precision and recall rates were 97.7% and 89.3%, respectively. The mAP was 94%. The rate of frames per second (FPS) was 72, which met real-time application requirements. Thus, the proposed method could easily be applied in the construction industry.},
  archive      = {J_IETIP},
  author       = {Zhijing Xu and Jiajing Huang and Kan Huang},
  doi          = {10.1049/ipr2.12696},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1071-1085},
  shortjournal = {IET Image Process.},
  title        = {A novel computer vision-based approach for monitoring safety harness use in construction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video frame interpolation via residual blocks and feature
pyramid networks. <em>IETIP</em>, <em>17</em>(4), 1060–1070. (<a
href="https://doi.org/10.1049/ipr2.12695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various deep learning-based video frame interpolation methods have been proposed in the past few years, but how to generate high quality interpolated frames in videos with large motions, complex backgrounds and rich textures is still a challenging issue. To deal with this limitation, a frame interpolation method based on residual blocks and feature pyramids is proposed. U-Net is the main architecture of our method, which can capture multi-layer information, segment objects from the background and obtain parameters with motion information to guide frame interpolation. However, the upsampling and subsampled of U-Net will lose important information. In order to acquire more detailed contextual information, shortcut connection is used in the encoder basic module. At the same time, feature pyramid network is employed to capture features at different scales of the decoder to improve the representation of inter-frame spatial-temporal features. The experimental results show that the proposed method outperform the baseline methods in both of objective and subjective evaluations on different datasets. In particular, the method has obvious advantages on datasets which contain complex background.},
  archive      = {J_IETIP},
  author       = {Xiaohui Yang and Haoran Zhang and Zhe Qu and Zhiquan Feng and Jinglan Tian},
  doi          = {10.1049/ipr2.12695},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1060-1070},
  shortjournal = {IET Image Process.},
  title        = {Video frame interpolation via residual blocks and feature pyramid networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D zebrafish tracking with topology association.
<em>IETIP</em>, <em>17</em>(4), 1044–1059. (<a
href="https://doi.org/10.1049/ipr2.12694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, zebrafish has received more and more attention due to its wide range of applications such as regeneration promoting therapeutics and drug discovery. Therefore, vision-based trackers are utilized to record the swimming trajectory of zebrafish. In this paper, a re-association method is introduced in the 3D reconstruction process to generate missed targets caused by occlusion. Since the variation of the overall tracking targets has the property of continuity and stability, a topology association model (TAM) is proposed by point group similarity into the tracking framework. TAM describes the movement of zebrafish from the macroscopic level and utilizes the changes of the point group structure for tracking. Experimental results show that the tracking framework enhances the overall performance and promotes the trajectory integrity. On the latest 3D-ZeF20 benchmark, state-of-the-art results are achieved. In addition, TAM tracking framework is applied to 2D general tracking to prove that the method is useful and have great advantage in other scenarios with relatively stable amount of targets as well.},
  archive      = {J_IETIP},
  author       = {Yuan Xu and Yichao Jin and Yang Zhang and Qunxiong Zhu and Yanlin He and Hao Sheng},
  doi          = {10.1049/ipr2.12694},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1044-1059},
  shortjournal = {IET Image Process.},
  title        = {3D zebrafish tracking with topology association},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hyperspectral anomaly detection via
weighted-sparsity-regularized tensor linear representation.
<em>IETIP</em>, <em>17</em>(4), 1029–1043. (<a
href="https://doi.org/10.1049/ipr2.12693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection aims at locating the spectral different objects of a specific scene without any prior information, and has gained increasing attention. By decomposing the input hyperspectral image (HSI) into a background tensor and an anomaly tensor, the tensor approximation is an efficient tool for detecting the anomalies. Low rankness is usually utilized as the regularizer during the background reconstruction process. Different from most existing hyperspectral anomaly detection methods which compute the truncated nuclear norm of the third folding of the original HSI, a novel weighted-sparsity-regularized tensor linear representation (WsrTLR) method is proposed for hyperspectral anomaly detection in this paper. Tensor linear representation is utilized to formulate the background HSI by a three-dimensional (3D) representation base and the corresponding 3D representation coefficient. Low rankness is applied to constrict the representation coefficient, an operation which avoids destroying the multi-way structure and losing information during the matrixing process, and ensures a satisfactory detection accuracy. Meanwhile, by incorporating the weighted-sparsity-regularized tensor linear representation to reconstruct the background tensor, the anomalies can be easily detected by eliminating the background tensor from the original scene. In addition, to avoid negative influence caused by the redundant bands and noisy bands in the representation process, informative bands have been first selected via an optimal neighborhood reconstruction strategy. Experimental results and data analysis on four real hyperspectral datasets, which contain anomalies with different sizes, have demonstrated the effectiveness of the proposed method.},
  archive      = {J_IETIP},
  author       = {Jingxuan Wang and Jinqiu Sun and Yong Xia and Yanning Zhang},
  doi          = {10.1049/ipr2.12693},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1029-1043},
  shortjournal = {IET Image Process.},
  title        = {Hyperspectral anomaly detection via weighted-sparsity-regularized tensor linear representation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonlinear kernel based feature maps for blur-sensitive
unsharp masking of JPEG images. <em>IETIP</em>, <em>17</em>(4),
1010–1028. (<a href="https://doi.org/10.1049/ipr2.12692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a method for estimating the blur regions of an image is first proposed, resorting to a mixture of linear and nonlinear convolutional kernels. The blur map obtained is then utilized to enhance images such that the enhancement strength is an inverse function of the amount of measured blur. The blur map can also be used for tasks such as attention-based object classification, low light image enhancement, and more. A CNN architecture is trained with nonlinear upsampling layers using a standard blur detection benchmark dataset, with the help of blur target maps. Further, it is proposed to use the same architecture to build maps of areas affected by the typical JPEG artifacts, ringing and blockiness. The blur map and the artifact map pair permit to build an activation map for the enhancement of a (possibly JPEG compressed) image. Extensive experiments on standard test images verify the quality of the maps obtained using the algorithm and their effectiveness in locally controlling the enhancement, for superior perceptual quality. Last but not least, the computation time for generating these maps is much lower than the one of other comparable algorithms.},
  archive      = {J_IETIP},
  author       = {Jhilik Bhattacharya and Stefano Marsi and Giovanni Ramponi},
  doi          = {10.1049/ipr2.12692},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1010-1028},
  shortjournal = {IET Image Process.},
  title        = {Nonlinear kernel based feature maps for blur-sensitive unsharp masking of JPEG images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Target region extraction and segmentation algorithm for
reflective tomography lidar image. <em>IETIP</em>, <em>17</em>(4),
1001–1009. (<a href="https://doi.org/10.1049/ipr2.12691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflective tomography Lidar is long-range, high-resolution imaging Lidar. Because the angular resolution is independent of detection range, it enjoys promising application prospects in imaging of small space targets, estimation of barycentre range of space debris, and many other fields. In practice, images generated by reflective tomography Lidar generally contain a large number of artefacts and noise that need to be removed to obtain the target profile. To improve the quality of the target profile, an algorithm is proposed for the extraction and segmentation of the target region in reflective tomography Lidar images. According to the experimental results, the algorithm can achieve better segmentation results than the traditional threshold segmentation algorithms. In particular, the algorithm can maintain good segmentation results for those images with noticeable ring artefacts, strip artefacts, and noise while avoiding under-segmentation or over-segmentation. It also guarantees the integrity of the target segmentation, preserves the outer contour and detailed structure information of the target as much as possible, and improves the accuracy of the target segmentation. Compared with conventional threshold segmentation algorithms, the algorithm improves the quality of image segmentation, and can improve the quality factor by more than 3%.},
  archive      = {J_IETIP},
  author       = {Xinyuan Zhang and Fei Han and Shiyang Shen and Yicheng Wang and Shilong Xu and Xiao Dong and Yihua Hu},
  doi          = {10.1049/ipr2.12691},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {1001-1009},
  shortjournal = {IET Image Process.},
  title        = {Target region extraction and segmentation algorithm for reflective tomography lidar image},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DCACorrCapsNet: A deep channel-attention correlative capsule
network for COVID-19 detection based on multi-source medical images.
<em>IETIP</em>, <em>17</em>(4), 988–1000. (<a
href="https://doi.org/10.1049/ipr2.12690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The raging trend of COVID-19 in the world has become more and more serious since 2019, causing large-scale human deaths and affecting production and life. Generally speaking, the methods of detecting COVID-19 mainly include the evaluation of human disease characterization, clinical examination and medical imaging. Among them, CT and X-ray screening is conducive to doctors and patients&#39; families to observe and diagnose the severity and development of the COVID-19 more intuitively. Manual diagnosis of medical images leads to low the efficiency, and long-term tired gaze will decline the diagnosis accuracy. Therefore, a fully automated method is needed to assist processing and analysing medical images. Deep learning methods can rapidly help differentiate COVID-19 from other pneumonia-related diseases or healthy subjects. However, due to the limited labelled images and the monotony of models and data, the learning results are biased, resulting in inaccurate auxiliary diagnosis. To address these issues, a hybrid model: deep channel-attention correlative capsule network, for channel-attention based spatial feature extraction, correlative feature extraction, and fused feature classification is proposed. Experiments are validated on X-ray and CT image datasets, and the results outperform a large number of existing state-of-the-art studies.},
  archive      = {J_IETIP},
  author       = {Aite Zhao and Huimin Wu and Ming Chen and Nana Wang},
  doi          = {10.1049/ipr2.12690},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {988-1000},
  shortjournal = {IET Image Process.},
  title        = {DCACorrCapsNet: A deep channel-attention correlative capsule network for COVID-19 detection based on multi-source medical images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-level deep mutual learning assisted multi-task network
for occluded person re-identification. <em>IETIP</em>, <em>17</em>(4),
979–987. (<a href="https://doi.org/10.1049/ipr2.12688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An occluded person re-identification (ReID) approach is presented by constructing a Bi-level deep Mutual learning assisted Multi-task network (BMM), where the holistic and occluded person ReID tasks are treated as two related but not identical tasks. This is inspired by the human perception characteristic that there exist both similarities and differences when human views a holistic image and the occluded one. Specifically, a multi-task network with two branches is designed, where the convolutional neural network based feature representation part shares the weights by two tasks for commonality extraction, while the following output layers have respective weights for difference representation. Furthermore, as the non-occluded regions convey discriminative information, a bi-level mutual learning strategy is proposed and applied mutually on two branches to obtain more effective information from the non-occluded regions in the occluded images for better identity recognition. This is achieved by both feature-level and output-level mutual loss functions. Extensive experiments prove the advantages of the BMM for person ReID.},
  archive      = {J_IETIP},
  author       = {Yi Wang and Liangbo Wang and Yu Zhou},
  doi          = {10.1049/ipr2.12688},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {979-987},
  shortjournal = {IET Image Process.},
  title        = {Bi-level deep mutual learning assisted multi-task network for occluded person re-identification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep MR parametric imaging with the learned l+s model and
attention mechanism. <em>IETIP</em>, <em>17</em>(4), 969–978. (<a
href="https://doi.org/10.1049/ipr2.12687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance (MR) parametric imaging can help the assessment of some certain diseases with its various contrast mechanisms. However, the main issue of MR parametric imaging is the long acquisition time, introducing many problems such as uncomfortable experiences for patients and motion artefacts. With the deep learning methods developing, some unrolling ones have been introduced to MR imaging as solutions. The purpose of this study is to improve both the quality and the speed of parametric imaging. The proposed method, named as LSA-Net, introduced a learned low rank plus sparsity model with attention mechanism to reconstruct highly under-sampled data for MR parametric imaging. The L + S model was used to represent the shared image structure among the parameter-weighted images as low-rank part and the difference among images and the ideal model as sparse part. Besides, the attention block was introduced to effectively improved quality for the region of interest, which was most concerned in clinical applications. Then the parameter map was generated from the reconstructed parameter-weighted images by its exponential model. The LSA-Net was evaluated on the in-vivo 3D T 1 ρ $T1\rho$ mapping, showing better performance on both image quality and time consumed than the comparing methods including LLR, SCOPE, L + S , and L + S -Net.},
  archive      = {J_IETIP},
  author       = {Wenyi Qu and Jing Cheng and Yanjie Zhu and Dong Liang},
  doi          = {10.1049/ipr2.12687},
  journal      = {IET Image Processing},
  month        = {3},
  number       = {4},
  pages        = {969-978},
  shortjournal = {IET Image Process.},
  title        = {Deep MR parametric imaging with the learned L+S model and attention mechanism},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single image super-resolution based on sparse representation
using edge-preserving regularization and a low-rank constraint.
<em>IETIP</em>, <em>17</em>(3), 956–968. (<a
href="https://doi.org/10.1049/ipr2.12685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation-based non-local self-similarity approaches have demonstrated promising performance in single image super-resolution reconstruction. This type of method, however, cannot always effectively preserve the key details of an image, resulting in edge artifacts and local structure blurring. To better preserve the image edge information, in this paper, a sparse representation super-resolution method based on non-local self-similarity is proposed. First, we impose slide window gradient domain guided filtering on both the low-resolution input image and the degraded restored image. Then, we utilize their difference as an edge-preserving regularization term and incorporate this regularization term into the non-local self-similarity-based sparse representation model to build a sparse coding model, which can enhance the restored high-resolution image patches’ detail information. Finally, the iterative threshold algorithm is used to calculate the sparse representation coefficients so that a high-resolution image can be estimated. Furthermore, to explore the potential structures of the subspaces spanned by similar patches, we enforce a low-rank matrix recovery technique on the generated super-resolution image, which can further refine the reconstruction quality. Experimental results prove that the new approach preserves the critical edge structures while suppressing noise and exceeding some popular methods both quantitatively and qualitatively.},
  archive      = {J_IETIP},
  author       = {Rui Gao and Deqiang Cheng and Qiqi Kou and Liangliang Chen},
  doi          = {10.1049/ipr2.12685},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {956-968},
  shortjournal = {IET Image Process.},
  title        = {Single image super-resolution based on sparse representation using edge-preserving regularization and a low-rank constraint},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and robust star detection algorithm based on the dyadic
wavelet transform. <em>IETIP</em>, <em>17</em>(3), 944–955. (<a
href="https://doi.org/10.1049/ipr2.12684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Star detection is an important part of the spacecraft attitude determination performed by star trackers. The current advanced star detection algorithms can effectively extract star points in complex backgrounds. However, there are still two problems: first, the parameters of the star detection algorithm are not adaptive, which limits its reliability and robustness; second, the high performance and low computational time of algorithms usually cannot be achieved simultaneously. In this work, a rapid star detection algorithm utilizing dyadic wavelet transform (SDDWT) was developed based on the à Trous algorithm. The authors constructed a suitable dyadic wavelet basis and applied the dyadic wavelet transform to the star image. The maximum wavelet coefficients at a certain scale were binarized according to a threshold value to separate stars from the background. The threshold was adaptively determined according to the white Gaussian noise (WGN) power. Finally, a method for separating individual stars was developed. Experiments indicated that the proposed algorithm was able to ensure a low false alarm rate and high detection rate by adaptively threshold value. Moreover, for a star image of 1024×1024, the computational time of the SDDWT algorithm was stable within 0.3 s, which was smaller than other algorithms.},
  archive      = {J_IETIP},
  author       = {Zhanglei Chen and Yong Zheng and Chonghui Li and Yinhu Zhan},
  doi          = {10.1049/ipr2.12684},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {944-955},
  shortjournal = {IET Image Process.},
  title        = {Fast and robust star detection algorithm based on the dyadic wavelet transform},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Four-branch siamese network based on sketch-specific data
augmentation for sketch recognition. <em>IETIP</em>, <em>17</em>(3),
932–943. (<a href="https://doi.org/10.1049/ipr2.12683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shortened abstract is as follows: Sketch recognition has become an important hotspot issue because of sketch&#39;s intuitiveness and visualization. The existing sketch-recognition methods based on handcrafted features and deep features are insufficient in the recognition of the local information of sketches, and the recognition accuracy is not ideal. Accordingly, this paper proposes a four-branch Siamese network based on sketch-specific data augmentation to generate discriminative feature representations and improve the sketch-recognition accuracy. A sketch is an ordered list of strokes, we adopt the semantic information of strokes as the decomposition criteria to divide a sketch into three disjoint local blocks, and then combine the local blocks in pairs to form three new sketches. In order to give full play to the positive effect of local blocks on category prediction and enhance the fine-grained capability of the network, three newly generated sketches and the original sketch are combined to construct a four-branch Siamese network. Each branch network adopts the Sketch-A-Net architecture with the fully connected layer removed as the basic network, and we improve it by adding shortcut connection layer and multi-scale weighted bilinear coding (MWBC) modules. Compared with the state-of-the-art methods, the experimental results on the TU-Berlin dataset demonstrate the excellent performance of our model.},
  archive      = {J_IETIP},
  author       = {Xiuying Wang and Qiang Zhao and Shoubiao Tan},
  doi          = {10.1049/ipr2.12683},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {932-943},
  shortjournal = {IET Image Process.},
  title        = {Four-branch siamese network based on sketch-specific data augmentation for sketch recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compressive sensing reconstruction of hyperspectral images
based on codec space-spectrum joint dense residual network.
<em>IETIP</em>, <em>17</em>(3), 916–931. (<a
href="https://doi.org/10.1049/ipr2.12682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial and spectral information contained in the hyperspectral image (HSI) make it widely used in many fields. However, the sharp increase of HSI data brings enormous pressure to the data storage and real-time transmission. The research shows that hyperspectral compressive sensing (HCS) breaks through the bottleneck of the Nyquist sampling theorem, which can relieve the massive pressure on data storage and real-time transmission. Existing HCS methods try to design advanced compression sampling matrix or reconstruction algorithms, but cannot connect the two through a unified framework. To further improve the image reconstruction quality, a novel codec space-spectrum joint dense residual network (CDS2-DResN) is proposed. The CDS2-DResN is divided into block compression sampling part and reconstruction part. For block compression sampling, coded convolutional layer (CCL) is leveraged to compress and sample HSI. For measurements reconstruction, deconvolution layer is first leveraged to initially reconstruct HSI, and then build a space-spectrum joint network to refine the initial reconstructed HSI. Moreover, the CCL and reconstruction network are optimized via a unified framework, which can simplify the pre-processing and post-processing process of HCS. Extensive experiments have shown that CDS2-DResN has an excellent HCS reconstruction effect at measurement rates 0.25, 0.10, 0.04 and 0.01, respectively.},
  archive      = {J_IETIP},
  author       = {Shuming Xiao and Ye Zhang and Xuling Chang and Jiajia Xu},
  doi          = {10.1049/ipr2.12682},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {916-931},
  shortjournal = {IET Image Process.},
  title        = {Compressive sensing reconstruction of hyperspectral images based on codec space-spectrum joint dense residual network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting visual difference maps for computer-generated
images by integrating human visual system model and deep learning.
<em>IETIP</em>, <em>17</em>(3), 901–915. (<a
href="https://doi.org/10.1049/ipr2.12681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quality of images generated by computer graphics rendering algorithms is mainly affected by visible distortion at some pixel locations. Image quality assessment (IQA) metrics are commonly utilized to assess the quality of rendered images, but their results are a global difference value, which does not provide pixel-wise differences to optimize the renderings. In contrast, visibility difference models including visual perception models and deep learning models can calculate pixel-wise visibility difference between distorted images and reference images. However, they either are only applied to a single type of visible distortion or are seriously dependent on datasets. To this end, the authors propose a novel model, dubbed Human Visual Perception and Deep Learning Image Difference Metric (HPDL-IDM), which combines the Human Visual System (HVS) model and deep learning. HPDL-IDM primarily consists of two modules: (i) the visual perception feature calculation module, which calculates difference maps between various kinds of features extracted from the reference image and the distorted image according to the visual characteristics of human eyes and concatenates them, and (ii) the deep learning module, which utilizes a neural network of encoder–decoder structure to train on the LocvisVC and VisTexRes datasets whose input and output are these concatenated feature difference maps and the final image distortion visibility difference map respectively. Additionally, the authors pool the final difference map into a global difference value between 0 and 1 to apply their model to many image processing tasks related to Image quality metrics (IQMs). Experimental results show that HPDL-IDM&#39;s generalization capacity and accuracy are improved by a large margin compared to other models.},
  archive      = {J_IETIP},
  author       = {Ling Li and Chunyi Chen and Jun Peng and Ripei Zhang},
  doi          = {10.1049/ipr2.12681},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {901-915},
  shortjournal = {IET Image Process.},
  title        = {Predicting visual difference maps for computer-generated images by integrating human visual system model and deep learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Salt-and-pepper denoising method for colour images based on
tensor low-rank prior and implicit regularization. <em>IETIP</em>,
<em>17</em>(3), 886–900. (<a
href="https://doi.org/10.1049/ipr2.12680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the information obtained by humans comes from colour images. However, salt-and-pepper noise (SPN) during signal acquisition, encoding, transmission, and decoding easily interferes with the quality of colour images. Most existing SPN denoising methods decompose a colour image into three independent matrices according to the colour channel and then recover each channel signal independently, ignoring the strong data correlation between channels. In addition, most existing SPN denoising methods apply only a single model-driven or data-driven approach and fail to take the advantages of their combination fully. Therefore, we first regard a colour image contaminated by SPN as the sum of an SPN tensor and a tensor with missing data. In this manner, we transform the denoising problem into a low-rank tensor reconstruction problem. We then introduce a model-driven-based parallel matrix factorization low-rank tensor reconstruction algorithm and a data-driven-based FFDNet denoising network to restore the colour image better. The proposed method not only enhances the similarity of the colour image channels but also explores the deep prior of the colour image to capture the image details. Finally, the proposed method is compared with some advanced denoising methods. The results show that the proposed method achieves a competitive denoising performance.},
  archive      = {J_IETIP},
  author       = {Jun Zhang and Zhao-yang Li and Ling-zhi Wang and Ying-pin Chen},
  doi          = {10.1049/ipr2.12680},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {886-900},
  shortjournal = {IET Image Process.},
  title        = {Salt-and-pepper denoising method for colour images based on tensor low-rank prior and implicit regularization},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-supervised learning dehazing algorithm based on the OSV
model. <em>IETIP</em>, <em>17</em>(3), 872–885. (<a
href="https://doi.org/10.1049/ipr2.12679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great progress that has been made in the task of single image dehazing, the results of the existing models in restoring image edge and texture information are still challenging. Besides, most dehazing models are trained on synthetic data, resulting in poor generalization ability to real-world images. To address the aforementioned problems, a semi-supervised learning dehazing method based on the decomposition model of Osher, Solé, and Vese(The OSV model) is presented. Specifically, the OSV model is first applied to decompose the hazy image into the structure layer and texture layer, save the texture layer and dehaze for the structure layer to restore images with sharper texture and edge. Furthermore, the network adopts a semi-supervised learning algorithm based on generative adversarial networks (GAN) to generalize better to real-world images, which includes two branches: supervised learning and unsupervised learning. Extensive experiments indicate that the proposed method preserves the texture and edge information of images more accurately while dehazing better, and performs favourably against the advanced dehazing algorithms on both synthetic outdoor datasets and real-world hazy images.},
  archive      = {J_IETIP},
  author       = {Lijun Zhu and Weibo Wei and Zhenkuan Pan and Lianshun Ji and Jintao Song and Jinhan Li},
  doi          = {10.1049/ipr2.12679},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {872-885},
  shortjournal = {IET Image Process.},
  title        = {Semi-supervised learning dehazing algorithm based on the OSV model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WaterSegformer: A lightweight model for water body
information extraction from remote sensing images. <em>IETIP</em>,
<em>17</em>(3), 862–871. (<a
href="https://doi.org/10.1049/ipr2.12678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient extraction of water body information from remote sensing images is of great help to monitor water resources at the macro level, natural disaster prediction, and water pollution detection and prevention. Although many large models have achieved extremely high accuracy in remote sensing image water segmentation tasks, lightweight models are still a non-negligible choice for many application scenarios because of the limitation of computing and storage resources. Here, WaterSegformer is described, an efficient and powerful lightweight water body segmentation model based on Segformer-b0. The Deepmask module is designed to make the model pay more attention to the details in the image and use Lovász loss to improve IoU. In addition, DeepLabv3+ is used as the teacher model to guide the training of the model in the way of relational knowledge distillation. WaterSegformer realizes 95.06% mIoU on the test set with only 6.38 G and 3.72 M of FLOPs and parameters, respectively. Experimental results show that WaterSegformer achieves an excellent balance between accuracy, computational complexity and model size, which is hardware-friendly, easy to deploy and enables real-time segmentation. This method provides a new idea for water body information extraction from remote sensing images in practical applications.},
  archive      = {J_IETIP},
  author       = {Xiao Yang and Mingwei Chen and Chengjun Yu and Haozhe Huang and Xiaobin Yue and Bei Zhou and Ming Ni},
  doi          = {10.1049/ipr2.12678},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {862-871},
  shortjournal = {IET Image Process.},
  title        = {WaterSegformer: A lightweight model for water body information extraction from remote sensing images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage local attention network for salient object
detection in remote sensing images. <em>IETIP</em>, <em>17</em>(3),
849–861. (<a href="https://doi.org/10.1049/ipr2.12677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of earth observation satellites, it is very meaningful to study methods for intelligently processing remote sensing images (RSIs). As an important pretreatment technique, salient object detection (SOD) can produce accurate saliency predictions in natural scenes. However, due to the complex object types, high noise levels, and different imaging angles in RSIs, it is still difficult to generate accurate saliency predictions in such images. This paper proposes a SOD method for RSIs, namely, the two-stage local attention network (TLANet). It uses two-stage attention (TSA) to capture the relationships between the pixels of features at different scales and uses an adjacent semantic refinement (ASR) module to enhance the flow of information across scales. In addition, a cascade structure is introduced to the decoder so that more semantic features can guide the feature decoding process. Moreover, a new loss function is proposed to improve the prediction effects obtained for salient edges. Through comprehensive experiments conducted on two RSIs datasets, it is demonstrated that the TLANet is superior to the state-of-the-art methods in both qualitative and quantitative comparisons.},
  archive      = {J_IETIP},
  author       = {Qihui Lin and Lurui Xia and Sen Li and Wanfeng Chen},
  doi          = {10.1049/ipr2.12677},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {849-861},
  shortjournal = {IET Image Process.},
  title        = {Two-stage local attention network for salient object detection in remote sensing images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel decoder based on bayesian rules for task-driven
object segmentation. <em>IETIP</em>, <em>17</em>(3), 832–848. (<a
href="https://doi.org/10.1049/ipr2.12676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a challenging problem in computer vision, salient object segmentation has attracted increasing attention in recent years. Though a lot of works based on encoder–decoder have been made, these methods can only recognize and segment one class of objects, but cannot segment the other classes of objects in the same image. To address this issue, this paper proposes a novel decoder based on Bayesian rules to perform task-driven object segmentation, in which a control signal is added to the decoder to determine which class of objects need to be segmented. What&#39;s more, a Bayesian rule is established in the decoder, in which the control signal is set as the prior, and the latent features learned in encoder is transferred to the corresponding layer of decoder as observation, thus the posterior probability of each object with respect to the specific-class can be calculated, and the objects belonging to this class can be segmented. This proposed method is evaluated for task-driven salient object segmentation on several benchmark datasets, including MS COCO, DUT-OMRON, ECSSD etc. Experimental results show that the approach tends to segment accurate, detailed, and complete objects, and improves the performance compared with the previous state-of-the-art.},
  archive      = {J_IETIP},
  author       = {Yuxiang Cai and Yuanlong Yu and Weijie Jiang and Rong Chen and Weitao Zheng and Xi Wu and Renjie Su},
  doi          = {10.1049/ipr2.12676},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {832-848},
  shortjournal = {IET Image Process.},
  title        = {A novel decoder based on bayesian rules for task-driven object segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hamate classification method based on feature-enhanced
residual network and probabilistic joint judgment. <em>IETIP</em>,
<em>17</em>(3), 819–831. (<a
href="https://doi.org/10.1049/ipr2.12675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate analysis of the maturity grade of the reference bone in the wrist is critical for bone age assessment. The main difference between the maturity grades of the reference bone in the wrist X-ray image is the difference in the texture and morphological characteristics of the bone, and the difference in the characteristics between adjacent levels is small, which brings great challenges in bone age assessment. The hamate (the wrist bone in line with the 4th and 5th fingers) is one of the most important reference bones in the standards of skeletal maturity of the hand and wrist for the Chinese (CHN) method. Aiming at the problem of hamate maturity level evaluation, a method of hamate maturity level classification based on feature enhanced residual network and probability joint judgment is proposed. In this method, we propose the enhanced characteristic residual network (ECR-Net) is proposed to enhance the feature extraction capability of the network and improve the loss function to reduce the impact of cross-grade errors on the accuracy of bone age assessment. On this basis, multiple convolutional neural networks to make joint probabilistic judgments are relied on to obtain the final hamate maturity grade. The proposed ECR-Net achieves a macro accuracy of 96.92% on the hamate maturity evaluation, and there are no errors across two levels. Also, this approach achieves a macro accuracy of 97.15% when the probabilistic joint judgment method is employed while avoiding errors that span more than two levels. The network model and method proposed in this paper have good accuracy and practicability for the automatic identification of the hamate maturity level, which is of great significance for the accurate assessment of bone age.},
  archive      = {J_IETIP},
  author       = {Wei-long Ding and Ze-yong Zong and Xiao Ding and Ke-ji Mao},
  doi          = {10.1049/ipr2.12675},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {819-831},
  shortjournal = {IET Image Process.},
  title        = {Hamate classification method based on feature-enhanced residual network and probabilistic joint judgment},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FAFEnet: A fast and accurate model for automatic license
plate detection and recognition. <em>IETIP</em>, <em>17</em>(3),
807–818. (<a href="https://doi.org/10.1049/ipr2.12674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic License Plate detection and Recognition (ALPR) is a key problem in intelligent transportation systems with wide applications in traffic monitoring, electronic toll collection (ETC), intelligent parking lots (IPLs), and elsewhere. Although numerous methods have been proposed in the literature, it is still challenging to strike a good balance between the accuracy and efficiency of ALPR. In this paper, a novel end-to-end CNN-based model is proposed, called Fast and Accurate Network with Feature Enhancement (FAFEnet), to jointly detect the license plates and recognize the characters with high accuracy and efficiency. Specifically, the FAFEnet model seamlessly integrates two CNN-based models, namely the detection and recognition modules, into a unified framework to reduce accumulated errors and computational overheads in two separate steps. The detection module is a lightweight model with only seven convolutional layers yet achieves over 99.8% accuracy rates for license plate detection across all datasets. The recognition module utilizes two feature enhancement blocks to compensate and enhance the shallow character features extracted by the detection module. Furthermore, the joint optimization of detection and recognition modules exploits the feature association in two modules, and thus improves the prediction accuracy while reducing the execution time. Finally, extensive experimental results on several real-world datasets demonstrate that FAFEnet outperforms all the competitors in terms of both accuracy and efficiency.},
  archive      = {J_IETIP},
  author       = {Xin Zhou and Yao Cheng and Liling Jiang and Bo Ning and Yanhao Wang},
  doi          = {10.1049/ipr2.12674},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {807-818},
  shortjournal = {IET Image Process.},
  title        = {FAFEnet: A fast and accurate model for automatic license plate detection and recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rich-scale feature fusion network for salient object
detection. <em>IETIP</em>, <em>17</em>(3), 794–806. (<a
href="https://doi.org/10.1049/ipr2.12673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully convolutional neural networks-based salient object detection has recently achieved great success with its performance benefits from the effective use of multi-layer features. Based on this, most of the existing saliency detectors designed complex network structures to fuse the multi-level features generated by the backbone network. However, the variable scale and complex shape of the target are always a great challenge for saliency detection tasks. In this paper, the authors propose a Rich-scale Feature Fusion Network (RFFNet) for salient object detection. The authors design a rich-scale feature interactive fusion module to obtain more efficient features from the multi-scale features. Moreover, the global feature enhance module is used to extract features with better characterization for the final saliency prediction. Extensive experiments performed on five benchmark datasets demonstrate that the proposed method can achieve satisfactory results on different evaluation metrics compared to other state-of-the-art salient object detection approaches.},
  archive      = {J_IETIP},
  author       = {Fengming Sun and Junjie Cui and Xia Yuan and Chunxia Zhao},
  doi          = {10.1049/ipr2.12673},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {794-806},
  shortjournal = {IET Image Process.},
  title        = {Rich-scale feature fusion network for salient object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A pseudo-colour enhancement algorithm for high-bit RAW
greyscale image of x-ray film displaying on low-bit monitors.
<em>IETIP</em>, <em>17</em>(3), 784–793. (<a
href="https://doi.org/10.1049/ipr2.12672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray flaw detection is widely used in non-destructive area. The intuitive defect information can be obtained through the X-ray film, which is usually digitized into high greyscale image by a 12-bit or 16-bit, called super 8-bit, industrial scanner. When an ordinary 8-bit monitor displays a super 8-bit greyscale image, it appears loss of detail information, blurring of the image appears and other problems. Therefore, in this paper, a pseudo-colour enhancement algorithm for displaying high-bit RAW images on low-bit monitor was proposed according to the chromatographic mapping relationship based on the visual characteristics of human eye. First, a high grey-scale pseudo-colour enhancement algorithm, called HOTM-HGL, based on hot metal coding was proposed based on the standard film, whose enhancement effect is better than current mainstream algorithms. Second, aiming at the non-standard film, a new stretching function called RAW-Optical-Stretching was reconstructed to improve HOTM-HGL algorithm, called HOTM-HGLS algorithm, whose display effect on ordinary monitors was improved in further. Finally, HOTM-HGLS algorithm was applied in the detection of X-ray film defect, which was convenient for the capture of defect information in the weld. Compared with the existing algorithms, various indicators have been greatly improved, enriching the amount of information and strengthening the image recognition effects.},
  archive      = {J_IETIP},
  author       = {Zhigang Lv and Liangliang Li and Hongxi Wang and Peng Wang and Jianheng Li and Lei Shu and Xiaoyan Li},
  doi          = {10.1049/ipr2.12672},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {784-793},
  shortjournal = {IET Image Process.},
  title        = {A pseudo-colour enhancement algorithm for high-bit RAW greyscale image of X-ray film displaying on low-bit monitors},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A 3D graph convolutional networks model for 2D
skeleton-based human action recognition. <em>IETIP</em>, <em>17</em>(3),
773–783. (<a href="https://doi.org/10.1049/ipr2.12671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of cameras, the application of action recognition is more and more extensive. After the emergence of RGB-D cameras and human pose estimation algorithms, human actions can be represented by a sequence of skeleton joints. Therefore, skeleton-based action recognition has been a research hotspot. In this paper, a novel 3D Graph Convolutional Network model (3D-GCN) with space-time attention mechanism for 2D skeleton data is proposed. Three-dimensional graph convolution is employed to extract spatiotemporal features of skeleton descriptor that is composed of joint coordinates, frame differences and angles. Meanwhile, different joints and different frames are given different attention to achieve action classification. A zebra crossing pedestrian dataset named ZCP is also provided, which simulates possible pedestrian actions on the zebra crossing in real scenes. Experimental evaluation is carried out on ZCP dataset and NTU RGB+D dataset. Experimental results show that our method is better than current 2D-based methods and is comparable with 3D methods.},
  archive      = {J_IETIP},
  author       = {Libo Weng and Weidong Lou and Xin Shen and Fei Gao},
  doi          = {10.1049/ipr2.12671},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {773-783},
  shortjournal = {IET Image Process.},
  title        = {A 3D graph convolutional networks model for 2D skeleton-based human action recognition},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FIMF score-CAM: Fast score-CAM based on local multi-feature
integration for visual interpretation of CNNS. <em>IETIP</em>,
<em>17</em>(3), 761–772. (<a
href="https://doi.org/10.1049/ipr2.12670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretability of the model is a hot issue in the field of computer vision. Score-CAM is a kind of interpretable CAM method with good discrimination and gradient free calculation. It is a representative work in this field. However, it has the disadvantages of long calculation time and incomplete heatmap coverage. Therefore, this paper proposes an improved Score-CAM method named FIMF Score-CAM, which can fast integrate multiple features. Its contribution is reflected in two aspects: The weight of the feature map is directly calculated by using the feature template. Unlike Score-CAM, this model greatly saves computation time because it only requires one convolutional calculation. Another contribution is that the feature map used to generate the heatmap integrates various semantic features of the local space, so that the heatmap of the object of interest can be generated with more complete coverage and better interpretation. The FIMF Score-CAM is superior to the existing mainstream models in interpreting the visual performance and fairness indicators of the decision-making, having more complete explanations of object classes and the advantage of faster calculation speed.},
  archive      = {J_IETIP},
  author       = {Jing Li and Dongbo Zhang and Bumin Meng and Yongxing Li and Lufeng Luo},
  doi          = {10.1049/ipr2.12670},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {761-772},
  shortjournal = {IET Image Process.},
  title        = {FIMF score-CAM: Fast score-CAM based on local multi-feature integration for visual interpretation of CNNS},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous learning deraining network based on residual FFT
convolution and contextual transformer module. <em>IETIP</em>,
<em>17</em>(3), 747–760. (<a
href="https://doi.org/10.1049/ipr2.12669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image deraining work has attracted a lot of attention. So an effective single image deraining network model, RFCTNet, is designed. Specifically, the residual Fast Fourier Transform convolution module is used as a baseline for the deraining network. The advantage is its ability to capture both long and short-term information interactions and to integrate high and low-frequency background information for delivery. And a novel Transformer module, the Contextual Transformer module, is introduced. Contextual knowledge of the rain streaks neighbourhood is learned to improve the recovery of textures and information in the image background based on focusing on the rain pattern features. Because of the overlap between rain streaks, removing them in one stage is difficult, so a multi-stage progressive rain removal method is used. Considering the catastrophic forgetting problem of deep learning, a continuous learning method, PIGWM, is adopted, which is c and enables the network model to memorize of the previous data set. Also, the rain removal method RFCTNet has been proven through extensive experiments to outperform other advanced methods&#39; evaluation metrics on both synthetic and real datasets. The source code can be found on https://github.com/SUTwu/RFCTNet},
  archive      = {J_IETIP},
  author       = {Zhijia Zhang and Sinan Wu and Xinming Peng and Wanting Wang and Rui Li},
  doi          = {10.1049/ipr2.12669},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {747-760},
  shortjournal = {IET Image Process.},
  title        = {Continuous learning deraining network based on residual FFT convolution and contextual transformer module},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An unsupervised multi-focus image fusion method based on
transformer and u-net. <em>IETIP</em>, <em>17</em>(3), 733–746. (<a
href="https://doi.org/10.1049/ipr2.12668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a multi-focus image fusion method based on Transformer and U-Net with an unsupervised training fashion. In this work, the authors introduce Transformer into image fusion because it has great ability to capture the global dependencies and low-frequency features. In image processing, convolutional neural network (CNN) has good performance of detailed feature extraction but a weakness for global feature extraction, and Transformer has limited power in local or detailed information extraction but a strong capacity for global feature extraction. Thus, this work combines the advantages of CNN and Transformer to propose an unsupervised decision map making model for image fusion joint U-Net. The authors construct a model including feature extraction and feature reconstruction modules which correspond to the encoder and decoder network of U-Net, respectively. In addition, perceptual loss is introduced on the basis of structural similarity loss because the combination of these two loss functions can achieve better performance with lower training cost. Experiments show that the proposed image fusion method performs better fusion performance compared with the existing methods.},
  archive      = {J_IETIP},
  author       = {Xin Jin and Xiuliang Xi and Ding Zhou and Xiaoxuan Ren and Jie Yang and Qian Jiang},
  doi          = {10.1049/ipr2.12668},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {733-746},
  shortjournal = {IET Image Process.},
  title        = {An unsupervised multi-focus image fusion method based on transformer and U-net},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fall detection based on OpenPose and MobileNetV2 network.
<em>IETIP</em>, <em>17</em>(3), 722–732. (<a
href="https://doi.org/10.1049/ipr2.12667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposed fall detection approach is aimed at building a support system for the elders. In this work, a method based on human pose estimation and lightweight neural network is used to detect falls. First, the OpenPose is used to extract human keypoints and label them in the images. After that, the modified MobileNetV2 network is used to detect falls by integrating both human keypoint information and pose information in the original images. The above operation can use the original image information to correct the deviation in the keypoint labeling process. Through experiments, the accuracy of the proposed method is 98.6% and 99.75% on the UR and Le2i datasets, which is higher than the listed comparison methods.},
  archive      = {J_IETIP},
  author       = {Mengqi Gao and Jiangjiao Li and Dazheng Zhou and Yumin Zhi and Mingliang Zhang and Bin Li},
  doi          = {10.1049/ipr2.12667},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {722-732},
  shortjournal = {IET Image Process.},
  title        = {Fall detection based on OpenPose and MobileNetV2 network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video abnormal behaviour detection based on pseudo-3D
encoder and multi-cascade memory mechanism. <em>IETIP</em>,
<em>17</em>(3), 709–721. (<a
href="https://doi.org/10.1049/ipr2.12666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frame prediction methods based on Auto-Encoder (AE) composed of convolutional neural networks (CNN) are very popular in detecting abnormal behaviour. The methods predict normal behaviour accurately and abnormal behaviour incorrectly, which is considered a criterion for abnormality discrimination. However, the emergence of problems such as too strong AE representation leading to detection failure, the insufficient ability of the network to extract spatio-temporal information, a large number of model parameters and slow running speed leads to the need for the method to be further improved. In this work, the authors propose a network framework for abnormal behaviour detection in video based on a pseudo-3D encoder and a multi-cascade memory mechanism (MMP3D). First of all, the encoder consisting of pseudo-3D convolution is used to extract spatio-temporal information from the video. Then, the multi-cascade memory mechanism (MM) and the multi-headed prototype attention mechanism are used to store and aggregate features of normal behaviour, which solves to some extent the problem of detection failure caused by strong AE representation power. Finally, the decoder designed by the 2D deconvolution layers is used to recover the prediction information. The efficiency and superiority of our method is validated on the Ped2 dataset, Avenue dataset, and ShanghaiTech dataset.},
  archive      = {J_IETIP},
  author       = {Xiaopeng Wen and Huicheng Lai and Guxue Gao and Yanjie Zhao},
  doi          = {10.1049/ipr2.12666},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {709-721},
  shortjournal = {IET Image Process.},
  title        = {Video abnormal behaviour detection based on pseudo-3D encoder and multi-cascade memory mechanism},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aircraft tracking in aerial videos based on fused RetinaNet
and low-score detection classification. <em>IETIP</em>, <em>17</em>(3),
687–708. (<a href="https://doi.org/10.1049/ipr2.12665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aircraft tracking in aerial videos is a challenge due to the small size and appearance affinity with other targets. Recently, the tracking-by-detection trackers, such as simple online and realtime tracking (SORT), attracted the attention of many researchers due to the great evolution in deep learning-based object detectors. SORT depends on motion cues in solving the data association problem by matching high-score detection bounding boxes to tracklets. However, motion cues are insufficient to track occluded or similar aircraft efficiently. In addition, most occluded aircrafts have a low-confidence percentage. The main objective of this paper is to propose an F-SORT tracker, an enhanced SORT tracker with the following improvements. Firstly, fused RetinaNet for small aerial targets is used as a detection framework to boost the detectability of SORT. Then, aircraft appearance features are extracted from the shallowest prediction layer and utilized as additional cues to improve the data association algorithm. Finally, the extracted features distinguish aircraft online from backgrounds via a K-nearest neighbour classifier. Experiments on an airport dataset show that F-SORT can significantly improve aircraft tracking. F-SORT outperforms other state-of-the-art trackers, achieving 72.75%, 59.63%, and 82.89% on MOTA, HOTA, and IDF1. In addition, F-SORT classifying algorithm boosts other trackers when applied.},
  archive      = {J_IETIP},
  author       = {M. Ahmed and Ali Maher and X. Bai},
  doi          = {10.1049/ipr2.12665},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {687-708},
  shortjournal = {IET Image Process.},
  title        = {Aircraft tracking in aerial videos based on fused RetinaNet and low-score detection classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pose-driven human activity anomaly detection in a CCTV-like
environment. <em>IETIP</em>, <em>17</em>(3), 674–686. (<a
href="https://doi.org/10.1049/ipr2.12664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity anomaly detection plays a crucial role in the next generation of surveillance and assisted living systems. Most anomaly detection algorithms are generative models and learn features from raw images. This work shows that popular state-of-the-art autoencoder-based anomaly detection systems are not capable of effectively detecting human-posture and object-positions related anomalies. Therefore, a human pose-driven and object-detector-based deep learning architecture is proposed, which simultaneously leverages human poses and raw RGB data to perform human activity anomaly detection. It is demonstrated that pose-driven learning overcomes the raw RGB based counterpart limitations in different human activities classification. Extensive validation is provided by using popular datasets. Then, it is demonstrated that with the aid of object detection, the human activities classification can be effectively used in human activity anomaly detection. Moreover, novel challenging datasets, that is, BMbD, M-BMbD and JBMOPbD, are proposed for single and multi-target human posture anomaly detection and joint human posture and object position anomaly detection evaluations.},
  archive      = {J_IETIP},
  author       = {Yuxing Yang and Federico Angelini and Syed Mohsen Naqvi},
  doi          = {10.1049/ipr2.12664},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {674-686},
  shortjournal = {IET Image Process.},
  title        = {Pose-driven human activity anomaly detection in a CCTV-like environment},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Full-scale attention network for automated organ
segmentation on head and neck CT and MR images. <em>IETIP</em>,
<em>17</em>(3), 660–673. (<a
href="https://doi.org/10.1049/ipr2.12663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MRI and CT images have been routinely used in clinical practice for treatment planning of the head-and-neck (HAN) radiotherapy. Delineating organs-at-risk (OAR) is an essential step in radiotherapy, however, it is time-consuming and prone to inter-observer variation. The existing automatic segmentation approaches are either limited by image registration or lack of global spatial awareness, thus under-performed when dealing with segmentation of complex anatomies. Herein, we propose a full-scale attention network (FSANet) that integrates bi-side skip connections, full-scale feature fusion modules (FFM), a feature pyramid fusion and supervision module (FPFSM) to accurately and efficiently delineate OARs in HAN region on CT and MRI scans. Specifically, bi-side skip connections were adopted to keep small targets in the deep network and to capture semantic features at different scales. The FFM with cascaded attention mechanisms were used to recalibrate the significant channels and salient regions in the feature maps. The FPFSM was used to guide the network to learn the hierarchical representation so as to improve the segmentation robustness. The proposed algorithm was validated on the public benchmark HAN CT dataset and an in-house MR dataset. Both results show significant improvement compared to state-of-the-art OAR single-stage segmentation methods for the HAN region.},
  archive      = {J_IETIP},
  author       = {Zhiqiang Zhong and Lian He and Changxiu Chen and Xingli Yang and Li Lin and Ziye Yan and Mengqiu Tian and Ying Sun and Yinwei Zhan},
  doi          = {10.1049/ipr2.12663},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {660-673},
  shortjournal = {IET Image Process.},
  title        = {Full-scale attention network for automated organ segmentation on head and neck CT and MR images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ice accretion thickness prediction using flash infrared
thermal imaging and BP neural networks. <em>IETIP</em>, <em>17</em>(3),
649–659. (<a href="https://doi.org/10.1049/ipr2.12662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present study investigated ice accretion thickness under non-incoming flow icing conditions on the ground using an infrared thermography system that converts infrared radiation temperature. Two back propagation (BP) neural network models were developed to measure ice thickness. Both theoretical model and polynomials were employed to fit the icing surface temperature elevation sequence to extract the pixel-level temperature attenuation characteristics, which were served as the input to the BP neural networks. The prediction method of ice thickness by the BP neural network was analysed from three perspectives of sensitivity, dimension, and precision. In addition, K-nearest neighbour (KNN) and support vector regression (SVR) algorithms were compared with BP neural network. In terms of prediction effect, the BP neural network performed the best. The verification of the BP neural network based on the characteristics of the theoretical model proved that the method can effectively predict the thickness of ice accretion, and its prediction error does not exceed 10%.},
  archive      = {J_IETIP},
  author       = {Lu Hao and Qingying Li and Weichen Pan and Rao Yao and Senyun Liu},
  doi          = {10.1049/ipr2.12662},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {649-659},
  shortjournal = {IET Image Process.},
  title        = {Ice accretion thickness prediction using flash infrared thermal imaging and BP neural networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pain estimation with integrating global-wise and region-wise
convolutional networks. <em>IETIP</em>, <em>17</em>(3), 637–648. (<a
href="https://doi.org/10.1049/ipr2.12639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pain is a common phenomenon in clinical patients, which indicates patients are suffering from uncomfortable conditions for necessary treatments. So the assessment of pain status becomes a significant task in current medical institutions. Of late, various conventional hand-crafted or deep learning methods on face images are presented to estimate pain intensity automatically. However, these approaches usually feed the whole face into the automatic estimation system and explore little information on the interdependencies of related regions during the formation of pain expression. In this paper, a hierarchical deep network (HDN) involving regional and holistic information simultaneously is proposed via two scale branches. In HDN, a region-wise branch is designed to extract features from pain related regions of face images while a global-wise branch explores the interdependencies of pain related regions. Besides, in global-wise branch, a multi-task learning method is employed to detect action units while estimating pain intensity. Finally, the pain estimation outputs of two branches are fused in a decision level. On current pain estimation benchmarks, it is empirically shown that the proposed HDN outperforms the existing methods and the essential components in HDN have key influences on final prediction.},
  archive      = {J_IETIP},
  author       = {Dong Huang and Zhaoqiang Xia and Lei Li and Yupeng Ma},
  doi          = {10.1049/ipr2.12639},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {3},
  pages        = {637-648},
  shortjournal = {IET Image Process.},
  title        = {Pain estimation with integrating global-wise and region-wise convolutional networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamically adaptive adjustment loss function biased towards
few-class learning. <em>IETIP</em>, <em>17</em>(2), 627–635. (<a
href="https://doi.org/10.1049/ipr2.12661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution neural networks have been widely used in the field of computer vision, which effectively solve practical problems. However, the loss function with fixed parameters will affect the training efficiency and even lead to poor prediction accuracy. In particular, when there is a class imbalance in the data, the final result tends to favor the large-class. In detection and recognition problems, the large-class will dominate due to its quantitative advantage, and the features of few-class can be not fully learned. In order to learn few-class, batch nuclear-norm maximization is introduced to the deep neural networks, and the mechanism of the adaptive composite loss function is established to increase the diversity of the network and thus improve the accuracy of prediction. The proposed loss function is added to the crowd counting, and verified on ShanghaiTech and UCF_CC_50 datasets. Experimental results show that the proposed loss function improves the prediction accuracy and convergence speed of deep neural networks.},
  archive      = {J_IETIP},
  author       = {Guoqi Liu and Lu Bai and Junlin Li and Xusheng Li and Linyuan Ru and Baofang Chang},
  doi          = {10.1049/ipr2.12661},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {627-635},
  shortjournal = {IET Image Process.},
  title        = {Dynamically adaptive adjustment loss function biased towards few-class learning},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An efficient anchor-free method for pig detection.
<em>IETIP</em>, <em>17</em>(2), 613–626. (<a
href="https://doi.org/10.1049/ipr2.12659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the rapid growth of commercial pig farms, the need to automatically monitor pig behaviour becomes more important in order to assist farmers. Recent advances in convolutional neural networks may pave the way for new solutions. However, the primary task of individual pig detection under real-world conditions is still a challenging task. Previous studies used anchor-based frameworks that are unsuitable for such crowded scenarios with extreme overlapping. Furthermore, most applications focus on specific levels of brightness, farm facilities, or pig species without considering generalization. To tackle these problems, an anchor-free pig detection method based on pig centre localization is first proposed. Then, a novel negative training data augmentation technique is introduced using examples from outside the training distribution. Furthermore, using the test time augmentation technique is proposed to improve the model performance. Experiments are conducted on two online pig detection datasets; the network surpasses state-of-the-art results for both datasets. It is also found that the proposed method outperforms the latest anchor-free techniques commonly used in crowded scenarios. The method can detect pigs individually, even if their bounding boxes overlap strongly or occlude each other. Moreover, the real-time system achieves an improvement of 10% in F measure $F_{\text{measure}}$ when testing in unconstrained real-world conditions.},
  archive      = {J_IETIP},
  author       = {Morann Mattina and Abdesslam Benzinou and Kamal Nasreddine and Francis Richard},
  doi          = {10.1049/ipr2.12659},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {613-626},
  shortjournal = {IET Image Process.},
  title        = {An efficient anchor-free method for pig detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QTMT-LNN: A fast intra CU partition using lightweight neural
network for 360-degree video coding on VVC. <em>IETIP</em>,
<em>17</em>(2), 597–612. (<a
href="https://doi.org/10.1049/ipr2.12658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the newest generation of video coding standard, Versatile Video Coding (VVC), a new technique called Quad Tree with nested Multi-type Tree (QTMT) structure is introduced. QTMT significantly improves the coding efficiency, but the improvement in compression performance comes at the cost of drastically increased complexity. This paper proposes a fast intra partition algorithm using Lightweight Neural Network (LNN) to skip QTMT partition steps which are unlikely to be chosen as the best split modes. Specifically, five LNNs (for five QTMT split modes) are trained, using features that consider the characteristic of 360-degree videos. The experimental results demonstrate that the proposed QTMT-LNN can reduce the encoding time from 52.28% to 72.17% on average, with coding efficiency losses ranging from 0.71% to 1.78%. Compared to other fast algorithms for intra coding unit (CU) partition, the method outperforms the related works in terms of Bjontegaard delta bit-rate (BDBR) and encoding time reduction.},
  archive      = {J_IETIP},
  author       = {Zhewen Sun and Li Yu and Wei Peng},
  doi          = {10.1049/ipr2.12658},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {597-612},
  shortjournal = {IET Image Process.},
  title        = {QTMT-LNN: A fast intra CU partition using lightweight neural network for 360-degree video coding on VVC},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MSFRNet: Two-stream deep forgery detector via multi-scale
feature extraction. <em>IETIP</em>, <em>17</em>(2), 581–596. (<a
href="https://doi.org/10.1049/ipr2.12657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery represented by DeepFake technique has raised severe societal concerns. Due to the different scales of tampering traces and the different resolutions of face images, adopting common processing pipelines and standard form of convolutional neural networks (CNNs) will lead to problems such as omission, redundancy, and bias when extracting key discriminative features. To solve the above issues, unlike most existing methods that treat face forensics as a vanilla binary classification task, the authors instead reformulate it as a multi-scale object detection problem and propose a novel framework called MSFRNet based on multi-scale feature extraction. Concretely, to alleviate the issues of features omission and redundancy, the authors construct a two-stream prediction network, where the shallow branch discovers small-scale objects such as tiny noise by capturing low-level features with higher resolution and more details, while the deep stream exploits larger receptive fields to detect large-scale blocky artefacts. Moreover, a multi-scale feature extraction module is designed to enrich feature representations in each stream. To solve the problem of features bias and ensure that unbiased feature representations are learned, more appropriate data augmentation approaches are proposed by introducing counterfactual causal reasoning. Extensive experiments demonstrate that our framework outperforms most ordinary binary classifiers and achieves positive performance.},
  archive      = {J_IETIP},
  author       = {Miaomiao Yu and Jun Zhang and Shuohao Li and Jun Lei},
  doi          = {10.1049/ipr2.12657},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {581-596},
  shortjournal = {IET Image Process.},
  title        = {MSFRNet: Two-stream deep forgery detector via multi-scale feature extraction},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). YOLOv4-dense: A smaller and faster YOLOv4 for real-time
edge-device based object detection in traffic scene. <em>IETIP</em>,
<em>17</em>(2), 570–580. (<a
href="https://doi.org/10.1049/ipr2.12656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-device-based object detection is crucial in many real-world applications, such as self-driving cars, ADAS, driver behavior analysis. Although deep learning (DL) has become the de-facto approach for object detection, the limited computing resources of embedded devices and the large model size of current DL-based methods increase the difficulty of real-time object detection on edge devices. To overcome these difficulties, in this work a novel YOLOv4-dense model is proposed to detect objects in an accurate, fast manner, which is built on top of the YOLOv4 framework but with substantial improvements. More specifically, lots of CSP layers are pruned since it will decrease inference speed. And to address the losing small objects problem, a dense block is introduced. In addition, a lightweight two-stream YOLO head is also designed to further reduce the computational complexity of the model. Experimental results on NVIDIA JETSON TX2 embedded platform demonstrate that YOLOv4-dense can achieve a higher accuracy, faster speed with smaller model size. For instance, on the KITTI dataset, YOLOv4-dense obtains 84.3% mAP and 22.6 FPS with only 20.3 M parameters, surpassing the state-of-the-art models with comparable parameter budget such as YOLOv3-tiny, YOLOv4-tiny, PP-YOLO-tiny by a large margin.},
  archive      = {J_IETIP},
  author       = {Yue Jiang and Wenjing Li and Jun Zhang and Fang Li and Zhongcheng Wu},
  doi          = {10.1049/ipr2.12656},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {570-580},
  shortjournal = {IET Image Process.},
  title        = {YOLOv4-dense: A smaller and faster YOLOv4 for real-time edge-device based object detection in traffic scene},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast intra partition and mode prediction for equirectangular
projection 360-degree video coding. <em>IETIP</em>, <em>17</em>(2),
558–569. (<a href="https://doi.org/10.1049/ipr2.12655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360-degree videos have drawn great attention from both the academia and the industry. For transmission and storage of 360-degree video, the joint video exploration team proposed the Versatile Video Coding (VVC) standard. VVC can significantly reduce the bitrate while maintaining the same subjective visual quality compared to the preceding high efficiency video coding. However, the computational complexity of VVC is extremely high which hinders the interactive applications. This paper proposes a fast intra partition method and mode prediction algorithm for equirectangular projection 360-degree video coding. First, a latitude-based preprocessing is introduced to early terminate the Coding Unit (CU) partition in the polar region. Second, the support vector machine is used to predict the CU partition type. Third, the fast intra mode search method accelerates the intra mode prediction. Experimental results show that the proposed algorithm can significantly obtain an average time reduction rate of 60.40% and a Bjontegaard delta rate increase of 1.96%.},
  archive      = {J_IETIP},
  author       = {Shu Zheng-Jie and Peng Zong-Ju and Jiang Gang-Yi and Chen Fen and Yuan Bo-Sen},
  doi          = {10.1049/ipr2.12655},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {558-569},
  shortjournal = {IET Image Process.},
  title        = {Fast intra partition and mode prediction for equirectangular projection 360-degree video coding},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning the model update with local trusted templates for
visual tracking. <em>IETIP</em>, <em>17</em>(2), 544–557. (<a
href="https://doi.org/10.1049/ipr2.12654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Siamese trackers usually do not update templates or adopt single-updating strategies. However, historical information cannot be effectively utilized when using these strategies, and model drift from complex tracking challenges cannot be addressed. To address this issue, a novel tracking framework that learns the model update with local trusted templates is proposed in this paper. The authors propose a complementary confidence evaluation method to select local trusted templates in a sliding window. This provides high-confidence historical information. The authors also propose a method including linear learning and deep learning to learn to model updates. Different from traditional update strategies, the authors’ method combines non-linear and linear updates to obtain reliable templates with the most abundant historical information, which solves the complex tracking challenges to a certain extent. Finally, the adaptive fusion response maps of the two strategies determine the final tracking based on the confidence evaluation. Experimental results on NFS, UAVDT, UAV123, UAV20L and VOT2016 show that our method performs favourably when compared with current state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Zhiyong An and Geng Chen and Ximin Zhang and Zhuhai Wang and Qingsong Xie and Bin Zhang},
  doi          = {10.1049/ipr2.12654},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {544-557},
  shortjournal = {IET Image Process.},
  title        = {Learning the model update with local trusted templates for visual tracking},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prediction of breast cancer metastasis by deep learning
pathology. <em>IETIP</em>, <em>17</em>(2), 533–543. (<a
href="https://doi.org/10.1049/ipr2.12652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of social economy, the incidence of breast cancer is increasing year by year. Whether there is lymph node metastasis in frozen tissue sections during breast cancer surgery is of tremendous priority for breast cancer surgical decision-making. Therefore, it is very significant to diagnose the pathological sections of breast cancer quickly and accurately. In this study, a model which can quickly fine segmentation of lesion regions in high-resolution breast cancer pathology sections is proposed. Firstly, pathology sections are processed by pre-processing module; Secondly, the main lesion region in pathology sections can be quickly recognized by recognition module; Thirdly, the fine segmentation of lesion region can be accomplished by segmentation module. The dataset is selected from two medical institutions to evaluate the proposed model; it achieved the average recognition precision of 0.936 for region of interest in high-resolution pathology section, with an F1-score of 0.787; and the dice for lesion region segmentation is 0.8517. The proposed model outperforms several similar works, which can effectively improve the speed and precision of pathologist&#39;s diagnosis for high-resolution breast cancer pathology sections.},
  archive      = {J_IETIP},
  author       = {Yuanyue Lu and Jun Zhang and Xueyu Liu and Zhihong Zhang and Wangxing Li and Xiaoshuang Zhou and Rongshan Li},
  doi          = {10.1049/ipr2.12652},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {533-543},
  shortjournal = {IET Image Process.},
  title        = {Prediction of breast cancer metastasis by deep learning pathology},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An image layered scrambling encryption algorithm based on a
novel discrete chaotic map. <em>IETIP</em>, <em>17</em>(2), 518–532. (<a
href="https://doi.org/10.1049/ipr2.12651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, the image encryption algorithm based on the bit-plane has the problem that the encryption speed and the encryption effect cannot be well balanced. To address the problem, an image layered scrambling encryption algorithm is proposed in this paper. First, a new one-dimensional Cosine Logistic compound chaotic map (1DCLC) is proposed. Several characteristics of the new chaotic map are analyzed by using various measures, such as bifurcation diagram, Lyapunov exponent, sample entropy etc. And the results show that the novel chaotic map has a better chaotic performance. Second, based on the chaotic map, an image encryption algorithm is proposed. The algorithm first performs an XOR encryption operation on the image. Second, the layered scrambling is applied to the XOR encrypted image. Then the layered and scrambled image is recombined for the second XOR encryption operation. In particular, in the layered scrambling, not only can the position of the pixel be changed but also the value of the pixel can be modified, which improves the security of the algorithm. In addition, in order to defend against various plaintext attacks and increase the security of the overall algorithm, the Hash function is introduced to update the key of the proposed algorithm. Finally, the original algorithm is improved and the improved algorithm&#39;s ability to resist differential attacks is analyzed. The experimental simulation and security test results show that the proposed encryption algorithm has a comparable encryption effect and a faster encryption speed.},
  archive      = {J_IETIP},
  author       = {Jianeng Tang and Zezong Zhang and Peiyang Chen and Feng Zhang and Hui Ni and Zhongming Huang},
  doi          = {10.1049/ipr2.12651},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {518-532},
  shortjournal = {IET Image Process.},
  title        = {An image layered scrambling encryption algorithm based on a novel discrete chaotic map},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimization algorithm for low-light image enhancement based
on retinex theory. <em>IETIP</em>, <em>17</em>(2), 505–517. (<a
href="https://doi.org/10.1049/ipr2.12650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the visual quality of low-light images and discover hidden details in images, an image enhancement algorithm is proposed, which is based on a fast and robust fuzzy C-means (FRFCM) clustering algorithm combined with Retinex theory. The algorithm is based on Retinex theory to solve the above problems as followings: Firstly, the initial illumination estimation image is constructed by max-RGB and segmented by FRFCM algorithm. Secondly, initial illumination estimation image and its segmented image linearly fused with a certain proportion is to obtain the optimized illumination estimation image, then is smoothed by guided filtering. Finally, reflected image is obtained by Retinex theory and the edge details of the image by equal ratio are enhanced, showing the enhanced image rich in detail texture. In order to verify the proposed algorithm, a large number of low-light image datasets are applied to test the proposed algorithm. And the effects of image enhancement of the algorithm and other existing enhanced algorithms are also compared. The experimental results show that the proposed algorithm performs well in both subjective and objective evaluation, especially the good ability to keep meticulous of details and colour retention.},
  archive      = {J_IETIP},
  author       = {Jie Yang and Jun Wang and LinLu Dong and ShuYuan Chen and Hao Wu and YaWen Zhong},
  doi          = {10.1049/ipr2.12650},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {505-517},
  shortjournal = {IET Image Process.},
  title        = {Optimization algorithm for low-light image enhancement based on retinex theory},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Residual dense collaborative network for salient object
detection. <em>IETIP</em>, <em>17</em>(2), 492–504. (<a
href="https://doi.org/10.1049/ipr2.12649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the renaissance of deep convolutional neural networks (CNN), salient object detection based on fully convolutional neural networks (FCNs) has attracted widespread attention. However, the scale variation of prominent objects, complex background features and fuzzy edges have historically been a great challenge to us. All these are closely associated with the utilization of multi-level and multi-scale features. At the same time, deep learning methods meet the challenges of computation and memory consumption in practice. To address these problems, the authors propose a different salient object detection method based on residuals learning and dense fusion learning framework. The proposed network is named Residual Dense Collaborative Network (RDCNet). First of all, the authors design a multi-layer residual learning (MRL) module to extract salient object features in more detail, getting the utmost out of the object&#39;s multi-scale and multi-level information. Then, on the basis of the vigoroso stage-wise convolution feature, the authors put forward the dilated convolution module (DCM) to acquire a rough global saliency map. Finally, the final accurate saliency detection map is obtained through dense cooperation learning (DCL), and the remaining learning is also used to improve gradually, so as to achieve high compactness and high-efficiency results. Experimental results show that this method is the most advanced method for five widely used datasets (DUTS-TE, HKU-IS, PASCAL-S, ECSSD, DUT-OMRON) without any pre-processing and post-processing. Especially on the ECSSD dataset, the F-measure of RDCNet achieves 95.2%.},
  archive      = {J_IETIP},
  author       = {Yibo Han and Liejun Wang and Shuli Cheng and Yongming Li and Anyu Du},
  doi          = {10.1049/ipr2.12649},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {492-504},
  shortjournal = {IET Image Process.},
  title        = {Residual dense collaborative network for salient object detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compression artifact reduction of low bit-rate videos via
deep neural networks using self-similarity prior. <em>IETIP</em>,
<em>17</em>(2), 480–491. (<a
href="https://doi.org/10.1049/ipr2.12648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a hybrid scheme that integratedly uses self-similarity prior and deep convolutional neural network (CNN) fusion for compression artifact reduction in low bit-rate video applications. Based on the temporal correlation hypothesis, the self-similarity prior is extended to the temporal domain by using as references not only the current decoded frame but also its neighbouring frames. Furthermore, being cognizant of that the bicubic downsampling process can typically improve the perceptual quality of a video coded at low bit-rate, for each small patch in the current frame, we search for similar patches in down-scaled versions of these references, and then form several self-similarity prior based predictions by tiling these similar patches at corresponding positions. To further exploit information flow across scales, a deep CNN model is constructed that contains two sub-networks to estimate the final output. One sub-network takes the self-similarity prior based predictions along with the decoded frame itself; and the other takes the down-scaled versions of these frames as network input. Experimental results demonstrate that the proposed method can remarkably improve, both subjectively and objectively, quality of compressed video sequences of low bit-rates.},
  archive      = {J_IETIP},
  author       = {Dongsheng Liu and Runyi Yu and Wei-Gang Chen},
  doi          = {10.1049/ipr2.12648},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {480-491},
  shortjournal = {IET Image Process.},
  title        = {Compression artifact reduction of low bit-rate videos via deep neural networks using self-similarity prior},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contribution of neural networks in image steganography,
watermarking and encryption. <em>IETIP</em>, <em>17</em>(2), 463–479.
(<a href="https://doi.org/10.1049/ipr2.12646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In front of information fishing, it is necessary to protect the information to be circulated over public channel. The goal of this paper is to propose an approach which inserts a logo inside of cover image and having three different possible results according to user&#39;s needs. The algorithm can perform steganography, watermarking or host encryption. This technique is based on singular value decomposition (SVD) and discrete wavelet transform (DWT) composed by three steps: pre-processing, embedding and extraction/decryption. Reference matrix and reference parameter are the two factors deciding the operation and the intervention of neural networks makes the choice of their parameters easier according to the desired result. For measuring the steganography performance, the authors adopt the structural similarity index measure (SSIM) which calculates the similarity between original and watermarked image and peak signal-to-noise ratio. For watermarking, the normalized correlation (NC) coefficient investigates the correlation between the original watermark and the extracted watermark. Attacking the watermarked image with common attacks used by previous publication searchers, the value of SSIM and NC coefficient are closed to one. Regarding peak signal-to-noise ratio, the overall score is around 61.73 dB. The performance score is not negligible also for the encryption method.},
  archive      = {J_IETIP},
  author       = {Maminiaina Alphonse Rafidison and Sabine Harisoa Jacques Rafanantenana and Andry Harivony Rakotomihamina and Rajaonarison Faniriharisoa Maxime Toky and Hajasoa Malalatiana Ramafiarisona},
  doi          = {10.1049/ipr2.12646},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {463-479},
  shortjournal = {IET Image Process.},
  title        = {Contribution of neural networks in image steganography, watermarking and encryption},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coarse trimap expansion based on one-class classification
for image matting. <em>IETIP</em>, <em>17</em>(2), 450–462. (<a
href="https://doi.org/10.1049/ipr2.12645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trimap is required for most image matting algorithms, as it provides partial regions with known opacity. As capturing elaborate trimaps is a time-consuming process, in practice, users prefer to provide coarse trimaps with large unknown regions. However, extant image matting algorithms cannot provide high-quality alpha mattes based on coarse trimaps. Although some matting algorithms include trimap expansion in the pre-processing stage, if this is done by directly comparing the similarity of image features between pixels, errors and omissions can easily occur. To overcome this issue, in this paper, a coarse trimap expansion model based on one-class classification is presented, in which the problem is treated as a process of reclassifying pixels in unknown regions. For this purpose, a coarse trimap expansion method denoted as CTE-OC is proposed, in which the similarity between pixels is reliably determined by measuring semantic features, allowing newly developed one-class classifiers to adequately classify pixels in entire unknown regions. The validity of these strategies is tested experimentally, and the results show that CTE-OC can significantly improve the quality of alpha mattes obtained by extant image matting methods when provided with coarse trimaps.},
  archive      = {J_IETIP},
  author       = {Yihui Liang and Yongjun Zhu and Chunjian Deng and Fujian Feng and Zhaoquan Cai},
  doi          = {10.1049/ipr2.12645},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {450-462},
  shortjournal = {IET Image Process.},
  title        = {Coarse trimap expansion based on one-class classification for image matting},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic convolutional capsule network for in-loop filtering
in HEVC video codec. <em>IETIP</em>, <em>17</em>(2), 439–449. (<a
href="https://doi.org/10.1049/ipr2.12644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, several in-loop filtering algorithms based on convolutional neural network (CNN) have been proposed to improve the efficiency of HEVC (High Efficiency Video Coding). Conventional CNN-based filters only apply a single model to the whole image, which cannot adapt well to all local features from the image. To solve this problem, an in-loop filtering algorithm based on a dynamic convolutional capsule network (DCC-net) is proposed, which embeds localized dynamic routing and dynamic segmentation algorithms into capsule network, and integrate them into the HEVC hybrid video coding framework as a new in-loop filter. The proposed method brings average 7.9% and 5.9% BD-BR reductions under all intra (AI) and random access (RA) configurations, respectively, as well as, 0.4 dB and 0.2 dB BD-PSNR gains, respectively. In addition, the proposed algorithm has an outstanding performance in terms of time efficiency.},
  archive      = {J_IETIP},
  author       = {LiChao Su and Mengqing Cao and Yue Yu and Jian Chen and XiuZhi Yang and Dapeng Wu},
  doi          = {10.1049/ipr2.12644},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {439-449},
  shortjournal = {IET Image Process.},
  title        = {Dynamic convolutional capsule network for in-loop filtering in HEVC video codec},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MPGI-terminal defect detection based on m-FRCNN.
<em>IETIP</em>, <em>17</em>(2), 428–438. (<a
href="https://doi.org/10.1049/ipr2.12643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-precision glass insulated terminal (MPGI-Terminal) is a high-precision and widely used micro-precision component. The MPGI-Terminal is small in size with 2–5 mm in diameter. It is difficult to distinguish missing blocks on the surface during manual inspection, which is prone to false detection and missing detection. The advantages of deep learning methods in the field of object detection have gradually highlighted. They can be used to construct the missing block detection model of MPGI-Terminal. A Max-Min Intersection over Union (MIoU) loss function based on overlap area, distance and aspect ratio is proposed, which helps the model to perform bounding box regression more reasonably, so as to obtain the object position more accurately. In addition, the Spatial Attention Module (SAM) and the Region of Interest Alignment (RoI Align) module are introduced to further improve the model performance. Based on the above modules, a MPGI-Terminal missing block detection model based on M-FRCNN is constructed. The results show that the proposed model can meet the requirements of fast and high accuracy of MPGI-Terminal missing block detection.},
  archive      = {J_IETIP},
  author       = {Qunpo Liu and Mengke Wang and Haixing Wang and Naohiko Hanajima},
  doi          = {10.1049/ipr2.12643},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {428-438},
  shortjournal = {IET Image Process.},
  title        = {MPGI-terminal defect detection based on M-FRCNN},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global weighted average pooling network with multilevel
feature fusion for weakly supervised brain tumor segmentation.
<em>IETIP</em>, <em>17</em>(2), 418–427. (<a
href="https://doi.org/10.1049/ipr2.12642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation plays a vital role in computer-aided diagnosis and intelligent medical treatment. It can preprocess medical images to help doctors better diagnose diseases. Class activation map (CAM) is an important technology in weakly supervised segmentation, which can achieve image segmentation without pixel-level label training. This technology can well meet the needs of medical image segmentation. However, CAM obtaining is still unperfect due to global average pooling (GAP). GAP will cause important and non-important regions to be given equal attention during the training process. So, CAM cannot demarcate the boundary of the target regions well. In order to solve this problem, a global weighted average pooling network fusing the grayscale information of medical images is proposed. The proposed network can solve the problem that GAP has the same concern for important regions and non-important regions of the feature map, because the different weights can be learned for different positions of the feature map before the GAP in the proposed model. At the same time, because of the grayscale difference between the tumor area and the non-tumor area in the brain tumor image, the low-level grayscale information of the medical image is fused with the high-level semantic information extracted by the network to learn the weights. This operation gives full play to the advantages of feature maps of different levels. The experiment results on the popular medical image dataset BraTS2019 show that the proposed method can well improve the performance of CAM and help CAM fit the boundaries of objects. Meanwhile, in the DSC evaluation, the proposed method achieves a score of 64.1%, which is a 4.6% improvement over a recent research method.},
  archive      = {J_IETIP},
  author       = {Zi-Wei Li and Shi-Bin Xuan and Xue-Dong He and Li Wang},
  doi          = {10.1049/ipr2.12642},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {418-427},
  shortjournal = {IET Image Process.},
  title        = {Global weighted average pooling network with multilevel feature fusion for weakly supervised brain tumor segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GSGAN: Learning controllable geospatial images generation.
<em>IETIP</em>, <em>17</em>(2), 401–417. (<a
href="https://doi.org/10.1049/ipr2.12641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with natural images, geospatial images cover larger area and have more complex image contents. There are few algorithms for generating controllable geospatial images, and their results are of low quality. In response to this problem, this paper proposes Geospatial Style Generative Adversarial Network to generate controllable and high-quality geospatial images. Current conditional generators suffer the mode collapse problem in geospatial field. The problem is addressed via a modified mode seeking regularization term with contrastive learning theory. Besides, the discriminator network architecture is modified to process global feature information and texture information of geospatial images. Feature loss in the generator is introduced to stabilize the training process and improve generated image quality. Comprehensive experiments are conducted on UC Merced Land Use Dataset, NWPU-RESISC45 Dataset, and AID Dataset to evaluate all compared methods. Experiment results show our method outperforms state-of-the-art models. Our method not only generates high-quality and controllable geospatial images, but also enhances the discriminator to learn better representations.},
  archive      = {J_IETIP},
  author       = {Xingzhe Su and Yijun Lin and Quan Zheng and Fengge Wu and Changwen Zheng and Junsuo Zhao},
  doi          = {10.1049/ipr2.12641},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {401-417},
  shortjournal = {IET Image Process.},
  title        = {GSGAN: Learning controllable geospatial images generation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal fusion method for human action recognition based
on IALC. <em>IETIP</em>, <em>17</em>(2), 388–400. (<a
href="https://doi.org/10.1049/ipr2.12640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In occlusion and interaction scenarios, human action recognition (HAR) accuracy is low. To address this issue, this paper proposes a novel multi-modal fusion framework for HAR. In this framework, a module called improved attention long short-term memory (IAL) is proposed, which combines the improved SE-ResNet50 (ISE-ResNet50) with long short-term memory (LSTM). IAL can extract the video sequence features and the skeleton sequence features of human behaviour. To improve the performance of HAR at a high semantic level, the obtained multi-modal sequence features are fed into a couple hidden Markov model (CHMM), and a multi-modal IAL+CHMM method called IALC is developed based on a probability graph model. To test the performance of the proposed method, experiments are conducted on the HMDB51, UCF101, Kinetics 400k, and ActivityNet datasets, and the obtained recognition accuracy are 86.40%, 97.78%, 81.12%, and 69.36% on the four datasets, respectively. The experimental results show that when the environment is complex, the proposed multi-modal fusion method for HAR based on the IALC can achieve more accurate target recognition results.},
  archive      = {J_IETIP},
  author       = {Yinhuan Zhang and Qinkun Xiao and Xing Liu and Yongquan Wei and Chaoqin Chu and Jingyun Xue},
  doi          = {10.1049/ipr2.12640},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {388-400},
  shortjournal = {IET Image Process.},
  title        = {Multi-modal fusion method for human action recognition based on IALC},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning-based framework for fast generation of
photorealistic hair animations. <em>IETIP</em>, <em>17</em>(2), 375–387.
(<a href="https://doi.org/10.1049/ipr2.12638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hair is the most important but onerous step for depicting dynamic 3D virtual characters. The photorealistic hair animation requires high-quality simulation and rendering models. These models are based on complex calculations of mechanics and optics. Because of the huge time budget, it is difficult to apply in the interactive scene. A promising solution to overcome the time budget is the reduced model that struggles to reduce the computation of physical details by various interpolation methods. However, current reduced models compromise too much reality. This research intends to achieve photorealistic hair animation in a fast way. Building a deep learning-based framework to synthesize photorealistic hair is aimed at. Furthermore, this research also presents a pipeline for hair merging into the scene. This new framework enables the model to significantly improve the appearances of hair animation while adding little computation overhead.},
  archive      = {J_IETIP},
  author       = {Zhi Qiao and Tianxing Li and Li Hui and Ruijun Liu},
  doi          = {10.1049/ipr2.12638},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {375-387},
  shortjournal = {IET Image Process.},
  title        = {A deep learning-based framework for fast generation of photorealistic hair animations},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LW-CovidNet: Automatic covid-19 lung infection detection
from chest x-ray images. <em>IETIP</em>, <em>17</em>(2), 362–374. (<a
href="https://doi.org/10.1049/ipr2.12637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus Disease 2019 (Covid-19) overtook the worldwide in early 2020, placing the world&#39;s health in threat. Automated lung infection detection using Chest X-ray images has a ton of potential for enhancing the traditional covid-19 treatment strategy. However, there are several challenges to detect infected regions from Chest X-ray images, including significant variance in infected features similar spatial characteristics, multi-scale variations in texture shapes and sizes of infected regions. Moreover, high parameters with transfer learning are also a constraints to deploy deep convolutional neural network(CNN) models in real time environment. A novel covid-19 lightweight CNN(LW-CovidNet) method is proposed to automatically detect covid-19 infected regions from Chest X-ray images to address these challenges. In our proposed hybrid method of integrating Standard and Depth-wise Separable convolutions are used to aggregate the high level features and also compensate the information loss by increasing the Receptive Field of the model. The detection boundaries of disease regions representations are then enhanced via an Edge-Attention method by applying heatmaps for accurate detection of disease regions. Extensive experiments indicate that the proposed LW-CovidNet surpasses most cutting-edge detection methods and also contributes to the advancement of state-of-the-art performance. It is envisaged that with reliable accuracy, this method can be introduced for clinical practices in the future.},
  archive      = {J_IETIP},
  author       = {Noor Ahmed and Xin Tan and Lizhuang Ma},
  doi          = {10.1049/ipr2.12637},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {362-374},
  shortjournal = {IET Image Process.},
  title        = {LW-CovidNet: Automatic covid-19 lung infection detection from chest X-ray images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motion stereo at sea: Dense 3D reconstruction from image
sequences monitoring conveyor systems on board fishing vessels.
<em>IETIP</em>, <em>17</em>(2), 349–361. (<a
href="https://doi.org/10.1049/ipr2.12636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A system that reconstructs 3D models from a single camera monitoring fish transported on a conveyor system is investigated. Models are subsequently used for training a species classifier and for improving estimates of discarded biomass. It is demonstrated that a monocular camera, combined with a conveyor&#39;s linear motion produces a constrained form of multiview structure from motion, that allows the 3D scene to be reconstructed using a conventional stereo pipeline analogous to that of a binocular camera. Although motion stereo was proposed several decades ago, the present work is the first to compare the accuracy and precision of monocular and binocular stereo cameras monitoring conveyors and operationally deploy a system. The system exploits Convolutional Neural Networks (CNNs) for foreground segmentation and stereo matching. Results from a laboratory model show that when the camera is mounted 750 mm above the conveyor, a median accuracy of &lt;5 mm can be achieved with an equivalent baseline of 62 mm. The precision is largely limited by error in determining the equivalent baseline (i.e. distance travelled by the conveyor belt). When ArUco markers are placed on the belt, the inter quartile range (IQR) of error in z (depth) near the optical centre was found to be ±4 mm.},
  archive      = {J_IETIP},
  author       = {Mark Fisher and Geoffrey French and Artjoms Gorpincenko and Helen Holah and Lauren Clayton and Rebecca Skirrow and Michal Mackiewicz},
  doi          = {10.1049/ipr2.12636},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {349-361},
  shortjournal = {IET Image Process.},
  title        = {Motion stereo at sea: Dense 3D reconstruction from image sequences monitoring conveyor systems on board fishing vessels},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Salient object detection based on edge-interior feature
fusion. <em>IETIP</em>, <em>17</em>(2), 337–348. (<a
href="https://doi.org/10.1049/ipr2.12635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, existing FCNs-based methods have shown their advantages in processing object boundaries. However, these methods still suffer from false object interference, which appears in saliency predictions. To solve this problem, an edge-interior feature fusion (EIFF) framework is proposed, which consists of an internal-boundary decoupled generation structure with receptive field enlargement and attention mechanism enhancement, and a salient feature refinement module. Specifically, the framework first learns edge features and interior features through an internal-boundary decoupling generation network, which is supervised by labels obtained by decoupling ground-truth through an image erosion algorithm. Then, feature refinement module (FRM) is designed to purify the coarse prediction by focusing on the ambiguous regions through a mining strategy to generate the final saliency map. To compensate for shortcomings of the BCE and IU loss, we also introduce a weighted loss to guide our model to focus more on the error-prone parts. Experimental results on five benchmark datasets demonstrate that the proposed method performs favorably against 19 state-of-the-art approaches under four standard metrics.},
  archive      = {J_IETIP},
  author       = {Yadi Shi and Guihe Qin and Yanhua Liang and Xinchao Wang and Jie Yan and Zhonghan Zhang},
  doi          = {10.1049/ipr2.12635},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {337-348},
  shortjournal = {IET Image Process.},
  title        = {Salient object detection based on edge-interior feature fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A screen-shooting resilient document image watermarking
scheme using deep neural network. <em>IETIP</em>, <em>17</em>(2),
323–336. (<a href="https://doi.org/10.1049/ipr2.12653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the screen-reading era, the confidential documents displayed on the screen can be easily captured by a camera without leaving any traces. Thus, this paper proposes a novel screen-shooting resilient watermarking scheme for document image using deep neural network. By applying this scheme, when the watermarked image is displayed on the screen and captured by a camera, the watermark can be still extracted from the captured photographs. Specifically, the scheme is an end-to-end neural network with an encoder to embed watermark and a decoder to extract watermark. During the training process, a distortion layer between encoder and decoder is added to simulate the distortions introduced by screen-shooting process in real scenes, such as camera distortion, shooting distortion, and light source distortion. Furthermore, a background sensitive loss and a lpips loss are used to improve visual quality of the watermarked document images in the training process. Besides, a strength factor adjustment strategy is also designed to improve the visual quality with little loss of bit extraction accuracy. The experimental results show that the proposed scheme has higher visual quality and robustness than the other two recent state-of-the-art methods.},
  archive      = {J_IETIP},
  author       = {Sulong Ge and Jianwei Fei and Zhihua Xia and Yao Tong and Jian Weng and Jianan Liu},
  doi          = {10.1049/ipr2.12653},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {323-336},
  shortjournal = {IET Image Process.},
  title        = {A screen-shooting resilient document image watermarking scheme using deep neural network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Review of surface defect detection of steel products based
on machine vision. <em>IETIP</em>, <em>17</em>(2), 303–322. (<a
href="https://doi.org/10.1049/ipr2.12647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steel plays an important role in industry, and the surface defect detection for steel products based on machine vision has been widely used during the last two decades. This paper attempts to review state-of-art of vision-based surface defect inspection technology of steel products by investigating about 170 publications. This review covers the overall aspects of vision-based surface defect inspection for steel products including hardware system, automated vision-based inspection method, existing problems and latest development. The types of steel product surface defects composition of visual inspection system are briefly described, and image acquisition system is introduced as well. The image processing algorithms for surface defect detection of steel products are reviewed, including image pre-processing, region of interest (ROI) detection, image segmentation for ROI, feature extraction and selection and defect classification. The important problems such as small sample and real time of steel surface defect detection are discussed. Finally, the challenge and development trend of steel surface defect detection are prospected.},
  archive      = {J_IETIP},
  author       = {Bo Tang and Li Chen and Wei Sun and Zhong-kang Lin},
  doi          = {10.1049/ipr2.12647},
  journal      = {IET Image Processing},
  month        = {2},
  number       = {2},
  pages        = {303-322},
  shortjournal = {IET Image Process.},
  title        = {Review of surface defect detection of steel products based on machine vision},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Retraction: IPR special issue retraction statement.
<em>IETIP</em>, <em>17</em>(1), 301. (<a
href="https://doi.org/10.1049/ipr2.12609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We, the Editors and Publisher of the journal IET Image Processing , have retracted the articles below, which make up the Special Section on Evolutionary Computation for Image Processing. Following publication, it has come to our attention that the people named as the Guest Editors of this Special Section were being impersonated by a fraudulent entity and the articles were not reviewed in line with the journal&#39;s peer review standards, and therefore, the decision has been taken to retract the articles. We did not find any evidence of misconduct by the authors. The authors have been informed of the decision to retract. Jinxing Niu, Yajie Jiang, Yayun Fu (2020) Research on image sharpening algorithm in weak illumination environment https://doi.org/10.1049/iet-ipr.2019.1588 Yongcun Cao, Saisai Ji, Yong Lu (2020) Improved artificial bee colony algorithm with opposition-based learning https://doi.org/10.1049/iet-ipr.2020.0111 Qingshan Hou, Jinsheng Xing (2020) KSSD: single-stage multi-object detection algorithm with higher accuracy https://doi.org/10.1049/iet-ipr.2020.0077 Ying Sun, Yaoqing Weng, Bowen Luo, Gongfa Li, Bo Tao, Du Jiang, Disi Chen (2020) Gesture recognition algorithm based on multi-scale feature fusion in RGB-D images https://doi.org/10.1049/iet-ipr.2020.0148 A. Kavipriya, A. Muthukumar (2021) Innovative approach for multimodal fusion recognition based feature extraction using band-limited phase-only correlation and discrete orthonormal Stockwell transform https://doi.org/10.1049/iet-ipr.2020.0145 Jingjing Yang, Jinzhao Wu, Xiaojing Wang (2020) Convolutional neural network based on differential privacy in exponential attenuation mode for image classification https://doi.org/10.1049/iet-ipr.2020.0078 Yuan Cui, Yang Yu, Zhuyang Chen, Bo Xue (2020) Research on measurement and energy efficiency improvement of flat panel display based on industrial control https://doi.org/10.1049/iet-ipr.2020.0081 Li Huang, Meiling He, Chong Tan, Du Jiang, Gongfa Li, Hui Yu (2020) Jointly network image processing: multi-task image semantic segmentation of indoor scene based on CNN https://doi.org/10.1049/iet-ipr.2020.0088 Mingyue Qian, Zhaoting Zhang, Jiechun Chen (2020) Combined mixed Gaussian model with pattern recognition in the automatic diagnosis of Alzheimer&#39;s disease https://doi.org/10.1049/iet-ipr.2019.1629 Guest Editorial: Evolutionary Computation for Image Processing https://doi.org/10.1049/iet-ipr.2020.1284},
  archive      = {J_IETIP},
  doi          = {10.1049/ipr2.12609},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {301},
  shortjournal = {IET Image Process.},
  title        = {Retraction: IPR special issue retraction statement},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motion deblurring algorithm for wind power inspection images
based on ghostnet and SE attention mechanism. <em>IETIP</em>,
<em>17</em>(1), 291–300. (<a
href="https://doi.org/10.1049/ipr2.12634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of motion blur in the inspection image of wind power generation equipment, an improved fast motion blur removal method based on deblurganv2 is proposed in this paper. Firstly, according to the characteristics of disparate local motion blur degrees and different global motion blur directions of the wind power inspection image, the real wind power fuzzy image data set is collected and produced. Secondly, according to the characteristics of the single background of the wind power inspection image and the requirements of fast and effective processing, the lightweight network Ghostnet is redesigned as the backbone network of the Deblurganv2 generator to reduce the number of network parameters and calculations. Finally, five SE channel attention mechanism layers are added to GhostNet to strengthen feature extraction, and the upsampling process is optimized through bilinear interpolation, to improve the deblurring performance of the wind power inspection image. The experimental results show that compared with other algorithms, the proposed algorithm has a higher peak signal-to-noise ratio and structural similarity, the network model parameters are compressed to 6.1 MB, and the reasoning speed is improved to 0.42 s.},
  archive      = {J_IETIP},
  author       = {Ruxin Gao and Tengfei Wang},
  doi          = {10.1049/ipr2.12634},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {291-300},
  shortjournal = {IET Image Process.},
  title        = {Motion deblurring algorithm for wind power inspection images based on ghostnet and SE attention mechanism},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bilateral association tracking with parzen window density
estimation. <em>IETIP</em>, <em>17</em>(1), 274–290. (<a
href="https://doi.org/10.1049/ipr2.12633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking is an important branch of computer vision, which is mostly used for behavior recognition and event analysis. At present, most of the research focuses on the accuracy of tracking. However, the real-time performance is also urgently desired but there is lack of research. As a result, Deepsort, proposed 5 years ago, is still the most widely used tracker in real applications. In this paper, a Bilateral Association Tracking (BAT) framework is proposed. It uses tracklet as the basic node instead of discrete detection for tracking. Meanwhile, a Parzen density based Hierarchical Agglomerative Clustering (P-HAC) algorithm is introduced to describe the density distribution of targets and generate tracklets with high confidence. In addition, Dual Appearance Features (DAF) is proposed which considers both spatial and temporal features of tracklets and promotes the accuracy of tracklet association. Experiments are conducted on popular benchmarks such as MOT2017, Visdrone and KITTI. BAT outperforms Deepsort on both association accuracy and trajectory integrity without obvious efficiency decline. Compared with other state-of-the-art trackers, BAT shows significant advantage on computational cost while performing competitive tracking accuracy as well. It is hoped that the research can promote the applications on real-time tracking in the near future.},
  archive      = {J_IETIP},
  author       = {Yuan Xu and Youyuan Chen and Yang Zhang and Qunxiong Zhu and Yanlin He and Hao Sheng},
  doi          = {10.1049/ipr2.12633},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {274-290},
  shortjournal = {IET Image Process.},
  title        = {Bilateral association tracking with parzen window density estimation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hybrid network model based on 3D convolutional neural
network and scalable graph convolutional network for hyperspectral image
classification. <em>IETIP</em>, <em>17</em>(1), 256–273. (<a
href="https://doi.org/10.1049/ipr2.12632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) contain hundreds of continuous spectral bands and are rich in spectral-spatial information. In terms of HSIs’ classification, traditional convolutional neural networks (CNNs) extract features based on HSI&#39;s spectral-spatial information through 2D convolution. However, 2D convolution extracts features in 2D plane without considering the relationships between spectral bands, which inevitably leads to insufficient feature extraction. 3D convolutional neural networks (3DCNNs) take account of the correlations among spectral bands and outperform 2D convolutional networks in feature extraction, but the computational cost is rather expensive. To address the above problem, a light-weight three-layer 3D convolutional network Module (3D-M) for HSIs’ spectral-spatial feature extraction is proposed. Another challenge is that neither 2D convolution nor 3D convolution utilizes the structural information inherent in the data. Graph convolution networks (GCNs) can model and utilize such information through the similarity matrix, also known as adjacency matrix. However, traditional GCNs cannot handle large-scale data because they construct adjacency matrix on all data, which results in high computational complexity and large storage requirement. To conquer this challenge, this article proposes a batch-graph strategy on which a scalable GCN is developed. Finally, a hybrid network model (HNM) based on the proposed light-weight 3D-M and scalable GCN is presented. HNM extracts spectral-spatial features of HSIs with low computational complexity through the light-weight 3D convolution network and leverages the structural information in data via the scalable GCN. The experimental results on three public datasets with different sizes demonstrate that the proposed HNM produces better classification results than other state-of-the-art hyperspectral images classification models in terms of overall accuracy (OA), average accuracy (AA) and kappa coefficient (Kappa).},
  archive      = {J_IETIP},
  author       = {Xili Wang and Zhengyin Liang},
  doi          = {10.1049/ipr2.12632},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {256-273},
  shortjournal = {IET Image Process.},
  title        = {Hybrid network model based on 3D convolutional neural network and scalable graph convolutional network for hyperspectral image classification},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised skin lesion segmentation based on
spot-seeds guided optimal regions. <em>IETIP</em>, <em>17</em>(1),
239–255. (<a href="https://doi.org/10.1049/ipr2.12631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic skin lesion segmentation is the most critical and relevant task in computer-aided skin cancer diagnosis. Methods based on convolutional neural networks (CNNs) are mainly used in current skin lesion segmentation. The requirement of huge pixel-level labels is a significant obstacle to achieve semantic segmentation of skin lesion by CNNs. In this paper, a novel weakly supervised framework for skin lesion segmentation is presented, which generates high-quality pixel-level annotations and optimizes the segmentation network. A hierarchical image segmentation algorithm can predict a boundary map for training images. Then, the optimal regions of candidate hierarchical levels are selected. Afterward, Superpixels-CRF built on the optimal regions is guided by spot seeds to propagate information from spot seeds to unlabeled regions, resulting in high-quality pixel-level annotations. Using these high-quality pixel-level annotations, a segmentation network can be trained and segmentation masks can be predicted. To iteratively optimize the segmentation network, the predicted segmentation masks are refined and the segmentation network are retrained. Comparative experiments demonstrate that the proposed segmentation framework reduces the gap between weakly and fully supervised skin lesion segmentation methods, and achieves state-of-the-art performance while reducing human labeling efforts.},
  archive      = {J_IETIP},
  author       = {Zaid Al-Huda and Yuan Yao and Jing Yao and Bo Peng and Ali Raza},
  doi          = {10.1049/ipr2.12631},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {239-255},
  shortjournal = {IET Image Process.},
  title        = {Weakly supervised skin lesion segmentation based on spot-seeds guided optimal regions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An effective model for the iris regional characteristics and
classification using deep learning alex network. <em>IETIP</em>,
<em>17</em>(1), 227–238. (<a
href="https://doi.org/10.1049/ipr2.12630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris biometrics is one of the fastest-growing technologies, and it has received a lot of attention from the community. Iris-biometric-based human recognition does not require contact with the human body. Iris is a combination of crypts, wolflin nodules, concentrated furrows, and pigment spots. The existing methods feed the eye image into deep learning network which result in improper iris features and certainly reduce the accuracy. This research study proposes a model to feed preprocessed accurate iris boundary into Alexnet deep learning neural network-based system for classification. The pupil centre and boundary are initially recorded and identified from the given eye images. The iris boundary and the centre are then compared for the identification using the reference pupil centre and boundary. The iris portion, exclusive feature of the pupil area is segmented using the parameters of multiple left-right point (MLRP) algorithms. The Alexnet deep learning multilayer networks 23, 24, and 25 are replaced according to the segmented iris classes. The remaining Alexnet layers are trained using the square gradient decay factor (GDF) in accordance with the iris features. The trained Alexnet iris is validated using suitable classes. The proposed system classifies the iris with an accuracy of 99.1%. The sensitivity, specificity, and F1-score of the proposed system are 99.68%, 98.36%, and 0.995. The experimental results show that the proposed model has advantages over current models.},
  archive      = {J_IETIP},
  author       = {Thiyaneswaran Balashanmugam and Kumarganesh Sengottaiyan and Martin Sagayam Kulandairaj and Hien Dang},
  doi          = {10.1049/ipr2.12630},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {227-238},
  shortjournal = {IET Image Process.},
  title        = {An effective model for the iris regional characteristics and classification using deep learning alex network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware siamese network for object tracking.
<em>IETIP</em>, <em>17</em>(1), 215–226. (<a
href="https://doi.org/10.1049/ipr2.12629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, temporal and spatial contexts are widely used to improve the adaptability of a tracker. However, most existing methods usually focus on one aspect of the temporal or spatial context and rarely exploit them simultaneously. In this paper, a context-aware Siamese Network (CSNet) is proposed, which skilfully integrates the modelling of temporal and spatial context into the Siamese tracking framework. Specifically, CSNet consists of a context-based channel attention module and a context-based cross-attention module. The former aggregates spatial context information from different channels and dynamically emphasizes target features, which makes it easier for the tracker to distinguish the target from the background. The latter propagates the temporal context from the previous frames to the current frame to establish the part-level relationship between the search region and the historical target state, which enables the tracker better adapt to the target deformation. In addition, to further mine context information, the CSNet is equipped with a state-aware strategy to control the contribution of different context information in tracking. Extensive experiments on OTB2015, UAV123, GOT-10k, LaSOT, and TrackingNet show that the proposed tracking method achieves comparable performance to the advanced trackers.},
  archive      = {J_IETIP},
  author       = {Jianwei Zhang and Jingchao Wang and Huanlong Zhang and Mengen Miao and Di Wu},
  doi          = {10.1049/ipr2.12629},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {215-226},
  shortjournal = {IET Image Process.},
  title        = {Context-aware siamese network for object tracking},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-similarity based hyperrelation network for few-shot
segmentation. <em>IETIP</em>, <em>17</em>(1), 204–214. (<a
href="https://doi.org/10.1049/ipr2.12628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation aims at recognizing the object regions of unseen categories with only a few annotated examples as supervision. The key to few-shot segmentation is to establish a robust semantic relationship between the support and query images and to prevent overfitting. In this paper, an effective multi-similarity hyperrelation network (MSHNet) is proposed to tackle the few-shot semantic segmentation problem. In MSHNet, a new generative prototype similarity (GPS) is proposed, which, together with cosine similarity, establishes a strong semantic relationship between supported images and query images. In addition, a symmetric merging block (SMB) in MSHNet is proposed to efficiently merge multi-layer, multi-shot, multi-similarity features to generate hyperrelation features for semantic segmentation. Experimenting on two benchmark semantic segmentation datasets (Pascal − 5 i and COCO − 20 i ) shows that this method achieves a mean intersection-over-union score of 72.3% and 56.0%, respectively, which outperforms the state-of-the-art methods by 1.9% and 6.5%.},
  archive      = {J_IETIP},
  author       = {Xiangwen Shi and Zhe Cui and Shaobing Zhang and Miao Cheng and Lian He and Xianghong Tang},
  doi          = {10.1049/ipr2.12628},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {204-214},
  shortjournal = {IET Image Process.},
  title        = {Multi-similarity based hyperrelation network for few-shot segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-view image fusion algorithm for industrial weld.
<em>IETIP</em>, <em>17</em>(1), 193–203. (<a
href="https://doi.org/10.1049/ipr2.12627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view image fusion can be used to extract features from redundant and complementary multisource images. And the technique of obtaining high quality fusion images has become one of the research hotspots for image processing. In order to realize defect detection and intelligent grinding smoothly, multi-view fusion technology was applied in the field of overexposure and underexposure industrial welds, achieving high quality image enhancement. When preparing the data set of multi-view images, a hybrid registration algorithm with high matching ability is proposed. The data set of overexposure and underexposure weld images was obtained successfully by using the registration algorithm. In order to improve the fusion ability of overexposure and underexposure industrial welds, we propose a novel multi-view image fusion algorithm based on deep learning. The multi-view fusion algorithm uses an autoencoder network structure, and its innovation lies in a parallel branch network with lightweight structure and strong generalization ability. The experimental results demonstrate that compared with other classical multi-view algorithms, our proposed algorithm gets the best parameters on the industrial weld data set in peak signal to noise ratio (PSNR) and root mean square error (RMSE), reaching 59.12 and 0.084, respectively. And the ablation and performance comparison experiments verify that the proposed parallel branch network has better generalization ability and fusion accuracy than other classical multi branch networks.},
  archive      = {J_IETIP},
  author       = {Qingchun Zheng and Yangyang Zhao and Xu Zhang and Peihao Zhu and Wenpeng Ma},
  doi          = {10.1049/ipr2.12627},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {193-203},
  shortjournal = {IET Image Process.},
  title        = {A multi-view image fusion algorithm for industrial weld},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human elbow flexion behaviour recognition based on posture
estimation in complex scenes. <em>IETIP</em>, <em>17</em>(1), 178–192.
(<a href="https://doi.org/10.1049/ipr2.12626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the difficulty of recognising the smoking and making phone calls behaviours of people in the complex background of construction sites, a method of recognising human elbow flexion behaviour based on posture estimation is proposed. The human upper body key points needed are retrained based on AlphaPose to achieve human object localization and key points detection. Then, a mathematical model for human elbow flexion behaviour discrimination (HEFBD model) is proposed based on human key points, as well as locating the region of interest for small object detection and reducing the interference of complex background. A super-resolution image reconstruction method is used for pre-processing some blurred images. In addition, YOLOv5s is improved by adding a small object detection layer and integrating a convolutional block attention model to improve the detection performance. The detection precision of this method is improved by 5.6%, and the false detection rate caused by complex background is reduced by 13%, which outperforms other state-of-the-art detection methods and meets the requirement of real-time performance.},
  archive      = {J_IETIP},
  author       = {Faming Gong and Yunjing Li and Xiangbing Yuan and Xin Liu and Yating Gao},
  doi          = {10.1049/ipr2.12626},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {178-192},
  shortjournal = {IET Image Process.},
  title        = {Human elbow flexion behaviour recognition based on posture estimation in complex scenes},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SA-BiSeNet: Swap attention bilateral segmentation network
for real-time inland waterways segmentation. <em>IETIP</em>,
<em>17</em>(1), 166–177. (<a
href="https://doi.org/10.1049/ipr2.12625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology for autonomous navigation on inland waterways is worth investigating, and navigable water surface segmentation is a key part of this technology. Semantic segmentation methods based on deep learning are able to distinguish between water surface areas and non-water surface areas. However, existing semantic segmentation methods cannot meet the requirements of the water surface segmentation task in terms of both segmentation precision and real-time performance. In this study, a Swap Attention Bilateral Segmentation Network (SA-BiSeNet) is proposed to improve segmentation performance while ensuring model inference speed by better fusing the two features of the dual-branch down-sampling network using the attention mechanism. Specifically, an innovative Swap Attention Module is designed to model the dependency between the features of the spatial detail branch and the features of the semantic branches, thus expanding the receptive fields of the spatial detail and semantic branches to each other&#39;s global contexts. This design can effectively fuse features and thus enhance feature representation. Experiments were conducted on the inland waterway dataset USVInland to verify the performance of SA-BiSeNet in terms of segmentation precision and inference speed, and SA-BiSeNet achieved 93.65% Mean IoU and maintained the same level of fps as the baseline.},
  archive      = {J_IETIP},
  author       = {W.B. Zhang and C.Y. Wu and Z.S. Bao},
  doi          = {10.1049/ipr2.12625},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {166-177},
  shortjournal = {IET Image Process.},
  title        = {SA-BiSeNet: Swap attention bilateral segmentation network for real-time inland waterways segmentation},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Image super-resolution based on self-similarity generative
adversarial networks. <em>IETIP</em>, <em>17</em>(1), 157–165. (<a
href="https://doi.org/10.1049/ipr2.12624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention has been successfully leveraged for long-range feature-wise similarities in deep learning super-resolution (SR) methods. However, most of the SR methods only explore the features on the original scale, but do not take full advantage of self-similarities features on different scales especially in generative adversarial networks (GAN). In this paper, self-similarity generative adversarial networks (SSGAN) are proposed as the SR framework. The framework establishes the multi-scale feature correlation by adding two modules to the generative network: downscale attention block (DAB) and upscale attention block (UAB). Specifically, DAB is designed to restore the repetitive details from the corresponding downsampled image, which achieves multi-scale feature restoration through self-similarity. And UAB improves the baseline up-sampling operations and captures low-resolution to high-resolution feature mapping, which enhances the cross-scale repetitive features to reconstruct the high-resolution image. Experimental results demonstrate that the proposed SSGAN achieve better visual performance especially in the similar pattern details.},
  archive      = {J_IETIP},
  author       = {Shuang Wang and Zhengxing Sun and Qian Li},
  doi          = {10.1049/ipr2.12624},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {157-165},
  shortjournal = {IET Image Process.},
  title        = {Image super-resolution based on self-similarity generative adversarial networks},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geodesic gramian denoising applied to the images
contaminated with noise sampled from diverse probability distributions.
<em>IETIP</em>, <em>17</em>(1), 144–156. (<a
href="https://doi.org/10.1049/ipr2.12623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As quotidian use of sophisticated cameras surges, people in modern society are more interested in capturing fine-quality images. However, the quality of the images might be inferior to people&#39;s expectations due to the noise contamination in the images. Thus, filtering out the noise while preserving vital image features is an essential requirement. Existing denoising methods have assumptions, on the probability distribution in which the contaminated noise is sampled, for the method to attain its expected denoising performance. In this paper, the recent Gramian-based filtering scheme to remove noise sampled from five prominent probability distributions from selected images is utilized. This method preserves image smoothness by adopting patches partitioned from the image, rather than pixels, and retains vital image features by performing denoising on the manifold underlying the patch space rather than in the image domain. Its denoising performance is validated, using six benchmark computer vision test images applied to two state-of-the-art denoising methods, namely BM3D and K-SVD.},
  archive      = {J_IETIP},
  author       = {Kelum Gajamannage and Yonggi Park and Alexey Sadovski},
  doi          = {10.1049/ipr2.12623},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {144-156},
  shortjournal = {IET Image Process.},
  title        = {Geodesic gramian denoising applied to the images contaminated with noise sampled from diverse probability distributions},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EDAfuse: A encoder–decoder with atrous spatial pyramid
network for infrared and visible image fusion. <em>IETIP</em>,
<em>17</em>(1), 132–143. (<a
href="https://doi.org/10.1049/ipr2.12622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible images come from different sensors, and they have their advantages and disadvantages. In order to make the fused images contain as much salience information as possible, a practical fusion method, termed EDAfuse, is proposed in this paper. In EDAfuse, the authors introduce an encoder–decoder with the atrous spatial pyramid network for infrared and visible image fusion. The authors use the encoding network which includes three convolutional neural network (CNN) layers to extract deep features from input images. Then the proposed atrous spatial pyramid model is utilized to get five different scale features. The same scale features from the two original images are fused by our fusion strategy with the attention model and information quantity model. Finally, the decoding network is utilized to reconstruct the fused image. In the training process, the authors introduce a loss function with saliency loss to improve the ability of the model for extracting salient features from original images. In the experiment process, the authors use the average values of seven metrics for 21 fused images to evaluate the proposed method and the other seven existing methods. The results show that our method has four best values and three second-best values. The subjective assessment also demonstrates that the proposed method outperforms the state-of-the-art fusion methods.},
  archive      = {J_IETIP},
  author       = {Cairen Nie and Dongming Zhou and Rencan Nie},
  doi          = {10.1049/ipr2.12622},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {132-143},
  shortjournal = {IET Image Process.},
  title        = {EDAfuse: A encoder–decoder with atrous spatial pyramid network for infrared and visible image fusion},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-source heterogeneous iris segmentation method based on
lightweight convolutional neural network. <em>IETIP</em>,
<em>17</em>(1), 118–131. (<a
href="https://doi.org/10.1049/ipr2.12621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Iris segmentation is a critical step in the iris recognition system. Since the quality of iris database taken under different camera sensors varies greatly, thus most existing iris segmentation methods are designed for a particular collection device. Meanwhile, many iris segmentation methods based on convolutional neural networks (CNNs) require a lot of computational costs and hardware costs (storage space), which are not suitable for deploying on low-performance devices. To address the above problems, an accurate and efficient heterogeneous iris segmentation network is proposed in this paper. First, the authors design an efficient feature extraction network, which combines depth-wise separable convolution with traditional convolution to greatly reduce model parameters and computational cost while maintaining segmentation accuracy. Then, a Multi-scale Context Information Extraction Module (MCIEM) is proposed to extract multi-scale spatial information at a more granular level and enhance the discriminability of the iris region. Finally, a Multi-layer Feature Information Fusion Module (MFIFM) is proposed to reduce the loss of information during the downsampling process. Experimental results on multi-source heterogeneous iris database show that the proposed network can not only achieve state-of-the-art performance but is also more efficient in terms of required parameters, calculated load, and storage space.},
  archive      = {J_IETIP},
  author       = {Guang Huo and Dawei Lin and Meng Yuan},
  doi          = {10.1049/ipr2.12621},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {118-131},
  shortjournal = {IET Image Process.},
  title        = {Multi-source heterogeneous iris segmentation method based on lightweight convolutional neural network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Research on the application of body posture action feature
extraction and recognition comparison. <em>IETIP</em>, <em>17</em>(1),
104–117. (<a href="https://doi.org/10.1049/ipr2.12620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly conducts a comparative study of body posture action feature extraction and recognition in three-dimensional (3D) space through motion capture technology and virtual reality technology. Firstly, this paper proposes a body posture feature extraction method in 3D space to obtain joint point data and conduct body posture feature extraction research. At the same time, this method is used for action recognition research, which solves the difficulty of traditional methods in 3D scene feature extraction and motion recognition. In addition, this paper proposes two action comparison methods in 3D space. One is a ‘distance threshold method’ that calculates the distance between bones in the form of joint points and bones, which can finally provide the effect of dynamic display of sliders; the other is the ‘feature plane method’ that calculates the detailed information based on different skeletal joint point planes, and finally provides a text display effect with detailed body posture difference values. Finally, combined with virtual reality technology, an application platform for body posture feature recognition and comparison is designed and implemented, which solves the problems of poor visualization effect and weak interaction of traditional methods.},
  archive      = {J_IETIP},
  author       = {Jia-Jun Zhao and Zhi-Qiang Liu and Si-Jia Xie and Chuan-Qian Tang},
  doi          = {10.1049/ipr2.12620},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {104-117},
  shortjournal = {IET Image Process.},
  title        = {Research on the application of body posture action feature extraction and recognition comparison},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Brain tumour detection in magnetic resonance imaging using
levenberg–marquardt backpropagation neural network. <em>IETIP</em>,
<em>17</em>(1), 88–103. (<a
href="https://doi.org/10.1049/ipr2.12619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) is a high-quality medical image that is used to detect brain tumours in a complex and time-consuming manner. In this study, a back propagation neural network (BPNN) along with the Levenberg–Marquardt algorithm (LMA) is proposed to classify MRIs and diagnose brain tumours in a simple and fast process. The BPNN has 10 neurons in the hidden layer, and the default function of the feedforward feeds is mean squared error (MSE). The LMA is optimized as a multivariable adaptive approach and considerably decreases the MSE of the BPNN, so the errors of the tumour classification are diminished. The proposed method follows four steps including preprocessing, skull removal, feature extraction, and classification. The input MRIs are converted to greyscale, resized, and thresholding is performed in the preprocessing step and followed by skull removal. Morphological operations of closing, opening, and dilation are used to segment abnormal areas in the MRIs, and the opening operator recognizes the tumour more accurately. Using statistical analysis and a grey-level co-occurrence matrix (GLCM) 12 features are extracted from the MRIs and used as the inputs of the BPNN. To evaluate the proposed method, 670 normal and 670 abnormal brain MRIs are used as input data, and the classification is performed in 0.494 s. The accuracy, sensitivity, specificity, precision, dice, recall, and MSE are 98.7%, 97.61%, 99.7%, 97.61%, 98.6%, 97.61%, and 0.005, respectively. The approach is accurate and fast for medical images classification.},
  archive      = {J_IETIP},
  author       = {Marzieh Ghahramani and Nabiollah Shiri},
  doi          = {10.1049/ipr2.12619},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {88-103},
  shortjournal = {IET Image Process.},
  title        = {Brain tumour detection in magnetic resonance imaging using Levenberg–Marquardt backpropagation neural network},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A modified 3D EfficientNet for the classification of
alzheimer’s disease using structural magnetic resonance images.
<em>IETIP</em>, <em>17</em>(1), 77–87. (<a
href="https://doi.org/10.1049/ipr2.12618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural magnetic resonance imaging (MRI) provides useful information for biomarker exploration and intelligent classification of Alzheimer&#39;s disease (AD). Machine learning and deep learning methods have been used in feature extraction and computer-aided diagnosis or prediction of AD. In this research, a new deep learning model is developed to detect or predict AD in an effective manner. A modified 3D EfficientNet with a sequentially connected 3D mobile inverted bottleneck convolution (MBConv) block is proposed to explore the multiscale characteristics of brain MR images for AD classification. The 3D MBConv block consists of depthwise convolution and a squeeze-and-excitation module to transform the input features into more compact features with fewer parameters than those in standard convolution. The proposed method was evaluated considering four classification tasks: (1) NC versus AD, (2) NC versus all MCI, (3) NC versus pMCI, and (4) sMCI versus pMCI. The proposed model achieved accuracies of 95.00%, 86.67%, and 83.33% for the classification of NC versus AD, NC versus pMCI, and sMCI versus pMCI, respectively, and exhibited a high performance in comparison with the classical networks and several existing methods. Efficient deep learning networks are powerful tools for the early diagnosis and prediction of AD.},
  archive      = {J_IETIP},
  author       = {Bowen Zheng and Ang Gao and Xiaona Huang and Yuhan Li and Dong Liang and Xiaojing Long},
  doi          = {10.1049/ipr2.12618},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {77-87},
  shortjournal = {IET Image Process.},
  title        = {A modified 3D EfficientNet for the classification of alzheimer&#39;s disease using structural magnetic resonance images},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An automatic plant leaf stoma detection method based on
YOLOv5. <em>IETIP</em>, <em>17</em>(1), 67–76. (<a
href="https://doi.org/10.1049/ipr2.12617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stomata on the leaf surface are mainly responsible for the material exchange between the internal and external environments of the plant, a large number of methods have been proposed to automatically measure the distribution position and number of stomatal, but few methods could achieve both stomatal count and open/closed-state judgment. Therefore, this study proposes an automatic detection method for leaf stomatal morphology analysis based on an attention mechanism and deep learning. In order to obtain more stomatal feature information and send it to the network for learning, the proposed method adds a coordinate attention (CA) mechanism to the YOLOV5 backbone part. At the same time, in order to avoid the overfitting of the model during the training process, the authors added the training trick of label smoothing. Finally, the detection ability of the proposed method for stomata is verified on the broad bean leaves stomata dataset. The experimental results show that our method achieves a detection accuracy of 0.934 and an mAP of 0.968. By comparing with other state-of-the-art algorithms, the detection capability of our method has been significantly improved. The generalization of the model is verified on the wheat leaf stomatal dataset. The experimental results show that our method can achieve a detection accuracy of 0.894 and an mAP of 0.907.},
  archive      = {J_IETIP},
  author       = {Xin Li and Siyu Guo and Linrui Gong and Yuan Lan},
  doi          = {10.1049/ipr2.12617},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {67-76},
  shortjournal = {IET Image Process.},
  title        = {An automatic plant leaf stoma detection method based on YOLOv5},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). I-CenterNet: Road infrared target detection based on
improved CenterNet. <em>IETIP</em>, <em>17</em>(1), 57–66. (<a
href="https://doi.org/10.1049/ipr2.12616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared target detection has strong anti-interference ability, long working distance and can work day and night. So it is widely used in military security and transportation fields, and infrared road object detection is critical in traffic checkpoints and autonomous driving. However, the target scale in infrared images changes greatly, small targets are difficult to detect, the poor image quality and low signal-to-noise ratio are still huge challenges in infrared target detection. This paper proposes an improved infrared target detection model I-CenterNet based on the anchor-free model CenterNet. The EfficientNetV2 with the channel attention mechanism is used instead of the traditional structure as the backbone network to enhance feature extraction. In order to reduce the noise of the input infrared image, Dilated-Residual U-net (DRUNet) is used. Meanwhile, feature pyramid and Sub-Pixel are combined for multi-scale feature fusion. Data enhancement is implemented to improve model performance. The experimental results show that the average detection accuracy of this model on the Flir infrared data set is 87.9%, and the average detection speed reaches 14.2 frames/s.},
  archive      = {J_IETIP},
  author       = {Xiang Li and Yurong Qian and Rui Guo and Naixiang Ao},
  doi          = {10.1049/ipr2.12616},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {57-66},
  shortjournal = {IET Image Process.},
  title        = {I-CenterNet: Road infrared target detection based on improved CenterNet},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visible part prediction and temporal calibration for
pedestrian detection. <em>IETIP</em>, <em>17</em>(1), 42–56. (<a
href="https://doi.org/10.1049/ipr2.12615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite their great advancement, current pedestrian detection methods focus on single static images, which fail to employ richer information available from the video sequences. Compared with still images, videos can offer temporal information of objects in the time dimension, thus providing the potential to obtain more robust detection performance. Here, a novel pedestrian detection method based on visible part detection and temporal calibration is proposed. Specifically, a part-aware module to predict the visible body part of each pedestrian instance, which enables us to obtain precise motion information of partially occluded pedestrians in a video sequence, is first developed. Then, the temporal coherence for each pedestrian instance based on the predicted motion information is constructed. After that, an adaptive temporal calibration method is introduced to effectively calibrate the final detection result. This method on two video pedestrian detection benchmarks, that is, Caltech-New and MOT17Det, is evaluated. Experimental results show that this method performs favourably against existing pedestrian detection approaches.},
  archive      = {J_IETIP},
  author       = {Peiyu Yang and Weixi Li and Lu Wang and Lisheng Xu and Qingxu Deng},
  doi          = {10.1049/ipr2.12615},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {42-56},
  shortjournal = {IET Image Process.},
  title        = {Visible part prediction and temporal calibration for pedestrian detection},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FAFNet: Fully aligned fusion network for RGBD semantic
segmentation based on hierarchical semantic flows. <em>IETIP</em>,
<em>17</em>(1), 32–41. (<a
href="https://doi.org/10.1049/ipr2.12614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth maps are acquirable and irreplaceable geometric information that significantly enhances traditional color images. RGB and Depth (RGBD) images have been widely used in various image analysis applications, but they are still very limited due to challenges from different modalities and misalignment between color and depth. In this paper, a Fully Aligned Fusion Network (FAFNet) for RGBD semantic segmentation is presented. To improve cross-modality fusion, a new RGBD fusion block is proposed, features from color images and depth maps are first fused by an attention cross fusion module and then aligned by a semantic flow. A multi-layer structure is also designed to hierarchically utilize the RGBD fusion block, which not only eases issues of low resolution and noises for depth maps but also reduces the loss of semantic features in the upsampling process. Quantitative and qualitative evaluations on both the NYU-Depth V2 and the SUN RGB-D dataset demonstrate that the FAFNet model outperforms state-of-the-art RGBD semantic segmentation methods.},
  archive      = {J_IETIP},
  author       = {Jiazhou Chen and Yangfan Zhan and Yanghui Xu and Xiang Pan},
  doi          = {10.1049/ipr2.12614},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {32-41},
  shortjournal = {IET Image Process.},
  title        = {FAFNet: Fully aligned fusion network for RGBD semantic segmentation based on hierarchical semantic flows},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A two-stage hue-preserving and saturation improvement color
image enhancement algorithm without gamut problem. <em>IETIP</em>,
<em>17</em>(1), 24–31. (<a
href="https://doi.org/10.1049/ipr2.12613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel hue-preserving and color enrichment color image enhancement algorithm without gamut problem is proposed in this paper. First, the pixel values of the input image are stretched to the edge of RGB color space to improve the saturation. Then, the pixel values are converted to the complementary color space for saturation enhancement, so that the whole method has no gamut problem and keeps the contrast intensity of the output picture unchanged. Experimental results demonstrate that the proposed algorithm can overcome the poor color effect of traditional methods in the condition of low exposure and can restore the picture color better. Besides, higher saturation can be achieved through the proposed algorithm than up-to-date color image enhancement methods.},
  archive      = {J_IETIP},
  author       = {Datang Zhou and Gang He and Kang Xu and Chang Liu},
  doi          = {10.1049/ipr2.12613},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {24-31},
  shortjournal = {IET Image Process.},
  title        = {A two-stage hue-preserving and saturation improvement color image enhancement algorithm without gamut problem},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Investigation on the blurred image restoration based on
brain-inspired model. <em>IETIP</em>, <em>17</em>(1), 12–23. (<a
href="https://doi.org/10.1049/ipr2.12612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though relatively good effect has been achieved by the image de-blurring method based on deep learning, the existing methods still suffer from the problem of unclear restoration of the edges. Therefore, brain-inspired image restoration model based on human attention and “fine vision” is proposed to improve the blind restoration quality of the image in this paper according to the response mechanism of the different cerebral cortices for high and low spatial resolutions. The designed brain-inspired model consists of dual-channel network available to realize the function of feature merger for low and high resolutions, which is used to extract the image edges with detailed information filtered out. Confirmatory experiment is implemented based on the blurred image in the data set of GOPRO, LIVE and set14. As per the result, the model proposed is available for relatively good restoration of blurred image and super-resolution, as well as looking results by visual inspection.},
  archive      = {J_IETIP},
  author       = {Meng Xiangyan and Zhao Chen and Zhao Li and Yan Keding and Ren Yumiao and Pan Haixian},
  doi          = {10.1049/ipr2.12612},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {12-23},
  shortjournal = {IET Image Process.},
  title        = {Investigation on the blurred image restoration based on brain-inspired model},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep side group sparse coding network for image denoising.
<em>IETIP</em>, <em>17</em>(1), 1–11. (<a
href="https://doi.org/10.1049/ipr2.12610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has made significant progress in image denoising. However, most of existing deep learning based methods are purely data-driven, without considering the knowledge of image denoising. Moreover, the parameters of deep denoising network are not explainable. According to these issues, this paper proposes a deep side group sparse coding network for image denoising, named a side group sparse coding (SGSC)-Net. First, SGSC model for image denoising by exploiting prior information regarding the group sparse coefficients consistency is developed. Specifically, the side information is constructed as the weighted combination of intermediate estimations, and updated iteratively. Then, the optimisation solution of SGSC model is turned into a deep neural network using deep unfolding, that is, SGSC-Net. The computational path of SGSC-Net fully follows the iterations of optimisation solution, and consequently the network parameters are interpretable. Furthermore, the design of SGSC-Net employs the insight of SGSC denoising model. The experimental results on well-known datasets quantitatively and qualitatively demonstrate that SGSC-Net is competitive to existing deep unfolding-based and typical deep neural network-based methods.},
  archive      = {J_IETIP},
  author       = {Haitao Yin and Tianyou Wang},
  doi          = {10.1049/ipr2.12610},
  journal      = {IET Image Processing},
  month        = {1},
  number       = {1},
  pages        = {1-11},
  shortjournal = {IET Image Process.},
  title        = {Deep side group sparse coding network for image denoising},
  volume       = {17},
  year         = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
